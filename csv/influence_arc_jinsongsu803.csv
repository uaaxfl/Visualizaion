2020.acl-main.273,W18-6402,0,0.192307,"n multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships"
2020.acl-main.273,P18-1026,0,0.039539,"Missing"
2020.acl-main.273,W17-4746,0,0.343479,"Missing"
2020.acl-main.273,N19-1422,0,0.199371,"model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. Note that if we directly adapt the approach proposed by Huang et al. (2016) into Transformer, the model (O"
2020.acl-main.273,D17-1105,0,0.0280843,"boarding ramp” is not translated correctly by all baselines, while our model correctly translates it. This reveals that our encoder is able to learn more accurate representations. 3.6 Results on the En⇒Fr Translation Task We also conduct experiments on the EN⇒Fr dataset. From Table 3, our model still achieves better performance compared to all baselines, which demonstrates again that our model is effective and general to different language pairs in multi-modal NMT. 4 Related Work Multi-modal NMT Huang et al. (2016) first incorporate global or regional visual features into attention-based NMT. Calixto and Liu (2017) also study the effects of incorporating global visual features into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-m"
2020.acl-main.273,P17-1175,0,0.136326,"Missing"
2020.acl-main.273,P19-1642,0,0.690464,"translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model performance. To this end, a lot of efforts have been made, roughly consisting of: (1) encoding each input image into a global feature vector, which can be used to initialize different components of multi-modal NMT models, or as additional source tokens (Huang et al., 2016; Calixto et al., 2017), or to learn the joint multi-modal representation (Zhou et al., 2018; Calixto et al., 2019); (2) extracting object-based image features to initialize the model, or supplement source sequences, or generate attention-based visual context (Huang et al., 2016; Ive et al., 2019); and (3) representing each image as spatial features, which can be exploited as extra context (Calixto et al., 2017; Delbrouck and Dupont, 2017a; Ive et al., 2019), or a supplement to source semantics (Delbrouck and Dupont, 2017b) via an attention mechanism. Despite their success, the above studies do not fully exploit the fine-grained semantic correspondences between semantic units within an input sentence-image"
2020.acl-main.273,D17-1095,0,0.143417,"Missing"
2020.acl-main.273,W14-3348,0,0.106532,"el tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based multi-modal fusion layers. Baseline Models"
2020.acl-main.273,D18-1329,0,0.234241,"into different NMT components. Elliott and K´ad´ar (2017) share an encoder between a translation model and an image prediction model to learn visually grounded representations. Besides, the most common practice is to use attention mechanisms to extract visual contexts for multimodal NMT (Caglayan et al., 2016; Calixto et al., 2017; Delbrouck and Dupont, 2017a,b; Barrault et al., 2018). Recently, Ive et al. (2019) propose a translate-and-refine approach and Calixto et al. (2019) employ a latent variable model to capture the multi-modal interactions for multi-modal NMT. Apart from model design, Elliott (2018) reveal that visual information seems to be ignored by the multimodal NMT models. Caglayan et al. (2019) conduct a systematic analysis and show that visual information can be better leveraged under limited textual context. Different from the above-mentioned studies, we first represent the input sentence-image pair as a unified graph, where various semantic relationships between multi-modal semantic units can be effectively captured for multi-modal NMT. Benefiting from the multi-modal graph, we further introduce an extended GNN to conduct graph encoding via multi-modal semantic interactions. No"
2020.acl-main.273,W16-3210,0,0.251902,"Missing"
2020.acl-main.273,I17-1014,0,0.363334,"Missing"
2020.acl-main.273,P18-1150,0,0.107975,"rge-scale pretraining, while we utilize visual grounding to capture explicit cross-modal correspondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardram"
2020.acl-main.273,W18-6441,0,0.530768,"Missing"
2020.acl-main.273,W16-2360,0,0.23828,"ce and image using a unified multi-modal graph, which captures various semantic relationships between multi-modal semantic units (words and visual objects). We then stack multiple graph-based multi-modal fusion layers that iteratively perform semantic interactions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. †"
2020.acl-main.273,P19-1653,0,0.649723,"Missing"
2020.acl-main.273,P16-1162,0,0.0622395,"dal English⇒German (En⇒De) and English⇒French (En⇒Fr) translation tasks. 3028 40.8 Setup Datasets We use the Multi30K dataset (Elliott et al., 2016), where each image is paired with one English description and human translations into German and French. Training, validation and test sets contain 29,000, 1,014 and 1,000 instances respectively. In addition, we evaluate various models on the WMT17 test set and the ambiguous MSCOCO test set, which contain 1,000 and 461 instances respectively. Here, we directly use the preprocessed sentences 2 and segment words into subwords via byte pair encoding (Sennrich et al., 2016) with 10,000 merge operations. Visual Features We first apply the Stanford parser to identify noun phrases from each source sentence, and then employ the visual ground toolkit released by Yang et al. (2019) to detect associated visual objects of the identified noun phrases. For each phrase, we keep the visual object with the highest prediction probability, so as to reduce negative effects of abundant visual objects. In each sentence, the average numbers of objects and words are around 3.5 and 15.0 respectively. 3 Finally, we compute 2,048-dimensional features for these objects with the pre-tra"
2020.acl-main.273,Q19-1002,1,0.876509,"Missing"
2020.acl-main.273,D19-1514,0,0.177044,"r” semantically corresponds to the blue dashed region. The neglect of this important clue may be due to two big challenges: 1) how to construct a unified representation to bridge the semantic gap between two different modalities, and 2) how to achieve semantic interactions based on the unified representation. However, we believe that such semantic correspondences can be exploited to refine multimodal representation learning, since they enable the representations within one modality to incorporate cross-modal information as supplement during multi-modal semantic interactions (Lee et al., 2018; Tan and Bansal, 2019). 3025 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3025–3035 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Multi-modal Graph Image ??? ??? Text Two boys are playing with a toy car ??? ??? ??? ??? ??? ??? ??? Two boys are playing with a ??? ??? toy car Figure 1: The multi-modal graph for an input sentence-image pair. The blue and green solid circles denote textual nodes and visual nodes respectively. An intra-modal edge (dotted line) connects two nodes in the same modality, and an inter-modal edge (solid line) links two no"
2020.acl-main.273,P02-1040,0,0.110641,"corpus is small and the trained model tends to be over-fitting, we first perform a small grid search to obtain a set of hyper-parameters on the En⇒De validation set. Specifically, the word embedding dimension and hidden size are 128 and 256 respectively. The decoder has Ld =4 layers4 and the number of attention heads is 4. The dropout is set to 0.5. Each batch consists of approximately 2,000 source and target tokens. We apply the Adam optimizer with a scheduled learning rate to optimize various models, and we use other same settings as (Vaswani et al., 2017). Finally, we use the metrics BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the quality of translations. Particularly, we run all models three times for each experiment and report the average results. 40.4 BLEU 3.1 39.6 39.2 http://www.statmt.org/wmt18/multimodal-task.html There is no parsing failure for this dataset. If no noun is detected for a sentence, the object representations will be set to zero vectors and the model will degenerate to Transformer. 4 The encoder of the text-based Transformer also has 4 layers. 1 2 3 Le 4 5 Figure 3: Results on the En⇒De validation set regarding the number Le of graph-based mul"
2020.acl-main.273,P18-1030,0,0.0280822,"respondences. (3) We focus on multi-modal NMT rather than visionand-language reasoning in (Tan and Bansal, 2019). Graph Neural Networks Recently, GNNs (Marco Gori and Scarselli, 2005) including gated graph neural network (Li et al., 2016), graph convolutional network (Duvenaud et al., 2015; Kipf and Welling, 2017) and graph attention network (Velickovic et al., 2018) have been shown effective in many tasks such as VQA (Teney et al., 2017; Norcliffe-Brown et al., 2018; Li et al., 2019), text generation (Gildea et al., 2018; Becky et al., 2018; Song et al., 2018b, 2019) and text representation (Zhang et al., 2018; Yin et al., 2019; Song et al., 3032 Source: A boy riding a skateboard on a skateboarding ramp . Reference: Ein junge fährt skateboard auf einer skateboardrampe . Tranformer: Ein junge fährt auf einem skateboard auf einer rampe . Doubly-att(TF): Ein junge fährt mit einem skateboard auf einer rampe . Enc-att(TF): Ein junge fährt ein skateboard auf einer rampe . ObjectAsToken(TF): Ein junge fährt auf einem skateboard auf einer rampe . Our model: Ein junge fährt auf einem skateboard auf einer skateboardrampe . Figure 6: A translation example of different multi-modal NMT models. The baseline mode"
2020.acl-main.273,D18-1400,0,0.102639,"ctions to learn node representations. Finally, these representations provide an attention-based context vector for the decoder. We evaluate our proposed encoder on the Multi30K datasets. Experimental results and in-depth analysis show the superiority of our multi-modal NMT model. 1 Introduction Multi-modal neural machine translation (NMT) (Huang et al., 2016; Calixto et al., 2017) has become an important research direction in machine translation, due to its research significance in multimodal deep learning and wide applications, such as translating multimedia news and web product information (Zhou et al., 2018). It significantly extends the conventional text-based machine translation by taking images as additional inputs. The assumption behind this is that the translation is expected to be more accurate compared to purely text-based ∗ This work is done when Yongjing Yin was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China. † Corresponding author. translation, since the visual context helps to resolve ambiguous multi-sense words (Ive et al., 2019). Apparently, how to fully exploit visual information is one of the core issues in multi-modal NMT, which directly impacts the model p"
2020.acl-main.639,D14-1181,0,\N,Missing
2020.acl-main.639,D17-1230,0,\N,Missing
2020.acl-main.639,D18-1041,1,\N,Missing
2020.acl-main.712,P17-2021,0,0.0210526,"lignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist tra"
2020.acl-main.712,P19-1080,0,0.0229762,"e girl wants the boy to go”, which conveys an opposite meaning to the AMR graph. In particular, this can be very likely if “the girl wants” appears much more frequent than “the boy wants” in the training corpus. This is a very important issue, because of its wide existence across many neural graph-to-text 7987 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may no"
2020.acl-main.712,W13-2322,0,0.0245618,"veness of our approach over a state-of-the-art baseline. Our code is available at http://github.com/ Soistesimmer/AMR-multiview. 1 boy ARG2 ARG1 eat-01 Above the Veil ARG0 followedBy lunch precededBy girl mod (a) beautiful Into Battle Aenir (b) Figure 1: (a) An AMR graph meaning “The boy wants the beautiful girl to eat lunch with him.”, and (b) A knowledge graph carrying the meaning “Above the Veil is an Australian novel and the sequel to Aenir. It was followed by Into the Battle.” Many text generation tasks take graph structures as their inputs, such as Abstract Meaning Representation (AMR) (Banarescu et al., 2013), Knowledge Graph (KG) and database tables. For example, as shown in Figure 1(a), AMR-to-text generation is to generate a sentence that preserves the meaning of an input AMR graph, which is composed by a set of concepts (such as “boy” and “want-01”) and their relations (such as “:ARG0” and “:ARG1”). Similarly, as shown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks main"
2020.acl-main.712,P18-1026,0,0.0217616,"hown in Figure 1(b), KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to prod"
2020.acl-main.712,N19-1223,0,0.0333895,"ormation of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) exte"
2020.acl-main.712,N19-1366,0,0.0480077,"le relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction l"
2020.acl-main.712,W14-3348,0,0.0263306,"One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pourdamghani et al., 2014). We use this da"
2020.acl-main.712,W19-8652,0,0.0315152,"Missing"
2020.acl-main.712,W18-6539,0,0.033398,"Missing"
2020.acl-main.712,S16-1186,0,0.0222153,"to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN"
2020.acl-main.712,W17-3518,0,0.413529,"y” in Figure 1(a)) contains a pair of entities and their relation. As the next step, the alignments between graph nodes and target words are generated to ground this view into the target sentence for reconstruction. Our second view is the linearization of each input graph produced by depth-first graph traversal, and this view is reconstructed token-by-token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effective"
2020.acl-main.712,Q19-1019,0,0.023917,"tion is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucia"
2020.acl-main.712,N19-1235,0,0.0208537,"iew focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (201"
2020.acl-main.712,D19-6310,0,0.299588,"res, only the most frequent 20K are kept, while the rest are mapped into a special UNK feature.1 X i2[1..N ] p(yi |si ; ✓), (5) where ✓ represents all model parameters. R2 | (3) where W Q , W K and W R2 are model parameters, and dh denotes the encoder-state dimension. The encoder adopts L self-attention layers and H L = L (hL 1 . . . h|V |) represents the concatenated top-layer hidden states of the encoder, which will be used in attention-based decoding. 4 Multi-View Autoencoding Losses Figure 2 visualizes the training framework using our multi-view autoencoding losses, where the 1 Zhu et al. (2019) also mentions other (such as CNN-based or self-attention-based) alternatives to calculate ij . While the GPU memory consumption of these alternatives is a few times more than our baseline, ours actually shows a comparable performance. 7989 View 1: triple relations ARG2 ARG1 want-01 ARG1 ARG0 boy ARG2 ARG1 lunch ARG0 mod ARG0 ARG1 The boy wants the beautiful girl to eat lunch with him eat-01 ARG0 girl Encoder Attention Language modeling loss Decoder mod View 2: linearized graph beautiful want :ARG0 boy :ARG1 eat ( :ARG0 (girl :mod beautiful) :ARG1 lunch :ARG2 boy) Figure 2: The training framew"
2020.acl-main.712,P17-1089,0,0.0189482,". . . sN ) denotes the concatenated states for the target sentence (Equation 4), and the loss for reconstructing this view is defined as the negative log-likelihood for the linearized graph: X lauto2 = log p(xi |ti ; ✓), (15) i2[1..M ] (12) where [x] in the subscript represents choosing the x-th item from the corresponding vector. As the final step, the loss for reconstructing this view is defined as the negative log-likelihood of all target arcs E 0 (the grounded triples from E): X lauto1 = log p(yj , l|yi ) (13) (yj ,l,yi )2E 0 4.2 infer the original graph structure. Besides, previous work (Iyer et al., 2017; Konstas et al., 2017) has shown the effectiveness of generating linearized graphs as sequences for graph parsing, which also confirms our observation. Given a linearized graph represented as a sequence of tokens x1 , . . . , xM , where each token xi can be a graph node, a edge label or a inserted bracket, we adopt another standard Transformer decoder (SADecoderg ) to produce the sequence: Loss 2: Reconstructing Linearized Graphs with a Transformer Decoder As a supplement to our first loss for reconstructing the local information of each grounded triple, we introduce the second loss for predi"
2020.acl-main.712,P19-1236,1,0.815011,"or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for traini"
2020.acl-main.712,N19-1238,0,0.067091,"Missing"
2020.acl-main.712,P17-1014,0,0.415658,"and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “t"
2020.acl-main.712,D18-1183,0,0.0602147,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,D18-1264,0,0.0957235,"er than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decod"
2020.acl-main.712,W18-6501,0,0.143887,"oder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (recon"
2020.acl-main.712,W03-3017,0,0.220221,"al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003) to convert tree parsing into the prediction of transition actions, while we study reconstructing graphs, where there is no common parsing algorithm for all graph types. 7988 (Liu et al., 2018b) and sentiment analysis (Rei and Søgaard, 2019). Since input reconstruction is not intuitively related to these tasks, the autoencoding loss only serves as more training signals. Different from these efforts, we leverage autoencoding loss as a means to preserve input knowledge. Besides, we study reconstructing complex graphs, proposing a general multi-view approach for this goal. 3 Base: Structure-Aware"
2020.acl-main.712,P02-1040,0,0.106933,"ighly sensitive to the input order. One major merit for Loss 2 is the generality, as node-to-word alignments may not be easily obtained, especially for multi-lingual tasks. 4.4 Training with Autoencoding Losses The final training signal with both proposed autoencoding losses is formalized as: 7991 lf inal = lbase + ↵lauto1 + lauto2 , (16) where ↵ and are coefficients for our proposed losses. Both coefficient values are selected by a development experiment. 5 Experiments We study the effectiveness of our autoencoding training framework on AMR-to-text generation and KG-to-text generation. BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014) scores are reported for comparison. Following previous work, we use the multi-bleu.perl from Moses2 for BLEU evaluation. 5.1 Data AMR datasets3 We take LDC2015E86 that contains 16,833, 1,368 and 1,371 instances for training, development and testing, respectively. Each instance contains a sentence and an AMR graph. Following previous work, we use a standard AMR simplifier (Konstas et al., 2017) to preprocess our AMR graphs, and take the PTB-based Stanford tokenizer4 to tokenize the sentences. The node-toword alignments are produced by the ISI aligner (Pou"
2020.acl-main.712,D14-1048,0,0.178545,"rds of the entity in order to represent the whole-entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder"
2020.acl-main.712,P17-1194,0,0.169993,"nal Linguistics, pages 7987–7998 c July 5 - 10, 2020. 2020 Association for Computational Linguistics generation models, hindering the usability of these models for real-world applications (Duˇsek et al., 2018, 2019; Balakrishnan et al., 2019). A potential solution for this issue is improving the training signal to enhance preserving of structural information. However, little work has been done to explore this direction so far, probably because designing such signals is non-trivial. As a first step towards this goal, we propose to enrich the training signal with additional autoencoding losses (Rei, 2017). Standard autoencoding for graph-to-sequence tasks requires reconstructing (parsing into) input graphs, while the parsing algorithm for one type of graphs (such as knowledge graphs) may not generalize to other graph types or may not even exist. To make our approach general across different types of graphs, we propose to reconstruct different views of each input graph (rather than the original graph), where each view highlights one aspect of the graph and is easy to produce. Then through multi-task learning, the autoencoding losses of all views are back-propagated to the whole model so that th"
2020.acl-main.712,D19-1314,0,0.0725088,"a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and re"
2020.acl-main.712,P16-1162,0,0.042332,"than LDC2015E86, we may conclude that the problem of dropping input information may not be effectively reduced by simply adding more supervised data, and as a result, our approach can still be effective on a larger dataset. This conclusion can also be confirmed by comparing the gains of our approach on both AMR datasets regarding BLEU score (2.3 vs 2.5). 5.9 Main Results on WebNLG Table 6 shows the comparison of our results with previous results on the WebNLG testset. ADAPT (Gardent et al., 2017) is based on the standard encoder-decoder architecture (Cho et al., 2014) with byte pair encoding (Sennrich et al., 2016), and it was the best system of the challenge. GCNEC (Marcheggiani and Perez-Beltrachini, 2018) is a recent model using a graph convolution network (Kipf and Welling, 2017) for encoding KGs. Our baseline shows a comparable performance with the previous state of the art. Based on this baseline, applying either loss leads to a significant improvement, and their combination brings a gain of more than 2 BLEU points. Although the baseline already achieves a very high BLEU score, yet the gains on this task are still comparable with those on AMR-to-text generation. This observation may imply that the"
2020.acl-main.712,Q19-1002,1,0.855603,"riments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple record"
2020.acl-main.712,P18-1150,1,0.905082,", KG-to-text generation is to produce a sentence representing a KG, which contains worldwide factoid information of entities (such as “Australia” and “Above the Veil”) and their relations (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentence"
2020.acl-main.712,N18-1106,0,0.0225874,"Missing"
2020.acl-main.712,P18-1151,0,0.0378542,"token from the last decoder state. Overall the first view highlights the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target"
2020.acl-main.712,D17-1129,0,0.0198313,"entity information in the sentence. Taking the edge “followedBy” in Figure 1(b) as an example, we first ground it onto the target sentence to connect words “Above” and “Into”. Next, we create edges with label “compound” from “Above” to words “the” and “Veil”, and from “Into” to words “the” and “Battle” to indicate the two associated entity mentions. For many tasks on graph-to-text generation, the node-to-word alignments can be easily generated from off-the-shell toolkits. For example, in AMRto-text generation, there have been several aligners (Pourdamghani et al., 2014; Flanigan et al., 2016; Wang and Xue, 2017; Liu et al., 2018c; Szubert et al., 2018) available for linking AMR nodes to words. For knowledge graphs, the alignments can be produced by simple rule-based matching or an entity-linking system. The resulting structure with labeled arcs connecting word pairs resembles a dependency tree, and thus we employ a deep biaffine model (Dozat and Manning, 2017) to predict this structure from the decoder states. More specifically, the model factorizes the probability for making each arc into two parts: an unlabeled factor and a labeled one. Given the decoder states s1 , . . . , sN , the representation"
2020.acl-main.712,2020.tacl-1.2,0,0.0576841,"orts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, which conveys an opposite meaning to the AM"
2020.acl-main.712,D18-1509,0,0.0228232,"struct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based algorithms (Nivre, 2003"
2020.acl-main.712,D17-1239,0,0.0254359,"2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wiseman et al. (2017) extended the reconstruction loss of Tu et al. (2017) on table-to-text generation, where a table contains multiple records that fit into several fields.We study a more challenging topic on how to reconstruct a complex graph structure rather than a sentence or a table, and we propose two general and effective methods that reconstruct different complementary views of each input graph. Besides, we propose methods to breakdown the whole (graph, sentence) pair into smaller pieces of (edge, word) pairs with alignments, before training our model to reconstruct each edge given the corresponding word."
2020.acl-main.712,P17-1065,0,0.0213542,"ur model to reconstruct each edge given the corresponding word. On the other hand, neither of the previous efforts tried to leverage this valuable information. 2 Autoencoding loss by input reconstruction was mainly adopted on sequence labeling tasks, such as named entity recognition (NER) (Rei, 2017; Liu et al., 2018a; Jia et al., 2019), simile detection Related Work Previous work for neural graph-to-text generation (Konstas et al., 2017; Song et al., 2018; Beck Our work is remotely related to the previous efforts on string-to-tree neural machine translation (NMT) (Aharoni and Goldberg, 2017; Wu et al., 2017; Wang et al., 2018), which aims at generating target sentences with their syntactic trees. One major difference is that their goal is producing grammatical outputs, while ours is preserving input structures. Besides, our multi-view reconstruction framework is a detachable component on top of the decoder states for training, so no extra error propagation (for structure prediction) can be introduced. Conversely, their models generate trees together with target sentences, thus extra efforts (Wu et al., 2017) are introduced to alleviate error propagation. Finally, there exist transition-based alg"
2020.acl-main.712,D18-1112,1,0.768306,"hts the local information of each triple relation, the second view focuses on the global semantic information of the entire graph. Experiments on AMR-to-text generation and WebNLG (Gardent et al., 2017) show that our graph-based multi-view autoencoding loss improves the performance of a state-of-the-art baseline by more than 2 BLEU points without introducing any parameter during decoding. Besides, human studies show that our approach is indeed beneficial for preserving more concepts and relations from input graphs. et al., 2018; Trisedya et al., 2018; Marcheggiani and Perez-Beltrachini, 2018; Xu et al., 2018; Cao and Clark, 2019; Damonte and Cohen, 2019; Hajdik et al., 2019; Koncel-Kedziorski et al., 2019; Hong et al., 2019; Song et al., 2019; Su et al., 2017) mainly studied how to effectively represent input graphs, and all these models are trained only with the standard language modeling loss. As the most similar one to our work, Tu et al. (2017) proposed an encoder-decoder-reconstructor model for machine translation, which is trained not only to translate each source sentence into its target reference, but also to translate the target reference back into the source text (reconstruction). Wisem"
2020.acl-main.712,D19-1548,0,0.667496,"ns (such as “followedBy”). Recent efforts on graph-to-text generation tasks mainly focus on how to effectively represent input graphs, so that an attention mechanism can better transfer input knowledge to the decoder when Corresponding author ARG1 country Introduction ⇤ Austrilia want-01 generating sentences. Taking AMR-to-text generation as an example, different graph neural networks (GNNs) (Beck et al., 2018; Song et al., 2018; Guo et al., 2019; Ribeiro et al., 2019) have been introduced to better represent input AMRs than a sequence-to-sequence model (Konstas et al., 2017), and later work (Zhu et al., 2019; Cai and Lam, 2019; Wang et al., 2020) showed that relationaware Transformers can achieve even better results than GNNs. These advances for encoding have largely pushed the state-of-the-art performance. Existing models are optimized by maximizing the conditional word probabilities of a reference sentence, a common signal for training language models. As a result, these models can learn to produce fluent sentences, but some crucial input concepts and relations may be messed up or even dropped. Taking the AMR in Figure 1(a) as an example, a model may produce “the girl wants the boy to go”, whic"
2020.autosimtrans-1.5,P18-1118,0,0.0638382,"s Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018"
2020.autosimtrans-1.5,D18-1325,0,0.679378,"016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018). Specifically, we first parse the input document to obtain its discourse structure. Then, we introduce a Transformer-based path encoder to embed the discourse structure information of each word. Finally, we combine the discourse structure information with the word embedding before it is fed into the encoder. Experimental results on the Englishto-German dataset show that our model can significantly outperform both Transformer and Transformer+HAN. 1 To address the above problem, we propose to improve document-level NMT with the aid of discourse structure information. First, we represent each in"
2020.autosimtrans-1.5,C18-1203,0,0.0264746,"rd using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments using the same configuration as HAN. Sp"
2020.autosimtrans-1.5,W17-5535,0,0.0190985,"ina 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Miculicich et al., 2018)"
2020.autosimtrans-1.5,W17-4811,0,0.0446932,"ntext information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st W"
2020.autosimtrans-1.5,P02-1040,0,0.112505,"Specifically, both sentence encoder and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) he"
2020.autosimtrans-1.5,Q17-1007,0,0.0376646,"uction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Tra"
2020.autosimtrans-1.5,S16-1057,0,0.020639,"Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structure information. Our encoder is based on a hierarchical attention network (HAN) (Mic"
2020.autosimtrans-1.5,P18-1117,0,0.0696288,"utational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer"
2020.autosimtrans-1.5,W17-4814,0,0.0255062,"r to model context information in a hierarchical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding autho"
2020.autosimtrans-1.5,2006.amta-papers.25,0,0.0432281,"der and decoder are composed of 6 hidden layers, while path encoder is composed of 2 hidden layers. We use three previous sentences as contextual sentences for current sentence. The hidden size and pointwise FFN size are 512 and 2048 respectively. The dropout rates for all hidden states are set to 0.1. The source and target vocabulary sizes are both 30K. At the training phase, we use the Adam optimizer (Kingma and Ba, 2015) and the batch sizes of context-agnostic model and context-aware model are 4096 and 1024, respectively. Finally, we use case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) to measure the translation quality. Document-level Context Modeling Unlike the above modeling, here we mainly on capturing the context information from previous K sentences for the i-th word of the current sentence. (3) where fd is a linear transformation, and CSi = [csi,1 , csi,2 , · · ·, csi,K ] is the sentence-level context of K contextual sentences. Integrating Document-level Context into the Translation Encoder Finally, we integrate the above-mentioned document-level context into the translation encoder via a gating operation: λi = σ(Wh hi + Wcd cdi ) (4) hei = λi hi + (1 − λi )cdi (5) D"
2020.autosimtrans-1.5,D17-1301,0,0.270329,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,P12-1048,1,0.795182,"ijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al."
2020.autosimtrans-1.5,P17-2029,0,0.0985072,"hical manner. Introduction Neural machine translation (NMT) has made great progress in the past decade. In practical applications, the need for NMT systems has expanded from individual sentences to complete documents. Therefore, document-level NMT has gradually drawn much more attention. Contextual information is particularly important for obtaining highquality document translation. To get better contextual information, researchers have proposed many methods (e.g., memory network and hierarchical attention network) for document-level translation (Sim Smith, 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017a; Tu et al., 2017; Wang et al., Our contributions are as follows: (i) We propose a novel and efficient approach to explicitly exploit discourse structure information for documentlevel NMT. Particularly, our approach is applicable for any other context encoder of document-level NMT; (ii) We carry out experiments on English-toGerman translation task and experimental results show that our model outperforms competitive baselines. ∗ This work is done when Junxuan Chen was interning at Xiaomi AI Lab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automati"
2020.autosimtrans-1.5,2011.mtsummit-papers.13,0,0.0299431,"ab, Xiaomi Inc., Beijing, China. † Corresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the docum"
2020.autosimtrans-1.5,P12-1079,0,0.0311157,"orresponding author. 30 Proceedings of the 1st Workshop on Automatic Simultaneous Translation, pages 30–36 c July 10, 2020. 2020 Association for Computational Linguistics Figure 1: The architecture of our proposed encoder 2 Related Work the discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and H"
2020.autosimtrans-1.5,D19-1164,0,0.0122307,"e discourse structure information, which can be used to not only enrich word embedding but also guide the selection of relevant context for the current sentence. In the era of statistical machine translation, document-level machine translation has become one of the research focuses in the community of machine translation. (Xiao et al., 2011; Su et al., 2012; Xiao et al., 2012; Su et al., 2015). Recently, with the rapid development of NMT, documentlevel NMT has also gradually attracted people’s attention (Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Typically, existing studies aim to improve document-level translation quality with the help of document context, which is usually extracted from neighboring sentences of the current sentence. For example,some researchers applied cache-based models to selectively remember the most relevant context information of the document (Voita et al., 2018; Maruf and Haffari, 2018; Kuang et al., 2018), while some researchers employed hierarchical context networks to catch document context information for Transformer (Miculicich et al., 2018; Maruf et al., 2019; Yang et al., 2019). Specifically, Miculicic"
2020.autosimtrans-1.5,N16-1174,0,0.0850258,"ourse structure information of each word using its discourse path from root node to its corresponding leaf node. Each path is a mixed label sequence composed of the discourse relationship and the importance label (e.g., NUCLEUS ELABORATION, SATELLITE BACKGROUND). Please note that all tokens in the same EDU share the same discourse 3.3 HAN-based Context Modeling Following (2018), we apply hierarchical attention network (HAN) as our context encoder. Due to the advantage of accurately capturing different levels of contexts, HAN has been widely used in many tasks, such as document classification (Yang et al., 2016), stance detection (Sun et al., 2018), sentencelevel NMT (Su et al., 2018b). Using this encoder, we mainly focus on two levels of context modeling: Sentence-level Context Modeling For the i-th word of the current sentence, we employ muti-head 32 attention (Vaswani et al., 2017) to summarize the context from the k-th context sentence: csi,k = MultiHead(fs (hi ), Hk ), Training Development Test (2) Settings We use Transformer (Vaswani et al., 2017) as our context-agnostic baseline system and Transformer+HAN (Miculicich et al., 2018) as our context-aware baseline system. We conduct experiments us"
2020.autosimtrans-1.5,D14-1196,0,0.029133,", Jiarui Zhang1 , Chulun Zhou1 , Jianwei Cui2 , Bin Wang2 , Jinsong Su1† 1 Xiamen University, Xiamen, China 2 Xiaomi AI Lab, Xiaomi Inc., Beijing, China {chenjx,zhangjiarui,clzhou}@stu.xmu.edu.cn jssu@xmu.edu.cn {lixiang21,cuijianwei,wangbin11}@xiaomi.com Abstract 2017a; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Yang et al., 2019). Discourse structure, as well as raw contextual sentences, is a major component of the document. And it has been proved to be effective in many other tasks, such as automatic document summarization (Yoshida et al., 2014; Isonuma et al., 2019) and sentiment classification (Schouten and Frasincar, 2016; Nejat et al., 2017). However, to the best of our knowledge, discourse structure has not been explicitly used in document-level NMT. Recently, document-level neural machine translation (NMT) has become a hot topic in the community of machine translation. Despite its success, most of existing studies ignored the discourse structure information of the input document to be translated, which has shown effective in other tasks. In this paper, we propose to improve document-level NMT with the aid of discourse structur"
2020.autosimtrans-1.5,D16-1050,1,0.66979,"Missing"
2020.autosimtrans-1.5,D18-1049,0,0.109063,"Missing"
2021.acl-long.116,N19-1361,0,0.0297941,"Missing"
2021.acl-long.116,N19-1423,0,0.00715179,", neutral, and mixed citation functions, and ample examples for each valence category. Our codebook including definitions and examples of citation functions is shown in Table 1. After the annotation, we randomly split the dataset into 800 instances for training and the remaining 400 for testing. We then used the 800 human-annotated instances to train a citation function labeling model with 10-fold cross validation. The labeling task was treated as a multi-class classification problem. Our labeling model was built upon SciBERT (Beltagy et al., 2019), a pre-trained language model based on BERT (Devlin et al., 2019) but trained on a large corpus of scientific text. We added a multilayer perceptron (MLP) to SciBERT, and finetuned the whole model on our dataset. As for the input, we concatenated each citing sentence with its context in the citing paper, and inserted a special tag [CLS] at the beginning and another special tag [SEP] to separate them. The final hidden state that corresponded to [CLS] was used as the aggregate sequence representation. This state was fed into the MLP, followed by the softmax function for predicting the citation function of the citing sentence. We report details of test results"
2021.acl-long.116,C10-2049,0,0.0410777,"a to build a large-scale dataset. We summarize our contributions as follows: • We propose a BAckground knowledge- and COntent-based framework, named BACO, for citing sentence generation. • We manually annotated a subset of citing sentences with citation functions to train a SciBERTbased model to automatically label the rest data for citing sentence generation. • Based on the results from experiments, we show that BACO outperforms comparative baselines by at least 2.57 points on ROUGE-2. 2 Related Work Several studies on citing sentence generation have used keyword-based summarization methods (Hoang and Kan, 2010; Chen and Zhuge, 2016, 2019). To that end, they built keyword-based trees to extract sentences from cited papers as related work write-ups. These studies have two limitations: First, since related work sections are not simply (chronological) summaries of cited papers, synthesizing prior work in this manner is insufficient. Second, extractive summarization uses verbatim content from cited papers, which implies intellectual property issues (e.g., copyright violations) as well as ethical problems, such as a lack of intellectual engagement with prior work. Alternatively, abstractive summarization"
2021.acl-long.116,D14-1170,0,0.127116,"information overload for researchers by providing examples of concise citing sentences that address information from cited papers in the context of a new research problem and related write up. While this task cannot and is not meant to replace the scholarly tasks of finding, reading, and synthesizing prior work, the proposed computational solution is intended to support especially new researchers in practicing the process of writing effective and focused reflections on prior work given a new context or problem. A number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing paper A, and the sentences before and after the citing sentences in paper A are considered as context. However, previous work has mainly utilized limited information from citing and cited papers to solve this task. We acknowledge that any such solution, including ours, is a simplification of the intricate process of how scholars write citing sentences. Given this motivation, we explore two sets of information to generate citing sentences, namely back"
2021.acl-long.116,W04-1013,0,0.0153288,", which is a one-hot vector: Lfunc N K 1 XX i i =− yfunc (j) log yˆfunc (j), (12) N i=1 j=1 where N refers to the size of training data and K is the number of different citation functions. Finally, all aforementioned losses were combined as the training objective of the whole framework: J (θ) = Lgen + λS Lsal + λF Lfunc , (13) where λS and λF are the hyper-parameters to balance these losses. 5 5.1 Experiments Metrics and Baselines Following previous work, we report ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-L (longest common subsequence) scores to evaluate the generated citing sentences (Lin, 2004). Implementation details are shown in the Appendix, Section A.2. We also report ROUGE F1 score on our dataset. Finally, we compare our model to competitive baselines: • PTGEN (See et al., 2017): is the original pointer-generator network. • EXT-Oracle (Xing et al., 2020): selects the best possible sentence from the abstract of a cited paper that gives the highest ROUGE w.r.t. the ground truth. This method can be seen as an upper bound of extractive methods. • PTGEN-Cross (Xing et al., 2020): enhances the original pointer-generator network with a cross attention mechanism applied to the citing s"
2021.acl-long.116,W04-3252,0,0.135375,"f a cited paper that gives the highest ROUGE w.r.t. the ground truth. This method can be seen as an upper bound of extractive methods. • PTGEN-Cross (Xing et al., 2020): enhances the original pointer-generator network with a cross attention mechanism applied to the citing sentence’s context and the cited paper’s abstract. Additionally, we report results from using several extractive methods that have been used for summarization tasks3 , including: • LexRank (Erkan and Radev, 2004): is an unsupervised graph-based method for computing relative importance of extractive summarization. • TextRank (Mihalcea and Tarau, 2004): is an unsupervised algorithm where sentence importance 3 We apply extractive methods on the cited paper’s abstract to extract one sentence as the citing sentence. scores are computed based on eigenvector centrality within weighted-graphs. 5.2 Experimental Results As the results in Table 2 show, our proposed framework (BACO) outperformed all of the considered baselines. BACO achieved scores of 32.54 (ROUGE-1), 9.71 (ROUGE-2), and 24.90 (ROUGE-L). We also observed that the extractive methods performed comparatively poorly and notably worse than the abstractive methods. All abstractive methods"
2021.acl-long.116,P18-1186,0,0.0121203,"oded hidden states. The attention distribution is calculated as in (Bahdanau et al., 2015). Since we considered both the citing sentence’s context and the cited paper’s abstract on the source side, we applied the attention mechanism to {hcw i } and {haw } separately to obtain two attention vecj abs tors actx t , at , and their corresponding context vecctx tors ct , cabs t at the step t. We then aggregated input context c∗t from the citing sentence’s context, the cited paper’s abstract, and background knowledge by applying a dynamic fusion operation based on modality attention as described in (Moon et al., 2018b,a), which selectively attenuated or amplified each modality based on their importance to the task: abstract, we calculated the generation probability and copy probabilities as follows: [pgen , pcopy1 , pcopy2 ] = softmax(Wctx cctx t net + Wabs cabs t + Wnet ct + Wdec st + Wemb e(wt−1 ) + bptr ), (4) where pgen is the probability of generating words, pcopy1 is the probability of copying words from the citing sentence’s context, pcopy2 is the probability of copying words from the cited paper’s abstract, st represents the hidden state of the decoder at step t, and e(wt−1 ) indicates the input w"
2021.acl-long.116,N18-1078,0,0.0189785,"oded hidden states. The attention distribution is calculated as in (Bahdanau et al., 2015). Since we considered both the citing sentence’s context and the cited paper’s abstract on the source side, we applied the attention mechanism to {hcw i } and {haw } separately to obtain two attention vecj abs tors actx t , at , and their corresponding context vecctx tors ct , cabs t at the step t. We then aggregated input context c∗t from the citing sentence’s context, the cited paper’s abstract, and background knowledge by applying a dynamic fusion operation based on modality attention as described in (Moon et al., 2018b,a), which selectively attenuated or amplified each modality based on their importance to the task: abstract, we calculated the generation probability and copy probabilities as follows: [pgen , pcopy1 , pcopy2 ] = softmax(Wctx cctx t net + Wabs cabs t + Wnet ct + Wdec st + Wemb e(wt−1 ) + bptr ), (4) where pgen is the probability of generating words, pcopy1 is the probability of copying words from the citing sentence’s context, pcopy2 is the probability of copying words from the cited paper’s abstract, st represents the hidden state of the decoder at step t, and e(wt−1 ) indicates the input w"
2021.acl-long.116,D14-1162,0,0.083814,"Missing"
2021.acl-long.116,P17-1099,0,0.670884,"their context for classification (Zhao et al., 2019; Cohan et al., 2019). Our paper involves citation functions into citing sentence generation so that the generated citing sentences can be coherent given their context, and can still contain the motivation for a specific citation. In this paper, we propose a BAckground knowledge- and COntent-based framework, named BACO. Specifically, we encode a citation network based on citation relations among papers to obtain background knowledge, and the given citing and cited papers to provide content information. We extend a standard pointer-generator (See et al., 2017) to copy words from cited and citing papers, and determine what to cite by estimating sentence salience in the cited paper. The various pieces of captured information are then combined as the context for the decoder. Furthermore, we extend our framework to include why to cite by jointly training the generation with citation function classification and facilitate the acquisition of the content information. As for the dataset, we extended the ACL Anthology Network corpus (AAN) (Radev et al., 2013) with extracted citing sentences by using RegEx. We then hand-annotated the citation functions on a"
2021.acl-long.116,W06-1613,0,0.141021,"citing and cited papers as a second set of features to capture two more in-depth content features: (1) What to cite - while the overall content of a cited paper needs to be understood by the authors of the citing paper, not all content is relevant for writing citing sentences. Therefore, we follow the example of estimating salient sentences (Yasunaga et al., 2019) and use the predicted salience to filter crucial information that should be integrated into the resulting citing sentence; (2) Why to cite - we define “citation function” as an approximation of an author’s reason for citing a paper (Teufel et al., 2006). A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019). Our paper involves citation functions into citing sentence generation so that the generated citing sentences can be coherent given their context, and can still contain the motivation for a specific citation. In this paper, we propose a BAckground knowledge- and COntent-based framework, named BACO. Specifically, we encode a citation network based on citation relations among papers to obtain background knowledge, and the given citing and ci"
2021.acl-long.116,D18-1204,0,0.0160085,"ed papers as related work write-ups. These studies have two limitations: First, since related work sections are not simply (chronological) summaries of cited papers, synthesizing prior work in this manner is insufficient. Second, extractive summarization uses verbatim content from cited papers, which implies intellectual property issues (e.g., copyright violations) as well as ethical problems, such as a lack of intellectual engagement with prior work. Alternatively, abstractive summarization approaches, such as methods based on linear programming (Hu and Wan, 2014) and neural seq2seq methods (Wang et al., 2018), have also been explored. These approaches 1467 mainly focus on utilizing papers’ content information, specifically on the text of cited papers directly. A recent paper that went beyond summarizing the content of cited papers (Xing et al., 2020) used a multi-source, pointer-generator network with a cross attention mechanism to calculate the attention distribution between the citing sentences’ context and the cited paper’s abstract. Our paper is based on the premise that citation network analysis can provide background knowledge that facilitates the understanding of papers in a field. Prior an"
2021.acl-long.116,2020.acl-main.550,0,0.496477,"providing examples of concise citing sentences that address information from cited papers in the context of a new research problem and related write up. While this task cannot and is not meant to replace the scholarly tasks of finding, reading, and synthesizing prior work, the proposed computational solution is intended to support especially new researchers in practicing the process of writing effective and focused reflections on prior work given a new context or problem. A number of recent papers have focused on the task of citing sentence generation (Hu and Wan, 2014; Saggion et al., 2020; Xing et al., 2020), which is defined as generating a short text that describes a cited paper B in the context of a citing paper A, and the sentences before and after the citing sentences in paper A are considered as context. However, previous work has mainly utilized limited information from citing and cited papers to solve this task. We acknowledge that any such solution, including ours, is a simplification of the intricate process of how scholars write citing sentences. Given this motivation, we explore two sets of information to generate citing sentences, namely background knowledge in the form of citation n"
2021.acl-long.116,K17-1045,0,0.0171963,"context vector cabs t was updated accordingly. 4.3 Model Training During model training, the objective of our framework covers three parts: generation loss, salience estimation loss, and citation function classification. 4.3.1 Generation Loss The generation loss was based on the prediction of words from the decoder. We minimized the negative log-likelihood of all target words wt∗ and used them as the objective function of generation: X Lgen = − log P (wt∗ ). (9) t 4.3.2 Salience Estimation Loss To include extra supervision into the salience estimation, we adopted a ROUGE-based approximation (Yasunaga et al., 2017) as the target. We assume citing sentences to depend heavily on salient sentences from the cited papers’ abstracts. Based on this premise, we calculated the ROUGE scores between the citing sentence and sentences in the corresponding cited paper’s abstract to obtain an approximation of the salience distribution as the ground-truth. If a sentence shared a high ROUGE score with the citing sentence, this sentence would be considered as a salient sentence because the citing sentence was likely to be generated based on this sentence, while a low ROUGE score implied that this sentence may be ignored"
2021.acl-long.116,D19-1524,0,0.120868,"ntent of a cited paper needs to be understood by the authors of the citing paper, not all content is relevant for writing citing sentences. Therefore, we follow the example of estimating salient sentences (Yasunaga et al., 2019) and use the predicted salience to filter crucial information that should be integrated into the resulting citing sentence; (2) Why to cite - we define “citation function” as an approximation of an author’s reason for citing a paper (Teufel et al., 2006). A number of previous research on citation functions has used citing sentences and their context for classification (Zhao et al., 2019; Cohan et al., 2019). Our paper involves citation functions into citing sentence generation so that the generated citing sentences can be coherent given their context, and can still contain the motivation for a specific citation. In this paper, we propose a BAckground knowledge- and COntent-based framework, named BACO. Specifically, we encode a citation network based on citation relations among papers to obtain background knowledge, and the given citing and cited papers to provide content information. We extend a standard pointer-generator (See et al., 2017) to copy words from cited and citin"
2021.acl-long.310,P18-1094,0,0.0340529,"ained in the real-world scenarios, but also reflect the topic preference, stylistic characteristic, and context of user. Moreover, compared with pre-defined or userspecific labels, historical inputs can be updated with current source sentences, which is also in line with realistic scenario. In this work, we propose a novel framework for this task, where the NMT model is equipped with a cache module to restore and update historical inputs. Besides, in order to further transfer the traits from the seen users to the unseen ones, we design a regularization framework based on contrastive learning (Bose et al., 2018; Yang et al., 2019), which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users. In order to further train and assess the proposed framework, we construct a new User-Driven Machine Translation dataset called UDT-Corpus. This corpus consists of 6,550 users with totally 57,639 Chinese sentences collected from a realworld online MT system. Among them, 17,099 Chinese sentences are annotated with their English translations by linguistic experts according to the user-specific historical inputs. Experimental results demo"
2021.acl-long.310,W11-2107,0,0.0176359,"troduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical inputs with a special prefix to indicate that they are additional information. • TF-FT + UserBias (Michel and Neubig, 2018). It introduces user-specific biases to refine softmax-based predictions of Transformer NMT model. We change it to a zeroshot method similar to (Farajian et al., 2017) 32.6 32.4 32.3 Evaluation We assess the translation quality with two metrics: one is case-insensitive BLEU (mteval-v13a.pl, Papineni et al., 2002)4 and the other is METEOR5 (Denkowski and Lavie, 2011). 5.2 BLEU BLEU both encoder and decoder are set to 6, and the number of attention heads is set to 8. Besides, we use 4 GPUs for training. At the pre-training stage, we employ the Adam optimizer with β2 = 0.998. We use the batch size of 16,384 tokens and pre-train the model for 200,000 steps. Particularly, we adopt the dropout strategy (Srivastava et al., 2014) with rate 0.1 to enhance the robustness of our model. When fine-tuning the model, we keep the other settings consistent with the pre-training stage, but reduce the batch size to 2048 tokens and fine-tune the model with early-stopping st"
2021.acl-long.310,W17-4713,0,0.0187766,"ical inputs with their translations produced by our online translation system, forming additional data for fine-tuning TF-FT. • TF-FT + ConcHist (Tiedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical inputs with a special prefix to indicate that they are additional information. • TF-FT + UserBias (Michel and Neubig, 2018). It introduces user-specific biases to refine softmax-based predictions of Transformer NMT model. We change it to a zeroshot method similar to (Farajian et al., 2017) 32.6 32.4 32.3 Evaluation We assess the translation quality with two metrics: one is case-insensitive BLEU (mteval-v13a.pl, Papineni et al., 2002)4 and the other is METEOR5 (Denkowski and Lavie, 2011). 5.2 BLEU BLEU both encoder and decoder are set to 6, and the number of attention heads is set to 8. Besides, we use 4 GPUs for training. At the pre-training stage, we employ the Adam optimizer with β2 = 0.998. We use the batch size of 16,384 tokens and pre-train the model for 200,000 steps. Particularly, we adopt the dropout strategy (Srivastava et al., 2014) with rate 0.1 to enhance the robust"
2021.acl-long.310,D11-1084,0,0.606476,"ime. Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner. Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expression habits. Moreover, historical inputs of user has relatively fewer dependencies than the contexts 4009 used in document-level translation. Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vis"
2021.acl-long.310,2020.acl-main.740,0,0.0129794,"cenarios. • We collect UDT-Corpus and make it publicly available, which may contribute to the subsequent researches in the communities of NMT and user-driven models. • Extensive analyses indicate the effectiveness of our work and verify that NMT can profit from user behavior to generate diverse translations conforming to user traits. 2 Related Work This section mainly includes the related studies of personalized machine translation, cache-based NMT and contrastive learning for NMT. Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations. For example, Mirkin et al. (2015) show that the translation generated by the SMT model has an adverse effect on the prediction of author personalities, demonstrating the necessity of personalized machine translation. Furthermore, Sennrich et al. (2016a) control the politeness in the translation by adding a politeness label on the source side. Rabinovich et al. (2017) explore a gender-personalized SMT system that retains the original gender traits. These domain labels represent users in single dimension separately, which are insufficien"
2021.acl-long.310,P17-4012,0,0.0534181,"ze the prediction difference between the training instances hX (u) , Y (u) , H (u) i and + hX (u) , Y (u) , H (u ) i, and maximize the difference between the training instances hX (u) , Y (u) , H (u) i − and hX (u) , Y (u) , H (u ) i. In this way, the NMT model can not only exploit pesudo training instances, but also produce more consistent translations with user traits. 5 Experiments In this section, we carry out several groups of experiments to investigate the effectiveness of our proposed framework on UDT-Corpus. 5.1 Setup We develop the user-driven NMT model based on Open-NMT Transformer (Klein et al., 2017), and adopt a two-stage strategy to train this model: we first pre-train a Transformer-based NMT model on the WMT2017 Chinese-to-English dataset, and then fine-tune this model to our user-driven NMT model using UDT-Corpus. Datasets The WMT2017 Chinese-to-English dataset is composed of the News Commentary v12, UN Parallel Corpus v1.0, and CWMT corpora, with totally 25M parallel sentences. To fine-tune our model, we split UDT-Corpus into training, validation and test set, respectively. Table 1 provides more detailed statistics of these datasets. To improve the efficiency of model training, we tr"
2021.acl-long.310,C18-1050,1,0.936826,"them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner. Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expression habits. Moreover, historical inputs of user has relatively fewer dependencies than the contexts 4009 used in document-level translation. Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural langu"
2021.acl-long.310,D15-1166,0,0.0516325,"leads to synonymous yet stylistically different translations. merely translate the original content, as the example shown in Figure 1. However, current NMT models are mainly designed for the semantic transformation between the source and target sentences regardless of subtle traits with respect to user behavior. It can be said that the effect of user behavior on translation modeling is still far from utilization, which, to some extent, limits the applicability of NMT models in real-world scenarios. Introduction In recent years, neural machine translation (NMT) models (Sutskever et al., 2014; Luong et al., 2015; Vaswani et al., 2017) have shown promising quality and thus increasingly attracted users. When drawing on a translation system, every user has his own traits, including topic preference, stylistic characteristics, and expression habits, which can be implicitly embodied in their behavior, e.g., the historical inputs of these users. A good translation should implicitly mirror user traits rather than ∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group. 1 We release our source code and the associated benchmark at https://github."
2021.acl-long.310,P18-2050,0,0.230121,"Group. 1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT. More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable. Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require the users in the training set and the test set to be consistent, therefore can not 4008 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics an"
2021.acl-long.310,D15-1130,0,0.483368,"ed in their behavior, e.g., the historical inputs of these users. A good translation should implicitly mirror user traits rather than ∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group. 1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT. More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable"
2021.acl-long.310,W10-2602,0,0.0265155,"large-scale users in a fine-grained way. The most correlated work to ours is Michel and Neubig (2018) which introduces a speaker-specific bias into the conventional NMT model. However, these methods are unable to deal with users unseen at the training time. Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner. Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expres"
2021.acl-long.310,W17-4811,0,0.0524219,"the WMT2017 corpus. This model yields 24.61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing. • TF-FT. This model is also a Transformerbased NMT model that is further fine-tuned on the parallel sentences of UDT-Corpus. • TF-FT + PesuData. This model is a variant of TF-FT. When constructing it, we pair historical inputs with their translations produced by our online translation system, forming additional data for fine-tuning TF-FT. • TF-FT + ConcHist (Tiedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical inputs with a special prefix to indicate that they are additional information. • TF-FT + UserBias (Michel and Neubig, 2018). It introduces user-specific biases to refine softmax-based predictions of Transformer NMT model. We change it to a zeroshot method similar to (Farajian et al., 2017) 32.6 32.4 32.3 Evaluation We assess the translation quality with two metrics: one is case-insensitive BLEU (mteval-v13a.pl, Papineni et al., 2002)4 and"
2021.acl-long.310,W04-3225,0,0.156365,"gle dimension separately, which are insufficient to distinguish large-scale users in a fine-grained way. The most correlated work to ours is Michel and Neubig (2018) which introduces a speaker-specific bias into the conventional NMT model. However, these methods are unable to deal with users unseen at the training time. Different from them, user-driven NMT can generate personalized translations for these unseen users in a zero-shot manner. Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such"
2021.acl-long.310,P02-1040,0,0.110833,"iedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical inputs with a special prefix to indicate that they are additional information. • TF-FT + UserBias (Michel and Neubig, 2018). It introduces user-specific biases to refine softmax-based predictions of Transformer NMT model. We change it to a zeroshot method similar to (Farajian et al., 2017) 32.6 32.4 32.3 Evaluation We assess the translation quality with two metrics: one is case-insensitive BLEU (mteval-v13a.pl, Papineni et al., 2002)4 and the other is METEOR5 (Denkowski and Lavie, 2011). 5.2 BLEU BLEU both encoder and decoder are set to 6, and the number of attention heads is set to 8. Besides, we use 4 GPUs for training. At the pre-training stage, we employ the Adam optimizer with β2 = 0.998. We use the batch size of 16,384 tokens and pre-train the model for 200,000 steps. Particularly, we adopt the dropout strategy (Srivastava et al., 2014) with rate 0.1 to enhance the robustness of our model. When fine-tuning the model, we keep the other settings consistent with the pre-training stage, but reduce the batch size to 2048"
2021.acl-long.310,E17-1101,0,0.127335,"e historical inputs of these users. A good translation should implicitly mirror user traits rather than ∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group. 1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT. More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable. Previous methods (Mirkin et al.,"
2021.acl-long.310,N16-1005,0,0.270189,"translation should implicitly mirror user traits rather than ∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group. 1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT. More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable. Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require"
2021.acl-long.310,P16-1162,0,0.842325,"translation should implicitly mirror user traits rather than ∗ Jinsong Su is the corresponding author. This work was done when Huan Lin was interning at DAMO Academy, Alibaba Group. 1 We release our source code and the associated benchmark at https://github.com/DeepLearnXMU/ User-Driven-NMT. More recently, several studies have shown that the prominent signals in terms of personal characteristics can be served as inductive biases and reflected in translation results using domain adaptation approaches, such as personality (Mirkin et al., 2015), gender (Rabinovich et al., 2017), and politeness (Sennrich et al., 2016a). However, previously explored signals characterize users from a single dimension, which insufficiently represent fine-grained user traits. Furthermore, Michel and Neubig (2018) pay their attention to personalized TED talk translation, in which they train a speakerspecific bias to revise the prediction distribution. In contrast with these studies, our work investigates a more realistic online scenario: a real-world MT system serves extensive users, where the user-behavior annotated data covering all users is unavailable. Previous methods (Mirkin et al., 2015; Michel and Neubig, 2018) require"
2021.acl-long.310,Q18-1029,0,0.309967,"generate personalized translations for these unseen users in a zero-shot manner. Cache-Based Machine Translation Inspired by the great success of cache on language modeling (Kuhn and de Mori, 1990; Goodman, 2001; Federico et al., 2008), Nepveu et al. (2004) propose a cache-based adaptive SMT system. Tiedemann (2010) explore a cache-based translation model that fills the cache with bilingual phrase pairs extracted from previous sentence pairs in a document. Bertoldi et al. (2013) use a cache mechanism to achieve online learning in phrase-based SMT. Gong et al. (2011), Kuang et al. (2018), and Tu et al. (2018) further exploit cache-based approaches to leverage contextual information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expression habits. Moreover, historical inputs of user has relatively fewer dependencies than the contexts 4009 used in document-level translation. Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to"
2021.acl-long.310,D13-1140,0,0.0378361,"information for document-level machine translation. Contrast with the documentlevel NMT that learns to capture contextual information, our study aims at modeling user traits, such as, topic preference, stylistic characteristics, and expression habits. Moreover, historical inputs of user has relatively fewer dependencies than the contexts 4009 used in document-level translation. Contrastive Learning for NMT Contrastive learning has been extensively applied in the communities of computer vision and natural language processing due to its effectiveness and generality on self-supervised learning (Vaswani et al., 2013; Mnih and Kavukcuoglu, 2013; Liu and Sun, 2015; Bose et al., 2018). Towards raising the ability of NMT in capturing global dependencies, Wiseman and Rush (2016) first introduce contrastive learning into NMT, where the ground-truth translation and the model output are considered as the positive and contrastive samples, respectively. Yang et al. (2019) construct contrastive examples by deleting words from ground-truth translation to reduce word omission errors in NMT. Contrast to these studies, we employ contrastive learning to create broader learning signals for our user-driven NMT model, wher"
2021.acl-long.310,2020.emnlp-main.80,1,0.745414,"25 35 45 (b) Context Cache Size ?? (?? =25) Figure 3: Effects of cache size on translation quality. Model BLEU METEOR w/o user behavior TF 27.52 44.05 TF-FT 28.61 45.35 45.40 TF-FT + PesuData 29.02 w/ user behavior TF-FT + ConcHist 30.85 46.08 TF-FT + UserBias 31.36 46.79 32.35 48.20 UD-NMT Baselines We represent our user-driven NMT model as UDNMT and compare it with the following baselines: • TF. It is a Transformer-based NMT model pretrained on the WMT2017 corpus. This model yields 24.61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing. • TF-FT. This model is also a Transformerbased NMT model that is further fine-tuned on the parallel sentences of UDT-Corpus. • TF-FT + PesuData. This model is a variant of TF-FT. When constructing it, we pair historical inputs with their translations produced by our online translation system, forming additional data for fine-tuning TF-FT. • TF-FT + ConcHist (Tiedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all t"
2021.acl-long.310,D16-1137,0,0.0637888,"Missing"
2021.acl-long.310,P19-1623,0,0.206895,"orld scenarios, but also reflect the topic preference, stylistic characteristic, and context of user. Moreover, compared with pre-defined or userspecific labels, historical inputs can be updated with current source sentences, which is also in line with realistic scenario. In this work, we propose a novel framework for this task, where the NMT model is equipped with a cache module to restore and update historical inputs. Besides, in order to further transfer the traits from the seen users to the unseen ones, we design a regularization framework based on contrastive learning (Bose et al., 2018; Yang et al., 2019), which forces our model to decrease the divergence between translations of similar users while increasing the diversity on dissimilar users. In order to further train and assess the proposed framework, we construct a new User-Driven Machine Translation dataset called UDT-Corpus. This corpus consists of 6,550 users with totally 57,639 Chinese sentences collected from a realworld online MT system. Among them, 17,099 Chinese sentences are annotated with their English translations by linguistic experts according to the user-specific historical inputs. Experimental results demonstrate that the pro"
2021.acl-long.310,2020.coling-main.399,1,0.785128,"T-Corpus and make it publicly available, which may contribute to the subsequent researches in the communities of NMT and user-driven models. • Extensive analyses indicate the effectiveness of our work and verify that NMT can profit from user behavior to generate diverse translations conforming to user traits. 2 Related Work This section mainly includes the related studies of personalized machine translation, cache-based NMT and contrastive learning for NMT. Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations. For example, Mirkin et al. (2015) show that the translation generated by the SMT model has an adverse effect on the prediction of author personalities, demonstrating the necessity of personalized machine translation. Furthermore, Sennrich et al. (2016a) control the politeness in the translation by adding a politeness label on the source side. Rabinovich et al. (2017) explore a gender-personalized SMT system that retains the original gender traits. These domain labels represent users in single dimension separately, which are insufficient to distinguish la"
2021.acl-long.310,D19-1078,1,0.895003,"Missing"
2021.acl-long.310,D18-1041,1,0.872702,"Missing"
2021.acl-long.310,N19-1189,0,0.0195067,"raits in zero-shot scenarios. • We collect UDT-Corpus and make it publicly available, which may contribute to the subsequent researches in the communities of NMT and user-driven models. • Extensive analyses indicate the effectiveness of our work and verify that NMT can profit from user behavior to generate diverse translations conforming to user traits. 2 Related Work This section mainly includes the related studies of personalized machine translation, cache-based NMT and contrastive learning for NMT. Personalized Machine Translation Recently, some researchers have employed domain adaptation (Zhang et al., 2019; Gururangan et al., 2020; Yao et al., 2020) to generate personalized translations. For example, Mirkin et al. (2015) show that the translation generated by the SMT model has an adverse effect on the prediction of author personalities, demonstrating the necessity of personalized machine translation. Furthermore, Sennrich et al. (2016a) control the politeness in the translation by adding a politeness label on the source side. Rabinovich et al. (2017) explore a gender-personalized SMT system that retains the original gender traits. These domain labels represent users in single dimension separate"
2021.acl-long.310,2020.acl-main.620,1,0.763174,"xt Cache Size ?? (?? =25) Figure 3: Effects of cache size on translation quality. Model BLEU METEOR w/o user behavior TF 27.52 44.05 TF-FT 28.61 45.35 45.40 TF-FT + PesuData 29.02 w/ user behavior TF-FT + ConcHist 30.85 46.08 TF-FT + UserBias 31.36 46.79 32.35 48.20 UD-NMT Baselines We represent our user-driven NMT model as UDNMT and compare it with the following baselines: • TF. It is a Transformer-based NMT model pretrained on the WMT2017 corpus. This model yields 24.61 BLEU score on WMT2017 Chinese-to-English translation task, which is comparable with reported results in (Wan et al., 2020; Zhou et al., 2020), which makes our subsequent experiments convincing. • TF-FT. This model is also a Transformerbased NMT model that is further fine-tuned on the parallel sentences of UDT-Corpus. • TF-FT + PesuData. This model is a variant of TF-FT. When constructing it, we pair historical inputs with their translations produced by our online translation system, forming additional data for fine-tuning TF-FT. • TF-FT + ConcHist (Tiedemann and Scherrer, 2017). In this model, we introduce user behavior into TF-FT by concatenating each input sentence with several historical inputs. We mark all tokens in historical"
2021.acl-long.394,P16-1004,0,0.18599,"uage (NL) description, which has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on"
2021.acl-long.394,P18-1068,0,0.0857169,"ments To investigate the effectiveness and generalizability of our model, we carry out experiments on several commonly-used datasets. X λ Lrl (o; θ), |Nmb | Lrl (o; θ) = −Eo∼π [r(o)] (12) 4.2 Baselines To facilitate the descriptions of experimental results, we refer to the enhanced TRANX model as TRANX-RL. In addition to TRANX, we compare our enhanced model with several competitive models: r(o) = (Lmle (ˆ o) − Lmle (o)) ∗ (max(η − p(o), 0)). (11) 5080 • TRANX (w/ pre-train). It is an enhanced version of TRANX with pre-training. We DJANGO Acc. ATIS Acc. GEO Acc. CONALA BLEU / Acc. COARSE2FINE (Dong and Lapata, 2018)† TRANX (Yin and Neubig, 2019)† TREEGEN (Sun et al., 2020) – 77.3 ±0.4 – 87.7 87.6 ±0.1 88.1 ±0.6 88.2 88.8 ±1.0 – – 24.35 ±0.4 / 2.5 ±0.7 – TRANX TRANX (w/ pre-train) TRANX-R2L TRANX-RAND 77.2 ±0.6 77.5 ±0.4 75.9 ±0.8 74.6 ±1.1 87.6 ±0.4 87.8 ±0.7 87.5 ±0.9 86.4 ±1.4 88.8 ±1.0 88.4±1.1 86.4 ±1.0 81.7 ±1.8 24.38 ±0.5 / 2.2 ±0.5 24.57 ±0.5 / 1.4 ±0.3 24.88 ±0.5 / 2.4 ±0.5 19.73 ±1.1 / 1.6 ±0.6 TRANX-RL (w/o pre-train) TRANX-RL 76.3 ±0.7 77.9 ±0.5 87.2 ±0.8 89.1 ±0.5 87.1 ±1.6 89.5 ±1.2 23.38 ±0.8 / 2.1 ±0.2 25.47 ±0.7 / 2.6 ±0.4 Model Table 2: The performance of our model in comparison with var"
2021.acl-long.394,Q19-1042,0,0.0180992,"e above studies that deal with multi-branch nodes in left-to-right order, our model determines the optimal expansion orders of branches for multi-branch nodes. Some researchers have also noticed that the selection of decoding order has an important impact on the performance of neural code generation models. For example, Alvarez-Melis and Jaakkola (2017) introduce a doubly RNN model that combines width and depth recurrences to traverse each node. Dong and Lapata (2018) firstly generate a rough code sketch, and then fill in missing details by considering the input NL description and the sketch. Gu et al. (2019a) present an insertionbased Seq2Seq model that can flexibly generate a sequence in an arbitrary order. In general, these researches still deal with multi-branch AST nodes in a left-to-right manner. Thus, these models are theoretically compatible with our proposed branch selector. (a) The first example. Finally, it should be noted that have been many NLP studies on exploring other decoding methods to improve other NLG tasks (Zhang et al., 2018; Su et al., 2019; Zhang et al., 2019; Welleck et al., 2019; Stern et al., 2019; Gu et al., 2019a,b). However, to the best of our knowledge, our work is"
2021.acl-long.394,2020.emnlp-main.175,0,0.508154,"ltistep expansion order selection and how to determine the optimal expansion order lead to challenges for the model training. To deal with these issues, we introduce reinforcement learning to train the extended Seq2Tree model in an end-to-end way. Concretely, we first pre-train a conventional Seq2Tree model. Then, we employ self-critical training with a reward function that measures loss difference between different branch expansion orders to train the extended Seq2Tree model. 3.2.1 Pre-training It is known that a well-initialized network is very important for applying reinforcement learning (Kang et al., 2020). In this work, we require the model to automatically quantify effects of different branch expansion orders on the quality of the generated action sequences. Therefore, we expect that the model has the basic ability to generate action sequences in random order at the beginning. To do this, instead of using the pre-order traversal based action sequences, we use the randomly-organized action sequences to pre-train the Seq2Tree model. Concretely, for each multi-branch node in an AST, we sample a branch expansion order from a 5079 uniform distribution, and then reorganize the corresponding actions"
2021.acl-long.394,P16-1057,0,0.0266076,"ode, which frequently occurs with ‘gzip’. In the second example, TRANX incorrectly predicts the second child node at the t10 -th timestep, while TRANX-RL firstly predicts it at the timestep t6 . We think this error results from the sequentially generated nodes and the errors in early timesteps would accumulatively harm the predictions of later sibling nodes. By comparison, our model can flexibly generate subtrees with shorter lengths, alleviating error accumulation. 5 Related Work With the prosperity of deep learning, researchers introduce neural networks into code generation. In this aspect, Ling et al. (2016) first explore a Seq2Seq model for code generation. Then, due to the advantage of tree structure, many attempts resort to Seq2Tree models, which represent codes as trees of meaning representations (Dong and Lapata, 2016; Alvarez-Melis and Jaakkola, 2017; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018; Sun et al., 2019, 2020). Typically, Yin and Neubig (2018) propose TRANX, which introduces ASTs as intermediate representations of codes and has become the most influential Seq2Tree model. Then, Sun et al. (2019, 2020) respectively explore CNN and Transformer 5082 ever, is not suitable to han"
2021.acl-long.394,P17-1105,0,0.103927,"has attracted increasing attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches"
2021.acl-long.394,W19-3620,0,0.0633131,"Missing"
2021.acl-long.394,2020.acl-main.538,0,0.155716,"ee (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch nodes are expanded in a left-to-right order. Figure 1 gives an example of the NL-to-Code conversion conducted by"
2021.acl-long.394,P17-1041,0,0.106221,"attention recently due to its potential value in simplifying programming. Instead of modeling the abstract syntax tree (AST) of code snippets directly, most of methods for code generation convert AST into a sequence of tree-construction actions. This allows for using natural language generation (NLG) models, such as the widely-used encoder-decoder Joint work with Pattern Recognition Center, WeChat AI, Tencent Inc, China. * Equal contribution † Corresponding author {wuqq,jssu}@xmu.edu.cn models, and obtains great success (Ling et al., 2016; Dong and Lapata, 2016, 2018; Rabinovich et al., 2017; Yin and Neubig, 2017, 2018, 2019; Hayati et al., 2018; Sun et al., 2019, 2020; Wei et al., 2019; Shin et al., 2019; Xu et al., 2020; Xie et al., 2021). Specifically, an encoder is first used to learn word-level semantic representations of the input NL description. Then, a decoder outputs a sequence of tree-construction actions, with which the corresponding AST is generated through pre-order traversal. Finally, the generated AST is mapped into surface codes via certain deterministic functions. Generally, during the generation of dominant Seq2Tree models based on pre-order traversal, branches of each multi-branch n"
2021.acl-long.394,D18-2002,0,0.166909,"rders for code generation. • Experimental results and in-depth analyses GenToken Reduce AST ? ? ? ? ExceptHandler type name Name Name id id Exception Code: body (empty) e token constructor except Exception as e: Figure 1: An example of code generation using the conventional Seq2Tree model in pre-order traversal. demonstrate the effectiveness and generality of our model on various datasets. 2 Background As shown in Figure 1, the procedure of code generation can be decomposed into three stages. Based on the learned semantic representations of the input NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 P"
2021.acl-long.394,P19-1447,0,0.114297,"NL utterance, the dominant Seq2Tree model (Yin and Neubig, 2018) first outputs a sequence of abstract syntax description language (ASDL) grammar-based actions. These actions can then be used to construct an AST following the preorder traversal. Finally, the generated AST is mapped into surface code via a user-specified function AST to MR(∗). In the following subsections, we first describe the basic ASDL grammars of Seq2Tree models. Then, we introduce the details of TRANX (Yin and Neubig, 2018), which is selected as our basic model due to its extensive applications and competitive performance (Yin and Neubig, 2019; Shin et al., 2019; Xu et al., 2020). 1 1 Please note that our approach is also applicable to other Seq2Tree models. 5077 2.1 ASDL Grammar Formally, an ASDL grammar contains two components: type and constructors. The value of type can be composite or primitive. As shown in the ‘ActionSequence’ and ‘AST z’ parts of Figure 1, a constructor specifies a language component of a particular type using its fields, e.g., ExceptHandler (expr? type, expr? name, stmt∗ body). Each field specifies the type of its child node and contains a cardinality (single, optional ? and sequential ∗) indicating the num"
2021.acl-long.468,2020.acl-main.688,0,0.112962,"and parameters are learned from the downstream corpus. Along this line, Sato et al. (2020) exploit external monolingual data to construct a new embedding layer and achieve improvements in domain adaptation. This series of studies empirically confirm the necessity of the suitable vocabulary for the finetuning stage. However, these methods have to learn the task-specific embeddings separately before each adaptation, which brings in additional computational cost thus limiting their applicability. Besides, they completely discard the pre-trained embeddings, which have been proved to be useful by Aji et al. (2020). Extra encoder or embedding layer may fail to be well optimized with insufficient downstream resources. Accordingly, Rothe et al. (2020) employ a task-specific vocabulary to retrain M-BERT, which is then used to initialize neural machine translation (NMT) model. Considering more robust approaches, Kudo (2018) and Provilkov et al. (2020) randomly sample segmentations for each sentence at the training time. Unlike the above methods, our goal is to build a plug-andplay component, that involves neither retraining the pre-trained model nor learning task-specific embeddings separately. G(motocycle)"
2021.acl-long.468,W05-0909,0,0.108894,"ntence pieces for downstream tasks, respectively. Machine Translation Considering machine translation, we examine our method on the widely used English-to-German (En⇒De) benchmarks: WMT14. We follow Rothe et al. (2020) and Liu et al. (2020c) to deal this task. Question Generation We use the SQuAD v1.1 (Rajpurkar et al., 2016) dataset for question generation. We follow the common setting to preprocess dataset and train our models (Liu et al., 2020a). The answer and the passage are taken as the model input, while the question is the target output. ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics. Results As illustrated in Table 3, the randomly initialized NMT model yields comparable results 6007 ¶ || Single NVIDIA v100 GPU with batch size being 32. https://dumps.wikimedia.org 119 0.2 109 0.18 99 0.16 Inference Speed Inference ECE 89 3.3 3.5 3.7 3.9 Averaged Token Length 30 29.6 BLEU 0.22 Inference ECE Inference Speed (sentence/s) 129 29.2 28.8 w/ M-BERT 28.4 +Ours 28 0.14 0 4.1 150 200 Figure 4: Effects of the training steps of embedding generators on BLEU scores of downstream models. Segmented Token with the reported system with the same archite"
2021.acl-long.468,D18-1461,0,0.0208229,"heir pre-trained counterparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation to maintain the coverage of a large amount of diverse vocabulary. Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths. As one example, M-BERT exploits 100 thousand fine-grained subwords to encode hundreds of languages, while most of downstream NLG tasks, in fact, require only one language and its"
2021.acl-long.468,N19-1213,0,0.0166625,"required tokens. • Extensive experiments show that our strategy is able to efficiently decrease the vocabulary gaps in pretrain-finetune paradigm and significantly boost the performance of NLG models. 2 Related Work Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020). This problem has been attributed to many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers u"
2021.acl-long.468,N19-1423,0,0.19433,"from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution"
2021.acl-long.468,N19-1409,0,0.25271,"distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of"
2021.acl-long.468,2020.findings-emnlp.434,0,0.0317193,"LG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator is able to produce embeddings of any tokens, since these embed"
2021.acl-long.468,2021.acl-long.394,1,0.786399,"Missing"
2021.acl-long.468,W18-2705,0,0.0191607,"us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large"
2021.acl-long.468,W18-6325,0,0.0215876,"frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020). Considering the English sequence “Cenozoic palaeohydrodynamic” shown in Table 1, all the words are frequent in a thesis domain translation task and can be well preserved in its vocabulary. Nevertheless, they are segmented into under-represented tokens by pre-trained models, preventing the finetuning stage from better learning their compositionality for generation. An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020). However, retraining the upstream model from scratch for each task is time-consuming and unavailable for largescale models like M-BERT, GPT, etc. To this end, we propose a simple yet generalized pretrain-finetune strategy, where an embedding transfer stage is inserted between pre-training and finetuning to eliminate their token granularity gaps. Unlike the prior strategy using a fixed vocabulary, our vocabulary is changeable and its items including mismatched ones can be easily initialized by the pre-trained embeddings. Concretely,"
2021.acl-long.468,P18-1007,0,0.128231,"However, these methods have to learn the task-specific embeddings separately before each adaptation, which brings in additional computational cost thus limiting their applicability. Besides, they completely discard the pre-trained embeddings, which have been proved to be useful by Aji et al. (2020). Extra encoder or embedding layer may fail to be well optimized with insufficient downstream resources. Accordingly, Rothe et al. (2020) employ a task-specific vocabulary to retrain M-BERT, which is then used to initialize neural machine translation (NMT) model. Considering more robust approaches, Kudo (2018) and Provilkov et al. (2020) randomly sample segmentations for each sentence at the training time. Unlike the above methods, our goal is to build a plug-andplay component, that involves neither retraining the pre-trained model nor learning task-specific embeddings separately. G(motocycle) waiter, ##er, writer, Vocabulary G(motocycle) waiter, ##er, writer, … worker, motocycle waiter, ##er, writer, worker, motocycle … worker, motocycle … Initialize Train Figure 1: Illustration of our pretrain-finetune pipeline. We pretrain an embedding generator for the initialization of embeddings of unseen tok"
2021.acl-long.468,D18-2012,0,0.0347329,"Missing"
2021.acl-long.468,2020.acl-main.703,0,0.180876,"o many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. ("
2021.acl-long.468,N03-1020,0,0.281089,"Missing"
2021.acl-long.468,2020.tacl-1.47,0,0.250501,"cient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a"
2021.acl-long.468,2020.acl-main.170,0,0.483897,"terparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation to maintain the coverage of a large amount of diverse vocabulary. Consequently, downstream NLG models may suffer from more serious exposure bias (Bengio et al., 2015) and expensive computational cost caused by the increased sequence lengths. As one example, M-BERT exploits 100 thousand fine-grained subwords to encode hundreds of languages, while most of downstream NLG tasks, in fact, require only one language and its associate tokens. Second"
2021.acl-long.468,D16-1264,0,0.0102312,"and decoder. Same as configurations in domain adaptation, we merely perform the embedding transferring in decoder. Since the two language models exploit different segmentation tools, i.e., WordPiece (Wu et al., 2016) and SentencePiece (Kudo, 2018), we set 32K and 10K as the number of word and sentence pieces for downstream tasks, respectively. Machine Translation Considering machine translation, we examine our method on the widely used English-to-German (En⇒De) benchmarks: WMT14. We follow Rothe et al. (2020) and Liu et al. (2020c) to deal this task. Question Generation We use the SQuAD v1.1 (Rajpurkar et al., 2016) dataset for question generation. We follow the common setting to preprocess dataset and train our models (Liu et al., 2020a). The answer and the passage are taken as the model input, while the question is the target output. ROUGE-L (Lin and Hovy, 2003), BLEU, and METEOR (Banerjee and Lavie, 2005) are treated as the assessment metrics. Results As illustrated in Table 3, the randomly initialized NMT model yields comparable results 6007 ¶ || Single NVIDIA v100 GPU with batch size being 32. https://dumps.wikimedia.org 119 0.2 109 0.18 99 0.16 Inference Speed Inference ECE 89 3.3 3.5 3.7 3.9 Avera"
2021.acl-long.468,2020.tacl-1.18,0,0.425062,"ream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first answer from the subword discrepancy aspect, namely, the subword vocabulary extracted according to the pretraining data distribution is insufficient to cope with the downstream NLG tasks. Such inflexibility stems from the fact that downstream NLG models have to inherit the vocabulary from their pre-trained counterparts. In order to deal with the op"
2021.acl-long.468,N19-1353,0,0.0819414,"Extensive experiments show that our strategy is able to efficiently decrease the vocabulary gaps in pretrain-finetune paradigm and significantly boost the performance of NLG models. 2 Related Work Recent studies observe that pre-trained models suffer a bottleneck when they are applied to NLG tasks (Edunov et al., 2019; Zhu et al., 2020; Rothe et al., 2020). This problem has been attributed to many reasons. For example, Yang et al. (2019b) point out pretrain-finetune discrepancy caused by the absent masked frames in real data when adopting pretrained masked language models. Chronopoulou et al. (2019) investigate catastrophic forgetting in finetuning stage. It can be said that how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers u"
2021.acl-long.468,2020.findings-emnlp.381,0,0.214001,"nd extensive analyses show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu"
2021.acl-long.468,P16-1162,0,0.0502572,"es: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first answer from the subword discrepancy aspect, namely, the subword vocabulary extracted according to the pretraining data distribution is insufficient to cope with the downstream NLG tasks. Such inflexibility stems from the fact that downstream NLG models have to inherit the vocabulary from their pre-trained counterparts. In order to deal with the open-vocabulary problem, it is de-facto standard for pre-trained models to employ heuristic subword segmentation methods (Sennrich et al., 2016; Kudo 6001 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6001–6011 August 1–6, 2021. ©2021 Association for Computational Linguistics and Richardson, 2018). However, the segmentation learns on the upstream corpus other than the finetuned data and is likely to be sub-optimal (Cherry et al., 2018; Provilkov et al., 2020). We argue that these lead to subword discrepancy and bring two defects. Firstly, the pre-trained model usually learns a fine-grained subword segmentation t"
2021.acl-long.468,tian-etal-2014-um,0,0.0197633,"LG tasks: machine translation (MT) and answer-aware question generation (QG). 4.1 Domain Adaptation We conduct experiments on English-to-Chinese (En⇒Zh) domain adaptation translation tasks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up st"
2021.acl-long.468,I17-2050,0,0.153585,"are in upstream task but frequent in downstream task may be segmented end up poorly understood (Provilkov et al., 2020). Considering the English sequence “Cenozoic palaeohydrodynamic” shown in Table 1, all the words are frequent in a thesis domain translation task and can be well preserved in its vocabulary. Nevertheless, they are segmented into under-represented tokens by pre-trained models, preventing the finetuning stage from better learning their compositionality for generation. An alternative solution is reconstructing the pre-trained model by exploiting either a taskspecific vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018) or a subword regularization approach (Provilkov et al., 2020). However, retraining the upstream model from scratch for each task is time-consuming and unavailable for largescale models like M-BERT, GPT, etc. To this end, we propose a simple yet generalized pretrain-finetune strategy, where an embedding transfer stage is inserted between pre-training and finetuning to eliminate their token granularity gaps. Unlike the prior strategy using a fixed vocabulary, our vocabulary is changeable and its items including mismatched ones can be easily initialized by the pre-trained"
2021.acl-long.468,2020.emnlp-main.80,1,0.724909,"free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, whic"
2021.acl-long.468,P02-1040,0,0.109443,"ks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up strategy where the linear warm-up phase takes 4K steps, reaching its maximum learning rate to 5 × 10−4 . The training of each model is early-stopped to maximize BLEU score on the development s"
2021.acl-long.468,2020.acl-main.278,0,0.0120737,"stigate three problems: Q1: How subword granularity affects NLG models? (§ 5.1) Q2: How embedding transfer benefits to downstream models? (§ 5.2) Q3: Dose our strategy acquire large computational costs? (§ 5.3) Q4: Can our strategy exactly handle under-represented tokens? (§ 5.4) 5.1 100 Training Steps Figure 3: Effects of different token granularities on En⇒De task. As seen, the segmentation granularity remarkably affects inference speed and inference ECE. 5 50 Impact of Subword Granularity Figure 3 visualizes the inference speed and exposure bias (Inference Expected Calibration Error (ECE), Wang et al., 2020) of translation models with different token granularities in their vocabulary. Obviously, for a translation model, neither too small nor too large granularity regarding to subwords can reach a satisfactory performance on inference speed. At the same time, the granularity indeed affects the problem of exposure bias in translation task. The experiments confirm the suitable segmentation strategy can effectively alleviate the problem of exposure bias. Source Reference Translations M-BERT: s dan k bar Ours: dankbar it’s very gratifying to have this kind of reception here. ich bin sehr dankbar f¨ur"
2021.acl-long.468,D17-1010,0,0.022109,"hat how to successfully employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator"
2021.acl-long.468,N19-1242,0,0.0163376,"zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we"
2021.acl-long.468,P19-1226,0,0.116453,"dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at DAMO Academy, Alibaba Group. model using a large-scale corpus, which is then finetuned to various downstream tasks via a few adjustments. Due to its simplicity yet impressive performance, pretrain-finetune paradigm becomes the undoubtedly dominant solution for building state-of-the-art models in many natural language understanding tasks (Xu et al., 2019; Yang et al., 2019a; Liu et al., 2020b). In comparison, this strategy often achieves disappointing or barely satisfactory performance in natural language generation (NLG) tasks. For example, several studies observe that M-BERT (Devlin et al., 2019) fails to enhance the decoder of a translation model (Edunov et al., 2019; Zhu et al., 2020), while Rothe et al. (2020) reach the same conclusion even when adapting an autoregressive model GPT (Radford et al., 2019). A natural problem arises: What is the crucial bottleneck in current pretrain-finetune framework and how to break it? In this paper, we provide the first"
2021.acl-long.468,2020.coling-main.399,1,0.737658,"es show that the proposed strategy offers us opportunities to feel free to transfer the vocabulary, leading to more efficient and better performed downstream NLG models. 1 1 Table 1: Segmentation of English sequence “Cenozoic palaeohydrodynamic” learned from different data distribution as described in § 4. High frequent words in thesis domain are split into fine-grained and underrepresented tokens in pre-trained models. Introduction Pretrain-finetune paradigm has been highly successful on tackling challenging problems in natural language processing, e.g., domain adaptation (Sato et al., 2020; Yao et al., 2020), incremental learning (Khayrallah et al., 2018; Wan et al., 2020), as well as knowledge transferring (Liu et al., 2020b). The rise of large-scale pre-trained language models further attracts increasing attention towards this strategy (Devlin et al., 2019; Edunov et al., 2019). Typically, these methods first pretrain a universal 1 Ce no zo ic pala eo hy dro dyn ami c Cen ozo ic pal a e o hydro dynamic Cenozoic palaeohydrodynamic We release the code at https://github.com/ DeepLearnXMU/embedding-transfer * Jinsong Su is the corresponding author. This work was done when Xin Liu was interning at D"
2021.acl-long.468,D18-1041,1,0.854743,"question generation (QG). 4.1 Domain Adaptation We conduct experiments on English-to-Chinese (En⇒Zh) domain adaptation translation tasks, where the pretrain-finetune paradigm resort as standard. The pre-training corpus is extracted from an 6005 out-of-domain dataset LDC† , in which 1.25M (M = million), 3K (K = thousand), 3K sentences pairs are randomly sampled as training, development and test set, respectively. We verify the effectiveness of our strategy on two downstream domains: Thesis and Laws, of which data are collected from UM-Corpus (Tian et al., 2014). We follow the same settings as Zeng et al. (2018) and Su et al. (2021) to preprocess two corpus and train models. The translation quality is evaluated by cased BLEU (Papineni et al., 2002), which is caculated by mteval-v13a.pl. Implementation Details All the compared methods are re-implemented on top of FairSeq‡ and built on Transformer (Vaswani et al., 2017). We apply Adam Optimizer (Kingma and Ba, 2015) with β1 and β2 being 0.9 and 0.999, respectively. The dropout ratio is set to 0.3 and each iteration batch consists of 25K tokens. For both pre-training and finetuning, we employ warm-up strategy where the linear warm-up phase takes 4K step"
2021.acl-long.468,D18-1059,0,0.0935781,"ly employ pretrain-finetune to enhance NLG models remains a great challenge. We explore this problem from another direction, i.e., the unsuitable subword segmentation for downstream tasks. Task-Specific Vocabulary A natural manner to address this issue is to adopt a task-specific vocabulary. Lewis et al. (2020) first replace the embedding 6002 Embedding Generator Our work is also related to studies with respect to generating embeddings for out-of-vocabulary (OOV) words. In this context, researchers use embeddings of characters or subwords to predict those of unseen words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019; Fukuda et al., 2020). For example, Zhao et al. (2018) train an embedding generator through reconstructing the original representation of each word from its bag of subwords. Sasaki et al. (2019) progressively improve the generator using attention mechanism. Fukuda et al. (2020) further leverage similar words to enhance this procedure. Our work significantly differs from the above studies in two aspects. Due to the vocabulary is fixed once predefined, the embedding reconstruction can be merely drawn on a few of selected words. By contrast, our generator is able to produce"
2021.emnlp-main.186,D16-1091,0,0.159107,"meng@tencent.com, jssu@xmu.edu.cn Abstract and extractive summarization (Barzilay et al., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings b"
2021.emnlp-main.186,N16-1147,0,0.0983536,"Missing"
2021.emnlp-main.186,D18-1465,0,0.174015,"ise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-based in the lightweig"
2021.emnlp-main.186,2020.emnlp-main.511,0,0.200556,"eepLearnXMU/IRSEG. ordered sentences. Overall, these two kinds of models have their 1 Introduction own strengths, which are complementary to each With the rapid development and increasing applica- other. To combine their advantages, Yin et al. tions of natural language processing (NLP), model- (2020) propose FHDecoder that is equipped with ing text coherence has become a significant task, s- three pairwise ordering prediction modules to enince it can provide beneficial information for under- hance the pointer network decoder. Along this line, standing, evaluating and generating multi-sentence Cui et al. (2020) introduce BERT to exploit the texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering"
2021.emnlp-main.186,N19-1423,0,0.0302209,"Missing"
2021.emnlp-main.186,P11-2022,0,0.0285887,"ns ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use"
2021.emnlp-main.186,P19-1102,0,0.0244365,".9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et al., 2021) 46.71 59.47 IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder 47.48 49.84 51.01 60.01 61.81 62.87 Table 5: Coherence probabilities of summaries reordered by different models using weights of 0.8 (left) and 0.5 (right). spect the validity of our proposed framework via multi-document summarization. Concretely, we train different neural sentence ordering models on a large-scale summarization corpus (Fabbri et al., 2019), and then individually use them to reorder the small-scale summarization data of DUC2004 (Task2). Finally, we use coherence probability proposed by (Nayeem and Chali, 2017) to evaluate the coherence of summaries. In this group of experiments, we conduct experiments using different weights: 0.5 and 0.8, as implemented in (Nayeem and Chali, 2017) and (Yin et al., 2020) respectively. The results are reported in Table 5. We can observe that the summaries reordered by IRSE-GRN and its variants achieve higher coherence probabilities than baseline, verifying the effectiveness of our proposed framewo"
2021.emnlp-main.186,D19-1633,0,0.0190762,"we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly different from them, we borrow the idea from the mask-predict framework (Gu et al., 2018; Ghazvininejad et al., 2019; Deng et al., 2020) to progressively incorporate pairwise ordering information into SE-Graph, which is the basis of our graph-based sentence ordering model. To the best of our knowledge, our work is the first attempt to explore iteratively refined GNN for sentence ordering. 3 Background resented as an undirected sentence-entity graph G = (V , E), where V ={vi }Ii=1 ∪{ˆ vj }Jj=1 and E ={ei,i0 }I,I ei,j }I,J ej,j 0 }J,J i=1,j=1 ∪{ˆ i=1,i0 =1 ∪{¯ j=1,j 0 =1 represent the nodes and edges respectively. Here, nodes include sentence nodes (such as vi ) and entity nodes (such as vˆj ), and each edge"
2021.emnlp-main.186,N12-1093,0,0.0223129,"texts. As an important subtask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence orderi"
2021.emnlp-main.186,P03-1069,0,0.2014,"ltzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzilay and Lee, 2004; Barzilay and Lapata, 2005, 2008; Elsner and Charniak, 2011; GuinIn this paper, we propose a novel iterative pairaudeau and Strube, 2013). Recently, neural netwise ordering prediction framework which introwork based sentence ordering models have become duces two classifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise order"
2021.emnlp-main.186,2020.acl-main.703,0,0.0185589,"eness of et al., 2019, 2020), Yin et al. (2019, 2021) repreour framework, we conduct extensive experiments sented input sentences with a unified SE-Graph and on several commonly-used datasets. Experimental then applied GRN to learn sentence representationresults and in-depth analyses show that our model s. Very recently, we notice that Chowdhury et al. enhanced with some proposed technologies (De- (2021) proposes a BART-based sentence ordering vlin et al., 2019; Yin et al., 2020) achieves the model. Please note that our porposed framework state-of-the-art performance. is compatible with BART (Lewis et al., 2020). For 2408 Figure 1: The architecture of SE-GRN model (Yin et al., 2019, 2021). example, we can easily adapt the BART encoder as our sentence encoder. With similar motivation with ours, that is, to combine advantages of above-mentioned two kinds of models, Yin et al. (2020) introduced three pairwise ordering predicting modules (FHDecoder) to enhance the pointer network decoder of ATTOrderNet. Recently, Cui et al. (2020) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences. However, significantly d"
2021.emnlp-main.186,D17-1019,0,0.0162027,"assifiers to make better use of pairwise dominant , consisting of the following two kinds of orderings for graph-based sentence ordering (Yin models: et al., 2019, 2021). As an extension of Sentence1) Pairwise models. Generally, they first preEnity Graph Recurrent Network (SE-GRN) (Yin dict the pairwise orderings between sentences and et al., 2019, 2021), our framework enriches the then use them to produce the final sentence order graph representation with iteratively predicted orvia ranking algorithms (Chen et al., 2016; Agrawal derings between pairwise sentences, which further et al., 2016; Li and Jurafsky, 2017; Kumar et al., benefits the subsequent generation of ordered sen2020; Prabhumoye et al., 2020; Zhu et al., 2021). tences. The basic intuitions behind our work are For example, Chen et al. (2016) first framed sentwo-fold. First, learning contextual sentence reptence ordering as a ranking task conditioned on resentations is helpful to predict pairwise orderpairwise scores. Agrawal et al. (2016) conducted ings. Second, difficulties of predicting ordering the same experiments as (Chen et al., 2016) in the vary with respect to different sentence pairs. Thus, task of image caption storytelling. Sim"
2021.emnlp-main.186,P18-1052,0,0.0865212,"e classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relative orderings between pairwise senconfident pairwise orderings, another classifier tences, which are then leveraged to produce the is used to predict the remaining uncertain pairfinal ordered sentence sequence. Its advantage lies wise orderings. At last, we adapt a GRN-base"
2021.emnlp-main.186,D19-1231,0,0.0114904,"., 2002; Nallapati et al., 2012). Dominant sentence ordering models can be Recently, inspired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al."
2021.emnlp-main.186,N16-1098,0,0.0204435,"adapt Equation 1 to incorporate ss-edge weights into the message aggregation of sentence-level nodes: X m(l) i = (l-1) wi,i0 · w(κ(l-1) , κ(l-1) i i0 )κi0 , (6) vi0 ∈Ni w(κ(l-1) , κ(l-1) i i0 ) = σ(Wg [κ(l-1) ; κ(l-1) i i0 ]). Here σ denotes sigmoid function and Wg is learnable parameter matrix. Equation 6 expresses that the sentence-level aggregation should consider not only the semantic representations of the two involved sentences, but also the relative ordering between them. In addition, other Equations are the same as those of conventional GRN, which have been described in Section §3.2. (Mostafazadeh et al., 2016) is about commonsense stories. Both two datasets are composed of 5-sentence stories and randomly split by 8:1:1 for the training/validation/test sets. • NIPS Abstract, AAN Abstract, arXiv Abstract. These three datasets consist of abstracts from research papers, which are collected from NIPS, ACL anthology and arXiv, respectively (Radev et al., 2016; Chen et al., 2016). The partitions for training/validation/test of each dataset are as follows: NIPS Abstract: 2,427/408/377, AAN Abstract: 8,569/962/2,626, arXiv Abstract: 884,912/110,614/110,615 for the training/validation/test sets. Settings. Fo"
2021.emnlp-main.186,2020.emnlp-main.181,0,0.0576172,"Missing"
2021.emnlp-main.186,P13-1010,0,0.0178174,"Missing"
2021.emnlp-main.186,W17-2407,0,0.167596,"erative predictions of pairwise ordering indeed benefit the learning of graph representations. Finally, the result in the last line indicates that removing noisy weights leads to a significant performance drop. It suggests that the utilization of noisy weights is useful for the training of iterative classifier, which makes our model more robust. Ablation Study 5.6 Summary Coherence Evaluation We conduct several experiments to investigate the impacts of our proposed components on ROCstory Following previous studies (Barzilay and Lapata, dataset and arXiv dataset which are the two largest 2005; Nayeem and Chali, 2017), we further in2414 Dataset SE-GRN IRSE-GRN IRSE-GRN+FHDecoder IRSE-GRN+BERT+FHDecoder Runtime #Params Runtime #Params Runtime #Params Runtime #Params NIPS abstract 6s 23.9M 6.2s 24.0M 18s 25.0M 29s 128.0M AAN abstract 31s 23.9M 32.5s 24.0M 1min8s 25.0M 1min20s 128.0M SIND 1min6s 23.9M 1min9s 24.0M 2min3s 25.0M 2min16s 128.0M ROCStory 2min 23.9M 2min5s 24.0M 4min2s 25.0M 4min42s 128.0M arXiv abstract 25min 23.9M 27min57s 24.0M 46min 25.0M 56min 128.0M Table 6: The runtime on the validation sets and the numbers of parameters for our enhanced models and baseline. Model Coherence SE-GRN (Yin et a"
2021.emnlp-main.186,P18-1152,0,0.0172356,"btask, sentence order- deep semantic connection and relative orderings ing aims at recovering unordered sentences back between sentences and achieve SOTA performance to naturally coherent paragraphs. It is required to when equipped with FHDecoder. However, there deal with logic and syntactic consistency, and has still exist two drawbacks: 1) their pairwise ordering increasingly attracted attention due to its wide ap- predictions only depend on involved sentence pairs, plications on several tasks such as text generation without considering other sentences in the same (Konstas and Lapata, 2012; Holtzman et al., 2018) set; 2) their one-pass pairwise ordering predictions ∗ Corresponding author are relatively rough, ignoring distinct difficulties in 2407 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2407–2417 c November 7–11, 2021. 2021 Association for Computational Linguistics predicting different sentence pairs. Therefore, we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited. 2 Related Work Early studies mainly focused on exploring humandesigned features for sentence ordering (Lapata, 2003; Barzil"
2021.emnlp-main.186,P17-1121,0,0.0409366,"Missing"
2021.emnlp-main.186,D19-1232,0,0.0324438,"Missing"
2021.emnlp-main.186,2020.acl-main.248,0,0.221993,"spired by the great success of deep classified into pairwise ordering models and learning in other NLP tasks, researchers have reset-to-sequence models. However, there is litsorted to neural sentence ordering models, which tle attempt to combine these two types of modcan be classified into: pairwise ordering models els, which inituitively possess complementary advantages. In this paper, we propose a nov(Chen et al., 2016; Agrawal et al., 2016; Li and el sentence ordering framework which introJurafsky, 2017; Moon et al., 2019; Kumar et al., duces two classifiers to make better use of pair2020; Prabhumoye et al., 2020; Zhu et al., 2021) wise orderings for graph-based sentence orderand set-to-sequence models (Gong et al., 2016; Ning (Yin et al., 2019, 2021). Specially, givguyen and Joty, 2017; Logeswaran et al., 2018; en an initial sentence-entity graph, we first Mohiuddin et al., 2018; Cui et al., 2018; Yin et al., introduce a graph-based classifier to predict 2019; Oh et al., 2019; Yin et al., 2020; Cui et al., pairwise orderings between linked sentences. Then, in an iterative manner, based on the 2020; Yin et al., 2021). Generally, the former pregraph updated by previously predicted highdicts the relativ"
2021.emnlp-main.186,Q19-1002,1,0.882575,"Missing"
2021.emnlp-main.186,2020.acl-main.712,1,0.704554,"Missing"
2021.emnlp-main.186,P18-1030,0,0.0181408,"y. During the process of updating hidden states, the messages for each node are aggregated from its adjacent nodes. Specifically, the sentence-level message m(l) i and (l) entity-level message m ˜ i for a sentence si are defined as follows: m(l) i = X (l-1) w(κ(l-1) , κ(l-1) i i0 )κi0 , vi0 ∈Ni m ˆ (l) i = X (l-1) w(κ ¯ (l-1) , (l-1) i j , r ij )j , (1) ˆi vj ∈N In this section, we give a brief introduction to the SE-GRN (Yin et al., 2019, 2021), which is selected as our baseline due to its competitive performance. As shown in Figure 1, SE-GRN is composed of a Bi-LSTM sentence encoder, GRN (Zhang et al., 2018) paragraph encoder, and a pointer network (Vinyals et al., 2015b) decoder. 3.1 Sentence-Entity Graph where κ(l-1) and (l-1) stand for the neighboring senj i0 tence and entity representations of the i-th sentence ˆi denote node vi at the (l − 1)-th layer, Ni and N the sets of neighboring sentences and entities of vi , and both w(∗) and w(∗) ¯ are gating functions with single-layer networks, involving associated node states and edge label rij (if any). Afterwards, κ(l-1) is updated by concatenating i its original representation κ(0) i , the messages from (l) (l) neighbours (mi and m ˆ i ) and t"
2021.emnlp-main.6,D19-1459,0,0.0282992,"for human evaluation: (1) Coherence measures whether the translation is semantically coherent with the dialogue history; (2) Speaker measures whether the translation preserves the personality of the speaker; (3) Fluency measures whether the translation is fluent and gramDialogue Coherence Following (Lapata and Barzilay, 2005; Xiong et al., 2019), we measure dialogue coherence as sentence similarity, which is determined by the cosine similarity between two sentences s1 and s2 : 10 https://code.google.com/archive/p/word2vec/ Due to no available German dialogue datasets, we choose Taskmaster-1 (Byrne et al., 2019), where the English side of BConTrasT (Farajian et al., 2020) also comes from it. 11 sim(s1 , s2 ) = cos(f (s1 ), f (s2 )), 74 matically correct. First, we randomly sample 200 conversations from the test set of BMELD in Zh⇒En direction. Then, we use the 6 models in Tab. 6 to generate the translated utterances of these sampled conversations. Finally, we assign the translated utterances and their corresponding dialogue history utterances in target language to three postgraduate human annotators, and ask them to make evaluations from the above three criteria. The results in Tab. 6 show that our m"
2021.emnlp-main.6,2021.acl-long.444,1,0.676997,"onality, (4) we design the speaker identification task that judges whether the translated text is consistent with the personality of its original speaker. Together with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Ex"
2021.emnlp-main.6,D11-1084,0,0.0312702,"Missing"
2021.emnlp-main.6,2021.acl-long.310,1,0.78471,"Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by the speaker sx. A negative sample p(` ="
2021.emnlp-main.6,2020.acl-main.321,0,0.209009,"et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be applied to help participants communi"
2021.emnlp-main.6,2020.emnlp-main.742,0,0.0181876,"nslations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder hidden state at the t-th decoding step, Wcrg and bcrg are trainable parameters. No"
2021.emnlp-main.6,P18-1118,0,0.0191437,"In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that spe"
2021.emnlp-main.6,W18-6311,0,0.0725571,"“CSA-NCT” represents our proposed approach. Existing Context-Aware NMT Systems. Results on En⇔De. Under the Base setting, our model substantially outperforms the sentencelevel/context-aware baselines by a large margin (e.g., the previous best “Gate-Transformer+FT”), 1.02↑ on En⇒De and 1.12↑ on De⇒En. In term of TER, CSA-NCT also performs better on the two directions, 0.9↓ and 0.7↓ lower than “GateTransformer+FT” (the lower the better), respectively. Under the Big setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to"
2021.emnlp-main.6,2020.emnlp-main.175,0,0.0345857,"Missing"
2021.emnlp-main.6,N19-1313,0,0.0122593,"t al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a chat translator can be"
2021.emnlp-main.6,D18-1325,0,0.0154581,"al Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking"
2021.emnlp-main.6,W04-3250,0,0.035116,"raining objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experime"
2021.emnlp-main.6,C18-1050,0,0.017875,"the NCT model can be enhanced to generate more coherent and speaker-relevant translations through multi-task learning. Cross-lingual Response Generation (CRG). The CRG task is similar to the MRG as shown in Fig. 3(b), where the NCT model is trained to generate the corresponding utterance Yu in target language which is coherent to the given dialogue history context CXu in source language. We first use the encoder of the NCT model to encode CXu , and then use the NCT decoder to predict Yu . The training objective of this task can be formulated as: 3.3.1 Dialogue Coherence Modeling Many studies (Kuang et al., 2018; Wang et al., 2019b; Xiong et al., 2019; Wang and Wan, 2019; Huang et al., 2020) have indicated that the modeling of global textual coherence can lead to more coherent text generation. Inspired by this, we add two response generation tasks and an utterance discrimination task during the NCT model training. All the three tasks are related to the dialogue coherence of conversations, thus introducing the modeling of dialogue coherence into the NCT model. LCRG = − |Yu | X log(p(Yu,t |CXu , Yu,<t )), t=1 p(Yu,t |CXu , Yu,<t ) = Softmax(Wc hL d,t + bc ), where hL d,t denotes the top-layer decoder h"
2021.emnlp-main.6,P19-1116,0,0.0223492,"Missing"
2021.emnlp-main.6,D15-1130,0,0.0255851,"oposed approach. 1 Figure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., th"
2021.emnlp-main.6,P02-1040,0,0.1097,"ther with the main chat translation task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the ch"
2021.emnlp-main.6,P18-1117,0,0.0188357,"ever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as"
2021.emnlp-main.6,P19-1050,0,0.0330435,"Missing"
2021.emnlp-main.6,I17-3009,0,0.0152874,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,W18-6319,0,0.016912,"raining, with the main chat translation task and four auxiliary tasks, the total training objective is finally formulated as Experiments Datasets and Metrics Datasets. As shown in Algorithm 1, the training of our CSA-NCT framework consists of two stages: (1) pre-train the model on a large-scale sentencelevel NMT corpus (WMT208 ); (2) fine-tune on the chat translation corpus (BConTrasT (Farajian et al., 2020) and BMELD (Liang et al., 2021a)). The dataset details (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is"
2021.emnlp-main.6,2020.wmt-1.60,0,0.0508315,"Missing"
2021.emnlp-main.6,2020.wmt-1.74,0,0.0176454,"ations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different"
2021.emnlp-main.6,D19-1085,0,0.0969453,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,P16-1162,0,0.146835,"Missing"
2021.emnlp-main.6,2006.amta-papers.25,0,0.320291,"ion task, the NCT model is optimized through the joint objectives of all these auxiliary tasks. In this way, the model is enhanced to capture dialogue coherence and speaker personality in conversation, which thus can generate more coherent and speaker-relevant translations. We validate our CSA-NCT framework on the datasets of different language pairs: BConTrasT (Farajian et al., 2020) (En⇔De1 ) and BMELD (Liang et al., 2021a) (En⇔Zh2 ). The experimental results show that our model achieves consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006), demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can generate more coherent and speaker-relevant translations compared to the existing related methods. Our contributions are summarized as follows: • We propose a multi-task learning framework with four auxiliary tasks to help the NCT model generate more coherent and speakerrelevant translations. • Extensive experiments on datasets of different language pairs demonstrate that our model with multi-task learning achieves the state-ofthe-art performances on the chat translation task and signif"
2021.emnlp-main.6,D17-1301,0,0.0245697,"gure 1: A dialogue example (En⇔Zh) when translating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherenc"
2021.emnlp-main.6,L16-1436,0,0.0257929,"at our model yields more coherent and speaker-relevant translations. Acknowledgements Related Work The research work descried in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Nature Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialo"
2021.emnlp-main.6,2020.amta-research.11,0,0.0387902,"ls (e.g., splits of training, validation or test sets) are described in Appendix A. Metrics. For fair comparison, we use the SacreBLEU9 (Post, 2018) and TER (Snover et al., 2006) with the statistical significance test (Koehn, 2004). For En⇔De, we report case-sensitive score following the WMT20 chat task (Farajian et al., 2020). For Zh⇒En, we report case-insensitive score. For En⇒Zh, the reported SacreBLEU is at the character level. 4.2 En⇒Zh 29.69/55.4 30.52/54.6 attention. All our Transformer models contain L = 6 encoder layers and L = 6 decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. The training step for the first pre-training stage is set to T1 = 200,000 while that of the second fine-tuning stage is set to T2 = 5,000. The batch size for each GPU is set to 4096 tokens. All experiments in the first stage are conducted utilizing 8 NVIDIA Tesla V100 GPUs, while we use 4 GPUs for the second stage, i.e., fine-tuning. That gives us about 8*4096 and 4*4096 tokens per update for all experiments in the first-stage and second-stage, respectively. All models are optimized using Adam (Kingma and Ba, 2014) with β1 = 0.9 and β2 = 0.998, and learning rate is set to 1.0 for a"
2021.emnlp-main.6,2021.naacl-industry.14,0,0.0299036,"d suggestions to improve this paper. Chat NMT. Little prior work is available due to the lack of human-annotated publicly available data (Farajian et al., 2020). Therefore, some existing studies (Wang et al., 2016; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pay attention to designing methods to automatically construct the subtitle corpus, which may contain noisy bilingual utterances. Recently, Farajian et al. (2020) organize the WMT20 chat translation task and first provide a chat corpus post-edited by humans. More recently, based on document-level parallel corpus, Wang et al. (2021) propose to jointly identify omissions and typos within dialogue along with translating utterances by using the context. As a concurrent work, Liang et al. (2021a) provide a clean bilingual dialogue dataset and design a variational framework for NCT. Different from them, we focus on introducing the modeling of dialogue coherence and speaker personality into the NCT model with multi-task learning to promote the translation quality. References JinYeong Bak and Alice Oh. 2019. Variational hierarchical user-based conversation model. In Proceedings of EMNLP-IJCNLP, pages 1941–1950. Calvin Bao, Yow-"
2021.emnlp-main.6,W17-4811,0,0.0197872,"a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation invol"
2021.emnlp-main.6,W18-6312,0,0.023112,"ranslating the utterance Xu . CRG: cross-lingual response generation. MRG: monolingual response generation. tion task becomes more important and has a wider range of applications. In recent years, although sentence-level Neural Machine Translation (NMT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter fo"
2021.emnlp-main.6,D19-1511,0,0.0743925,"al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and another in Chinese as shown in Fig. 1), where a ch"
2021.emnlp-main.6,Q18-1029,0,0.0199723,"MT) models (Sutskever et al., 2014; Vaswani et al., 2017; Hassan et al., 2018; Meng and Zhang, 2019; Yan et al., 2020; Zhang et al., 2019) have achieved remarkable progress and can be directly used as the chat translator, they often lead to incoherent and speakerirrelevant translations (Mirkin et al., 2015; Wang et al., 2017a; Läubli et al., 2018; Toral et al., 2018) due to ignoring the chat history that contains useful contextual information. To exploit chat history, context-aware NMT models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Bawden et al., 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Wang et al., 2019a; Maruf et al., 2019; Chen et al., 2020; Ma et al., 2020, etc) can also be directly adapted to chat translation. However, their performances are usually limited because of lacking the modeling of the inherent dialogue characteristics (e.g., the dialogue coherence and speaker personality), which matter for chat translation task as pointed out by Farajian et al. (2020). In this paper, we propose a Coherence-SpeakerIntroduction A cross-lingual conversation involves participants that speak in different languages (e.g., one speaking in English and an"
2021.emnlp-main.6,2020.acl-main.7,0,0.0390104,"1|CYsxu , Yu )) − log(p(` = 0|CYsyu , Yu )). (5) For a training sample (CYs u , Yu ) with s∈{sx, sy}, we also use the NCT encoder to obtain the representations HYu of the target utterance Yu and HCYs u of the given speaker-specific history context CYs u . P|Yu |L Similar to the NUD task, HYu = |Y1u |t=1 he,t and L s the he,0 of CYu is used as HCYs . Then, to estimate u the probability in Eq. 5, the concatenation of HYu and HCYs is fed into a binary SI classifier, which u is another fully-connected layer on top of the NCT encoder: Speaker Identification (SI). As explored in (Bak and Oh, 2019; Wu et al., 2020; Liang et al., 2021b; Lin et al., 2021), the history utterances of a speaker can reflect a distinctive personality. Fig. 3(d) depicts the SI task in detail, where the NCT model is used to distinguish whether a translated utterance and a given speaker-specific history utterances are spoken by the same speaker. We also construct positive and negative training samples from the chat corpus. A positive sample (CYsxu , Yu ) with the label ` = 1 consists of the target utterance Yu and the speaker sx-specific history context CYsxu , because Yu is the translation of the utterance originally spoken by"
2021.emnlp-main.6,D19-1081,0,0.0254488,"Missing"
2021.emnlp-main.6,P19-1193,0,0.0575932,"Missing"
2021.emnlp-main.6,D18-1049,0,0.0126714,"setting, on En⇒De and De⇒En, our model consistently surpasses the baselines and other existing systems again. • Dia-Transformer+FT (Maruf et al., 2018): The original model is RNN-based and an additional encoder is used to incorporate the mixed-language dialogue history. We reimplement it based on Transformer where an additional encoder layer is used to introduce the dialogue history into NMT model. • Doc-Transformer+FT (Ma et al., 2020): A state-of-the-art document-level NMT model based on Transformer sharing the first encoder layer to incorporate the dialogue history. • Gate-Transformer+FT (Zhang et al., 2018): A document-aware Transformer that uses a gate to incorporate the context information. Note that we share the Transformer encoder to obtain the context representation instead of utilizing the additional context encoder, which performs better in our experiments. 4.5 Results on En⇔Zh. We also conduct experiments on the BMELD dataset. Concretely, on En⇒Zh and Zh⇒En, our model also presents notable improvements over all comparison models by at least 2.43↑ and 0.77↑ BLEU gains under the Base setting, and by 1.73↑ and 1.43↑ BLEU gains under the Big setting, respectively. These results demonstrate t"
2021.emnlp-main.6,2020.emnlp-main.279,0,0.0525763,"Missing"
2021.findings-acl.383,P12-1027,0,0.02912,"d reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Cai et al., 2017) have been rep"
2021.findings-acl.383,2020.acl-main.734,0,0.448392,"nihao 1 processing task for many NLP tasks. In this aspect, Chinese word segmentation (CWS) is widely acknowledged as an essential task for Chinese NLP. CWS has made substantial progress in recent studies on several benchmarks, which is reported by Huang and Zhao (2007) and Zhao et al. (2019). In particular, pretrained language models (PLMs), like BERT (Devlin et al., 2019), have established new state-of-the-art in sequence labeling (Meng et al., 2019). Various fine-tuning methods have been proposed to improve the performance of indomain and cross-domain CWS based on PLMs (Huang et al., 2020; Tian et al., 2020). The two challenging problems in CWS, segmentation ambiguity and out-of-vocabulary (OOV) words, have been significantly mitigated by PLM-based methods that are fine-tuned on large-scale annotated CWS corpora. Such methods are even reaching human performance on benchmarks. Nevertheless, CWS is more valuable as a prelude for downstream NLP tasks than as a standalone task. Intrinsic evaluation of CWS on benchmark datasets only examines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from t"
2021.findings-acl.383,I05-3027,0,0.129456,"decoding is thus fast for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al.,"
2021.findings-acl.383,O03-4002,0,0.318519,"er strong baselines for CWS by the traditional intrinsic evaluation. • The lightweight student can be deployed on a small-scale device, even in a non-GPU environment. We abandon the PLM neural architectures (teacher model) during decoding. The speed of decoding is thus fast for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and"
2021.findings-acl.383,I08-4017,0,0.0369771,"st for practical application. Our method reduces the complexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao,"
2021.findings-acl.383,D13-1061,0,0.0913855,"Missing"
2021.findings-acl.383,D17-1079,0,0.10168,"standalone task. Intrinsic evaluation of CWS on benchmark datasets only examines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from the perspective of practicability. In this paper, we define the practicability of CWS with two aspects: low complexity as a standalone task and high beneficiality to downstream tasks. The complexity is twofold: 1) complexity of implementation and 2) time and space complexity of a CWS algorithm. Previous neural methods usually require additional resources (Zhou et al., 2017; Ma et al., 2018; Zhang et al., 2018b; Zhao et al., 2018; Yang et al., 2019; Qiu et al., 2020), such as external pre-trained embeddings. The complexity of implementation is reflected in the difficulty of acquiring external resources. External resources 4369 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4369–4381 August 1–6, 2021. ©2021 Association for Computational Linguistics vary in quality and the length of time for computation, For example, it is time-consuming to obtain effective pre-trained embeddings as they are trained on a huge amount of data. Gene"
2021.findings-acl.383,N19-1278,0,0.0772693,"ines the effectiveness of current neural methods on word boundary detection. To better apply CWS in downstream NLP tasks, we should comprehensively re-think CWS from the perspective of practicability. In this paper, we define the practicability of CWS with two aspects: low complexity as a standalone task and high beneficiality to downstream tasks. The complexity is twofold: 1) complexity of implementation and 2) time and space complexity of a CWS algorithm. Previous neural methods usually require additional resources (Zhou et al., 2017; Ma et al., 2018; Zhang et al., 2018b; Zhao et al., 2018; Yang et al., 2019; Qiu et al., 2020), such as external pre-trained embeddings. The complexity of implementation is reflected in the difficulty of acquiring external resources. External resources 4369 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4369–4381 August 1–6, 2021. ©2021 Association for Computational Linguistics vary in quality and the length of time for computation, For example, it is time-consuming to obtain effective pre-trained embeddings as they are trained on a huge amount of data. Generally, it is difficult to maintain high CWS performance for many previous ne"
2021.findings-acl.383,D13-1031,0,0.0269367,"lexity of implementation, inference time, and memory consumption. • We empirically investigate the effectiveness of the proposed method to downstream Chinese NLP tasks and analyze the impact of segmentation results on them via extrinsic evaluations. 2 Related Work Since Xue (2003) formalizes CWS as a sequence labeling problem, many traditional statistical methods have achieved high performance for CWS on several benchmarks (Emerson, 2005). According to (Huang and Zhao, 2007) and (Zhao et al., 2019), CRF-based models (Tseng et al., 2005; Zhao and Kit, 2008; Zhao et al., 2010; Sun et al., 2012; Zhang et al., 2013) and neural methods (Zheng et al., 2013; 4370 3 Proposed Framework Aiming at not only keeping competitive performance on benchmarks but also reducing the complexity of the CWS methods, our proposed framework consists of two essential modules: a student model and a teacher model, as shown in Figure 1. There is an obvious performance gap between the model based on PLMs (Huang et al., 2020) and the lightweight model (Duan and Zhao, 2020). The Teacher Model RoBERTa Encoder B Softmax Linear Pei et al., 2014; Chen et al., 2015; Cai and Zhao, 2016; Cai et al., 2017) have been reported to outperform t"
2021.findings-acl.383,Y98-1020,0,0.280737,"Missing"
C10-2136,P05-1067,0,0.108695,"ing of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over"
C10-2136,P03-2041,0,0.165157,"while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achie"
C10-2136,W02-1039,0,0.0891427,"cently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (Chiang, 2007). So 1 A block is a bilingual phrase without maximum length limitation. 1185 Coling 2010: Poster Volume, pages 1185–1193, Beijing, August 2010 we think it will be a promising way to integrate the target-side dependency str"
C10-2136,P06-1121,0,0.0348927,"9). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a"
C10-2136,P07-1090,0,0.0199902,"cted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be d"
C10-2136,P08-1066,0,0.446303,"influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical phrase-based system (C"
C10-2136,C10-1123,1,0.783262,"Missing"
C10-2136,P96-1021,0,0.0727476,"w that our system achieves significant improvements over the baseline system on various test sets even with fewer phrases. 1 Introduction Bracketing transduction grammar (BTG) (Wu, 1995) is an important subclass of synchronous context free grammar, which employs a special synchronous rewriting mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these met"
C10-2136,P06-1066,1,0.905951,"writing mechanism to parse parallel sentence of both languages. Due to the prominent advantages such as the simplicity of grammar and the good coverage of syntactic diversities in different language pairs, BTG has attracted increasing attention in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue th"
C10-2136,C08-1127,0,0.0148475,"chine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constitu"
C10-2136,D09-1127,1,0.839344,"DIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the training corpus. During the process of bilingual phrase extraction, we collect the neighboring blocks without 1 The training corpus consists of six LDC corpora: LDC2002E18, LDC2003E07, LDC2003E14, Hansards part"
C10-2136,D09-1073,0,0.0134639,"MT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Ga"
C10-2136,C04-1090,0,0.414998,"g the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a signi"
C10-2136,zhang-etal-2004-interpreting,0,0.068922,"Missing"
C10-2136,P09-1063,1,0.918036,"Missing"
C10-2136,P00-1056,0,0.0838278,"s-head) · P (wl2 |wl1 , wh -as-head) 6 where ‘-as-head’ is used to distinguish the head word from child word in the language model. In like manner, P (wR |wh -as-head) has a similar calculation method. KDIK Figure 5: Dependency combination of the illformed dependency structure dl with the right well-formed dependency structure dr . G denotes gap and the dotted line denotes the substitution of the gap G with dr . P (wL |wh -as-head) ... · P (wln |wln−1 , wln−2 ) JE Setup The training corpus1 comes from LDC with 1.54M bilingual sentences (41M Chinese words and 48M English words). We run GIZA++ (Och and Ney, 2000) to obtain word alignments with the heuristic method “grow-diag-final-and”. Then we parse the English sentences to generate a string-to-dependency word-aligned corpus using the parser (Huang et al., 2009). From this corpus, we extract bilingual phrases with dependency structure. Here, the maximum length of the source phrase is set to 7. For the n-gram LM, we use SRILM Toolkits (Stolcke, 2002) to train a 4-gram LM on the Xinhua portion of the Gigaword corpus. For the dependency LM, we train different 3-gram dependency LMs at word level and POS level separately on the English side of the trainin"
C10-2136,D07-1056,0,0.0175994,"on in statistical machine translation (SMT). In flat reordering model (Wu, 1996; Zens et al., 2004), which assigns constant reordering probabilities depending on the language pairs, BTG constraint proves to be very effective for reducing the search space of phrase reordering. To pursue a better method to predict the order between two neighboring blocks1 , Xiong et al. (2006) present an enhanced BTG with a maximum entropy (ME) based reordering model. Along this line, source-side syntactic knowledge is introduced into the reordering model to improve BTG-based translation (Setiawan et al., 2007; Zhang et al., 2007; Xiong et al., 2008; Zhang and Li, 2009). However, these methods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two cate"
C10-2136,P03-1021,0,0.041434,"Missing"
C10-2136,P08-1064,0,0.0141864,"thods mainly focus on the utilization of source syntactic knowledge, while ignoring the modeling of the target-side syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language"
C10-2136,P02-1040,0,0.0785539,"Missing"
C10-2136,P05-1034,0,0.0710455,"syntax that directly influences the translation quality. As a result, how to obtain better translation by exploiting target syntactic knowledge is somehow neglected. Thus, we argue that it is important to model the target-side syntax in BTG-based translation. Recently, modeling syntactic information on the target side has progressed significantly. Depending on the type of output, these models can be divided into two categories: the constituentoutput systems (Galley et al., 2006; Zhang et al., 2008; Liu et al., 2009) and dependencyoutput systems (Eisner, 2003; Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Shen et al., 2008). Compared with the constituent-output systems, the dependency-output systems provide a simpler platform to capture the target-side syntactic information, while also having the best interlingual phrasal cohesion properties (Fox, 2002). Typically, Shen et al. (2008) propose a string-todependency model, which integrates the targetside well-formed dependency structure into translation rules. With the dependency structure, this system employs a dependency language model (LM) to exploit long distance word relations, and achieves a significant improvement over the hierarchical ph"
C10-2136,N03-1017,0,\N,Missing
C10-2136,J07-2003,0,\N,Missing
C16-1240,J07-2003,0,0.0691293,"following intrinsic and extrinsic evaluations are based on this similarity measurement. 6 Experiments In this section, we carried out a series of experiments to validate the effectiveness of our proposed bilingual autoencoders on NIST Chinese-English translation tasks using large-scale bilingual training data. In particular, we investigated 1) whether our model is able to distinguish parallel sentence pairs from nonparallel sentences, and 2) whether our model can improve machine translation quality. 6.1 Setup Our translation decoder is a state-of-the-art hierarchical phrased-based SMT system (Chiang, 2007). The bilingual training data is the combination of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News), which contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words. We used a 4-gram language model which was trained on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM4 toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. In addition to the baseline decoder, we also compared our bilingual autoencoder against the abovementioned BBoWAE model (Chandar et al., 2014). To train this model, we used the"
C16-1240,P11-2031,0,0.0194853,"site direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathchandar.in/crl.html 6 We make this choice due to the following two reasons: 1) the correlation term is meaningless"
C16-1240,P13-1088,0,0.18244,"continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual sem"
C16-1240,P14-1006,0,0.0212393,"ntence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that"
C16-1240,W13-3214,0,0.105523,"epresentations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have pr"
C16-1240,P14-1062,0,0.0595323,"ences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, fo"
C16-1240,D14-1181,0,0.0300596,"intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following eff"
C16-1240,C12-1089,0,0.0339658,"; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are re"
C16-1240,P15-2029,0,0.0710106,"nd extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual"
C16-1240,P03-1021,0,0.00997899,"and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the following experiments. 4 http://www.speech.sri.com/projects/srilm/download.html http://www.sarathch"
C16-1240,P02-1040,0,0.0961855,"erms of input and target descriptors, we investigated two variants of the proposed bilingual autoencoder: 1) min and max as the input descriptors, while avg and std as the target descriptors (MM2AS); and 2) the opposite direction (AS2MM). Throughout all experiments, we set B = 100, α = 0.100, λL = 10−6 , λW = 10−3 and m = 2d. With respect to the dimensionality of word embeddings, we tried four different dimensions from 25 to 100 with an increment of 25 each time. We used the NIST evaluation set of MT05 as our development set, and sets of MT06/MT08 as the test sets. Case-insensitive NIST BLEU (Papineni et al., 2002) was used to measure translation performance. We used minimum error rate training (MERT) (Och, 2003) to optimize the feature weights. In order to alleviate the instability of MERT, we followed Clark et al. (2011) to run MERT three times and report average BLEU scores over the three runs for all our MT experiments. 6.2 Intrinsic Evaluation: Semantic Analysis The first group of experiments aims at analyzing the ability of our model in distinguishing parallel sequences from nonparallel sequences, at both the phrase and sentence level. For convenience, we used MM2AS model and set d = 25 in all the"
C16-1240,D11-1014,0,0.313555,"our bilingual antoencoder is able to learn continuous-valued latent representations for parallel sentences. Experiments on both intrinsic and extrinsic evaluations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence represent"
C16-1240,D15-1146,1,0.9177,"ed. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g."
C16-1240,P15-1150,0,0.163365,"uations on statistical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings ("
C16-1240,W11-0329,0,0.0345195,"arallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing"
C16-1240,P14-1011,0,0.325772,"e language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words"
C16-1240,D15-1266,1,0.931462,"ical machine translation tasks show that our autoencoder achieves substantial improvements over the baselines. 1 Introduction Neural sentence modeling that learns continuous-valued vector representations for sentences in a lowdimensional latent semantic space, has recently attracted considerable interests in the field of nature language processing (NLP). A variety of models have been proposed (Collobert and Weston, 2008; Socher et al., 2011; Mikolov et al., 2011; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014; Ma et al., 2015; Tai et al., 2015; Zhang et al., 2015a). Most of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2"
C16-1240,P15-1042,0,0.107821,"ally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and scalable. However, they often heavily rely on only one descriptor of the bag-of-words embeddings (e.g. the avg representation) and hence are weak in capturing fine-grained complex linguistic phenomena. Therefore we believe that modeling parallel se"
C16-1240,D13-1141,0,0.0312817,"ost of these models, however, focus on monolingual cases where sentences from a single language are modeled. They do not explore semantic correspondences among parallel sentences. On account of this, monolingual neural sentence models are not naturally fit for bilingual or cross-lingual NLP tasks, such as machine translation, cross-lingual classification and information retrieval. In order to induce sentence representations in a bilingual rather than monolingual semantic space, researchers have proposed a few approaches, following efforts of bilingual word embeddings (Klementiev et al., 2012; Zou et al., 2013). These studies either explore recursive/recurrent neural networks (Zhang et al., 2014; Su et al., 2015) or bag-of-words based neural networks (Yih et al., 2011; Chandar et al., 2014; Lauly et al., 2014; Hermann and Blunsom, 2014; Zhou et al., 2015) to learn bilingual sentence representations. The former recursively compose sentences from the bottom up, taking into account bilingual constraints from word alignments. Due to the complexity, they are not easy to be scalable. Additionally, they also suffer from errors and noises of word alignments. In contrast, the latter are relatively simple and"
C16-1289,P14-1129,0,0.0338229,"the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it i"
C16-1289,P14-1066,0,0.112826,"ic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive auto"
C16-1289,D15-1181,0,0.100224,"n of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the convolution-enhanced bilingual recursive neural network. ??? ??? ???? ??? ??? ???? ???? ? ?1 ? ???? ?2 ??? ?1 ??? ?2 ?3 Figure 2: An illustration of the proposed RecNN. We use a yellow/green circle to represent the preference score of a node to be an SAC/non-SAC node. (Socher et al., 2011a), where they compute an interaction matrix from which discriminative features are dynamically extracted for paraphrase identification. He et al. (2015) and Yin et al. (2015b) further extend this idea to convolutional neural network. Although our method is partially inspired by them, we implement it in a completely different manner. 3 Convolution-Enhanced Bilingual Recursive Neural Network This section elaborates the proposed ConvBRNN model, of which network structure is shown in Figure 1. We begin with the generation of phrase structures via a recursive neural network. We then elaborate how to perform convolution upon the generated phrase structures. After that, we describe our bilingual semantic similarity model. Finally, we provide a detai"
C16-1289,P15-2088,0,0.0697352,"n Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive autoencoder (RAE) as the underlying model to build tree s"
C16-1289,P14-1062,0,0.270956,"ion in RAE (i.e. reconstruction errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the underlying phrase structure for a sentence is generated automatically in our model, instead of taking from a given constituency or dependency tree. Besides, the exploration of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the c"
C16-1289,D14-1181,0,0.00380963,"tion errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the underlying phrase structure for a sentence is generated automatically in our model, instead of taking from a given constituency or dependency tree. Besides, the exploration of the semantic embeddings at different levels of granularity is firstly investigated in 3072 Bilingual Semantic Supervision TreeCNN RecNN Figure 1: An illustration of the convolution-e"
C16-1289,W04-3250,0,0.0380397,"e FBIS corpus and Handsards part of LDC2004T07 corpus. We ran GIZA++3 on the training data in two directions and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration number Niter =100. To train BCorrRAE, we used the same training data and method for hyper"
C16-1289,D13-1054,0,0.12769,"NN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two pr"
C16-1289,P13-1078,0,0.0210658,"o train the ConvBRNN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often s"
C16-1289,P14-1140,0,0.0174179,"inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for th"
C16-1289,D15-1279,0,0.0536095,"ber 11-17 2016. To solve these problems, we propose a convolution enhanced bilingual recursive neural network (ConvBRNN), which exploits word alignments to guide the generation of phrase structures and then integrates embeddings of different linguistic units on the phrase structures into bilingual semantic modeling. Specifically, we develop a new recursive neural network, in which the composition criterion for tree construction is the degree of consistency to word alignments rather than the reconstruction error. Furthermore, we propose a variant of the tree-based convolutional neural network (Mou et al., 2015) to fully access all embeddings on the phrase structures, which can be used to produce better phrase representations (see Section 3.2). All these make ConvBRNN more suitable for the subsequent bilingual semantic modeling, where a bilinear model is introduced to interact and compare the source and target phrase representations in terms of the degree of semantic equivalence. To train our model, we introduce two max-margin losses: one for the bilingual semantic structure inference and the other for the semantic similarity model, both of which are derivable. We conduct experiments on large-scale c"
C16-1289,P02-1038,0,0.0337119,"λRT λwa λsem kθL k2 + kθRT k2 + kθwa k2 + kθsem k2 2 2 2 2 (12) We apply L-BFGS to tune parameters based on gradients over the joint error, as implemented in (Socher et al., 2011c). Word vector embeddings θL are initialized with the toolkit Word2Vec2 on a large scale unlabeled data. Other parameters are randomly initialized according to a normal distribution (µ = 0,σ = 0.01). With the trained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN),"
C16-1289,J03-1002,0,0.0520476,"e pairs, we design a new semantic composition metric based on word alignments. As word alignments are shared across the source and target language, they are suitable to act as a desirable bridge for modeling bilingual semantics. To achieve this goal, we first use the structural alignment consistency (SAC) (Su et al., 2015) that is the basis of our model to classify resultant nodes of semantic compositions into two categories. Specifically, if the node n covers a sub-phrase, and there exists a sub-phrase in the other language such that these two sub-phrases are consistent with word alignments (Och and Ney, 2003), we say n satisfies the structural alignment consistency, and it is referred to as an SAC node, otherwise, it is a non-SAC node. Then, we introduce two functions Scorecon (n) and Scoreinc (n) to measure the preference strength of node n to be an SAC or a non-SAC node, respectively (sac) (sac) Scorecon (n) = Wcon pn , Scoreinc (n) = Winc (sac) pn (2) (sac) where Wcon ∈R1×d and Winc ∈R1×d are parameter matrices. Furthermore, we calculate the final semantic composition score of node n as follows Scoresc (n) = exp(Scorecon (n)) exp(Scoreinc (n)) (3) Obviously, the larger Scorecon (n) is than Scor"
C16-1289,P02-1040,0,0.0963678,"corpus contains 1.0M sentence pairs (25.2M Chinese words and 29M English words) that are from the FBIS corpus and Handsards part of LDC2004T07 corpus. We ran GIZA++3 on the training data in two directions and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration nu"
C16-1289,P15-1004,0,0.0199993,"other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semant"
C16-1289,D11-1014,0,0.3969,"ded vector. Unlike the work mentioned above, our model mainly explore word alignments to guide the generation of bilingual phrase structures. The most relevant work to ours is the model proposed by Su et al. (2015), where they treat word alignments as a constraint to the RAE model. However, as discussed in Section 1, the composition criterion in RAE (i.e. reconstruction errors) does not allow us to fully benefit from word alignments. Therefore, we introduce a new composition criterion based on word alignment consistency. The proposed recursive neural network works in a way similar to that in (Socher et al., 2011b) except for our specific bilingual supervision. Zhang et al. (2014b) also propose a recursive neural network. However, their model mainly focuses on the composition in machine translation process (namely, swap or monotone), which is different from ours. Additionally, our model also adapts convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014) to extract semantic information encoded in phrase structures. Our model is related to the treebased convolution (Mou et al., 2015). The differences are 1) that we treat the whole tree structure as the window for convolution; and 2) that the"
C16-1289,D15-1146,1,0.634702,"on. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ the recursive autoencoder (RAE) as the underlying mod"
C16-1289,J97-3002,0,0.555749,"With the trained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN), which simply treats the embedding of root node of the phrase structure as the semantic representation of the whole phrase, instead of the convoluted one. Other components of ConvBRNN-CNN are the same as those in the ConvBRNN model. All translation systems used the log-linear framework. The adopted sub-models include: (1) rule translation probabilities in two directio"
C16-1289,P10-1049,0,0.0261707,"rections and applied the “grow-diag-final-and” heuristic rule to obtain word alignments. We trained a 5-gram language model on the Xinhua portion of the GIGAWORD corpus using SRILM Toolkit4 with modified Kneser-Ney Smoothing. We chose the 2005 NIST MT evaluation test data as the development set, and the 2006, 2008 NIST MT evaluation test data as the test sets. We used case-insensitive BLEU-4 metric (Papineni et al., 2002) to evaluate translation quality, and conducted paired bootstrap sampling (Koehn, 2004) for significance test. Network Training To train ConvBRNN, we applied forced decoding (Wuebker et al., 2010) on the training corpus to extract high-quality bilingual phrases for model training. We tuned the optimal hyperparameters via random search method (Bergstra and Bengio, 2012) to minimize the joint error on a small portion of our training data. Finally, we set ds = dt = dsem = 50, h = 5, L = 10, k = 3, α = 0.116, λL = 2.14e−7 , λRT = 2.43e−5 , λwa = 7.33e−5 and λsem = 4.03e−6 , the L-BFGS iteration number Niter =100. To train BCorrRAE, we used the same training data and method for hyper-parameter optimization. 1 Note that the source and target languages have different four sets of parameters."
C16-1289,P06-1066,1,0.623752,"rained model parameters, we can easily obtain the dense semantic vectors for bilingual phrases. During translation, we incorporate the derived phrasal similarity feature into the standard log-linear framework (Och and Ney, 2002) of SMT for translation selection. 4 Experiment We conducted experiments on NIST Chinese-English translation task to validate the effectiveness of ConvBRNN. System Overview Our baseline decoder is a state-of-the-art phrase-based translation system equipped with a maximum entropy based reordering model, which adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006). We compared the proposed model with two models: (1) the bilingual correspondence model (BCorrRAE) proposed by Su et al. (2015); (2) the proposed model without the convolutional neural network (ConvBRNN-CNN), which simply treats the embedding of root node of the phrase structure as the semantic representation of the whole phrase, instead of the convoluted one. Other components of ConvBRNN-CNN are the same as those in the ConvBRNN model. All translation systems used the log-linear framework. The adopted sub-models include: (1) rule translation probabilities in two directions, (2) lexical weigh"
C16-1289,P13-1017,0,0.0180214,"max-margin losses to train the ConvBRNN model: one for the phrase structure inference and the other for the semantic similarity model. Experiments on NIST Chinese-English translation tasks demonstrate the high quality of the generated bilingual phrase structures with respect to word alignments and the effectiveness of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their suc"
C16-1289,N15-1091,0,0.054072,"Missing"
C16-1289,P15-1007,0,0.0593269,"Missing"
C16-1289,P14-1011,0,0.2221,"ess of learned semantic similarities on machine translation. 1 Introduction Recently, adapting deep neural networks to statistical machine translation (SMT) is of growing interest due to their superior capacity against conventional lexical models in feature learning and representation (Yang et al., 2013; Liu et al., 2013; Li et al., 2013; Devlin et al., 2014; Liu et al., 2014; Setiawan et al., 2015). As phrases are the basic translation units in many SMT systems, one line of research among these studies is to learn the semantic similarity of bilingual phrases for translation selection in SMT (Zhang et al., 2014a; Gao et al., 2014; Cho et al., 2014; Su et al., 2015; Hu et al., 2015). Typically, these bilingual semantic similarity models learn source and target phrase representations with some bilingual constraints (Gao et al., 2014; Hu et al., 2015; Zhang et al., 2014a). In spite of their success, they often suffer from two problems. Firstly, it is difficult for them to recover the semantic hierarchy (binary tree structure) of a bilingual phrase. In this respect, Su et al. (2015) improve tree construction by incorporating word alignments into their objective function. Unfortunately, they still employ"
C18-1124,buck-etal-2014-n,0,0.0611415,"Missing"
C18-1124,P15-1001,0,0.0303019,"ranslated text conditioned on the input representation. A potential issue with this encoderdecoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with translating long sentences. A recent successful extension of NMT models is the attention mechanism which conducts a soft search over source tokens and yields an attentive vector to represent the most relevant segments of the source sentence for the current decoding state (Bahdanau et al., 2014; Jean et al., 2015; Luong et al., 2014; Vaswani et al., 2017). The typical attention mechanism frees the neural translation model from having to squash all the information of the source sentence into a fixed vector, however, it ignores some important information hidden in the target sequence (Cheng et al., 2016). At least three challenges still remains in the translation process. The first issue relates to the memory compression problems in the decoding process. During each translation step, a hidden state vector implicitly maintains at least two types of information, including both the most relevant source con"
C18-1124,P17-1064,1,0.851355,"for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the simil"
C18-1124,D15-1166,0,0.0604492,"ext incrementally from left to right. In the NMT task, Wang et al. (2016) present a decoder enhanced decoder with an external shared memory which extends the capacity of the network and has the potential to read, write, and forget information. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 201"
C18-1124,D16-1096,0,0.0185334,"ion. In fact D HE A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be vie"
C18-1124,N18-1124,0,0.0379124,"Missing"
C18-1124,P02-1040,0,0.100662,"mplementation of gate combination where the the gating weights are equal to the attention weights. 4 4.1 Experiments Datasets We mainly evaluated our approaches on the widely used NIST Chinese-English translation task. In order to show the usefulness of our approaches, we also provide results on other two translation tasks: English-French, English-German. The evaluation metric is BLEU. For Chinese-English task, we apply case-insensitive NIST BLEU. For other tasks, we tokenized the reference and evaluated the performance with multi-bleu.pl. The metrics are exactly the same as in previous work (Papineni et al., 2002). For Chinese-English, our training data consists of 1.25M sentence pairs extracted from LDC corpora1 , with 27.9M Chinese words and 34.5M English words respectively. We choose NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04), 2005 (MT05), and 2006 (MT06) datasets as our test sets. For English-German, to compare with the results reported by previous work, we used the same subset of the WMT 2014 training corpus that contains 4.5M sentence pairs with 91M English words and 87M German words. The concatenation of news-test 2012 and news-test 2013 is used as the"
C18-1124,P16-5005,0,0.360122,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1008,0,0.340085,"ination At time step i, D HE A reads from M B : zi` = Read(si , M B ) = i X ` ` αij (s`−1 j WV ) (9) j=1 Similarly, αij is computed by Eqn.(6) where j ≤ i and e`ij is computed as : 1 ` T e`ij = √ (s`−1 WQ` )(s`−1 j WK ) ds i 1467 (10) One simple way to aggregate information from zi and ci is by summing them, then the new context vector is computed as: cˆ = z + c (11) It is also worth mentioning that we use s`−1 as the context information to update the hidden state on layer `, since the lower layer states can be perpared in advance to facilitate parallel training. Gate Combination As argued by Tu et al. (2016a), the source side context and the target side context plays a different role during the decoding process, we therefore design a context gate which assigns an element-wise weight to the two-side input: cˆ = g(c, z) · c + (1 − g(c, z)) · z (12) where g(c, z) ∈ (0, 1) is a sigmoid neural network which dynamically controls the amount of information flowing from the source and target contexts. cˆ is the new context vector fed to the decoder in Eqn.(3) which refines s`t = LSTM(s`t−1 , s`−1 ˆ`t ). t ,c With the gated control, the new context vector cˆ can be rectified based on the decoding history"
C18-1124,P16-1125,0,0.0295194,"Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approac"
C18-1124,D16-1093,0,0.0257529,"the potential to better handle sentences of arbitrary length. Second, we forcus on controlling the information flow between the source side memory and the target side memory and design a gate to balance the contribution of the two-sides. Recurrent Residual Networks Our work is also related to residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2015; Szegedy et al., 2016). Recently, several architectures using residual connections with LSTMs have been proposed (Kim et al., 2017; Wang, 2017; Wang and Tian, 2016) for sequence prediction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016;"
C18-1124,P17-1013,1,0.800529,"Missing"
C18-1124,D16-1160,0,0.0213139,"diction. These connections create a direct path from previous layers, helping the transmission of information. Related to our work, Miculicich et al. (2018) propose a target sideattentive residual recurrent network for decoding, where attention over previous words contributes directly to the prediction of the next word. Comparatively, D HE A attends to the previous hidden state and make a combination with the source context. Exploiting Contextual Information A thread of work in sequence to sequence learning attempts to exploit auxiliary context information(Wang and Cho, 2016; Li et al., 2017; Zhang and Zong, 2016). Recently Tu et al. (2016a) propose using context gates in NMT to dynamically control the contributions from the source contexts and the RNN hidden state. Our approach focuses on integrating the decoding history and the source side context to NMT architecture. In addition, we have a multi-layer approach to better utilize the contextual information. Experiments in Section 4.3 show the superiority of D HE A. In the same period of our work, Lin et al. (2018) and Xia et al. (2017) first turn eyes to the target side attention of NMT architecture. Our approach share the similar idea with these work"
C18-1124,P17-1140,1,0.837212,"A can be viewed as a special case of memory networks, with only reading mechanism for the translation task. Quite remarkably D HE A incorporates two different types of memory (source memory and decoding history memory) and significantly improves upon state-of-the-arts. Attention Mechanism Attention in neural networks (Bahdanau et al., 2014; Luong et al., 2015) is designed to assign weights to different inputs instead of threating all input sequences equally as original neural networks do. A number of efforts have been made to improve the attention mechanism(Tu et al., 2016b; Mi et al., 2016; Zhang et al., 2017). Some of them incorporated the previous attention history into the current attention for better alignment, but none of them are based on the decoding history. The application of self-attention mechanisms in RNNs have been previously studied, and in general, it appears to capture syntactic dependencies among distant words (Liu and Lapata, 2017; Lee et al., 2017; Kim et al., 2017; Lin et al., 2017). Vaswani et al. (2017) resort to self-attention mechanism and showed outstanding performance. Our approach is diffrent from their work in two aspect. First, our method can be viewed as a variant of R"
C18-1124,Q16-1027,0,0.0191054,"de attention based NMT. Encoder The goal of the encoder is to build meaningful representations of source sentences. The typical encoder consists of a bidirectional RNN which processes the raw input in backward and forward direction with two separate layers, and then concatenates them together. In this work, we choose another bidirectional approach to process the sequence in order to learn more temporal dependencies. Specifically, an RNN layer processes the input sequence in a forward direction. The output of this layer is taken by an upper RNN layer as input, processed in a reverse direction (Zhou et al., 2016). More formally,The encoder network reads the source input x = {x1 , ..., xT } and processes it into a source side memory M s = {h1 , h2 · · · , hT }. where xi ∈ Rdx . The output on layer ` is  xt , `=1 ` (2) ht = `−1 ` LSTM(ht+d , ht ), ` > 1 where • h`t ∈ Rdh gives the output of layer ` at location t. • The directions are marked by a direction term d = (−1)` . If we fixed d to −1, the input will be processed in forward direction, otherwise backward direction. • We only apply the top-most hidden states as the source side memory which is then fed to the decoder. Decoder The decoder uses anoth"
C18-1276,2015.iwslt-evaluation.1,0,0.168575,"Missing"
C18-1276,D17-1014,0,0.021304,"semantics and pragmatics, the syntactic analysis of utterance can be guided by the global lexical-semantic and discourse information (Altmann and Steedman, 1988; Trueswell et al., 1994, 1993; Tyler and Marslen-Wilson, 1977). In brief, the process of translation is in need of the global information from the target-side context, but the decoding pattern of the conventional Seq2Seq model in NMT does not meet the requirement. Recent researches in NMT have taken this issue into consideration by the implementation of bidirectional decoding. Some methods of bidirectional decoding (Liu et al., 2016; Cong et al., 2017) rerank the candidate translations with the scores from the bidirectional decoding. However, these bidirectional This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3260 Proceedings of the 27th International Conference on Computational Linguistics, pages 3260–3271 Santa Fe, New Mexico, USA, August 20-26, 2018. decoding methods cannot provide effective complementary information due to the limited search space of beam search. In this article, we extend the conventional attention-based Seq2Seq model by"
C18-1276,D13-1176,0,0.535509,"that our model is more competitive compared with the state-of-the-art methods, and the analysis reflects that our model is also robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding. 1 Introduction Deep learning has achieved tremendous success in machine translation, outperforming the traditional linguistic-rule-based and statistical methods. In recent studies of Neural Machine Translation (NMT), most models are based on the sequence-to-sequence (Seq2Seq) model based on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2"
C18-1276,P14-1062,0,0.0232025,"ry to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Also, its application in NMT has been successful (Gehring et al., 2017). Recently, deconvolution was applied to modeling text (Zhang et al., 2017; Shen et al., 2017), which is able to construct a representation of high quality with the self-contained information. 3268 Text: 基因 科学家 的 目标 是 , 提供 诊断 工具 以 发现 致病 的 缺陷 基因 Gold: the goal of geneticists is to provide diagnostic tools to identify defective genes that cause diseases Seq2Seq: the objective of genetic scientists is to provide genes to detect genetic genetic genes DeconvDec: the objective of the gene"
C18-1276,D14-1181,0,0.00305773,"ternal memory to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Also, its application in NMT has been successful (Gehring et al., 2017). Recently, deconvolution was applied to modeling text (Zhang et al., 2017; Shen et al., 2017), which is able to construct a representation of high quality with the self-contained information. 3268 Text: 基因 科学家 的 目标 是 , 提供 诊断 工具 以 发现 致病 的 缺陷 基因 Gold: the goal of geneticists is to provide diagnostic tools to identify defective genes that cause diseases Seq2Seq: the objective of genetic scientists is to provide genes to detect genetic genetic genes DeconvDec"
C18-1276,P18-2027,1,0.854882,"application of the encoder-decoder framework on the machine translation task, which launched the development of NMT. Another significant innovation in this field is the attention mechanism, which builds connection between the translated contents and the source text (Bahdanau et al., 2014; Luong et al., 2015). To improve the quality of NMT, researchers have focused on improving the attention mechanism. Tu et al. (2016) and Mi et al. (2016) modeled coverage in the NMT, Meng et al. (2016) and Xiong et al. (2017) incorporated the external memory to the attention, and Xia et al. (2017) as well as Lin et al. (2018a) utilized the information from the previous generation by target-side attention and memory for attention history respectively. For more target-side information, Ma et al. (2018b) incorporated bag of words as target. A breakthrough of NMT in recent years is that Vaswani et al. (2017) invented a model only with the attention mechanism that reached the state-of-the-art performance. Although many researches in NLP focused on the application of RNN, CNN is also an important type of network for the study of language (Kim, 2014; Kalchbrenner et al., 2014; Zhang et al., 2015; Lin et al., 2018b). Als"
C18-1276,N16-1046,0,0.0312171,"the perspective of semantics and pragmatics, the syntactic analysis of utterance can be guided by the global lexical-semantic and discourse information (Altmann and Steedman, 1988; Trueswell et al., 1994, 1993; Tyler and Marslen-Wilson, 1977). In brief, the process of translation is in need of the global information from the target-side context, but the decoding pattern of the conventional Seq2Seq model in NMT does not meet the requirement. Recent researches in NMT have taken this issue into consideration by the implementation of bidirectional decoding. Some methods of bidirectional decoding (Liu et al., 2016; Cong et al., 2017) rerank the candidate translations with the scores from the bidirectional decoding. However, these bidirectional This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3260 Proceedings of the 27th International Conference on Computational Linguistics, pages 3260–3271 Santa Fe, New Mexico, USA, August 20-26, 2018. decoding methods cannot provide effective complementary information due to the limited search space of beam search. In this article, we extend the conventional attention-ba"
C18-1276,2015.iwslt-evaluation.11,0,0.061468,"the referred articles, and the models are trained on the identical training data or larger training data) on the Chinese-to-English translation, tested on the NIST Machine Translation tasks in 2003, 2004, 2005, 2006 by BLEU score evaluation. Model RNNSearch-1 RNNSearch-2 LabelEmb NPMT Seq2Seq+Attention +DeconvDec BLEU 23.30 26.10 26.80 27.69 26.93 28.47 Table 2: Results of our model and the baselines (directly reported in the referred articles) on the Englishto-Vietnamese translation, tested on the TED tst2013 with the BLEU score evaluation. • RNNsearch-1 The attention-based Seq2Seq model by Luong and Manning (2015); • RNNsearch-2 The implementation of the attention-based Seq2Seq by Huang et al. (2017); • LabelEmb Extending RNNSearch with soft target representation (Sun et al., 2017); • NPMT The Neural Phrased-based Machine Translation model by Huang et al. (2017); 4 4.1 Results and Analysis Results Table 1 shows the overall results of the models on the Chinese-to-English translation task. Beside our reimplementation of the attention-based Seq2Seq model, we report the results of the recent NMT models, which are results in their original articles or improved results of the reimplementation. To facilitate"
C18-1276,D15-1166,0,0.798692,"o robust to translating sentences of different lengths and it also reduces repetition with the instruction from the target-side context for decoding. 1 Introduction Deep learning has achieved tremendous success in machine translation, outperforming the traditional linguistic-rule-based and statistical methods. In recent studies of Neural Machine Translation (NMT), most models are based on the sequence-to-sequence (Seq2Seq) model based on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the ling"
C18-1276,N18-1018,1,0.91309,"ich both contain M trix of the deconvolution-based decoder E and the word embedding matrix E elements), which is more robust to outliers (Girshick, 2015), as well as the cross-entropy loss between the prediction of the deconvolution-based decoder yˆ and reference y given the parameters of the encoder 0 and the deconvolution-based decoder θ . Therefore, the generated matrix E can be closer to the word ˜ and it contains information beneficial to the prediction of the target words. Moreembedding matrix E, over, for the cross entropy loss of the deconvolution-based decoder, we apply the method of Ma et al. (2018a) as it increases no parameter for the prediction by computing the cosine similarity between the output and the word embeddings. To sum up, the loss function is defined as below: N T M T X X 1XX 0 (i) (i) (i) (i) ˜ L=− ( log P (yt |˜ y<t , x , θ) + smoothL1 (Em − Em ) + log P (yt |x(i) , θ )) (12) N i=1 t=1 m=1 t=1 where smooth L1 loss is defined below:  smoothL1 (x, y) = 0.5 ||x − y ||22 if ||x − y ||< 1 ||x − y ||1 −0.5 if ||x − y ||≥ 1 (13) We have tested L1 loss, L2 loss as well as smooth L1 loss in our experiments and found that smooth L1 loss encourages the model to reach the best perf"
C18-1276,P18-2053,1,0.92029,"ich both contain M trix of the deconvolution-based decoder E and the word embedding matrix E elements), which is more robust to outliers (Girshick, 2015), as well as the cross-entropy loss between the prediction of the deconvolution-based decoder yˆ and reference y given the parameters of the encoder 0 and the deconvolution-based decoder θ . Therefore, the generated matrix E can be closer to the word ˜ and it contains information beneficial to the prediction of the target words. Moreembedding matrix E, over, for the cross entropy loss of the deconvolution-based decoder, we apply the method of Ma et al. (2018a) as it increases no parameter for the prediction by computing the cosine similarity between the output and the word embeddings. To sum up, the loss function is defined as below: N T M T X X 1XX 0 (i) (i) (i) (i) ˜ L=− ( log P (yt |˜ y<t , x , θ) + smoothL1 (Em − Em ) + log P (yt |x(i) , θ )) (12) N i=1 t=1 m=1 t=1 where smooth L1 loss is defined below:  smoothL1 (x, y) = 0.5 ||x − y ||22 if ||x − y ||< 1 ||x − y ||1 −0.5 if ||x − y ||≥ 1 (13) We have tested L1 loss, L2 loss as well as smooth L1 loss in our experiments and found that smooth L1 loss encourages the model to reach the best perf"
C18-1276,C16-1205,0,0.496737,"rk (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation involves complex syntactic analysis and"
C18-1276,D16-1096,0,0.328593,"r-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation involves complex synt"
C18-1276,P02-1040,0,0.100435,"Missing"
C18-1276,P17-1099,0,0.0178442,"on, the information from the deconvolution-based decoder is important, which brings significant improvement to the conventional attention-based Seq2Seq model. 4.2 Analysis As our model generates translation with global information from the deconvolution-based decoder, it should learn to reduce repetition as it can learn to avoid generating same contents according to the conjecture by the deconvolution-based decoder about the target-side contexts. In order to test whether our model can mitigate the problem of repetition in translation, we test the repetition on the NIST 2003 dataset, following See et al. (2017). The proportions of the duplicates of 1-gram, 2-gram, 3-gram and 4-gram in each sentence are calculated. Results on Figure 3(a) show that our model generates less repetitive translation. In particular, the proportion of duplicates of our model is less than half of that of the conventional Seq2Seq model. Moreover, to validate its robustness on different sentence-length levels, we test the BLEU scores on sentences of length no shorter than 10 to 60 of the NIST 2003 dataset. According to the results on Figure 3(b), though with the increase in length, the performance of our model is always strong"
C18-1276,P16-1008,0,0.425075,"sed on the encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014) with the attention mechanism (Bahdanau et al., 2014; Luong et al., 2015). While traditional linguistic-rule-based and statistical methods of machine translation require much work of feature engineering, NMT can be trained in the end-to-end fashion. Besides, the attention mechanism can model the alignment relationship between the source text and translation (Bahdanau et al., 2014; Luong et al., 2015), and some recent improved versions of attention have proved successful in this task (Tu et al., 2016; Mi et al., 2016; Meng et al., 2016; Xiong et al., 2017; Vaswani et al., 2017). However, the decoding pattern of the recent Seq2Seq models is inconsistent with the linguistic analysis. As the conventional decoder translates words in a sequential order, the current generation is highly dependent on the previous generation and it is short of the knowledge about future generation. Nida (1969) pointed out that translation goes through a process of analysis, transfer and reconstruction, involving the deep syntactic and semantic structure of the source and target languages. Language generation invo"
C18-1276,D16-1027,0,0.0507872,"-based Seq2Seq with fine-tuned hyperparameters (Bahdanau et al., 2014); • Coverage The method extends RNNSearch with a coverage model for the attention mechanism that tackles the problem of over-translation and under-translation (Tu et al., 2016); • Lattice The Seq2Seq model with a word-lattice-based RNN encoder that tackles the problem of tokenization in NMT (Su et al., 2016); • InterAtten The Seq2Seq model that records the interactive history of decoding (Meng et al., 2016); • MemDec Based on the RNNSearch, it is equipped with external memory that the model reads and writes during decoding (Wang et al., 2016). For the English-to-Vietnamese translation, we compare our model with the recent NMT models for this task, and we present the results of the baselines reported in their articles. 2 http://pytorch.org 3265 Model Moses RNNSearch Lattice Coverage InterAtten MemDec Seq2Seq+Attention +DeconvDec MT-03 32.43 33.08 34.32 34.49 35.09 36.16 35.32 38.04 MT-04 34.14 35.32 36.50 38.34 37.73 39.81 37.25 39.75 MT-05 31.47 31.42 32.40 34.91 35.53 35.91 33.52 36.77 MT-06 30.81 31.61 32.77 34.25 34.32 35.98 33.54 36.32 Ave. 32.21 32.86 34.00 35.49 35.67 36.97 34.91 37.73 Table 1: Results of our model and the b"
D15-1145,D13-1106,0,0.0200271,"ncies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been appl"
D15-1145,D07-1007,0,0.758393,"associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance depende"
D15-1145,P07-1005,0,0.747251,"to two factors: 1) associations of selected translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing"
D15-1145,J90-1003,0,0.339875,"ence, the most ideal translation graph is a graph that includes all source words and their candidate translations. However, this ideal graph has two problems: intensive computation for graph inference and difficulty in modeling dependencies between function and content words. In order to get around these two issues, we only consider lexical selection for source content words2 . We first identify source-side content word pairs using statistical metrics, and then keep word pairs with a high relatedness strength in the translation graph. To be specific, we use pointwise mutual information (PMI) (Church and Hanks, 1990) and co-occurrence frequency to measure the relatedness strength of two source-side words s and s0 within a window ds . Content word pairs will be kept when their co-occurrence frequencies are more than cf times in our training corpus and PMI values are larger than pmi . In this process, we remove noisy word pairs using the following heuristic rules: (1) As an adjective only has relations with its head nouns or dependent adverbs, we remove all word pairs where an adjective is paired with words other than its head nouns or dependent adverbs; (2) We apply a similar constraint to adverbs too, s"
D15-1145,P11-2031,0,0.0127217,"set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermor"
D15-1145,P13-2061,0,0.0248668,"Missing"
D15-1145,P14-1129,0,0.0370286,"Missing"
D15-1145,D08-1039,0,0.140317,"ed translations with lexical items on the source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the"
D15-1145,C08-1041,0,0.290315,"source side, including corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the tran"
D15-1145,E14-1003,0,0.0123714,"another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approa"
D15-1145,D13-1176,0,0.0103886,"lations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et"
D15-1145,N03-1017,0,0.0296397,"tion. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇ"
D15-1145,W04-3250,0,0.0291274,"emented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)5 (TSM), which explores document-level topic information for translation rule selection in the HPB system. Furthermore, we combined our model with the two models to see if we could"
D15-1145,D08-1010,0,0.0188469,"o model global interdependences among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic model"
D15-1145,P14-1030,0,0.100474,"where N (t˜) denotes the set of candidate translations that link to t˜, and RS(t˜, t˜0 ) measures the strength of relatedness between t˜ and t˜0 which is calculated as the average word-level relatedness over all content words in these two translations t˜ and t˜0 . As for the word-level relatedness RS(t, t0 ) for a content word pair (t, t0 ), we estimate it with the following two approaches over collected cooccurring word pairs within a window of size dt : (1) RS(t, t0 ) is computed as a bigram conditional probability plm (t0 |t) via the language model; (2) Following (Xiong et al., 2011) and (Liu et al., 2014), we employ PMI to define RS(t, t0 ) as p(t,t0 ) ln p(t)p(t 0) . 3 Collective Lexical Selection Algorithm Based on the translation graph, we propose a collective lexical selection algorithm to jointly identify translations of all source words in the graph. 3.1 Problem Statement and Solution Method As stated previously, the translation of a sourceside content word s should be: 1) associated with s; 2) related to the translations of other source-side content words. Thus, in the translation graph, the translation of s should be a target-side node which has: 1) an association edge with the node of"
D15-1145,D09-1022,0,0.0150969,"among different EL decisions. We successfully adapt this algorithm to lexical selection in SMT. Other related work mainly includes the following two strands. (1) Lexical selection in SMT. In order to capture source-side context for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers with rich context information to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Shen et al. (2009) introduce four new linguistic and contextual features for HPB system. We have also witnessed increasing efforts in the exploitation of documentlevel context information. Xiao et al. (2011) impose a hard constraint to guarantee the translation consistency in document-level translation. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Hardmeier et al. (2012, 2013) introduce a document-wide phrase-based decoder and integrate a semantic language model that cross sentence boundaries into the decoder. Based on topic models, Xiao et al. (2012"
D15-1145,J03-1002,0,0.00618241,"Missing"
D15-1145,P03-1021,0,0.0293498,"of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection in the HPB system; (2) topic similarity model (Xiao et al., 2012)"
D15-1145,P12-1079,1,0.818387,"rget translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctl"
D15-1145,P14-1137,1,0.830823,"Missing"
D15-1145,P11-1129,1,0.870636,"Missing"
D15-1145,D14-1021,0,0.0151054,"n the exploitation of global dependencies among target translations, which has attracted little attention before. Different from exploring source-side context, other researchers pay attention to the utilization of target-side context information. The common practice in SMT is to use an n-gram language model to capture local dependencies between translations (Koehn et al., 2003; Xiong et al., 2011). Yet another approach exploring targetside context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the bes"
D15-1145,D13-1050,0,0.0242003,"Missing"
D15-1145,P02-1040,0,0.0939119,"his, we converted each word into its corresponding lemma with the exception of adjectives and adverbs. In the procedure of collective lexical selection, the difference threshold  was set as 10−10 , and the maximal iteration number maxIter 100. We reimplemented the decoder of Hiero (Chiang, 2007), a famous hierarchical phrase-based (HPB) system. HPB system is a formally syntaxbased system and delivers good performance in various translation evaluations. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by caseinsensitive BLEU-4 metric (Papineni et al., 2002). To alleviate the impact of the instability of MERT (Och, 2003), we ran it three times for each experiment and reported the average BLEU scores as suggested in (Clark et al., 2011). Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Our Method vs Other Methods In the first group of experiments, we investigated the effectiveness of our model by comparing it against the baseline as well as two additional models: (1) lexicalized rule selection model (He et al., 2008) (LRSM), which employs local context to improve rule selection i"
D15-1145,P08-1066,0,0.228479,"source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each other, our baseline can only correctly translate “l`ıchˇang” as “stance”. It inappropriately translates the other two words as “problem” and null, respectively, even with the support of an n-gram language model. If we could model long-distance dependencies among target translations of source words “w`ent´ı”(issue), “ch´ıyˇou”(hold) and “l`ıc"
D15-1145,D09-1008,0,0.130528,"luding corresponding source items and their neighboring words, and 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemo"
D15-1145,D14-1003,0,0.0131094,"ide context information is proposed by Shen et al. (2008), who use a dependency language model to capture long-distance relations on the target side. Moreover, Zhang et al. (2014) treat translation as an unconstrained target sentence generation task, using soft features to capture lexical and syntactic correspondences between the source and target language. Recently, many researcher have proposed to use deep neural networks to model long-distance dependencies of arbitrary length for SMT (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Hu et al., 2014; Liu et al., 2014; Sundermeyer et al., 2014). Our work is significantly different from these methods. We use a graph representation to capture local and global context information, which, to the best of our knowledge, is the first attempt to explore graph-based representations for lexical selection. Furthermore, our model do not resort to any syntactic resources such as dependency parsers of the target language. (2) Random walk for SMT. Because of the advantage of global consistency, random walk al1245 gorithm has been applied in SMT. For example, Cui et al. (2013) develop an effective approach to optimize phrase scoring and corpus weig"
D15-1145,N12-1046,0,0.0463556,"Missing"
D15-1145,2011.mtsummit-papers.13,0,0.158413,"d 2) dependencies1 between selected target translations and other items on the target side. As translation rules and widely-used n-gram language models can only capture local associations and dependencies, we have witnessed increasing efforts that attempt to incorporate nonlocal associations/dependencies into lexical selection. Efforts using source-side associations mainly focus on the exploitation of either sentence-level context (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or the utilization of document-level context (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Xiong et al., 2013). In contrast, target-side dependencies attract little attention, although they have an important impact on the accuracy of lexical selection. The most common practice is to use language models to estimate the strength of target-side dependencies (Koehn et al., 2003; Shen et al., 2008; Xiong et al., 2011). However, conventional ngram language models are not good at capturing long-distance dependencies. Consider the example shown in Figure 1. As the translations of polysemous words “w`ent´ı”, “ch´ıyˇou” and “l`ıchˇang” are far from each"
D15-1145,J07-2003,0,\N,Missing
D15-1146,D14-1082,0,0.10557,"units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phr"
D15-1146,P14-1013,0,0.0880714,"ressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us"
D15-1146,P14-1129,0,0.0900992,"Missing"
D15-1146,P14-1066,0,0.0451391,"l “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase representations si"
D15-1146,D14-1176,0,0.224828,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P14-1006,0,0.0826745,"Missing"
D15-1146,D13-1176,0,0.0714049,"tures, Bengio et al. (2003) convert words to dense, real-valued vectors by learning probability distributions of n-grams. Mikolov et al. (2013) generate word vectors by predicting their limited context words. Instead of exploiting outside context information, recursive auto-encoder is usually adopted to learn the composition of internal words (Socher et al., 2010; Socher et al., 2011b; Socher et al., 2013b; Socher et al., 2013a). Recently, convolution architecture has drawn more and more attention due to its ability to explicitly capture short and long-range relations (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Kalchbrenner et al., 2014; Kim, 2014). (2) Bilingual Word/Phrase Embeddings. In the field of machine translation and cross-lingual information processing, bilingual embedding learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn b"
D15-1146,P14-1062,0,0.234774,"ation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and targ"
D15-1146,D14-1181,0,0.0346093,"ed from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases."
D15-1146,D13-1054,1,0.947567,"tations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal stru"
D15-1146,P13-1078,0,0.088091,"ic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more c"
D15-1146,P14-1140,0,0.190425,"on for x, we employ a greedy algorithm (Socher et al., 2011c) to minimize the sum of reconstruction error at each node in the binary tree T (x): X 1 Erec (x; θ) = k [c1 ; c2 ]n −[c01 ; c02 ]n k2 (3) 2 n∈T (x) where θ denotes model parameters and n represents a node in T (x). 2.2 BRAE BRAE jointly learns two RAEs for source and target phrase embeddings as shown in Figure 1(a). The core idea behind BRAE is that a source phrase and its target correct translation should share the same semantic representations, while non-equivalent pairs should have different semantic representations. Zhang et al. (2014) use this intuition to constrain semantic pharse embedding learning. As shown in Figure 2, in addition to the abovementioned reconstruction error, BRAE introduces a max-semantic-margin error to minimize the semantic distance between translation equivalents and maximize the semantic distance between non(5) where Esem (f |e, θ) is defined as the semantic distance between the learned vector representations of f and e, denoted by pf and pe , respectively. Since phrase embeddings for the source and target language are learned separately in different vec(3) tor spaces, a transformation matrix Wf ∈ R"
D15-1146,P02-1038,0,0.273486,"es a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combina"
D15-1146,J03-1002,0,0.00554671,"consistency encoded in bilingual phrase structure learning, which is the basis of our model. Then, we describe the objective function which is composed of three types of errors. Finally, we provide details on the training of our model. 3.1 Structural Alignment Consistency We adapt word alignment to structural alignment and introduce some related concepts. Given a bilingual phrase (f, e) with its binary tree structures (Tf , Te ), if the source node nf¯ ∈ Tf covers a source-side sub-phrase f¯, and there exists a target-side sub-phrase e¯ such that (f¯, e¯) are consistent with word alignments (Och and Ney, 2003), we say nf¯ satisfies the structural alignment consistency, and it is referred to as a structuralalignment-consistent (SAC) node. Further, if e¯ is covered by a target node ne¯ ∈ Te , we say ne¯ is the aligned node of nf¯. In this way, several different target nodes may be all aligned to the same source node because of null alignments. For this, we choose the target node with the smallest span as the aligned one for the considered source node. This is because a smaller span reflects a stronger semantic relevance in most situations. Likewise, we have similar definitions for target nodes. Note"
D15-1146,P03-1021,0,0.0252438,"he reordering model with the following parameters: iteration number iter=200 and gaussian prior g=1.0. Following Xiong et al. (2006), we use only boundary words of blocks to trigger the reordering model. The whole translation model is organized in a log-linear framework (Och and Ney, 2002). The adopted sub-models mainly include: (1) rule translation probabilities in two directions, (2) lexical weights in two directions, (3) targets-side word number, (4) phrase number, (5) language model score, and (6) the score of maximal entropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram"
D15-1146,P02-1040,0,0.0961193,"es in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 2"
D15-1146,W04-3250,0,0.191213,"reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Parameter α β γ λL λrec λcon λlcrec BRAE 0.119 4.95 ×10−5 2.64 ×10−7 9.31 ×10−5 BCorrRAE 0.121 0.6331 0.2459 3.13 ×10−5 2.05 ×10−5 7.32 ×10−6 5.25 ×10−6 Table 1: Hyper-parameters for BCorrRAE and BRAE model. Method BCorrRAESM BCorrRAEST d 25 50 75 100 25 50 75 100 MT06 30.81 30.58↓ 30.50 30.34"
D15-1146,D11-1014,0,0.360999,"Missing"
D15-1146,P14-2037,0,0.0449862,"Missing"
D15-1146,P13-1045,0,0.253422,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D13-1170,0,0.0484163,". However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual c"
D15-1146,D14-1003,0,0.014634,"f BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initia"
D15-1146,P14-1138,0,0.0135589,"e the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, man"
D15-1146,D14-1175,0,0.0183492,"learning has become an increasingly important study. The bilingual embedding research origins in the word embedding learning, upon which Zou et al. (2013) utilize word alignments to constrain translational equivalence. Koˇcisk´y et al. (2014) propose a probability model to capture more semantic information by marginalizing over word alignments. More specifically to SMT, its main components have been exploited to learn better bilingual phrase embeddings in different aspects: language models (Wang et al., 2014; Garmash and Monz, 2014), reordering models (Li et al., 2013) and translation models (Tran et al., 2014; Zhang et al., 2014). Instead of exploiting a single model, Liu et al. (2014) combine the recursive and recurrent neural network to incorporate the language and translation model. Different from the methods mentioned above, our model considers both the cross-language consistency of phrase structures and internal correspondence relations inside bilingual phrases. The most related works include Zhang et al. (2014) and Socher et al. (2011a). Compared with these works, our model exploits different levels of correspondence relations inside bilingual phrases instead of only the top level of entire"
D15-1146,D14-1023,0,0.0803451,"both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning rep"
D15-1146,D14-1015,0,0.0152033,"pecially the BCorrRAEST model, tends to choose shorter translations that are consistent with word alignments. 6 Related Work A variety of efforts have been devoted to learning vector representations for words/phrases with deep neural networks. According to the difference of learning contexts, previous work mainly include the following two strands. (1) Monolingual Word/Phrase Embeddings. The straightforward approach to represent word/phrases is to learn their hidden representations with traditional feature vectors, which requires manual and task-dependent feature engineering (Cui et al., 2014; Wu et al., 2014; 1255 Source Phrase 䌓㥎 (advocate) 惮䜃 坝揔 (serious challenge) 䋺㟙 䀛 嗪䛢 (data released) BRAE to advocate the in preaching the the promotion of as well as severe challenges a serious challenge to a serious challenge from by the figures published by the the statistics released by data published by the BCorrRAESM out to advocate been encouraging an advocate of rigorous challenges as well as severe challenges of severe challenges to the estimates announced at the figures published the statistics released by BCorrRAEST encouraging claimed advocate rigorous challenge enormous challenge severe challenge"
D15-1146,J97-3002,0,0.606435,"de n, and Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reor"
D15-1146,P10-1049,0,0.0513633,"ntropy based reordering model. We perform minimum error rate training (Och, 2003) to tune various feature weights. During decoding, we set ttablelimit=20 for translation candidates kept for each source phrase, stack-size=100 for hypotheses in each span, and swap-span=15 for the length of the maximal reordering span. 5.2 Setup Our bilingual data is the combination of the FBIS corpus and Hansards part of LDC2004T07 corpus, which contains 1.0M parallel sentences (25.2M Chinese words and 29M English words). Following Zhang et al. (2014), we collected 1.44M bilingual phrases using forced decoding (Wuebker et al., 2010) to train BCorrRAE from the training data. We used a 5-gram language model trained on the Xinhua portion of Gigaword corpus using SRILM Toolkits4 . Translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). We performed paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. In our experiments, we used NIST MT05 and MT06/MT08 data set as the 2 A block is a bilingual phrase without maximum length limitation. 3 http://homepages.inf.ed.ac.uk/lzhang10/maxent toolkit.html 4 http://www.speech.sri.com/projects/srilm/download.html Pa"
D15-1146,P06-1066,1,0.784584,"Cf and Ce count the number of nodes in the source and target tree structure respectively. Note that if we only compute the similarity for root nodes in the bilingual tree of (f, e), the structural similarity equals to the semantic similarity in Eq. (19). 5 Experiments We conducted experiments on NIST ChineseEnglish translation task to validate the effectiveness of BCorrRAE. 5.1 System Overview Our baseline decoder is a state-of-the-art phrasebased translation system equipped with a maximum entropy based reordering model (MEBTG). It adopts three bracketing transduction grammar rules (Wu, 1997; Xiong et al., 2006): merging 1253 rules A → [A1 , A2 ]|hA1 , A2 i which are used to merge two neighboring blocks2 A1 and A2 in a straight|inverted order, and lexical rule A → f /e used to translate a source phrase f into a target phrase e. The MEBTG system features a maximal entropy classifier based reordering model that predicts orientations of neighboring blocks. During training, we extract bilingual phrases containing up to 7 words on the source side from the training corpus. With the collected reordering examples, we adopt the maximal entropy toolkit3 developed by Zhang to train the reordering model with the"
D15-1146,P13-1017,0,0.0236294,"nt levels of semantic relations within bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer exp"
D15-1146,P14-1011,0,0.106705,"rucial for successful “deep” SMT. To address this issue, many efforts have been initiated on learning representations for bilingual phrases in the context of SMT, inspired by the success of work on monolingual phrase embeddings (Socher et al., 2010; Socher et al., 2011a; Socher et al., 2013b; Chen and Manning, 2014; Kalchbrenner et al., 2014; Kim, 2014). The learning process of bilingual phrase embeddings in these efforts is normally interacted and mingled with single or multiple essential components of SMT, e.g., with reordering models (Li et al., 2013), translation models (Cui et al., 2014; Zhang et al., 2014; Gao et al., 2014), or both language and translation models (Liu et al., 2014). In spite of their success, these approaches center around capturing relations between entire source and target phrases. They do not take into account internal phrase structures and bilingual correspondences of sub-phrases within source and target phrases. The neglect of these important clues may be due to the big challenge imposed by the integration of them into the learning process of bilingual phrase representations. However, we believe such internal structures and correspondences can help us learn better phrase"
D15-1146,D13-1141,0,0.299247,"n bilingual phrases. In order to examine the effectiveness of BCorrRAE, we incorporate both semantic and structural similarity features built on bilingual phrase representations and tree structures learned by BCorrRAE into a state-of-the-art SMT system. Experiments on NIST Chinese-English test sets show that our model achieves a substantial improvement of up to 1.55 BLEU points over the baseline. 1 Introduction Recently a variety of “deep architecture” approaches, including autoencoders, have been successfully used in statistical machine translation (SMT) (Yang et al., 2013; Liu et al., 2013; Zou et al., 2013; Devlin et al., 2014; Tamura et al., 2014; Sundermeyer et al., 2014; Wang et al., 2014; Koˇcisk´y et al., 2014). Typically, these approaches represent words as dense, low-dimensional and ∗ Corresponding author. real-valued vectors, i.e., word embeddings. However, translation units in machine translation have long since shifted from words to phrases (sequence of words), of which syntactic and semantic information cannot be adequately captured and represented by word embeddings. Therefore, learning compact vector representations for phrases or even longer expressions is more crucial for success"
D15-1266,C14-1160,0,0.0563749,", “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specificall"
D15-1266,P14-1065,0,0.0174588,"Missing"
D15-1266,D14-1181,0,0.0388993,"and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer on the top o"
D15-1266,prasad-etal-2008-penn,0,0.902888,"logical relationship of coherent text (e.g., T EMPORAL, C ONTIN GENCY , E XPANSION , etc). It provides important information to many other natural language processing systems, such as question answering (Verberne et al., 2007), information extraction (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern"
D15-1266,P13-1047,0,0.347474,"e the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is"
D15-1266,E14-1068,0,0.630863,"l.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neg"
D15-1266,D13-1054,0,0.0139689,"al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple convolution layer"
D15-1266,D14-1220,0,0.0166044,"for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite"
D15-1266,D09-1036,0,0.825769,"2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al.,"
D15-1266,W10-4310,0,0.306051,"anslation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al"
D15-1266,D13-1170,0,0.00350287,"et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep learning work specifically for implicit DRR. The neglect of this important domain may be due to the following two reasons: (1) discourse relation distribution is rather unbalanced, where the generalization of deep models is relatively insufficient despite their powerful studying ability; (2) training dataset in implicit DRR is relatively small, where overfitting problems become more prominent. In this paper, we propose a Shallow Convolutional Neural Network (SCNN) for implicit DRR, with only one simple c"
D15-1266,P13-2013,0,0.24879,"ress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our k"
D15-1266,W13-0123,0,0.0109976,"nnectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). However, to the best of our knowledge, there is little deep le"
D15-1266,C12-1168,0,0.189073,"Missing"
D15-1266,W12-1614,0,0.682233,"on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 2011b; Socher et al., 2011a; Socher et al., 2013; Li et al., 2013; Kim, 2014). Howe"
D15-1266,C08-2022,0,0.0204587,"scourse analysis, discourse relation recognition (DRR) aims to automatically identify the internal structure and logical relationship of coherent text (e.g., T EMPORAL, C ONTIN GENCY , E XPANSION , etc). It provides important information to many other natural language processing systems, such as question answering (Verberne et al., 2007), information extraction (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these"
D15-1266,P09-1077,0,0.869714,"tion (Cimiano et al., 2005), machine translation (Guzm´an et al., 2014) and so on. Despite great progress in explicit DRR where the discourse connectives (e.g., “because”, “but” et al.) explicitly exist in the text (Miltsakaki et al., 2005; Pitler et al., 2008), implicit DRR remains a serious challenge because of the absence of discourse connectives (Prasad et al., 2008). Conventional methods for implicit DRR directly rely on feature engineering, wherein researchers generally exploit various hand-crafted features, such as words, part-of-speech tags and ∗ Corresponding author production rules (Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Wang et al., 2012; Park and Cardie, 2012; McKeown and Biran, 2013; Lan et al., 2013; Versley, 2013; Braud and Denis, 2014; Rutherford and Xue, 2014). Although these methods have proven successful, these manual features are labor-intensive and weak to capture intentional, semantic and syntactic aspects that govern discourse coherence (Li et al., 2014), thus limiting the effectiveness of these methods. Recently, deep learning models have achieved remarkable results in natural language processing (Bengio et al., 2003; Bengio et al., 2006; Socher et al., 201"
D15-1266,miltsakaki-etal-2004-penn,0,\N,Missing
D16-1037,D15-1262,0,0.0464737,"hich we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015"
D16-1037,P15-2015,0,0.0197483,"(Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three as"
D16-1037,C14-1088,0,0.0210779,"3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models and propose a variational neural discourse"
D16-1037,Q15-1024,0,0.411507,"and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation"
D16-1037,N16-1037,0,0.0279266,"implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly, which can be summarized in the following three aspects: 1) they employ the recurrent neural network to represent the discourse arguments, while we use the simple feedforward neural network; 2) they treat the discourse relations directly as latent variables, rather than the underlying semantic representation of discourses; 3) their model is optimized in terms of the data likelihood, since the discourse relations are observed during training. However, VarNDRR is optimized under the variational theory. Variational Neural Model In the presence"
D16-1037,P13-1047,0,0.0602548,"ies (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from theirs significantly,"
D16-1037,D09-1036,0,0.36099,"sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this l"
D16-1037,W10-4310,0,0.0179631,"nition and variational neural model, which we describe in succession. Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud"
D16-1037,W12-1614,0,0.0150111,"Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capab"
D16-1037,D13-1094,0,0.0931866,"Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestig"
D16-1037,P09-1077,0,0.340564,"rization technique to sample z from qφ (z|x, y) that not only bridges the gap between the recognizer and the approximator but also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic"
D16-1037,prasad-etal-2008-penn,0,0.672371,"us making the setting z˜ = µ0 during testing reasonable. The second term is the approximate expectation of Eqφ (z|x,y) [log pθ (x, y|z)], which is also differentiable. As the objective function in Eq. (13) is differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 6"
D16-1037,E14-1068,0,0.0315926,"Due to the release of Penn Discourse Treebank (Prasad et al., 2008) corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, Pilter et al. (2009) exploit several linguistically informed features, such as polarity tags, modality and lexical features. Lin et al. (2009) further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have bee"
D16-1037,N15-1081,0,0.502665,"h02 = dm = dhy = 400, dy = 2 for all experiments.7 . All parameters of VarNDRR are initialized by a Gaussian distribution (µ = 0, σ = 0.01). For Adam, we set β1 = 0.9, β2 = 0.999 with a learning rate 0.001. Additionally, we tied the following parameters in practice: Wh1 and Wh2 , Wx01 and Wx02 . We compared VarNDRR against the following two different baseline methods: • SVM: a support vector machine (SVM) classifier8 trained with several manual features. • SCNN: a shallow convolutional neural network proposed by Zhang et al. (2015). We also provide results from two state-of-the-art systems: • Rutherford and Xue (2015) convert explicit discourse relations into implicit instances. • Ji and Eisenstein (2015) augment discourse representations via entity connections. 7 8 http://nlp.stanford.edu/software/corenlp.shtml 387 There is one dimension in dx1 and dx2 for unknown words. http://svmlight.joachims.org/ 45 40 35 30 25 20 15 10 5 0 80 70 60 50 40 30 20 10 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 117 218 19 13 420 521 622 723 824 25 9 26 10 27 11 28 1229 1330 1431 1532 1633 1734 1835 1936 2037 2138 1 39 22 2340 2441 2542 43 26 44 27 45 28 46 2947 3048 3149 3250 3351 3452 3553 3654 -1270.24 25.3012 -207.21 26.0"
D16-1037,C12-1168,0,0.0167503,"en exploited: entities (Louis et al., 2010), word embeddings (Braud and Denis, 2015), Brown cluster pairs and co-reference patterns (Rutherford and Xue, 2014). With these features, Park and Cardie (2012) perform feature set optimization for better feature combination. Different from feature engineering, predicting 389 discourse connectives can indirectly help the relation classification (Zhou et al., 2010; Patterson and Kehler, 2013). In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement (Wang et al., 2012; Lan et al., 2013; Braud and Denis, 2014; Fisher and Simmons, 2015; Rutherford and Xue, 2015). Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning (Ji and Eisenstein, 2015; Zhang et al., 2015). Despite their successes, most of them focus on the discriminative models, leaving the field of generative models for implicit DRR a relatively uninvestigated area. In this respect, the most related work to ours is the latent variable recurrent neural network recently proposed by Ji et al. (2016). However, our work differs from thei"
D16-1037,D14-1196,0,0.0259392,", Deyi Xiong2∗, Jinsong Su1 , Qun Liu3,4 , Rongrong Ji1 , Hong Duan1 , Min Zhang2 Xiamen University, Xiamen, China 3610051 Provincial Key Laboratory for Computer Information Processing Technology Soochow University, Suzhou, China 2150062 ADAPT Centre, School of Computing, Dublin City University3 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences4 zb@stu.xmu.edu.cn, {jssu, rrji, hduan}@xmu.edu.cn qun.liu@dcu.ie, {dyxiong, minzhang}@suda.edu.cn Abstract other relevant natural language processing tasks, such as text summarization (Yoshida et al., 2014), conversation (Higashinaka et al., 2014), question answering (Verberne et al., 2007) and information extraction (Cimiano et al., 2005). Generally, discourse relations can be divided into two categories: explicit and implicit, which can be illustrated in the following example: Implicit discourse relation recognition is a crucial component for automatic discourselevel analysis and nature language understanding. Previous studies exploit discriminative models that are built on either powerful manual features or deep discourse representations. In this paper, instead, we explore generative models a"
D16-1037,D15-1266,1,0.82891,"also allows us to use the standard stochastic gradient ascent techniques for optimization (see section 3.3). y N Figure 1: Graphical illustration for VarNDRR. Solid lines denote the generative model pθ (x|z)pθ (y|z), dashed lines denote the variational approximation qφ (z|x, y) to the posterior p(z|x, y) and qφ0 (z|x) to the prior p(z) for inference. The variational parameters φ are learned jointly with the generative model parameters θ. tures in SVM-based recognition (Pitler et al., 2009; Lin et al., 2009) or sentence embeddings in neural networks-based recognition (Ji and Eisenstein, 2015; Zhang et al., 2015)), and then directly model the conditional probability of the corresponding discourse relation y given x, i.e. p(y|x). In spite of their success, these discriminative approaches rely heavily on the goodness of discourse representation x. Sophisticated and good representations of a discourse, however, may make models suffer from overfitting as we have no large-scale balanced data. Instead, we assume that there is a latent continuous variable z from an underlying semantic space. It is this latent variable that generates both discourse arguments and the corresponding relation, i.e. p(x, y|z). The"
D16-1037,C10-2172,0,0.205752,"differentiable, we can optimize both the model parameters θ and variational parameters φ jointly using standard gradient ascent techniques. The training procedure for VarNDRR is summarized in Algorithm 1. 4 Experiments We conducted experiments on English implicit DRR task to validate the effectiveness of VarNDRR.4 4.1 Dataset We used the largest hand-annotated discourse corpus PDTB 2.05 (Prasad et al., 2008) (PDTB hereafter). This corpus contains discourse annotations over 2,312 Wall Street Journal articles, and is organized in different sections. Following previous work (Pitler et al., 2009; Zhou et al., 2010; Lan et 4 Source code is available https://github.com/DeepLearnXMU/VarNDRR. 5 http://www.seas.upenn.edu/ pdtb/ at Model R & X (2015) J & E (2015) SVM SCNN VarNDRR Acc 70.27 63.10 60.42 63.30 P 22.79 22.00 24.00 R 64.47 67.76 71.05 F1 41.00 35.93 33.68 33.22 35.88 Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR (a) C OM vs Other Model (R & X (2015)) (J & E (2015)) SVM SCNN VarNDRR Acc 69.80 60.71 63.00 57.36 P 65.89 56.29 56.46 Acc 76.95 62.62 63.00 53.82 P 39.14 39.80 35.39 R 72.40 75.29 88.53 F1 53.80 52.78 50.82 52.04 50.56 R 68.24 62.35 97.65 F1 33.30 27.63 24.73 30.54 29.54 (b) C ON"
D16-1050,D16-1025,0,0.0129473,"ts that gain 0.86 and 1.35 BLEU points over Moses and GroundHog respectively. Besides, without the KL objective, VNMT w/o KL obtains even worse results than GroundHog. These results indicate the following two points: 1) explicitly modeling underlying semantics by a latent variable indeed benefits neural machine translation, and 2) the improvements of our model are not from enlarging the network. https://github.com/DeepLearnXMU/VNMT. 527 Results on Long Sentences We further testify VNMT on long sentence translation where the vanilla NMT usually suffers from attention failures (Tu et al., 2016; Bentivogli et al., 2016). We believe that the global latent variable can play an important role on long sentence translation. Our first experiment is carried out on 6 disjoint groups according to the length of source sentences in our test sets. Figure 3 shows the BLEU scores of two neural models. We find that the performance curve of our VNMT model always appears to be on top of that of GroundHog with a certain margin. Specifically, on the final group with the longest source sentences, our VNMT obtains the biggest improvement (3.55 BLEU points). Overall, these obvious improvements on all groups in terms of the length"
D16-1050,P15-1001,0,0.416218,"t2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the default settings except for the language model. We trained a 4-gram language model on the Xinhua section of the English Gigaword corpus (306M words) using the SRILM7 toolkit with modified Kneser-Ney smoothing. Important"
D16-1050,D13-1176,0,0.0324504,"the vanilla neural machine translation baselines. 1 Introduction Neural machine translation (NMT) is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an impl"
D16-1050,P07-2045,0,0.0263654,"the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not contain the KL part in the objective (VNMT w/o KL). This is achieved by setting hz to µ0 . For Moses, we adopted all the defau"
D16-1050,W04-3250,0,0.0315104,"80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT system. an Additionally, we also compared with a variant of VNMT, which does not con"
D16-1050,P09-1067,0,0.0159698,"hat allows the iterative construction of complex images. Very recently, Miao et al. (2015) propose a generic variational inference framework for generative and conditional models of text. The most related work is that of Bowman et 529 al. (2015), where they develop a variational autoencoder for unsupervised generative language modeling. The major difference is that they focus on the monolingual language model, while we adapt this technique to bilingual translation. Although variational neural models have been widely used in NLP tasks and the variational decoding has been investigated for SMT (Li et al., 2009), the adaptation and utilization of variational neural model to neural machine translation, to the best of our knowledge, has never been investigated before. 6 Conclusion and Future Work In this paper, we have presented a variational model for neural machine translation that incorporates a continuous latent variable to model the underlying semantics of sentence pairs. We approximate the posterior distribution with neural networks and reparameterize the variational lower bound. This enables our model to be an end-to-end neural network that can be optimized through the stochastic gradient algori"
D16-1050,D15-1166,0,0.478478,") is an emerging translation paradigm that builds on a single and unified end-to-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau"
D16-1050,P15-1002,0,0.0600673,"Missing"
D16-1050,P02-1040,0,0.0970109,"n translation tasks. Our Chinese-English training data4 consists of 2.9M sentence pairs, with 80.9M Chinese words and 86.4M English words respectively. We used the NIST MT05 dataset as the development set, and the NIST MT02/03/04/06/08 datasets as the test sets for the Chinese-English task. Our English-German training data5 consists of 4.5M sentence pairs with 116M English words and 110M German words6 . We used the newstest2013 (3000 sentences) as the development set, and the newstest2014 (2737 sentences) as the test set for English-German translation. We employed the case-insensitive BLEU-4 (Papineni et al., 2002) metric to evaluate translation quality, and paired bootstrap sampling (Koehn, 2004) for significance test. We compared our model against two state-of-theart SMT and NMT systems: • Moses (Koehn et al., 2007): a phrase-based SMT system. 4 This corpus consists of LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News). 5 This corpus is from the WMT’14 training data (Jean et al., 2015; Luong et al., 2015a) 6 The preprocessed data can be found and downloaded from http://nlp.stanford.edu/projects/nmt/ 526 • GroundHog (Bahdanau et al., 2014): attention-based NMT"
D16-1050,P16-5005,0,0.196025,"-end neural network, instead of using a variety of sub-models tuned in a long training pipeline. It requires a much smaller memory than ∗ Corresponding author phrase- or syntax-based statistical machine translation (SMT) that typically has a huge phrase/rule table. Due to these advantages over traditional SMT system, NMT has recently attracted growing interests from both deep learning and machine translation community (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015a; Luong et al., 2015b; Shen et al., 2015; Meng et al., 2015; Tu et al., 2016). Current NMT models mainly take a discriminative encoder-decoder framework, where a neural encoder transforms source sentence x into distributed representations, and a neural decoder generates the corresponding target sentence y according to these representations1 (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014). Typically, the underlying semantic representations of source and target sentences are learned in an implicit way in this framework, which heavily relies on the attention mechanism (Bahdanau et al., 2014) to identify semantic alignments between source and target words"
D16-1050,D07-1091,0,\N,Missing
D16-1253,P13-2013,0,0.108149,"e experiment. Both exp and conn can be considered as tasks on SynData. The results in Table 5 show that M T N incorporating BiSynData (Line 3) performs better than using SynData (Line 1 and 2), for the task on the CDTB. 1 2 3 System M T Nexp M T Nconn M T Nbi macro F1 56.42 56.86 58.28 Table 5: M T N with different auxiliary tasks on the CDTB. 5 Related Work One line of research related to DRRimp tries to take advantage of explicit discourse data. Zhou et al. (2010) predict the absent connectives based on a language model. Using these predicted connectives as features is proven to be helpful. Biran and McKeown (2013) aggregate word-pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem. More recently, Braud and Denis (2014) and Ji et al. (2015) consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them. Rutherford and Xue (2015) propose to gather weakly labeled data from explicit instances via connective classification, which are 2310 used as additional training data directly. Lan et al. (2013) and Liu et al. (2016) combine explicit and implicit data using multi-task learning models and"
D16-1253,C14-1160,0,0.3142,"ion Science and Technology, Xiamen University, China2 School of Software, Xiamen University, China3 {wcxnlp, huangyanzhou163}@163.com {mandel, ydchen, jssu}@xmu.edu.cn Abstract thetic implicit data (hereafter SynData). However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data. They also show that using SynData directly degrades the performance. Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results. To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chin"
D16-1253,D15-1262,0,0.0801079,"urse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [society reckons the existence of youth problems, ]Arg1 but [many young people do not think there is anything wron"
D16-1253,P15-1162,0,0.0201026,"Missing"
D16-1253,Q15-1024,0,0.156519,"ial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [society reckons the existence of youth problems, ]Arg1 but [many young people do not think there is anything wrong with them.]Arg2 Figure 1: An example illustr"
D16-1253,D15-1264,0,0.0773091,"Missing"
D16-1253,P13-1047,0,0.463043,"nzhou163}@163.com {mandel, ydchen, jssu}@xmu.edu.cn Abstract thetic implicit data (hereafter SynData). However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data. They also show that using SynData directly degrades the performance. Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results. To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to under"
D16-1253,C14-1055,0,0.0185598,"14294 2580 1951 1521 1122 Conn. while bef ore also since because Freq. 1031 822 552 511 503 Table 1: Top 10 most frequent connectives in our BiSynData. In our experiments, we extract our BiSynData from a combined corpus (FBIS and HongKong Law), with about 2.38 million Chinese-English sentence pairs. We generate 30,032 synthetic English instances and the same number of Chinese instances, with 80 connectives, as our BiSynData. Table 1 lists the top 10 most frequent connectives in our BiSynData, which are roughly consistent with the statistics of Chinese/English implicit/explicit mismatches in (Li et al., 2014a). According to connectives and their related relations in the PDTB, in most cases, and and also indicate the Expansion relation, if and because the Contigency relation, bef ore the T emporal relation, and but the Comparison relation. Connectives as, when, while and since are ambiguous. For example, while can indicate the Comparison or T emporal relation. Overall, our constructed BiSynData covers all four main discourse relations defined in the PDTB. With our BiSynData, we define two connective classification tasks: 1) given (Arg1en , Arg2en ) to predict the connective Connen , and 2) given ("
D16-1253,D14-1224,0,0.0174862,"14294 2580 1951 1521 1122 Conn. while bef ore also since because Freq. 1031 822 552 511 503 Table 1: Top 10 most frequent connectives in our BiSynData. In our experiments, we extract our BiSynData from a combined corpus (FBIS and HongKong Law), with about 2.38 million Chinese-English sentence pairs. We generate 30,032 synthetic English instances and the same number of Chinese instances, with 80 connectives, as our BiSynData. Table 1 lists the top 10 most frequent connectives in our BiSynData, which are roughly consistent with the statistics of Chinese/English implicit/explicit mismatches in (Li et al., 2014a). According to connectives and their related relations in the PDTB, in most cases, and and also indicate the Expansion relation, if and because the Contigency relation, bef ore the T emporal relation, and but the Comparison relation. Connectives as, when, while and since are ambiguous. For example, while can indicate the Comparison or T emporal relation. Overall, our constructed BiSynData covers all four main discourse relations defined in the PDTB. With our BiSynData, we define two connective classification tasks: 1) given (Arg1en , Arg2en ) to predict the connective Connen , and 2) given ("
D16-1253,D09-1036,0,0.212169,"icant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no"
D16-1253,N15-1092,0,0.0132983,"ta from explicit instances via connective classification, which are 2310 used as additional training data directly. Lan et al. (2013) and Liu et al. (2016) combine explicit and implicit data using multi-task learning models and gain improvements. Different from all the above work, we construct additional training data from a bilingual corpus. Multi-task neural networks have been successfully used for many NLP tasks. For example, Collobert et al. (2011) jointly train models for the Partof-Speech tagging, chunking, named entity recognition and semantic role labeling using convolutional network. Liu et al. (2015) successfully combine the tasks of query classification and ranking for web search using a deep multi-task neural network. Luong et al. (2016) explore multi-task sequence to sequence learning for constituency parsing, image caption generation and machine translation. 6 Conclusion In this paper, we introduce bilingually-constrained synthetic implicit data (BiSynData), which are generated based on the bilingual implicit/explicit mismatch, into implicit discourse relation recognition for the first time. On both the PDTB and CDTB, using BiSynData as the auxiliary task significantly improves the pe"
D16-1253,W10-4310,0,0.187314,"over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [soci"
D16-1253,P14-5010,0,0.00490193,"Missing"
D16-1253,P02-1047,0,0.18737,"tream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [society reckons the existence of youth problems, ]Arg1 but [many young people do not think there is anything wrong with them.]Arg2 Figure 1: An example illustrating the implicit/explicit mismatch between Chinese (ch) and English (en). A Chinese i"
D16-1253,P09-1077,0,0.163011,"model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people t"
D16-1253,prasad-etal-2008-penn,0,0.565972,"Missing"
D16-1253,E14-1068,0,0.119435,"both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [society reckons the existence o"
D16-1253,N15-1081,0,0.459737,"ftware, Xiamen University, China3 {wcxnlp, huangyanzhou163}@163.com {mandel, ydchen, jssu}@xmu.edu.cn Abstract thetic implicit data (hereafter SynData). However, Sporleder and Lascarides (2008) argue that SynData has two drawbacks: 1) meaning shifts in some cases when removing connectives, and 2) a different word distribution with the real implicit data. They also show that using SynData directly degrades the performance. Recent work seeks to derive valuable information from SynData while filtering noise, via domain adaptation (Braud and Denis, 2014; Ji et al., 2015), classifying connectives (Rutherford and Xue, 2015) or multi-task learning (Lan et al., 2013; Liu et al., 2016), and shows promising results. To alleviate the shortage of labeled data, we propose to use bilingually-constrained synthetic implicit data for implicit discourse relation recognition. These data are extracted from a bilingual sentence-aligned corpus according to the implicit/explicit mismatch between different languages. Incorporating these data via a multi-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation"
D16-1253,N03-1030,0,0.0451119,"ulti-task neural network model achieves significant improvements over baselines, on both the English PDTB and Chinese CDTB data sets. 1 Introduction Discovering the discourse relation between two sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 bu"
D16-1253,D15-1266,1,0.750969,"wo sentences is crucial to understanding the meaning of a coherent text, and also beneficial to many downstream NLP applications, such as question answering and machine translation. Implicit discourse relation recognition (DRRimp ) remains a challenging task due to the absence of strong surface clues like discourse connectives (e.g. but). Most work resorts to large amounts of manually designed features (Soricut and Marcu, 2003; Pitler et al., 2009; Lin et al., 2009; Louis et al., 2010; Rutherford and Xue, 2014), or distributed features learned via neural network models (Braud and Denis, 2015; Zhang et al., 2015; Ji and Eisenstein, 2015). The above methods usually suffer from limited labeled data. Marcu and Echihabi (2002) attempt to create labeled implicit data automatically by removing connectives from explicit instances, as additional training data. These data are usually called as syn∗ Corresponding author. ch: [社会 认为 有 青少年 问题,]Arg1 society reckon existence youth problems, implicit=但是 [很多 青少年 认为 自己 没 问题.]Arg2 but many young people think themselves no problems. en: [society reckons the existence of youth problems, ]Arg1 but [many young people do not think there is anything wrong with them.]Arg2 Fi"
D16-1253,P12-1008,0,0.0606114,"Missing"
D16-1253,C10-2172,0,0.239835,"t of the Chinese Gigaword corpus. We collect explicit instances with the top 20 most frequent Chinese connectives and sample 20,000 instances for the experiment. Both exp and conn can be considered as tasks on SynData. The results in Table 5 show that M T N incorporating BiSynData (Line 3) performs better than using SynData (Line 1 and 2), for the task on the CDTB. 1 2 3 System M T Nexp M T Nconn M T Nbi macro F1 56.42 56.86 58.28 Table 5: M T N with different auxiliary tasks on the CDTB. 5 Related Work One line of research related to DRRimp tries to take advantage of explicit discourse data. Zhou et al. (2010) predict the absent connectives based on a language model. Using these predicted connectives as features is proven to be helpful. Biran and McKeown (2013) aggregate word-pair features that are collected around the same connectives, which can effectively alleviate the feature sparsity problem. More recently, Braud and Denis (2014) and Ji et al. (2015) consider explicit data from a different domain, and use domain adaptation methods to explore the effect of them. Rutherford and Xue (2015) propose to gather weakly labeled data from explicit instances via connective classification, which are 2310"
D18-1041,D15-1041,0,0.0260727,"ared contexts into annotations. However, by contrast, we introduce domain classifier and adversarial domain classifier simultaneously to distinguish different kinds of contexts for NMT more explicitly. Here we describe only the modeling procedure of the domain classifier, while it is also applicable to the adversarial domain classifier. Specifically, Er (x) is defined as follows: Er (x) = N X α i hi , (3) i=1 exp(ei ) where αi = PN , i′ exp(ei′ ) ei = (va )⊤ tanh(Wa hi ), and va and Wa are the relevant attention parameters. Then, we feed Er (x) into a fully connected layer with ReLU function (Ballesteros et al., 2015), and then pass its output through a softmax layer to implement domain classification P where H(p(·))=− K k=1 pk (·) log pk (·) is an entropy of distribution p(·) with K domain labels, s1 and θ s2 denote the parameters of softmax θadc adc layer and the generation layer of Es (x) in this classifier, respectively. By this means, Er (x) and Es (x) are expected to encode the domain-specific and domain-shared semantic representations of x, respectively. It should be noted that our utilization of domain classifiers is similar to adversarial training used in (Pryzant et al., 2017) which injects s p(·"
D18-1041,W17-3205,0,0.383123,"gs of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447–457 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics target words will be assigned greater weights than others in the objective function of our model. Our work demonstrates the benefits of separate modeling of the domain-specific and domainshared contexts, which echoes with the successful applications of the multi-task learning based on shared-private architecture in many tasks, such as discourse relation recognition (Liu et al., 2017b), word segmentation (Chen et al., 2017b), text classification (Liu et al., 2017a), and image classification (Liu et al., 2016). Overall, the main contributions of our work are summarized as follows: ent roles in multi-domain NMT, nevertheless, they are not being distinguished by the current models. Take the sentence shown in Figure 1 for example. The Chinese words “‘Œ¬”(congress), “Æ Y”(bills), “ ”(inclusion), and “Æ§”(agenda) are frequently used in Laws domain and imply the Laws style of the sentence, while other words in this sentence are common in all domains and they mainly indicate the semantic meaning of the sentence. Thus,"
D18-1041,2016.amta-researchers.10,0,0.0830563,"riminative mixing that jointly models NMT with domain classification, adversarial discriminative mixing, and target token mixing which appends a domain token to the target sequence. Sajjad et al. (2017) explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploiting domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when implementing multi-domain NMT. Inspired by topicbased SMT, some researchers resorted to incorporating topical contexts into NMT. Chen et al. (2016) used the topic information of input sentence as an additional input to decoder. Zhang et al. (2016) enhanced the word representation by adding its topic embedding. However, these methods require to have explicit document boundaries between training data, which unfortunately do not exist in most datasets. Overall, our work is related to the second type of approach with (Pryzant et al., 2017) and (Chen et al., 2017a) most related to ours. Unlike (Pryzant et al., 2017) applying adversarial training to only capture domain-shared translation knowledge, we further exploit domain-specific translatio"
D18-1041,P17-1110,0,0.0998462,"Missing"
D18-1041,2015.iwslt-evaluation.11,0,0.162787,"Missing"
D18-1041,P17-2061,0,0.292514,"tence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,D13-1176,0,0.110763,"oblem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT. On the one hand, training a NMT model for a spe∗ Corresponding author 447 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 447–457 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics target words will be assigned greater weights than others in the objective function of our model. Our work demonstrates the benefits of separate modeling of the domain-specific and domainshared c"
D18-1041,2012.amta-papers.4,0,0.0646248,"Missing"
D18-1041,P02-1040,0,0.101761,"nsure encoding accuracy of domain-shared contexts, we follow Chen et al. (2017b) to adopt an alternative two-phase strategy in training, where we alternatively optimize s1 and {θ-θ s1 } respectively J (D; θ) with θadc adc fixed at a time. 3 Domain Train Dev Test Laws Spoken Thesis News Medical News Parliamentary 219K 219K 299K 300K 1.09M 180K 2.04M 600 600 800 800 800 800 800 456 455 625 650 2000 2000 2000 Table 1: Sentence numbers of data sets in our experiments. Encoding (Sennrich et al., 2016) to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU (Papineni et al., 2002). Contrast Models. Since our model is essentially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-single (Bahdanau et al., 2015). A reimplemented attentional NMT trained on a single domain dataset. 3.1 Setup Datasets. Fo"
D18-1041,P15-1166,0,0.0245519,"tation on Chinese sentences using Stanford Segmenter3 , and tokenized English and French sentences using MOSES script4 . Then, we employed Byte Pair • DL4NMT-mix (Bahdanau et al., 2015). A reimplemented attentional NMT trained on mix-domain training set. • DL4NMT-finetune (Luong and Manning, 2015). A reimplemented attentional NMT which is first trained using out-of-domain training corpus and then fine-tuned using indomain dataset. • +Domain Control (+DC) (Kobus et al., 2016). It directly introduces embeddings of source domain tag to enrich annotations of encoder. • +Multitask Learning (+ML1) (Dong et al., 2015). It adopts a multi-task learning framework that shares encoder representation and separates the decoder modeling of different domains. • +Multitask Learning (+ML2) (Pryzant et al., 2017). This model jointly trains 1 https://www.ldc.upenn.edu/. http://opus.nlpl.eu/ 3 https://nlp.stanford.edu/ 4 http://www.statmt.org/moses/ 2 5 451 http://opennmt.net/. Model NMT with domain classification via multitask learning. Laws Spoken Contrast Models (1×hd) OpenNMT 45.82 9.15 DL4NMT-single 43.66 5.49 46.82 8.95 DL4NMT-mix DL4NMT-finetune 54.19 8.77 +DC 49.83 9.18 +ML1 46.82 6.66 +ML2 48.95 9.45 +ADM 48.30"
D18-1041,W17-4712,0,0.714484,"eral to different domains. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT"
D18-1041,W17-4713,0,0.183833,"ins. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practical applications of NMT. On the one hand, train"
D18-1041,E17-2045,0,0.0883275,"source sequence. Similarly, Johnson et al. (2016) added an artificial token to the input sequence to indicate the required target language. Contrastingly, Farajian et al. (2017) utilized the similarity between each test sentence and the training instances to dynamically set the hyper-parameters of the learning algorithm and update the generic model on the fly. Pryzant et al. (2017) proposed three novel models: discriminative mixing that jointly models NMT with domain classification, adversarial discriminative mixing, and target token mixing which appends a domain token to the target sequence. Sajjad et al. (2017) explored data concatenation, model stacking, data selection and multi-model ensemble to train multi-domain NMT. By exploiting domain as a tag or a feature, Tars and Fishel (2018) treated text domains as distinct languages in order to use multi-lingual approaches when implementing multi-domain NMT. Inspired by topicbased SMT, some researchers resorted to incorporating topical contexts into NMT. Chen et al. (2016) used the topic information of input sentence as an additional input to decoder. Zhang et al. (2016) enhanced the word representation by adding its topic embedding. However, these meth"
D18-1041,P16-1162,0,0.0958223,"θ s1 , θ s2 }, and λ is tively, θ={θnmt , θdc dc adc adc the hyper-parameter for adversarial learning. Particularly, to ensure encoding accuracy of domain-shared contexts, we follow Chen et al. (2017b) to adopt an alternative two-phase strategy in training, where we alternatively optimize s1 and {θ-θ s1 } respectively J (D; θ) with θadc adc fixed at a time. 3 Domain Train Dev Test Laws Spoken Thesis News Medical News Parliamentary 219K 219K 299K 300K 1.09M 180K 2.04M 600 600 800 800 800 800 800 456 455 625 650 2000 2000 2000 Table 1: Sentence numbers of data sets in our experiments. Encoding (Sennrich et al., 2016) to convert all words into subwords. The translation quality was evaluated by case-sensitive BLEU (Papineni et al., 2002). Contrast Models. Since our model is essentially a standard attentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-s"
D18-1041,P13-1082,0,0.10203,"Missing"
D18-1041,2015.mtsummit-papers.19,0,0.0776266,"Missing"
D18-1041,N16-1082,0,0.0306063,"re 4 (b), we observe that the sentence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Ta"
D18-1041,tian-etal-2014-um,0,0.109532,"ttentional NMT model enhanced with word-level domain contexts, we refer to it as +WDC. We compared it with the following models, namely: Experiment • OpenNMT5 . A famous open-source NMT system used widely in the NMT community trained on mix-domain training set. To investigate the effectiveness of our model, we conducted multi-domain translation experiments on Chinese-English and English-French datasets. • DL4NMT-single (Bahdanau et al., 2015). A reimplemented attentional NMT trained on a single domain dataset. 3.1 Setup Datasets. For Chinese-English translation, our data comes from UM-Corpus (Tian et al., 2014) and LDC1 . To ensure data quality, we chose only the parallel sentences with domain label Laws, Spoken, and Thesis from UM-Corpus, and the LDC bilingual sentences related to News domain as our dataset. We used randomly selected sentences from UM-Corpus and LDC as development set, and combined the test set of UM-Corpus and randomly selected sentences from LDC to construct our test set. For English-French translation, we conducted experiments on the datasets of OPUS corpus2 , containing sentence pairs from Medical, News, and Parliamentary domains. We also divided these datasets into training, d"
D18-1041,P17-1001,0,0.376499,"tence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,P17-2089,0,0.0641518,"Missing"
D18-1041,D17-1155,0,0.0716763,"Missing"
D18-1041,C16-1170,0,0.0934532,"ring a NMT model general to different domains. Since the textual styles, sentence structures and terminologies in different domains are often remarkably distinctive, whether such domainspecific translation knowledge is effectively preserved could have a direct effect on the performance of the NMT model. Therefore, how to simultaneously exploit the exclusive and shared translation knowledge of mixed-domain parallel sentences for multi-domain NMT remains a challenging task. To tackle this problem, recently, researchers have carried out many constructive and in-depth studies (Kobus et al., 2016; Zhang et al., 2016; Pryzant et al., 2017; Farajian et al., 2017). However, most of these studies mainly focus on the utilization of domain contexts as a whole in NMT, while ignoring the discrimination of domain contexts at finer-grained level. In each sentence, some words are closely associated with its domain, while others are domain-independent. Intuitively, these two kinds of words play differIntroduction In recent years, neural machine translation (NMT) has achieved great advancement (Nal and Phil, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, two difficulties are encountered in the practic"
D18-1041,P17-1101,0,0.0202202,"that the sentence representation vectors of Laws domain does not completely coincide with 3.3 Experimental Analysis Furthermore, we conducted several visualization experiments to empirically analyze the individual effectiveness of the added model components. 3.3.1 Visualizations of Gating Vectors We first visualized the gating vectors gir and gis to quantify their effects on extracting domainspecific and domain-shared contexts from initial source-side annotations. Since both gir and gis are high dimension vectors, which are difficult to be visualized directly, we followed Li et al. (2016) and Zhou et al. (2017) to visualize their individual contributions to the final output, which can be 453 Domain Laws Spoken Thesis News Table 3: Top10 Target Words Article, Chapter, Principles, regulations, Provisions, Political, Servants, specify, China, Municipal meanly, Rusty, 1910s, scours, mountaintops, paralyze, Puff, perpetrators, hitter, weightlifting aggregation, Activities, Computation, Alzheimer, nn, Contemporarily, EVALUATION, ethoxycarbonyl, sCRC, Announced months, agency, outweighed, unconstitutionally, Congolese, session, Asia, news, hurts, francs Examples of Domain-Specific Target Words. those of th"
D18-1041,D16-1163,0,0.0298909,"WDC(T) 81.51 33.76 +WDC 83.35 34.17 News 30.22 29.56 31.62 34.04 33.94 31.92 33.48 33.43 33.37 30.23 33.28 34.20 33.52 31.90 33.62 33.34 33.68 34.31 33.78 34.87 Table 4: Overall Evaluation on the English-French translation task. 2015; Sennrich et al., 2013). As for NMT, the dominant strategies for domain adaptation generally fall into two categories: The first category is to transfer out-of-domain knowledge to in-domain translation. The conventional method is fine-tuning, which first trains the model on out-of-domain dataset and then finetunes it on in-domain dataset (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016). Freitag and Al-Onaizan (2016) proceeded further by ensembling the fine-tuned model with the original one. Chu et al. (2017) fine-tuned the model using the mix of in-domain and out-of-domain training corpora. From the perspective of data selection, Chen et al. (2017a) scaled the top-level costs of NMT system according to each training sentence’s similarity to the development set. Wang et al. (2017a) explored the data selection strategy based on sentence embeddings for NMT domain adaptation. Moreover, Wang et al. (2017b) further proposed several sentence and domain weight"
D18-1459,D15-1075,0,0.0398847,"Missing"
D18-1459,P16-1139,0,0.0297346,"Missing"
D18-1459,buck-etal-2014-n,0,0.0330415,"Missing"
D18-1459,D15-1141,0,0.0565569,"Missing"
D18-1459,D16-1053,0,0.0417364,"Missing"
D18-1459,P17-1012,0,0.333363,"hitectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture for LSTM,t GRU the memory cell specific the LSTM ∗ Operation Transfer Layer 1- × × σnetwork. x∗ and h∗ denote the input and output hidden states respectively. σ σ tanh σ+ x tectures as alternatives to RNNs for neural translax tion. With carefu"
D18-1459,P15-1001,0,0.508996,"tion mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidirectional recurrent"
D18-1459,D15-1166,0,0.689307,"ns. As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based encoder-decoder system, which employs a bidirectional recurrent network as its encoder and a two-layer hierarchical unidi"
D18-1459,P15-1002,0,0.0597018,"Missing"
D18-1459,P02-1040,0,0.106518,"17). It can be considered as a forward unnormalized selfattention where each hidden state attends to all its previous positions. As the self-attention mechanism has proved very useful in NMT (Vaswani et al., 2017), we conjecture that such property of ATR partially contributes to its success in machine translation as shown in our experiments. We visualize the dependencies captured by Equation (10) in Section 5.3. 4 4.1 Experiments Setup We conducted our main experiments on WMT14 English-German and English-French translation tasks. Translation quality is measured by casesensitive BLEU-4 metric (Papineni et al., 2002). Details about each dataset are as follows: English-German To compare with previous reported results (Luong et al., 2015b; Jean et al., 2015; Zhou et al., 2016; Wang et al., 2017a), we used the same training data of WMT 2014, which consist of 4.5M sentence pairs. We used the newstest2013 as our dev set, and the newstest2014 as our test set. English-French We used the WMT 2014 training data. This corpora contain 12M selected sentence pairs. We used the concatenation of newstest2012 and newstest2013 as our dev set, and the newstest2014 as our test set. The used NMT system is an attention-based"
D18-1459,P14-1028,0,0.0705337,"Missing"
D18-1459,D14-1162,0,0.0819338,"Missing"
D18-1459,P16-1162,0,0.130298,"two-layer hierarchical unidirectional recurrent network as its decoder, companied with an additive attention mechanism (Bahdanau et al., 2015). We replaced the recurrent unit with our proposed ATR model. More details are given in Appendix A.1. We also conducted experiments on ChineseEnglish translation, natural language inference and Chinese word segmentation. Details and experiment results are provided in Appendix A.2. 4.2 Training We set the maximum length of training instances to 80 words for both English-German and EnglishFrench task. We used the byte pair encoding compression algorithm (Sennrich et al., 2016) to reduce the vocabulary size as well as to deal with the issue of rich morphology. We set the vocabulary size of both source and target languages to 40K for all translation tasks. All out-of-vocabulary words were replaced with a token “unk”. We used 1000 hidden units for both encoder and decoder. All word embeddings had dimensionality 620. We initialized all model parameters randomly according to a uniform distribution ranging from -0.08 to 0.08. These tunable parameters were then optimized using Adam algorithm (Kingma and Ba, 2015) with the two momentum parameters set to 0.9 and 0.999 respe"
D18-1459,P16-1159,0,0.0463158,"Missing"
D18-1459,W03-1719,0,0.0550863,"Missing"
D18-1459,P17-1013,0,0.0769732,"their RNN model can be trained as fast as CNNs. However, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture for LSTM,t GRU the memory cell specific the LSTM ∗ Operation Transfer Layer 1- × × σnetwork. x∗ and h∗ denote the input"
D18-1459,N16-1170,0,0.0487476,"Missing"
D18-1459,P18-1166,1,0.774222,"K Wang et al. (2017a) GRU with 4 layers + LAU + PosUnk 80K Wang et al. (2017a) GRU with 4 layers + LAU + PosUnk + ensemble 80K Wu et al. (2016) LSTM with 8 layers + RL-refined WPM 32K Wu et al. (2016) LSTM with 8 layers + RL-refined ensemble 80K Transformer with 6 layers + base model 37K Vaswani et al. (2017) Comparable NMT systems (the same tokenization) Luong et al. (2015a) LSTM with 4 layers + local att. + unk replace 50K Zhang et al. (2017a) GRU with gated attention + BPE 40K Gehring et al. (2017b) CNN with 15 layers + BPE 40K CNN with 15 layers + BPE + ensemble 40K Gehring et al. (2017b) Zhang et al. (2018a) Transformer with 6 layers + aan + base model 32K Our end-to-end NMT systems RNNSearch + GRU + BPE 40K RNNSearch + LSTM + BPE 40K RNNSearch + RAN + BPE 40K this work RNNSearch + ATR + BPE 40K RNNSearch + ATR + CA + BPE 40K GNMT + ATR + BPE 40K RNNSearch + ATR + CA + BPE + ensemble 40K tok BLEU - detok BLEU 20.70 20.60 20.70 23.32 23.80 26.10 24.61 26.30 27.30 - 20.90 23.84 25.16 26.43 26.31 - 22.54 22.96 22.14 22.48 23.31 24.16 24.97 22.06 22.39 21.40 21.99 22.70 23.59 24.33 Table 2: Tokenized (tok) and detokenized (detok) case-sensitive BLEU scores on the WMT14 EnglishGerman translation tas"
D18-1459,P17-1140,0,0.0198865,"16) respectively. “aan” denotes the average attention network proposed by Zhang et al. (2018a). of word embedding and hidden state which we set to be 512. 4.3 Results on English-German Translation The translation results are shown in Table 2. We also provide results of several existing systems that are trained with comparable experimental settings to ours. In particular, our single model yields a detokenized BLEU score of 21.99. In order to show that the proposed model can be orthogonal to previous methods that improve LSTM/GRU-based NMT, we integrate a singlelayer context-aware (CA) encoder (Zhang et al., 2017b) into our system. The ATR+CA system further reaches 22.7 BLEU, outperforming the winner system (Buck et al., 2014) by a substantial improvement of 2 BLEU points. Enhanced with the deep GNMT architecture, the GNMT+ATR system yields a gain of 0.89 BLEU points over the RNNSearch+ATR+CA and 1.6 BLEU points over the RNNSearch + ATR. Notice that different from our system which was trained on the parallel corpus alone, the winner system used a huge monolingual text to enhance its language model. Compared with the existing LSTM-based (Luong et al., 2015a) deep NMT system, our shallow/deep model achi"
D18-1459,D13-1061,0,0.0655976,"Missing"
D18-1459,Q18-1011,0,0.0310456,"Missing"
D18-1459,Q16-1027,0,0.246768,"ove parallelism. Very recently, Lei and Zhang (2017) propose a simple recurrent unit (SRU). With the cuDNN optimization, their RNN model can be trained as fast as CNNs. However, to obtain promising results, QRNN and SRU have to use deep architectures. In practice, 4-layer QRNN encoder and decoder are used to gain translation quality that is comparable to that of singlelayer LSTM/GRU NMT. In particular, our onelayer model achieves significantly higher performance than a 10-layer SRU system. Finally, our work is also related to the efforts in developing alternative architectures for NMT models. Zhou et al. (2016) introduce fast-forward connections between adjacent stacked RNN layers to ease gradient propagation. Wang et al. (2017a) propose a linear associate unit to reduce the gradient propagation length along layers in deep NMT. Gehring et al. (2017b) and Vaswani et al. (2017) explore purely convolutional and attentional archi4274 σ σ tanh σ σ ht−1 × σ ct + × ht−1 tanh σ × tanh × σ ht−1 tanh xt xt ct−1 σ ht × + 1- σ ht × ht−1 × σ xt × (b) GRU + × xt (a) LSTM h ht σσ+ tanh ht xt + (c) ATR h ht−1 and ATR. c× indicates + ht Neural Network Pointwise Vector to Concatenate Copy Figure t−1 1: Architecture f"
D19-1020,D17-1209,0,0.142049,"endency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary informati"
D19-1020,P18-1026,0,0.0260887,"nce. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative i"
D19-1020,H05-1091,0,0.867277,"the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (a) nmod comp comp nmod comp amod comp ... observed ... interaction of orexin receptor antagonist almorexant (b) Figure 1: (a) 1-best dependency tree and (b) dependency forest for a medical-domain sentence, where edge label “comp” represents “compound”. Associated mentions are in different colors. Some irrelevant words and edges are omitted for simplicity. Previous work has shown that dependency syntax is important for guiding relation extraction (Culotta and Sorensen, 2004; Bunescu and Mooney, 2005; Liu et al., 2015; Gormley et al., 2015; Xu et al., 2015a,b; Miwa and Bansal, 2016; Zhang et al., 2018b), especially in biological and medical domains (Quirk and Poon, 2017; Peng et al., 2017; Song et al., 2018b). Compared with sequential surface-level structures, such as POS tags, dependency trees help to model word-toword relations more easily by drawing direct connections between distant words that are syntactically correlated. Take the phrase “effect on the medicine” for example; “effect” and “medicine” are directly connected in a dependency tree, regardless of how many modifiers are adde"
D19-1020,W11-2905,0,0.0727527,"Missing"
D19-1020,W11-0216,0,0.0751446,"Missing"
D19-1020,C96-1058,0,0.0517257,"w recall given an imperfect parser. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the fi"
D19-1020,D15-1205,1,0.779495,"Missing"
D19-1020,P19-1024,0,0.154261,"Missing"
D19-1020,W09-2415,0,0.0207045,"Missing"
D19-1020,W05-1506,0,0.102938,"r. We investigate two algorithms to generate high-quality forests by judging “quality” from different perspectives: one focusing on arcs, and the other focusing on trees. E DGEWISE This algorithm focuses on the local relation of each individual edge and uses parser probabilities as confidence scores to assess edge qualities. Starting from the whole parser search space, it keeps all the edges with scores greater than a threshold . The time complexity is O(N 2 ), where N represents the sentence length.1 KB EST E ISNER This algorithm extends the Eisner algorithm (Eisner, 1996) with cube pruning (Huang and Chiang, 2005) for finding K highest-scored tree structures. The Eisner algorithm is a standard method for decoding 1-best trees for graph-based dependency parsing. Based on bottom-up dynamic programming, it stores the 1-best subtree for each span and takes O(N 3 ) time complexity for decoding a sentence of N words. KB EST E ISNER keeps a sorted list of K-best hypotheses for each span. Cube pruning (Huang and Chiang, 2005) is adopted to generate the Kbest list for each larger span from the K-best lists of its sub-spans. After the bottom-up decoding, we merge the final K-bests by combining identical dependen"
D19-1020,D15-1137,0,0.031279,"2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semant"
D19-1020,I05-1006,0,0.184,"Missing"
D19-1020,Q17-1029,1,0.895317,"Missing"
D19-1020,P15-2047,0,0.103796,"Missing"
D19-1020,D11-1149,0,0.0191059,"ms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine"
D19-1020,P18-1249,0,0.0333378,"Missing"
D19-1020,D17-1159,0,0.500712,"iment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of"
D19-1020,J93-2004,0,0.064101,"able 1: Statistics on forests generated with various (upper half) and K (lower half) on the development set. • D EP T REE: Our baseline using 1-best dependency trees, as shown in Section 4. 7.4 • E DGEWISE PS and E DGEWISE: Our models using the forests generated by our E DGEWISE algorithm with or without parser scores. • KB EST E ISNER PS and KB EST E ISNER: Our model using the forests generated by our KB EST E ISNER algorithm with or without parser scores, respectively. 7.3 Settings We take a state-of-the-art deep biaffine parser (Dozat and Manning, 2017), trained on the Penn Treebank (PTB) (Marcus and Marcinkiewicz, 1993) converted to Universal Dependency, to obtain 1-best trees and full search spaces for generating forests. Using standard PTB data split (02–21 for training, 22 for development and 23 for testing), it gives UAS and LAS scores of 95.7 and 94.6, respectively. For the other hyper-parameters, word embeddings are initialized with the 200-dimensional BioASQ vectors5 , pretrained on 10M abstracts of biomedical articles, and are fixed during training. The dimension of hidden vectors in Bi-LSTM is set to 200, and the number of message passing steps T is set to 2 based on Zhang et al. (2018b). We use Ada"
D19-1020,P08-2026,0,0.0861827,"Missing"
D19-1020,W16-3009,0,0.0314234,"Missing"
D19-1020,C10-1123,0,0.0261185,"ature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering"
D19-1020,P08-1023,0,0.0537829,"t al., 2017) and a recent dataset focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees f"
D19-1020,N18-1080,0,0.246962,"present an edge for simplicity, and p✏ is the parser probability for edge ✏. The edge probabilities are not adjusted during end-task training. 6 Training Relation loss Given a set of training instances, each containing a sentence s with two target mentions ⇠ and ⇣, and a dependency structure D (tree or forest), we train our models with a crossentropy loss between the gold-standard relations r and model distribution: lR = log p(r|s, ⇠, ⇣, D; ✓), (13) where ✓ represents the model parameters. Using additional NER loss For training on BioCreative VI CPR, we follow previous work (Liu et al., 2017; Verga et al., 2018) to take NER loss as additional supervision, though the mention boundaries are known during testing. lN ER = N 1 X log p(tn |s, D; ✓), N (14) n=1 where tn is the gold NE tag of wn with the “BIO” scheme. Both losses are conditionally independent given the deep features produced by our Experiments We conduct experiments on two medical benchmarks to test the usefulness of dependency forest. 7.1 Data BioCreative VI CPR (Krallinger et al., 2017) This task2 focuses on the relations between chemical compounds (such as drugs) and proteins (such as genes). The full corpus contains 1020, 612 and 800 ext"
D19-1020,P16-1105,0,0.0839479,"Missing"
D19-1020,Q17-1008,0,0.357248,"on. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two."
D19-1020,D15-1062,0,0.231787,"Missing"
D19-1020,E17-1110,0,0.0869635,"wed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target entity mentions (⇠ and ⇣). We focus on the classic binary relation extraction setting (Quirk and Poon, 2017), where the number of associated mentions is two. The output is a relation from a predefined relation set R = (r1 , . . . , rM , None), where “None” means that no relation holds for the entities. Two steps are taken for predicting the correct relation given an input sentence. First, a dependency parser is used to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode bot"
D19-1020,W08-0504,0,0.0520198,"Missing"
D19-1020,C10-2133,0,0.0320316,"from parsing noise. Results on two biomedical benchmarks show that our method outperforms the standard tree-based methods, giving the state-of-the-art results in the literature. 1 Introduction The sheer amount of medical articles and their rapid growth prevent researchers from receiving comprehensive literature knowledge by direct reading. This can hamper both medical research and clinical diagnosis. NLP techniques have been used for automating the knowledge extraction process from the medical literature (Friedman et al., 2001; Yu and Agichtein, 2003; Hirschman et al., 2005; Xu et al., 2010; Sondhi et al., 2010; Abacha and Zweigenbaum, 2011). Along this line of work, a long-standing task is relation extraction, which mines factual knowledge from free text by labeling relations between entity mentions. As shown in Figure 1, the sub-clause “previously observed cytochrome P450 3A4 ( CYP3A4 ) interaction of the dual orexin receptor antagonist almorexant” contains two entities, namely “orexin receptor” and “almorexant”. There is an “adversary” relation between these two entities, denoted as“CPR:6”. ⇤ Yue Zhang is the corresponding author nmod comp comp amod comp ... observed ... interaction of orexin rec"
D19-1020,Q19-1002,1,0.928194,"e usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated"
D19-1020,P18-1150,1,0.94662,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,D18-1110,1,0.810915,"neration (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi repres"
D19-1020,D15-1206,0,0.201939,"Missing"
D19-1020,C18-1120,0,0.0200441,"focused on phenotype-gene relations (PGR) (Sousa et al., 2019) show that our method outperforms a strong baseline that uses 1-best dependency trees as features, giving the state-of-the-art accuracies in the literature. To our knowledge, we are the first to study dependency forests for medical information extraction, showing their advantages over 1-best tree structures. Our code is available at http://github.com/freesunshine/ dep-forest-re. 2 Related work Syntactic forests There have been previous studies leveraging constituent forests for machine translation (Mi et al., 2008; Ma et al., 2018; Zaremoodi and Haffari, 2018), sentiment analysis (Le and Zuidema, 2015) and text generation (Lu and Ng, 2011). However, the usefulness of dependency forests is relatively rarely studied, with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labe"
D19-1020,D18-1246,1,0.942065,"with one exception being Tu et al. (2010), who use dependency forests to enhance long-range word-to-word dependencies for statistical machine translation. To our knowledge, we are the first to study the usefulness of dependency forests for relation extraction under a strong neural framework. Graph neural network Graph neural networks (GNNs) have been successful in encoding dependency trees for downstream tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), semantic parsing (Xu et al., 2018), machine translation (Song et al., 2019; Bastings et al., 2017), relation extraction (Song et al., 2018b) and sentence ordering (Yin et al., 2019). In particular, Song et al. (2018b) showed that GNNs are more effective than DAG networks (Peng et al., 2017) for modeling syntactic trees in relation extraction, which cause loss of important structural information. We are the first to exploit GNNs for encoding search spaces in the form of dependency forests. 3 Task Formally, the input to our task is a sentence s = w1 , w2 , . . . , wN , where N is the number of words in the sentence and wi represents the i-th input word. s is annotated with boundary information (⇠1 : ⇠2 and ⇣1 : ⇣2 ) of target enti"
D19-1020,P18-1030,1,0.867698,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1020,N19-1152,0,0.0439742,"Missing"
D19-1020,D18-1244,0,0.376206,"to label the syntactic structure of the input. Here our baseline system takes the standard approach, using the 1-best parser output tree DT as features. In contrast, our proposed model uses the most confident parser forest DF as features. Given DT or DF , the second step is to encode both s and DT /DF using a neural network, before making a prediction. We make use of the same graph neural network encoder structure to represent dependency syntax information for both the baseline and our model. In particular, a graph recurrent neural network architecture (Beck et al., 2018; Song et al., 2018a; Zhang et al., 2018a) is used, which has been shown effective in encoding graph structures (Song et al., 2019), giving competitive results with alternative graph networks such as graph convolutional neural networks (Marcheggiani and Titov, 2017; Bastings et al., 2017). 4 Baseline: D EP T REE As shown in Figure 2, our baseline model stacks a bidirectional LSTM layer to encode an input sentence w1 , . . . , wN with a graph recurrent network 209 In order to capture non-local interactions between words, the GRN layer adopts a message passing framework that performs iterative information exchange between directly con"
D19-1078,P17-2061,0,0.306,"s research and applications. To deal with this issue, many researchers have conducted studies on the domain adaptation for NMT, which can be classified into two general categories. One is to transfer the rich-resource domain (out-of-domain) translation knowledge to benefit the low-resource (in-domain) NMT model. The other is to use the mixed-domain training corpus to construct a unified NMT model for all domains. Here, we mainly focus on the first type of research, of which typical methods include finetuning (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016), mixed fine-tuning (Chu et al., 2017), cost weighting (Chen et al., 2017), data selection (Wang et al., 2017a,b; Zhang et al., 2019a) and so on. The underlying assumption of these approaches is that in-domain and out-ofdomain NMT models share the same parameter space or prior distributions, and the useful out-ofdomain translation knowledge can be completely transferred to in-domain NMT model in a onepass manner. However, it is difficult to achieve this goal due to domain differences. Particularly, when the domain difference is significant, such conventional brute-force transfer may be unsuccessful, facing the similar issue as the"
D19-1078,C18-1111,0,0.0238287,"anwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to ex"
D19-1078,Q19-1002,1,0.797814,"28.78 — 31.86 27.49 29.67 — 31.33 27.96 29.64 — Multi-domain NMT Methods 31.13 28.02 29.57 21.61 31.57 27.60 29.58 21.75 31.87 27.82 29.84 21.86 IDDA Framework 32.11 28.10 30.11 22.01 32.93 28.88 30.91† 22.17∗ Out-of-domain2 Medical EMEA 51.11 50.60 — — — 52.25 52.60 52.84 52.07 53.39† Table 4: Experimental results of the English-German translation task. * indicates statistically significantly better than (ρ &lt;0.05) the result of WDCD. Model IDDA-mix IDDA IDDA Transfer Order —— {θoutnews , θoutmedical } {θoutmedical , θoutnews } AVE . 30.17 30.51 30.91 (Bahdanau et al., 2015; Su et al., 2018; Song et al., 2019), so as to verify the generality of our framework. Table 5: Experimental results of IDDA using different configurations. Acknowledgments The authors were supported by National Natural Science Foundation of China (No. 61672440), Beijing Advanced Innovation Center for Language Resources, NSF Award (No. 1704337), the Fundamental Research Funds for the Central Universities (Grant No. ZK1024), and Scientific Research Project of National Language Committee of China (Grant No. YB135-49). We also thank the reviewers for their insightful comments tic distance between each out-of-domain and indomain tra"
D19-1078,W17-4713,0,0.0342627,",θ v ∗ if EvalModel(Dout out ) &gt; EvalModel(Dout , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019)"
D19-1078,N19-1312,0,0.0550436,"(2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation. Furthermore, we extend our fr"
D19-1078,N18-2080,0,0.0238204,"2016) combined the fine-tuned model with the baseline via ensemble method. Meanwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain"
D19-1078,P17-2089,0,0.0374011,"Missing"
D19-1078,D16-1139,0,0.441352,"pirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 845–855, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 2 spectively, and then iteratively perform bidirectional translation knowledge transfer (from indomain to out-of-domain and then vice versa). In this way, both in-domain and out-of-domain NMT models are expected to constantly reinforce each other, which is likely to achieve better NMT domain adaptation. Particularly, we employ a knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016) based approach to transfer translation knowledge. During this process, the targetdomain NMT model is first initialized with the source-domain NMT model, and then trained to fit its own training data and match the output of its previous best model simultaneously. By doing so, the previously transferred translation knowledge can be effectively retained for better NMT domain adaptation. Finally, we further extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the abovementioned bidirectional knowledge transfer is performed sequentially between the in-dom"
D19-1078,D17-1155,0,0.0787754,"Missing"
D19-1078,2015.iwslt-evaluation.11,0,0.278219,"Missing"
D19-1078,P02-1040,0,0.105168,"as in-domain test sets, WMT news-test2014 (News topic) and 1K sampled sentence pairs of OPUS EMEA corpus were used as two out-ofdomain test sets. We first employed Stanford Segmenter3 to conduct word segmentation on Chinese sentences and MOSES script4 to tokenize English and German sentences. Then, we limited the length of sentences to 50 words in the training stage. Besides, we employed Byte Pair Encoding (Sennrich et al., 2016) to split words into subwords and set the vocabulary size for both ChineseEnglish and English-German as 32,000. We evaluated the translation quality with BLEU scores (Papineni et al., 2002) as calculated by multi-bleu.perl script . Settings. We chose Transformer (Vaswani et al., 2017) as our NMT model, which exhibits excellent performance due to its flexibility in parallel computation and long-range dependency modeling. We followed Vaswani et al. (2017) to set the configurations. The dimensionality of all input and output layers is 512, and that of FFN layer is 2048. We employed 8 parallel attention heads in both encoder and decoder. Parameter optimization was performed using stochastic gradient descent, where Adam (Kingma and Ba, 2015) was used to automatically adjust the learn"
D19-1078,W17-4712,0,0.476982,"t out ) &gt; EvalModel(Dout , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019) introduced knowledge d"
D19-1078,E17-2045,0,0.0359692,"ut , θout ) (k) ∗ ←θ θout out end if (k) (k) ∗ ) θin ← TransferModel(θout , Din , θin v , θ (k) ) &gt; EvalModel(D v , θ ∗ ) if EvalModel(Din in in in ∗ ← θ (k) θin in end if end for lation under our framework, we iteratively update teacher models for better domain adaptation. In contrast, all language-specific teacher NMT models in (Tan et al., 2019) remain fixed. of multi-domain NMT, which focus on building a unified NMT model trained on the mixed-domain training corpus for translation tasks in all domains (Kobus et al., 2016; Tars and Fishel, 2018; Farajian et al., 2017; Pryzant et al., 2017; Sajjad et al., 2017; Zeng et al., 2018; Bapna and Firat, 2019). Although our framework is also able to refine outof-domain NMT model, it is still significantly different from multi-domain NMT, since only the performance of in-domain NMT model is considered. 3 Iterative Dual Domain Adaptation Framework In this section, we first detailedly describe our proposed framework for conventional one-to-one NMT domain adaptation, and then extend this framework to the scenario of multiple out-ofdomain corpora (many-to-one). Finally, note that similar to our work, Tan et al. (2019) introduced knowledge distillation into mult"
D19-1078,P16-1162,0,0.0699703,"S EMEA corpus2 . As for development sets, we chose IWSLT tst2012, WMT tst2012 and 1K sampled sentence pairs of OPUS EMEA corpus, respectively. In addition, IWSLT tst2013, tst2014 were used as in-domain test sets, WMT news-test2014 (News topic) and 1K sampled sentence pairs of OPUS EMEA corpus were used as two out-ofdomain test sets. We first employed Stanford Segmenter3 to conduct word segmentation on Chinese sentences and MOSES script4 to tokenize English and German sentences. Then, we limited the length of sentences to 50 words in the training stage. Besides, we employed Byte Pair Encoding (Sennrich et al., 2016) to split words into subwords and set the vocabulary size for both ChineseEnglish and English-German as 32,000. We evaluated the translation quality with BLEU scores (Papineni et al., 2002) as calculated by multi-bleu.perl script . Settings. We chose Transformer (Vaswani et al., 2017) as our NMT model, which exhibits excellent performance due to its flexibility in parallel computation and long-range dependency modeling. We followed Vaswani et al. (2017) to set the configurations. The dimensionality of all input and output layers is 512, and that of FFN layer is 2048. We employed 8 parallel att"
D19-1078,D18-1041,1,0.823069,"combined the fine-tuned model with the baseline via ensemble method. Meanwhile, applying data weighting into NMT domain adaptation has attracted much attention. Wang et al. (2017a) and Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain"
D19-1078,N19-1189,0,0.103432,"d Wang et al. (2017b) proposed several sentence and domain weighting methods with a dynamic weight learning strategy. Zhang et al. (2019a) ranked unlabeled domain training samples based on their similarity to in-domain data, and then adopts a probabilistic curriculum learning strategy during training. Chen et al. (2017) applied the sentence-level cost weighting to refine the training of NMT model. Recently, Vilar (2018) introduced a weight to each hidden unit of out-of-domain model. Chu and Wang (2018) gave a comprehensive survey of the dominant domain adaptation techniques for NMT. Gu et al. (2019) not only maintained a private encoder and a private decoder for each domain, but also introduced a common encoder and a common decoder shared by all domains. Significantly different from the above methods, along with the studies of dual learning for NMT (He et al., 2016; Wang et al., 2018; Zhang et al., 2019b), we iteratively perform bidirectional translation knowledge transfer between in-domain and out-of-domain training corpora. To the best of our knowledge, our work is the first attempt to explore such a dual learning based framework for NMT domain adaptation. Furthermore, we extend our fr"
D19-1078,D16-1163,0,0.15549,"fore, NMT for low-resource domains becomes a challenge in its research and applications. To deal with this issue, many researchers have conducted studies on the domain adaptation for NMT, which can be classified into two general categories. One is to transfer the rich-resource domain (out-of-domain) translation knowledge to benefit the low-resource (in-domain) NMT model. The other is to use the mixed-domain training corpus to construct a unified NMT model for all domains. Here, we mainly focus on the first type of research, of which typical methods include finetuning (Luong and Manning, 2015; Zoph et al., 2016; Servan et al., 2016), mixed fine-tuning (Chu et al., 2017), cost weighting (Chen et al., 2017), data selection (Wang et al., 2017a,b; Zhang et al., 2019a) and so on. The underlying assumption of these approaches is that in-domain and out-ofdomain NMT models share the same parameter space or prior distributions, and the useful out-ofdomain translation knowledge can be completely transferred to in-domain NMT model in a onepass manner. However, it is difficult to achieve this goal due to domain differences. Particularly, when the domain difference is significant, such conventional brute-force t"
E14-1018,D12-1038,0,0.0271901,"Missing"
E14-1018,P09-1058,0,0.0269917,"Missing"
E14-1018,P04-1015,0,0.0511352,"ntroduced tagging tasks. To investigate the effects of regularization methods on the parsing tasks, we fully re-implement the lineartime incremental shift-reduce dependency parser by Huang and Sagae (2010). The structure perceptron is used to train such model. The model totally employs 28 feature templates proposed by Huang and Sagae (2010). Since the search space for parsing tasks is quite larger than the search space for tagging tasks, Exact search algorithms such as dynamic programming can not be used. Besides, beam search with state merging is used for decoding. The early update strategy (Collins and Roark, 2004) is also employed. In order to compare to the related work, unlike the Chinese word segmentation and the POS tagging task, we split the CTB5 corpus following Zhang et al.(2008). Two types of accuracies are used to measure the performances, namely word and complete match (excluding punctuations) (Huang and Sagae, 2010). tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the"
E14-1018,W02-1001,0,0.560741,"n the input layer and the hidden layer are usually hand-crafted and fixed during training and predicting. And the output layer of the network is a scalar w · Φ(x, y) which is used to evaluate the matching of the vector x and y. Besides the common process to calculate the output layer given the input layer, there is a process called decoding, which is to find a vector z to maximum the activation of the output layer: z 3 3.1 =w X i Averaging Averaged Perceptron Averaging the weight vectors in the learning process is one of the most popular regularization techniques of the structured perceptron (Collins, 2002). And it is also the only used regularization technique for many practical studies on NLP (Jiang et al., 2009; Huang and Sagae, 2010). Suppose the learning algorithm stopped after T updates. The final weight vector is calculated as: (1) By carefully designing the feature vector, the decoding can be efficiently performed using dynamic programming. Beam search is also commonly used for the decoding of syntactical parsing tasks. In the predicting precess, the vector z is the structured output corresponding to x. In the training precess, what we expect is that for every input xi , the vector zi th"
E14-1018,I05-3017,0,0.0299371,"t the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input is x = 菊次郎的夏天 (21) y = BMESBE, (22) the corresponding segmentation result is 菊次郎 的 夏天. (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.2 Averaging First, we investigate the effect of averaging techniques for regularization. Figure 2 shows the influence of the number of the averaged models by using the “shuffle-and-average” method described in section 3.2. The performances of the Chinese word segmentation, POS tagging and parsing tasks are all increased by averaging models trained with the same training data with different orders. The “shuffle-and-average” method is effective to reduce the bias caused by the specific order of the training examples. For the Chinese word segmenta"
E14-1018,W04-3236,0,0.029635,"s of accuracies are used to measure the performances, namely word and complete match (excluding punctuations) (Huang and Sagae, 2010). tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input is x = 菊次郎的夏天 (21) y = BMESBE, (22) the corresponding segmentation result is 菊次郎 的 夏天. (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.2 Averaging First, we investigate the effect of averaging techniques for regularization. Figure 2 shows the influence of the number of the averaged models by using the “shuffle-and-average” method described in section 3.2. The performances of the Chinese word segmentation, POS tagging and parsing tasks are all increased by averaging models trained with the same training data with different order"
E14-1018,P12-1025,0,0.0241332,"Missing"
E14-1018,P10-1110,0,0.167581,"r of the network is a scalar w · Φ(x, y) which is used to evaluate the matching of the vector x and y. Besides the common process to calculate the output layer given the input layer, there is a process called decoding, which is to find a vector z to maximum the activation of the output layer: z 3 3.1 =w X i Averaging Averaged Perceptron Averaging the weight vectors in the learning process is one of the most popular regularization techniques of the structured perceptron (Collins, 2002). And it is also the only used regularization technique for many practical studies on NLP (Jiang et al., 2009; Huang and Sagae, 2010). Suppose the learning algorithm stopped after T updates. The final weight vector is calculated as: (1) By carefully designing the feature vector, the decoding can be efficiently performed using dynamic programming. Beam search is also commonly used for the decoding of syntactical parsing tasks. In the predicting precess, the vector z is the structured output corresponding to x. In the training precess, what we expect is that for every input xi , the vector zi that maximums the activation of the output layer is exactly the gold standard output yi . We define the loss function as the sum of the"
E14-1018,N12-1015,0,0.0359292,"Missing"
E14-1018,P11-1139,0,0.0178334,"es, namely word and complete match (excluding punctuations) (Huang and Sagae, 2010). tag S indicates that the character forms a singlecharacter words; tag B / E indicates that the character is at the beginning / end of a multi-character words; tag M indicates that the character is in the middle of a multi-character words. For example, if the tag sequence for the input is x = 菊次郎的夏天 (21) y = BMESBE, (22) the corresponding segmentation result is 菊次郎 的 夏天. (23) Table 1 shows the set of the feature templates which is a subset of some related work (Ng and Low, 2004; Jiang et al., 2009) . Following Sun (2011), we split the Chinese treebank 5 into training set, development set and test set. F-measure (Emerson, 2005) is used as the measurement of the performance. 6.2 Averaging First, we investigate the effect of averaging techniques for regularization. Figure 2 shows the influence of the number of the averaged models by using the “shuffle-and-average” method described in section 3.2. The performances of the Chinese word segmentation, POS tagging and parsing tasks are all increased by averaging models trained with the same training data with different orders. The “shuffle-and-average” method is effec"
E14-1018,P08-1102,0,0.0386686,"Missing"
E14-1018,P09-1054,0,0.0277819,"do not share the same feature set. Features may be used in one [i] model but not in another one. When wj = 0, it does not imply that this feature has no effect on this problem. It only implies that this feature does not have chances to be tested. We propose a modified equation to only average the non-zero components: Pn X i Pn [i] i=1 wj L2-norm penalty (t+1) wi Penalty (t) ← max{0, |wi |− ηλ1 } (t) |wi | (t) (t) wi − η∆φi (12) This ensures that the weight decay will not change the sign of the weight. An modified version of the L1 penalty for the online learning is the cumulative L1 penalty (Tsuruoka et al., 2009), which is used to make the stochastic gradient of the penalty term more close to the true gradient. The update is divided into two steps. In the first step, the weight vector is updated according to the loss function without the penalty term (t+ 1 ) (t) (t) (13) wi 2 ← wi − η∆φi Adding penalty term to the loss function is a common and traditional regularization method to avoid over-fitting. It is widely used for the optimization problems of logistic regression, support vector machine, conditional random field and other models. Penalty terms for probabilistic models can be interpreted as a pri"
E14-1018,I11-1035,0,0.0299822,"Missing"
E14-1018,P09-1110,0,0.045491,"Missing"
E14-1018,D08-1059,0,0.0764584,"Missing"
E14-1018,D10-1082,0,0.0323063,"Missing"
E14-1018,J11-1005,0,0.0344749,"Missing"
E14-1018,W12-6308,1,0.805254,"d model + Shuffle + L2 + Shuffle + L2 + Dropout+ Shuffle sf 0.9758 0.9787 0.9791 0.9791 Table 3: Final results of the word-based Chinese word segmentation task on CTB5. results in this paper. The results in Table 2 shows that with proper regularization methods, the models trained using perceptron algorithm can outperform CRF models with the same feature templates and other models with more sophisticated features trained using the averaged perceptron without other regularization methods. We further re-implemented a word-based Chinese word segmentation model with the feature templates following Zhang et al. (2012), which http://crfpp.googlecode.com/svn/trunk/doc/index.html 170 (Jiang et al., 2008) (Kruengkrai et al., 2009) (Zhang and Clark, 2010) (Sun, 2011) Character-based model + Shuffle + Dropout + Dropout+ Shuffle + word-based re-ranking sf 0.9785 0.9787 0.9778 0.9817 0.9779 0.9802 0.9789 0.9809 0.9813 jf 0.9341 0.9367 0.9367 0.9402 0.9336 0.9375 0.9361 0.9407 0.9438 Table 4: Final results of the POS tagging task on CTB5. (Huang and Sagae, 2010) our re-implementation + Shuffle + Dropout + Dropout+ Shuffle word 85.20 85.22 85.65 85.32 85.71 6.5.3 compl. 33.72 34.15 34.52 34.04 34.57 Dependency Parsi"
E14-1018,N13-1038,0,0.0233649,"Missing"
E14-1018,N10-1069,0,\N,Missing
E14-1018,P09-1059,0,\N,Missing
I13-1051,D07-1090,0,0.0249087,"to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model. Tam et al. (2007) a"
I13-1051,J93-2003,0,0.0426971,"of the translated-document. As there is no explicit correspondence between topics on both sides, we project the source-side topic distribution to the target side as a trigger to our topic specific language models. As compared with previous works, our model takes advantage of the topical information on both sides, thus breaking down the context barrier for language model. Experimental results on various Chinese-English test sets show that our method gains an average improvement of +0.76 Bleu points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack t"
I13-1051,2003.mtsummit-papers.6,0,0.40867,"ese-English test sets show that our method gains an average improvement of +0.76 Bleu points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan,"
I13-1051,J07-2003,0,0.329253,"n 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned tra"
I13-1051,W07-0722,0,0.0240884,"o account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information,"
I13-1051,W07-0717,0,0.0196706,"Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more"
I13-1051,2011.mtsummit-papers.57,0,0.0369799,"across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabilities into training corpus and trains several topic-specific LMs, 3 Topic triggered Language Model Polysemy is a difficult problem for statistical m"
I13-1051,D09-1123,0,0.0304243,"Missing"
I13-1051,P12-1048,1,0.869348,"Missing"
I13-1051,P07-1066,0,0.358669,"But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al. (2007) use topic distribution to cluster the training corpora and train LMs accordingly. Our method is different from theirs in that we assign topic probabilities to training sentences rather than segment them into different topics, so our model is more robust to data sparse problem. Besides, Foster and Kuhn (2007), Civera and Juan (2007), L¨u et al. (2007) also adapt mixture modeling framework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used"
I13-1051,P11-1021,0,0.0139907,"Chinese Academy of Sciences ∗ Software School of Xiamen University ‡ Centre for Next Generation Localisation Faculty of Engineering and Computing, Dublin City University {yuheng,lvyajuan,liuqun}@ict.ac.cn jssu@xmu.edu.cn qliu@computing.dcu.ie Abstract parts of SMT, such as word-alignment (Zhao and Xing, 2006; Zhao and Xing, 2007; Gong et al., 2011), translation model (Xiao et al., 2012). All these works show that a particular translation often appears in some specific topical context, so it is reasonable to enhance the prediction ability of language model by incorporating topical information. Tan et al. (2011) introduces a large scale distributed composite language model incorporating document-level information. But they only focus on the target side and explore in n-best reranking task which has a limited search space, while another promising application is taking account of topical information on both sides and integrate the LM into decoding to online select translation hypotheses. However, the integration is not easy. Since the test-document can be from any topic, it is hard to dynamically estimate language model probability according to various topic distributions. Language model is an essentia"
I13-1051,N03-1017,0,0.00644178,"ranslation tasks. The bilingual training data for translation model contain 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, an"
I13-1051,W04-3250,0,0.341936,"M toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a ing this process, we empirically set the paramegood complement for n-gram model to further imter values for HTMM training as: α = 1.5, β = prove translation quality. We can also see that our 1.01, iters = 100. See Gruber et al. (2007) for the method generally out-performs the Tam’s method, meanings of these parameters. and set the topic because our model can capture n-gram level topic number to 30 1 for both source and target side. The information, rather than only focus on estimatin"
I13-1051,D07-1036,1,0.870824,"Missing"
I13-1051,J04-4002,0,0.0168302,"than N-gram model. 4.1 Experiment setup We present our experiments on the NIST ChineseEnglish translation tasks. The bilingual training data for translation model contain 1.5M sentence pair with 38M Chinese words and 32M English words. The monolingual data for training English language model includes the Xinhua portion of the GIGAWORD corpus, which contains 10M sentences. We used the NIST evaluation set of 2006(MT06) as our development set, and sets of MT04/05/08 as test sets. Corpus statistics are shown in Table 1. We obtain symmetric word alignments of training data by first running GIZA++ (Och and Ney, 2004) in both directions and then applying refinement rule ”grow-diag-final-and” (Koehn et al., 2003). We re-implement the Hierarchical phrasebased system (Chiang, 2007) and extract SCFG (t f ,te ,a) (i, j)∈a where δ(x; y) is the Kronecker function, which is 1 if x = y and 0 otherwise. We then compute the 450 ppl 04 05 08 documents 980K Base LM 158.42 134.59 208.11 99.4K Topic LM 148.11 119.17 200.41 52 Table 3: 4-gram word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test"
I13-1051,P12-1079,1,0.887159,"is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabilities into training corpus and trains several topic-specific LMs, 3 Topic triggered Language Model Polysemy is a difficult problem for statistical machine translation. As shown in Figure 1, English sentence ”give me a shot” has different meanings in different domains. Using traditional LM, which only considers the local context information"
I13-1051,P03-1021,0,0.00872365,"for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned training data. A 4adapt n-gram language model. gram language model is trained on the monolinTable 2 reports the Bleu and TER scores on gual data by SRILM toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a ing this process, we empirically set the paramegood complement for n-gram model to further imter values for HTMM training as: α = 1.5, β = prove translation quality. We can also see"
I13-1051,P02-1040,0,0.0873504,"m word perplexity results of our 200 method in terms of ppl, We compare our model 100 with baseline n-gram model (”Base LM”) on three 109 test-sets. Table 1: Training, tuning and test data used for evaluating Bleu score. follow the method by Tam et al. (2007), denote as ”Tam”, and generate topic-based marginals to rules from this word-aligned training data. A 4adapt n-gram language model. gram language model is trained on the monolinTable 2 reports the Bleu and TER scores on gual data by SRILM toolkit (Stolcke, 2002). Caseall test-sets. The baseline system achieves Bleu insensitive NIST BLEU (Papineni et al., 2002) is score of 37.43 on NIST04, 33.67 on NIST05 and used to measure translation performance. We use 28.54 on NIST08 set. Our method(STLM) gains minimum error rate training (Och, 2003) for optian average improvement of +0.76 Bleu and an avmizing the feature weights. erage reduction of −0.88 TER over the baseline. To obtain topic distribution, We use the open Results on NIST MT 04, 05, 08 are statistically source LDA tool Open HTMM developed by Grusignificant with p < 0.05 (Koehn, 2004). This verber et al. (2007) for estimation and inference. Durifies that our topic-triggered language model is a i"
I13-1051,P11-1129,0,0.0363899,"Missing"
I13-1051,W11-2133,0,0.0152618,"mami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model. Tam et al. (2007) and Ruiz and Federico (2011) introduce topic model for cross-lingual language model adaptation task. They use bilingual topic model to project latent topic distribution across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and X"
I13-1051,W06-1626,0,0.0203118,"ach of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor decomposition based on a segmented training corpus. They estimate unigram topic-based probability and combine it with standard n-gram model"
I13-1051,P11-1063,0,0.0213179,"amework to exploit the full potential of existing corpus. Adopting this framework, the training corpus is first divided into different parts, each of which is used to train a sub model, then these sub models are used together with different weights during decoding. Those works typically use word similarities and sentence level information, while our work extents the context into the document level. the baseline model. 2 Related Work Previous works devoted to improving language models in SMT mostly focus on utilizing more contextual information, such as syntax-based LMs (Charniak et al., 2003; Schwartz et al., 2011; Shen et al., 2008; Hassan et al., 2009), Forward & MI trigger LM (Xiong et al., 2011), and large-scale language models (Zhang et al., 2006; Brants et al., 2007; Emami et al., 2007; Talbot and Osborne, 2007). Since our philosophy is fundamentally different from them in that we incorporate information at document level to build language models. So we discuss previous works that explore topic information for SMT in this section. Researchers have been trying to incorporate topic information into language models in several ways. Gildea and Hofmann (1999) use EM algorithm to perform a topic factor"
I13-1051,P06-2124,0,0.0223876,"ico (2011) introduce topic model for cross-lingual language model adaptation task. They use bilingual topic model to project latent topic distribution across languages. Based on the BLSA, they are able to transfer source-side topic weights into target-side and use them to generate topic-based marginals to adapt ngram language model. Our model is different from theirs in that rather than using topic-based probabilities to adapt n-gram model, we directly calculate LM probability conditioned on topic distributions. There are also some valuable applications of topic model for machine translation. Zhao and Xing (2006) propose the Bilingual Topic Admixture Model (BiTAM) for word alignment and extract topic-dependent translation model accordingly. Gong et al. (2011) introduce topic model for filtering topic-mismatched phrase pairs. Su et al. (2012) use the topic distribution of in-domain monolingual corpus to adapt the translation model. Xiao et al. (2012) introduce a topic similarity model to select the synchronous rules for hierarchical phrase-based translation. Our work is in the same spirit with those works, but we are interested in LM problem rather than other parts in SMT. Our work models topic probabi"
I13-1051,P08-1066,0,0.124574,"points and a perplexity reduction over Language model (LM) measures the fluency of translation outputs (Brown et al., 1993), and plays an important role in statistical machine translation (SMT). Traditional language model predicts the next word conditioning only on the preceding n−1 words, thus ignores syntactic structures in the sentence and global information over the document. One direct approach to handle this problem is to explore sentence-level context, such as syntaxbased language model for reranking (Charniak et al., 2003), and dependency language model for String-to-Dependency model (Shen et al., 2008). But these methods are still not robust enough to handle the polysemy and domain changes, as they lack the global-context information. Another interesting line is to utilize information at document-level. Intuitively, different domains or topics have different n-gram probability distributions. Thus, we should take into account the topic information when we translate a document. Topic model has been learned in several 447 International Joint Conference on Natural Language Processing, pages 447–454, Nagoya, Japan, 14-18 October 2013. so it is in the same spirit of mixture modeling. Heidel et al"
I13-1051,P07-1065,0,\N,Missing
P10-2003,P05-1022,0,0.10367,"Missing"
P10-2003,P08-1067,0,0.0732652,"Missing"
P10-2003,N03-1017,0,0.0461596,"ations of a sentence pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering mod"
P10-2003,P07-2045,0,0.162702,"ion evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presence of the adjacent bi"
P10-2003,W04-3250,0,0.0231813,"e used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (Tillmann, 2004) is also closely related to our model. However,"
P10-2003,J03-1002,0,0.00561231,"inuous. If a msd-bidirectional-fe model is used, then the number of features doubles: one for each direction. 3.1 Experiment Setup Two different sizes of training corpora are used in our experiments: one is a small-scale corpus that comes from FBIS corpus consisting of 239K bilingual sentence pairs, the other is a large-scale corpus that includes 1.55M bilingual sentence pairs from LDC. The 2002 NIST MT evaluation test data is used as the development set and the 2003, 2004, 2005 NIST MT test data are the test sets. We choose the MOSES1 (Koehn et al., 2007) as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Fin"
P10-2003,J04-4002,0,0.0483088,"pair, to learn lexicalized reordering models efficiently. Experimental results on the NIST Chinese-English test sets show that our approach significantly outperforms the baseline method. 1 Figure 1: Occurrence of a swap with different numbers of adjacent bilingual phrases: only one phrase in (a) and three phrases in (b). Black squares denote word alignments and gray rectangles denote bilingual phrases. [s,t] indicates the target-side span of bilingual phrase bp and [u,v] represents the source-side span of bilingual phrase bp. Introduction Phrase-based translation systems (Koehn et al., 2003; Och and Ney, 2004) prove to be the stateof-the-art as they have delivered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advant"
P10-2003,P03-1021,0,0.210537,"cally, the model collects bilingual phrases and distinguishes their orientations with respect to the previous bilingual phrase into three categories:   M o= S   D ai − ai−1 = 1 ai − ai−1 = −1 |ai − ai−1 |6= 1 (1) Using the relative-frequency approach, the reordering probability regarding bp is Count(o, bp) 0 o0 Count(o , bp) p(o|bp) = P (2) 2.2 Reordering Graph For a parallel sentence pair, its reordering graph indicates all possible translation derivations consisting of the extracted bilingual phrases. To construct a reordering graph, we first extract bilingual phrases using the way of (Och, 2003). Then, the adjacent bilingual phrases are linked according to the targetside order. Some bilingual phrases, which have no adjacent bilingual phrases because of maximum length limitation, are linked to the nearest bilingual phrases in the target-side order. Shown in Figure 2(b), the reordering graph for the parallel sentence pair (Figure 2(a)) can be represented as an undirected graph, where each rectangle corresponds to a phrase pair, each link is the orientation relationship between adjacent bilingual phrases, and two distinguished rectangles bs and be indicate the beginning and ending of th"
P10-2003,P02-1040,0,0.082129,"GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a word-aligned corpus, where we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train a 4-gram language model on the Xinhua portion of Gigaword corpus. In exception to the reordering probabilities, we use the same features in the comparative experiments. During decoding, we set ttable-limit = 20, stack = 100, and perform minimum-error-rate training (Och, 2003) to tune various feature weights. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU scores differences. 3.2 Experimental Results Table 2 shows the results of experiments with the small training corpus. For the msd-fe model, the BLEU scores by our method are 30.51 32.78 and 29.50, achieving absolute improvements of 0.89, 0.66 and 0.62 on the three test sets, respectively. For the msd-bidirectional-fe model, our method obtains BLEU scores of 30.49 32.73 and 29.24, with absolute improvements of 1.11, 0.73 and 0.60 over the baseline method. 1 The phrase-based lexical reordering model (T"
P10-2003,N04-4026,0,0.371205,"vered translation performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model"
P10-2003,P06-1066,1,0.837358,"n performance in recent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) de"
P10-2003,W06-3108,0,0.02101,"ent machine translation evaluations. While excelling at memorizing local translation and reordering, phrase-based systems have difficulties in modeling permutations among phrases. As a result, it is important to develop effective reordering models to capture such non-local reordering. The early phrase-based paradigm (Koehn et al., 2003) applies a simple distance-based distortion penalty to model the phrase movements. More recently, many researchers have presented lexicalized reordering models that take advantage of lexical information to predict reordering (Tillmann, 2004; Xiong et al., 2006; Zens and Ney, 2006; Koehn et al., 2007; Galley and Manning, 2008). These models are learned from a word-aligned corpus to predict three orientations of a phrase pair with respect to the previous bilingual phrase: monotone (M ), swap (S), and discontinuous (D). Take the bilingual phrase bp in Figure 1(a) for example. The wordbased reordering model (Koehn et al., 2007) analyzes the word alignments at positions (s − 1, u − 1) and (s − 1, v + 1). The orientation of bp is set to D because the position (s − 1, v + 1) contains no word alignment. The phrase-based reordering model (Tillmann, 2004) determines the presenc"
P10-2003,D08-1089,0,\N,Missing
P12-1048,J05-4003,0,0.0943735,"Missing"
P12-1048,W09-0432,0,0.0300305,"t Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of"
P12-1048,D07-1007,0,0.0296716,") also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of addit"
P12-1048,P07-1005,0,0.406273,"uccessfully in NLP community. Based on the “bag-of-words” assumption that the order of words can be ignored, these methods model the text corpus by using a co-occurrence matrix of words and documents, and build generative models to infer the latent aspects or topics. Using these models, the words can be clustered into the derived topics with a probability distribution, and the correlation between words can be automatically captured via topics. However, the “bag-of-words” assumption is an unrealistic oversimplification because it ignores the order of words. To remedy this problem, Gruber et al.(2007) propose HTMM, which models the topics of words in the document as a Markov chain. Based on the assumption that all words in the same sentence have the same topic and the successive sentences are more likely to have the same topic, HTMM incorporates the local dependency between words by Hidden Markov Model for better topic estimation. HTMM can also be viewed as a soft clustering tool for words in training corpus. That is, HTMM can estimate the probability distribution of a topic over words, i.e. the topic-word distribution P (word|topic) during training. Besides, HTMM derives inherent topics i"
P12-1048,P10-1086,0,0.0111327,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P10-1146,0,0.0129201,"ship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific tra"
P12-1048,W07-0722,0,0.159911,"dapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpo"
P12-1048,eck-etal-2004-language,0,0.0665867,"Missing"
P12-1048,2005.mtsummit-papers.30,0,0.060786,"Missing"
P12-1048,W07-0717,0,0.227606,"re we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Lingui"
P12-1048,D10-1044,0,0.0292696,"in SMT. Assuming each bilingual sentence constitutes a mixture of hidden topics and each word pair follows a topic-specific bilingual translation model, Zhao and Xing (2006,2007) presented a bilingual topical admixture formalism to improve word alignment by capturing topic sharing at different levels of linguistic granularity. Tam et al.(2007) proposed a bilingual LSA, which enforces one-to-one topic correspondence and enables latent topic distributions to be efficiently transferred across languages, to crosslingual language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; C"
P12-1048,P06-1121,0,0.0055969,"ility estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-d"
P12-1048,D08-1039,0,0.0258649,"Missing"
P12-1048,C08-1041,1,0.583545,"odeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain m"
P12-1048,2005.eamt-1.19,0,0.0557771,"Missing"
P12-1048,J03-1002,0,0.00333165,"conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al.,"
P12-1048,J04-4002,0,0.647358,"translation probability estimation. Our method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the"
P12-1048,N03-1017,0,0.0215688,"Missing"
P12-1048,W04-3250,0,0.0780006,"generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes of in-domain monolingual corpora: 5K and 40K. Adaptation Met"
P12-1048,P07-2045,0,0.00336531,"vely, we use HTMM tool developed by Gruber et al.(2007) to conduct topic model training. During this process, we empirically set the same parameter values for the HTMM training of different corpora: topics = 50, α = 1.5, β = 1.01, iters = 100. See (Gruber et al., 2007) for the meanings of these parameters. Besides, we set the interpolation weight θ in formula (10) to 0.5 by observing the results on development set in the additional experiments. We choose MOSES, a famous open-source 1 2 http://blog.sohu.com/ http://u.cs.biu.ac.il/ koppel/BlogCorpus.html phrase-based machine translation system (Koehn et al., 2007), as the experimental decoder. GIZA++ (Och and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evalu"
P12-1048,P06-1077,1,0.129484,"method establishes the relationship between the out-of-domain bilingual corpus and the in-domain monolingual corpora via topic mapping and phrase-topic distribution probability estimation from in-domain monolingual corpora. Experimental result on the NIST Chinese-English translation task shows that our approach significantly outperforms the baseline system. 1 Introduction In recent years, statistical machine translation(SMT) has been rapidly developing with more and more novel translation models being proposed and put into practice (Koehn et al., 2003; Och and Ney, 2004; Galley et al., 2006; Liu et al., 2006; Chiang, 2007; Chiang, 2010). However, similar to other natural language processing(NLP) tasks, SMT systems often suffer from domain adaptation problem during practical applications. The simple reason is that the underlying statistical models always tend to closely ∗ Part of this work was done during the first author’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual co"
P12-1048,D07-1036,1,0.789966,"l, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach i"
P12-1048,D09-1022,0,0.0366086,"l language modeling and translation lexicon adaptation. Recently, Gong and Zhou (2010) also applied topic modeling into domain adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to"
P12-1048,D09-1074,0,0.027418,"Missing"
P12-1048,W11-2133,0,0.0654102,"gnoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if “bank” occurs more often in the sentences rela"
P12-1048,P02-1040,0,0.105802,"ch and Ney, 2003) and the heuristics “grow-diag-final-and” are used to generate a wordaligned corpus, from which we extract bilingual phrases with maximum length 7. We use SRILM Toolkits (Stolcke, 2002) to train two 4-gram language models on the filtered English Blog Authorship corpus and the Xinhua portion of Gigaword corpus, respectively. During decoding, we set the ttable-limit as 20, the stack-size as 100, and perform minimum-error-rate training (Och and Ney, 2003) to tune the feature weights for the log-linear model. The translation quality is evaluated by case-insensitive BLEU-4 metric (Papineni et al., 2002). Finally, we conduct paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 4.2 Result and Analysis 4.2.1 Effect of Different Smoothing Methods Our first experiments investigate the effect of different smoothing methods for the in-domain phrasetopic distribution: “Noisy-OR” and “Averaging”. We build adapted phrase tables with these two methods, and then respectively use them in place of the out-of-domain phrase table to test the system performance. For the purpose of studying the generality of our approach, we carry out comparative experiments on two sizes"
P12-1048,2009.mtsummit-posters.17,0,0.0128304,"ion emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computati"
P12-1048,2007.mtsummit-tutorials.1,0,0.0760626,"rpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates of phrases. In a monolingual corpus, if"
P12-1048,C08-1125,1,0.73343,"or’s internship at Baidu. According to adaptation emphases, domain adaptation in SMT can be classified into translation model adaptation and language model adaptation. Here we focus on how to adapt a translation model, which is trained from the large-scale out-of-domain bilingual corpus, for domain-specific translation task, leaving others for future work. In this aspect, previous methods can be divided into two categories: one paid attention to collecting more sentence pairs by information retrieval technology (Hildebrand et al., 2005) or synthesized parallel sentences (Ueffing et al., 2008; Wu et al., 2008; Bertoldi and Federico, 2009; Schwenk and Senellart, 2009), and the other exploited the full potential of existing parallel corpus in a mixture-modeling (Foster and Kuhn, 2007; Civera and Juan, 2007; Lv et al., 2007) framework. However, these approaches focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings"
P12-1048,N04-1033,0,0.157585,"which is decomposed into the topic posterior distribution at the word level, and θ is the interpolation weight that can be optimized over the development data. Given the number of the phrase f˜ in sentence f denoted as countf (f˜), we compute the in-domain phrase-topic distribution in the following way: Pmle (tf P = f ∈Cf P tf in ˜ in |f ) countf (f˜) · P (tf in |f ) in countf (f˜) · P (tf P f ∈Cf in in |f ) (11) Under the assumption that the topics of all words in the same phrase are independent, we consider two methods to calculate Pword (tf in |f˜). One is a “Noisy-OR” combination method (Zens and Ney, 2004) which has shown good performance in calculating similarities between bags-of-words in different languages. Using this method, Pword (tf in |f˜) is defined as: Pword (tf in |f˜) = 1 − Pword (t¯f in |f˜) Y ≈ 1− P (t¯f in |fj ) fj ∈f˜ = 1− Y (1 − P (tf in |fj )) (12) fj ∈f˜ where Pword (t¯f in |f˜) represents the probability that tf in is not the topic of the phrase f˜. Similarly, P (t¯f in |fj ) indicates the probability that tf in is not the topic of the word fj . The other method is an “Averaging” combination one. With the assumption that tf in is the topic of f˜ if at least one of the words"
P12-1048,W06-1626,0,0.0730106,"Missing"
P12-1048,C04-1059,0,0.0592364,"Missing"
P12-1048,P06-2124,0,0.82898,"hes focused on the studies of bilingual corpus synthesis and exploitation while ignoring the monolingual corpora, therefore limiting the potential of further translation quality improvement. In this paper, we propose a novel adaptation method to adapt the translation model for domainspecific translation task by utilizing in-domain 459 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 459–468, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics monolingual corpora. Our approach is inspired by the recent studies (Zhao and Xing, 2006; Zhao and Xing, 2007; Tam et al., 2007; Gong and Zhou, 2010; Ruiz and Federico, 2011) which have shown that a particular translation always appears in some specific topical contexts, and the topical context information has a great effect on translation selection. For example, “bank” often occurs in the sentences related to the economy topic when translated into “y´inh´ ang”, and occurs in the sentences related to the geography topic when translated to “h´ ea `n”. Therefore, the co-occurrence frequency of the phrases in some specific context can be used to constrain the translation candidates"
P12-1048,D08-1010,1,0.848211,"in adaptation in SMT. Their method employed one additional feature function to capture the topic inherent in the source phrase and help the decoder dynamically choose related target phrases according to the specific topic of the source phrase. Besides, our approach is also related to contextdependent translation. Recent studies have shown that SMT systems can benefit from the utilization of context information. For example, triggerbased lexicon model (Hasan et al., 2008; Mauser et al., 2009) and context-dependent translation selection (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). The former generated triplets to capture long-distance dependencies that go beyond the local context of phrases, and the latter built the classifiers which combine rich context information to better select translation during decoding. With the consideration of various local context features, these approaches all yielded stable improvements on different translation tasks. As compared to the above-mentioned works, our work has the following differences. • We focus on how to adapt a translation model for domain-specific translation task with the help of additional in-domain monolingual corpora,"
P12-1048,J07-2003,0,\N,Missing
P12-1048,2006.iwslt-evaluation.15,0,\N,Missing
P15-1023,W11-1014,0,0.0265826,"Missing"
P15-1023,D07-1007,0,0.24854,"ish translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish transl"
P15-1023,P07-1005,0,0.0775738,"Missing"
P15-1023,P11-2031,0,0.106229,"Missing"
P15-1023,P12-2023,0,0.214218,"r a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Associati"
P15-1023,P06-1121,0,0.055962,"a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local"
P15-1023,D12-1010,1,0.890599,"Missing"
P15-1023,D08-1039,0,0.104532,"e to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presen"
P15-1023,E14-1035,0,0.15347,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,W14-3358,0,0.182188,"anslations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conf"
P15-1023,C08-1041,0,0.0994588,"xts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if l"
P15-1023,N03-1017,0,0.0655323,"l is integrated into a translation system for lexical selection. Experiment results on NIST ChineseEnglish test sets demonstrate that 1) our model significantly outperforms previous lexical selection methods and 2) modeling correlations between local words and global topics can further improve translation quality. 1 Introduction Lexical selection is a very important task in statistical machine translation (SMT). Given a sentence in the source language, lexical selection statistically predicts translations for source words, based on various translation knowledge. Most conventional SMT systems (Koehn et al., 2003; Galley et al., 2006; Chiang, 2007) exploit very limited context information contained in bilingual rules for lexical selection. ∗ Corresponding author. duì gāi wèntí zhōngguó bǎochí zhōnglì lìchǎng {problem, issue ...}wèntí {stance, attitude ...}lìchǎng [Economy topic, Politics topic ...] Figure 1: A Chinese-English translation example to illustrate the effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that"
P15-1023,W04-3250,0,0.175547,"baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233 Model CATM (± 6w) CATM (± 8w) CATM (± 10w) CATM (± 12w) CATM (± 14w) MT05 33.35 33.43 33.42 33.49 33.30 Table 2: Experiment results on the development set using different window sizes ws . To train CATM, we set the topic number Nz as 25.5 For hyperparameters α and β, we empirically set α=50/Nz and β=0.1, as implemented in (Griffiths and Steyvers, 2004). Following Han et al. (2012), we se"
P15-1023,D08-1010,1,0.79717,"inspired by (Han and Sun, 2012), where an entity-topic model is presented for entity linking. We successfully adapt this work to lexical selection in SMT. The related work mainly includes the following two strands. (1) Lexical Selection in SMT. In order to explore rich context information for lexical selection, some researchers propose trigger-based lexicon models to capture long-distance dependencies (Hasan et al., 2008; Mauser et al., 2009), and many more researchers build classifiers to select desirable translations during decoding (Chan et al., 2007; Carpuat and Wu, 2007; He et al., 2008; Liu et al., 2008). Along this line, Shen et al. (2009) introduce four new linguistic and contextual features for translation selection in SMT. Recently, we have witnessed an increasing efforts in exploiting document-level context information to improve lexical selection. Xiao et al. (2011) impose a hard constraint to guarantee 235 Target-side Topical Items UNHCR republic refugee refugee Kosovo federal military missile military United States system war country development economy international economic trade Taiwan China relation cross-strait cross-strait relation issue Topic Source-side Contextual Words J¬(ref"
P15-1023,D09-1022,0,0.124816,"effect of local contexts and global topics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On t"
P15-1023,P02-1038,0,0.0449124,"topical items. Formally, for the position i in the document corresponding to the content word f , we collect the sampled count that translation e˜ generates f , denoted by Csam (˜ e, f ). This count can be normalized to form a new translation probability in the following way: Csam (˜ e, f ) + k p(˜ e|f ) = Csam + k · Ne˜,f where Csam is the total number of samples during inference and Ne˜,f is the number of candidate translations of f . Here we apply add-k smoothing to refine this translation probability, where k is a tunable global smoothing constant. Under the framework of log-linear model (Och and Ney, 2002), we use this translation probability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-"
P15-1023,J03-1002,0,0.00668871,"robability as a new feature to improve lexical selection in SMT. 4 Experiments In order to examine the effectiveness of our model, we carried out several groups of experiments on Chinese-to-English translation. 4.1 Setup Our bilingual training corpus is from the FBIS corpus and the Hansards part of LDC2004T07 corpus (1M parallel sentences, 54.6K documents, with 25.2M Chinese words and 29M English words). We first used ZPar toolkit2 and Stanford toolkit3 to preprocess (i.e., word segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the"
P15-1023,P03-1021,0,0.0641451,"Missing"
P15-1023,J04-4002,0,0.305645,"Missing"
P15-1023,D09-1008,0,0.126118,"pics as well as their correlations on lexical selection. Each black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts sugges"
P15-1023,P12-1048,1,0.783622,"or missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into translation model. Hewavitharana et al. (2013) propose an incremental topic based translation model adaptation approach that satisfies the causality constraint imposed by spoken conversations. Hasl"
P15-1023,N12-1046,0,0.0474752,"Missing"
P15-1023,J97-3002,0,0.0509132,"ord segmenting, PoS tagging) the Chinese and English parts of training corpus, and then word-aligned them using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/do"
P15-1023,2011.mtsummit-papers.13,0,0.169815,"h black line indicates a set of translation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated"
P15-1023,P12-1079,1,0.93568,"ation candidates for a Chinese content word (within a dotted box). Green lines point to translations that are favored by local contexts while blue lines show bidirectional associations between global topics and their consistent target-side translations. Previous studies that explore richer information for lexical selection can be divided into two categories: 1) incorporating sentence-level contexts (Chan et al., 2007; Carpuat and Wu, 2007; Hasan et al., 2008; Mauser et al., 2009; He et al., 2008; Shen et al., 2009) or 2) integrating document-level topics (Xiao et al., 2011; Ture et al., 2012; Xiao et al., 2012; Eidelman et al., 2012; Hewavitharana et al., 2013; Xiong et al., 2013; Hasler et al., 2014a; Hasler et al., 2014b) into SMT. The methods in these two strands have shown their effectiveness on lexical selection. However, correlations between sentence- and document-level contexts have never been explored before. It is clear that local contexts and global topics are often highly correlated. Consider a ChineseEnglish translation example presented in Figure 1. On the one hand, if local contexts suggest that the source word “á|/l`ıchˇang” should be translated in229 Proceedings of the 53rd Annual M"
P15-1023,P06-1066,1,0.778544,"using GIZA++ (Och and Ney, 2003) with the option “grow-diag-final-and”. We chose the NIST evaluation set of MT05 as the development set, and the sets of MT06/MT08 as test sets. On average, these three sets contain 17.2, 13.9 and 14.1 content words per sentence, respectively. We trained a 5-gram language model on the Xinhua portion of Gigaword corpus using the SRILM Toolkit (Stolcke, 2002). Our baseline system is a state-of-the-art SMT system, which adapts bracketing transduction grammars (Wu, 1997) to phrasal translation and equips itself with a maximum entropy based reordering model (MEBTG) (Xiong et al., 2006). We used the toolkit4 developed by Zhang (2004) to train the reordering model with the following parameters: iteration number iter=200 and Gaussian prior g=1.0. During decoding, we set the ttable-limit as 20, the stack-size as 100. The translation quality is evaluated by case-insensitive BLEU-4 (Papineni et al., 2002) metric. Finally, we conducted paired bootstrap sampling (Koehn, 2004) to test the significance in BLEU score differences. 2 http://people.sutd.edu.sg/∼yue zhang/doc/index.html http://nlp.stanford.edu/software 4 http://homepages.inf.ed.ac.uk/lzhang10/maxenttoolkit.html 3 (6) 233"
P15-1023,P14-1137,1,0.852105,"Missing"
P15-1023,P06-2124,0,0.0377211,"topical items and contextual words learned by CATM with Nz =25 and Ws =12. Chinese words that do not have direct English translations are denoted with ”*”. Here “q” and “|” are Chinese quantifiers for missile and war, respectively; “ü” and “W” together means cross-starit. the document-level translation consistency. Ture et al. (2012) soften this consistency constraint by integrating three counting features into decoder. Also relevant is the work of Xiong et al.(2013), who use three different models to capture lexical cohesion for document-level SMT. (2) SMT with Topic Models. In this strand, Zhao and Xing (2006, 2007) first present a bilingual topical admixture formalism for word alignment in SMT. Tam et al. (2007) and Ruiz et al. (2012) apply topic model into language model adaptation. Su et al. (2012) conduct translation model adaptation with monolingual topic information. Gong et al. (2010) and Xiao et al. (2012) introduce topic-based similarity models to improve SMT system. Axelrod et al. (2012) build topic-specific translation models from the TED corpus and select topic-relevant data from the UN corpus to improve coverage. Eidelman et al. (2012) incorporate topic-specific lexical weights into t"
P15-1023,P02-1040,0,\N,Missing
P15-1023,W11-2133,0,\N,Missing
P15-1023,P13-2122,0,\N,Missing
P15-1023,J07-2003,0,\N,Missing
P17-2042,D16-1020,0,0.161037,"cult does never nor refused want neither impossible if limited know declined anybody nobody yet little either denied Table 3: Results of using different word embeddings. We also list the Precision, Recall and F1 score for each relation. mation in explicit data, and thus benefits implicit discourse relation recognition. We also compare our method with three recent systems which also use explicit data to boost the performance: 1) R&X2015: Rutherford and Xue (2015) construct weakly labeled data from explicit data based on the chosen connectives, to enlarge the training data directly. 2) B&D2016: Braud and Denis (2016) learn connective-based word representations and build a logistic regression model based on them6 . 3) Liu2016: Liu et al. (2016) use a multi-task neural network to incorporate several discourserelated data, including explicit data and the RSTDT corpus (William and Thompson, 1988). System R&X2015 B&D2016 Liu2016 CDRR+DSWE Accuracy 57.10 52.81 57.27 58.85 good word2vec DSWE great great bad lot terrific very decent better nice success excellent well fantastic happy better certainly solid respect lousy fine wonderful import terrible positive Good help tough useful best welcome Table 5: Top 15 clo"
P17-2042,P16-1163,0,0.278043,"f Information Science and Technology, Xiamen University, China2 Xiamen University, China3 wcxnlp@163.com boliwang@stu.xmu.edu.cn {mandel, ydchen, jssu}@xmu.edu.cn Abstract may mean a Contrast relation. However, classifiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (Zhang et al., 2015; Ji and Eisenstein, 2015; Qin et al., 2016; Chen et al., 2016; Liu and Li, 2016). While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance. In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around the discourse connective and (but). Oth"
P17-2042,Q15-1024,0,0.147449,"Systems, Xiamen University, China1 School of Information Science and Technology, Xiamen University, China2 Xiamen University, China3 wcxnlp@163.com boliwang@stu.xmu.edu.cn {mandel, ydchen, jssu}@xmu.edu.cn Abstract may mean a Contrast relation. However, classifiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (Zhang et al., 2015; Ji and Eisenstein, 2015; Qin et al., 2016; Chen et al., 2016; Liu and Li, 2016). While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance. In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around th"
P17-2042,P13-1047,0,0.279413,"Missing"
P17-2042,D09-1036,0,0.70358,"Missing"
P17-2042,D16-1130,0,0.514637,"ce and Technology, Xiamen University, China2 Xiamen University, China3 wcxnlp@163.com boliwang@stu.xmu.edu.cn {mandel, ydchen, jssu}@xmu.edu.cn Abstract may mean a Contrast relation. However, classifiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (Zhang et al., 2015; Ji and Eisenstein, 2015; Qin et al., 2016; Chen et al., 2016; Liu and Li, 2016). While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance. In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around the discourse connective and (but). Other connectives can"
P17-2042,D15-1266,1,0.880463,"in-like Intelligent Systems, Xiamen University, China1 School of Information Science and Technology, Xiamen University, China2 Xiamen University, China3 wcxnlp@163.com boliwang@stu.xmu.edu.cn {mandel, ydchen, jssu}@xmu.edu.cn Abstract may mean a Contrast relation. However, classifiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (Zhang et al., 2015; Ji and Eisenstein, 2015; Qin et al., 2016; Chen et al., 2016; Liu and Li, 2016). While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance. In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs"
P17-2042,W10-4310,0,0.574019,"Missing"
P17-2042,D14-1162,0,0.0939421,"Missing"
P17-2042,P09-1077,0,0.16407,"Missing"
P17-2042,P09-2004,0,0.241158,"Missing"
P17-2042,prasad-etal-2008-penn,0,0.106963,"tions of words, to capture discourse relationships between them. To this end, we use a simple neural network to perform connective classification on massive explicit data. Explicit data can be considered to be automatically labeled by connectives. While they cannot be directly used as training data for implicit discourse relation recognition and contain some noise, they are effective enough to provide weakly supervised signals for training the discourse-specific word embeddings. We apply DSWE as features in a supervised neural network for implicit discourse relations recognition. On the PDTB (Prasad et al., 2008), using DSWE yields significantly better performance than using off-the-shelf word embeddings, We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as features achieves significant improvements over baselines. 1 Introduction Recognizing discourse relations (e.g., Contrast, Conjunction) between two sentences is"
P17-2042,D16-1246,0,0.514538,"y, China1 School of Information Science and Technology, Xiamen University, China2 Xiamen University, China3 wcxnlp@163.com boliwang@stu.xmu.edu.cn {mandel, ydchen, jssu}@xmu.edu.cn Abstract may mean a Contrast relation. However, classifiers based on word pairs in previous work do not work well because of the data sparsity problem. To address this problem, recent researches use word embeddings (aka distributed representations) instead of words as input features, and design various neural networks to capture discourse relationships between arguments (Zhang et al., 2015; Ji and Eisenstein, 2015; Qin et al., 2016; Chen et al., 2016; Liu and Li, 2016). While these researches achieve promising results, they are all based on pre-trained word embeddings ignoring discourse information (e.g., good, great, and bad are often mapped into close vectors). Intuitively, using word embeddings sensitive to discourse relations would further boost the performance. In this paper, we propose to learn discoursespecific word embeddings (DSWE) from explicit data for implicit discourse relation recognition. Our method is inspired by the observation that synonym (antonym) word pairs tend to appear around the discourse connec"
P17-2042,E14-1068,0,0.119455,"Missing"
P17-2042,N15-1081,0,0.350554,"0.72 63.91 81.60 79.00 69.63 70.66 57.17 58.85 40.71 44.84 not word2vec DSWE do no n’t did anymore never necessarily nothing anything neither anyway none difficult does never nor refused want neither impossible if limited know declined anybody nobody yet little either denied Table 3: Results of using different word embeddings. We also list the Precision, Recall and F1 score for each relation. mation in explicit data, and thus benefits implicit discourse relation recognition. We also compare our method with three recent systems which also use explicit data to boost the performance: 1) R&X2015: Rutherford and Xue (2015) construct weakly labeled data from explicit data based on the chosen connectives, to enlarge the training data directly. 2) B&D2016: Braud and Denis (2016) learn connective-based word representations and build a logistic regression model based on them6 . 3) Liu2016: Liu et al. (2016) use a multi-task neural network to incorporate several discourserelated data, including explicit data and the RSTDT corpus (William and Thompson, 1988). System R&X2015 B&D2016 Liu2016 CDRR+DSWE Accuracy 57.10 52.81 57.27 58.85 good word2vec DSWE great great bad lot terrific very decent better nice success excelle"
P17-2042,D16-1253,1,0.840335,"ht these raw frequencies. In this way, they represent words in the space of connectives to directly encode their discourse function. The major limitation of their approach is that the dimension of the word representations must be less than or equal to the number of connectives. By comparison, we learn DSWE by predicting connectives conditioning on arguments, which yields better performance and has no such dimension limitation. Some researchers use explicit data as additional training data via multi-task learning (Lan et al., 2013; Liu et al., 2016) or data selection (Rutherford and Xue, 2015; Wu et al., 2016). Discourse-specific Word Embeddings In this section, we first introduce the neural network model for learning discourse-specific word embeddings (DSWE), and then the way of collecting explicit discourse data for training. Finally, we highlight the differences between our work and the related researches. w1arg1 … … w marg1 avg wh2 wc conn w1h w1arg2 … … w narg2 Figure 1: Neural network model for learning DSWE. An explicit instance is denoted as (arg1 , arg2 , conn). w1arg1 , ..., wm arg1 mean the words in arg1 . Two arguments are concatenated as input and the number of hidden layers is not lim"
P17-2042,P13-2013,0,\N,Missing
P18-1166,P17-1012,0,0.0972114,"rchitectures have been explored ∗ Corresponding author. Source code is available https://github.com/bzhangXMU/transformer-aan. RNN 2 CNN 3 Transformer Figure 1: Illustration of the decoding procedure under different neural architectures. We show which previous target words are required to predict the current target word yj in different NMT architectures. k indicates the filter size of the convolution layer. Introduction 1 1 at as the backbone network for translation, ranging from recurrent neural networks (RNN) (Sutskever et al., 2014; Luong et al., 2015), convolutional neural networks (CNN) (Gehring et al., 2017a,b) to full attention networks without recurrence and convolution (Vaswani et al., 2017). Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art performance on several language pairs (Vaswani et al., 2017). Most interestingly, the neural Transformer is capable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path. The parallelization property enables training NMT very quickly, while the dependency modeling property endows the Transformer with strong ability"
P18-1166,C16-1291,0,0.0194238,"dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both long-range dependencies and temporal order information. Mi et al. (2016) and Liu et al. (2016) employ standard word alignment to supervise the automatically generated attention weights. Our work also focus on the evolution of attention network, but unlike previous work, we seek to simplify the selfattention network so as to accelerate the decoding procedure. The design of our model is partially inspired by the highway network (Srivastava et al., 2015) and the residual network (He et al., 2015). In the respect of speeding up the decoding of the neural Transformer, Gu et al. (2018) change the auto-regressive architecture to speed up translation by directly generating target words without"
P18-1166,E17-2061,0,0.0205918,"we have mentioned in Section 1, it suffers from decoding inefficiency. The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT. It contributes a lot to make NMT outperform SMT. Recently, a variety of efforts are made to further improve its accuracy and capability. Luong et al. 1790 (2015) explore several attention formulations and distinguish local attention from global attention. Zhang et al. (2016) treat RNN as an alternative to the attention to improve model’s capability in dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both long-range dependencies and temporal order information. Mi et al. (2016) and Liu et al. (2016) employ standard word alignment to super"
P18-1166,P16-5005,0,0.0184881,"and short dependency path significantly improve the training speed as well as model performance for the Transformer. Unfortunately, as we have mentioned in Section 1, it suffers from decoding inefficiency. The attention mechanism is originally proposed to induce translation-relevant source information for predicting next target word in NMT. It contributes a lot to make NMT outperform SMT. Recently, a variety of efforts are made to further improve its accuracy and capability. Luong et al. 1790 (2015) explore several attention formulations and distinguish local attention from global attention. Zhang et al. (2016) treat RNN as an alternative to the attention to improve model’s capability in dealing with long-range dependencies. Yang et al. (2017) introduce a recurrent cycle on the attention layer to enhance the model’s memorization of previous translated source words. Zhang et al. (2017a) observe the weak discrimination ability of the attention-generated context vectors and propose a GRU-gated attention network. Kim et al. (2017) further model intrinsic structures inside attention through graphical models. Shen et al. (2017) introduce a direction structure into a selfattention network to integrate both"
P18-1166,D15-1166,0,0.0854619,"l., 2015). Under this framework, various advanced neural architectures have been explored ∗ Corresponding author. Source code is available https://github.com/bzhangXMU/transformer-aan. RNN 2 CNN 3 Transformer Figure 1: Illustration of the decoding procedure under different neural architectures. We show which previous target words are required to predict the current target word yj in different NMT architectures. k indicates the filter size of the convolution layer. Introduction 1 1 at as the backbone network for translation, ranging from recurrent neural networks (RNN) (Sutskever et al., 2014; Luong et al., 2015), convolutional neural networks (CNN) (Gehring et al., 2017a,b) to full attention networks without recurrence and convolution (Vaswani et al., 2017). Particularly, the neural Transformer, relying solely on attention networks, has refreshed state-of-the-art performance on several language pairs (Vaswani et al., 2017). Most interestingly, the neural Transformer is capable of being fully parallelized at the training phase and modeling intra-/inter-dependencies of source and target sentences within a short path. The parallelization property enables training NMT very quickly, while the dependency m"
P18-1166,D16-1249,0,0.0355611,"Missing"
P18-1166,P02-1040,0,0.103169,"pendencies with previous predicted words:   ˜sl = AAN sl−1 (8) Carrying these dependencies, the decoder stacks another two sub-layers to seek translation-relevant source semantics for bridging the gap between the 5.1 Experiments WMT14 English-German Translation We examine various aspects of our AAN on this translation task. The training data consist of 4.5M sentence pairs, involving about 116M English words and 110M German words. We used newstest2013 as the development set for model selection, and newstest2014 as the test set. We evaluated translation quality via case-sensitive BLEU metric (Papineni et al., 2002). 5.1.1 Model Settings We applied byte pair encoding algorithm (Sennrich et al., 2016) to encode all sentences and limited the vocabulary size to 32K. All out-ofvocabulary words were mapped to an unique token “unk”. We set the dimensionality d of all input and output layers to 512, and that of innerFFN layer to 2048. We employed 8 parallel attention heads in both encoder and decoder layers. We batched sentence pairs together so that they were approximately of the same length, and each batch had roughly 25000 source and target tokens. During training, we used label smoothing with value ls = 0."
P18-1166,P16-1162,0,0.219561,"encies, the decoder stacks another two sub-layers to seek translation-relevant source semantics for bridging the gap between the 5.1 Experiments WMT14 English-German Translation We examine various aspects of our AAN on this translation task. The training data consist of 4.5M sentence pairs, involving about 116M English words and 110M German words. We used newstest2013 as the development set for model selection, and newstest2014 as the test set. We evaluated translation quality via case-sensitive BLEU metric (Papineni et al., 2002). 5.1.1 Model Settings We applied byte pair encoding algorithm (Sennrich et al., 2016) to encode all sentences and limited the vocabulary size to 32K. All out-ofvocabulary words were mapped to an unique token “unk”. We set the dimensionality d of all input and output layers to 512, and that of innerFFN layer to 2048. We employed 8 parallel attention heads in both encoder and decoder layers. We batched sentence pairs together so that they were approximately of the same length, and each batch had roughly 25000 source and target tokens. During training, we used label smoothing with value ls = 0.1, attention dropout and residual dropout with a rate of p = 0.1. During decoding, we"
P19-1053,P14-2009,0,0.255066,"Missing"
P19-1053,D14-1181,0,0.0111454,"Missing"
P19-1053,S14-2076,0,0.121985,"o Lu1∗ , Jinsong Su1† , Yubin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mec"
P19-1053,W04-3250,0,0.0188998,"69 71.70 80.35 71.36 80.59 71.15 80.32 71.01 80.50 72.04 80.96 72.67 81.33 72.90∗∗ 81.53∗ TWITTER Macro-F1 Accuracy — — 66.17 67.71 66.18 67.78 67.20 68.90 67.47 69.17 67.88∗∗ 69.64∗∗ 73.60 74.97 76.82 77.60 76.78 77.54 76.53 77.46 76.58 77.46 77.42 78.08 77.63 78.47 77.72∗∗ 78.61∗ Table 4: Experimental results on various datasets. We directly cited the best experimental results of MN and TNet reported in (Wang et al., 2018; Li et al., 2018). ∗∗ and ∗ means significant at p <0.01 and p <0.05 over the baselines (MN, TNet) on each test set, respectively. Here we conducted 1,000 bootstrap tests (Koehn, 2004) to measure the significance in metric score differences. First, both of our reimplemented MN and TNet are comparable to their original models reported in (Wang et al., 2018; Li et al., 2018). These results show that our reimplemented baselines are competitive. When we replace the CNN of TNet with an attention mechanism, TNet-ATT is slightly inferior to TNet. Moreover, when we perform additional K+1-iteration of training on these models, their performance has not changed significantly, suggesting simply increasing training time is unable to enhance the performance of the neural ASC models. Sec"
P19-1053,P18-1087,0,0.532047,"a-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-freq"
P19-1053,C16-1291,0,0.0132555,"on weights of “cute” in training instances with neural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation probl"
P19-1053,P17-1164,0,0.139556,"sses both TNet and TNet-ATT. These results strongly demonstrate the effectiveness and generality of our approach. 4.3 Case Study Following this trend, researchers have resorted to more sophisticated attention mechanisms to refine neural ASC models. Chen et al., (2017) proposed a multiple-attention mechanism to capture sentiment features separated by a long distance, so that it is more robust against irrelevant information. An interactive attention network has been designed by Ma et al., (2017) for ASC, where two attention networks were introduced to model the target and context interactively. Liu et al., (2017) proposed to leverage multiple attentions for ASC: one obtained from the left context and the other one acquired from the right context of a given aspect. Very recently, transformation-based model has also been explored for ASC (Li et al., 2018), and the attention mechanism is replaced by CNN. In order to know how our method improves neural ASC models, we deeply analyze attention results of TNet-ATT and TNet-ATT(+AS). It has been found that our proposed approach can solve the above-mentioned two issues well. Table 5 provides two test cases. TNet-ATT incorrectly predicts the sentiment of the fi"
P19-1053,C18-1193,0,0.0198957,"eural or negative polarity are significantly decreased. Specifically, in these instances, the average weight of “cute” is reduced to 0.07 times of the original. Hence, TNet-ATT(+AS) assigns a smaller weight (0.1090→0.0062) to “cute” and achieves the correct sentiment prediction. 5 Different from these work, our work is in line with the studies of introducing attention supervision to refine the attention mechanism, which have become hot research topics in several NNbased NLP tasks, such as event detection (Liu et al., 2017), machine translation (Liu et al., 2016), and police killing detection (Nguyen and Nguyen, 2018). However, such supervised attention acquisition is labor-intense. Therefore, we mainly commits to automatic mining supervision information for attention mechanisms of neural ASC models. Theoretically, our approach is orthogonal to these models, and we leave the adaptation of our approach into these models as future work. Our work is inspired by two recent models: one is (Wei et al., 2017) proposed to progressively mine discriminative object regions using classification networks to address the weakly-supervised semantic segmentation problems, and the other one is (Xu et al., 2018) where a drop"
P19-1053,D14-1162,0,0.0823334,"Missing"
P19-1053,D17-1047,0,0.480008,"f Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is l"
P19-1053,C16-1311,0,0.569845,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,D16-1021,0,0.460212,"a 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus"
P19-1053,S14-2036,0,0.0370387,"bin Ge4 , Linfeng Song5 , Le Sun2 , Jiebo Luo5 1 Xiamen University, Xiamen, China 2 Institute of Software, Chinese Academy of Sciences, Beijing, China 3 University of Chinese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital rol"
P19-1053,P18-1088,0,0.616588,"na, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a res"
P19-1053,D16-1058,0,0.335356,"inese Academy of Sciences, Beijing, China 4 University of Illinois at Urbana-Champaign, Urbana, IL 61801, USA 5 Department of Computer Science, University of Rochester, Rochester NY 14627, USA jialong2019@iscas.ac.cn, ziyaolu2018@stu.xmu.edu.cn jssu@xmu.edu.cn Abstract vious representative models are mostly discriminative classifiers based on manual feature engineering, such as Support Vector Machine (Kiritchenko et al., 2014; Wagner et al., 2014). Recently, with the rapid development of deep learning, dominant ASC models have evolved into neural network (NN) based models (Tang et al., 2016b; Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Li et al., 2018; Wang et al., 2018), which are able to automatically learn the aspect-related semantic representation of an input sentence and thus exhibit better performance. Usually, these NN-based models are equipped with attention mechanisms to learn the importance of each context word towards a given aspect. It can not be denied that attention mechanisms play vital roles in neural ASC models. However, the existing attention mechanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent w"
P19-1053,K18-1055,0,0.0203569,"hanism in ASC suffers from a major drawback. Specifically, it is prone to overly focus on a few frequent words with sentiment polarities and little attention is laid upon low-frequency ones. As a result, the performance of attentional neural ASC models is still far from satisfaction. We speculate that this is because there exist widely “apparent patterns” and “inapparent patterns”. Here, “apparent patterns” are interpreted as high-frequency words with strong sentiment polarities and “inapparent patterns” are referred to as low-frequency ones in training data. As mentioned in (Li et al., 2018; Xu et al., 2018; Lin et al., 2017), NNs are easily affected by these two modes: “apparent patterns” tend to be overly learned while “inapparent patterns” often can not be fully learned. Here we use sentences in Table 1 to explain this defect. In the first three training sentences, given the fact that the context word “small” occurs frequently with negative sentiment, the attenIn aspect-level sentiment classification (ASC), it is prevalent to equip dominant neural models with attention mechanisms, for the sake of acquiring the importance of each context word on the given aspect. However, such a mechanism tend"
P19-1053,N16-1174,0,0.159322,"Missing"
P19-1053,E17-2091,0,0.113118,"Missing"
P19-1053,S14-2004,0,\N,Missing
Q19-1002,P17-2021,0,0.0274704,"ledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association"
Q19-1002,D15-1198,0,0.0207633,"MRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experim"
Q19-1002,J12-2006,0,0.0211864,"en training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling,"
Q19-1002,W13-2322,0,0.484662,"ow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the concepts and edges (such as :ARG0 and :ARG1) represent the relations between concepts they connect. Comparing with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they"
Q19-1002,S16-1186,0,0.244245,"ohn and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorpo"
Q19-1002,D17-1209,0,0.0409234,"eto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4."
Q19-1002,P14-1134,0,0.0836165,"with semantic roles, AMRs capture more relations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representat"
Q19-1002,P18-1026,0,0.190312,"se layer: − ← − → s 0 = W 1 [ h 0 ; h N ] + b1 , and Transformer (Vaswani et al., 2017) on classification and sequence labeling tasks; Song et al. (2018) build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend Song et al. (2018) by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation. In addition to GRNs and GCNs, there have been other graph neural networks, such as graph gated neural network (GGNN) (Li et al., 2015b; Beck et al., 2018). Because our main concern is to empirically investigate the effectiveness of AMR for NMT, we leave it to future work to compare GCN, GGNN, and GRN for our task. 3 where W 1 and b1 are model parameters. For each decoding step m, the decoder feeds the concatenation of the embedding of the current input eym and the previous context vector ζ m−1 into the LSTM model to update its hidden state: Baseline: Attention-Based BiLSTM sm = LSTM(sm−1 , [eym ; ζ m−1 ]). We take the attention-based sequence-to-sequence model of Bahdanau et al. (2015) as the baseline, but use LSTM cells (Hochreiter and Schmidh"
Q19-1002,P18-1170,0,0.0223019,"2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. I"
Q19-1002,P17-1112,0,0.0195771,"ed by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional"
Q19-1002,D18-1198,0,0.0365651,") for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a ful"
Q19-1002,P17-1177,0,0.0237591,"ntion-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distr"
Q19-1002,P82-1020,0,0.739452,"Missing"
Q19-1002,D17-1263,0,0.0258882,"feng Song,1 Daniel Gildea,1 Yue Zhang,2 Zhiguo Wang,3 and Jinsong Su4 1 Department of Computer Science, University of Rochester, Rochester, NY 14627 2 School of Engineering, Westlake University, China 3 IBM T.J. Watson Research Center, Yorktown Heights, NY 10598 4 Xiamen University, Xiamen, China 1 {lsong10,gildea}@cs.rochester.edu 2 yue.zhang@wias.org.cn 3 zgw.tomorrow@gmail.com 4 jssu@xmu.edu.cn Abstract from SRL can improve the performance of an attention-based sequence-to-sequence model by alleviating the ‘‘argument switching’’ problem,1 one frequent and severe issue faced by NMT systems (Isabelle et al., 2017). Figure 1(a) shows one example of semantic role information, which only captures the relations between a predicate (gave) and its arguments (John, wife, and present). Other important information, such as the relation between John and wife, cannot be incorporated. In this paper, we explore the usefulness of abstract meaning representation (AMR) (Banarescu et al., 2013) as a semantic representation for NMT. AMR is a semantic formalism that encodes the meaning of a sentence as a rooted, directed graph. Figure 1(b) shows an AMR graph, in which the nodes (such as give-01 and John) represent the co"
Q19-1002,D14-1179,0,0.0368049,"Missing"
Q19-1002,C12-1083,0,0.0622695,"not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid"
Q19-1002,W14-3348,0,0.0650882,"Missing"
Q19-1002,P14-5010,0,0.00592251,"Missing"
Q19-1002,P17-4012,0,0.114376,"Missing"
Q19-1002,N18-2078,0,0.264809,"s to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. cantly improves a strong attention-based sequenceto-sequence baseline (25.5 vs 23.7 BLEU). When trained with small-scale (226K) data, the improvement increases"
Q19-1002,W04-3250,0,0.44934,"Missing"
Q19-1002,P17-1014,0,0.345035,"n dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can signifi"
Q19-1002,P02-1040,0,0.103866,"Missing"
Q19-1002,P17-1064,0,0.0168608,"ove a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published 3/2019. c 2019 Association for Computational"
Q19-1002,K15-1004,1,0.859168,"relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman da"
Q19-1002,W15-4502,0,0.34139,", which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 2018) as the AMR encoder. In particular, a full AMR graph is considered as a single state, with nodes in the graph being its substates. State transitions are performed on the graph recurrently, allowing substates to exchange information through edges. At each r"
Q19-1002,P18-1171,1,0.850218,"a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; Guo and Lu, 2018), and have made it possible for automatically generated AMRs to benefit downstream tasks, such as question answering (Mitra and Baral, 2015), summarization (Takase et al., 2016), and event detection (Li et al., 2015a). However, to our knowledge, no existing work has exploited AMR for enhancing NMT. We fill in this gap, taking an attention-based sequence-to-sequence system as our baseline, which is similar to Bahdanau et al. (2015). To leverage knowledge within an AMR graph, we adopt a graph recurrent network (GRN) (Song et al., 2018; Zhang et al., 20"
Q19-1002,C10-1081,1,0.785947,"viate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layer"
Q19-1002,D15-1136,0,0.0408605,"ations, such as the relation between John and wife (represented by the subgraph within dotted lines). In addition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard"
Q19-1002,P18-1037,0,0.181496,"y capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a strong attention-based s"
Q19-1002,P16-1162,0,0.0975416,"ns around 4.5 million sentence pairs for training. In addition, we use a subset of the full dataset (News Commentary v11 [NC-v11], containing around 243,000 sentence pairs) for development and additional experiments. For all experiments, we use newstest2013 and newstest2016 as the development and test sets, respectively. To preprocess the data, the tokenizer from Moses4 is used to tokenize both the English and German sides. The training sentence pairs where either side is longer than 50 words are filtered out after tokenization. To deal with rare and compound words, byte-pair encoding (BPE)5 (Sennrich et al., 2016) is applied to both sides. In particular, 8,000 and 16,000 BPE merges are used on the News Commentary v11 subset and the full training set, respectively. On the other hand, JAMR6 (Flanigan et al., 2016) is adopted to parse the English sentences into AMRs before BPE is applied. The statistics of the training data and vocabularies after preprocessing are shown in Tables 1 and 2, respectively. For the experiments with the full training set, we used the top 40K where el and ei are the embeddings of edge label l and source node vi , and W 4 and b4 are model parameters. 4.2 Training Incorporating AM"
Q19-1002,2006.amta-papers.25,0,0.0803469,"Missing"
Q19-1002,D17-1129,0,0.0841465,"dition, AMRs directly capture entity relations and abstract away inflections and function words. As a result, they can serve as a source of knowledge for machine translation that is orthogonal to the textual input. Furthermore, structural information from AMR graphs can help reduce data sparsity when training data is not sufficient for large-scale training. Recent advances in AMR parsing keep pushing the boundary of state-of-the-art performance (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Peng et al., 2015; Flanigan et al., 2016; Buys and Blunsom, 2017; Konstas et al., 2017; Wang and Xue, 2017; Lyu and Titov, 2018; It is intuitive that semantic representations can be useful for machine translation, mainly because they can help in enforcing meaning preservation and handling data sparsity (many sentences correspond to one meaning) of machine translation models. On the other hand, little work has been done on leveraging semantics for neural machine translation (NMT). In this work, we study the usefulness of AMR (abstract meaning representation) on NMT. Experiments on a standard English-toGerman dataset show that incorporating AMR as additional knowledge can significantly improve a str"
Q19-1002,N06-1056,0,0.151113,"Missing"
Q19-1002,P18-1150,1,0.928432,"ir graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Grosc"
Q19-1002,N09-2004,0,0.0443451,"n from AMR can alleviate data sparsity when training data are not sufficient. To our knowledge, we are the first to investigate AMR for NMT. Our code and parallel data (training/dev/test) with automatically parsed AMRs are available at https://github.com/freesunshine0316/semantic-nmt. 2 Figure 1: (a) A sentence with semantic roles annotations; (b) the corresponding AMR graph of that sentence. Related Work Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate–argument structure from semantic role labeling (Wong and Mooney, 2006; Wu and Fung, 2009; Liu and Gildea, 2010; Baker et al., 2012). Jones et al. (2012) first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolution"
Q19-1002,P16-2049,0,0.0266319,"g AMR as additional knowledge can significantly improve a strong attention-based sequenceto-sequence neural translation model. 1 Introduction It is intuitive that semantic representations ought to be relevant to machine translation, given that the task is to produce a target language sentence with the same meaning as the source language input. Semantic representations formed the core of the earliest symbolic machine translation systems, and have been applied to statistical but non-neural systems as well. Leveraging syntax for neural machine translation (NMT) has been an active research topic (Stahlberg et al., 2016; Aharoni and Goldberg, 2017; Li et al., 2017; Chen et al., 2017; Bastings et al., 2017; Wu et al., 2017; Chen et al., 2018). On the other hand, exploring semantics for NMT has so far received relatively little attention. Recently, Marcheggiani et al. (2018) exploited semantic role labeling (SRL) for NMT, showing that the predicate–argument information 1 That is, flipping arguments corresponding to different roles. 19 Transactions of the Association for Computational Linguistics, vol. 7, pp. 19–31, 2019. Action Editor: Philipp Koehn. Submission batch: 6/2018; Revision batch: 10/2018; Published"
Q19-1002,D16-1112,0,0.0991011,"Missing"
Q19-1002,P18-1030,1,0.718086,"g representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a largescale machine translation task. Recently, Marcheggiani et al. (2018) investigated SRL on NMT. The predicate–argument structures are encoded via graph convolutional network (GCN) layers (Kipf and Welling, 2017), which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT. In addition, we leverage a GRN (Song et al., 2018; Zhang et al., 2018) for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly attentive LSTM decoder, which is another major difference from Marcheggiani et al. (2018). GRNs have recently been used to model graph structures in NLP tasks. In particular, Zhang et al. (2018) use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs Peng et al., 2018; Groschwitz et al., 2018; G"
