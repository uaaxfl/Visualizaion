2006.amta-papers.11,P04-1023,0,0.0239046,"nslation problem, we are given a source sentence, and the task is to output a target sentence which conveys the same meaning. In the process, the system produces a mapping between words of the source sentence and target sentence. In the alignment problem, we are given both sentences, and the task is simply to find this mapping. Alignment can therefore be thought of as a 90 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 90-99, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas Citation Koehn et al. (2003) 1 Callison-Burch et al. (2004) Callison-Burch et al. (2004) Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is"
2006.amta-papers.11,H05-1098,1,0.137604,"Alignments affect the quality of the lexical weighting feature. Here the correlation is direct: poor alignment will cause this feature to favor translation featuring word pairs which are not translations 4 Experiments All of our experiments were performed on ChineseEnglish translation in the news domain. The data we used in our experiments were divided into four parts. For phrase extraction and training of submodels, we used a large training set consisting of over 1 million sentences from various newswire corpora. This corpus is roughly the same as the one used for large-scale experiments by Chiang et al. (2005). We 6 An anonymous reviewer nicely summed up the relationship between word alignment and phrase extraction: “a couple currently if uneasily holding hands on the road to high-quality machine translation.” 93 supervised method. However, we believe that the GIZA++ alignments on this corpus are reasonably close to state-of-the-art. We refer to this alignment as Best. We wished to avoid confounding our study by considering alignments with vastly different profiles, such as recall-oriented alignments versus precision-oriented alignments. For our purposes, we were interested in the impact of the qua"
2006.amta-papers.11,P05-1033,0,0.0203216,"ences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system"
2006.amta-papers.11,W06-3105,0,0.0136155,"irst is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on"
2006.amta-papers.11,P06-1002,0,0.142402,"aluation metrics for MT is longstanding and beyond the scope of this paper. The question of alignment metrics is actually closer to the problem at hand. Assuming that we have decided upon a satisfactory translation metric, one possible approach would be to optimize our alignment for different alignment metrics, in order to see which one best correlates with the final MT metric. A slightly different approach would be to create a parameterized alignment metric, and tune its parameters for MT output performance using logistic regression or similar techniques. Some of these issues are explored by Ayan and Dorr (2006) and Fraser and Marcu (2006). In this paper, we do not address the issues of specific metrics. Our experiments address the issue of alignment quality directly by using alignments whose qualitative rankings are consistent across all metrics. 3. The answer could be the obvious one: phrasebased translation is simply insensitive to the quality of the underlying alignment. It may simply be that the quality of word alignment links does not significantly impact the quality of the extracted phrase tables. The interaction between word alignments and phrase-based translation occurs in the learning step."
2006.amta-papers.11,H05-1009,0,0.0768425,"then the ith source word is aligned to the jth target word. Intrinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show"
2006.amta-papers.11,H05-1012,0,0.066911,"task is to output a target sentence which conveys the same meaning. In the process, the system produces a mapping between words of the source sentence and target sentence. In the alignment problem, we are given both sentences, and the task is simply to find this mapping. Alignment can therefore be thought of as a 90 Proceedings of the 7th Conference of the Association for Machine Translation in the Americas, pages 90-99, Cambridge, August 2006. ©2006 The Association for Machine Translation in the Americas Citation Koehn et al. (2003) 1 Callison-Burch et al. (2004) Callison-Burch et al. (2004) Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney"
2006.amta-papers.11,J96-1002,0,0.00904826,"Missing"
2006.amta-papers.11,N03-1017,0,0.572947,"urce and target sentences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a wor"
2006.amta-papers.11,W06-3123,0,0.00589422,"unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on alignments between ph"
2006.amta-papers.11,koen-2004-pharaoh,0,0.349616,"properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of log-linear feature weights, we use minimum error rate training (Och, 2003). The baseline translation model we consider in the following sections has eight features, following the example of the phrase-based Pharaoh system (Koehn, 2004). 1. A conditional phrase-to-phrase model that incorporates the probability of each phrase pair used in the derivation D (Equation 1). 2. The inverse conditional phrase-to-phrase probability model (Equation 2). 3. A lexical weighting feature (Equation 3). This feature operates over word alignments within phrase pairs. 4. The inverse lexical weighting (Equation 4). 5. A trigram language model feature. 6. A distortion count feature (Marcu and Wong, 2002; Koehn et al., 2003). 7. A feature counting the number of phrase pairs used in the translation. 8. A feature counting the number of target words"
2006.amta-papers.11,H01-1033,1,0.765317,"(AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e"
2006.amta-papers.11,W02-1018,0,0.170098,"to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine tra"
2006.amta-papers.11,H05-1095,0,0.0169738,"ining data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments ar"
2006.amta-papers.11,1996.amta-1.13,0,0.023964,"ent error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair"
2006.amta-papers.11,H05-1011,0,0.011763,"rget word. Intrinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input align"
2006.amta-papers.11,W04-3207,0,0.0132889,"Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e1 ...eJ ) in which the source and target sentences contain I and J words, respectively. The alignment A"
2006.amta-papers.11,C00-2163,0,0.179259,"ukos (2005) 2 Ittycheriah and Roukos (2005) 2 Ittycheriah and Roukos (2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and"
2006.amta-papers.11,W04-3201,0,0.0199762,"iments are not necessarily comparable. AER is sensitive to annotation differences, and in particular to the presence or absence of probable links (Section 2). For a thorough explanation refer to Fraser and Marcu (2006). 4 Och and Ney (2000) report a similar relationship between AER and the word error rate metric for translation. 91 links that were uncertain (Och and Ney, 2000).5 Given the set of hypothesized alignment links A, we compute the standard metrics precision (P), recall (R), and alignment error rate (AER) as follows: found elsewhere in natural language processing (Ratnaparkhi, 1996; Taskar et al., 2004), translation models typically use a small feature space in which all features are active, and have non-integer values. These features are estimated using maximum likelihood methods. While the former approach has attractive properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of"
2006.amta-papers.11,J03-1002,0,0.0674202,"in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained using IBM Model 1 and IBM Model 4. The difference in performance of these two methods is known to be large; under similar conditions in a German-English evaluation the difference in AER was reported to be 9.3 absolute (Och and Ney, 2003). 2 The results in Ittycheriah and Roukos (2005) are reported in terms of alignment F-score. However, they point out that because their evaluation data for alignment contained only sure links (Section 2), we can obtain alignment error rate simply by subtracting the F-score from 1. We have done this here. 3 It should be noted that the AER numbers reported in these experiments are not necessarily comparable. AER is sensitive to annotation differences, and in particular to the presence or absence of probable links (Section 2). For a thorough explanation refer to Fraser and Marcu (2006). 4 Och and"
2006.amta-papers.11,H05-1010,0,0.0153172,"trinsic evaluation is performed by comparison of the alignment set A with alignments created by human annotators. Annotations may contain two sets of links: the sure set S, containing only links about which all annotators are certain, and the probable set P, which includes all links in S as well as Because the heuristic approach depends on a word alignment, it is often assumed that the quality of the word alignment is critical to its success. A number of recent word alignment methods achieve impressive results on extrinsic metrics (Ayan et al., 2005; Ittycheriah and Roukos, 2005; Moore, 2005; Taskar et al., 2005). Often, it is implied that these improvements will propagate to a downstream translation system. However, several recent papers have reported that large gains in alignment accuracy often lead to, at best, minor gains in translation performance. Some examples are listed in Table 1. These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained us"
2006.amta-papers.11,W99-0604,0,0.18997,"gnments between source and target sentences, the training data must contain examples of these alignment decisions. However, the data available for training translation systems contains only pairs of sentences. There are two solutions to this problem. The first is to treat the unseen alignments as a hidden input and apply unsupervised learning methods. The second is to first solve the easier problem of alignment, and then use supervised methods for learning. Current state-of-the-art models in machine translation are based on alignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we"
2006.amta-papers.11,2005.eamt-1.36,0,0.010191,"tease apart the effects of the two 8 Because the BLEU score is computed using aggregate statistics over the output, the locally best output for any given input sentence is not necessarily the one that results in the best overall BLEU score (indeed, due to BLEU’s use of a geometric average, most single sentences turn out to have a BLEU score of 0, which is not very useful even for determining the locally optimal sentence). Computing the choice of sentences which results in the best global BLEU is an intractable search problem, so we resorted to a greedy hill-climbing search (Och et al., 2004; Venugopal and Vogel, 2005). This works as follows: we first choose for each input sentence an output that maximizes a local non-zero approximation to BLEU. We then iterate over our input sentences and at each step choose a new output from the 1000-best list that optimizes the global BLEU score while holding all the other outputs constant. This is repeated until no further gains in BLEU score can be found. 95 Alignment Quality Best Slightly Degraded Moderately Degraded Highly Degraded BLEU score Dev Test .293 .272 .289 .268 .278 .262 .274 .252 Alignment Quality Best Slightly Degraded Moderately Degraded Highly Degraded"
2006.amta-papers.11,H01-1035,0,0.02398,"red using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2002; Smith and Smith, 2004; Hwa et al., 2005). For this reason, it is a topic of significant study in its own right. For translation, the only truly important metric is the translation metric. However, since word alignment has uses outside of learning translation models, many word alignment studies report results using intrinsic metrics, which we briefly review here. Formally, we say that the objective of the word alignment task is to discover the word-to-word correspondences in a sentence pair (F = f1 ... fI , E = e1 ...eJ ) in which the source and target sentences contain I"
2006.amta-papers.11,P03-1021,0,0.0377431,"h all features are active, and have non-integer values. These features are estimated using maximum likelihood methods. While the former approach has attractive properties and can be optimized directly for translation accuracy metrics, as has been shown for other NLP tasks, it is very slow (Taskar et al., 2004). Maximum likelihood probabilistic estimation is much faster, and for this reason it is very attractive for machine translation, where very large corpora are used, and efficiency is at a premium. To train the small number of log-linear feature weights, we use minimum error rate training (Och, 2003). The baseline translation model we consider in the following sections has eight features, following the example of the phrase-based Pharaoh system (Koehn, 2004). 1. A conditional phrase-to-phrase model that incorporates the probability of each phrase pair used in the derivation D (Equation 1). 2. The inverse conditional phrase-to-phrase probability model (Equation 2). 3. A lexical weighting feature (Equation 3). This feature operates over word alignments within phrase pairs. 4. The inverse lexical weighting (Equation 4). 5. A trigram language model feature. 6. A distortion count feature (Marc"
2006.amta-papers.11,I05-3011,0,0.0354292,"ignments between phrases – sequences of words within each sentence (Och et al., 1999; Koehn et al., 2003; Chiang, 2005; Simard et al., 2005). Unfortunately, unsupervised learning of phrase-based models is intractable. It requires numerous approximations and tradeoffs, and often produces poor results (Marcu and Wong, 2002; DeNero et al., 2006; Birch et al., 2006). Therefore, supervised methods are usually employed (Och et al., 1999; Koehn et al., 2003; Chiang, 2005). This requires a method for phrase alignment. However, there are only a few examples of research on the phrase alignment problem (Zhao and Waibel, 2005). An alternative is to first generate word alignments as if we were training a word-based system. Phrase alignments are then inferred heuristically from these word alignments. This approach was first described by Och et al. (1999) and later explored in some deState-of-the-art statistical machine translation is based on alignments between phrases – sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results sugge"
2006.amta-papers.11,P02-1040,0,0.105407,"(2005) 2 MT Test Corpus Europarl German Verbmobil German Hansards French MT-Eval 2003 Arabic MT-Eval 2004 Arabic MT-Eval 2005 Arabic worst * 12.17 16.59 23.7 23.7 23.7 AER best * 7.52 13.55 12.2 12.2 12.2 diff * 4.6 3.0 11.5 11.5 11.5 worst 24.5 27.0 12.6 45.9 41.9 45.6 BLEU best 25.2 28.2 12.8 47.9 43.3 46.5 diff 0.7 1.2 0.2 2.0 1.4 0.9 Table 1: The impact of alignment performance on machine translation performance as reported in several recent studies. Alignment performance is measured using the alignment error rate (AER) (Och and Ney, 2000).3 Translation performance is measured using BLEU (Papineni et al., 2002).4 2 Word-Based Alignment tail by Koehn et al. (2003). It has since been widely adopted. Word alignment originated in the training step of word-based translation models (Brown et al., 1993). In these models, the units of correspondence between sentences are individual words, and so word alignment corresponds exactly to the translation model. Over the past decade, a number of additional uses have been found for it, including the automatic acquisition of bilingual dictionaries (e.g. (Melamed, 1996; Resnik et al., 2001)) and cross-lingual syntactic learning (Yarowsky et al., 2001; Lopez et al., 2"
2006.amta-papers.11,W96-0213,0,\N,Missing
2006.amta-papers.11,J93-2003,0,\N,Missing
2006.amta-papers.11,J07-3002,0,\N,Missing
2006.amta-papers.11,N04-1021,0,\N,Missing
2008.amta-papers.13,J07-2003,0,0.0414222,"oes not depend heavily on such overlap? Answering these questions will make it possible to characterize the utility of paraphrase-based optimization in real-world scenarios, and and how best to leverage it in those scenarios where it does prove useful. 3 Research Questions Paraphrasing Model We generate sentence-level paraphrases via Englishto-English translation using phrase table pivoting, following (Madnani et al., 2007). The translation system we use (for both paraphrase generation and translation) is based on a state-of-the-art hierarchical phrase-based translation model as described in (Chiang, 2007). English-to-English hierarchical phrases are induced using the pivot-based technique proposed in (Bannard and Callison-Burch, 2005) with primary features similar to those used by (Madnani et al., 2007): the joint probability p(e¯1 , e¯2 ), the two conditionals p(e¯1 |e¯2 ) & p(e¯2 |e¯1 ) and the target length. To limit noise during pivoting, we only keep the top 20 paraphrase pairs resulting from each pivot, as determined by the induced fractional counts. Furthermore, we pre-process the source to identify all named entities using BBN IdentiFinder (Bikel et al., 1999) and strongly bias our dec"
2008.amta-papers.13,C04-1051,0,0.191571,"Missing"
2008.amta-papers.13,W04-3250,0,0.0429334,"he paraphraser only on a subset—1 million sentences—instead of the full set. • We use a 1-3 split of the 4 reference translations from the NIST MT02 test set to tune the feature weights for the paraphraser similar to Madnani et al. (2007). • No changes are made to the number of references in any validation set. Only the tuning sets differed in the number of references across different experiments. • BLEU and TER are calculated on lowercased translation output. Brevity penalties for BLEU are indicated if not equal to 1. • For each experiment, BLEU scores shown in bold are significantly better (Koehn, 2004) than the appropriate baselines for that experiment (p &lt; 0.05). 4.1 Table 1: BLEU and TER scores are shown for MT04+05. 1H=Tuning with 1 human reference, 1H+1P=Tuning with the human reference and its paraphrase. Lower TER scores are better. BLEU TER 1H 37.65 56.39 1H+1P 39.32 54.39 Single Reference Datasets In this section, we attempt to gauge the utility of the paraphrase approach in a realistic scenario where only a single reference translation is available for the tuning set. We use the NIST MT03 data, which has four references per development item, to simulate a tuning set in which only a"
2008.amta-papers.13,A00-2023,0,0.0610845,"Missing"
2008.amta-papers.13,W07-0716,1,0.921324,"a for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases. 1 In ou"
2008.amta-papers.13,P08-1023,0,0.0319501,"Missing"
2008.amta-papers.13,P03-1021,0,0.0175466,"n Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al"
2008.amta-papers.13,P02-1040,0,0.0870838,"ference, Hawaii, 21-25 October 2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking"
2008.amta-papers.13,2006.amta-papers.25,1,0.830776,"2008] Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization Nitin Madnani§ , Philip Resnik§ , Bonnie J. Dorr§ & Richard Schwartz† § Laboratory for Computational Linguistics and Information Processing § Institute for Advanced Computer Studies § University of Maryland, College Park † BBN Technologies {nmadnani,resnik,bonnie}@umiacs.umd.edu Abstract It is standard practice to tune the feature weights in models of this kind in order to maximize a translation quality metric such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), using heldout “development” sentences paired with their corresponding reference translations. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that inv"
2008.amta-papers.13,strassel-etal-2006-integrated,0,0.0152588,"ns. Och (2003) showed that system achieves its best performance when the model parameters are tuned using the same objective function being used for evaluating the system. However, this reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. For example, producing reference translations at the Linguistic Data Consortium, a common source of translated data for MT research, requires undertaking an elaborate process that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using heldout test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases"
2010.amta-workshop.3,W04-1408,0,0.0272354,"quality. 1 Introduction For most of the world’s languages, the availability of translation is limited to two possibilities: high quality at high cost, via professional bilingual translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with nonprofessional translators, which can be remarkably Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. In this presentation, we discuss a novel use of crowdsourcing that makes it possible to explore the middle ground. We take"
2010.amta-workshop.3,D10-1041,0,0.0624391,"f options including communities of volunteers. The availability question is closely tied to the question of cost. Again, the crowdsourcing world is facing questions of how much to pay people, in terms of both getting the job done and ethical compensation. Once you start thinking about the process differently, we can ask ourselves how does one change the technology to take best advantage of it? Right now, we’re using the MT system as a black box, but there are clearly avenues one can take to do better. On the MT side, a clear avenue to explore is paraphrase lattices based on human paraphrases (Du et al., 2010; Dyer et al., 2008). On the human side, there is exploring a wider space of contributions that monolingual human participants can make while participating in the process (Bederson et al., 2010). There are surely other limitations to our approach and ideas that need more exploration. And it is key that they be explored. Yes, a minority of the community has worked for years on human assisted machine translation or machine assisted human translation (e.g. (Bowker and Barlow, 2004; Esteban et al., 2004; Khadivi et al., 2006; Laurian, 1984) and others), but it’s time to broaden that conception bye"
2010.amta-workshop.3,P08-1115,1,0.591695,"ng communities of volunteers. The availability question is closely tied to the question of cost. Again, the crowdsourcing world is facing questions of how much to pay people, in terms of both getting the job done and ethical compensation. Once you start thinking about the process differently, we can ask ourselves how does one change the technology to take best advantage of it? Right now, we’re using the MT system as a black box, but there are clearly avenues one can take to do better. On the MT side, a clear avenue to explore is paraphrase lattices based on human paraphrases (Du et al., 2010; Dyer et al., 2008). On the human side, there is exploring a wider space of contributions that monolingual human participants can make while participating in the process (Bederson et al., 2010). There are surely other limitations to our approach and ideas that need more exploration. And it is key that they be explored. Yes, a minority of the community has worked for years on human assisted machine translation or machine assisted human translation (e.g. (Bowker and Barlow, 2004; Esteban et al., 2004; Khadivi et al., 2006; Laurian, 1984) and others), but it’s time to broaden that conception byeond ”assistance”. Wh"
2010.amta-workshop.3,P04-3001,0,0.0688619,"Missing"
2010.amta-workshop.3,P06-2061,0,0.0207171,"mited to two possibilities: high quality at high cost, via professional bilingual translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with nonprofessional translators, which can be remarkably Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. In this presentation, we discuss a novel use of crowdsourcing that makes it possible to explore the middle ground. We take advantage of a virtually unutilized resource for translation: speakers who know only the sour"
2010.amta-workshop.3,P84-1051,0,0.538394,"in translation quality. 1 Introduction For most of the world’s languages, the availability of translation is limited to two possibilities: high quality at high cost, via professional bilingual translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with nonprofessional translators, which can be remarkably Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. In this presentation, we discuss a novel use of crowdsourcing that makes it possible to explore"
2010.amta-workshop.3,D10-1064,0,\N,Missing
2010.amta-workshop.3,W10-0735,1,\N,Missing
2010.amta-workshop.3,W09-0431,0,\N,Missing
2010.amta-workshop.3,W09-2503,0,\N,Missing
2010.amta-workshop.3,H01-1035,0,\N,Missing
2010.amta-workshop.3,E09-1008,0,\N,Missing
2010.amta-workshop.3,P10-1063,0,\N,Missing
2010.amta-workshop.3,P09-4005,0,\N,Missing
2010.amta-workshop.3,D09-1030,0,\N,Missing
2010.amta-workshop.3,P05-1074,0,\N,Missing
2010.amta-workshop.3,W07-0729,0,\N,Missing
2010.amta-workshop.3,N04-1021,0,\N,Missing
2020.acl-main.29,D16-1211,0,0.0649997,"Missing"
2020.acl-main.29,Q17-1010,0,0.0302678,"label case. 4.5 Model Setup For each task and dataset, we use the same set of hyperparameters: Adam optimizer (Kingma and Ba, 2015) with learning rate 0.001 and weight decay 0.9. Dropout (Srivastava et al., 2014) is applied after each layer except the final classification layers; we use a single dropout probability of 0.1 for every instance. For models with exploration, we employ teacher forcing for 10 epochs. Model weights are initialized using Xavier normal initialization (Glorot and Bengio, 2010). All LSTM hidden-layer sizes are set to 200. We use fixed 300-dimensional FastText embeddings (Bojanowski et al., 2017) for both English and German, and project them down to 200 dimensions using a trainable linear layer. 5 Results and Analysis There are five major takeaways from the experimental results and analysis. First, the jointly trained S-LSTM model shows major improvement over prior work that modeled document segmentation and segment labeling tasks separately. Second, segment alignment and exploration during training reduces error rates. Third, the segment pooling layer leads to improvements for both segmentation and segment labeling. Fourth, S-LSTM outperforms an IOB-tagging CRF-decoded model for sing"
2020.acl-main.29,A00-2004,0,0.711231,"two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation that the segment bounds and topicality are tightly interwoven, and should ideally be considered jointly rather than sequentially. We start by examining three properties about text segmentation: (1) segment bounds and segment labels contain co"
2020.acl-main.29,N19-1423,0,0.0265637,"ls on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a single-label labeling task. 3 Mo"
2020.acl-main.29,D08-1035,0,0.852696,"ng a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivat"
2020.acl-main.29,S16-2016,0,0.785267,"pics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation that the segment bounds a"
2020.acl-main.29,P96-1024,0,0.149931,"Missing"
2020.acl-main.29,J97-1003,0,0.913733,"lem because it requires solving a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perf"
2020.acl-main.29,P18-1031,0,0.0202187,"(Section 3.2), and a segment pooling network which pools over predicted segments to classify them (Section 3.3). The segment predictor is allowed to make mistakes that the labeler must learn to be robust to, a process which we refer to as exploration, and accomplish by aligning predicted and ground truth segments (Section 3.4). The full architecture is presented in Figure 1, and the loss is discussed in Section 3.5. 3.1 Encoding Sentences The first stage is encoding sentences. S-LSTM is agnostic to the choice of sentence encoder, though in this work we use a concat pooled bi-directional LSTM (Howard and Ruder, 2018). First, the embedded words are passed through the LSTM encoder. Then, the maximum and mean of all hidden states are concatenated with the final hidden states, and this is used as the sentence encoding. 3.2 Predicting Segment Bounds The second step of our model is a Segment Predictor LSTM, which predicts segment boundaries within the document. For this step we use a bidirectional LSTM that consumes each sentence vector and predicts an indicator variable, (B)eginning or (I)nside a segment. It is trained from presegmented documents using a binary cross entropy loss. This indicator variable deter"
2020.acl-main.29,N18-2075,0,0.226637,"on (Swaffar et al., 1991; Ajideh, 2003). ? ∗ Work done while interning at Adobe. Uncovering latent, topically coherent segments of text is a difficult problem because it requires solving a chicken-and-egg problem: determining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models ("
2020.acl-main.29,N16-1030,0,0.20192,"y, evaluating the pretrained models on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a"
2020.acl-main.29,W03-0430,0,0.0365605,"nt technique to assign partial credit to labels of incorrect segmentations, both for 314 training and evaluation. In addition, we explicitly consider the problem of model transferability, evaluating the pretrained models on additional datasets. IOB Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the lab"
2020.acl-main.29,P12-1009,1,0.810565,"s of coherence to find topic shifts in documents. Hearst (1997) introduced the TextTiling algorithm, which uses term co-occurrences to find coherent segments in a document. Eisenstein and Barzilay (2008) introduced BayesSeg, a Bayesian method that can incorporate other features such as cue phrases. Riedl and Biemann (2012) later introduced TopicTiling, which uses coherence shifts in topic vectors to find segment bounds. Glavaš et al. (2016) proposed GraphSeg, which constructs a semantic relatedness graph over the document using lexical features and word embeddings, and segments using cliques. Nguyen et al. (2012) proposed SITS, a model for topic segmentation in dialogues that incorporates a per-speaker likelihood to change topics. While the above models are unsupservised, Arnold et al. introduced a supervised method to compute sentence-level topic vectors using Wikipedia articles. The authors created the WikiSection dataset and proposed the SECTOR neural model. The SECTOR model predicts a label for each sentence, and then performs post hoc segmentation looking at the coherence of the latent sentence representations, addressing segmentation and labeling separately. We propose a model capable of jointly"
2020.acl-main.29,P03-1021,0,0.297244,"Missing"
2020.acl-main.29,N18-1202,0,0.0122959,"Tagging. The problem of jointly learning to segment and classify is well-studied in NLP, though largely at a lower level, with Inside-OutsideBeginning (IOB) tagging (Ramshaw and Marcus, 1999). Conditional random field (CRF) decoding has long been used with IOB tagging to simultaneously segment and label text, e.g. for named entity recognition (NER, McCallum and Li, 2003). The models that perform best at joint segmentation/classification tasks like NER or phrase chunking were IOB tagging models, typically LSTMs with a CRF decoder (Lample et al., 2016) until BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018). Tepper et al. (2012) proposed the use of IOB tagging to segment and label clinical documents, but argued for a pipelined approach. CRF-decoded IOB tagging models are more difficult to apply to the multilabel case. Segment bounds need to be consistent across all labels, so modeling the full transition from |L |−→ |L| (where |L |is the size of the label space) at every time step is computationally expensive. In contrast, our joint model performs well at multilabel prediction, while also outperforming a neural CRFdecoded model on a single-label labeling task. 3 Modeling In order to jointly mode"
2020.acl-main.29,W12-3307,0,0.742529,"etermining the segment topics is easier if segment boundaries are given, and identifying the boundaries of segments is easier if the topic(s) addressed in parts of the document are known. Prior approaches to text segmentation can largely be split into two categories that break the cycle by sequentially solving the two problems: those that attempt to directly predict segment bounds (Koshorek et al., 2018), and those that attempt to predict topics per passage (e.g., per sentence) and use measures of coherence for post hoc segmentation (Hearst, 1997; Arnold et al.; Eisenstein and Barzilay, 2008; Riedl and Biemann, 2012; Glavaš et al., 2016). The benefit of the topic modeling approach is that it can work in unsupervised settings where collecting ground truth segmentations is difficult and labeled data is scarce (Eisenstein and Barzilay, 2008; Choi, 2000). Recent work uses Wikipedia as a source of segmentation labels by eliding the segment bounds of a Wikipedia article to train supervised models (Koshorek et al., 2018; Arnold et al.). This enables models to directly learn to predict segment bounds or to learn sentence-level topics and perform post hoc segmentation. Our work is motivated by the observation tha"
2020.acl-main.29,tepper-etal-2012-statistical,0,0.449585,"o learn text segmentation as a supervised task. However, learning only to predict segment bounds does not necessarily capture the topicality of a segment that is useful for informative labeling. The task of document segmentation and labeling is well-studied in the clinical domain, where both segmenting and learning segment labels are important tasks. Pomares-Quimbaya et al. (2019) provide a current overview of work on clinical segmentation. Ganesan and Subotin (2014) trained a logistic regression model on a clinical segmentation task, though they did not consider the task of segment labeling. Tepper et al. (2012) considered both tasks of segmentation and segment labeling, and proposed a two-step pipelined method that first segments and then classifies the segments. Our proposed model is trained jointly on both the segmentation and segment labeling tasks. Concurrent work considers the task of document outline generation (Zhang et al., 2019). The goal of outline generation is to segment and generate (potentially hierarchical) headings for each segment. The authors propose the HiStGen model, a hierarchical LSTM model with a sequence decoder. The work offers an alternative view of the joint segmentation a"
2020.acl-main.723,W18-2501,0,0.0301728,"Missing"
2020.acl-main.723,D15-1162,0,0.0500021,"Missing"
2020.acl-main.723,N19-1357,0,0.0624915,"tention Attention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a"
2020.acl-main.723,W16-0312,0,0.135864,"07) and the rise of research on mental 1 Approximately: ACL is international, but these figures use prevalence statistics for U.S. adults (SAMHSA, 2019). Douglas W. Oard iSchool/UMIACS University of Maryland College Park, MD oard@umd.edu health using social media (Choudhury, 2013), algorithmic classification has reached the point where it can now dramatically outstrip performance of prior, more traditional prediction methods (Linthicum et al., 2019; Coppersmith et al., 2018). Further progress is on the way as the community shows increasing awareness and enthusiasm in this problem space (e.g., Milne et al., 2016; Losada et al., 2020; Zirikly et al., 2019). The bad news is that moving these methods from the lab into practice will create a major new challenge: identifying larger numbers of people who may require clinical assessment and intervention will increase stress on a severely resource-limited mental health ecosystem that cannot easily scale up.2 This motivates a reformulation of the technological problem from classification to prioritization of individuals who might be at risk, for clinicians or other suitably trained staff as downstream users. Perhaps the most basic way to do prioritization is"
2020.acl-main.723,N18-1100,0,0.0146865,"the downstream users’ prioritization task as taking a key step closer to the real-world problem. Hierarchical Attention Attention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a d"
2020.acl-main.723,D14-1162,0,0.0904087,"Missing"
2020.acl-main.723,D19-1002,0,0.0981771,"ttention, especially in the context of NLP, has two main advantages: it allows the network to attend to likely-relevant parts of the input (either words or sentences), often leading to improved performance, and it provides insight into which parts of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a"
2020.acl-main.723,N16-1174,0,0.137332,"of the input are being used to make the prediction. These characteristics have made attention mechanisms a popular choice for deep learning that requires human investigation, such as automatic clinical coding (Baumel et al., 2018; Mullenbach et al., 2018; Shing et al., 2019). Although concerns about using attention for interpretation exist (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019; Wallace, 2019), Shing et al. (2019) show hierarchical document attention can align well with human-provided ground truth. Our prediction model, 3HAN, is a variant of Hierarchical Attention Networks (HAN, Yang et al., 2016). Yang et al. use a two-level attention mechanism that learns to pay attention to specific words in a sentence to form a sentence representation, and at the next higher level to weight specific sentences in 8125 a document in forming a document representation. Adapting this approach to suicide assessment of at-risk individuals, our model moves a level up the representational hierarchy, learning also to weight documents to form representations of individuals. This allows us to jointly model ranking individuals and ranking their documents as potentially relevant evidence, without document-level"
2020.acl-main.723,D17-1322,0,0.446084,"d ** .. Figure 1: Illustration of an assessment framework in which individuals are ranked by predicted suicide risk based on social media posts, posts are ranked by expected usefulness for downstream review by a clinician, and word-attention highlighting helps foreground important information for risk assessment. Real Reddit posts, obfuscated and altered for privacy. Biased Gain (TBG, Smucker and Clarke, 2012), an IR evaluation measure that models the expected number of relevant items a user can find in a ranked list given a time budget. We observe that in many risk assessment settings (e.g., Yates et al. (2017); Coppersmith et al. (2018); Zirikly et al. (2019)), the available information comprises a (possibly large and/or longitudinal) set of documents, e.g. social media posts, associated with each individual, of which possibly only a small number contain a relevant signal.3 This gives rise to a formulation of our scenario as a nested, or hierarchical, ranking problem, in which individuals are ordered by priority, but each individual’s documents must also be ranked (Figure 1). Accordingly, we introduce hierarchical Time-Biased Gain (hTBG), a variant of TBG in which individuals are the top level rank"
2020.acl-main.723,W19-3003,1,0.83684,"Approximately: ACL is international, but these figures use prevalence statistics for U.S. adults (SAMHSA, 2019). Douglas W. Oard iSchool/UMIACS University of Maryland College Park, MD oard@umd.edu health using social media (Choudhury, 2013), algorithmic classification has reached the point where it can now dramatically outstrip performance of prior, more traditional prediction methods (Linthicum et al., 2019; Coppersmith et al., 2018). Further progress is on the way as the community shows increasing awareness and enthusiasm in this problem space (e.g., Milne et al., 2016; Losada et al., 2020; Zirikly et al., 2019). The bad news is that moving these methods from the lab into practice will create a major new challenge: identifying larger numbers of people who may require clinical assessment and intervention will increase stress on a severely resource-limited mental health ecosystem that cannot easily scale up.2 This motivates a reformulation of the technological problem from classification to prioritization of individuals who might be at risk, for clinicians or other suitably trained staff as downstream users. Perhaps the most basic way to do prioritization is with a single priority queue that the user s"
2020.acl-main.723,W18-0603,1,0.887191,"Missing"
2020.emnlp-main.137,D18-1096,0,0.0823865,"Missing"
2020.emnlp-main.137,L16-1540,0,0.0214177,"e subjective differences in topic quality. So that readers may form their own judgments, Appendix G presents 15 aligned pairs for each corpus, selected randomly by stratifying across levels of alignment quality to create a fair sample to review. 6 Related Work Integrating embeddings into topic models. A key goal in our use of knowledge distillation is to incorporate relationships between words that may not be well supported by the topic model’s input documents alone. Some previous topic models have sought to address this issue by incorporating external word information, including word senses (Ferrugento et al., 2016) and pretrained word embeddings (Hu and Tsujii, 2016; Yang et al., 2017; Xun et al., 2017; Ding et al., 2018). More recently, Bianchi et al. (2020) have incorporated BERT embeddings into the encoder to improve 1758 NPMI Topic 20ng S CHOLAR S CHOLAR+BAT 0.454 0.523 nhl hockey player coach ice playoff team league stanley european nhl hockey player team coach playoff cup wings stanley leafs Wiki S CHOLAR S CHOLAR+BAT 0.547 0.621 jtwc jma typhoon monsoon luzon geophysical pagasa guam cyclone southwestward jtwc jma typhoon meteorological intensification monsoon dissipating shear outflow trough IMDb"
2020.emnlp-main.137,2020.acl-main.740,0,0.0472749,"Missing"
2020.emnlp-main.137,N19-1112,0,0.146555,"d from large quantities of data using unsupervised objectives, can serve as a foundation for more specialized endeavors. Current practice involves taking the full model that has amassed such general knowledge and fine-tuning it with a second objective appropriate to the new task (see Raffel et al., 2019, for an overview). Using these methods, pre-trained transformer-based language models (e.g., BERT, Devlin et al., 2019) have been employed to great effect on a wide variety of NLP problems, thanks, in part, to a fine-grained ability to capture aspects of linguistic context (Clark et al., 2019; Liu et al., 2019; Rogers et al., 2020). However, this paradigm introduces a subtle but insidious limitation that becomes evident when the downstream application is a topic model. A topic model may be cast as a (stochastic) autoencoder (Miao et al., 2016), and we could fine-tune a preEqual contribution. Marcel Duchamp was a painter, sculptor, chess player, and writer whose work is associated with Cubism, Dada, and conceptual art. BoW Introduction ∗ Philip Resnik Linguistics / UMIACS University of Maryland College Park, MD resnik@umd.edu BAT Base neural topic model θd B · Figure 1: Improving a base neural topic"
2020.emnlp-main.137,P16-2062,0,0.0176735,"s may form their own judgments, Appendix G presents 15 aligned pairs for each corpus, selected randomly by stratifying across levels of alignment quality to create a fair sample to review. 6 Related Work Integrating embeddings into topic models. A key goal in our use of knowledge distillation is to incorporate relationships between words that may not be well supported by the topic model’s input documents alone. Some previous topic models have sought to address this issue by incorporating external word information, including word senses (Ferrugento et al., 2016) and pretrained word embeddings (Hu and Tsujii, 2016; Yang et al., 2017; Xun et al., 2017; Ding et al., 2018). More recently, Bianchi et al. (2020) have incorporated BERT embeddings into the encoder to improve 1758 NPMI Topic 20ng S CHOLAR S CHOLAR+BAT 0.454 0.523 nhl hockey player coach ice playoff team league stanley european nhl hockey player team coach playoff cup wings stanley leafs Wiki S CHOLAR S CHOLAR+BAT 0.547 0.621 jtwc jma typhoon monsoon luzon geophysical pagasa guam cyclone southwestward jtwc jma typhoon meteorological intensification monsoon dissipating shear outflow trough IMDb S CHOLAR S CHOLAR+BAT 0.197 0.218 adaptation versio"
2020.emnlp-main.137,D16-1139,0,0.0169746,"orpus-dependent information, by way of the pretraining and fine-tuning regime. By regularizing toward representations conditioned on the document, we remain coherent relative to the topic model data. An additional key advantage for our method is that it involves only a slight change to the underlying topic model, rather than the specialized designs by the above methods. Knowledge distillation. While the focus was originally on single-label image classification, KD has also been extended to the multi-label setting (Liu et al., 2018b). In NLP, KD has usually been applied in supervised settings (Kim and Rush, 2016; Huang et al., 2018; Yang et al., 2020), but also in some unsupervised tasks (usually using an unsupervised teacher for a supervised student) (Hu et al., 2020; Sun et al., 2020). Xu et al. (2018) use word embeddings jointly learned with a topic model in a procedure they term distillation, but do not follow the method from Hinton et al. (2015) that we employ (instead opting for joint-learning). Recently, pretrained models like BERT have offered an attractive choice of teacher model, used successfully for a variety of tasks such as sentiment classification and paraphrasing (Tang et al., 2019a,b"
2020.emnlp-main.137,E14-1056,0,0.0614573,"Maas et al., 2011).9 These are commonly used in neural topic modeling, with preprocessed versions provided by various authors; see references in Table 1 for details. For consistency with prior work, we use a train/dev/test split of 48/12/40 for 20NG, 70/15/15 for Wiki, and 50/25/25 for IMDb.10 We seek to discover a latent space of topics that is meaningful and useful to people (Chang et al., 2009). Accordingly, we evaluate topic coherence using normalized mutual pointwise information (NPMI), which is significantly correlated with human judgments of topic quality (Aletras and Stevenson, 2013; Lau et al., 2014) and widely used to evaluate topic models.11 We follow precedent and calculate (internal) NPMI using the top ten words in each topic, taking the mean across the NPMI scores for individual topics. Internal NPMI is estimated with reference co-occurrence counts from a held-out dataset from the same corpus, 5 D ISTIL BERT’s light weight accommodates longer documents, necessary for topic modeling. Even with this change, we divide very long documents into chunks, estimating logits for each chunk and taking the pointwise mean. More complex schemes (i.e., LSTMs, Hochreiter and Schmidhuber, 1997) yield"
2020.emnlp-main.137,P11-1015,0,0.0386469,"), and average document length (Nd ). working in a classification setting, find that truncating the logits to the top-n classes and assigning uniform mass to the rest improves accuracy. We instead choose the top c Nd , c ∈ R+ logits and assign zero probability to the remaining elements to enforce sparsity. 3 Experimental Setup 3.1 Data and Metrics We validate our approach using three readily available datasets that vary widely in domain, corpus and vocabulary size, and document length: 20 Newsgroups (20NG, Lang, 1995),7 Wikitext-103 (Wiki, Merity et al., 2017),8 and IMDb movie reviews (IMDb, Maas et al., 2011).9 These are commonly used in neural topic modeling, with preprocessed versions provided by various authors; see references in Table 1 for details. For consistency with prior work, we use a train/dev/test split of 48/12/40 for 20NG, 70/15/15 for Wiki, and 50/25/25 for IMDb.10 We seek to discover a latent space of topics that is meaningful and useful to people (Chang et al., 2009). Accordingly, we evaluate topic coherence using normalized mutual pointwise information (NPMI), which is significantly correlated with human judgments of topic quality (Aletras and Stevenson, 2013; Lau et al., 2014) a"
2020.emnlp-main.137,J00-2004,0,0.103039,"S CHOLAR and S CHOLAR+BAT models. Now, we look more closely at the extent to which those improvements are meaningful at the level of individual topics. To do so we directly compare topics discovered by the baseline neural topic model (S CHOLAR) with corresponding topics obtained when that model is augmented with BAT, looking at the NPMIs of the corresponding topics as well as considering them qualitatively. We align the topics in the base and augmented S CHOLAR models using a variation of competitive linking, which produces a greedy approximation to optimal weighted bipartite graph matching (Melamed, 2000). A fully connected weighted bipartite graph is constructed by linking all topic pairs across (but not within) the two models, with the weight for a topic pair being the similarity between their word distributions as measured by Jenson-Shannon (JS) divergence (Wong and You, 1985; Lin, 1991). We pick the pair (ti , tj ) with the lowest JS divergence and add it to the resulting alignment, then remove ti and tj from consideration and iterate until no pairs are left. The resulting aligned topic pairs can then be sorted by their JS divergences to directly compare corresponding topics.15 Fig. 2 show"
2020.emnlp-main.137,P19-1640,0,0.179113,"ribution. We use the former to guide the latter, essentially as if predicting word distributions were a multi-class labeling problem.1 Our approach, which we call BERT-based Autoencoder as Teacher (BAT), obtains best-in-class results on the most commonly used measure of topic coherence, normalized pointwise mutual information (NPMI, Aletras and Stevenson, 2013) compared against recent state-of-the-art-models that serve as our baselines. In order to accomplish this, we adopt neural topic models (NTM, Miao et al., 2016; Srivastava and Sutton, 2017; Card et al., 2018; Burkhardt and Kramer, 2019; Nan et al., 2019, inter alia), 1 An interesting conceptual link here can be found in Latent Semantic Analysis (LSA, Landauer and Dumais, 1997), an early predecessor of today’s topic models. The original discussion introducing LSA has a very autoencoder-like flavor, explicitly illustrating the deconstruction of a collection of sparsely represented documents and the reconstruction of a dense document-word matrix. which use various forms of black-box distributionmatching (Kingma and Welling, 2014; Tolstikhin et al., 2018).2 These now surpass traditional methods (e.g. LDA, Blei, 2003, and variants) in topic coher"
2020.emnlp-main.137,2020.tacl-1.54,0,0.032093,"ities of data using unsupervised objectives, can serve as a foundation for more specialized endeavors. Current practice involves taking the full model that has amassed such general knowledge and fine-tuning it with a second objective appropriate to the new task (see Raffel et al., 2019, for an overview). Using these methods, pre-trained transformer-based language models (e.g., BERT, Devlin et al., 2019) have been employed to great effect on a wide variety of NLP problems, thanks, in part, to a fine-grained ability to capture aspects of linguistic context (Clark et al., 2019; Liu et al., 2019; Rogers et al., 2020). However, this paradigm introduces a subtle but insidious limitation that becomes evident when the downstream application is a topic model. A topic model may be cast as a (stochastic) autoencoder (Miao et al., 2016), and we could fine-tune a preEqual contribution. Marcel Duchamp was a painter, sculptor, chess player, and writer whose work is associated with Cubism, Dada, and conceptual art. BoW Introduction ∗ Philip Resnik Linguistics / UMIACS University of Maryland College Park, MD resnik@umd.edu BAT Base neural topic model θd B · Figure 1: Improving a base neural topic model with knowledge"
2020.emnlp-main.137,D17-1203,1,0.838717,"judgments, Appendix G presents 15 aligned pairs for each corpus, selected randomly by stratifying across levels of alignment quality to create a fair sample to review. 6 Related Work Integrating embeddings into topic models. A key goal in our use of knowledge distillation is to incorporate relationships between words that may not be well supported by the topic model’s input documents alone. Some previous topic models have sought to address this issue by incorporating external word information, including word senses (Ferrugento et al., 2016) and pretrained word embeddings (Hu and Tsujii, 2016; Yang et al., 2017; Xun et al., 2017; Ding et al., 2018). More recently, Bianchi et al. (2020) have incorporated BERT embeddings into the encoder to improve 1758 NPMI Topic 20ng S CHOLAR S CHOLAR+BAT 0.454 0.523 nhl hockey player coach ice playoff team league stanley european nhl hockey player team coach playoff cup wings stanley leafs Wiki S CHOLAR S CHOLAR+BAT 0.547 0.621 jtwc jma typhoon monsoon luzon geophysical pagasa guam cyclone southwestward jtwc jma typhoon meteorological intensification monsoon dissipating shear outflow trough IMDb S CHOLAR S CHOLAR+BAT 0.197 0.218 adaptation version novel bbc version"
2020.nlpcovid19-2.30,E12-1021,0,0.0756012,"Missing"
2020.nlpcovid19-2.30,E14-1056,0,0.0350348,".1 We begin with preprocessing, including conventional steps such as tokenization (including identification of relevant multi-word expressions), lowercasing, removal of stopwords, and down-selection of the vocabulary to high-value words based on frequency and other statistical properties.2 This is followed by creation of multiple initial topic models of differing granularities. We do not optimize the number K of topics automatically, since doing so typically relies on automatic approximations to human judgment such as normalized pointwise mutual information (NPMI, Aletras and Stevenson, 2013; Lau et al., 2014). Rather, we will use human judgments directly by constructing models across a range of K and assessing how promising each model is as a starting point, via a combination of qualitative assessment and by comparing human quality ratings for a random sample of topics as assigned by two independent SMEs. Having selected an initial starting point, the human-in-the-loop process includes drilling down to better understand the model (including, for example, identifying documents that are highly representative of a given topic, or visualizing topic similarity), interleaved with human-feedback operatio"
2020.nlpcovid19-2.30,2020.nlpcovid19-acl.1,0,0.137603,"ic models can produce results that fail to meet the needs of users. We advocate for a set of user-focused desiderata in topic modeling for the COVID-19 literature, and describe an effort in progress to develop a curated topic model for COVID-19 articles informed by subject matter expertise and the way medical researchers engage with medical literature. 1 Introduction The language technology community has been responding in force to the coronavirus pandemic with sustained energy and creativity, and the medical research literature, facilitated by the CORD-19 dataset, is a major hub of activity (Wang et al., 2020). A crucial goal of all this technological effort is to support non-technologist users, namely the medical researchers, clinicians, and policy makers who are involved with the response to COVID19. However, to date there has been little explicit discussion in the technology-development community about what functionalities will actually help these users in the trenches. Zhang et al. (2020) sum things up nicely when they write,“we don’t actually know how our systems . . . can concretely contribute to efforts to tackle the ongoing pandemic until we receive guidance from real users who are engaged"
2020.nlpcovid19-2.30,2020.nlpcovid19-acl.2,0,0.020401,"logy community has been responding in force to the coronavirus pandemic with sustained energy and creativity, and the medical research literature, facilitated by the CORD-19 dataset, is a major hub of activity (Wang et al., 2020). A crucial goal of all this technological effort is to support non-technologist users, namely the medical researchers, clinicians, and policy makers who are involved with the response to COVID19. However, to date there has been little explicit discussion in the technology-development community about what functionalities will actually help these users in the trenches. Zhang et al. (2020) sum things up nicely when they write,“we don’t actually know how our systems . . . can concretely contribute to efforts to tackle the ongoing pandemic until we receive guidance from real users who are engaged in those efforts . . . [The] challenge [is] how to build improved fire-fighting capabilities for tomorrow without bothering those who are trying to fight the fires that already raging in front of us”. In this paper we describe a cross-disciplinary collaborative effort that is intended to help close that gap by developing a curated topic model for COVID-19 medical research literature, inf"
2021.acl-long.126,D19-1290,1,0.78767,", or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First"
2021.acl-long.126,E17-1024,0,0.105542,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,W17-5104,0,0.114145,"simultaneously represents relationships such as relative stance, relative specificity, or whether a claim paraphrases another. We build syntopical graphs by transferring pretrained pairwise models, requiring no additional training data to be annotated. We decompose the problem of viewpoint reconstruction into the subtasks of stance detection and aspect detection, and evaluate the benefits of syntopical graphs — which are a collection-level approach — on both tasks. For stance detection, we use the sentential argumentation mining collection (Stab et al., 2018) and the IBM claim stance dataset (Bar-Haim et al., 2017a). For aspect detection we use the argument frames collection (Ajjour et al., 2019). We treat the graph as an input to: (a) a graph neural network architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields sta"
2021.acl-long.126,2020.emnlp-main.3,0,0.031921,"approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our syntopical graph is to systematically model the salient interactions of all claims in a col"
2021.acl-long.126,P19-3022,0,0.0191178,"order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express: V = (Nuclear Energy, env. impact, PRO) T"
2021.acl-long.126,C04-1051,0,0.324634,"nodes can have multiple edges of different types between them; a claim can both contradict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-cla"
2021.acl-long.126,P19-1456,0,0.0971144,"lated claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also"
2021.acl-long.126,W16-2816,0,0.0149183,"ce- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it woul"
2021.acl-long.126,P19-1049,0,0.0191126,"ose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance ba"
2021.acl-long.126,I13-1191,0,0.0251487,"as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and par"
2021.acl-long.126,2020.emnlp-main.4,0,0.0759605,"Missing"
2021.acl-long.126,W17-5114,0,0.0222256,"rt-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for ed"
2021.acl-long.126,C18-1316,0,0.0233389,"ructure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics, aspects, claims, and participants in a debate. In a similar vein, Li et al. (2018) embedded debate posts and authors jointly based on their interactions, in order to classify a post’s stance towards the debate topic. Durmus et al. (2019) encoded related pairs of claims using BERT to predict the stance and specificity of any claim in a complex structure of online debates. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft lo"
2021.acl-long.126,2021.findings-acl.126,0,0.137173,"(Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the"
2021.acl-long.126,2021.ccl-1.108,0,0.0468629,"Missing"
2021.acl-long.126,N15-1046,0,0.0300103,"s. However, neither of these exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation g"
2021.acl-long.126,2020.emnlp-demos.16,0,0.0599366,"Missing"
2021.acl-long.126,D15-1110,0,0.0280648,". Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to"
2021.acl-long.126,N13-1123,0,0.0269051,"erms (using normalized mutual information for edge weights) as well as sentences and documents (using TF-IDF for edge weights) to improve sentence- or documentlevel classification. Our work generalizes this approach, focusing on incorporating many edge types with different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we inst"
2021.acl-long.126,W13-4008,0,0.0304941,"unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporating implicit and explicit structure induced by the topics,"
2021.acl-long.126,D19-1410,0,0.0877671,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,P19-1054,0,0.105891,"hese exploited the full graph structure resulting from all the relations and interactions in a debate, which is the gap we fill in this paper. Sridhar et al. (2015) model collective information about debate posts, authors, and their agreement and disagreement using probabilistic soft logic. Whereas they are restricted to the structure available in a forum, our approach can in principle be applied to arbitrary collections of text. We also tackle aspect detection, which may at first seem more content-oriented in nature. Accordingly, previous research such as the works of Misra et al. (2015) and Reimers et al. (2019b) employed word-based features or contextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed,"
2021.acl-long.126,2020.coling-main.426,0,0.0250927,", basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures implicit claim relations as well as explicit structure. In addition, text-based graph neural models have been proposed to facilitate classification, such as TextGCN (Yao et al., 2019) as well as the followup work BertGCN (Lin et al., 2021). These approaches build a graph over terms (using normalized mutual information for edge weights) as well as sentences and documents"
2021.acl-long.126,P09-1026,0,0.0669409,"rk architecture for stance detection, and (b) graph algorithms for unsupervised tasks such as aspect clustering. In both settings, our results show that the syntopical graph approach improves significantly over content-only baselines. The contributions of the work are two-fold: 1. A well-motivated data structure for capturing the latent structure of an argumentative corpus, the syntopical graph. 2. An instantiation of syntopical graphs that yields state-of-the-art results on stance detection and aspect detection. 2 Related Work First attempts at stance detection used contentoriented features (Somasundaran and Wiebe, 2009). Later approaches, such as those by Ranade et al. (2013) and Hasan and Ng (2013), exploited common patterns in dialogic structure to improve stance detection. More tailored to argumentation, BarHaim et al. (2017a) first identified the aspects of a discussed topic in two related claims and the sentiment towards these aspects. From this information, they derived stance based on the contrastiveness of the aspects. Later, Bar-Haim et al. (2017b) mod1584 eled the context of a claim to account for cases without sentiment. Our work follows up on and generalizes this idea, systematically incorporatin"
2021.acl-long.126,W16-2814,0,0.0237338,"ntextualized word embeddings for topic-specific aspect clustering. Ajjour et al. (2019), whose argument frames dataset we use, instead clustered aspects with Latent Semantic Analysis (LSA) and topic modeling. But, in general, aspects might not be mentioned in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs."
2021.acl-long.126,2020.argmining-1.5,0,0.0354702,"uage inference model that predicts whether the claim entails the topic. We initialize the document representations with a sentence vectorizer over the text of the document. 4 Viewpoint Reconstruction A viewpoint can be understood as a judgment of some aspect of a topic that conveys a stance towards the topic. The goal of viewpoint reconstruction is to identify the set of viewpoints in a collection given a topic, starting with the claims. An example of this process is shown on the right in Figure 1. To denote viewpoints, we borrow notation in line with the idea of aspect-based argument mining (Trautmann, 2020), which in turn was inspired by aspectbased sentiment analysis. In particular, we express a viewpoint as a triple V : V = (topic, aspect, stance) A claim is an expression of a viewpoint in natural language, and a single viewpoint can be expressed in several ways throughout a collection in many claims. Aspects are facets of the broader argument around the topic. While some actual claims may encode multiple viewpoints simultaneously, henceforth we consider each claim to encode one viewpoint for simplicity. To tackle viewpoint reconstruction computationally, we decompose it into two sub-tasks, st"
2021.acl-long.126,D17-1165,0,0.0242068,"h different meanings, such as relative stance or relative specificity. We compare our approach with a BertGCN baseline, and we ablate all considered edge types, in order to show the importance of capturing these different textual relationships. Ultimately, we seek to facilitate understanding of the main viewpoints in a text collection. Qiu and Jiang (2013) used clustering-based viewpoint discovery to study the impact of the interaction of topics and users in forum discussions. Egan et al. (2016) used multi-document summarization techniques to mine and organize the main points in a debate, and Vilares and He (2017) mined the main topics and their aspects using a Bayesian model. Bar-Haim et al. (2020) introduced the idea of keypoint analysis, grouping arguments found in a collection by the viewpoint they reflect and summarizing each group to a salient keypoint. While our graph-based analysis is likely to be suitable for finding keypoints, we instead focus on reconstructing latent viewpoints by grouping claims, leaving open the option to identify the key claims in future work as it would require manual evaluation. 3 Syntopical Graphs We now introduce the concept of a syntopical graph. The goal of our synt"
2021.acl-long.126,W17-5106,1,0.932597,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,E17-1105,1,0.929104,"yntopical reading process computationally in order to help individuals make sense of a collection of documents for a given topic. Viewed through the lens of computational argumentation, these documents state claims or conclusions that can be grouped by the aspects of the topic they discuss as well as by the stance they convey towards the topic (Stede and Schneider, 2018). An individual aiming to form a thorough understanding of the topic needs to get an overview of these viewpoints and their interactions. This may be hard even if adequate tool support for browsing the collection is available (Wachsmuth et al., 2017a; Stab et al., 2018; Chen et al., 2019). We seek to enable systems that are capable of reconstructing viewpoints within a collection, where a viewpoint is expressed as a triple V = (topic, aspect, stance). We consider the argumentative unit of a claim to be the minimal expression of a viewpoint in natural language, such that a single viewpoint can have many claims expressing it. As an example, consider the following two claims: “Nuclear energy emits zero CO2 .” “Nuclear can provide a clean baseload, eliminating the need for fracking and coal mining.” Within a collection these claims express:"
2021.acl-long.126,N18-1101,0,0.0323565,"adict and refute another claim, for instance. Edge Weights An edge can have a real-valued weight associated with it on the range (−1, 1), representing the strength of the connection. The relative stance edge between a claim which strongly refutes another would receive a weight close to −1. 3.2 Graph Construction For graph edges, we combine four pretrained models and two similarity measures. The pretrained edge types are: relative stance and relative specificity from Durmus et al. (2019), paraphrase edges from Dolan et al. (2004); Morris et al. (2020), and natural language inference edges from Williams et al. (2018); Liu et al. (2019). The edge weights are the confidence scores defined by weight(u, v, r) = ppos(u,v) − pneg(u,v) , where u and v are claims, r is the relation type, and ppos(u,v) is the probability of a positive association between the claims (e.g., “is a paraphrase” or “does entail”), pneg(u,v) for a negative one. For similarity-based edges, we use standard TF-IDF for term-based similarity and LDA for topic-based similarity (Blei et al., 2003), using cosine similarity as the edge weight. The document-claim edges have a single type, contains, with an edge weight of 1. We compute each of the"
2021.acl-long.126,2020.acl-main.291,0,0.0143002,"in a text explicitly. Therefore, we follow these other approaches, treating the task as a clustering problem. Unlike them, however, we do not model only the content and linguistic structure of texts, but we combine them with the debate structure. Different types of argumentation graphs have been proposed, covering expert-stance information (Toledo-Ronen et al., 2016), basic argument and debate structure (Peldszus and Stede, 2015; Gemechu and Reed, 2019), specific effect relations (Al-Khatib et al., 2020; Kobbe et al., 2020), social media graphs (Aldayel and Magdy, 2019), and knowledge graphs (Zhang et al., 2020). Our main focus is not learning to construct ground-truth graphs, but how to use an approximated graph to derive properties such as stance and aspect. Our work resembles approaches that derive the relevance of arguments (Wachsmuth et al., 2017b) or their centrality and divisiveness in a discussion (Lawrence and Reed, 2017) from respective graphs. Sawhney et al. (2020) used a neural graph attention network to classify speech stance based on a graph with texts, speakers, and topics as nodes. While we also use a relational graph convolutional network for learning, the graph we propose captures i"
2021.clpsych-1.7,2021.clpsych-1.8,0,0.0203212,"r character TF-IDF features. For Subtask 1, they also submitted results from a contextualized language model classifier, and, for Subtask 2, a voting ensemble method. SoS (Wang et al., 2021). Team SoS introduced the C-Attention Network, which uses latent feature information implicitly in the embeddings. This was compared with submissions using KNN and SVM classifiers. Latent features included using Doc2vec embeddings (Lau and Baldwin, 2016). Hand-crafted features included emotion lexicons, part-of-speech tags, and a custom dictionary that models various stages of suicidal behavior. UlyaLamia (Bayram and Benhiba, 2021). In the UlyaLamia submissions, the authors were motivated by real-life applicability of their model to use tweet-level classification. The team’s submissions used a majority voting approach over individual tweets. In order to pick which machine learning method to use, the team experimented with multiple methods tuned on the training data using a leave-one-out strategy. Their final submissions were the top methods from the leave-one-out results. 7 Results We evaluated each system in terms of F1 , F2 (favoring recall), True Positive Rate (TPR), False Alarm (Positive) Rate (FAR), and Area Under"
2021.clpsych-1.7,D19-1371,0,0.014976,"ecific fine-tuning. To some extent, pre-training data may capture generalizations about language that transfer well to problems in the mental health space. However, many offthe-shelf language resources that are commonly used, such as BERT (Devlin et al., 2019), are built from sources such as books and Wikipedia entries. These may translate poorly to systems dependent on social media posts from Twitter, Facebook, or an online discussion forum. It is well known that systems perform better when they are trained on materials similar to the materials the system will run on (Alsentzer et al., 2019; Beltagy et al., 2019). Therefore using task-specific data from immediately relevant sources as training data for social media based mental health tasks is a high priority that requires attention. Data sources We began with data donated to the OurDataHelps.org platform, discussed in greater detail by Coppersmith et al. (2018). Donations to the platform include data from people who have survived a suicide attempt, data from people who died by suicide that has been donated by loved ones, and data donated by people who have not attempted suicide but want to help. When donations take place, a questionnaire is filled ou"
2021.clpsych-1.7,W14-3200,0,0.0893255,"chafer et al. (2021) for community-level mental health datasets to be easily available for research so that the predictive ability of models can be compared and research can be replicated. Those kinds of comparisons and replications are instrumental in modern data-driven research because without them it is impossible to gain insight into which approaches are most promising or to rule out the possibility that apparent differences are related to idiosyncratic differences in data. et al., 2018; Thorstad and Wolff, 2019). As two particularly well known and influential examples, Coppersmith et al. (2014) infer mental health diagnoses of Twitter users by looking for publicly self-reported diagnoses, and De Choudhury et al. (2016) infer mental health progressions to suicidal ideation by examining when Reddit users shift from mental health subreddits to the SuicideWatch subreddit. Such data tend to have the advantages of being readily accessible and large in size. However, Ernala et al. (2019) note a variety of problems and limitations in using proxies rather than clinically grounded variables. Coppersmith et al. (2018) offer a rare exception in this kind of work, using an ethical process of dat"
2021.clpsych-1.7,2021.clpsych-1.12,0,0.0665459,"Missing"
2021.clpsych-1.7,W15-1204,1,0.775073,"enefit by taking a predictive approach. The shared task itself involved assessment of suicide risk via prediction of suicide attempts, based on the natural language of users on Twitter. There were two subtasks: Subtask 1 involved assessing suicide risk given 30 days of tweets prior to the date of an attempt (or a corresponding date when no attempt was made), and Subtask 2 involved as71 this kind involves the availability of data to work with, and the interplay, or even tension, between the need for research and the need to respect privacy and other ethical considerations. Horvitz and Mulligan (2015) provide one short, useful discussion specifically focused on data and privacy, and Benton et al. (2017) and Chancellor et al. (2019) discuss ethical issues specifically with regard to social media and work on mental health. Lane and Schur (2010) provide a valuable entry point to the concept of data enclaves as a way to balance the need for data access in order to make progress in healthcare with respect for patient privacy — this concept ties in directly with the call by Schafer et al. (2021) for community-level mental health datasets to be easily available for research so that the predictive"
2021.clpsych-1.7,W16-0311,1,0.815625,"rformance between using tweets 90 to 0 days prior to an attempt and using tweets 180 to 90 days prior. In Coppersmith et al. (2018), the AUC score using tweets 30 days prior to an attempt is .89 and the AUC score using tweets six months prior to an attempt is .93. At the same time, it is important to note that those results are not directly comparable to the present task, given differences in dataset size and composition. Coppersmith et al. (2018) used more OurDataHelps data, and this was augmented with a dataset of users who had made publicly self-stated suicide attempts, building on work in Coppersmith et al. (2016). In total, Coppersmith et al. (2018) performed their experimentation using a dataset containing 418 users with suicide attempts, compared to this task’s 97 users. 8 Enclave Lessons Learned We solicited feedback from all registered teams (both those who submitted results and those who did not) regarding the shared task experience. This discussion and our lessons learned for the future are informed by their comments. Onboarding. Shared tasks are bursty by nature, 76 the first burst involving participants getting started. In contrast, the ongoing operations of a data enclave involve a more conti"
2021.clpsych-1.7,N19-1423,0,0.0111359,"with outcomes for research on prediction of suicide attempts; our shared task is based on a subset of their data. 3 Data We briefly describe our data sources, and how we constructed the shared task datasets for binary classification tasks. 3.1 Related, the most current paradigms in NLP and machine learning involve both general-purpose pretraining and task-specific fine-tuning. To some extent, pre-training data may capture generalizations about language that transfer well to problems in the mental health space. However, many offthe-shelf language resources that are commonly used, such as BERT (Devlin et al., 2019), are built from sources such as books and Wikipedia entries. These may translate poorly to systems dependent on social media posts from Twitter, Facebook, or an online discussion forum. It is well known that systems perform better when they are trained on materials similar to the materials the system will run on (Alsentzer et al., 2019; Beltagy et al., 2019). Therefore using task-specific data from immediately relevant sources as training data for social media based mental health tasks is a high priority that requires attention. Data sources We began with data donated to the OurDataHelps.org"
2021.clpsych-1.7,W16-1609,0,0.0402012,"aditional models versus fine-tuned deep learning models. In both Subtasks 1 and 2, they submitted results from gradient-boosted classifiers. One used syntax features and the other character TF-IDF features. For Subtask 1, they also submitted results from a contextualized language model classifier, and, for Subtask 2, a voting ensemble method. SoS (Wang et al., 2021). Team SoS introduced the C-Attention Network, which uses latent feature information implicitly in the embeddings. This was compared with submissions using KNN and SVM classifiers. Latent features included using Doc2vec embeddings (Lau and Baldwin, 2016). Hand-crafted features included emotion lexicons, part-of-speech tags, and a custom dictionary that models various stages of suicidal behavior. UlyaLamia (Bayram and Benhiba, 2021). In the UlyaLamia submissions, the authors were motivated by real-life applicability of their model to use tweet-level classification. The team’s submissions used a majority voting approach over individual tweets. In order to pick which machine learning method to use, the team experimented with multiple methods tuned on the training data using a leave-one-out strategy. Their final submissions were the top methods f"
2021.clpsych-1.7,2021.clpsych-1.11,0,0.0950243,"Missing"
2021.clpsych-1.7,W19-3003,1,0.834275,"empts and dates associated with them, although dates are not provided in all cases. Although the platform permits collection of a wide range of data, including, for example, social media, fitness, and wearable data, in this shared task we restricted our attention to Twitter data and a subset of basic information from the questionnaire. Only publicly available tweets are used, typically visible to friends and family, and these were deidentified before being provided to the Enclave. On the Enclave, participants also had access to a copy of the UMD Reddit Suicidality Dataset (Shing et al., 2018; Zirikly et al., 2019). This dataset was used by one of the teams (NUSIDS) in their submission. Another theme found in related literature involves the nature and quality of the variables being predicted. The sensitivity of mental health data has led to a proliferation of proxy variables taken from publicly available data rather than groundtruth clinical variables or real-world outcomes (e.g. De Choudhury and De, 2014; Coppersmith et al., 2014; Yates et al., 2017; Shing et al., 2018; Cohan 72 In addition, a non-sensitive practice dataset using the shared task data format was provided to participants so they could wo"
2021.clpsych-1.7,W18-0603,1,0.855431,"of past suicide attempts and dates associated with them, although dates are not provided in all cases. Although the platform permits collection of a wide range of data, including, for example, social media, fitness, and wearable data, in this shared task we restricted our attention to Twitter data and a subset of basic information from the questionnaire. Only publicly available tweets are used, typically visible to friends and family, and these were deidentified before being provided to the Enclave. On the Enclave, participants also had access to a copy of the UMD Reddit Suicidality Dataset (Shing et al., 2018; Zirikly et al., 2019). This dataset was used by one of the teams (NUSIDS) in their submission. Another theme found in related literature involves the nature and quality of the variables being predicted. The sensitivity of mental health data has led to a proliferation of proxy variables taken from publicly available data rather than groundtruth clinical variables or real-world outcomes (e.g. De Choudhury and De, 2014; Coppersmith et al., 2014; Yates et al., 2017; Shing et al., 2018; Cohan 72 In addition, a non-sensitive practice dataset using the shared task data format was provided to partic"
2021.clpsych-1.7,2021.clpsych-1.9,0,0.017576,"ns varied the distributions for the priors and hyperparameters (type of regression) for the logisticregression model. sentimenT5 (Morales et al., 2021). SentimenT5 took different approaches in their submissions to explore the performance of simple traditional models versus fine-tuned deep learning models. In both Subtasks 1 and 2, they submitted results from gradient-boosted classifiers. One used syntax features and the other character TF-IDF features. For Subtask 1, they also submitted results from a contextualized language model classifier, and, for Subtask 2, a voting ensemble method. SoS (Wang et al., 2021). Team SoS introduced the C-Attention Network, which uses latent feature information implicitly in the embeddings. This was compared with submissions using KNN and SVM classifiers. Latent features included using Doc2vec embeddings (Lau and Baldwin, 2016). Hand-crafted features included emotion lexicons, part-of-speech tags, and a custom dictionary that models various stages of suicidal behavior. UlyaLamia (Bayram and Benhiba, 2021). In the UlyaLamia submissions, the authors were motivated by real-life applicability of their model to use tweet-level classification. The team’s submissions used a"
2021.findings-acl.332,W18-4904,1,0.837339,"description of how word-by-word predictors are convolved to estimate the fMRI BOLD signal in studies like the present one. In order to look at correlations between predictors and brain activity, our analyses employ the General Linear Model (GLM; carried out using SPM12, Friston et al., 2007).6 GLM is a hierarchical model with two levels that is typically used in fMRI data analysis (Poldrack et al., 2011), and its use within neuro-computational models of language processing for continuous, naturalistic fMRI studies is well-established (Brennan et al., 2012; Brennan, 2016; Willems et al., 2016; Bhattasali et al., 2018; Li et al., 2018; Bhattasali et al., 2019). At the first level of the GLM model, the data for each subject is modeled separately to calculate subject-specific parameter estimates and within5 For more details about the hemodynamic response, see chapter 2 of Kemmerer (2014). 6 Processing time on a Mac OS 10.13 takes 1.5 hours per subject and increases linearly with additional subjects. subject variance such that for each subject, a regression model is estimated for each voxel against the fMRI time series. The second-level model takes subject-specific parameter estimates as input and uses the be"
2021.findings-acl.332,N01-1021,0,0.626029,"adds to a growing literature using methods from computational linguistics to operationalize and test hypotheses about neuro-cognitive mechanisms in sentence processing. 1 Introduction (a) I could recognize at first glance Narratives unfold over time and comprehenders incrementally process words and sentences. In order to understand the current word and sentence, we have to integrate current input with the information from the previous context. In characterizing this process, the notion of surprisal from information theory has been prevalent in psycholinguistic modeling, following the work of Hale (2001) and Levy (2008). Surprisal (b) So I had to choose another profession, and I learned to fly airplanes. I flew a little in many places around the world. And geography it’s true has served me well. I could recognize at first glance Following prior studies, we use measures of lexical surprisal to capture the influence of local context, and we introduce a new measure, topical 3786 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3786–3798 August 1–6, 2021. ©2021 Association for Computational Linguistics surprisal, to operationalize the predictive role of broader co"
2021.findings-acl.332,P18-1132,0,0.0242473,"Missing"
2021.findings-acl.332,W18-2808,0,0.0516511,"§4, the sequence of neuroimages collected during their session becomes the dependent variable in a regression against word-by-word predictors that have been derived from the text of the story. 3.2 Stimuli The English audio stimulus was Antoine de SaintExup´ery’s The Little Prince, translated by David Wilkinson and read by Karen Savage. It constitutes a fairly lengthy exposure to naturalistic language, comprising 19,171 tokens; 15,388 words and 1,388 sentences, and lasting over an hour and a half. The Little Prince has been used in a number of previous fMRI studies of language processing, e.g. Li et al. (2018); Bhattasali et al. (2019); Zhang (2020); Stanojevi´c et al. (2021) 3.3 Participants 56 participants were scanned and 5 of them were excluded since they had incomplete scanning sessions. Participants included were fifty-one volunteers (32 women and 19 men, 18-37 years old) with no history of psychiatric, neurological, or other medical illness or history of drug or alcohol abuse that might compromise cognitive functions. All strictly qualified as right-handed on the Edinburgh handedness inventory (Oldfield, 1971). All selfidentified as native English speakers and gave their written informed con"
2021.findings-acl.332,Q16-1037,0,0.0265769,"a connection we plan to explore further. Looking just at the ngram and LSTM models of lexical surprisal, our results provide additional corroboration for previous findings in the cognitive neuroscience literature involving sequential and syntactic processing (Willems et al., 2016; Brennan, 2016; Lopopolo et al., 2017; Shain et al., 2020). They also constitute an addition to the literature on understanding the linguistic properties captured by deep learning architectures. Numerous authors have shown that LSTM models capture not only sequential but also longer-distance structural dependencies (Linzen et al., 2016; Gulordava et al., 2018; van Schijndel and Linzen, 2018; Kuncoro et al., 2018; Futrell et al., 2019). In our study, we find that, while overlapping in the bilateral Anterior Temporal Lobe, the ngram and syntactic surprisal models also give rise to differently localized brain activity: the ngram model implicates the left Inferior Frontal Gyrus (IFG), while the LSTM surprisal model implicates the left Superior Temporal Gyrus (STG). The key observation here is that left-lateralized STG activity is uncontroversially correlated with syntactic processing (Pallier et al., 2011; Thompson and Meltzer-"
2021.findings-acl.332,2020.conll-1.53,0,0.0151425,"l as a cognitive measure Prior neurolinguistic work has used surprisal as an index of cognitive processing effort. Behavioral measurements like self-paced reading are one way to infer how much effort is involved while processing some piece of linguistic input (e.g., Futrell et al., 2021); other methods more directly measure activity in the brain, including functional magnetic resonance imaging (fMRI), which we will focus on in this paper, as well as magnetoencephalography (MEG) (e.g., Brodbeck et al., 2018) and electroencephalography (EEG) (e.g., Ettinger et al., 2016; Brennan and Hale, 2019; Michaelov and Bergen, 2020). In such work, the logic is generally as follows. First, as noted in §1 we assume that when a word is less expected given the context, processing it during comprehension will require more work in the brain. Then, we computationally estimate a model of surprisal using a corpus: surprisalM (wi ) = log PM (wi |w1 ...wi 1) (1) Two common instantiations for M include ngram models and models conditioned on prior syntactic context (Hale, 2001). By the first assumption, the value of Eq (1) is taken to be a predictor of processing effort at word wi . Therefore, the key final step is to analyze the rel"
2021.findings-acl.332,2020.findings-emnlp.415,0,0.0232092,"surprisal (§3.5), based on topic modeling, as a way to look at functional localization of correlates of broader, non-sequential contextual processing using fMRI. 3 3.1 fMRI Study Method We follow Brennan et al. (2012) in using a spoken narrative as a stimulus. Participants hear a 1 Although not directly relevant to the scientific strategy we discuss here, we note that there is also a body of work that goes in the other direction, using methods from psycholinguistics and neuroscience to improve our understanding and use of neural language models, e.g. Toneva and Wehbe (2019); Ettinger (2020); Misra et al. (2020). story over headphones while they are in the MRI scanner. As we describe in greater detail in §4, the sequence of neuroimages collected during their session becomes the dependent variable in a regression against word-by-word predictors that have been derived from the text of the story. 3.2 Stimuli The English audio stimulus was Antoine de SaintExup´ery’s The Little Prince, translated by David Wilkinson and read by Karen Savage. It constitutes a fairly lengthy exposure to naturalistic language, comprising 19,171 tokens; 15,388 words and 1,388 sentences, and lasting over an hour and a half. The"
2021.findings-acl.332,N18-1202,0,0.0231423,"ing trend of using neural language models in cognitive neuroscience research, often using neural data collected from individuals 3787 during naturalistic listening.1 As one salient example, Wehbe et al. (2014) investigated how well vector representations predicted brain activity for subjects reading fiction, in their case material from Harry Potter and the Sorcerer’s Stone, based on within-sentence context. Also working within the sentence using naturalistic listening, Toneva et al. (2020) derived composed representations of “supraword meaning” using contextualized word representations (ELMo, Peters et al., 2018) to capture the compositional meaning of multi-word expressions and event/argument structure. Jain and Huth (2018) make predictions of neural activity using LSTM representations from up to the previous 20 words of context (which would be on the order of 8-10 seconds of speech on average). Work of this kind has a number of dimensions of variation. One is the nature of the neural measurement, e.g. fMRI versus MEG, which relates crucially to the cognitive questions being asked, since some questions involve temporal locality, a strength of MEG, and others involve spatial locality, a strength of fM"
2021.findings-acl.332,D18-1499,0,0.0370238,"Missing"
2021.findings-acl.332,2021.cmcl-1.3,1,0.812713,"Missing"
2021.findings-acl.332,D14-1030,0,0.0251399,"020). Syntactic surprisal has also mapped onto a left-lateralized network consisting of the Inferior Parietal Lobule, Inferior Frontal Gyrus, Middle Temporal Gyrus, along with some evidence for bilateral processing in the Anterior Temporal Lobe (Brennan et al., 2016; Henderson et al., 2016; Lopopolo et al., 2017; Shain et al., 2020). 2.2 Neural language models in cognitive neuroscience There has been a growing trend of using neural language models in cognitive neuroscience research, often using neural data collected from individuals 3787 during naturalistic listening.1 As one salient example, Wehbe et al. (2014) investigated how well vector representations predicted brain activity for subjects reading fiction, in their case material from Harry Potter and the Sorcerer’s Stone, based on within-sentence context. Also working within the sentence using naturalistic listening, Toneva et al. (2020) derived composed representations of “supraword meaning” using contextualized word representations (ELMo, Peters et al., 2018) to capture the compositional meaning of multi-word expressions and event/argument structure. Jain and Huth (2018) make predictions of neural activity using LSTM representations from up to"
A97-1050,J93-2003,0,0.00598615,"e, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs). SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995). The iterative filtering module then alternates between estimating the most likely translations among word tokens in the bitext and estimating the most likely translations between word types. This re-estimation paradigm was pioneered by Brown et al. (1993). However, their models were not designed for human inspection, and though some have tried, it is not clear how to extract translation lexicons from their models (Wu and Xia, 1995). In contrast, SABLE automatically constructs an explicit translation lexicon, the lexicon consisting 342 of word type pairs that are not filtered out during the re-estimation cycle. Neither of the translation lexicon construction modules pay any attention to word order, so they work equally well for language pairs with different word order. 2.3 Thresholding Translation lexicon recall can be automatically computed wi"
A97-1050,J96-2004,0,0.0245462,"Missing"
A97-1050,P93-1002,0,0.0313318,"e character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &quot;plug-in&quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) a n d / o r &quot;seed&quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts are far more rare than one might expect. Moreov"
A97-1050,P93-1001,0,0.0473128,"imard et al., 1992; Melamed, 1995) a n d / o r &quot;seed&quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bitexts are far more rare than one might expect. Moreover, even when these larger text units can be found, their size imposes an upper bound on the resolution of the bitext map. On the other end of the spectrum, character-based bitext mapping algorithms (Church, 1993; Davis et al., 1995) are limited to language pairs where cognates are common; in addition, they may easily be misled by superficial differences in formatting and page layout and must sacrifice precision to be computationally tractable. SIMR filters candidate points of correspondence using a geometric pattern rec"
A97-1050,A94-1006,0,0.225728,"Missing"
A97-1050,W93-0301,0,0.2831,"Missing"
A97-1050,E95-1010,0,0.011057,"ated components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &quot;plug-in&quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) a n d / o r &quot;seed&quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, a"
A97-1050,H91-1026,0,0.291228,"8. word order differences between languages. The filtering algorithm can be efficiently interleaved with the point generation algorithm so that SIMR runs in linear time and space with respect to the size of the input bitext. 2.2 Translation Lexicon Extraction Since bitext maps can represent crossing correspondences, they are more general than &quot;alignments&quot; (Melamed, 1996a). For the same reason, bitext maps allow a more general definition of token cooccurrence. Early efforts at extracting translation lexicons from bitexts deemed two tokens to co-occur if they occurred in aligned sentence pairs (Gale and Church, 1991). SABLE counts two tokens as cooccurring if their point of correspondence lies within a short distance 8 of the interpolated bitext map in the bitext space, as illustrated in Figure 1. To ensure that interpolation is well-defined, minimal sets of non-monotonic points of correspondence are replaced by the lower left and upper right corners of their minimum enclosing rectangles (MERs). SABLE uses token co-occurrence statistics to induce an initial translation lexicon, using the method described in (Melamed, 1995). The iterative filtering module then alternates between estimating the most likely"
A97-1050,W95-0115,1,0.951419,"A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &quot;plug-in&quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) a n d / o r &quot;seed&quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) reports that such bi"
A97-1050,W96-0201,1,0.824611,"mmers, part-of-speech taggers, and stop lists when they are available. • Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved. Robustness: The system performs well even in the face of omissions or inversions in translations. * Scalability: SABLE has been used successfully on input bitexts larger than 130MB. * Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE's main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character positio"
A97-1050,1996.amta-1.13,1,0.849612,"mmers, part-of-speech taggers, and stop lists when they are available. • Black box functionality: Automatic acquisition of translation lexicons requires only that the user provide the input bitexts and identify the two languages involved. Robustness: The system performs well even in the face of omissions or inversions in translations. * Scalability: SABLE has been used successfully on input bitexts larger than 130MB. * Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c). The following is a brief description of SABLE's main components. A more detailed description of the entire system is available in (Melamed, 1997). 2.1 Mapping Bitext Correspondence After both halves of the input bitext(s) have been tokenized, SABLE invokes the Smooth Injective Map Recognizer (SIMR) algorithm (Melamed, 1996a) and related components to produce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character positio"
A97-1050,1992.tmi-1.7,0,0.125397,"roduce a bitext map. A bitext map is an injective partial function between the character positions in the two halves of the bitext. Each point of correspondence (x,y) in the bitext map indicates that the word centered around character position x in the first half of the bitext is a translation of the word centered around character position y in the second half. SIMR produces bitext maps a few points at a time, by interleaving a point generation phase and a point selection phase. SIMR is equipped with several &quot;plug-in&quot; matching heuristic modules which are based on cognates (Davis et al., 1995; Simard et al., 1992; Melamed, 1995) a n d / o r &quot;seed&quot; translation lexicons (Chen, 1993). Correspondence points are generated using a subset of these matching heuristics; the particular subset depends on the language pair and the available resources. The matching heuristics all work at the word level, which is a happy medium between larger text units like sentences and smaller text units like character n-grams. Algorithms that map bitext correspondence at the phrase or sentences level are limited in their applicability to bitexts that have easily recognizable phrase or sentence boundaries, and Church (1993) repo"
A97-1050,J93-1007,0,0.0827496,"Missing"
A97-1050,J96-1001,0,0.121027,"Missing"
A97-1050,1994.amta-1.26,0,0.0558036,"Missing"
C18-1152,D13-1160,0,0.0633134,"Missing"
C18-1152,D11-1037,0,0.0612235,"f matching the SemRole task. 1798 Methodologically the most closely related work is that of Adi et al. (2016), which uses classification tasks to probe for information in sentence embeddings. As discussed above, we depart from that work in targeting deeper and more linguistically-motivated aspects of sentence meaning, and we incorporate careful controls of our datasets to ensure elimination of bias in the results. Our focus on assessing linguistically-motivated information relates to work on evaluations that aim for fine-grained analysis of systems’ linguistic capacities (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). The present work contributes to this effort with new tasks that assess composition per se, and that do so in a highly targeted manner via careful controls. Our use of synthetically generated data to achieve this level of control relates to work like that of Weston et al. (2015), which introduces synthetic question-answering tasks for evaluating the capacity of systems to reason with natural language input. Our examination of the capacity of neural sequence models to identify abstract relations in sentence representations also relates to work by Linzen et al. (2016), wh"
C18-1152,P16-1139,0,0.102504,"t verb—but that these sequence models fall significantly short when it comes to capturing semantic role compositionally. Another point that emerges from these results is that despite the fairly substantial differences in architecture, objective, and training of these models, capacity to capture the compositional information is fairly similar across models, suggesting that these distinct design decisions are not having a very significant impact on compositional meaning extraction. We plan to test more substantially distinct models, like those with explicit incorporation of syntactic structure (Bowman et al., 2016; Dyer et al., 2016; Socher et al., 2013) in future work. 8 Related work This work relates closely to a growing effort to increase interpretability of neural network models in NLP—including use of visualization to analyze what neural networks learn (Li et al., 2015; K´ad´ar et al., 2016), efforts to increase interpretability by generating explanations of model predictions (Ribeiro et al., 2016; Lei et al., 2016; Li et al., 2016), and work submitting adversarial examples to systems in order to identify weaknesses (Zhao et al., 2017; Jia and Liang, 2017; Ettinger et al., 2017). 6 The slightly hi"
C18-1152,D17-1070,0,0.219819,"serves as an additional check for biases in the datasets, to ensure that neither the sentence vectors nor the probe vectors alone are sufficient to perform above chance on the tasks. For all tasks, these random vectors produce chance performance. 7.1 Sentence encoding models We test a number of composition models on these classification tasks. These models represent a range of influential current models designed to produce task-general sentence embeddings. They employ a number of different architectures and objectives, and have shown reasonable success on existing metrics (Hill et al., 2016; Conneau et al., 2017). All sentence embeddings used are of 2400 dimensions. Because our pre-trained models (SDAE, SkipThought) are trained on the Toronto Books Corpus (Zhu et al., 2015), we use this as our default training corpus, except when other supervised training data is required (as in the case of InferSent). Before sentence generation, the chosen vocabulary was checked against the training corpora to ensure that no words were out-of-vocabulary (or below a count of 50). 4 See footnote 1 for link to all classification datasets used in these experiments. 1796 BOW SDAE ST-UNI ST-BI InferSent Content1Probe 100.0"
C18-1152,C94-2149,0,0.377312,"maps from the event to a surface sentence via a simple rule-based mapping. Since the representations specify syntactic information and use lexicalized meaning information, there is no significant process of lexical selection required during surface realization—only morphological inflection derivable from syntactic characteristics. As a result, the event representations map deterministically to their corresponding surface 1794 forms. We use a grammar specified using the NLTK feature grammar framework (Bird et al., 2009). Morphological inflections are drawn from the XTAG morphological database (Doran et al., 1994). 4.4 Sentence quality To ensure the quality of generation system output, we manually inspected large samples of generated sentences throughout development and after generation of the final sets, to confirm that sentences were grammatical and of the expected form. Table 1 shows a representative sample of generated sentences.2 the men were sleeping the woman followed the lawyer that the student is meeting the women were being helped by the lawyers the student called the man the scientist that the professors met is dancing the doctors that helped the lawyers are being recommended by the student"
C18-1152,N16-1024,0,0.0843257,"Missing"
C18-1152,W16-2524,1,0.867871,"evaluation and interpretability: after all, in order to improve meaning extraction, we need to be able to evaluate it. But with sentence representations increasingly taking the form of dense vectors (embeddings) from neural network models, it is difficult to assess what information these representations are capturing—and this problem is particularly acute for assessing abstract content like compositional meaning information. Here we introduce an analysis method for targeting and evaluating compositional meaning information in sentence embeddings. The approach builds on a proposal outlined in Ettinger et al. (2016), and involves designing classification tasks that directly target the information of interest (e.g., “Given a noun n, verb v, and an embedding s of sentence s: is n the agent of v in s?”). By contrast to related work analyzing surface variables like word content and word order in sentence embeddings (Adi et al., 2016), we specifically target compositional meaning information relevant to achieving language understanding—and in order to isolate this more abstract information, we exert careful control over our classification datasets to ensure that we are targeting information arrived at by comp"
C18-1152,W17-5401,1,0.864792,"Missing"
C18-1152,N18-2017,0,0.0627326,"to perform the tasks successfully. Third, we employ a sanity check leveraging known limitations of bag-of-words (BOW) composition models: for any tasks requiring order information from the source sentence, which BOW models cannot logically retain, we check to ensure that BOW composition models are at chance performance. These controls serve to combat a problem that has gained increasing attention in recent work: many existing evaluation datasets contain biases that allow for high performance based on superficial cues, thus inflating the perceived success of systems on these downstream tasks (Gururangan et al., 2018; Bentivogli et al., 2016). In the present work, our first priority is careful control of our tasks such that biases are eliminated to the greatest extent possible, allowing more confident conclusions about systems’ compositional capacities than are possible with existing metrics. The contributions of this paper are threefold. 1) We introduce a method for analyzing compositional meaning information in sentence embeddings, along with a generation system that enables controlled creation of datasets for this analysis. 2) We provide experiments with a range of sentence composition models, to demon"
C18-1152,N16-1162,0,0.379015,"respectively. This serves as an additional check for biases in the datasets, to ensure that neither the sentence vectors nor the probe vectors alone are sufficient to perform above chance on the tasks. For all tasks, these random vectors produce chance performance. 7.1 Sentence encoding models We test a number of composition models on these classification tasks. These models represent a range of influential current models designed to produce task-general sentence embeddings. They employ a number of different architectures and objectives, and have shown reasonable success on existing metrics (Hill et al., 2016; Conneau et al., 2017). All sentence embeddings used are of 2400 dimensions. Because our pre-trained models (SDAE, SkipThought) are trained on the Toronto Books Corpus (Zhu et al., 2015), we use this as our default training corpus, except when other supervised training data is required (as in the case of InferSent). Before sentence generation, the chosen vocabulary was checked against the training corpora to ensure that no words were out-of-vocabulary (or below a count of 50). 4 See footnote 1 for link to all classification datasets used in these experiments. 1796 BOW SDAE ST-UNI ST-BI InferS"
C18-1152,D17-1215,0,0.0381698,"icit incorporation of syntactic structure (Bowman et al., 2016; Dyer et al., 2016; Socher et al., 2013) in future work. 8 Related work This work relates closely to a growing effort to increase interpretability of neural network models in NLP—including use of visualization to analyze what neural networks learn (Li et al., 2015; K´ad´ar et al., 2016), efforts to increase interpretability by generating explanations of model predictions (Ribeiro et al., 2016; Lei et al., 2016; Li et al., 2016), and work submitting adversarial examples to systems in order to identify weaknesses (Zhao et al., 2017; Jia and Liang, 2017; Ettinger et al., 2017). 6 The slightly higher accuracy on the order task is most likely the result of a very slight bias due to our use of only noun-verb order probe pairs for the sake of matching the SemRole task. 1798 Methodologically the most closely related work is that of Adi et al. (2016), which uses classification tasks to probe for information in sentence embeddings. As discussed above, we depart from that work in targeting deeper and more linguistically-motivated aspects of sentence meaning, and we incorporate careful controls of our datasets to ensure elimination of bias in the res"
C18-1152,D16-1011,0,0.0510233,"Missing"
C18-1152,Q16-1037,0,0.161181,"009; Bender et al., 2011; Marelli et al., 2014). The present work contributes to this effort with new tasks that assess composition per se, and that do so in a highly targeted manner via careful controls. Our use of synthetically generated data to achieve this level of control relates to work like that of Weston et al. (2015), which introduces synthetic question-answering tasks for evaluating the capacity of systems to reason with natural language input. Our examination of the capacity of neural sequence models to identify abstract relations in sentence representations also relates to work by Linzen et al. (2016), who explore whether LSTMs can learn syntactic dependencies, as well as Williams et al. (2017), who investigate the extent to which parsers that are learned based on a semantic objective produce conventional syntax. Finally, importantly related work is that concerned specifically with testing systematic composition. Lake and Baroni (2017) investigate the capacity of RNNs to perform zero-shot generalization using composition, and Dasgupta et al. (2018) construct an entailment dataset with balanced lexical content in order to target composition more effectively. We contribute to this line of in"
C18-1152,marelli-etal-2014-sick,0,0.0388544,"e task. 1798 Methodologically the most closely related work is that of Adi et al. (2016), which uses classification tasks to probe for information in sentence embeddings. As discussed above, we depart from that work in targeting deeper and more linguistically-motivated aspects of sentence meaning, and we incorporate careful controls of our datasets to ensure elimination of bias in the results. Our focus on assessing linguistically-motivated information relates to work on evaluations that aim for fine-grained analysis of systems’ linguistic capacities (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). The present work contributes to this effort with new tasks that assess composition per se, and that do so in a highly targeted manner via careful controls. Our use of synthetically generated data to achieve this level of control relates to work like that of Weston et al. (2015), which introduces synthetic question-answering tasks for evaluating the capacity of systems to reason with natural language input. Our examination of the capacity of neural sequence models to identify abstract relations in sentence representations also relates to work by Linzen et al. (2016), who explore whether LSTMs"
C18-1152,J05-1004,0,0.088389,"erbs and optional transitive or intransitive relative clauses on those arguments. These representations are comparable in many ways to abstract meaning representation (AMR) (Banarescu et al., 2012), but rather than abstracting entirely away from syntactic structure as in AMR, our 1793 Figure 1: Event representation for “The student who is sleeping was not helped by the professor” event representations encode syntactic information directly, along with the more abstract meaning information, in order to maintain a deterministic mapping to surface forms. Relatedly, while AMR uses PropBank frames (Palmer et al., 2005) to encode meaning information, we encode information via English lemmas, to maintain control over lexical selection during generation. These representations can be partially specified to reflect a desired constraint, and can then be passed in this partial form as input to the generation system—either as a required component, or as a prohibited component. This allows us to constrain the semantic and syntactic characteristics of the output sentences. In addition to partial events, the system can also take lists of required or prohibited lexical items. 4.2 Event population The system uses a numb"
C18-1152,N16-3020,0,0.0205232,"n decisions are not having a very significant impact on compositional meaning extraction. We plan to test more substantially distinct models, like those with explicit incorporation of syntactic structure (Bowman et al., 2016; Dyer et al., 2016; Socher et al., 2013) in future work. 8 Related work This work relates closely to a growing effort to increase interpretability of neural network models in NLP—including use of visualization to analyze what neural networks learn (Li et al., 2015; K´ad´ar et al., 2016), efforts to increase interpretability by generating explanations of model predictions (Ribeiro et al., 2016; Lei et al., 2016; Li et al., 2016), and work submitting adversarial examples to systems in order to identify weaknesses (Zhao et al., 2017; Jia and Liang, 2017; Ettinger et al., 2017). 6 The slightly higher accuracy on the order task is most likely the result of a very slight bias due to our use of only noun-verb order probe pairs for the sake of matching the SemRole task. 1798 Methodologically the most closely related work is that of Adi et al. (2016), which uses classification tasks to probe for information in sentence embeddings. As discussed above, we depart from that work in targeting d"
C18-1152,D09-1085,0,0.0226747,"pairs for the sake of matching the SemRole task. 1798 Methodologically the most closely related work is that of Adi et al. (2016), which uses classification tasks to probe for information in sentence embeddings. As discussed above, we depart from that work in targeting deeper and more linguistically-motivated aspects of sentence meaning, and we incorporate careful controls of our datasets to ensure elimination of bias in the results. Our focus on assessing linguistically-motivated information relates to work on evaluations that aim for fine-grained analysis of systems’ linguistic capacities (Rimell et al., 2009; Bender et al., 2011; Marelli et al., 2014). The present work contributes to this effort with new tasks that assess composition per se, and that do so in a highly targeted manner via careful controls. Our use of synthetically generated data to achieve this level of control relates to work like that of Weston et al. (2015), which introduces synthetic question-answering tasks for evaluating the capacity of systems to reason with natural language input. Our examination of the capacity of neural sequence models to identify abstract relations in sentence representations also relates to work by Lin"
C18-1152,D13-1170,0,0.00852351,"Missing"
C18-1152,W16-1628,0,0.0206836,"we use both the uni-skip (“ST-UNI”) and bi-skip (“ST-BI”) variants: uni-skip consists of an encoding based on a forward pass of the sentence, while bi-skip consists of a concatenation of encodings of the forward and backward passes of the sentence (each of 1200 dimensions, for 2400 total). We use the publicly available pre-trained Skip-Thought model for both of these variants.5 Skip-Thought sentence embeddings have been used as pre-trained embeddings for a variety of tasks. They have proven to be generally effective for supervised tasks and passable for unsupervised tasks (Hill et al., 2016; Triantafillou et al., 2016; Wieting et al., 2016). Like the SDAE model, the Skip-Thought model is able to use unsupervised learning, though it requires sequential sentence data. However, more than the SDAE model, the Skip-Thought model uses an objective intended to capture semantic and syntactic properties, under the authors’ assumption that prediction of adjacent sentences will encourage more syntactically and semantically similar sentences to map to similar embeddings. InferSent Our final model is the InferSent model (Conneau et al., 2017), which uses multi-layer BiLSTM encoders with max pooling on the hidden states"
C18-1152,D14-1004,0,0.06734,"Missing"
C92-1032,E85-1019,0,0.133091,"Missing"
C92-1032,P91-1012,0,\N,Missing
C92-2065,H91-1025,0,0.0157793,"Missing"
C92-2065,P90-1034,0,0.0207706,"Missing"
C92-2065,H92-1027,0,0.0668837,"Missing"
C92-2065,P90-1032,0,0.0179373,"Missing"
C92-2065,C92-2066,0,\N,Missing
C92-2065,E89-1001,0,\N,Missing
C92-2065,P89-1010,0,\N,Missing
C92-2065,1991.iwpt-1.22,0,\N,Missing
C92-2065,E91-1004,0,\N,Missing
C94-2195,P91-1030,0,0.088907,"Missing"
C94-2195,H93-1054,1,0.532712,"Missing"
C94-2195,H91-1037,0,0.0584758,"Missing"
C94-2195,P90-1004,0,0.0524791,"Missing"
C94-2195,W94-0111,0,\N,Missing
C94-2195,J93-2004,0,\N,Missing
C94-2195,J93-1005,0,\N,Missing
C94-2195,H94-1048,0,\N,Missing
C94-2195,P93-1035,1,\N,Missing
C94-2195,H93-1047,1,\N,Missing
C94-2195,J92-4003,0,\N,Missing
D08-1024,2007.mtsummit-papers.3,0,0.732994,"weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical"
D08-1024,P08-1024,0,0.283377,"tence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a la"
D08-1024,W08-0304,0,0.435015,"on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surp"
D08-1024,P08-1009,0,0.0445565,"onal cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach. By capturing how reordering depends on constituent length, these features improve translation quality"
D08-1024,P05-1033,1,0.688957,"for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a traini"
D08-1024,J07-2003,1,0.558972,"f 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that"
D08-1024,W02-1001,0,0.0620584,"The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this technique to MIRA. Note that the objective (1) is not the same as that used by Watanabe et al.; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al. (2005). We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αi j = C for a single value of j such that ei j = e∗i , and initialize αi j = 0 for all other values of j. Then, repeatedly choose a sentence i and a pair of hypotheses j, j0 , and let Let e, by abuse of notation, stan"
D08-1024,N04-4038,0,0.0240612,"rom the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled µ = 1. Our scheme would select one of the µ = 0.5 points, which have B scores almost as high as the max-B translations, yet are not very far from the translations preferred by the model. 2.4 Selecting hypothesis translations What is the set {ei j } of translation hypotheses? Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account. This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions. Since our loss function cannot be so decomposed, we select: • the 10-best translations according to the model; we then rescore the forest to obtain • the 10-best translations according to equation (11) with µ = 0.5, the first of which is the oracle translation, and • the 10-best translations with µ = ∞, to serve as negative examples. The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al. (2005) call lossaugmented infe"
D08-1024,W07-0414,0,0.0188539,"axB points in the upper left are not included (and would have no effect even if they were included). The µ = ∞ points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left. 227 To perform the forest rescoring, we need to use several approximations, since an exact search for B-optimal translations is NP-hard (Leusch et al., 2008). For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application of the rule (1 ≤ n ≤ 4) • the (unclipped) number of n-gram matches formed by the application of the rule (1 ≤ n ≤ 4) We keep track of n-grams using the same scheme"
D08-1024,P08-2010,0,0.0419974,"ures rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we o"
D08-1024,W04-3250,0,0.11639,"ting a word into itself) – two classes of number/name translation rules – glue rules The probability features are base-100 logprobabilities. The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions). Hierarchical rules were extracted 230 Results Table 1 shows the results of our experiments with the training methods and features described above. All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling (Koehn, 2004). First of all, we find that MIRA is competitive with MERT when both use the baseline feature set. In1 The only notable consequence this had for our experimentation is that proclitic Arabic prepositions were fused onto the first word of their NP object, so that the PP and NP brackets were coextensive. 2 We chose this policy for MIRA to avoid overfitting. However, we could have used the tuning set for this purpose, just as with MERT: in none of our runs would this change have made more than a 0.2 B difference on the development set. Train MERT MIRA Features baseline syntax (coarse) syntax (f"
D08-1024,D08-1088,0,0.0197708,"j · w) in order to approximate the effect of using the whole forest. See Figure 1 again for an illustration of the hypotheses selected for a single sentence. The maxB points in the upper left are not included (and would have no effect even if they were included). The µ = ∞ points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left. 227 To perform the forest rescoring, we need to use several approximations, since an exact search for B-optimal translations is NP-hard (Leusch et al., 2008). For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application"
D08-1024,P06-1096,0,0.848802,"Missing"
D08-1024,C04-1072,0,0.23212,"ged together. (If multiple 225 δ= (`i j − `i j0 ) − (∆hi j − ∆hi j0 ) · w0 k∆hi j − ∆hi j0 k2 [−αi j ,αi j0 ] clip (5) where the function clip[x,y] (z) gives the closest number to z in the interval [x, y]. 2.2 Loss function Assuming B as the evaluation criterion, the loss `i j of ei j relative to e∗i should be related somehow to the difference between their B scores. However, B was not designed to be used on individual sentences; in general, the highest-B translation of a sentence depends on what the other sentences in the test set are. Sentence-level approximations to B exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform B computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007). However, we don’t try to accumulate translations for the entire dataset, but simply maintain an exponentially-weighted moving average of previous translations. 1 0.9 B score More precisely: For an input sentence f, let e be some hypothesis translation and let {rk } be the set of reference translations for f. Let c(e; {rk }), or simply c(e) for short, be the vector of the following counts: |e|, the effective referen"
D08-1024,P08-1114,1,0.74718,"nt improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations"
D08-1024,P05-1012,0,0.162454,"standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this technique to MIRA. Note that the objective (1) is not the same as that used by Watanabe et al.; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al. (2005). We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αi j = C for a single value of j such that ei j = e∗i , and initialize αi j = 0 for all other values of j. Then, repeatedly choose a sentence i and a pair of hypotheses j, j0 , and let Let e, by abuse of notation, stand for both output strings and thei"
D08-1024,P08-1023,0,0.0181847,"same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-ba"
D08-1024,P02-1038,0,0.187192,"08. 2008 Association for Computational Linguistics training all of them simultaneously; and, second, we introduce a novel structural distortion model. We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined. The paper proceeds as follows. We describe our training algorithm in section 2; our generalization of Marton and Resnik’s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5. 2 Learning algorithm The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this te"
D08-1024,P03-1021,0,0.442838,"tion quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how simi"
D08-1024,P06-2101,0,0.275923,"a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA"
D08-1024,P08-1058,0,0.0108772,"e introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define • word penalty from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder. We trained the coarse-grained distortion model on 10,000 sentences of the training data. Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English. Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing (Botelho et al., 2005). We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences). The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences). To obtain syntactic parses for this data, we tokenized it according to the Arabic Treebank standard using AMIRA (Diab et al., 2004), parsed it with the Stanford parser (Klein and Manning, 2003), and then forced the trees back into the MT system’s token"
D08-1024,W04-3201,0,0.0135648,"far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled µ = 1. Our scheme would select one of the µ = 0.5 points, which have B scores almost as high as the max-B translations, yet are not very far from the translations preferred by the model. 2.4 Selecting hypothesis translations What is the set {ei j } of translation hypotheses? Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account. This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions. Since our loss function cannot be so decomposed, we select: • the 10-best translations according to the model; we then rescore the forest to obtain • the 10-best translations according to equation (11) with µ = 0.5, the first of which is the oracle translation, and • the 10-best translations with µ = ∞, to serve as negative examples. The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al. (2005) call lossaugmented infe"
D08-1024,P06-1091,0,0.533842,"alized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make us"
D08-1024,D07-1080,0,0.791062,"o optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure. First, we generalize Marton an"
D08-1024,P96-1021,0,0.0250127,"an the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application of the rule (1 ≤ n ≤ 4) • the (unclipped) number of n-gram matches formed by the application of the rule (1 ≤ n ≤ 4) We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder (Wu, 1996; Chiang, 2007). To find the best derivation in the forest, we traverse it bottom-up as usual, and for every set of alternative subtranslations, we select the one with the highest score. But here a rough approximation lurks, because we need to calculate B on the nodes of the forest, but B does not have the optimal substructure property, i.e., the optimal score of a parent node cannot necessarily be calculated from the optimal scores of its children. Nevertheless, we find that this rescoring method is good enough for generating high-B oracle translations and low-B negative examples. 2.5 P"
D08-1024,P08-1064,0,0.0154052,"res, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach. By capt"
D08-1024,P02-1040,0,\N,Missing
D08-1024,P07-1005,1,\N,Missing
D08-1024,N03-1017,0,\N,Missing
D09-1040,P01-1008,0,0.560903,"search directions in Section 6. 2 More recently, Callison-Burch (2008) has improved performance of this pivoting technique by imposing syntactic constraints on the paraphrases. The limitation of such an approach is the reliance on a good parser (in addition to reliance on bitexts), but a good parser is not available in all languages, especially not in resource-poor languages. Another approach using a pivoting technique augments the human reference translation with paraphrases, creating additional translation “references” (Madnani et al., 2007). Both approaches have shown gains in B LEU score. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. In addition to the parallel corpus usage limitations described above, this technique is further limited by the small size of such materials, which are even scarcer than the resources in the pivoting case. Dolan et al. (2004) explore generating paraphrases by edit-distance and headlines of timeand topic-clustered news articles; they do not address the OOV problem directly, as their focus is sentence-level paraphrases; although they use a standard SMT measure, alignment error rate (AER),"
D09-1040,N06-1003,1,0.193059,"s used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this"
D09-1040,C04-1051,0,0.245926,"ally not in resource-poor languages. Another approach using a pivoting technique augments the human reference translation with paraphrases, creating additional translation “references” (Madnani et al., 2007). Both approaches have shown gains in B LEU score. Barzilay and McKeown (2001) extract paraphrases from a monolingual parallel corpus, containing multiple translations of the same source. In addition to the parallel corpus usage limitations described above, this technique is further limited by the small size of such materials, which are even scarcer than the resources in the pivoting case. Dolan et al. (2004) explore generating paraphrases by edit-distance and headlines of timeand topic-clustered news articles; they do not address the OOV problem directly, as their focus is sentence-level paraphrases; although they use a standard SMT measure, alignment error rate (AER), they only report results of the alignment quality, and not of an end-to-end SMT system. Much of the previous research largely focused on morphological analysis in order to reduce type sparseness; Callison-Burch et al. (2006) list some of the influential work in that direction. Related Work This is not the first to attempt to amelio"
D09-1040,D08-1094,0,0.0671729,"Missing"
D09-1040,W06-1605,0,0.0164058,"ntic similarity). Collocational Profiles The sliding window and word association (SoA) measures. Some researchers count positional collocations in a sliding window, i.e., the cocounts and SoA measures are calculated per relative position (e.g., for some word/token u, position 1 is the token immediately after u; position -2 is the token preceding the token that precedes u) (Rapp, 1999); other researchers use nonpositional (which we dub here flat) collocations, meaning, they count all token occurrences within the sliding window, regardless of their positions in it relative to u (McDonald, 2000; Mohammad and Hirst, 2006). We use here flat collocations in a 6-token sliding window. Beside simple cooccurrence counts within sliding windows, other SoA measures include functions based on TF/IDF (Fung and Yee, 1998), mutual information (PMI) (Lin, 1998), conditional probabilities (Schuetze and Pedersen, 1997), chi-square test, and the loglikelihood ratio (Dunning, 1993). The distributional hypothesis and distributional profiles. Natural language processing (NLP) applications that assume the distributional hypothesis (Harris, 1940; Firth, 1957) typically keep track of word co-occurrences in distributional profiles (a"
D09-1040,J05-4003,0,0.110194,", and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraph"
D09-1040,C04-1151,0,0.0548243,"anslations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity"
D09-1040,P00-1056,0,0.109801,"or all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. The paraphrase-augmented systems were identical to the corresponding baseline system, with the exception of additional (paraphrase-based) translation rules, and additional feature(s). Similarly to Callison-Burch et al. (2006), we added the following feature: 5.1 English-to-Chinese Translation For the English-Chinese (E2C) baseline system, we trained on the LCD Sinorama and FBIS tests ("
D09-1040,P98-1069,0,0.542655,"ems cannot learn from non-aligned corpora, while sentence-aligned parallel corpora (bitexts) are a limited resource (See Section 2 for discussion of automaticallycompiled bitexts). Another direction might be to make use of non-parallel corpora for training. However, this requires developing techniques to extract alignments or translations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006;"
D09-1040,P02-1038,0,0.094899,"that this method is insensitive to the order in which the paraphrases are processed. We only augment the phrase table with a single rule from f to e, and in it are the feature values of the phrase fi for which the score sim(fi , f ) was the highest. We examined the application of the system’s paraphrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the forei"
D09-1040,P03-1021,0,0.0293555,"essed. We only augment the phrase table with a single rule from f to e, and in it are the feature values of the phrase fi for which the score sim(fi , f ) was the highest. We examined the application of the system’s paraphrases to handling unknown phrases when translating from English into Chinese (E2C) and from Spanish into English (S2E). For all baselines we used the phrase-based statistical machine translation system Moses (Koehn et al., 2007), with the default model features, weighted in a log-linear framework (Och and Ney, 2002). Feature weights were set with minimum error rate training (Och, 2003) on a development set using B LEU (Papineni et al., 2002) as the objective function. Test results were evaluated using B LEU and TER (Snover et al., 2005). The phrase translation probabilities were determined using maximum likelihood estimation over phrases induced from word-level alignments produced by performing Giza++ training (Och and Ney, 2000) on both source and target sides of the parallel training sets. When the baseline system encountered unknown words in the test set, its behavior was simply to reproduce the foreign word in the translated output. The paraphrase-augmented systems were"
D09-1040,N03-1017,0,0.0945165,"Missing"
D09-1040,P99-1067,0,0.850725,"n-aligned corpora, while sentence-aligned parallel corpora (bitexts) are a limited resource (See Section 2 for discussion of automaticallycompiled bitexts). Another direction might be to make use of non-parallel corpora for training. However, this requires developing techniques to extract alignments or translations from them, and in a sufficiently fast, memory-efficient, and scalable manner. One approach that can, in principle, better exploit both alignments from bitexts and make use of non-parallel corpora is the distributional collocational approach, e.g., as used by Fung and Yee (1998) and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al.,"
D09-1040,J03-3002,1,0.403077,"999; Diab and Finch, 2000). This approach is sometimes viewed as, or combined with, an information retrieval (IR) approach, and normalizes strength-of-association measures (see Section 3) with IR-related measures such as TF/IDF (Fung and Yee, 1998). To date, reported implementations suffer from scalability issues, as they pre-compute and hold in memory a huge collocation matrix; we know of no report of using this approach in an end-to-end SMT system. Another approach aiming to reduce OOV rate concentrates on increasing parallel training set size without using more dedicated human translation (Resnik and Smith, 2003; Oard et al., 2003). 3 ∀u, v, w ∈ V, [semsim(u, v) > semsim(u, w)] =⇒ [psim(DPu , DPv ) > psim(DPu , DPw )], where V is the language vocabulary, DPword is the distributional profile of word, and psim() is a 2-place vector similarity function (all further described below). Paraphrasing and other NLP applications that are based on the distributional hypothesis assume entailment in the reverse direction: the right-hand-side of Formula (2) (profile/vector similarity) entails the left-hand-side (semantic similarity). Collocational Profiles The sliding window and word association (SoA) measures. So"
D09-1040,W04-3250,0,0.0661927,"Missing"
D09-1040,koen-2004-pharaoh,0,0.013606,"Missing"
D09-1040,2005.mtsummit-papers.11,0,0.0791843,"ontroversial question about our obviously with the developments this morning community staffing of community centres perhaps we are getting rather impatient er around the inner edge interested in going to the topics and that is the day that as a as a final point left which it may still have Spanish-to-English Translation In order to to permit a more direct comparison with the pivoting technique, we also experimented with Spanish to English (S2E) translation, following Callison-Burch et al. (2006). For baseline we used the Spanish and English sides of the Europarl multilingual parallel corpus (Koehn, 2005), with the standard training, development, and test sets. We created training subset models of 10,000, 20,000, and 80,000 aligned sentences, as described in Callison-Burch et al. (2006). For better comparison with their pivoting system, we used the same 5-gram language model, development and test sets: For development, we used the Europarl dev2006 Spanish and English sides, and for testing we used the Europarl 2006 test set. We trained the Spanish paraphrase generation system on the Spanish corpora available from Score 0.56 0.53 0.45 0.42 0.33 0.32 0.30 0.87 0.82 0.68 0.67 0.65 0.56 0.54 0.74"
D09-1040,W07-0716,1,0.938011,"and Rapp (1999). However, the systems described there are not easily scalable, and require pre-computation of a very large collocation counts matrix. Related attempts propose generating bitexts from comparable and “quasicomparable” bilingual texts by iteratively bootstrapping documents, sentences, and words (Fung and Cheung, 2004), or by using a maximum entropy classifier (Munteanu and Marcu, 2005). Alignment accuracy remains a challenge for them. Recent work has proposed augmenting the training data with paraphrases generated by pivoting through other languages (Callison-Burch et al., 2006; Madnani et al., 2007). This indeed alleviates the vocabulary coverage problem, especially for the so-called “low density” languages. However, these approaches still require bitexts where Untranslated words still constitute a major problem for Statistical Machine Translation (SMT), and current SMT systems are limited by the quantity of parallel training texts. Augmenting the training data with paraphrases generated by pivoting through other languages alleviates this problem, especially for the so-called “low density” languages. But pivoting requires additional parallel texts. We address this problem by deriving par"
D09-1040,D09-1081,1,0.833919,"Missing"
D09-1040,J93-2003,0,\N,Missing
D09-1040,N03-2026,1,\N,Missing
D09-1040,C98-1066,0,\N,Missing
D09-1040,D08-1021,1,\N,Missing
D09-1040,P07-2045,1,\N,Missing
D09-1040,P05-1033,0,\N,Missing
D09-1040,J07-2003,0,\N,Missing
D09-1040,I05-3027,0,\N,Missing
D09-1081,W06-2503,0,0.0367625,"Missing"
D09-1081,W06-1605,1,0.0644995,"pplicability. They can be applied only if a WordNet exists for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domainStrictly co"
D09-1081,D07-1060,1,0.870678,"Missing"
D09-1081,W04-2607,0,0.0135592,"make use of their cross-lingual DPCs, to compute semantic distance in a resource-poor language, just as they did. We leave that for future work. 2 Background and Related Work Strictly speaking, semantic distance/closeness is a property of lexical units—a combination of the surface form and word sense.3 Two terms are considered to be semantically close if there is a lexical semantic relation between them. Such a relation may be a classical relation such as hypernymy, troponymy, meronymy, and antonymy, or it may be what have been called an ad-hoc nonclassical relation, such as cause-and-effect (Morris and Hirst, 2004). If the closeness in meaning is due to certain specific classical relations such as hypernymy and troponymy, then the terms are said to be semantically similar. Semantic relatedness is the term used to describe the more general form of semantic closeness caused by any semantic relation (Hirst and Budanitsky, 2005). So the nouns liquid and water are both semantically similar and semantically related, whereas the nouns boat and rudder are semantically related, but not similar. The next three sub-sections describe three kinds of automatic distance measures: (1) lexicalresource-based measures tha"
D09-1081,P06-1014,0,0.0132628,"can be computed from corpus statistics. Within WordNet, the is-a hierarchy is much more well-developed than that of other lexical semantic relations. So, not surprisingly, the best WordNet-based measures are those that rely only on the is-a hierarchy. Therefore, they are good at measuring semantic similarity (e.g., doctor–physician), but not semantic relatedness (e.g., doctor–scalpel). Further, the measures can only be used in languages that have a (sufficiently developed) WordNet. WordNet sense information has been criticized to be too fine grained (Agirre and Lopez de Lacalle Lekuona, 2003; Navigli, 2006). See Hirst and Budanitsky (2005) for a comprehensive survey of WordNet-based measures. sider the following. The noun bank has two senses “river bank” and “financial institution”. Assume that bank, when used in the “financial institution” sense, co-occurred with the noun money 100 times in a corpus. Similarly, assume that bank, when used in the “river bank” sense, co-occurred with the noun boat 80 times. So the DP of bank will have co-occurrence information with money as well as boat: 2.2 Both Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) proposed measures that are not only dist"
D09-1081,W06-2501,0,0.0942612,"s for the language of interest (which is not the case for the “low-density” languages); and even if there is a WordNet, a number of domainspecific terms may not be encoded in it. On the other hand, corpus-based distributional measures of semantic distance, such as cosine and α-skew divergence (Dagan et al., 1999), rely on raw text alone (Weeds et al., 2004; Mohammad, 2008). However, when used to rank word pairs in order of semantic distance or correct real-word spelling errors, they have been shown to perform poorly (Weeds et al., 2004; Mohammad and Hirst, 2006). Mohammad and Hirst (2006) and Patwardhan and Pedersen (2006) argued that word sense ambiguity is a key reason for the poor performance of traditional distributional measures, and they proposed hybrid approaches that are distributional in nature, but also make use of information in lexical resources such as published thesauri and WordNet. However, both these approaches can be applied to estimate the semantic distance between two terms only if both terms exist in the lexical resource they rely on. We know lexical resources tend to have limited vocabulary and a large number of domainStrictly corpus-based measures of semantic distance conflate co-occurrenc"
D09-1081,N06-1025,0,0.0870394,"Missing"
D09-1081,J93-1003,0,0.0753405,"pping method proposed in Mohammad and Hirst (2006) has the effect of resetting to 0 the small co-occurrence counts. The noise from these small co-occurrence counts affects the sense-filtered-counts method more adversely (since any non-zero value will cause the inclusion of the corresponding collocate’s full cooccurrence count) and so the bootstrapped matrix is more suitable for this method. The results also show that the cosine of loglikelihood ratios method mostly performs better than cosine of conditional probabilities and the pmi methods on the noun sets. This further supports the claim by Dunning (1993) that loglikelihood ratio is much less sensitive than pmi to low counts. Interestingly, on the verb set, the pmi methods, and especially hybrid*-prop-cospmi, did extremely well. Further investigation is needed in order to determine if pmi is indeed more suitable for verb semantic similarity, and why. Acknowledgments We thank Mona Diab for her help with her verb test set, Raluca Budiu for her help and clarifications regarding the GLSA method and its implementation details, and the anonymous reviewers for their valuable feedback. This work was supported, in part, by the National Science Foundati"
D09-1081,D08-1094,0,0.153404,"Missing"
D09-1081,C04-1146,0,0.166627,"Missing"
D10-1005,baccianella-etal-2010-sentiwordnet,0,0.0233095,"nference from corpora. We show M L SLDA can build topics that are consistent across languages, discover sensible bilingual lexical correspondences, and leverage multilingual corpora to better predict sentiment. Sentiment analysis (Pang and Lee, 2008) offers the promise of automatically discerning how people feel about a product, person, organization, or issue based on what they write online, which is potentially of great value to businesses and other organizations. However, the vast majority of sentiment resources and algorithms are limited to a single language, usually English (Wilson, 2008; Baccianella and Sebastiani, 2010). Since no single language captures a majority of the content online, adopting such a limited approach in an increasingly global community risks missing important details and trends that might only be available when text in multiple languages is taken into account. In this paper, we introduce Multilingual Supervised Latent Dirichlet Allocation (M L SLDA), a model for sentiment analysis on a multilingual corpus. M L SLDA discovers a consistent, unified picture of sentiment across multiple languages by learning “topics,” probabilistic partitions of the vocabulary that are consistent in terms of"
D10-1005,D08-1014,0,0.0881432,"Missing"
D10-1005,D07-1109,1,0.703866,"topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge, since it is well known, offers convenient and intuitive terminology, and demonstrates the full flexibility of our approach. However, the model we describe generalizes to any tree-structured representation of multilingual knowledge; we"
D10-1005,N09-1009,0,0.0113895,"1 The latter property has also made LDA popular for information retrieval (Wei and Croft, 2006)). 46 p(hˇao|z) all tend to be high at the same time, or low at the same time. More generally, the structure of our model must encourage topics to be consistent across languages, and Dirichlet distributions cannot encode correlations between elements. One possible solution to this problem is to use the multivariate normal distribution, which can produce correlated multinomials (Blei and Lafferty, 2005), in place of the Dirichlet distribution. This has been done successfully in multilingual settings (Cohen and Smith, 2009). However, such models complicate inference by not being conjugate. Instead, we appeal to tree-based extensions of the Dirichlet distribution, which has been used to induce correlation in semantic ontologies (Boyd-Graber et al., 2007) and to encode clustering constraints (Andrzejewski et al., 2009). The key idea in this approach is to assume the vocabularies of all languages are organized according to some shared semantic structure that can be represented as a tree. For concreteness in this section, we will use WordNet (Miller, 1990) as the representation of this multilingual semantic bridge,"
D10-1005,N09-1057,1,0.455555,"ary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which wou"
D10-1005,P08-1088,0,0.0205951,"Missing"
D10-1005,isahara-etal-2008-development,0,0.013282,"“is a” more specific instantiation of its parent concept (thus, hyponomy is often called an “isa” relationship). For example, a “dog” is a “canine” is an “animal” is a “living thing,” etc. As an approximation, it is not unreasonable to assume that WordNet’s structure of meaning is language independent, i.e. the concept encoded by a synset can be realized using terms in different languages that share the same meaning. In practice, this organization has been used to create many alignments of international WordNets to the original English WordNet (Ordan and Wintner, 2007; Sagot and Fiˇser, 2008; Isahara et al., 2008). Using the structure of WordNet, we can now describe a generative process that produces a distribution over a multilingual vocabulary, which encourages correlations between words with similar meanings regardless of what language each word is in. For each synset h, we create a multilingual word distribution for that synset as follows: 1. Draw transition probabilities βh ∼ Dir (τh ) 2. Draw stop probabilities ωh ∼ Dir (κh ) 3. For each language l, draw emission probabilities for that synset φh,l ∼ Dir (πh,l ). For conciseness in the rest of the paper, we will refer to this generative process as"
D10-1005,P10-1117,0,0.0172073,"a” (Chinese for “I’m afraid that . . . ”) and “tuo” (Chienese for “discard”), and positive sentiment-bearing topics have reasonable words such as “great,” “good,” and “juwel” (German for “jewel”). The qualitative topics also betray some of the weaknesses of the model. For example, in one of the negative sentiment topics, the German word “gut” (good) is present. Because topics are distributions over words, they can encode the presence of negations like “kein” (no) and “nicht” (not), but not collocations like “nicht gut.” More elaborate topic models that can model local syntax and collocations (Johnson, 2010) provide options for addressing such problems. We do not report the results for sentiment prediction for this corpus because the baseline of predicting a positive review is so strong; most algorithms do extremely well by always predicting a positive review, ours included. 4.3 Sentiment Prediction We gathered 330 film reviews from a German film review site (Vetter et al., 2000) and combined them with a much larger English film review corpus of over god us religion church human himmel gedanken glaube unsere kirche wahrheit -1.2 diet food eat weight eating healthy fat -0.4 book books one life per"
D10-1005,2005.mtsummit-papers.11,0,0.0134009,"distinct from the per-language expression. with these techniques, constructing appropriate hierarchies from these resources required many arbitrary decisions about cutoffs and which words to include. Thus, we do not consider them in this paper. 4 Experiments We evaluate M L SLDA on three criteria: how well it can discover consistent topics across languages for matching parallel documents, how well it can discover sentiment-correlated word lists from nonaligned text, and how well it can predict sentiment. 4.1 Matching on Multilingual Topics We took the 1996 documents from the Europarl corpus (Koehn, 2005) using three bridges: GermaNet, dictionary, and the uninformative flat matching.4 The model is unaware that the translations of documents in one language are present in the other language. Note that this does not use the supervised framework 4 For English and German documents in all experiments, we removed stop words (Loper and Bird, 2002), stemmed words (Porter and Boulton, 1970), and created a vocabulary of the most frequent 5000 words per language (this vocabulary limit was mostly done to ensure that the dictionary-based bridge was of manageable size). Documents shorter than fifty content w"
D10-1005,P07-1055,0,0.0135154,"s to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws away information important to sentiment analysis like syntactic constructions (Greene and Resnik, 2009) and document structure (McDonald et al., 2007) that may impact the sentiment rating. Second, a single real number is not always sufficient to capture the nuances of sentiment. Less critically, assuming that sentiment is normally distributed is not true of all real-world corpora; review corpora often have a skew toward positive reviews. We standardize responses by the mean and variance of the training data to partially address this issue, but other response distributions are possible, such as generalized linear models (Blei and McAuliffe, 2007) and vector machines (Zhu et al., 2009), which would allow more traditional classification predic"
D10-1005,D09-1092,0,0.0675304,"enough data, a monolingual model is no longer helped by adding additional multilingual data. 5 Relationship to Previous Research The advantages of M L SLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another langu"
D10-1005,P05-1015,0,0.0171248,"gt” (convinced) have positive regression parameters. For the German-Chinese corpus, note the presence of “gut” (good) in one of the negative sentiment topics, showing the difficulty of learning collocations. Train DE EN EN + DE Test DE DE DE GermaNet 73.8 7.44 1.17 Dictionary 24.8 2.68 1.46 Flat 92.2 18.3 1.39 Table 1: Mean squared error on a film review corpus. All results are on the same German test data, varying the training data. Over-fitting prevents the model learning on the German data alone; adding English data to the mix allows the model to make better predictions. 5000 film reviews (Pang and Lee, 2005) to create a multilingual film review corpus.6 The results for predicting sentiment in German documents with 25 topics are presented in Table 1. On a small monolingual corpus, prediction is very poor. The model over-fits, especially when it has the entire vocabulary to select from. The slightly better performance using GermaNet and a dictionary as topic priors can be viewed as basic feature selection, removing proper names from the vocabulary to 6 We followed Pang and Lee’s method for creating a numerical score between 0 and 1 from a star rating. We then converted that to an integer by multipl"
D10-1005,P06-1003,0,0.0410904,"Missing"
D10-1005,P95-1050,0,0.0911089,"Missing"
D10-1005,W03-0404,0,0.0344506,"able. Although GermaNet is richer, its coverage is incomplete; the dictionary structure had a much larger vocabulary and could build a more complete multilingual topics. Using comparable input information, this more flexible model performed better on the matching task than the existing multilingual topic model available for unaligned text. The degenerate flat bridge did no better than the baseline of random guessing, as expected. 4.2 Qualitative Sentiment-Correlated Topics One of the key tasks in sentiment analysis has been the collection of lists of words that convey sentiment (Wilson, 2008; Riloff et al., 2003). These resources are often created using or in reference to resources like WordNet (Whitelaw et al., 2005; Baccianella and Sebastiani, 2010). M L SLDA provides a method for extracting topical and sentimentcorrelated word lists from multilingual corpora. If was updated more frequently. 51 a WordNet-like resource is used as the bridge, the resulting topics are distributions over synsets, not just over words. As our demonstration corpus, we used the Amherst Sentiment Corpus (Constant et al., 2009), as it has documents in multiple languages (English, Chinese, and German) with numerical assessment"
D10-1005,P08-1036,0,0.0331898,"ation. For example, the vocabulary hierarchies could be structured to encourage topics that encourage correlation among similar sentiment-bearing words (e.g. clustering words associated with price, size, etc.). Future work could also more rigorously validate that the multilingual topics discovered by M L SLDA are sentiment-bearing via human judgments. In contrast, M L SLDA draws on techniques that view sentiment as a regression problem based on the topics used in a document, as in supervised latent Dirichlet allocation (SLDA) (Blei and McAuliffe, 2007) or in finer-grained parts of a document (Titov and McDonald, 2008). Extending these models to multilingual data would be more straightforward. 6 Conclusions M L SLDA is a “holistic” statistical model for multilingual corpora that does not require parallel text or expensive multilingual resources. It discovers connections across languages that can recover latent structure in parallel corpora, discover sentimentcorrelated word lists in multiple languages, and make accurate predictions across languages that improve with more multilingual data, as demonstrated in the context of sentiment analysis. More generally, M L SLDA provides a formalism that can be used to"
D10-1005,P09-1027,0,0.0257049,"y, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the topics present in a document. This is a modeling decision with pros and cons. It allows a language agnostic decision about sentiment to be made, but it restricts the expressiveness of the model in terms of sentiment in two ways. First, it throws a"
D10-1005,P06-2124,0,0.282652,"ing additional multilingual data. 5 Relationship to Previous Research The advantages of M L SLDA reside largely in the assumptions that it makes and does not make: documents need not be parallel, sentiment is a normally distributed document-level property, words are exchangeable, and sentiment can be predicted as a regression on a K-dimensional vector. By not assuming parallel text, this approach can be applied to a broad class of corpora. Other multilingual topic models require parallel text, either at the document (Ni et al., 2009; Mimno et al., 2009) or word-level (Kim and Khudanpur, 2004; Zhao and Xing, 2006). Similarly, other multilingual sentiment approaches also require parallel text, often supplied via automatic translation; after the translated text is available, either monolingual analysis (Denecke, 2008) or co-training is applied (Wan, 2009). In contrast, our approach requires fewer resources for a language: a dictionary (or similar knowledge structure relating words to nodes in a graph) and comparable text, instead of parallel text or a machine translation system. Rather than viewing one language through the lens of another language, M L SLDA views all languages through the lens of the top"
D10-1005,P06-4018,0,\N,Missing
D10-1005,I05-3027,0,\N,Missing
D10-1005,W02-0109,0,\N,Missing
D10-1013,E09-1008,0,0.0271209,"qualified bilingual translator is absolutely necessary. However, it does include a great many real-world scenarios, such as following news reports in another country, reading international comments about a product, or generating a decent first draft translation of a Wikipedia page for Wikipedia editors to improve. The use of monolingual participants in a humanmachine translation process is not entirely new. Callison-Burch et al. (2004) pioneered the exploration of monolingual post-editing within the MT community, an approach extended more recently to provide richer information to the user by Albrecht et al. (2009) and Koehn (2009). There have also been at least two independently developed human-machine translation frameworks that employ an iterative protocol involving monolinguals on both the source and target side. Morita and Ishida (2009) describe a system in which target and source language speakers perform editing of MT output to improve fluency and adequacy, respectively; they utilize source-side paraphrasing at a course grain level, although their approach is limited to requests to paraphrase the entire sentence when the translation cannot be understood. 127 Proceedings of the 2010 Conference on"
D10-1013,P05-1074,0,0.235376,"is given a sentence with a phrase span marked, and is asked to replace the marked text with a different way of saying the same thing, so that the resulting sentence still makes sense and means the same thing as the original sentence. To illustrate in English, someone seeing John and Mary took a European vacation this summer might supply the paraphrase Mary went on a European, verifying that the resulting John and Mary went on a European vacation this summer preserves the original meaning. This step can also be fully automated (Max, 2009) by taking advantage of bilingual phrasetable pivoting (Bannard and Callison-Burch, 2005); see Max (2010), in these proceedings, for a related approach in which the paraphrases of a source phrase are used to refine the estimated probability distribution over its possible target phrases. Generating sentential source paraphrases. For each sentence, there may be multiple paraphrased spans. These are multiplied out to provide fullsentence paraphrases. For example, if two nonoverlapping source spans are each paraphrased in three ways, we generate 9 sentential source paraphrases, each of which represents an alternative way of expressing the original sentence. Machine translation of alte"
D10-1013,W04-1408,0,0.0285079,"ranslation quality. 1 Introduction For most of the world’s languages, the availability of translation is limited to two possibilities: high quality at high cost, via professional translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with non-professional translators, which can be remarkably effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu In this paper, we report on a new exploration of the middle ground, taking advantage of a virtually unutilized resource:"
D10-1013,W10-0735,1,0.854781,"ns on the target side, human generation of paraphrases for problematic sub-sentential spans on the source side, and both automatic hypothesis selection and human selection (via fluency ratings, in Section 3). In this section, we take a step toward more automated processing, replacing human identification of mistranslated spans with an a fully automatic method.6 The idea behind our automatic error identification is straightforward: if the source sentence 5 “Gains” refer to a lower score: since TERp is an error measure, lower is better. 6 This section contains material we originally reported in Buzek et al. (2010). GT: WTO chief negotiator on behalf of the United States to propose substantial reduction of agricultural subsidies, Kai Fa countries substantially reduce industrial products import tariffs to Dapo ?? Doha Round of negotiations deadlock. TP: World Trade Organization negotiator suggested the United States today, a substantial reduction of agricultural subsidies, developing countries substantially reduce industrial products?? Import tariffs, in order to break the deadlock in the Doha Round of trade negotiations. REF: the main delegates at the world trade organization talks today suggested that"
D10-1013,2004.eamt-1.4,0,0.743909,"Missing"
D10-1013,D09-1030,0,0.0651753,"Missing"
D10-1013,D10-1041,0,0.167044,"istranslated spans. This step identifies parts of the source sentence that lead to ungrammatical, nonsensical, or apparently incorrect translations on the target side. In the experiments of Sections 3 and 4, this step is performed by having monolingual target speakers identify likely error spans on the target side, as in the French example above, and projecting those spans back to the source spans that generated them using word alignments as the bridge (Hwa et al., 2005; Yarowsky et al., 2001). In Section 5, we describe a heuristic but effective method for performing this fully automatically. Du et al. (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al., 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. Source paraphrase generation. This step generates alternative expressions for the source spans identified in the previous step. In this paper, it is performed by monolingual source speakers who perform the paraphrase task: the speaker is given a sentence with a phrase span marked, and is asked to replace the marked text with a different way of saying t"
D10-1013,P08-1115,1,0.836049,"ng up in Section 6 with conclusions and directions for future work. 2 Targeted Paraphrasing The starting point for our approach is an observation: the source sentence provided as input to an MT system is just one of many ways in which the meaning could have been expressed, and for any given MT system, some forms of expression are easier to translate than others. The same basic observation has been applied quite fruitfully over the past several years to deal with statistical MT challenges involving segmentation, morphological analysis, and more recently, source language word order (Dyer, 2007; Dyer et al., 2008; Dyer and Resnik, 2010). Here we apply it to the surface expression of meaning. For example, consider the following real example of translation from English to French by an automatic MT system: • Source: Polls indicate Brown, a state senator, and Coakley, Massachusetts’ Attorney General, are locked in a virtual tie to fill the late Sen. Ted Kennedy’s Senate seat. 128 • System: Les sondages indiquent Brown, un s´enateur d’´etat, et Coakley, Massachusetts’ Procureur g´en´eral, sont enferm´es dans une cravate virtuel a` remplir le regrett´e s´enateur Ted Kennedy’s si`ege au S´enat. A French spea"
D10-1013,W07-0729,0,0.0257266,"efore wrapping up in Section 6 with conclusions and directions for future work. 2 Targeted Paraphrasing The starting point for our approach is an observation: the source sentence provided as input to an MT system is just one of many ways in which the meaning could have been expressed, and for any given MT system, some forms of expression are easier to translate than others. The same basic observation has been applied quite fruitfully over the past several years to deal with statistical MT challenges involving segmentation, morphological analysis, and more recently, source language word order (Dyer, 2007; Dyer et al., 2008; Dyer and Resnik, 2010). Here we apply it to the surface expression of meaning. For example, consider the following real example of translation from English to French by an automatic MT system: • Source: Polls indicate Brown, a state senator, and Coakley, Massachusetts’ Attorney General, are locked in a virtual tie to fill the late Sen. Ted Kennedy’s Senate seat. 128 • System: Les sondages indiquent Brown, un s´enateur d’´etat, et Coakley, Massachusetts’ Procureur g´en´eral, sont enferm´es dans une cravate virtuel a` remplir le regrett´e s´enateur Ted Kennedy’s si`ege au S´"
D10-1013,P04-3001,0,0.0684224,"Missing"
D10-1013,W09-0431,0,0.042171,"Missing"
D10-1013,P06-2061,0,0.0166988,"tion is limited to two possibilities: high quality at high cost, via professional translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with non-professional translators, which can be remarkably effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu In this paper, we report on a new exploration of the middle ground, taking advantage of a virtually unutilized resource: speakers of the source and target language who are effectively monolingual, i.e. who each only"
D10-1013,P09-4005,0,0.016951,"ator is absolutely necessary. However, it does include a great many real-world scenarios, such as following news reports in another country, reading international comments about a product, or generating a decent first draft translation of a Wikipedia page for Wikipedia editors to improve. The use of monolingual participants in a humanmachine translation process is not entirely new. Callison-Burch et al. (2004) pioneered the exploration of monolingual post-editing within the MT community, an approach extended more recently to provide richer information to the user by Albrecht et al. (2009) and Koehn (2009). There have also been at least two independently developed human-machine translation frameworks that employ an iterative protocol involving monolinguals on both the source and target side. Morita and Ishida (2009) describe a system in which target and source language speakers perform editing of MT output to improve fluency and adequacy, respectively; they utilize source-side paraphrasing at a course grain level, although their approach is limited to requests to paraphrase the entire sentence when the translation cannot be understood. 127 Proceedings of the 2010 Conference on Empirical Methods"
D10-1013,P84-1051,0,0.487717,"provements in translation quality. 1 Introduction For most of the world’s languages, the availability of translation is limited to two possibilities: high quality at high cost, via professional translators, and low quality at low cost, via machine translation (MT). The spectrum between these two extremes is very poorly populated, and at any point on the spectrum the ready availability of translation is limited to only a small fraction of the world’s languages. There is, of course, a long history of technological assistance to translators, improving cost effectiveness using translation memory (Laurian, 1984; Bowker and Barlow, 2004) or other interactive tools to assist translators (Esteban et al., 2004; Khadivi et al., 2006). And there is a recent and rapidly growing interest in crowdsourcing with non-professional translators, which can be remarkably effective (Munro, 2010). However, all these alternatives face a central availability bottleneck: they require the participation of humans with bilingual expertise. Benjamin B. Bederson Computer Science and UMIACS University of Maryland bederson@cs.umd.edu In this paper, we report on a new exploration of the middle ground, taking advantage of a virtu"
D10-1013,W09-2503,0,0.0228749,"ngual source speakers who perform the paraphrase task: the speaker is given a sentence with a phrase span marked, and is asked to replace the marked text with a different way of saying the same thing, so that the resulting sentence still makes sense and means the same thing as the original sentence. To illustrate in English, someone seeing John and Mary took a European vacation this summer might supply the paraphrase Mary went on a European, verifying that the resulting John and Mary went on a European vacation this summer preserves the original meaning. This step can also be fully automated (Max, 2009) by taking advantage of bilingual phrasetable pivoting (Bannard and Callison-Burch, 2005); see Max (2010), in these proceedings, for a related approach in which the paraphrases of a source phrase are used to refine the estimated probability distribution over its possible target phrases. Generating sentential source paraphrases. For each sentence, there may be multiple paraphrased spans. These are multiplied out to provide fullsentence paraphrases. For example, if two nonoverlapping source spans are each paraphrased in three ways, we generate 9 sentential source paraphrases, each of which repre"
D10-1013,D10-1064,0,0.0518449,"marked, and is asked to replace the marked text with a different way of saying the same thing, so that the resulting sentence still makes sense and means the same thing as the original sentence. To illustrate in English, someone seeing John and Mary took a European vacation this summer might supply the paraphrase Mary went on a European, verifying that the resulting John and Mary went on a European vacation this summer preserves the original meaning. This step can also be fully automated (Max, 2009) by taking advantage of bilingual phrasetable pivoting (Bannard and Callison-Burch, 2005); see Max (2010), in these proceedings, for a related approach in which the paraphrases of a source phrase are used to refine the estimated probability distribution over its possible target phrases. Generating sentential source paraphrases. For each sentence, there may be multiple paraphrased spans. These are multiplied out to provide fullsentence paraphrases. For example, if two nonoverlapping source spans are each paraphrased in three ways, we generate 9 sentential source paraphrases, each of which represents an alternative way of expressing the original sentence. Machine translation of alternative sentence"
D10-1013,2006.amta-papers.25,0,0.193397,"Missing"
D10-1013,P10-1063,0,0.0136122,"onstituent of the sentence containing all of the tokens in a particular error string. At times, these constituents can be quite large, even the entire sentence. To weed out these cases, we restrict constituent length to no more than 7 tokens. For example, given F The most recent probe to visit Jupiter was the Pluto-bound New Horizons spacecraft in late February 2007. E La investigaci´on m´as reciente fue la visita de J´upiter a Plut´on de la envolvente sonda New Horizons a fines de febrero de 2007. 7 Exactly the same insight is behind the “source-side pseudoreferencebased feature” employed by Soricut and Echihabi (2010) in their system for predicting the trustworthiness of translations. 8 It is possible that the difficulty so identified involves backtranslation only, not translation in the original direction. If that is the case, then more paraphrasing will be done than necessary, but the quality of the TP process’s output should not suffer. 134 F’ The latest research visit Jupiter was the Pluto-bound New Horizons spacecraft in late February 2007. spans in the the bolded phrase in F would be identified, based on the TERp alignment and smallest containing constituent as shown in Figure 3. In order to evaluate"
D10-1013,H01-1035,0,0.0102538,"ed in this role, potentially at some cost to quality, by performing post hoc target-to-source alignment. Identification of mistranslated spans. This step identifies parts of the source sentence that lead to ungrammatical, nonsensical, or apparently incorrect translations on the target side. In the experiments of Sections 3 and 4, this step is performed by having monolingual target speakers identify likely error spans on the target side, as in the French example above, and projecting those spans back to the source spans that generated them using word alignments as the bridge (Hwa et al., 2005; Yarowsky et al., 2001). In Section 5, we describe a heuristic but effective method for performing this fully automatically. Du et al. (2010), in this proceedings, explore the use of source paraphrases without targeting apparent mistranslations, using lattice translation (Dyer et al., 2008) to efficiently represent and decode the resulting very large space of paraphrase alternatives. Source paraphrase generation. This step generates alternative expressions for the source spans identified in the previous step. In this paper, it is performed by monolingual source speakers who perform the paraphrase task: the speaker i"
D10-1013,N04-1021,0,\N,Missing
D10-1028,N10-1081,0,0.298245,", in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association"
D10-1028,N09-1057,1,0.894952,"Missing"
D10-1028,P07-1107,0,0.0220563,"st to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB"
D10-1028,N09-1036,0,0.0230009,"er takes in a set of documents and replaces all words not in the vocabulary with “out of vocabulary” markers. This process ensures that in all experiments the vocabulary is composed entirely of words from the training set. After the groups have been filtered, the group used as the test set has its labels removed. The test and training set are then sent, along with the grammar, into the adaptor grammar inference engine. Each experiment ran for 3000 iterations. For the runs where adaptation was used we set the initial Pitman-Yor a and b parameters to 0.01 and 10 respectively, then slice sample (Johnson and Goldwater, 2009). We use the resulting sentence parses for classification. By design of the grammar, each sentence’s words will belong to one and only one distribution. We identify that distribution from each of the test set sentence parses and use it as the sentence level classification for that particular sentence. We then use majority rule on the individual sentence classifications in a document to obtain the document classification. (In most cases the sentence-level assignments are overwhelmingly dominated by one class.) 3.3 Results and Analysis Table 4 gives the results and compares to prior work. The su"
D10-1028,P08-1046,0,0.112975,"y of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, U"
D10-1028,P10-1117,0,0.733562,"known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıve Bayes associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics by the properties of th"
D10-1028,D07-1072,0,0.0190581,"ity. In this paper, we employ nonparametric Bayesian models (Orbanz and Teh, 2010) in order to address this limitation. In contrast to parametric models, for which a fixed number of parameters are specified in advance, nonparametric models can “grow” to the size best suited to the observed data. In text analysis, models of this type have been employed primarily for unsupervised discovery of latent structure — for example, in topic modeling, when the true number of topics is not known (Teh et al., 2006); in grammatical inference, when the appropriate number of nonterminal symbols is not known (Liang et al., 2007); and in coreference resolution, when the number of entities in a given document is not specified in advance (Haghighi and Klein, 2007). Here we use them for supervised text classification. Specifically, we use adaptor grammars (Johnson et al., 2006), a formalism for nonparametric Bayesian modeling that has recently proven useful in unsupervised modeling of phonemes (Johnson, 2008), grammar induction (Cohen et al., 2010), and named entity structure learning (Johnson, 2010), to make supervised na¨ıve Bayes classification nonparametric in order to improve perspective modeling. Intuitively, na¨ıv"
D10-1028,W06-2915,0,0.768987,"es associates each class or label with a probability distribution over a fixed vocabulary. We introduce adaptive na¨ıve Bayes (ANB), for which in principle the vocabulary can grow as needed to include collocations of arbitrary length, as determined 284 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 284–292, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics by the properties of the dataset. We show that using adaptive na¨ıve Bayes improves on state of the art classification using the Bitter Lemons corpus (Lin et al., 2006), a document collection that has been used by a variety of authors to evaluate perspective classification. In Section 2, we review adaptor grammars, show how na¨ıve Bayes can be expressed within the formalism, and describe how — and how easily — an adaptive na¨ıve Bayes model can be created. Section 3 validates the approach via experimentation on the Bitter Lemons corpus. In Section 4, we summarize the contributions of the paper and discuss directions for future work. 2 Adapting Na¨ıve Bayes to be Less Na¨ıve In this work we apply the adaptor grammar formalism introduced by Johnson, Griffiths,"
D10-1052,P09-1088,1,0.923747,"association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntacti"
D10-1052,J93-2003,0,0.0769298,"state-of-the-art translation models has focused on long-distance reordering, but its counterpart in alignment models has remained focused on local reordering, typically modeling distortion based entirely on positional information. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational comp"
D10-1052,N10-1015,0,0.0149489,"heless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics i"
D10-1052,P06-2014,0,0.0495046,"t the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntactic divergences that drive the reordering. To our knowledge, however, this approach has been used mainly to constrain reordering possibilities, or to add to the generalization ability of association-based scores, not to directly model reordering in the context of alignment. 534 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534–544, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics In this paper, we introduce a"
D10-1052,P05-1033,0,0.0293107,"pair. Our training data consists of manually aligned corpora available from LDC (LDC2006E93 and LDC2008E57) and unaligned corpora, which include FBIS, ISI, HKNews and Xinhua. In total, the manually aligned corpora consist of more than 21 thousand sentence pairs, while the unaligned corpora consist of more than 710 thousand sentence pairs. The manually-aligned corpora are primarily used for training the reordering models and for discriminative training purposes. For translation experiments, we used cdec (Dyer et al., 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. We constructed the list of function words in English manually and in Chinese from (Howard, 2002). Punctuation marks were added to the list, resulting in 883 and 359 tokens in the Chinese and English lists, respectively. For the alignment experiments, we took the first 500 sentence pairs from the newswire genre of the manually-aligned corpora and used the first 250 sentences as the development set, with the remaining 250 as the test set. To ensure blind experimentation, we excluded these sentence pairs from the training of the features,"
D10-1052,P07-1003,0,0.0143765,"ation. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provide"
D10-1052,J93-1003,0,0.0181782,"o predict alignments, we use a linear model of the following form: Aˆ = arg max A∈A(S,T ) θ · f (A, S, T ) (9) where A(S, T ) is the set of all possible alignments of a source sentence S and target sentence T , and f (A, S, T ) is a vector of feature functions on A, S, and T , and θ is a parameter vector. In addition to the six reordering models, our model employs several association-based scores that look at alignments in isolation. These features include: 1. Normalized log-likelihood ratio (LLR). This feature represents an association score, derived from statistical testing statistics. LLR (Dunning, 1993) has been widely used especially to measure lexical association. Since the values of LLR are unnormalized, we normalize them on a per-sentence basis, so that the normalized LLRs of, say, a particular source word to the target words in a particular sentence sum up to one. 2. Translation table from IBM model 4. This feature represents another association score, derived from a generative model, in particular the wordbased IBM model 4. The use of this feature is widespread in recent alignment models, since it provides a relatively accurate initial prediction. 3. Translation table from manually-ali"
D10-1052,P10-4002,1,0.818476,"xtrinsically on a large-scale translation task, focusing on ChineseEnglish as the language pair. Our training data consists of manually aligned corpora available from LDC (LDC2006E93 and LDC2008E57) and unaligned corpora, which include FBIS, ISI, HKNews and Xinhua. In total, the manually aligned corpora consist of more than 21 thousand sentence pairs, while the unaligned corpora consist of more than 710 thousand sentence pairs. The manually-aligned corpora are primarily used for training the reordering models and for discriminative training purposes. For translation experiments, we used cdec (Dyer et al., 2010), a fast implementation of hierarchical phrase-based translation models (Chiang, 2005), which represents a state-of-the-art translation system. We constructed the list of function words in English manually and in Chinese from (Howard, 2002). Punctuation marks were added to the list, resulting in 883 and 359 tokens in the Chinese and English lists, respectively. For the alignment experiments, we took the first 500 sentence pairs from the newswire genre of the manually-aligned corpora and used the first 250 sentences as the development set, with the remaining 250 as the test set. To ensure blind"
D10-1052,D07-1006,0,0.0725776,"mply the set of alignments {A(1) , A(2) , . . . , A(n) } encountered in the stochastic search. While MERT does not scale to large numbers of features, the scarcity of manually aligned training data also means that models with large numbers of sparse features would be difficult to learn discriminatively, so this limitation is somewhat inherent in the problem space. Additionally, MERT has several advantages that make it particularly useful for our task. First, we can optimize F-measure of the alignments directly, which has been shown to correlate with translation quality in a downstream system (Fraser and Marcu, 2007b). Second, we are optimizing the quality of the 1-best alignments under the model. Since translation pipelines typically use only a single word alignment, this criterion is appropriate. Finally, and very importantly for us, MERT requires only an approximation of the model’s hypothesis space to carry out optimization. Since we are using a stochastic search, this is crucial, since subsequent evaluations of the same sentence pair (even with the same weights) may result in a different candidate set. Although MERT is a non-probabilistic optimizer, we explore the alignment space stochastically. Thi"
D10-1052,J07-3002,0,0.233907,"mply the set of alignments {A(1) , A(2) , . . . , A(n) } encountered in the stochastic search. While MERT does not scale to large numbers of features, the scarcity of manually aligned training data also means that models with large numbers of sparse features would be difficult to learn discriminatively, so this limitation is somewhat inherent in the problem space. Additionally, MERT has several advantages that make it particularly useful for our task. First, we can optimize F-measure of the alignments directly, which has been shown to correlate with translation quality in a downstream system (Fraser and Marcu, 2007b). Second, we are optimizing the quality of the 1-best alignments under the model. Since translation pipelines typically use only a single word alignment, this criterion is appropriate. Finally, and very importantly for us, MERT requires only an approximation of the model’s hypothesis space to carry out optimization. Since we are using a stochastic search, this is crucial, since subsequent evaluations of the same sentence pair (even with the same weights) may result in a different candidate set. Although MERT is a non-probabilistic optimizer, we explore the alignment space stochastically. Thi"
D10-1052,P09-1104,0,0.0397026,"syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the"
D10-1052,D09-1024,0,0.0114907,"ation with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model. 9 Conclusions Languages exhibit regularities of word order that are preserved when projected to another language. We use the notion of function words to infer such regularities, resulting in several reordering models that are employed as features in a discriminative alignment model. In particular, our models predict the reordering of function words by looking at their dependencies with respect to their neighboring phrases, their neighboring function words, and the sentence boundaries. By capturing such lon"
D10-1052,N03-1017,0,0.0555603,"n) plus the 5 best alignments according to the log-likelihood ratio. 4 Using only the A LIGN operator, it is possible to explore the full alignment space; however, using all three operators increases mobility. 540 l&apos; (a) m&apos; m A′ ∈N (A(i) ) In addition to the current ‘active’ alignment configuration A(i) , the algorithm keeps track of the highest scoring alignment observed so far, Amax . After n steps, the algorithm returns Amax as its approximaˆ In the experiments reported below, we tion of A. initialized A(1) with the Model 4 alignments symmetrized by using the grow-diag-final-and heuristic (Koehn et al., 2003). l l&apos; (b) m&apos; m (c) m&apos; Figure 3: Illustrations for (a) A LIGN, (b) A LIGN E XCLU SIVE , and (c) S WAP operators, as applied to align the dotted, smaller circle (l, m) to (l, m′ ). The left hand side represents A(i) , while the right hand side represents a candidate for A(i+1) . The solid circles represent the new alignment points added to A(i+1) . 6 Discriminative Training To set the model parameters θ, we used the minimum error rate training (MERT) algorithm (Och, 2003) to maximize the F-measure of the 1-best alignment of the model on a development set consisting of sentence pairs with manual"
D10-1052,W02-1018,0,0.0129243,"U points. 8 Related Work The focus of our work is to strengthen the reordering component of alignment modeling. Although the de facto standard, the IBM models do not generalize well in practice: the IBM approach employs a series of reordering models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the"
D10-1052,P04-1066,0,0.0373192,"rk The focus of our work is to strengthen the reordering component of alignment modeling. Although the de facto standard, the IBM models do not generalize well in practice: the IBM approach employs a series of reordering models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model propose"
D10-1052,P06-1090,0,0.0218572,"e pair and h/si is the end-of-sentence marker. 3.1 Six (Sub-)Models To model o(Li,S→T ), o(Ri,S→T ), i.e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by Setiawan et al. (2007). Formally, this model takes the form of probability distribution Pori (o(Li,S→T ), o(Ri,S→T )|Yi,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro2 This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R. 537 jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. To model d(F Wi−1,S→T ), d(F Wi+1,S→T ), i.e. whether Li,S→T and Ri,S→T ext"
D10-1052,J04-4002,0,0.109325,"→ T denotes the projection direction from source to target. The subscript for the other direction is T → S. tic approach. For instance, to estimate the spans of Li,S→T , Ri,S→T , our reordering model assumes that any span to the left of Yi,S is a possible Li,S and any span to the right of Yi,S is a possible Ri,S , deciding which is most probable via features, rather than committing to particular spans (e.g. as defined by a monolingual text chunker or parser). We only enforce one criterion on Li,S→T and Ri,S→T : they have to be the maximal alignment blocks satisfying the consistent heuristic (Och and Ney, 2004) that end or start with Yi,S→T on the source S side respectively.2 To infer these phrases, we decompose Li,S→T into (o(Li,S→T ), d(F Wi−1,S→T ), b(hsi)); similarly, Ri,S→T into (o(Ri,S→T ),d(F Wi+1,S→T ), b(h/si) )). Taking the decomposition of Li,S→T as a case in point, here o(Li,S→T ) describes the reordering of the left neighbor Li,S→T with respect to the function word Yi,S→T , while d(F Wi−1,S→T ) and b(hsi)) probe the span of Li,S→T , i.e. whether it goes beyond the preceding function word phrase pairs F Wi−1,S→T and up to the beginning-ofsentence marker hsi respectively. The same definit"
D10-1052,P03-1021,0,0.234047,", our reordering model enumerates the function words on both source and target sides, modeling their reordering relative to their neighboring phrases, their neighboring function words, and the sentence boundaries. Because the frequency of function words is high, we find that by predicting the reordering of function words accurately, the reordering of the remaining words improves in accuracy as well. In total, we introduce six sub-models involving function words, and these serve as features in a log linear model. We train model weights discriminatively using Minimum Error Rate Training (MERT) (Och, 2003), optimizing F-measure. The parameters of our sub-models are estimated from manually-aligned corpora, leading the reordering model more directly toward reproducing human alignments, rather than maximizing the likelihood of unaligned training data. This use of manual data for parameter estimation is a reasonable choice because these models depend on a small, fixed number of lexical items that occur frequently in language, hence only small training corpora are required. In addition, the availability of manually-aligned corpora has been growing steadily. The remainder of the paper proceeds as fol"
D10-1052,N10-1014,0,0.147938,"ny useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inference over the model. Some recent alignment models appeal to external linguistic knowledge, mostly by using monolingual syntactic parses (Cherry and Lin, 2006; Pauls et al., 2010), which at the same time, provides an approximation of the bilingual syntactic divergences that drive the reordering. To our knowledge, however, this approach has been used mainly to constrain reordering possibilities, or to add to the generalization ability of association-based scores, not to directly model reordering in the context of alignment. 534 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 534–544, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics In this paper, we introduce a new approach to impr"
D10-1052,P07-1090,1,0.846448,"of the left neighbor Li,S→T with respect to the function word Yi,S→T , while d(F Wi−1,S→T ) and b(hsi)) probe the span of Li,S→T , i.e. whether it goes beyond the preceding function word phrase pairs F Wi−1,S→T and up to the beginning-ofsentence marker hsi respectively. The same definition applies to the decomposition of Ri,S→T , where F Wi+1,S→T is the succeeding function word phrase pair and h/si is the end-of-sentence marker. 3.1 Six (Sub-)Models To model o(Li,S→T ), o(Ri,S→T ), i.e. the reordering of the neighboring phrases of a function word, we employ the orientation model introduced by Setiawan et al. (2007). Formally, this model takes the form of probability distribution Pori (o(Li,S→T ), o(Ri,S→T )|Yi,S→T ), which conditions the reordering on the lexical identity of the function word alignment (but independent of the lexical identity of its neighboring phrases). In particular, o maps the reordering into one of the following four orientation values (borrowed from Nagata et al. (2006)) with respect to the function word: Monotone Adjacent (MA), Monotone Gap (MG), Reverse Adjacent (RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original"
D10-1052,P09-1037,1,0.868891,"RA) and Reverse Gap (RG). The Monotone/Reverse distinction indicates whether the projected order follows the original order, while the Adjacent/Gap distinction indicates whether the pro2 This heuristic is commonly used in learning phrase pairs from parallel text. The maximality ensures the uniqueness of L and R. 537 jections of the function word and the neighboring phrase are adjacent or separated by an intervening phrase. To model d(F Wi−1,S→T ), d(F Wi+1,S→T ), i.e. whether Li,S→T and Ri,S→T extend beyond the neighboring function word phrase pairs, we utilize the pairwise dominance model of Setiawan et al. (2009). Taking d(F Wi−1,S→T ) as a case in point, this model takes the form Pdom (d(F Wi−1,S→T )|Yi−1,S→T , Yi,S→T ), where d takes one of the following four dominance values: leftFirst, rightFirst, dontCare, or neither. We will detail the exact formulation of these values in the next subsection. However, to provide intuition, the value of either leftFirst or neither for d(F Wi−1,S→T ) would suggest that the span of Li,S→T doesn’t extend to Yi−1,S→T ; the further distinction between leftFirst and neither concerns with whether the span of Ri−1,S→T extends to F Wi,S→T . To model b(hsi), b(h/si), i.e."
D10-1052,C96-2141,0,0.875717,"nslation models has focused on long-distance reordering, but its counterpart in alignment models has remained focused on local reordering, typically modeling distortion based entirely on positional information. This leaves most alignment decisions to association-based scores. Introduction In many Statistical Machine Translation (SMT) systems, alignment represents an important piece of information, from which translation rules are learnt. However, while translation models have evolved from word-based to syntax-based modeling, the de facto alignment model remains word-based (Brown et al., 1993; Vogel et al., 1996). This gap between alignment modeling and translation modeling is clearly undesirable as it often generates tensions that would prevent the extraction of many useful translation rules (DeNero and Klein, 2007). Recent work, e.g. by Blunsom et al. (2009) and Haghihi et Why is employing stronger reordering models more challenging in alignment than in translation? One answer can be attributed to the fact that alignment points are unobserved in parallel text, thus so are their reorderings. As such, introducing stronger reordering often further exacerbates the computational complexity to do inferenc"
D10-1052,J97-3002,0,0.239328,"models based on the word’s position, but reordering depends on syntactic context rather than absolute position in the sentence. Over the years, there have been many proposals to improve these reordering models, most notably Vogel et al. (1996), which adds a first-order dependency. Nevertheless, the use of these distortion-based models remains widespread (Marcu and Wong, 2002; Moore, 2004). Alignment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data"
D10-1052,P05-1059,0,0.0240985,"gnment modeling is challenging because it often has to consider a prohibitively large alignment space. Efforts to constrain the space generally comes from the use of Inversion Transduction Grammar (ITG) (Wu, 1997). Recent proposals that use ITG constraints include (Haghighi et al., 2009; Blunsom et al., 2009) just to name a few. More recent models have begun to use linguisticallymotivated constraints, often in combination with ITG, primarily exploiting monolingual syntactic information (Burkett et al., 2010; Pauls et al., 2010). Our reordering model is closely related to the model proposed by Zhang and Gildea (2005; 2006; 2007a), with respect to conditioning the reordering predictions on lexical items. These related models treat their lexical items as latent variables to be estimated from training data, while our model uses a fixed set of lexical items that correspond to the class of function words. With respect to the focus on function words, our reordering model is closely related to the UALIGN system (Hermjakob, 2009). However, UALIGN uses deep syntactic analysis and hand-crafted heuristics in its model. 9 Conclusions Languages exhibit regularities of word order that are preserved when projected to a"
D10-1052,P06-2122,0,0.0402909,"Missing"
D13-1133,J08-4004,0,0.0802249,"Missing"
D13-1133,N13-1021,0,0.0602565,"Missing"
D13-1133,W11-0610,0,0.0539506,"Missing"
D13-1133,N12-1012,0,0.0613365,"Missing"
D14-1182,D12-1101,0,0.0504196,"Missing"
D14-1182,D10-1005,1,0.868179,") 2. For each document d ∈ [1, D] (a) Draw topic distribution θd ∼ Dir(α) (b) For each word n ∈ [1, Nd ] i. Draw topic zd,n ∼ Mult(θd ) ii. Draw word wd,n ∼ Mult(φzd,n ) (c) Draw response yd ∼ N (η T z¯d , ρ) where PNd z¯d,k = N1d n=1 I [zd,n = k] where I [x] = 1 if x is true, and 0 otherwise. In SLDA, in addition to the K multinomials {φk }K k=1 , the global latent variables also contain the regression parameter ηk for each topic k. The local latent variables of SLDA resembles LDA’s: the topic proportion vector θd for each document d. Train: For posterior inference during training, following Boyd-Graber and Resnik (2010), we use stochastic EM, which alternates between (1) a Gibbs sampling step to assign a topic to each token, and (2) optimizing the regression parameters. The probability of assigning topic k to token n in the training document d is TR TR TR TR p(zd,n = k |z−d,n , w−d,n , wd,n = v) ∝ N (yd ; µd,n , ρ) · −d,n NTR ,d,k + α −d,n NTR ,d,· + Kα · −d,n NTR ,k,v + β −d,n NTR ,k,· + V β (9) PK −d,n where µd,n = ( k0 =1 ηk0 NTR ,d,k0 + ηk )/NTR,d is the TR mean of the Gaussian generating yd if zd,n = k. Here, NTR,d,k is the number of times topic k is assigned to tokens in the training document d; NTR,k,"
D14-1182,P12-1009,1,0.830051,"1 X S(i, TTE ). |TTR |i∈T In LDA, the global latent variables are topics {φk }K k=1 and the local latent variables for each document d are topic proportions θd . Train: During training, we use collapsed Gibbs sampling to assign each token in the training data with a topic (Steyvers and Griffiths, 2006). The probability of 1753 assigning token n of training document d to topic k is TR TR TR TR p(zd,n = k |z−d,n , w−d,n , wd,n = v) ∝ −d,n NTR ,d,k + α −d,n NTR ,d,· + Kα · −d,n NTR ,k,v + β −d,n NTR ,k,· +Vβ 1240 1160 (5) 2150 Test: Because we lack explicit topic annotations for these data (c.f. Nguyen et al. (2012)), we use perplexity– a widely-used metric to measure the predictive power of topic models on held-old documents. To compute perplexity, we follow the estimating θ method (Wallach et al., 2009, Section 5.1) and evenly split each test document d into wdTE1 and wdTE2 . We first run Gibbs sampling on wdTE1 to estimate the topic proportion θˆdTE of test document d. The probability of assigning topic k TE1 TE 1 ˆ to token n in wdTE1 is p(zd,n = k |z−d,n , w TE1 , φ(i)) ∝ · φˆk,w TE1 (i) where NTE1 ,d,k is the number of tokens in wdTE1 assigned to topic k. At each iteration j in test chain i, we can"
D14-1182,P05-1015,0,0.239662,"Missing"
D14-1182,D09-1026,0,0.0674365,"analysis, and summarization of the otherwise unstructured corpora (Blei, 2012; Blei, 2014). In addition to exploratory data analysis, a typical goal of topic models is prediction. Given a set of unannotated training data, unsupervised topic models try to learn good topics that can generalize to unseen text. Supervised topic models jointly capture both the text and associated metadata such as a continuous response variable (Blei and McAuliffe, 2007; Zhu et al., 2009; Nguyen et al., 2013), single label (Rosen-Zvi et al., 2004; Lacoste-Julien et al., 2008; Wang et al., 2009) or multiple labels (Ramage et al., 2009; Ramage et al., 2011) to predict metadata from text. Probabilistic topic modeling requires estimating the posterior distribution. Exact computation of the posterior is often intractable, which motivates approximate inference techniques (Asuncion et al., 2009). One popular approach is Markov chain Monte Carlo (MCMC), a class of inference algorithms to approximate the target posterior distribution. To make prediction, MCMC algorithms generate samples on training data to estimate corpus-level latent variables, and use them to generate samples to estimate document-level latent variables for test"
D15-1030,W09-2206,0,0.0247196,"f our model against LDA and MRTF and then perform link prediction tasks. We demonstrate improvements in link prediction as measured by predictive link rank and provide both qualitative and quantitative perspectives on the improvements achieved by the model. 2 Sk,i = Nk X p(wk,j |wk,i ), (1) j=1,j6=i where Nk denotes the number of words in topic k. We then select the three words with the highest sum of transition probabilities as the seed words for topic k. In the sampling process (Section 3), seed words are only assigned to their corresponding topics, similar to the use of hard constraints by Andrzejewski and Zhu (2009). Discriminative Links from Topics Figure 1 is a two-document segment of our model, which has the following generative process: 1. For each related-document cluster l ∈ {1, . . . , L} Draw πl ∼ Dir(α0 ) 2. For each topic k ∈ {1, . . . , K} (a) Draw word distribution φk ∼ Dir(β) (b) Draw topic regression parameter ηk ∼ N (0, ν 2 ) 3. For each word v ∈ {1, . . . , V } Draw lexical regression parameter τv ∼ N (0, ν 2 ) 4. For each document d ∈ {1, . . . , D} (a) Draw topic proportions θd ∼ Dir(απld ) (b) For each word td,n in document d i. Draw a topic assignment zd,n ∼ Mult(θd ) ii. Draw a word"
D15-1030,P14-1018,0,0.0714405,"Missing"
D15-1030,W03-1730,0,0.0366584,"tes for test. As the training corpus is generated ranthe indexes of documents which are linked to docdomly, seeding is not applied in this section. The −d,n ument d; πld ,k is estimated based on the maximal results are given in Table 1, where I- denotes that path assumption (Wallach, 2008) the model incorporates user interactions. P −d,n 0 The results confirm that our model outperforms d0 ∈S(ld ) Nd0 ,k + α πl−d,n = , (5) P both LDA and MRTF and that its use of user inter−d,n d ,k 0 d0 ∈S(ld ) Nd0 ,· + Kα actions holds promise. where S(ld ) denotes the cluster which contains 4 We use ICTCLAS (Zhang et al., 2003) for segmentation. document d (Step 1 in the generative process). After stopword and low-frequency word removal, the vocabulary includes 12,257 words, with ∼755 tokens per document and 5,404 links. 3 More details here and throughout this section appear in the supplementary materials. 263 (a) Mentioning 82.44 73.31 (a) Mentioning 60 76.71 (c) Following (b) Retweeting 74.38 82.44 110.99 73.31 76.71 74.38 98.99 101.34 104.65 103.79 100.09 82.13 40 50 40 50 60 RTM 72.89 70 IS-RTM 60 82.44 80 60 70 Lex-IS-RTM 80 90 100 MED-RTM 110 118.55 119.70 120.18 117.19 123.64 114.85 82.13 72.89 70 80 IS-MED-R"
D15-1030,P09-2074,0,0.179937,"Missing"
D15-1030,E14-1056,0,0.066577,"o generally increases as the models grow more sophisticated.6 4.5 In the SD/Avg row of Table 3, we also compute a ratio of standard deviations to mean values. Ratios given by the models with hinge loss are lower than those not using hinge loss. This means that the regression values given by the models with hinge loss are more concentrated around the average value, suggesting that these models can better identify linked pairs, even though the ratio of linked pairs’ average regression value to all pairs’ average value is lower. Quantitative Analysis Topic Quality. Automatic coherence detection (Lau et al., 2014) is an alternative to manual evaluations of topic quality (Chang et al., 2009). In each topic, the top n words’ average pointwise mutual information (PMI)—based on a reference corpus—serves as a measure of topic coherence.7 Topic quality improves with user interactions and max-margin learning (Table 3). PMI drops when lexical terms are added to the link probability function, however. This is consistent with the role of lexical terms in the model; their purpose is to improve link prediction performance, not improve topic quality. 5 Conclusions and Future Work We introduce a new topic model that"
D17-1203,D07-1109,1,0.782269,"human topic interpretability into the topic model optimization process in a way that is effective and more straightforward than previous methods (Newman et al., 2011). We take advantage of the human-centered evaluation of Chang et al. (2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets. tLDA topics are more coherent compared with “vanilla” LDA topics, while retaining and often slightly improving topics’ extrinsic performance as features for supervised"
D17-1203,J93-1003,0,0.353239,"(2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with these tree priors in Amazon reviews and the 20NewsGroups datasets. tLDA topics are more coherent compared with “vanilla” LDA topics, while retaining and often slightly improving topics’ extrinsic performance as features for supervised classification. Our approach can be viewed as a form of adaptation, and the flexibility of the tree prior approach—amenable to any kind of association score—suggests that there are many directions to pursue beyond the two fl"
D17-1203,P96-1024,0,0.397937,"jbg@umiacs.umd.edu CS , Abstract Models work best when they are optimized taking into account the evaluation criteria that people care about. For topic models, people often care about interpretability, which can be approximated using measures of lexical association. We integrate lexical association into topic optimization using tree priors, which provide a flexible framework that can take advantage of both first order word associations and the higher-order associations captured by word embeddings. Tree priors improve topic interpretability without hurting extrinsic performance. 1 Introduction Goodman (1996) introduces a key insight for machine learning models in natural language processing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others. Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance (Papineni et al., 2002). Chang et al. (2009) provide a similar insight for topic models (Blei"
D17-1203,P16-2062,0,0.232793,"tperform the ones built using the G 2 likelihood ratio. Among the three tree prior construction algorithms, the two-level is the best on the 20NewsGroups corpus. However, there is no such consistent pattern on Amazon reviews. 4.2 Topic Coherence Instead of manually evaluating topic quality using word intrusion (Chang et al., 2009), we use an automatic alternative to compute topic coherence (Lau et al., 2014). For every topic, we extract its top ten words and compute average pairwise PMI on a reference corpus (Wikipedia as of October 8, 2014). We include LDA and the latent concept topic model (Hu and Tsujii, 2016, LCTM) as baselines. LCTM also incorporates prior knowledge from word embeddings. It assumes that latent concepts exist in the embedding space and are Gaussian distributions over word embeddings, and a topic is a multinomial distribution over these concepts. We marginalize over concepts and obtain the probability mass of every word in every topic and compare against LDA and tLDA topics. 4 https://catalog.ldc.upenn.edu/ ldc2011t07. 1903 Topic KLD Christian 0.709 Security 0.720 Middle East 0.765 Sports 1.212 University Research 1.647 Health 1.914 Images 1.995 Hardware 2.127 People 2.512 Model L"
D17-1203,P14-1110,1,0.887693,"a topic. The generative process of tLDA is: 1. For topics k ∈ {1, . . . , K} and internal nodes ni (a) Draw child distribution1 πk,i ∼ Dir(β) 2. For each document d ∈ {1, . . . , D} (a) Draw topic distribution θd ∼ Dir(α) (b) For each token td,n in document d i. Draw topic assignment zd,n ∼ Mult(θd ) ii. Q Draw path yd,n to word wd,n with probability (i,j)∈yd,n πzd,n ,i,j tLDA can perform different tasks using different tree priors. If we encode synonyms in the tree prior, tLDA disambiguates word senses (BoydGraber et al., 2007). With word translation priors, it is a multilingual topic model (Hu et al., 2014). 1 Unlike other tree-based topic models such as Andrzejewski et al. (2009), all Dirichlet hyperparameters are the same for all internal nodes. Regardless of cardinality, all Dirichlet parameters are the same scalar β. match matches tournament Figure 2: A two-level tree example with N = 2. The words in the internal nodes denote concepts and have no effect in tLDA. 3 Tree Prior Construction from Word Association Scores A two-level tree is the most straightforward construction.2 Each internal node, ni , is a concept associated with a word vi in the vocabulary. Then we sort all other words in des"
D17-1203,E14-1056,0,0.453882,"UMIACS University of Maryland College Park, MD resnik@umd.edu We take the logical next step suggested when you bring together the insights of Goodman (1996) and Chang et al. (2009), namely incorporating an approximation of human topic interpretability into the topic model optimization process in a way that is effective and more straightforward than previous methods (Newman et al., 2011). We take advantage of the human-centered evaluation of Chang et al. (2009), which can be reasonably approximated using an automatic metric based on word associations derived from a large, more general corpus (Lau et al., 2014). We exploit LDA and its Bayesian formulation by bringing word associations into the picture using a prior—specifically, we use external lexical association to create a tree structure and then use tree LDA (Boyd-Graber et al., 2007, tLDA), which derives topics using a given tree prior. We construct tree priors with combinations of two types of word association scores (skip-gram probability (Mikolov et al., 2013) and G 2 likelihood ratio (Dunning, 1993)) and three construction algorithms (two-level, hierarchical clustering with and without leaf duplication). Then tLDA identifies topics with the"
D17-1203,P02-1040,0,0.0988489,"topic interpretability without hurting extrinsic performance. 1 Introduction Goodman (1996) introduces a key insight for machine learning models in natural language processing: if you know how performance on a problem is evaluated, it makes more sense to optimize using that evaluation metric, rather than others. Goodman applies his insight to parsing algorithms, but this insight has had an even larger impact in machine translation, where the introduction of the fully automatic BLEU metric makes it possible to tune systems using a score correlated with human rankings of MT system performance (Papineni et al., 2002). Chang et al. (2009) provide a similar insight for topic models (Blei et al., 2003, LDA): if what you care about is the interpretability of topics, the standard objective function for parameter inference (likelihood) is not only poorly correlated with a human-centered measurement of topic coherence, but inversely correlated. Nonetheless, most topic models are still trained using methods that optimize likelihood (McAuliffe and Blei, 2008; Nguyen et al., 2013). Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu We take the logical next step suggested whe"
D19-1120,E14-1056,0,0.08912,"Missing"
D19-1120,D09-1092,0,0.023202,"are similar, outperforming LDA and previous MTMs in classification tasks using documents’ topic posteriors as features. It also learns coherent topics on documents with low comparability. 1 Topic models explain document collections at a high level (Boyd-Graber et al., 2017). Multilingual topic models (MTMs) uncover latent topics across languages and reveal commonalities and differences across languages and cultures (Ni et al., 2009; Shi et al., 2016; Guti´errez et al., 2016). Existing models extend latent Dirichlet allocation (Blei et al., 2003, LDA) and learn aligned topics across languages (Mimno et al., 2009). Prior models work well because they implicitly assume—even if not part of the model—parallel or highly comparable data with well-aligned topics. However, this assumption does not always comport with reality. Even documents from the same place and time can discuss very different things across languages: in multicultural London, Hindi tweets focus on a Bollywood actor’s BBC appearance, French blogs fret about Brexit, and English articles focus on Tottenham’s lineup. Generally, corpora have a range of “nonparallelness” (Fung, 2000). In less comparable settings, while some † Now at Facebook Now"
D19-1120,P16-1064,0,0.120625,"ful in important low-resource language scenarios. Our MTM learns weighted topic links and connects cross-lingual topics only when the dominant words defining them are similar, outperforming LDA and previous MTMs in classification tasks using documents’ topic posteriors as features. It also learns coherent topics on documents with low comparability. 1 Topic models explain document collections at a high level (Boyd-Graber et al., 2017). Multilingual topic models (MTMs) uncover latent topics across languages and reveal commonalities and differences across languages and cultures (Ni et al., 2009; Shi et al., 2016; Guti´errez et al., 2016). Existing models extend latent Dirichlet allocation (Blei et al., 2003, LDA) and learn aligned topics across languages (Mimno et al., 2009). Prior models work well because they implicitly assume—even if not part of the model—parallel or highly comparable data with well-aligned topics. However, this assumption does not always comport with reality. Even documents from the same place and time can discuss very different things across languages: in multicultural London, Hindi tweets focus on a Bollywood actor’s BBC appearance, French blogs fret about Brexit, and English a"
D19-1120,L16-1521,0,0.0682902,"Missing"
D19-1120,D15-1037,1,0.927645,"translation dictionary. We validate the MTM in two classification tasks using inferred topic posteriors as features. Our 1243 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1243–1248, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics MTM has higher F1 than other models in both intra- and cross-lingual evaluations, while discovering coherent topics and meaningful topic links. 2 Multilingual Topic Model for Connecting Cross-Lingual Topics Yang et al. (2015) present a flexible framework for adding regularization to topic models. We extend this model to the multilingual setting by adding a potential function that links topics across languages. For simplicity of exposition, we focus on the bilingual case with languages S and T . Unlike Yang et al. (2015) that encode monolingual information only, our potential function encodes multilingual knowledge parameterized by two matrices, ρS→T and ρT →S , that transform topics between the two languages. Cells’ values are between 0 and 1 and a cell ρS→T,kT ,kS close to one is a strong connection of topics kT"
D19-1120,C18-1220,0,0.0365805,"Missing"
D19-1120,P14-1110,1,0.872849,"1 26.7 33.3 15.1 11.4 26.7 26.7 26.5 32.6 24.0 18.2 23.1 23.1 38.1 38.1 MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP Cross-Lingual 4.1 English 13.0 20.8 27.8 12.8 MCTA MTAnchor LDA ptLDA MTM MTM+TOP MTM+TF-IDF MTM+TF-IDF+TOP 0 25 50 75 100 Figure 3: The F1 scores on disaster response (upper) and Wikipedia (lower) datasets. Our MTM outperforms all the baselines in intra- and cross-lingual evaluations. which is the proportion of the tokens assigned to topic k in document d. The baselines include polylingual tree LDA (Hu et al., 2014, ptLDA) which encodes the dictionary as a tree prior (Andrzejewski et al., 2009), Multilingual Topic Anchoring (Yuan et al., 2018, MTAnchor), and Multilingual Cultural-common Topic Analysis (Shi et al., 2016, MCTA). We also include LDA, which runs monolingually in each language. We use 20 topics and set hyperparameters α = 0.1 and β = 0.01 (if applicable). Our evaluations are both intra- and crosslingual. The intra-lingual evaluation trains and tests classifiers on the same language, while the cross-lingual evaluation trains classifiers on one language and tests on another. In cross-lingual e"
H01-1033,J93-1003,0,0.127112,"Missing"
H01-1033,P99-1068,1,0.821638,"Missing"
H05-1098,P05-1033,1,0.428241,", and Analysis David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin Institute for Advanced Computer Studies (UMIACS) University of Maryland, College Park, MD 20742, USA {dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu Abstract Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of non"
H05-1098,W02-1039,0,0.35507,"evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a"
H05-1098,P02-1050,1,0.909542,"s. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG,"
H05-1098,P03-1040,0,0.00569351,"ase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any"
H05-1098,N03-1017,0,0.134136,"aper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based mo"
H05-1098,koen-2004-pharaoh,0,0.0221738,"e-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any commitment to a linguistically relevant analysis, and it does not require syntactically annotated training data. Chiang (2005) reported significant performance improvements in Chinese-English translation as compared with Pharaoh, a state-of-the-art phrase-based system (Koehn, 2004). In Section 2, we review the essential elements of Hiero. In Section 3 we describe extensions to this system, including new features involving named entities and numbers and support for a fourfold scale-up in training set size. Section 4 presents new evaluation results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochasti"
H05-1098,P04-1077,0,0.0151475,"on the FBIS data; Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded"
H05-1098,H05-2007,1,0.821428,"Missing"
H05-1098,W02-1018,0,0.0259841,"ew hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident w"
H05-1098,N03-2021,0,0.177666,"Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded again using an HMM wh"
H05-1098,P00-1056,0,0.0589312,"igure 1: Example synchronous CFG • the lexical weights Pw (γ |α) and Pw (α |γ) (Koehn et al., 2003);1 2.1 Grammar • a phrase penalty exp(1); A synchronous CFG or syntax-directed transduction grammar (Lewis and Stearns, 1968) consists of pairs of CFG rules with aligned nonterminal symbols. We denote this alignment by coindexation with boxed numbers (Figure 1). A derivation starts with a pair of aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (Figure 2). Training begins with phrase pairs, obtained as by Och, Koehn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the “final-and” method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not alread"
H05-1098,P02-1038,0,0.00566047,"onous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w(X → hγ, αi) = φi (X → hγ, αi)λi S → hX 1 , X 1 i X → hyu X 1 you X 2 , have X 2 with X 1 i X → hX 1 de X 2 , the X 2 that X 1 i X → hX 1 zhiyi, one of X 1 i X → hAozhou, Australiai X → hshi, isi i X → hshaoshu guojia, few countriesi where the φi are features defined on rules. The basic model uses the following features, analogous to Pharaoh’s default featu"
H05-1098,J04-4002,0,0.0105751,"luations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absenc"
H05-1098,P03-1021,0,0.00789852,"verage. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up to a much larger training set. 3.1 New features supplementary ph"
H05-1098,P02-1040,0,0.109612,"nment, Koehn et al. take the maximum lexical weight; Hiero uses a weighted average. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up"
H05-1098,W96-0213,0,0.00943678,"t work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of every possible tag sequence ti . . . t j in the reference corpus. Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs"
H05-1098,2003.mtsummit-papers.53,0,0.0144904,"ed Chinese number/date translator, and created a new model feature for it. Again, the weight given to this module was optimized during minimum-error-rate training. In some cases we wrote the rules to provide multiple uniformlyweighted English translations for a Chinese phrase (for example, kå (bari) could become “the 8th” or “on the 8th”), allowing the language model to decide between the options. 3.2 The LDC Chinese-English named entity lists (900k entries) are a potentially valuable resource, but previous experiments have suggested that simply adding them to the training data does not help (Vogel et al., 2003). Instead, we placed them in a supplementary phrase-translation table, giving greater weight to phrases that occurred less frequently in the primary training data. For each entry h f, {e1 , . . . , en }i, we counted the number of times c( f ) that f appeared in the primary training data, and assigned the entry the weight c( f1)+1 , which was then distributed evenly among the supplementary phrase pairs {h f, ei i}. We then created a new model feature for named entities. When one of these 781 Scaling up training Chiang (2005) reports on experiments in ChineseEnglish translation using a model tra"
H05-1098,P96-1021,0,0.157884,"tion results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochastic synchronous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w"
H05-1098,J97-3002,0,0.094693,"ned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not already used. Various filters are also applied to reduce the number of extracted rules. Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (Wu, 1997). 780 • a word penalty exp(l), where l is the number of terminals in α. The exceptions to the above are the two “glue” rules, which are the rules with left-hand side S in Figure 1. The second has weight one, and the first has weight w(S → hS 1 X 2 , S 1 X 2 i) = exp(−λg ), the idea being that parameter λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Phrase translation probabilities are estimated by relative-frequency estimation. Since the extraction process does not generate a unique derivation for each training sentence pair, a distribution over"
H05-1098,P02-1039,0,0.0166923,"pect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using a syntax based metric. We propose to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of ever"
H05-1098,N04-1021,0,\N,Missing
H05-1109,P97-1017,0,0.0436185,"(ajar, a car) = P (ajar|a car)P (a car). Using the channel model to address word merge/split errors without actually using a word level model is, to our knowledge, a novel contribution of our approach. 3 We implemented our post-processing system using the framework of weighted finite state machines (WFSM), which provides a strong theoretical foundation and reduces implementation time, thanks to freely available toolkits, such as the AT&T FSM Toolkit (Mohri et al., 1998). It also allows easy integration of our post-processor with numerous NLP applications that are implemented using FSMs (e.g. (Knight and Graehl, 1997; Kumar and Byrne, 2003)). 3.1 P (O, b, a, C) = P (O, b|a, C)P (a|C)P (C) Although word-based models generally perform better, moving from words to characters is a necessary compromise because word-based models are useless in the absence of a lexicon, which is the case for many low-density languages. In addition to eliminating the need for a lexicon, we developed a novel method for handling word merge/split errors.2 Rather than modeling these er2 A merge error occurs when two or more adjacent items are recognized as one, and a split error occurs when an item is recognized as two or more items."
H05-1109,N03-1018,1,0.922216,"anguages — that is, languages where few on-line resources exist. In addition, for many languages of interest input data are available mostly in printed form, and must be converted to electronic form prior to processing. Optical character recognition (OCR) is often the only feasible method to perform this conversion, owing to its speed and cost-effectiveness. Unfortunately, the performance of OCR systems is far from perfect and recognition errors significantly degrade the performance of NLP applications. This is true both in resource acquisition, such as automated bilingual lexicon generation (Kolak et al., 2003), and for end-user applications such as rapid machine translation (MT) in the battlefield for document filtering (Voss and Ess-Dykema, 2000). Moreover, for low density languages, there simply may not be an OCR system available. In this paper, we demonstrate that via statistical post-processing of existing systems, it is possible to achieve reasonable recognition accuracy for low density languages altogether lacking an OCR system, to significantly improve on the performance of a trainable commercial OCR system, and even to improve significantly on a native commercial OCR system.1 By taking a po"
H05-1109,P00-1037,0,0.0235994,"signs a probability to O given that it was generated from C. We experimented with two probabilistic string edit distance models for implementing the channel model. The first, following our earlier model (2003), permits single-character substitutions, insertions, and deletions, with associated probabilities. For example, P (ajar|a car) ≈ P (a7→a)P ( 7→)P (c7→j)P (a7→a)P (r7→r). Note that we are only considering the most likely edit sequence here, as opposed to summing over all possible ways to convert a car to ajar. The second is a slightly modified version of the spelling correction model of Brill and Moore (2000).3 This model allows many-to-many edit operations, which makes P (liter|litre) ≈ P (l7→l)P (i7→i)P (tre7→ter) possible. We will refer to the these as the singlecharacter (SC) and multi-character (MC) error models, respectively. We train both error models over a set of corresponding ground truth and OCR sequences, hC, Oi. Training is performed using expectationmaximization: We first find the most likely edit sequence for each training pair to update the edit counts, and then use the updated counts to reestimate edit probabilities. For MC, after finding the most likely edit sequence, extended ve"
H05-1109,P05-1033,0,0.0114742,"using the CMU-Cambridge toolkit and the translation model using the GIZA++ toolkit (Och and Ney, 2000). We used the ReWrite decoder (Germann, 2003) for translation. BLEU scores for OCR, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively. This establishes that OCR errors degrade the performance of the MT system, and we are able to bring the performance much closer to the level of performance on clean text by using post-processing. Clearly the BLEU scores are quite low; we are planning to perform experiments on Arabic using a more advanced translation system, such as Hiero (Chiang, 2005). MT System Systran Systran Systran ReWrite ReWrite ReWrite Input Text OCR Corrected Clean OCR Corrected Clean BLEU Score 0.2000 0.2606 0.3188 0.1792 0.2234 0.2590 Table 6: Spanish-English translation results In order to test in a scenario with better translation performance, we performed MT evaluations using Spanish. We used a commercial translation system, Systran, in addition to statistical translation. More resources being available for this language, corrected text for Spanish experiments was obtained using our original model that takes advantage of a lexicon (2003). Table 6 shows that sc"
H05-1109,N03-1010,0,0.0128915,".2, iterative correction is a way to address this problem. 4.4 Extrinsic Evaluation: MT While our post-processing methods reduce WER, our main interest is their impact on NLP applications. We have performed machine translation experiments to measure the effects of OCR errors and the post-processing approach on NLP application performance. For Arabic, we trained a statistical MT system using the first nine sections of the Bible data. The language model is trained using the CMU-Cambridge toolkit and the translation model using the GIZA++ toolkit (Och and Ney, 2000). We used the ReWrite decoder (Germann, 2003) for translation. BLEU scores for OCR, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively. This establishes that OCR errors degrade the performance of the MT system, and we are able to bring the performance much closer to the level of performance on clean text by using post-processing. Clearly the BLEU scores are quite low; we are planning to perform experiments on Arabic using a more advanced translation system, such as Hiero (Chiang, 2005). MT System Systran Systran Systran ReWrite ReWrite ReWrite Input Text OCR Corrected Clean OCR Corrected Clean BLEU Score 0.2000 0.260"
H05-1109,N03-1019,0,0.0119741,"car)P (a car). Using the channel model to address word merge/split errors without actually using a word level model is, to our knowledge, a novel contribution of our approach. 3 We implemented our post-processing system using the framework of weighted finite state machines (WFSM), which provides a strong theoretical foundation and reduces implementation time, thanks to freely available toolkits, such as the AT&T FSM Toolkit (Mohri et al., 1998). It also allows easy integration of our post-processor with numerous NLP applications that are implemented using FSMs (e.g. (Knight and Graehl, 1997; Kumar and Byrne, 2003)). 3.1 P (O, b, a, C) = P (O, b|a, C)P (a|C)P (C) Although word-based models generally perform better, moving from words to characters is a necessary compromise because word-based models are useless in the absence of a lexicon, which is the case for many low-density languages. In addition to eliminating the need for a lexicon, we developed a novel method for handling word merge/split errors.2 Rather than modeling these er2 A merge error occurs when two or more adjacent items are recognized as one, and a split error occurs when an item is recognized as two or more items. These errors can happen"
H05-1109,P00-1056,0,0.00868634,"ble to use high limits. As mentioned in Section 3.2, iterative correction is a way to address this problem. 4.4 Extrinsic Evaluation: MT While our post-processing methods reduce WER, our main interest is their impact on NLP applications. We have performed machine translation experiments to measure the effects of OCR errors and the post-processing approach on NLP application performance. For Arabic, we trained a statistical MT system using the first nine sections of the Bible data. The language model is trained using the CMU-Cambridge toolkit and the translation model using the GIZA++ toolkit (Och and Ney, 2000). We used the ReWrite decoder (Germann, 2003) for translation. BLEU scores for OCR, corrected, and clean text were 0.0116, 0.0141, and 0.0154, respectively. This establishes that OCR errors degrade the performance of the MT system, and we are able to bring the performance much closer to the level of performance on clean text by using post-processing. Clearly the BLEU scores are quite low; we are planning to perform experiments on Arabic using a more advanced translation system, such as Hiero (Chiang, 2005). MT System Systran Systran Systran ReWrite ReWrite ReWrite Input Text OCR Corrected Clea"
H05-1109,P02-1040,0,0.0796413,"bo, Cebuano, Arabic, and Spanish. For intrinsic evaluation, we use the conventional Word Error Rate (WER) metric, which is defined as W ER(C, O) = W ordEditDistance(C, O) W ordCount(C) 4 Ignoring errors that result in valid words, lexicon-based chunking is always error-free. 5 Inversion reverses the direction of the error model, mapping observed sequences to possible ground truth sequences. 870 We do not use the Character Error Rate (CER) metric, since for almost all NLP applications the unit of information is the words. For extrinsic evaluation of machine translation, we use the BLEU metric (Papineni et al., 2002). 4.1 Igbo: Creating an OCR System Igbo is an African language spoken mainly in Nigeria by an estimated 10 to 18 million people, written in Latin script. Although some Igbo texts use diacritics to mark tones, they are not part of the official orthography and they are absent in most printed materials. Other than grammar books, texts for Igbo, even hardcopy, are extremely difficult to obtain. To our knowledge, the work reported here creates the first OCR system for this language. For the Igbo experiments, we used two sources. The first is a small excerpt containing 6727 words from the novel “Juo"
H05-1109,W00-0501,0,0.0538689,"Missing"
H05-2007,H05-1098,1,0.780918,"up the associated tag sequence ti . . .t j and increment a counter recalled(ti . . .t j ) Using this method, we compute the recall of tag patterns, R(ti . . .t j ) = recalled(ti . . .t j )/freq(ti . . .t j ), for all patterns in the corpus. To compare two systems (which could include two versions of the same system), we identify POS n-grams that are recalled significantly more frequently by one system than the other, using a difference-of-proportions test to assess statistical significance. We have used this method to analyze the output of two different statistical machine translation models (Chiang et al., 2005). 3 Visualization Our demonstration system uses an HTML interface to summarize the observed pattern recall. Based on frequent or significantly-different recall, the user can select and visually inspect color-coded examples of each pattern of interest in context with both source and reference sentences. An example visualization is shown in Figure 1. 13 4 Acknowledgements The authors would like to thank David Chiang, Christof Monz, and Michael Subotin for helpful commentary on this work. This research was supported in part by ONR MURI Contract FCPO.810548265 and Department of Defense contract RD"
H05-2007,W02-1039,0,0.0409313,"need fine-grained error analysis. What we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with. Word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them. 2 Part-of-Speech Sequence Recall In developing a new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using a syntaxbased, recall-oriented metric. As an initial step, we would like to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text to the noisy output of MT systems produces unuseable results (e.g. (Och et al"
H05-2007,P02-1050,1,0.85571,"rained error analysis. What we would really like to know is how well the system is able to capture systematic reordering patterns in the input, which ones it is successful with, and which ones it has difficulty with. Word n-grams are little help here: they are too many, too sparse, and it is difficult to discern general patterns from them. 2 Part-of-Speech Sequence Recall In developing a new analysis method, we are motivated in part by recent studies suggesting that word reorderings follow general patterns with respect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems (or two versions of the same system), it may be useful to look for syntactic patterns that one system (or version) captures well in the target language and the other does not, using a syntaxbased, recall-oriented metric. As an initial step, we would like to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has confirmed the intuition that applying statistical analyzers trained on well-formed text to the noisy output of MT systems produces unuseable results (e.g. (Och et al., 2004)). Therefor"
H05-2007,J82-2005,0,0.491619,"Missing"
H05-2007,P02-1040,0,0.0729166,"42 alopez@cs.umd.edu resnik@umd.edu Abstract We describe a method for identifying systematic patterns in translation data using part-ofspeech tag sequences. We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems, and demonstrate how our application can be used by developers to explore patterns in machine translation output. 1 Introduction Over the last few years, several automatic metrics for machine translation (MT) evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Papineni et al., 2002; Melamed et al., 2003). All are predicated on the concept of n-gram matching between the sentence hypothesized by the translation system and one or more reference translations—that is, human translations for the test sentence. Although the formulae underlying these metrics vary, each produces a single number representing the “goodness” of the MT system output over a set of reference documents. We can compare the numbers of competing systems to get a coarse estimate of their relative performance. However, this comparison is holistic. It provides no insight into the specific competencies or wea"
H05-2007,W96-0213,0,0.0597204,"s unuseable results (e.g. (Och et al., 2004)). Therefore, we make the conservative choice to apply annotation only to the reference corpus. Word n-gram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. The method: 1. Part-of-speech tag the reference corpus. We used 12 Proceedings of HLT/EMNLP 2005 Demonstration Abstracts, pages 12–13, Vancouver, October 2005. Figure 1: Comparing two systems that differ significantly in their recall for POS n-gram JJ NN IN DT NN. The interface uses color to make examples easy to find. MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. 2. Compute the frequency freq(ti . . .t j ) of every possible tag sequence ti . . .t j in the reference corpus. 3. Compute the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs are order-sensitiv"
H05-2007,N03-2021,0,\N,Missing
H05-2007,N04-1021,0,\N,Missing
H93-1054,J92-4003,0,0.0496841,"Missing"
H93-1054,J82-3004,0,0.305398,"h i a , PA 19104 resnik @linc.cis.upenn.edu 2. C L A S S - B A S E D ABSTRACT In this paper we propose to define selectional preference and semantic similarity as information-theoretic relationships involving conceptual classes, and we demonstrate the applicability of these definitions to the resolution of syntactic ambiguity. The space of classes is defined using WordNet [8], and conceptual relationships are determined by means of statistical analysis using parsed text in the Penn Treebank. 1. I N T R O D U C T I O N The problem of syntactic ambiguity is a pervasive one. As Church and Patil [2] point out, the class of""every way ambiguous"" constructions - - those for which the number of analyses is the number of binary trees over the terminal elements includes such frequent constructions as prepositional phrases, coordination, and nominal compounds. They suggest that until it has more useful constraints for resolving ambiguities, a parser can do little better than to efficiently record all the possible attachments and move on. In general, it may be that such constraints can only be supplied by analysis of the context, domain-dependent knowledge, or other complex inferential processes"
H93-1054,P90-1034,0,0.0204983,"Missing"
H93-1054,P91-1030,0,0.150153,"everal business and university groups b. several businesses and university groups Semantic similarity of the conjoined heads also appears to play an important role: (5) If noun 1 and noun2 match in number and noun 1 and noun3 do not then conjoin nounl and noun2; - if nounl and noun3 match in number and noun 1 and noun2 do not then conjoin nounl and noun3; - otherwise remain undecided. • Meaning: a. a television and radio personality b. a psychologist and sex researcher In addition, for this particular construction, the appropriateness of noun-noun modification for noun1 and noun3 is relevant: (6) - a. mail and securities fraud b. corn and peanut butter - If sim(nounl,noun2) > sim(nounl,noun3) then conjoin nounl and noun2; - ifsim(nounl,noun3) > sim(nounl,noun2) then conjoin nounl and noun3; - otherwise remain undecided. • Modification: - If A(nounl,noun3) > r, a threshold, or if A(noun3,nounl) > r, then conjoin nounl and noun3; We investigated the roles of these cues by conducting a disambiguation experiment using the definitions in the previous section. Two sets of 100 noun phrases of the form [NP noun1 and noun2 noun3] were extracted from the Wall Street Journal (WSJ) corpus in the"
H93-1054,C92-1029,0,0.0364791,"cation of classbased statistical methods to a particular subset of coordinations, noun phrase conjunctions of the form nounl and noun2 noun3, as in (3): (3) a. a (bank and warehouse) guard b. a (policeman) and (park guard) Such structures admit two analyses, one in which nounl and noun2 are the two heads being conjoined (3a) and one in which the conjoined heads are noun1 and noun3 (3b). Each of the three sources of information - - form similarity, meaning similarity, and modification relationships-- was used alone as a disambiguation strategy, as follows: As pointed out by Kurohashi and Nagao [7], similarity of form and similarity of meaning are important cues to conjoinability. In English, similarity of form is to a great extent captured by agreement in number: (4) • Form: a. several business and university groups b. several businesses and university groups Semantic similarity of the conjoined heads also appears to play an important role: (5) If noun 1 and noun2 match in number and noun 1 and noun3 do not then conjoin nounl and noun2; - if nounl and noun3 match in number and noun 1 and noun2 do not then conjoin nounl and noun3; - otherwise remain undecided. • Meaning: a. a television"
H93-1054,P90-1004,0,0.193838,"Missing"
H93-1054,H93-1052,0,0.021306,"ion. The semantic similarity of nl and n2 is sim(nl,n2) The ""goodness of fit"" between a word and a particular class of arguments is captured by the following definition: Definition. The selectional association of w with c is the contribution c makes to the selectional preference of w. A ] 0.088 0.075 0.030 -0.001 The above table illustrates how this definition captures the qualitative differences in example (2). The ""best"" class for an argument is the class that maximizes selectional association. Notice that finding that class represents a form of sense disambiguation using local context (cf. [15]): of all the classes to which the noun wine belongs - - including (alcohol), (substance), (red), and (color), among others - - the class (beverage) is the sense of wine most appropriate as a direct object for drink. wine. gasoline. pencils. sadness. As an alternative, we have proposed the following formalization of selectional preference [11]: (beverage) (substance) (object) {psychological_feature) = Zai[-logPr(ci)], (5) i where {el} is the set of classes dominating both nl and n2. The ai, which sum to 1, are used to weight the contribution of each class - - for example, in accordance with wo"
H93-1054,J93-2006,0,\N,Missing
H93-1054,J93-1005,0,\N,Missing
I08-3008,W06-2912,0,0.0352328,"Missing"
I08-3008,P06-1109,0,0.0319591,"Missing"
I08-3008,nivre-etal-2006-talbanken05,0,0.0221452,"Missing"
I08-3008,P05-1022,0,0.021496,"Missing"
I08-3008,P00-1056,0,0.0310736,"t the paragraphs into sentences and pruned sentences with suspicious length, contents (sequence of dashes, for instance) or both. We ended up with 430,808 Swedish sentences and 6,154,663 tokens. Since the Acquis texts are available in 21 languages, we can also exploit the Danish Acquis and its alignment with the Swedish one. We use it to study the similarity of the two languages, and for the “gloss” experiment in Section 5.1. Paragraphlevel alignment is provided as part of Acquis and contains 283,509 aligned segments. Word-level alignment, needed for our experiment, was obtained using GIZA++ (Och and Ney 2000). The treebanks are manually tagged with parts of speech and morphological information. For some of our experiments, we needed to automatically retag the target (Swedish) treebank, and to tag the Swedish Acquis. For that purpose we used the Swedish tagger of Jan Hajič, a variant of Hajič’s Czech tagger (Hajič 2004) retrained on Swedish data. 3 Treebank Normalization The two treebanks were developed by different teams, using different annotation styles and guidelines. They would be systematically different even if their texts were in the same language, but it is 5 Legislative texts are a specia"
I08-3008,P97-1003,0,0.022247,"Missing"
I08-3008,P99-1065,0,0.0099563,"reebanks, while the Charniak-Johnson reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of labels, we found it easier to map head morpho-tags directly to Penn-Treebank-style non-terminals. Hence the parser can think it’s processing Penn Treebank data. The morphological annotation of the treebanks is further discussed in Section 4. We also experimented with a large body of unannotated Swedish texts. Such data could theoretically be acquired by crawling the Web; here, however, we used the freely avail"
I08-3008,P04-1061,0,0.0224556,"Missing"
I08-3008,N03-1017,0,0.0996805,"276 not exceeding 40 words. The Swedish treebank Talbanken05 (Nivre et al. 2006) contains 11,042 sentences (191,467 tokens). It was converted at Växjö from the much older Talbanken76 treebank, created at the Lund University. Again, the texts belong to mixed domains. We split the data to 10,700 training and 342 test sentences, out of which 317 do not exceed 40 words. Both treebanks are dependency treebanks, while the Charniak-Johnson reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of"
I08-3008,J93-2004,0,0.0429532,"Missing"
I08-3008,P06-1043,0,0.0234507,"t data. Both treebanks have also been parsed after delexicalization into various tag sets: Danish gold standard converted to the hybrid sv/da tag set, Swedish Mamba gold standard, and Swedish automatically tagged with hybrid tags. The reranker did not prove useful for lexicalized Swedish, although it helped with Danish. (We cur7 F = 2×P×R / (P+R) 39 P 44.59 42.94 61.85 60.22 63.47 64.74 R 42.04 40.80 65.03 62.85 67.67 68.15 F 43.28 41.84 63.40 61.50 65.50 66.40 Table 2. Cross-language parsing accuracy. 7 Self-Training Finally, we explored the self-training based domain-adaptation technique of McClosky et al. (2006) in this setting. McClosky et al. trained the Brown parser on one domain of English (WSJ), parsed a large corpus of a second domain (NANTC), trained a new Charniak (non-reranking) parser on WSJ plus the parsed NANTC, and tested the new parser on data from a third domain (Brown Corpus). They observed improvement over baseline in spite of the fact that the large corpus was not in the third domain. Our setting is similar. We train the Brown parser on Danish treebank and apply it to Swedish Acquis. Then we train new Charniak parser on Danish treebank and the parsed Swedish Acquis, and test the par"
I08-3008,J03-3002,1,0.354664,"s. “object” form (the distinction between English he and him). DDT calls the same paradigm “nominative” vs. “unmarked” case. We explore two techniques of making unknown words known. We call them glosses and delexicalization, respectively. • Most noun phrases in both languages distinguish just the common and neuter genders. However, some pronouns could be classified as masculine or feminine. Swedish tags use the masculine gender, Danish do not. • DDT does not use special part of speech for numbers — they are tagged as adjectives. This approach needs a Danish-Swedish (da-sv) bitext. As shown by Resnik and Smith (2003), parallel texts can be acquired from the Web, which makes this type of resource more easily available than a treebank. We benefited from the Acquis dasv alignments. Similarly to phrase-based translation systems, we used GIZA++ (Och and Ney 2000) to obtain one-to-many word alignments in both directions, then combined them into a single set of refined alignments using the “final-and” method of Koehn et al. (2003). The refined alignments provided us with two-way tables of a source word and all its possible translations, with weights. Using these tables, we glossed each Swedish word by its Danish"
I08-3008,E03-1008,0,0.0130025,"Missing"
I08-3008,H01-1014,0,0.0139302,"reranking parser works with phrase structures. For our experiments, we con3 There are other approaches to domain adaptation as well. For instance, Steedman et al. (2003) address domain adaptation using a weakly supervised method called co-training. Two parsers, each applying a different strategy, mutually prepare new training examples for each other. We have not tested co-training for crosslanguage adaptation. 4 We used the CoNLL 2006 versions of these treebanks. 36 verted the treebanks from dependencies to phrases, using the “flattest-possible” algorithm (Collins et al. 1999; algorithm 2 of Xia and Palmer 2001). The morphological annotation of the treebanks helped us to label the non-terminals. Although the Charniak’s parser can be taught a new inventory of labels, we found it easier to map head morpho-tags directly to Penn-Treebank-style non-terminals. Hence the parser can think it’s processing Penn Treebank data. The morphological annotation of the treebanks is further discussed in Section 4. We also experimented with a large body of unannotated Swedish texts. Such data could theoretically be acquired by crawling the Web; here, however, we used the freely available JRC-Acquis corpus of EU legislat"
I08-3008,steinberger-etal-2006-jrc,0,\N,Missing
J01-4009,Y01-1001,0,0.0230579,"Missing"
J03-3002,J90-2002,0,0.654334,"the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the s"
J03-3002,J93-1001,0,0.0541204,"Johns Hopkins University, Baltimore, MD 21218. E-mail: nasmith@cs.jhu.edu c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 3 Melamed 1997; Menezes and Richardson 2001), and the like. Even for the top handful of majority languages, the available parallel corpora tend to be unbalanced, representing primarily governmental or newswire-style texts. In addition, like other language resources, parallel corpora are often encumbered by fees or licensing restrictions. For all these reasons, it is difficult to follow the “more data are better data” advice of Church and Mercer (1993), abandoning balance in favor of volume, with respect to parallel text. Then there is the World Wide Web. People tend to see the Web as a reflection of their own way of viewing the world—as a huge semantic network, or an enormous historical archive, or a grand social experiment. We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads. This article describes our techniques for mining the Web in order to extract the parallel text it cont"
J03-3002,W02-0506,0,0.0174661,"Missing"
J03-3002,P02-1033,1,0.23898,"1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level alignments as the bridge, and then use robust statistical techniques in learning from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and Resnik 2002; Hwa et al. 2002; Lopez et al. 2002; Yarowsky, Ngai, and Wicentowski 2001; Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002). For these reasons, parallel corpora can be thought of as a critical resource. Unfortunately, they are not readily available in the necessary quantities. Until very recently, for example, statistical work in machine translation focused heavily on French-English translation because the Canadian parliamentary proceedings (Hansards) in English and French were the only large bitext available. Things have improved somewhat, but it is still fair to say that for all"
J03-3002,H91-1026,0,0.0603442,"performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel co"
J03-3002,P02-1050,1,0.32216,"Missing"
J03-3002,1999.mtsummit-1.79,0,0.806817,"rther discussion in Section 4.3), and a modest amount of monolingual data for training n-gram-based language identifiers (typically 50,000 to 100,000 characters of text per language). Word-level translations are worth exploiting when they are available. In Section 3 we describe a bitext-matching process using a content-based similarity score grounded in information theory, and in Section 5 we show how structural and content-based criteria can be combined in order to obtain performance superior to that obtained using either method alone. 5 Many details of this technique are left unspecified in Ma and Liberman (1999), such as the threshold for the similarity score, the distance threshold, and matching of non-one-word to one-word entries in the dictionary. 359 Computational Linguistics X: Maria does NULL Y: Maria n’ n’t Volume 29, Number 3 like NULL aime fruit NULL pas de fruits Figure 4 An example of two texts with links shown. There are seven link tokens, five of which are lexical (non-NULL) in X (the English side), six in Y (French). 3. Content-Based Matching The approach discussed thus far relies heavily on document structure. However, as Ma and Liberman (1999) point out, not all translators create tra"
J03-3002,W97-0311,0,0.0125252,"tentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English"
J03-3002,J00-2004,0,0.516677,"e for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language"
J03-3002,W01-1406,0,0.00995262,"Missing"
J03-3002,P02-1038,0,0.0881138,"arallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level"
J03-3002,resnik-1998-parallel,1,0.538868,"the Web as a reflection of their own way of viewing the world—as a huge semantic network, or an enormous historical archive, or a grand social experiment. We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads. This article describes our techniques for mining the Web in order to extract the parallel text it contains. It presents, in revised and considerably extended form, our early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incorporating new work on content-based detection of translations (Smith 2001, 2002), and efficient exploitation of the Internet Archive. In Section 2 we lay out the STRAND architecture, which is based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identified even without looking at content; we also show how we have improved STRAND’s performance by training a supervised classifier using structural parameters rather than relying on manually tuned thresholds. In Section 3 we present an approach to detecting translations"
J03-3002,P99-1068,1,0.418713,"Missing"
J03-3002,A97-1050,1,0.37856,"Missing"
J03-3002,H01-1033,1,0.330058,"Missing"
J03-3002,C02-1070,0,0.0162036,"Missing"
J03-3002,W02-1013,1,0.593288,"sim to tsim = number of two-word links in best matching number of links in best matching (4) The key reason to compute tsim under the equiprobability assumption is that we need not compute the MWBM, but may find just the maximum cardinality bipartite √ matching (MCBM), since all potential links have the same weight. An O(e v) (or  O(|X |· |Y |· |X |+ |Y|) for this purpose) algorithm exists for MCBM (Ahuja, Magnati, and Orlin 1993). For example, if the matching shown in Figure 4 is the MCBM (for some translation lexicon), then tsim(X, Y) = 47 under the simplifying assumption. In earlier work (Smith 2002), we sought to show how multiple linguistic resources could be exploited in combination to recognize translation, and how the equiprobability assumption allowed straightforward combination of resources (i.e., set union of translation lexicon entries). In Section 3.2.1 we provide a clean solution to the problem of using unweighted translation lexicons along with probabilistic ones that improves performance over the earlier result. This would appear to make the equiprobability assumption unnecessary (apart from concerns about computational expense). However, we found that, if p(x, y) is set to t"
J03-3002,W99-0626,0,0.0331893,"Missing"
J03-3002,P95-1026,0,0.20594,"gm for the construction of parallel corpora. Beginning with a seed set of translation information (either parallel corpora or a bilingual dictionary), high-precision initial classifiers might be constructed using content and/or structural features (whichever are available). We might then iteratively select additional page pairs in which the current classifier has high confidence of translational equivalence, gradually increasing the pool of parallel data and at the same time expanding the bilingual lexicon. This approach to minimally supervised classifier construction has been widely studied (Yarowsky 1995), especially in cases in which the features of interest are orthogonal in some sense (e.g., Blum and Mitchell 1998; Abney 2002). With respect to the generation of candidate pairs, we have described a progression from index-based searches on AltaVista to exhaustive matching of URLs on the Internet Archive. The combination of these approaches may be profitable, particularly for languages that are represented only very sparsely on the Web. For such languages, index-based searches on words from a language of interest might be used to identify sites potentially containing parallel text. Within such"
J03-3002,N01-1026,0,0.171684,"Missing"
J03-3002,H01-1035,0,0.0255538,"Missing"
J03-3002,2001.mtsummit-ebmt.4,0,\N,Missing
J03-3002,P02-1046,0,\N,Missing
N03-1018,P00-1037,0,0.122787,"Missing"
N03-1018,J90-2002,0,0.195743,"on Generation We used the problem of unsupervised creation of translation lexicons from automatically generated word alignment of parallel text as a representative NLP task to evaluate the impact of OCR correction on usability of OCR text. We assume that the English side of the parallel text is online and its foreign language translation is generated using an OCR system.8 Our goal is to apply our OCR error correcting procedures prior to alignment so the resulting translation lexicon has the same quality as if it had been derived from error-free text. We trained an IBM style translation model (Brown et al., 1990) using GIZA++ (Och and Ney, 2000) on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible. Word level alignments generated by GIZA++ were used to extract crosslanguage word co-occurrence frequencies, and candidate 8 Alternatively, the English side can be obtained via OCR and corrected. translation lexicon entries were scored according to the log likelihood ratio (Dunning, 1993) (cf. (Resnik and Melamed, 1997)). We generated three such lexicons by pairing the English with the French ground truth, uncorrected OCR output, and its corrected versio"
N03-1018,J93-1003,0,0.0369724,"r to alignment so the resulting translation lexicon has the same quality as if it had been derived from error-free text. We trained an IBM style translation model (Brown et al., 1990) using GIZA++ (Och and Ney, 2000) on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible. Word level alignments generated by GIZA++ were used to extract crosslanguage word co-occurrence frequencies, and candidate 8 Alternatively, the English side can be obtained via OCR and corrected. translation lexicon entries were scored according to the log likelihood ratio (Dunning, 1993) (cf. (Resnik and Melamed, 1997)). We generated three such lexicons by pairing the English with the French ground truth, uncorrected OCR output, and its corrected version. All text was tokenized, lowercased, and single character tokens and tokens with no letters were removed. This method of generating a translation lexicon works well; as Table 3 illustrates with the top twenty entries from the lexicon generated using ground truth French. and of god we christ not but lord the is et de dieu nous christ pas mais seigneur la est for if ye you the law jesus as that in car si vous vous le loi j´esus"
N03-1018,P00-1056,0,0.0232044,"of unsupervised creation of translation lexicons from automatically generated word alignment of parallel text as a representative NLP task to evaluate the impact of OCR correction on usability of OCR text. We assume that the English side of the parallel text is online and its foreign language translation is generated using an OCR system.8 Our goal is to apply our OCR error correcting procedures prior to alignment so the resulting translation lexicon has the same quality as if it had been derived from error-free text. We trained an IBM style translation model (Brown et al., 1990) using GIZA++ (Och and Ney, 2000) on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible. Word level alignments generated by GIZA++ were used to extract crosslanguage word co-occurrence frequencies, and candidate 8 Alternatively, the English side can be obtained via OCR and corrected. translation lexicon entries were scored according to the log likelihood ratio (Dunning, 1993) (cf. (Resnik and Melamed, 1997)). We generated three such lexicons by pairing the English with the French ground truth, uncorrected OCR output, and its corrected version. All text was tokenized, lowerc"
N03-1018,A97-1050,1,0.785923,"resulting translation lexicon has the same quality as if it had been derived from error-free text. We trained an IBM style translation model (Brown et al., 1990) using GIZA++ (Och and Ney, 2000) on the 500 test lines used in our experiments paired with corresponding English lines from an online Bible. Word level alignments generated by GIZA++ were used to extract crosslanguage word co-occurrence frequencies, and candidate 8 Alternatively, the English side can be obtained via OCR and corrected. translation lexicon entries were scored according to the log likelihood ratio (Dunning, 1993) (cf. (Resnik and Melamed, 1997)). We generated three such lexicons by pairing the English with the French ground truth, uncorrected OCR output, and its corrected version. All text was tokenized, lowercased, and single character tokens and tokens with no letters were removed. This method of generating a translation lexicon works well; as Table 3 illustrates with the top twenty entries from the lexicon generated using ground truth French. and of god we christ not but lord the is et de dieu nous christ pas mais seigneur la est for if ye you the law jesus as that in car si vous vous le loi j´esus comme qui dans Table 3: Transla"
N03-1018,H01-1033,1,0.886631,"Missing"
N03-2026,H01-1033,1,\N,Missing
N09-1057,P98-1046,0,0.0411672,"ternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2 Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). 504 participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic behavior across a wide"
N09-1057,D07-1069,0,0.0513483,"Missing"
N09-1057,C04-1121,0,0.0511801,"Missing"
N09-1057,W06-2915,0,0.533504,"same time, we should never ignore the risks of allowing the inmate to kill someone again. 5 Computational Application Having discussed linguistic motivation, empirical validation, and practical approximation of semantically relevant features, we now present two studies demonstrating their value in sentiment classification. For the first study, we have constructed a new data set particularly well suited for testing our approach, based on writing about the death penalty. In our second study, we make a direct comparison with prior state-of-the-art classification using the Bitter Lemons corpus of Lin et al. (2006). 5.1 Corpus. We constructed a new corpus for experimentation on implicit sentiment by downloading the contents of pro- and anti-death-penalty Web sites and manually checking, for a large subset, that the viewpoints expressed in documents were as expected. The collection, which we will refer to as the DP corpus, comprises documents from five pro-death-penalty sites and three anti-death-penalty sites, and the corpus was engineered to have an even balance, 596 documents per side.12 12 We parsed English text using the Stanford parser. 507 Predicting Opinions of the Death Penalty Details in Greene"
N09-1057,J91-4003,0,0.0567887,"climbed). These facts about participation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2 Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). 504 participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semanti"
N09-1057,W06-1639,0,0.0337323,"and Malouf (2008). Among prior authors, Gamon’s (2004) research is perhaps closest to the work described here, in that he uses some features based on a sentence’s logical 510 form, generated using a proprietary system. However, his features are templatic in nature in that they do not couple specific lexical entries with their logical form. Hearst (1992) and Mulder et al. (2004) describe systems that make use of argument structure features coupled with lexical information, though neither provides implementation details or experimental results. In terms of computational experimentation, work by Thomas et al. (2006), predicting yes and no votes in corpus of United States Congressional floor debate speeches, is quite relevant. They combined SVM classification with a min-cut model on graphs in order to exploit both direct textual evidence and constraints suggested by the structure of Congressional debates, e.g. the fact that the same individual rarely gives one speech in favor of a bill and another opposing it. We have extend their method to use OPUS features in the SVM and obtained significant improvements over their classification accuracy (Greene, 2007; Greene and Resnik, in preparation). 7 Conclusions"
N09-1057,P94-1019,0,0.0071733,"rticipation in the alternation turn out to be connected with the fact that a breaking event entails a change of state in Y but a climbing event does not. Grammatically relevant semantic properties of events and their 2 Supporters of an endangered species listing in Puget Sound generally referred to the animals as orcas, while opponents generally said killer whales (Harden, 2006). 504 participants — causation, change of state, and others — are central not only in theoretical work on lexical semantics, but in computational approaches to the lexicon, as well (e.g. (Pustejovsky, 1991; Dorr, 1993; Wu and Palmer, 1994; Dang et al., 1998)). The approach we propose draws on two influential discussions about grammatically relevant semantic properties in theoretical work on lexical semantics. First, Dowty (1991) characterizes grammatically relevant properties of a verb’s arguments (e.g. subject and object) via inferences that follow from the meaning of the verb. For example, expressions like X murders Y or X interrogates Y entail that subject X caused the event.3 Second, Hopper and Thompson (1980) characterize “semantic transitivity” using similar properties, connecting semantic features to morphosyntactic beh"
N09-1057,C98-1046,0,\N,Missing
N10-1052,P06-1090,0,0.057048,"Missing"
N10-1052,J97-3002,0,0.102261,"nterminals on the source side (or XX associated with the rules. rules, for brevity): ciated with the same features as any other hierarX→hd X1 X2 , X2 at X1 i X→hX1 X2 dddd, (4) rank 10th X1 X2 i X→hX1 X2 , X2 X1 i (5) (6) Note that although XX rules 4-6 can potentially increase the chance of modeling the pre-verbal to postverbal shift, not all of them are benecial to learn. For instance, Rule 5 models the word order shift but introduces spurious ambiguity, since the nonterminals are translated in monotone order. Rule 6, which resembles the inverted rule of the Inversion Transduction Grammar (Wu, 1997), is highly ambiguous because its application has no lexical grounding. Rule 4 avoids both problems, and is also easier to learn, since it is lexically anchored by a preposition, d(at), which we can expect to appear frequently in training. These observations will motivate us to fochical rules, since they are all learned via an identical training method. 3 In contrast, glue rules are introduced into the grammar in an ad hoc manner, and the only feature associated with them is a glue penalty. These distinct feature sets makes direct comparison of scores unreliable. As a result the decoder may"
N10-1052,W07-0701,0,\N,Missing
N10-1052,D08-1024,1,\N,Missing
N10-1052,J04-4002,0,\N,Missing
N10-1052,P05-1033,0,\N,Missing
N10-1052,P07-1090,1,\N,Missing
N10-1052,W06-3119,0,\N,Missing
N10-1128,P08-1024,0,0.262288,"of a weighted CFG and t(e|f0 ) to be an FST, the quantity (1), which sums over all reorderings (and derivations), can be computed in polynomial time with dynamic programming composition, as described in §2.2. 3.2 translation distribution with a penalty term due to the prior: X λi ∂L Ep(d,a|e,f;Λ) [hi ] − Ep(e,d,a|f;Λ) [hi ] − 2 = ∂λi σ he,fi Conditional training While it is straightforward to use expectation maximization to optimize the joint likelihood of the parallel training data with a latent variable model, instead we use a log-linear parameterization and maximize conditional likelihood (Blunsom et al., 2008; Petrov and Klein, 2008). This enables us to employ a rich set of (possibly overlapping, non-independent) features to discriminate among translations. The probability of a derivation from source to reordered source to target is thus written in terms of model parameters Λ = {λi } as: P exp i λi · Hi (e, d, f0 , a, f) 0 p(e, d, f , a|f; Λ) = Z(f; Λ) X X 0 0 where Hi (e, d, f , a, f) = hi (f , r) + hi (f, s) r∈d s∈a The derivation probability is globally normalized by the partition Z(f; Λ), which is just the sum of the numerator for all derivations of f (corresponding to any e). The Hi (written"
N10-1128,W09-0436,0,0.0626448,"finition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition BTEC Chinese-Eng. Arabic-Eng. Mono 47.4 29.0 41.2 PB 51.8 30.9 45.8 Hiero 52.4 32.1 46.6 Forest 54.1 32.4 44.9 use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but ofte"
N10-1128,J07-2003,0,0.345992,"uction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs."
N10-1128,P05-1066,0,0.408016,"er process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the f"
N10-1128,P81-1022,0,0.424667,"dealing with FSTs that define binary relations over strings, not FSAs defining strings, this operation is more properly composition. However, since CFG/FSA intersection is less 5 For computational tractability, we only consider all permutations only when the number of children is less than 5, otherwise we exclude permutations where a child moves more than 4 positions away from where it starts. 860 cumbersome to describe, we present the algorithm in terms of intersection. To compute the composition of a reordering forest, G, with an FSA, F , we will make use of a variant of Earley’s algorithm (Earley, 1970). Let weighted finite-state automaton F = hΣ, Q, q0 , qfinal , δ, wi. Σ is a finite alphabet; Q is a set of states; q0 and qfinal ∈ Q are start and accept states, respectively,6 δ is the transition function Q × Σ → 2Q , and w is the transition cost function Q × Q → R. We use variables that refer to states in the FSA with the letters q, r, and s. We use x to represent a variable that is an element of Σ. Variables u and v represent costs. X and Y are non-terminals. Lowercase Greek letters are strings of terminals and non-terminals. The function δ(q, x) returns the state(s) that are reachable fro"
N10-1128,D08-1089,0,0.0505788,"es, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have"
N10-1128,P07-1019,0,0.0794581,"Missing"
N10-1128,N09-1049,0,0.0236448,"Missing"
N10-1128,N03-1017,0,0.0467756,"last, and English is a head-initial language, where heads come first. As a result, the usual order for a declarative sentence in English is SVO (subject-verb-object), but in Japanese, it is SOV, and the desired translation is John-ga ringo-o [an apple] tabeta [ate]. In summary, when translating from English into Japanese, it is usually necessary to move verbs from their position between the subject and object to the end of the sentence. This reordering can happen in two ways, which we depict in Figure 1. In the derivation on the left, a memorized phrase pair captures the movement of the verb (Koehn et al., 2003). In the other derivation, the source is first reordered into target word order and then translated, using smaller translation units. In addition, we have assumed that the phrase translations were learned from a parallel corpus that is in the original ordering, so the reordering forest F should include derivations of phrase-size units in the source order as well as the target order. 2 Note that forests are isomorphic to context-free grammars. For example, what is referred to as the ‘parse forest’, and understood to encode all derivations of a sentence s under some grammar, can also be understo"
N10-1128,P09-1019,1,0.105321,"er state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then o"
N10-1128,P03-1051,0,0.0414838,"Missing"
N10-1128,D09-1005,0,0.0104559,"der using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9 Only sentences that can be generated by the model can be used in training. 863 Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f ) and p(f |e), ‘lexically smoothed’ translation probabilities plex (e|f ) and plex (f |e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example φVP→V NP (a) would fire once for each time the rule VP → V NP was used in a derivation, a. For the Arabic-English condition, we observed that the parse trees tended to be quite flat, with many repeated n"
N10-1128,N09-2003,0,0.0159856,"e ‘middle child’ between the V and the object constituent. This can be reordered into an English SVO order using our child-permutation rules; however, if the source VP is modified by a modal particle, the parser makes the particle the parent of the VP, and it is no longer possible to move the subject to the first position in the sentence. Richer reordering rules are needed to address this problem. 9 Only sentences that can be generated by the model can be used in training. 863 Other solutions to the reachability problem include targeting reachable oracles instead of the reference translation (Li and Khudanpur, 2009) or making use of alternative training criteria, such as minimum risk training (Li and Eisner, 2009). 4.1 Features We briefly describe the feature functions we used in our model. These include the typical dense features used in translation: relative phrase translation frequencies p(e|f ) and p(f |e), ‘lexically smoothed’ translation probabilities plex (e|f ) and plex (f |e), and a phrase count feature. For the reordering model, we used a binary feature for each kind of rule used, for example φVP→V NP (a) would fire once for each time the rule VP → V NP was used in a derivation, a. For the Arab"
N10-1128,P08-1023,0,0.0724852,"entence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particul"
N10-1128,J03-1002,0,0.0034079,"Missing"
N10-1128,N03-1028,0,0.0085968,", we also make use of a spherical Gaussian prior on the value of Λ with mean 0 and variance σ 2 , which helps prevent overfitting of the model (Chen and Rosenfeld, 1998). Our objective is thus to select Λ minimizing: Y ||Λ||2 L = − log p(e|f; Λ) − 2σ 2 he,fi = − X he,fi [log Z(e, f; Λ) − log Z(f; Λ)] − ||Λ||2 2σ 2 The gradient of L with respect to the feature weights has a parallel form; it is the difference in feature expectations under the reference distribution and the 862 The form of the objective and gradient are quite similar to the traditional fully observed training scenario for CRFs (Sha and Pereira, 2003). However, rather than matching the feature expectations in the model to an observable feature value, we have to sum over the latent structure that remains after observing our target e, which makes the form of the first summand an expectation rather than just a feature function value. 3.2.1 Computing the objective and gradient The objective and gradient that were just introduced can be computed in two steps. Given a training pair he, fi, we generate the forest of reorderings F from f as described in §2.1. We then compose this grammar with T , the FST representing the translation model, which y"
N10-1128,P05-1003,0,0.00859833,"ion model so that it contained phrases from both the original order and the 1-best reordering. To be competitive with other state-of-the-art systems, we would like to use Och’s minimum error training algorithm for training; however, we cannot tune the model as described with it, since it has far too many features. To address this, we converted the coefficients on the reordering features into a single reordering feature which then had a coefficient assigned to it. This technique is similar to what is done with logarithmic opinion pools, only the learned model is not a probability distribution (Smith et al., 2005). Once we collapsed the reordering weights into a single feature, we used the techniques described by Kumar et al. (2009) to optimize the feature weights to maximize corpus BLEU on a held-out development set. 5.2 Translation results Scores on a held-out test set are reported in Table 2 using case-insensitive BLEU with 4 reference translations (16 for BTEC) using the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based dist"
N10-1128,takezawa-etal-2002-toward,0,0.100444,"Missing"
N10-1128,D09-1105,0,0.340561,"t is also intuitively satisfying because from a task perspective, we are not concerned with values of f0 , but only with producing a good translation e. 3.1 A probabilistic translation model with a latent reordering variable The translation model we use is a two phase process. First, source sentence f is reordered into a targetlike word order f0 according to a reordering model r(f0 |f). The reordered source is then transduced into the target language according to a translation model t(e|f0 ). We require that r(f0 |f) can be represented by orderings from word aligned parallel corpora, refer to Tromble and Eisner (2009). 861 u Y − →γ∈G [X → α • Y β, q, s] : u [Y → γ•, s, r] : v [X → αY • β, q, r] : u ⊗ v Goal state: [S 0 → S•, q0 , qfinal ] Figure 5: Weighted logic program for computing the intersection of a weighted FSA and a weighted CFG. a recursion-free probabilistic context-free grammar, i.e. a forest as in §2.1, and that t(e|f0 ) is represented by a (cyclic) finite-state transducer, as in Figure 2. Since the reordering forest may define multiple derivations a from f to a particular f0 , and the transducer may define multiple derivations d from f0 to a particular translation e, we marginalize over these"
N10-1128,D07-1077,0,0.0692492,"rests are used to recover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part ha"
N10-1128,J97-3002,0,0.519504,"ase transduction. By treating the reordering process as a latent variable in a probabilistic translation model, we can learn a long-range source reordering model without example reordered sentences, which are problematic to construct. The resulting model has state-of-the-art translation performance, uses linguistically motivated features to effectively model long range reordering, and is significantly smaller than a comparable hierarchical phrase-based translation model. 1 Introduction Translation models based on synchronous contextfree grammars (SCFGs) have become widespread in recent years (Wu, 1997; Chiang, 2007). Compared to phrase-based models, which can be represented as finite-state transducers (FSTs, Kumar et al. (2006)), one important benefit that SCFG models have is the ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized"
N10-1128,N09-1028,0,0.0748437,"ecover from 1best parsing errors (as such, all derivations yield the same source string). Iglesias et al. (2009) use a SCFG-based translation model, but implement it using FSTs, although they use non-regular extensions that make FSTs equivalent to recursive transition networks. Galley and Manning (2008) use a context-free reordering model to score a phrasebased (exponential) search space. Syntax-based preprocessing approaches that have relied on hand-written rules to restructure source trees for particular translation tasks have been quite widely used (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Chang et al., 2009). Discriminatively trained reordering models have been extensively explored. A widely used approach has been to 12 Satta (submitted) discusses the theoretical possibility of this sort of model but provides no experimental results. 865 Discussion and conclusion We have described a new model of translation that takes advantage of the strengths of context-free modeling, but splits reordering and phrase transduction into two separate models. This lets the contextfree part handle what it does well, mid-to-long range reordering, and lets the finite-state part handle local phrasa"
N10-1128,P01-1067,0,0.638561,"NP NP ate an apple In this grammar, the phrases John and an apple are fixed and only the VP contains ordering ambiguity. 2.1 Reordering forests based on source parses Many kinds of reordering forests are possible; in general, the best one for a particular language pair will be one that is easiest to create given the resources available in the source language. It will also be the one that most compactly expresses the source reorderings that are most likely to be useful for translation. In this paper, we consider a particular kind of reordering forest that is inspired by the reordering model of Yamada and Knight (2001).4 These are generated by taking a source language parse tree and ‘expanding’ each node so that it 4 One important difference is that our translation model is not restricted by the structure of the source parse tree; i.e., phrases used in transduction need not correspond to constituents in the source reordering forest. However, if a phrase does cross a constituent boundary between constituents A and B, then translations that use that phrase will have A and B adjacent. f John ate an apple John ate an apple an apple ate f' John ate an apple John e ジョンが リンゴを 食べた ジョンが John-ga ringo-o tabeta John-g"
N10-1128,W06-3108,0,0.0602485,"sing the original definition of the brevity penalty. We report the results of our model along with three baseline conditions, one with no-reordering at all (mono), the performance of a phrase-based translation model with distance-based distortion, the performance of our implementation of a hierarchical phrase-based translation model (Chiang, 2007), and then our model. Table 2: Translation results (BLEU) Condition BTEC Chinese-Eng. Arabic-Eng. Mono 47.4 29.0 41.2 PB 51.8 30.9 45.8 Hiero 52.4 32.1 46.6 Forest 54.1 32.4 44.9 use a classifier to predict the orientation of phrases during decoding (Zens and Ney, 2006; Chang et al., 2009). These classifiers must be trained independently from the translation model using training examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just"
N10-1128,W06-3119,0,0.0224709,"ning examples extracted from the training data. A more ambitious approach is described by Tromble and Eisner (2009), who build a global reordering model that is learned automatically from reordered training data. The latent variable discriminative training approach we describe is similar to the one originally proposed by Blunsom et al. (2008). 7 6 Related work A variety of translation processes can be formalized as the composition of a finite-state representation of input (typically just a sentence, but often a more complex structure, like a word lattice) with an SCFG (Wu, 1997; Chiang, 2007; Zollmann and Venugopal, 2006). Like these, our work uses parsing algorithms to perform the composition operation. But this is the first time that the input to a finite-state transducer has a context-free structure.12 Although not described in terms of operations over formal languages, the model of Yamada and Knight (2001) can be understood as an instance of our class of models with a specific input forest and phrases restricted to match syntactic constituents. In terms of formal similarity, Mi et al. (2008) use forests as input to a tree-to-string transducer process, but the forests are used to recover from 1best parsing"
N10-1128,C08-1144,0,0.0249409,"e ability to process long range reordering patterns in space and time that is polynomial in the length of the displacement, whereas an FST must generally explore a number of states that is exponential in this length.1 As one would expect, for language 1 Our interest here is the reordering made possible by varying the arrangement of the translation units, not the local word order differences captured inside memorized phrase pairs. pairs with substantial structural differences (and thus requiring long-range reordering during translation), SCFG models have come to outperform the best FST models (Zollmann et al., 2008). In this paper, we explore a new way to take advantage of the computational benefits of CFGs during translation. Rather than using a single SCFG to both reorder and translate a source sentence into the target language, we break the translation process into a two step pipeline where (1) the source language is reordered into a target-like order, with alternatives encoded in a context-free forest, and (2) the reordered source is transduced into the target language using an FST that represents phrasal correspondences. While multi-step decompositions of the translation problem have been proposed b"
N10-1128,I05-3027,0,\N,Missing
N12-1046,W09-0432,0,0.0157583,"uch situations, which is why we implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that w"
N12-1046,2008.amta-papers.2,0,0.0151767,"ntence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in fav"
N12-1046,W09-2404,0,0.486585,"slating each sentence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat sug"
N12-1046,P05-1033,0,0.0855443,"e hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, although of course ﬁner-scale or broader-scale discourse units might also be explored in future work. Moreover, throughout this paper we use the hierarchical phrase-based translation system (Hiero), which is based on a synchronous contextfree grammar (SCFG) model (Chiang, 2005). In a can occur synchronously with X → β in the target language. In this case, we call α the left hand side (LHS) of the rule, and β the right hand side (RHS) of the rule. To determine the extent and nature of translation consistency choices made by human translators, we randomly selected one of the four sets of reference translations (ﬁrst set, with id 0) and we used forced decoding to ﬁnd all possible sequences of rules that could transform the source sentence into the target sentence. In forced decoding, given a pair of source and target sentences, and a grammar consisting of learned trans"
N12-1046,J07-2003,0,0.131737,"s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing"
N12-1046,P02-1033,1,0.5986,"utively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-translation-per-discourse hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, alt"
N12-1046,P08-1115,1,0.830792,"for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with mu"
N12-1046,P10-4002,1,0.127564,"fi is aligned to ej . After the ﬁrst pass, we compute the feature value of each observed pair, based on this count and the DF of the target-side of the pair. We chose to use only the target token in the DF computation (i.e., aggregating over all source tokens) to 422 C3 (r) = max f ∈LHS(r) e∈RHS(r) ⟨f,e⟩ aligned bm25(⟨f, e⟩) (4) Since each variant has its beneﬁts and drawbacks, we can include all three in the system and let the tuning process decide on how each should be weighted. 5 Evaluation and Discussion We have evaluated the one-translation-per-discourse feature using the cdec MT system (Dyer et al., 2010). We started by building a baseline system using standard features in cdec: lexical and phrase translation probabilities in both directions, word and arity penalty features, and a 5-gram language model. We then added each of the three consistency feature variants, along with all two-way and the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pa"
N12-1046,H92-1045,0,0.63396,"ation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more integrated approaches. Xiao et al. (2011) took this one step further and implement an approach where they identiﬁed ambiguous translations within each document, and at417 2012 Conference of the North Am"
N12-1046,W07-0719,0,0.0457346,"Missing"
N12-1046,2005.eamt-1.19,0,0.0291934,"we implement the heuristic as a model feature, and let the model score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in t"
N12-1046,P10-1085,0,0.0189264,"l approach of translating each sentence independently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, wh"
N12-1046,P11-1124,0,0.0618186,"Missing"
N12-1046,maamouri-etal-2008-enhancing,0,0.0221488,"d the one three-way combinations of them, thus yielding a total of eight systems for comparison, including the baseline. For training the Ar-En system, we used the dataset from the DARPA GALE evaluation (Olive et al., 2011), which consists of NIST and LDC releases. The corpus was ﬁltered to remove sentence pairs with anomalous length ratios and subsampled to yield a training set containing 3.4 million parallel sentence pairs. The Arabic text was preprocessed to produce two different segmentations (simple punctuation tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignme"
N12-1046,D11-1080,0,0.0203345,"9 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En with a BLEU score of 30.96. In any case, all three variants outperformed the baseline (see Table 2). When multiple features were combined, we generally observed an increase in BLEU,"
N12-1046,P03-1058,0,0.0267734,"on choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-translation-per-discourse hypothesis in the reference translations of a standard MT test collection. We used the Ar-En MT08 data set, which contains 74 newswire documents with a total of 813 sentences, each of which has four reference translations. Throughout this paper we consistently use the document (i.e., one news story) as a convenient discourse unit, although of course ﬁn"
N12-1046,J03-1002,0,0.00364752,"n tokenization with orthographic normalization, and LDC’s ATBv3 representation (Maamouri et al., 2008)), represented together using cdec’s lattice input format (Dyer et al., 2008). The Zh-En system was trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For"
N12-1046,P02-1040,0,0.106883,"as trained on parallel training text consisting of the non-UN portions and nonHK Hansards portions of the NIST training corpora. Chinese was automatically segmented by the Stanford segmenter (Tseng et al., 2005), and traditional characters were simpliﬁed. After subsampling and ﬁltering, we obtain a training corpus of 1.6 million parallel sentences. Both training sets were word-aligned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. A SCFG was then extracted from these alignments using a sufﬁx array extractor (Chiang, 2007). Evaluation was done with multi-reference BLEU (Papineni et al., 2002) on test sets with four references for each language pair, and MIRA was used for tuning (Crammer et al., 2006). In our experiments, we run the ﬁrst decoding phase using feature weights that are guessed heuristically based on weights from previously tuned systems. All feature weights, including the discourse feature, were then tuned together, based on the output of the second decoding phase. For Ar-En parameter tuning, we used the MT06 newswire dataset, which contains 104 documents and a total of 1,797 sentences. For testing, we used the MT08 dataset described above (74 documents, 813 sentences"
N12-1046,W10-2602,0,0.588246,"score decide for each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of s"
N12-1046,2011.mtsummit-papers.13,0,0.170413,"ndently, and argue that it can indeed also be beneﬁcial to consider document-scale context when translating text. Motivated by the success of a “one sense per discourse” heuristic in Word Sense Disambiguation (WSD), we explore the potential beneﬁt of leveraging a “one translation per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more"
N12-1046,P95-1026,0,0.399604,"n per discourse” heuristic in MT. The paper is organized as follows. We begin with related work in Section 2. Next, we provide new Related work Exploiting discourse-level context has to date received only limited attention in MT research (e.g., (Gim´enez and M`arquez, 2007; Liu et al., 2010; Carpuat, 2009; Brown, 2008; Xiao et al., 2011)). Exploratory analysis of reference translations by Carpuat (2009) motivates a hypothesis that MT systems might beneﬁt from the “one sense per discourse” heuristic, ﬁrst introduced by Gale et al. (1992), which has proven to be effective in the context of WSD (Yarowsky, 1995). Carpuat’s approach was to do post-processing on the translation output to impose a “one translation per discourse” constraint where the system would otherwise have made a different choice. A manual evaluation on a sample of sentences suggested promise from the technique, which Carpuat suggested in favor of exploring more integrated approaches. Xiao et al. (2011) took this one step further and implement an approach where they identiﬁed ambiguous translations within each document, and at417 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human La"
N12-1046,C04-1059,0,0.0176229,"each case. We are aware of a few other analyses that have shown promising results based on a similar motivation. For instance, Wasser and Dorr (2008)’s approach biases the MT system based on term statistics from relevant documents in comparable corpora. Ma et al. (2011) show that a translation memory can be used to ﬁnd similar source sentences, and consecutively adapt translation choices towards consistency. Domain adaptation for MT has has also been shown to be useful in some cases (Bertoldi and Federico, 2009; Hildebrand et al., 2005; Sanchis-Trilles and Casacuberta, 2010; Tiedemann, 2010; Zhao et al., 2004), so to the extent we consider documents to be micro-domains we might expect similar approaches to be useful at document scale. Indeed, hints that such ideas may work have been available for some time. For example, there is clear evidence that the behavior of human translators can provide evidence that is often useful for automating WSD (Diab and Resnik, 2002; Ng et al., 2003). When coupled with the one-sense-per-discourse heuristic, this suggests that the reverse may also be true. 3 als that we wish to translate (in this work, news stories), we ﬁrst explore the degree of support for our one-t"
N12-1046,P11-1085,0,0.0142718,"ments. When we used NIST’s ofﬁcial metric (BLEU-4) to compare our results to the ofﬁcial NIST evaluation (NIST, 2006; NIST, 2008), our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En wit"
N12-1046,Y11-1003,0,0.0111899,"ments. When we used NIST’s ofﬁcial metric (BLEU-4) to compare our results to the ofﬁcial NIST evaluation (NIST, 2006; NIST, 2008), our baseline system achieved 54.70 for Ar-En and 31.69 for ZhEn. Based on reported NIST results, our baseline would have ranked 4th in the Zh-En MT06 evaluation, and would have outperformed all Ar-En MT08 systems. We used a slightly different IBM-BLEU metric for the rest of our evaluation. In this case, the baseline system achieved 53.07 BLEU points for Ar-En and 30.43 points for Zh-En. Among more recent papers, the best reported results were 56.87 for Ar-En MT08 (Zhao et al., 2011a) and 35.87 for Zh-En MT06 (Zhao et al., 2011b), although many papers report BLEU scores below 53 points for Arabic (Carpuat et al., 2011) and 32 points for Chinese (Monz, 2011). The systems that outperformed our baseline applied novel techniques, and used larger language models, as well as many nonstandard features. We argue that these novelties are complementary to our approach, and therefore do not damage the credibility of our baseline. Among the single-feature runs, C3 had the best performance in Ar-En experiments, with 53.84 BLEU points, whereas C2 yielded the best results for Zh-En wit"
N12-1046,P10-2033,0,\N,Missing
N12-1046,I05-3027,0,\N,Missing
N13-1060,P08-1009,0,0.0152384,"traints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source se"
N13-1060,P05-1033,0,0.278373,"fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to tha"
N13-1060,J07-2003,0,0.789841,"into a hierarchical model, guiding translation phrase choices in favor of those that respect syntactic boundaries. Second, based on such translation phrases, we propose a predicate-argument structure reordering model that predicts reordering not only between an argument and its predicate, but also between two arguments. Experiments on Chinese-to-English translation demonstrate that both advances significantly improve translation accuracy. 1 Hal Daum´e III University of Maryland College Park, USA hal@umiacs.umd.edu Introduction Hierarchical phrase-based (HPB) translation models (Chiang, 2005; Chiang, 2007) that utilize synchronous context free grammars (SCFG) have been widely adopted in statistical machine translation (SMT). Although formally syntactic, such models rarely respect linguistically-motivated syntax, and have no formal notion of semantics. As a result, they tend to produce translations containing both grammatical errors and semantic role confusions. Our goal is to take advantage of syntactic and semantic parsing to improve translation quality of HPB translation models. Rather than introducing semantic structure into the HPB model directly, we construct an improved translation model"
N13-1060,W06-1628,0,0.127879,"Missing"
N13-1060,P06-1121,0,0.106268,"S reordering model short/simple phrase (e.g., friday) or a long/complex one (e.g., when I was 20 years old), which has impact on its reordering in translation. 6 Related Work While there has been substantial work on linguistically motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the majo"
N13-1060,N03-1017,0,0.424256,"feature φi . See (Chiang, 2007) for more details. 2.2 Syntactic Constraints Translation rules in an HPB model are extracted from initial phrase pairs, which must include at least one word inside one phrase aligned to a word inside the other, such that no word inside one phrase can be aligned to a word outside the other phrase. It is not surprising to observe that initial phrases frequently are non-intuitive and inconsistent with linguistic constituents, because they are based only on statistical word alignments. Nothing in the framework actually requires linguistic knowledge. 541 Koehn et al. (2003) conjectured that such nonintuitive phrases do not help in translation. They tested this conjecture by restricting phrases to syntactically motivated constituents on both the source and target side: only those initial phrase pairs are subtrees in the derivations produced by the model. However, their phrase-based translation experiments (on Europarl data) showed the restriction to syntactic constituents is actually harmful, because too many phrases are eliminated. The idea of hard syntactic constraints then seems essentially to have been abandoned: it doesn’t appear in later work. On the face o"
N13-1060,W04-3250,0,0.0735116,"all source parse trees to annotate semantic roles for all verbal predicates. We use the 2003 NIST MT evaluation test data (919 sentence pairs) as the development data, and the 2002, 2004 and 2005 NIST MT evaluation test data (878, 1788 and 1082 sentence pairs, respectively) as the test data. For evaluation, the NIST BLEU script (version 11b) is used to calculate the NIST BLEU scores, which measures caseinsensitive matching of n-grams with n up to 4. To test whether a performance difference is statistically significant, we conduct significance tests following the paired bootstrapping approach (Koehn, 2004). 4.1 Effects of Syntactic Constraints We have also tested syntactic constraints that simply require phrases on the source side to map to a subtree (called basic constraints). Similar to requiring initial phrases on the source side to satisfy the constraints in training process, we only perform chart parsing on text spans which satisfy the constraints in decoding process. Table 1 shows the results of applying syntactic constraints with different experimental settings. From the table, we have the following observations. • Consistent with the conclusion in Koehn et al. (2003), using the basic co"
N13-1060,P10-1113,1,0.903402,"Missing"
N13-1060,C10-1081,0,0.558038,"3.1 AM-MNR very much 喜欢 红色 的 汽车 wo rengran feichang xihua hongse de qiche a. Word alignment for an English-Chinese sentence pair with semantic roles for the English sentence PAS-S A01 AM-TMP2 VBP3 A14 AM-MNR5 X3 X4 PAS-T X1 X2 X5 b. PAS-S and PAS-T for predicate like Figure 2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given"
N13-1060,W06-1606,0,0.12216,"Missing"
N13-1060,D07-1079,0,0.0445253,"Missing"
N13-1060,P08-1114,1,0.876441,"re vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong"
N13-1060,P08-1023,0,0.0472373,"cally motivated SMT, we limit ourselves here to several approaches that leverage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the len"
N13-1060,P00-1056,0,0.217393,"ver fully cover some roles while partially cover other roles. For example, phrases like the red, the read car very in Figure 1 are invalid. 544 Experiments We have presented our two-level approach to incorporating syntactic and semantic structures in a HPB system. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese seman"
N13-1060,P03-1021,0,0.0424925,"ystem. In this section, we test the effect of such structural information on a Chinese-to-English translation task. The baseline system is a reproduction of Chiang’s (2007) HPB system. The bilingual training data contains 1.5M sentence pairs with 39.4M Chinese words and 46.6M English words.4 We obtain the word alignments by running GIZA++ (Och and Ney, 2000) on the corpus in both directions and applying “grow-diag-final-and” refinement (Koehn et al., 2003). We use the SRI language modeling toolkit to train a 5-gram language model on the Xinhua portion of the Gigaword corpus and standard MERT (Och, 2003) to tune the feature weights on the development data. To obtain syntactic parse trees for instantiating syntactic constraints and predicate-argument structures for integrating the PAS reordering model, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007) trained on Chinese TreeBank 6.0 and then ran the Chinese semantic role 4 This dataset includes LDC2002E18, LDC2003E14, Hansards portion of LDC2004T08 and LDC2005T06 LDC2003E07, LDC2004T07, max-phrase-length=10 max-char-span=10 max-phrase-length=∞ max-char-span=∞ System base HPB + basic constraints + unflattened"
N13-1060,J05-1004,0,0.0359002,"2: Example of PAS on both the source and target side. Items are aligned by indices. divergence between semantic frames of the source and target language. However, considering there is no efficient way of jointly performing MT and SRL, accurate SRL on target side can only be done after translation. Similar to related work (Liu and Gildea, 2010; Xiong et al., 2012), we obtain the PAS of the source language (PAS-S) via a shallow semantic parser and project the PAS of the target language (PAS-T) using the word alignment derived from the translation process. Specifically, we use PropBank standard (Palmer et al., 2005; Xue, 2008) which defines a set of numbered core arguments (i.e., A0-A5) and adjunct-like arguments (e.g., AM-TMP for temporal, AM-MNR for manner). Figure 2(b) shows an example of PAS projection from source language to target language.2 The PAS reordering model describes the probability of reordering PAS-S into PAST. Given a predicate p, it takes the following form: P (PAS-T |PAS-S, PRE=p) (2) Note that cases for untranslated roles can be naturally reflected in our PAS reordering model. For example, if the argument IA0 is untranslated in Figure 2, its PAS-T will be X2 X5 X3 X4 . 2 In PAS-S, w"
N13-1060,N07-1051,0,0.242769,"Missing"
N13-1060,J10-4005,0,0.0172666,"verage syntactic constraints yet still allow cross-constituent translations. In terms of tree-based SMT with crossconstituent translations, Cowan et al. (2006) allowed non-constituent sub phrases on the source side and adopted phrase-based translation model for modifiers in clauses. Marcu (2006) and Galley et al. (2006) inserted artificial constituent nodes in parsing tree as to capture useful but non-constituent phrases. The parse tree binarization approach (Wang et al., 2007; Marcu, 2007) and the forestbased approach (Mi et al., 2008) would also cover non-constituent phrases to some extent. Shen et al. (2010) defined well-formed dependency structure to cover uncompleted dependency structure in 547 translation rules. In addition to the fact that the constraints of Shen et al. (2010) and this paper are based on different syntactic perspectives (i.e., dependency structure vs. constituency structure), the major difference is that in this work we don’t limit the length of phrases to a fixed maximum size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints,"
N13-1060,D07-1078,0,0.0933571,"ned trees are more reliable than unflattened ones, in the sense that some bracketing errors in unflattened trees can be eliminated during tree flattening. Figure 1(b) illustrates flattening a syntactic parse by moving the head (like) and all its modifiers (I, still, the red car, and very much) to the same level. Third, initial phrase pair extraction in Chiang’s HPB generates a very large number of rules, which makes training and decoding very slow. To avoid this, a widely used strategy is to limit initial phrases to a reasonable length on either side during rule extraction (e.g., 10 in Chiang (2007)). A corresponding constraint to speed up decoding prohibits any X from spanning a substring longer than a fixed length, often the same as the maximum phrase length in rule extraction. Although the initial phrase length limitation mainly keeps non-intuitive phrases out, it also closes the door on some useful phrases. For example, a translation rule hI still like X, wo rengran xihuan Xi will be prohibited if the non-terminal X covers 8 or more words. In contrast, our hard constraints have already filtered out dominating nonintuitive phrases; thus there is more room to include additional useful"
N13-1060,N09-2004,0,0.0780833,"-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two main aspects: 1) we consider reordering not only between an argument and its predicate, but also between two arguments; and 2) our reordering model can naturally model cases of un"
N13-1060,P09-1036,0,0.0207385,"um size (e.g., 10 in Hiero). Consequently, we obtain some translation rules that are not found in Hiero systems constrained by the length. In terms of (hierarchical) phrase-based SMT with syntactic constraints, particular related to constituent boundaries, Koehn et al. (2003) tested constraints allowing constituent matched phrases only. Chiang (2005) and Cherry (2008) used a soft constraint to award or penalize hypotheses which respect or violate syntactic boundaries. Marton and Resnik (2008) further explored the idea of soft constraints by distinguishing among constituent types. Xiong et al. (2009; 2010) presented models that learn phrase boundaries from aligned dataset. On the other hand, semantics motivated SMT has also seen an increase in activity recently. Wu and Fung (2009) re-ordered arguments on the target side translation output, seeking to maximize the crosslingual match of the semantic frames of the reordered translation to that of the source sentence. Liu and Gildea (2010) added two types of semantic role features into a tree-to-string translation model. Although Xiong et al. (2012) and our work are both focusing on source side PAS reordering, our model differs from theirs in two m"
N13-1060,N10-1016,0,0.0365831,"Missing"
N13-1060,P12-1095,0,0.438706,"vated. In previous work, Liu and Gildea (2010) model the reordering/deletion of source-side semantic roles in a tree-to-string translation model. While it is natural to include semantic structures in a treebased translation model, the effect of semantic structures is presumably limited, since tree templates themselves have already encoded semantics to some 540 Proceedings of NAACL-HLT 2013, pages 540–549, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics extent. For example, template (VP (VBG giving) NP#1 NP#2) entails NP#1 as receiver and NP#2 as thing given. Xiong et al. (2012) model the reordering between predicates and their arguments by assuming arguments are translated as a unit. However, they only considered the reordering between arguments and their predicates. 2 Syntactic Constraints for HPB Translation Model In this section, we briefly review the HPB model, then present our approach to incorporating syntactic constraints into it. 2.1 HPB Translation Model In HPB models, synchronous rules take the form X → hγ, α, ∼i, where X is the non-terminal symbol, γ and α are strings of lexical items and nonterminals in the source and target side, respectively, and ∼ ind"
N13-1060,D10-1030,0,0.0451222,"Missing"
N13-1060,J02-3001,0,\N,Missing
N13-3009,P11-1026,1,\N,Missing
N13-3009,P12-1009,1,\N,Missing
N15-1052,P11-2037,0,0.0376754,"Missing"
N15-1052,D13-1135,0,0.253898,"Missing"
N15-1052,J95-2003,0,0.892876,"Missing"
N15-1052,N06-2015,0,0.149649,"Missing"
N15-1052,D10-1086,0,0.505565,"Missing"
N15-1052,N07-1051,0,0.0325048,"Missing"
N15-1052,D07-1057,0,\N,Missing
N15-1052,D07-1007,0,\N,Missing
N15-1052,D14-1084,0,\N,Missing
N16-1163,P14-1023,0,0.0576199,"them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them. While these models have demonstrated impressive success in capturing some aspects of word meaning (Landauer and Dumais, 1997; Turney et al., 2010; Mikolov et al., 2013; Baroni et al., 2014; Levy et al., 2014), they generally fail to capture the fact that single word forms often have multiple meanings. This can lead to counterintuitive results—for example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in"
N16-1163,D07-1007,1,0.78531,"ecent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego, California, June 12-"
N16-1163,P02-1033,1,0.7973,"aine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–"
N16-1163,J93-1003,0,0.134511,"s 48 items in ESL, 87 items in RD, and 77 items in TOEFL. 2 The designated development set of MEN-3k (2000 items) was used for tuning. 3 To alleviate sparsity we lemmatized the ukWaC corpus. Runs without lemmatization produced weaker results. 1380 ∼5.8M lines of segmented Chinese-English parallel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes corpus (Weischedel et al., 2013).4 We perform word alignment with the Berkeley aligner (Liang et al., 2006). We filter out noisy alignments using the Gtest statistic (Dunning, 1993), with a threshold selected during tuning on a development set. We set α (see Equation 1) to 1.0. Each sensesense edge hei (cj ), ei0 (cj )i has individual weight 0 < βr ≤ 1, computed by obtaining the G-test statistic for the alignment of ei with cj and for the alignment of ei0 with cj , running these values through a logistic function, and averaging. Parameters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the development set. Note that we have not currently incorporated special treatment for alig"
N16-1163,C14-1048,0,0.102654,"its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves perform"
N16-1163,P12-1092,0,0.335954,"arine@cs.umd.edu Abstract create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using paralle"
N16-1163,N15-1070,0,0.255088,"es, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction V"
N16-1163,S10-1003,0,0.101687,"al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego, California, June 12-17, 2016. 2016 Associatio"
N16-1163,W14-1618,0,0.0324777,"a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derived from WordNet, and combining the two representation types significantly improves performance. 1 Introduction Vector space models (VSMs) provide a powerful tool for representing word meanings and modeling the relations between them. While these models have demonstrated impressive success in capturing some aspects of word meaning (Landauer and Dumais, 1997; Turney et al., 2010; Mikolov et al., 2013; Baroni et al., 2014; Levy et al., 2014), they generally fail to capture the fact that single word forms often have multiple meanings. This can lead to counterintuitive results—for example, it should be possible for the nearest word to rock to be stone in everyday usage, punk in discussions of music, and crack (cocaine) in discussions about drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. re"
N16-1163,Q15-1016,0,0.0450174,"in and Goodenough, 1965), MC-30 (Miller and Charles, 1991), and the designated test subset (1000 items) of MEN3k (Bruni et al., 2014), using avgSim (Jauhar et al., 2015, eq. 8) as the similarity rating, and evaluating model ratings against human similarity ratings via Spearman’s rank correlation coefficient (ρ).2 Initial word representations. We use the word2vec (Mikolov et al., 2013) skip-gram architecture to train 80-dimensional word vectors (in keeping with Jauhar et al.), based on evidence that this model shows consistently strong performance on a wide array of tasks (Baroni et al., 2014; Levy et al., 2015). Training is on ukWaC (Ferraresi et al., 2008), a diverse 2B-word web corpus.3 Sense-graph construction from parallel text. To construct the sense graph per Section 2, we use 1 Because it is not clear how multi-word phrases should best be treated (and this is not a question being investigated here), we filter out any questions containing multi-word phrases for any of the relevant items (probe or possible response), and any questions for which any of the relevant items is completely out of vocabulary (no vectors available) for any of the evaluated models. This leaves 48 items in ESL, 87 items"
N16-1163,N06-1014,0,0.014505,"f vocabulary (no vectors available) for any of the evaluated models. This leaves 48 items in ESL, 87 items in RD, and 77 items in TOEFL. 2 The designated development set of MEN-3k (2000 items) was used for tuning. 3 To alleviate sparsity we lemmatized the ukWaC corpus. Runs without lemmatization produced weaker results. 1380 ∼5.8M lines of segmented Chinese-English parallel text from the DARPA BOLT project and the Broadcast Conversation subset of the segmented Chinese-English parallel data in the OntoNotes corpus (Weischedel et al., 2013).4 We perform word alignment with the Berkeley aligner (Liang et al., 2006). We filter out noisy alignments using the Gtest statistic (Dunning, 1993), with a threshold selected during tuning on a development set. We set α (see Equation 1) to 1.0. Each sensesense edge hei (cj ), ei0 (cj )i has individual weight 0 < βr ≤ 1, computed by obtaining the G-test statistic for the alignment of ei with cj and for the alignment of ei0 with cj , running these values through a logistic function, and averaging. Parameters for these computations, as well as the G-test statistic threshold below which we filtered out noisy alignments, were selected during tuning on the development se"
N16-1163,D14-1113,0,0.289377,"Missing"
N16-1163,P03-1058,0,0.230636,"out drugs. In a recent paper, Jauhar et al. (2015) introduce a method for “retrofitting” generic word vectors to create sense-specific vectors using the WordNet semantic lexicon (Miller, 1995). From WordNet, they In this paper, we observe that the crucial meaning relationships in the Jauhar et al. retrofitting process—the word sense graph—can be inferred based on another widely available resource: bilingual parallel text. This observation is grounded in a well-established tradition of using cross-language correspondences as a form of sense annotation (Gale et al., 1992; Diab and Resnik, 2002; Ng et al., 2003; Carpuat and Wu, 2007; Lefever and Hoste, 2010, and others). Using parallel text to define sense distinctions sidesteps the persistent difficulty of identifying a single correct sense partitioning based on human intuition, and avoids large investments in manual curation or annotation. We use parallel text and word alignment to infer both word sense identities and inter-sense relations required for the sense graph, and apply the approach of Jauhar et al. to retrofit existing word vector representations and create a sense-based vec1378 Proceedings of NAACL-HLT 2016, pages 1378–1383, c San Diego"
N16-1163,N10-1013,0,0.270359,"{aetting, resnik}@umd.edu, marine@cs.umd.edu Abstract create a graph structure comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations l"
N16-1163,C14-1016,0,0.0959426,"ture comprising two classes of relations: form-based relations between each word form and its respective senses, and meaning-based relations between word senses with similar meanings. This graph structure is then used to transform a traditional VSM into an enriched VSM, where each point in the space represents a word sense, rather than a word form. This approach is appealing as, unlike with prior sense-aware representations, senses are defined categories in a semantic lexicon, rather than clusters induced from raw text (Reisinger and Mooney, 2010; Huang et al., 2012; Neelakantan et al., 2015; Tian et al., 2014), and the method does not require performing word sense disambiguation (Guo et al., 2014). Jauhar et al. (2015) recently proposed to learn sense-specific word representations by “retrofitting” standard distributional word representations to an existing ontology. We observe that this approach does not require an ontology, and can be generalized to any graph defining word senses and relations between them. We create such a graph using translations learned from parallel corpora. On a set of lexical semantic tasks, representations learned using parallel text perform roughly as well as those derive"
N16-1163,W09-2413,0,\N,Missing
P01-1032,P00-1059,0,\N,Missing
P01-1032,1997.mtsummit-workshop.13,1,\N,Missing
P01-1032,P98-1081,0,\N,Missing
P01-1032,C98-1078,0,\N,Missing
P01-1032,P97-1020,1,\N,Missing
P01-1032,1997.mtsummit-workshop.3,1,\N,Missing
P01-1032,J96-2004,0,\N,Missing
P01-1032,A00-2026,0,\N,Missing
P02-1033,W99-0512,0,\N,Missing
P02-1033,W00-0801,1,\N,Missing
P02-1033,H91-1025,0,\N,Missing
P02-1033,H94-1047,0,\N,Missing
P02-1033,C92-2070,0,\N,Missing
P02-1033,H93-1052,0,\N,Missing
P02-1033,P95-1026,0,\N,Missing
P02-1033,P91-1048,0,\N,Missing
P02-1033,J94-4003,0,\N,Missing
P02-1033,P99-1068,1,\N,Missing
P02-1033,P00-1056,0,\N,Missing
P02-1050,han-etal-2000-handling,0,\N,Missing
P02-1050,P97-1003,0,\N,Missing
P02-1050,J93-2003,0,\N,Missing
P02-1050,W01-1406,0,\N,Missing
P02-1050,2001.mtsummit-ebmt.4,0,\N,Missing
P02-1050,W01-1403,0,\N,Missing
P02-1050,H01-1014,0,\N,Missing
P02-1050,W00-1306,1,\N,Missing
P02-1050,N01-1026,0,\N,Missing
P02-1050,J90-2002,0,\N,Missing
P02-1050,P98-1035,0,\N,Missing
P02-1050,C98-1035,0,\N,Missing
P02-1050,P01-1067,0,\N,Missing
P02-1050,J94-4004,0,\N,Missing
P02-1050,N01-1023,0,\N,Missing
P02-1050,P01-1017,0,\N,Missing
P04-1048,W04-0909,1,0.912344,"e challenges, we have developed SemFrame, a system that induces semantic frames automatically. Overall, the system performs two primary functions: (1) identification of sets of verb senses that evoke a common semantic frame (in the sense that lexical units call forth corresponding conceptual structures); and (2) identification of the conceptual structure of semantic frames. This paper explores the first task of identifying frame semantic verb classes. These classes have several types of uses. First, they are the basis for identifying the internal structure of the frame proper, as set forth in Green and Dorr, 2004. Second, they may be used to extend FrameNet. Third, they support applications needing access to sets of semantically related words, for example, text segmentation and word sense disambiguation, as explored to a limited degree in Green, 2004. Section 2 presents related research efforts on developing semantic verb classes. Section 3 summarizes the features of WordNet (http://www.cogsci.princeton.edu/~wn) and LDOCE (Procter, 1978) that support the automatic induction of semantic verb classes, while Section 4 sets forth the approach taken by SemFrame to accomplish this task. Section 5 presents a"
P04-1048,N03-1013,1,0.843617,"Missing"
P04-1048,W03-1604,0,0.0942637,"Missing"
P04-1048,W03-1601,0,\N,Missing
P04-1048,2003.mtsummit-systems.9,1,\N,Missing
P04-1048,W04-0804,0,\N,Missing
P05-3009,E03-2007,0,0.0613964,"Missing"
P05-3009,J03-3002,1,0.703822,"Missing"
P05-3009,C00-2157,0,\N,Missing
P08-1114,W07-0702,0,0.00938484,"l., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system"
P08-1114,P05-1033,0,0.823773,"stical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), an"
P08-1114,J07-2003,0,0.864099,"from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language an"
P08-1114,P05-1066,0,0.0659097,"ct parser-based constituency requirements, they explore the use of phrases spanning generalized, categorial-style constituents in the parse tree, e.g. type NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet |fsnippet ) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgm"
P08-1114,W06-1628,0,0.541631,"stantial improvements in performance for translation from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguist"
P08-1114,D07-1079,0,0.307419,"parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and find it useful to modify it in order to accommodate useful “phrasal” chunks that are present in parallel training data but not licensed by linguistically motivated parses of the target language. Similarly, Cowan et al. (2006) focus on using syntactically rich representations of source and target parse trees, but they resort to phrase-based translation for modifiers within 1 See (Lopez, to appear) for a comprehensive survey. 1003 Proceedings of ACL-08: HLT, pages 1003–1011, c Columbus, Ohio, USA, June 2008. 2"
P08-1114,P03-2041,0,0.0658122,"ource constituent and 0 otherwise. In the latter case λc φc (f¯, e¯) = 0 and the score in expression (1) is unaffected. 5 In fact, this turns out not to be the issue; see Section 4. 1005 Figure 1: Illustration of Chiang’s (2005) syntactic constituency feature, which does not distinguish among constituent types. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense ¯ extra credit when not just to give a rule X → h¯ e, fi ¯ f matches a constituent, but to incur a cost when f¯ violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f¯ is the minister gave a (and other cases, such as minister gave, min"
P08-1114,W02-1039,0,0.206991,"¯ spans a source constituent and 0 otherwise. In the latter case λc φc (f¯, e¯) = 0 and the score in expression (1) is unaffected. 5 In fact, this turns out not to be the issue; see Section 4. 1005 Figure 1: Illustration of Chiang’s (2005) syntactic constituency feature, which does not distinguish among constituent types. First, the constituency feature treats all syntactic constituent types equally, making no distinction among them. For any given language pair, however, there might be some source constituents that tend to map naturally to the target language as units, and others that do not (Fox, 2002; Eisner, 2003). Moreover, a parser may tend to be more accurate for some constituents than for others. Second, the Chiang (2005) constituency feature gives a rule additional credit when the rule’s source side overlaps exactly with a source-side syntactic constituent. Logically, however, it might make sense ¯ extra credit when not just to give a rule X → h¯ e, fi ¯ f matches a constituent, but to incur a cost when f¯ violates a constituent boundary. Using the example in Figure 1, we might want to penalize hypotheses containing rules where f¯ is the minister gave a (and other cases, such as min"
P08-1114,P06-1121,0,0.17672,"Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et"
P08-1114,P03-1054,0,0.0131144,"odels were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-E"
P08-1114,D07-1091,0,0.0298786,"al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves st"
P08-1114,N03-1017,0,0.0756169,"se extraction approach in order to acquire the synchronous rules of the grammar directly from word-aligned parallel text. Rules have the form X → h¯ e, f¯i, where e¯ and f¯ are phrases containing terminal symbols (words) and possibly co-indexed instances of the 1004 nonterminal symbol X.2 Associated with each rule is a set of translation model features, φ i (f¯, e¯); for example, one intuitively natural feature of a rule is the phrase translation (log-)probability φ( f¯, e¯) = log p(¯ e|f¯) , directly analogous to the corresponding feature in non-hierarchical phrase-based models like Pharaoh (Koehn et al., 2003). In addition to this phrase translation probability feature, Hiero’s feature set includes the inverse phrase translation prob¯ e), lexical weights lexwt(f¯|¯ ability log p(f|¯ e) and lexwt(¯ e|f¯), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing X λi φi (f¯, e¯) (1) i to the total log-probability of a derived hypothesis. Ea"
P08-1114,W04-3250,0,0.357743,"ser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseline with additional features as described in Section 3. All models were optimized and tested using the BLEU metric (Papineni et al., 2002) with the NISTimplemented (“shortest”) effective reference length, on lowercased, tokenized outputs/references. Statistical significance of difference from the baseline BLEU score was measured by using paired bootstrap re-sampling (Koehn, 2004).9 4.1 Chinese-English For the Chinese-English translation experiments, we trained the translation model on the corpora in Table 1, totalling approximately 2.1 million sentence pairs after GIZA++ filtering for length ratio. Chinese text was segmented using the Stanford segmenter (Tseng et al., 2005). 9 Whenever we use the word “significant”, we mean “statistically significant” (at p &lt; .05 unless specified otherwise). LDC ID LDC2002E18 LDC2003E07 LDC2005T10 LDC2003E14 LDC2005T06 LDC2004T08 Description Xinhua Ch/Eng Par News V1 beta Ch/En Treebank Par Corpus Ch/En News Mag Par Txt (Sinorama) FBI"
P08-1114,W06-1606,0,0.514927,"on from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and"
P08-1114,P00-1056,0,0.186377,"parses. 1006 definitions of XP+, XP_, and XP2 are analogous. • Similarly, since Chiang’s original constituency feature can be viewed as a disjunctive “alllabels=” feature, we also defined “all-labels+”, “all-labels2”, and “all-labels_” analogously. 4 Experiments We carried out MT experiments for translation from Chinese to English and from Arabic to English, using a descendant of Chiang’s Hiero system. Language models were built using the SRI Language Modeling Toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Word-level alignments were obtained using GIZA++ (Och and Ney, 2000). The baseline model in both languages used the feature set described in Section 2; for the Chinese baseline we also included a rule-based number translation feature (Chiang, 2007). In order to compute syntactic features, we analyzed source sentences using state of the art, tree-bank trained constituency parsers ((Huang et al., 2008) for Chinese, and the Stanford parser v.2007-08-19 for Arabic (Klein and Manning, 2003a; Klein and Manning, 2003b)). In addition to the baseline condition, and baseline plus Chiang’s (2005) original constituency feature, experimental conditions augmented the baseli"
P08-1114,P03-1021,0,0.0403824,"b¯ e), lexical weights lexwt(f¯|¯ ability log p(f|¯ e) and lexwt(¯ e|f¯), which are estimates of translation quality based on word-level correspondences (Koehn et al., 2003), and a rule penalty allowing the model to learn a preference for longer or shorter derivations; see (Chiang, 2007) for details. These features are combined using a log-linear model, with each synchronous rule contributing X λi φi (f¯, e¯) (1) i to the total log-probability of a derived hypothesis. Each λi is a weight associated with feature φ i , and these weights are typically optimized using minimum error rate training (Och, 2003). 2.2 Soft Syntactic Constraints When looking at Hiero rules, which are acquired automatically by the model from parallel text, it is easy to find many cases that seem to respect linguistically motivated boundaries. For example, X → hjingtian X1 , X1 this yeari, seems to capture the use of jingtian/this year as a temporal modifier when building linguistic constituents such as noun phrases (the election this year) or verb phrases (voted in the primary this year). However, it is important to observe that nothing in the Hiero framework actually requires nonterminal symbols to cover linguistically"
P08-1114,2006.amta-papers.17,0,0.0800506,"Missing"
P08-1114,N06-1032,0,0.00661729,"pe NP/NN denotes a phrase like the great that lacks only a head noun (say, wall) in order to comprise an NP. In addition, various researchers have explored the use of hard linguistic constraints on the source side, e.g. via “chunking” noun phrases and translating them separately (Owczarzak et al., 2006), or by performing hard reorderings of source parse trees in order to more closely approximate target-language word order (Wang et al., 2007a; Collins et al., 2005). Finally, another soft-constraint approach that can also be viewed as coming from the data-driven side, adding syntax, is taken by Riezler and Maxwell (2006). They use LFG dependency trees on both source and target sides, and relax syntactic constraints by adding a “fragment grammar” for unparsable chunks. They decode using Pharaoh, augmented with their own log-linear features (such as p(esnippet |fsnippet ) and its converse), side by side to “traditional” lexical weights. Riezler and Maxwell (2006) do not achieve higher BLEU scores, but do score better according to human grammaticality judgments for in-coverage cases. 7 Conclusion When hierarchical phrase-based translation was introduced by Chiang (2005), it represented a new and successful way t"
P08-1114,P07-1090,0,0.0274815,"hallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal coverage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide constraints on the extraction of hierarchical phrase pairs. Unlike Hiero, their translation model uses a full range of named nonterminal symbols in the synchronous grammar. As an alternative way to relax strict parser-based constituen"
P08-1114,D07-1077,0,0.268129,"ere to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal coverage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide cons"
P08-1114,D07-1078,0,0.147546,"ere to several approaches that seem most closely related. Among approaches using parser-based syntactic models, several researchers have attempted to reduce the strictness of syntactic constraints in order to better exploit shallow correspondences in parallel training data. Our introduction has already briefly noted Cowan et al. (2006), who relax parse-tree-based alignment to permit alignment of non-constituent subphrases on the source side, and translate modifiers using a separate phrase-based model, and DeNeefe et al. (2007), who modify syntax-based extraction and binarize trees (following (Wang et al., 2007b)) to improve phrasal coverage. Similarly, Marcu et al. (2006) relax their syntax-based system by rewriting target-side parse trees on the fly in order to avoid the loss of “nonsyntactifiable” phrase pairs. Setiawan et al. (2007) employ a “function-word centered syntax-based approach”, with synchronous CFG and extended ITG models for reordering phrases, and relax syntactic constraints by only using a small number function words (approximated by high-frequency words) to guide the phrase-order inversion. Zollman and Venugopal (2006) start with a target language parser and use it to provide cons"
P08-1114,J97-3002,0,0.473894,"es that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to training (e.g., (Koehn and Hoang, 2007; Birch et al., 2007; Hassan et al., 2007)), and also include systems employing contextfree models trained on parallel text without benefit of any prior linguistic analysis (e.g. (Chiang, 2005; Chiang, 2007; Wu, 1997)). Over time, however, there has been increasing movement in the direction of systems that are syntactic in both the formal and linguistic senses. In any such system, there is a natural tension between taking advantage of the linguistic analysis, versus allowing the model to use linguistically unmotivated mappings learned from parallel training data. The tradeoff often involves starting with a system that exploits rich linguistic representations and relaxing some part of it. For example, DeNeefe et al. (2007) begin with a tree-to-string model, using treebank-based target language analysis, and"
P08-1114,W06-3119,0,0.340935,"s in performance for translation from Chinese and Arabic to English. 1 Introduction The statistical revolution in machine translation, beginning with (Brown et al., 1993) in the early 1990s, replaced an earlier era of detailed language analysis with automatic learning of shallow source-target mappings from large parallel corpora. Over the last several years, however, the pendulum has begun to swing back in the other direction, with researchers exploring a variety of statistical models that take advantage of source- and particularly target-language syntactic analysis (e.g. (Cowan et al., 2006; Zollmann and Venugopal, 2006; Marcu et al., 2006; Galley et al., 2006) and numerous others). Chiang (2005) distinguishes statistical MT approaches that are “syntactic” in a formal sense, going beyond the finite-state underpinnings of phrasebased models, from approaches that are syntactic in a linguistic sense, i.e. taking advantage of a priori language knowledge in the form of annotations derived from human linguistic analysis or treebanking.1 The two forms of syntactic modeling are doubly dissociable: current research frameworks include systems that are finite state but informed by linguistic annotation prior to trainin"
P08-1115,J96-1002,0,0.0180756,"Missing"
P08-1115,J90-2002,0,0.449742,"te state techniques can be naturally extended to more expressive synchronous context-free grammarbased models. Additionally, we resolve a significant complication that non-linear word lattice inputs introduce in reordering models. Our experiments evaluating the approach demonstrate substantial gains for ChineseEnglish and Arabic-English translation. 1 Introduction When Brown and colleagues introduced statistical machine translation in the early 1990s, their key insight – harkening back to Weaver in the late 1940s – was that translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from sp"
P08-1115,P05-1033,0,0.398479,"for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f 0 ∈ F(o) and we seek eˆ = arg max max P r(e, f 0 |o) e eˆ = arg max P r(e|f ) e (1) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f . There, Bertoldi and others have recently found that, rather than translating a single-best transcription f , it is advantageous to allow the MT decoder to f 0 ∈F (o) = arg max max P r(e)P r(f 0 |e,"
P08-1115,J07-2003,0,0.845516,"cross a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation by showing that lattice decoding can also yield improvements for text by preserving alternative analyses of the input. In addition, we generalize lattice decoding algorithmically, extending it for the first time to hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007). Formally, the approach we take can be thought of as a “noisier channel”, where an observed signal o gives rise to a set of source-language strings f 0 ∈ F(o) and we seek eˆ = arg max max P r(e, f 0 |o) e eˆ = arg max P r(e|f ) e (1) An exception is the translation of speech recognition output, where the acoustic signal generally underdetermines the choice of source word sequence f . There, Bertoldi and others have recently found that, rather than translating a single-best transcription f , it is advantageous to allow the MT decoder to f 0 ∈F (o) = arg max max P r(e)P r(f 0 |e, o) e f 0 ∈F (o"
P08-1115,W07-0721,0,0.0249901,"Missing"
P08-1115,W07-0729,1,0.806539,"ximum probability value over all possible paths in the lattice for each jump considered, which is similar to the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs."
P08-1115,N04-1001,0,0.0109403,"ical phrase-based translation model, using our modified version of Hiero (Chiang, 2005; Chiang, 2007). These two translation model types illustrate the applicability of the theoretical contributions presented in Section 2 and Section 3. We observed that the coverage of named entities (NEs) in our baseline systems was rather poor. Since names in Chinese can be composed of relatively long strings of characters that cannot be translated individually, when generating the segmentation lattices that included cs arcs, we avoided segmenting NEs of type PERSON, as identified using a Chinese NE tagger (Florian et al., 2004). The results are summarized in Table 4. We see that using word lattices improves BLEU scores both in the phrase-based model and hierarchical model as compared to the single-best segmentation approach. All results using our word-lattice decoding for the hierarchical models (hs+ss and hs+ss+cs) are significantly better than the best segmentation (ss).4 For the phrase-based model, we obtain significant gains using our word-lattice decoder using all three segmentations on MT05. The other results, while better than the best segmentation (hs) by at least 0.3 BLEU points, are not statistically signi"
P08-1115,J99-4004,0,0.0214313,"in this table is a triple hFij , pij , Rij i 2.2 Parsing word lattices Chiang (2005) introduced hierarchical phrase-based translation models, which are formally based on synchronous context-free grammars (SCFGs). Translation proceeds by parsing the input using the source language side of the grammar, simultaneously building a tree on the target language side via the target side of the synchronized rules. Since decoding is equivalent to parsing, we begin by presenting a parser for word lattices, which is a generalization of a CKY parser for lattices given in Cheppalier et al. (1999). Following Goodman (1999), we present our lattice parser as a deductive proof system in Figure 2. The parser consists of two kinds of items, the first with the form [X → α • β, i, j] representing rules that have yet to be completed and span node i to node j. The other items have the form [X, i, j] and indicate that non-terminal X spans [i, j]. As with sentence parsing, the goal is a deduction that covers the spans of the entire input lattice [S, 0, |V |− 1]. The three inference rules are: 1) match a terminal symbol and move across one edge in the lattice 2) move across an -edge without advancing the dot in an incompl"
P08-1115,N06-2013,0,0.0347615,"mentation variant we made use of. The limitation of this approach is that as the amount and variety of training data increases, the optimal segmentation strategy changes: more aggressive segmentation results 2 The segmentation process is ambiguous, even for native speakers of Chinese. 1 硬 硬质 0 号 合金 质 2 金 合 4 5 称 工 6 号称 "" 7 8 工业 业 10 牙 9 牙齿 齿 11 "" 12 3 硬质合金 Figure 5: Sample Chinese segmentation lattice using three segmentations. in fewer OOV tokens, but automatic evaluation metrics indicate lower translation quality, presumably because the smaller units are being translated less idiomatically (Habash and Sadat, 2006). Lattices allow the decoder to make decisions about what granularity of segmentation to use subsententially. 4.1 Chinese Word Segmentation Experiments In our experiments we used two state-of-the-art Chinese word segmenters: one developed at Harbin Institute of Technology (Zhao et al., 2001), and one developed at Stanford University (Tseng et al., 2005). In addition, we used a character-based segmentation. In the remaining of this paper, we use cs for character segmentation, hs for Harbin segmentation and ss for Stanford segmentation. We built two types of lattices: one that combines the Harbi"
P08-1115,N03-1017,0,0.215202,"bic-English and Chinese-English translation. In Section 5 we discuss relevant prior work, and we conclude in Section 6. 2 Decoding Most statistical machine translation systems model translational equivalence using either finite state transducers or synchronous context free grammars (Lopez, to appear 2008). In this section we discuss the issues associated with adapting decoders from both classes of formalism to process word lattices. The first decoder we present is a SCFG-based decoder similar to the one described in Chiang (2007). The second is a phrase-based decoder implementing the model of Koehn et al. (2003). 2.1 Word lattices A word lattice G = hV, Ei is a directed acyclic graph that formally is a weighted finite state automaton (FSA). We further stipulate that exactly one node has no outgoing edges and is designated the ‘end node’. Figure 1 illustrates three classes of word lattices. 1013 c 2 a x 0 m=1 b 1 b 1 ε 1 x 0 3 2 d c 3 2 b c 3 y a Figure 1: Three examples of word lattices: (a) sentence, (b) confusion network, and (c) non-linear word lattice. A word lattice is useful for our purposes because it permits any finite set of strings to be represented and allows for substrings common to multi"
P08-1115,2005.iwslt-1.8,0,0.0223693,". 3.1 MT05 0.3063 0.3176 Experimental results We tested the effect of the distance metric on translation quality using Chinese word segmentation lattices (Section 4.1, below) using both a hierarchical and phrase-based system modified to translate word lattices. We compared the shortest-path distance metric with a baseline which uses the difference in node number as the distortion distance. For an additional datapoint, we added a lexicalized reordering model that models the probability of each phrase pair appearing in three different orientations (swap, monotone, other) in the training corpus (Koehn et al., 2005). Table 2 summarizes the results of the phrasebased systems. On both test sets, the shortest path metric improved the BLEU scores. As expected, the lexicalized reordering model improved translation quality over the baseline; however, the improvement was more substantial in the model that used the shortest-path distance metric (which was already a higher baseline). Table 3 summarizes the results of our experiment comparing the performance of two distance metrics to determine whether a rule has exceeded the decoder’s span limit. The pattern is the same, showing a clear increase in BLEU for the s"
P08-1115,P07-2045,0,0.0604115,"translation could be viewed as an instance of noisy channel modeling (Brown et al., 1990). They introduced a now standard decomposition that distinguishes modeling sentences in the target language (language models) from modeling the relationship between source and target language (translation models). Today, virtually all statistical translation systems seek the best hypothesis e for a given input f in the source language, according to consider all possibilities for f by encoding the alternatives compactly as a confusion network or lattice (Bertoldi et al., 2007; Bertoldi and Federico, 2005; Koehn et al., 2007). Why, however, should this advantage be limited to translation from spoken input? Even for text, there are often multiple ways to derive a sequence of words from the input string. Segmentation of Chinese, decompounding in German, morphological analysis for Arabic — across a wide range of source languages, ambiguity in the input gives rise to multiple possibilities for the source word sequence. Nonetheless, state-of-the-art systems commonly identify a single analysis f during a preprocessing step, and decode according to the decision rule in (1). In this paper, we go beyond speech translation"
P08-1115,W04-3250,0,0.288436,"Missing"
P08-1115,2000.eamt-1.5,0,0.135111,"Missing"
P08-1115,P02-1038,0,0.179596,"Missing"
P08-1115,2005.iwslt-1.18,0,0.522209,"o the approach we have taken. Mathias and Byrne (2006) build a phrase-based translation system as a cascaded series of FSTs which can accept any input FSA; however, the only reordering that is permitted is the swapping of two adjacent phrases. Applications of source lattices outside of the domain of spoken language translation have been far more limited. Costa-juss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs.lexRo MT05 BLEU 0.2833 0.2905 0.2894 0.2938 0.2993 0.3072 MT06 BLEU 0.2694 0.2835 0.2801 0.2870 0.2865 0.2992 (S"
P08-1115,E06-1006,0,0.0617151,"ss`a and Fonollosa (2007) take steps in this direction by using lattices to encode multiple reorderings of the source language. Dyer (2007) uses confusion networks to encode morphological alternatives in Czech-English translation, and Xu et al. (2005) takes an approach very similar to ours for Chinese-English translation and encodes multiple word segmentations in a lattice, but which is decoded with a conventionally trained translation model and without a sophisticated reordering model. The Arabic-English morphological segmentation lattices are similar in spirit to backoff translation models (Yang and Kirchhoff, 2006), which consider alternative morphological segmentations and simpli(Source Type) cs hs ss hs+ss hs+ss+cs hs+ss+cs.lexRo MT05 BLEU 0.2833 0.2905 0.2894 0.2938 0.2993 0.3072 MT06 BLEU 0.2694 0.2835 0.2801 0.2870 0.2865 0.2992 (Source Type) cs hs ss hs+ss hs+ss+cs (a) Phrase-based model MT05 BLEU 0.2904 0.3008 0.3071 0.3132 0.3176 MT06 BLEU 0.2821 0.2907 0.2964 0.3006 0.3043 (b) Hierarchical model Table 4: Chinese Word Segmentation Results (Source Type) surface morph morph+surface MT05 BLEU 0.4682 0.5087 0.5225 MT06 BLEU 0.3512 0.3841 0.4008 (Source Type) surface morph morph+surface (a) Phrase-ba"
P08-1115,2005.iwslt-1.2,0,0.0659287,"Missing"
P08-1115,zhang-etal-2004-interpreting,0,\N,Missing
P08-1115,I05-3027,0,\N,Missing
P09-1037,D08-1024,1,0.851317,"o hierdriven syntactic modeling, we address this archical phrases that violate syntactic boundaries problem by observing the inuential role in the source language, and he explored the use of function words in determining syntacof a constituent feature intended to reward the tic structure, and introducing soft conapplication of hierarchical phrases which respect straints on function word relationships as source language syntactic categories. part of a standard log-linear hierarchithis did not yield signicant improvements, Marcal phrase-based model. Experimentation ton and Resnik (2008) and Chiang et al. (2008) on Chinese-English and Arabic-English extended this approach by introducing soft syntranslation demonstrates that the approach tactic constraints similar to the constituent feature, yields signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in stati"
P09-1037,P08-1066,0,0.16649,"Missing"
P09-1037,J07-2003,0,0.343694,"gnicant improvements, Marcal phrase-based model. Experimentation ton and Resnik (2008) and Chiang et al. (2008) on Chinese-English and Arabic-English extended this approach by introducing soft syntranslation demonstrates that the approach tactic constraints similar to the constituent feature, yields signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in statistical machine translation (SMT), while syntactic boundaries in the target language synmaintaining the strengths of phrase-based systems tax. Whether the focus is on constraints from the (Koehn et al., 2003). The most important of these source language or the target language, the main is the ability to model long-distance reordering efingredient in both previous approaches is the idea ciently. of constraining the spans of hierarchical phrases to To model such a reordering, a hierarrespect syntac"
P09-1037,N03-1017,0,0.133033,"Missing"
P09-1037,2003.mtsummit-papers.51,0,0.0330186,"s signicant gains in performance. but more ne-grained and sensitive to distinctions Although among syntactic categories; these led to substanIntroduction tial improvements in performance. Zollman et al. Hierarchical phrase-based models (Chiang, 2005; (2006) took a complementary approach, constrainChiang, 2007) offer a number of attractive beneing the application of hierarchical rules to respect ts in statistical machine translation (SMT), while syntactic boundaries in the target language synmaintaining the strengths of phrase-based systems tax. Whether the focus is on constraints from the (Koehn et al., 2003). The most important of these source language or the target language, the main is the ability to model long-distance reordering efingredient in both previous approaches is the idea ciently. of constraining the spans of hierarchical phrases to To model such a reordering, a hierarrespect syntactic boundaries. chical phrase-based system demands no additional parameters, since long and short distance reorderIn this paper, we pursue a different approach ings are modeled identically using synchronous to improving reordering choices in a hierarchical context free grammar (SCFG) rules. The same phras"
P09-1037,P02-1038,0,0.163241,"model, we used a 5- the baseline, but without further improvements gram model with modied Kneser-Ney smoothing over (Kneser and Ney, 1995) trained on the English side Arabic-to-English experiments. of our training data as well as portions of the Gigamarizes the results of our Arabic-to-English exword v2 English corpus. We used the NIST MT03 periments. This set of experiments shows a pattest set as the development set for optimizing intertern consistent with what we observed in Chinesepolation weights using minimum error rate trainto-English translation, again generally consistent ing (MERT; (Och and Ney, 2002)). We carried out across MT06 and MT08 test sets although modLarger values of N N = 128. Table 3 sumevaluation of the systems on the NIST 2006 evaleling a small number of lexical items (N uation test (MT06) and the NIST 2008 evaluation brings a marginal improvement over the baseline. test (MT08). We segmented Chinese as a preproIn addition, we again nd that the pairwise domcessing step using the Harbin segmenter (Zhao et inance model with al., 2001). signicant gain over the baseline in the MT06, Arabic-to-English experiments. We trained N = 128 = 32) produces the most although, interestingly"
P09-1037,J04-4002,0,0.110896,"r our running example. span of the hierarchical phrases, and (2) the span of a hierarchical phrase at a higher level is al6 ways a superset of the span of all other hierarchical Experimental Setup phrases at the lower level of its substructure. Thus, We tested the effect of introducing the pairwise to establish soft estimates of dominance counts, dominance model into hierarchical phrase-based we utilize alignment information available in the translation on Chinese-to-English and Arabic-torule together with the consistent alignment heurisEnglish translation tasks, thus studying its effect tic (Och and Ney, 2004) traditionally used to guess in two languages where the use of function words phrase alignments. differs signicantly. Following Setiawan et al. (2007), we identify function words as the Specically, we dene the span of a function N most word as a maximal, consistent alignment in the frequent words in the corpus, rather than identifysource language that either starts from or ends ing them according to linguistic criteria; this apwith the function word. (Requiring that spans be proximation removes the need for any additional maximal ensures their uniqueness.) language-specic resources. We wil"
P09-1037,E09-1044,0,\N,Missing
P09-1037,P02-1040,0,\N,Missing
P09-1037,W08-0403,0,\N,Missing
P09-1037,P08-1114,1,\N,Missing
P09-1037,P05-1033,0,\N,Missing
P09-1037,2008.iwslt-papers.7,0,\N,Missing
P09-1037,J97-3002,0,\N,Missing
P09-1037,P07-1090,1,\N,Missing
P09-1037,W06-3119,0,\N,Missing
P09-1037,W04-3250,0,\N,Missing
P10-4002,W05-1506,0,0.0175505,"mentation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Tab"
P10-4002,P07-1019,0,0.10005,"and phrase-based models, these are strictly arranged in a monotone, leftbranching structure. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). 4 5 Rescoring with weighted FSTs Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ⊕ and ⊗, and their identities 0 and 1. Multiplication and addition must be associative. Multiplication must distribute over addition, and v ⊗ 0 The design of cdec separates the creation of a transl"
P10-4002,W06-3601,0,0.0172305,"s avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 20"
P10-4002,N03-1017,0,0.465271,", the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization"
P10-4002,P07-2045,1,0.0302608,"versity of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then"
P10-4002,P09-1019,1,0.765356,"ng et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLE"
P10-4002,D07-1104,1,0.804043,"ms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to"
P10-4002,D09-1005,0,0.00908811,"oal a 1 shell 100 a 1 2 1 little 101 1 little 1 small Goal ll se hou 1 small small she sma NN little ll JJ 010 110 1 a little 1 house 1 shell Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines (small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distortion limit of 1 (right). must equal 0. Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring"
P10-4002,W09-0424,1,0.545644,"alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unif"
P10-4002,D08-1023,1,0.337071,"T03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identi"
P10-4002,P08-1024,1,0.647817,"nd alignment algorithms then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, th"
P10-4002,E09-1061,1,0.355868,"red forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice). The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source"
P10-4002,J03-1006,0,0.00679634,"a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single"
P10-4002,J93-2003,0,0.0420135,"st translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative tra"
P10-4002,P03-1021,0,0.088916,"d Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply usin"
P10-4002,P02-1040,0,0.103814,"whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure"
P10-4002,N09-1025,0,0.0233546,"s then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly sep"
P10-4002,J07-2003,0,0.969957,"ation techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparabl"
P10-4002,N03-1028,0,0.299229,"ities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on p"
P10-4002,2006.amta-papers.25,0,0.0163455,"of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec conf"
P10-4002,N10-1128,1,0.770622,"nce pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-spec"
P10-4002,P08-1115,1,0.407306,"entation of all the derivations of the sentence pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kin"
P10-4002,N10-1033,1,\N,Missing
P12-1009,D08-1035,0,0.207999,"Missing"
P12-1009,D10-1124,0,0.0783457,"Missing"
P12-1009,P03-1071,0,0.207722,"Missing"
P12-1009,W06-2914,0,0.03896,"Missing"
P12-1009,J97-1003,0,0.819786,"Missing"
P12-1009,E06-1035,0,0.0389378,"Missing"
P12-1009,P10-1117,0,0.0601251,"Missing"
P12-1009,J91-1002,0,0.112383,"Missing"
P12-1009,H05-1122,0,0.0466397,"Missing"
P12-1009,J02-1002,0,0.0809409,"Missing"
P12-1009,P06-1003,0,0.138311,"Missing"
P12-1009,W06-1639,0,0.264531,"Missing"
P12-2023,D11-1033,0,0.0420528,"hrase entirely. In a food related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) e"
P12-2023,D10-1005,1,0.141903,"conditioned on soft domain membership directly into our model is an effective strategy for dynamically biasing SMT towards relevant translations, as evidenced by significant performance gains. This method presents several advantages over existing approaches. We can construct a topic model once on the training data, and use it infer topics on any test set to adapt the translation model. We can also incorporate large quantities of additional data (whether parallel or not) in the source language to infer better topics without relying on collection or genre annotations. Multilingual topic models (Boyd-Graber and Resnik, 2010) would provide a technique to use data from multiple languages to ensure consistent topics. Acknowledgments Vladimir Eidelman is supported by a National Defense Science and Engineering Graduate Fellowship. This work was also supported in part by NSF grant #1018625, ARL Cooperative Agreement W911NF-09-2-0072, and by the BOLT and GALE programs of the Defense Advanced Research Projects Agency, Contracts HR0011-12-C-0015 and HR0011-06-2-001, respectively. Any opinions, findings, conclusions, or recommendations expressed are the authors’ and do not necessarily reflect those of the sponsors. Referen"
P12-2023,P96-1041,0,0.243101,"and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a C"
P12-2023,P11-2080,0,0.531052,"(Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection features by computing a separate word translation table for each feature, estimated from only those sentences that comprise that genre or collection. The common thread throughout prior work is the concept of a domain. A domain is typically a hard constraint that is externally imposed and hand labeled, such as genre or corpus collection. For example, a sentence either comes from newswire, or weblog, but not both."
P12-2023,P10-4002,1,0.181997,"Missing"
P12-2023,W12-3160,1,0.806631,"the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate the effect topic granularity would have on translation, we varied the number of latent topics in each model to be 5, 10,"
P12-2023,D10-1044,0,0.354617,"ood related context, the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notio"
P12-2023,N03-1017,0,0.141356,"robabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize their weights on an arbitrary objective function. Incorporating these features into our hierarchical phrase-based translation system significantly improved translation performance, by up to 1 BLEU and 3 TER over a strong Chinese to English baseline. 2 Model Description Lexical Weighting Lexical weighting features estimate the quality of a phrase pair by combining the lexical translation probabilities of the words in the phrase2 (Koehn et al., 2003). Lexical conditional probabilities p(e|f ) are obtained with maximum likelihood estimates from relative frequencies 2 For hierarchical systems, these correspond to translation rules. 116 P c(f, e)/ e c(f, e) . Phrase pair probabilities p(e|f ) are computed from these as described in Koehn et al. (2003). Chiang et al. (2011) showed that is it beneficial to condition the lexical weighting features on provenance by assigning each sentence pair a set of features, fs (e|f ), one for each domain s, which compute a new word translation table ps (e|f ) estimated from P only those sentences which belo"
P12-2023,D09-1074,0,0.482162,"tion, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on provenance (i.e., the origin of the text) by removing the separate mapping step, directly optimizing the weight of the genre and collection"
P12-2023,J03-1002,0,0.00397406,"ights toward having topic information be useful, not toward a specific distribution. 3 Experiments Setup To evaluate our approach, we performed experiments on Chinese to English MT in two settings. First, we use the FBIS corpus as our training bitext. Since FBIS has document delineations, we compare local topic modeling (LTM) with modeling at the document level (GTM). The second setting uses the non-UN and non-HK Hansards portions of the NIST training corpora with LTM only. Table 1 summarizes the data statistics. For both settings, the data were lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). The Chinese data were segmented using the Stanford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS"
P12-2023,P02-1040,0,0.107979,"nford segmenter. We trained a trigram LM on the English side of the corpus with an additional 150M words randomly selected from the nonNYT and non-LAT portions of the Gigaword v4 corpus using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 4 By having as many topics as genres/collections and setting p(zn |di ) to 1 for every sentence in the collection and 0 to everything else. Corpus Sentences FBIS NIST 269K 1.6M Tokens En Zh 10.3M 7.9M 44.4M 40.4M Model Table 1: Corpus statistics 2010) as our decoder, and tuned the parameters of the system to optimize BLEU (Papineni et al., 2002) on the NIST MT06 tuning corpus using the Margin Infused Relaxed Algorithm (MIRA) (Crammer et al., 2006; Eidelman, 2012). Topic modeling was performed with Mallet (Mccallum, 2002), a standard implementation of LDA, using a Chinese stoplist and setting the per-document Dirichlet parameter α = 0.01. This setting of was chosen to encourage sparse topic assignments, which make induced subdomains consistent within a document. Results Results for both settings are shown in Table 2. GTM models the latent topics at the document level, while LTM models each sentence as a separate document. To evaluate"
P12-2023,D08-1090,0,0.0126658,"the Chinese sentence “粉丝很多 ” (“fˇens¯i hˇendu¯o”) would mean “They have a lot of vermicelli”; however, in an informal Internet conversation, this sentence would mean “They have a lot of fans”. Without the broader context, it is impossible to determine the correct translation in otherwise identical sentences. Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu This problem has led to a substantial amount of recent work in trying to bias, or adapt, the translation model (TM) toward particular domains of interest (Axelrod et al., 2011; Foster et al., 2010; Snover et al., 2008).1 The intuition behind TM adaptation is to increase the likelihood of selecting relevant phrases for translation. Matsoukas et al. (2009) introduced assigning a pair of binary features to each training sentence, indicating sentences’ genre and collection as a way to capture domains. They then learn a mapping from these features to sentence weights, use the sentence weights to bias the model probability estimates and subsequently learn the model weights. As sentence weights were found to be most beneficial for lexical weighting, Chiang et al. (2011) extends the same notion of conditioning on p"
P12-2023,P06-2124,0,0.396164,"nslation model. Unsupervised modeling of the training data produces naturally occurring subcorpora, generalizing beyond corpus and genre. Depending on the model used to select subcorpora, we can bias our translation toward any arbitrary distinction. This reduces the problem to identifying what automatically defined subsets of the training corpus may be beneficial for translation. In this work, we consider the underlying latent topics of the documents (Blei et al., 2003). Topic modeling has received some use in SMT, for instance Bilingual LSA adaptation (Tam et al., 2007), and the BiTAM model (Zhao and Xing, 2006), which uses a bilingual topic model for learning alignment. In our case, by building a topic distribution for the source side of the training data, we abstract the notion of domain to include automatically derived subcorpora with probabilistic membership. This topic model infers the topic distribution of a test set and biases sentence translations to appropriate topics. We accomplish this by introducing topic dependent lexical probabilities directly as features in the translation model, and interpolating them log-linearly with our other features, thus allowing us to discriminatively optimize"
P13-1110,P08-1024,0,0.594002,"and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting t"
P13-1110,P96-1041,0,0.162623,"n-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize B LEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and R AMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for R AMPION, MIRA and RM, and 1500 for PRO, with both PRO and R AMPION utilizing k-best aggregation across iterations. R AMPION settings"
P13-1110,N12-1047,0,0.296111,"ate-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increas"
P13-1110,D08-1024,1,0.883321,"ER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, gene"
P13-1110,N09-1025,0,0.116502,"Missing"
P13-1110,P10-4002,1,0.901832,"ne gradient-based optimization approach—an approach that is particularly attractive because its simple update is well suited for efficiently processing structured objects with sparse features (Crammer et al., 2012). The contributions of this paper include (1) introduction of a loss function for structured RMM in the SMT setting, with surrogate reference translations and latent variables; (2) an online gradientbased solver, RM, with a closed-form parameter update to optimize the relative margin loss; and (3) an efficient implementation that integrates well with the open source cdec SMT system (Dyer et al., 2010).1 In addition, (4) as our solution is not dependent on any specific QP solver, it can be easily incorporated into practically any gradientbased learning algorithm. After background discussion on learning in SMT (§2), we introduce a novel online learning algorithm for relative margin maximization suitable for SMT (§3). First, we introduce RMM (§3.1) and propose a latent structured relative margin objective which incorporates cost-augmented hypothesis selection and latent variables. Then, we derive a simple closed-form online update necessary to create a large margin solution while simultaneous"
P13-1110,W12-3160,1,0.764838,"an external measure of quality, such as 1-B LEU(yi , y), and a slack variable ξi is introduced to allow for non-separable instances. C acts as a regularization parameter, trading off between margin maximization and constraint violations. While solving the optimization problem relies on computing the margin between the correct output yi , and y 0 , in SMT our decoder is often incapable of producing the reference translation, i.e. yi ∈ / Y(xi ). We must instead resort to selecting a surrogate reference, y + ∈ Y(xi ). This issue has recently received considerable attention (Liang et al., 2006a; Eidelman, 2012; Chiang, 2012), with preference given to surrogate references obtained through cost-diminished hypothesis selection. Thus, y + is selected based on a combination of model score and error metric from the k-best list produced by our current model. A similar selection is made for the cost-augmented hypothesis y − ∈ Y(xi ): (y + , d+ ) ← (y,d)∈Y(xi ),D(xi ) s(xi , y, d) − ∆i (y) (y − , d− ) ← arg max s(xi , y, d) + ∆i (y) (y,d)∈Y(xi ),D(xi ) arg max In this setting, the optimization problem becomes: 1 wt+1 = arg min ||w − wt ||2 + Cξi 2 w (2) s.t. δs(xi , y + , y − ) ≥ ∆i (y − ) − ∆i (y + ) − ξi"
P13-1110,W09-0439,0,0.0932951,": (y ∗ , d∗ ) = arg max w&gt; f (x, y, d) (y,d)∈Y(x),D(x) where w&gt; f (x, y, d) is the weighted feature scoring function, hereafter s(x, y, d), and Y(x) is the space of possible translations of x. While many derivations d ∈ D(x) can produce a given translation, we are only able to observe y; thus we model d as a latent variable. Although our models are actually defined over derivations, they are always paired with translations, so our feature function f (x, y, d) is defined over derivation–translation pairs.2 The learning goal is then to estimate w. The instability of MERT in larger feature sets (Foster and Kuhn, 2009; Hopkins and May, 2011), has motivated many alternative tuning methods for SMT. These include strategies based on batch log-linear models (Tillmann and Zhang, 2006; Blunsom et al., 2008), as well as the introduction of online linear models (Liang et al., 2006a; Arun and Koehn, 2007). Recent batch optimizers, PRO and R AMPION, and Batch-MIRA (Cherry and Foster, 2012), have been partly motivated by existing MT infrastructures, as they iterate between decoding the entire tuning set and optimizing the parameters. PRO considers tuning a classification problem and employs a binary classifier to ran"
P13-1110,N12-1023,0,0.819932,"e feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield goo"
P13-1110,D11-1125,0,0.529868,"on tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes"
P13-1110,N03-1017,0,0.0135688,"e the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two Chinese-English translation test sets, using two different feature sets in each. For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize B LEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011)"
P13-1110,P09-1019,0,0.118217,"to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize B LEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and R AMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for R AMPION, MIRA and RM, and 1500 for PRO, with both PRO and R AMPION utilizing k-best aggregation across iterations. R AMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order to be competitive with the other optimizers. MIRA and RM were run with 15 parallel learners using iterative parameter mixing"
P13-1110,P03-1051,0,0.0268701,"st B LEU scores, but worse TER. In preliminary experiments with a smaller trigram LM, our RM method consistently yielded the highest scores in all Chinese-English tests – up to 1.6 B LEU and 6.4 TER from MIRA, the second best performer. Additional Experiments In order to explore the applicability of our approach to a wider range of languages, we also evaluated its performance on Arabic-English translation. All experimental details were the same as above, except those noted below. For training, we used the non-UN portion of the NIST training corpora, which was segmented using an HMM segmenter (Lee et al., 2003). Dataset statistics are given in the bottom part of Table 1. The sparse feature templates resulted here in a total of 4.9 million possible features, of which again only a fraction were active, as shown in Table 2. As can be seen in Table 4, in the smaller feature set, RM and MERT were the best performers, with the exception that on MT08, MIRA yielded somewhat better (+0.7) B LEU but a somewhat worse (-0.9) TER score than RM. On the large feature set, RM is again the best performer, except, perhaps, a tied B LEU score with MIRA on MT08, but with a clear 1.8 TER gain. In both Arabic-English fea"
P13-1110,P06-1096,0,0.188295,"Missing"
P13-1110,J03-1002,0,0.00621646,"he upsidedown triangle, and update so the distance from y + is no greater than B. 4 4.1 Experiments Setup To evaluate the advantage of explicitly accounting for the spread of the data, we conducted several experiments on two Chinese-English translation test sets, using two different feature sets in each. For training we used the non-UN and non-HK Hansards portions of the NIST training corpora, which was segmented using the Stanford segmenter (Tseng et al., 2005). The data statistics are summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize B LEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT ("
P13-1110,P03-1021,0,0.0397691,"gin maximization, which bounds the spread of the projected data while maximizing the margin. We evaluate our optimizer on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in"
P13-1110,D07-1080,0,0.288646,"2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature"
P13-1110,P02-1040,0,0.0879598,"summarized in the top half of Table 1. The English data was lowercased, tokenized and aligned using GIZA++ (Och and Ney, 2003) to obtain bidirectional alignments, which were symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 4-gram LM on the English side of the corpus with additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We used cdec (Dyer et al., 2010) as our hierarchical phrase-based decoder, and tuned the parameters of the system to optimize B LEU (Papineni et al., 2002) on the NIST MT06 corpus. We applied several competitive optimizers as baselines: hypergraph-based MERT (Kumar et al., 2009), k-best variants of MIRA (Crammer et al., 2006; Chiang et al., 2009), PRO (Hopkins and May, 2011), and R AMPION (Gimpel and Smith, 2012). The size of the k-best list was set to 500 for R AMPION, MIRA and RM, and 1500 for PRO, with both PRO and R AMPION utilizing k-best aggregation across iterations. R AMPION settings were as described in (Gimpel and Smith, 2012), and PRO settings as described in (Hopkins and May, 2011), with PRO requiring regularization tuning in order t"
P13-1110,P12-1002,0,0.671725,"of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost incurred by selecting the incorrect one. This c"
P13-1110,P06-2101,0,0.0879711,"er on Chinese-English and ArabicEnglish translation tasks, each with small and large feature sets, and show that our learner is able to achieve significant improvements of 1.2-2 B LEU and 1.7-4.3 TER on average over state-of-the-art optimizers with the large feature set. 1 Introduction The desire to incorporate high-dimensional sparse feature representations into statistical machine translation (SMT) models has driven recent research away from Minimum Error Rate Training (MERT) (Och, 2003), and toward other discriminative methods that can optimize more features. Examples include minimum risk (Smith and Eisner, 2006), pairwise ranking (PRO) (Hopkins and May, 2011), R AMPION (Gimpel and Smith, 2012), and variations of the margin-infused relaxation algorithm (MIRA) (Watanabe et al., 2007; Chiang et al., 2008; Cherry and Foster, 2012). While the objective function and optimization method vary for each optimizer, they can all be broadly described as learning a linear model, or parameter vector w, which is used to score alternative translation hypotheses. In every SMT system, and in machine learning in general, the goal of learning is to find a Philip Resnik Linguistics and UMIACS University of Maryland Colleg"
P13-1110,P06-1091,0,0.71176,"Philip Resnik Linguistics and UMIACS University of Maryland College Park, MD resnik@umd.edu model that generalizes well, i.e. one that will yield good translations for previously unseen sentences. However, as the dimension of the feature space increases, generalization becomes increasingly difficult. Since only a small portion of all (sparse) features may be observed in a relatively small fixed set of instances during tuning, we are prone to overfit the training data. An alternative approach for solving this problem is estimating discriminative feature weights directly on the training bitext (Tillmann and Zhang, 2006; Blunsom et al., 2008; Simianer et al., 2012), which is usually substantially larger than the tuning set, but this is complementary to our goal here of better generalization given a fixed size tuning set. In order to achieve that goal, we need to carefully choose what objective to optimize, and how to perform parameter estimation of w for this objective. We focus on large-margin methods such as SVM (Joachims, 1998) and passive-aggressive algorithms such as MIRA. Intuitively these seek a w such that the separating distance in geometric space of two hypotheses is at least as large as the cost i"
P13-1110,N10-1069,0,\N,Missing
P13-1110,I05-3027,0,\N,Missing
P13-4034,P96-1041,0,0.0297478,"cted and used as the initial weights for the next iteration; the emitted hypotheses are scored 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 5 By default, each line is treated as a key-value pair encoded in text, where the key and the value are separated by a &lt;tab&gt;. 6 202 http://www.statmt.org/wmt12/translation-task.html Tuning set size (corpus) (on disk, GB) dev 3.3 5k 7.8 10k 15.2 25k 37.2 50k 74.5 dev 3.3 5k 7.8 10k 15.2 25k 37.2 50k 74.5 Time/iteration (in seconds) 119 289 432 942 1802 232 610 1136 2395 4465 # splits # features Tuning B LEU 120 120 120 300 600 120 120 120 300 600 16 16 16 16 16 85k 159k 200k 200k 200k 22.38 32.60 33.16"
P13-4034,N09-1025,0,0.445855,"ue to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin structured learning method for NLP tasks (McDonald et al., 2005; Chiang et al., 2009; Chiang, 2012). The 1 https://github.com/kho/mr-mira 199 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199–204, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics main intuition is that we want our model to enforce a margin between the correct and incorrect outputs of a sentence that agrees with our cost function. This is done by making the smallest update we can to our parameters, w, on every sentence, that will ensure that the difference in model scores δfi (y 0 ) = w&gt; (f (xi , y + ) − f (xi , y 0 )) between th"
P13-4034,P10-4002,1,0.895476,"×n i ∗ i aged is defined as w = P ni i . Although averi aging yields different result from running a single learner over the entire data, we have found the difference to be quite small in terms of convergence and quality of tuned weights in practice. After the reducer finishes, the averaged weights are extracted and used as the initial weights for the next iteration; the emitted hypotheses are scored 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 5 By default, each line is treated as a key-value pair encoded in text, where the key and the value are separated by a &lt;tab&gt;. 6 202 http://www.statmt.org/wmt12/translation-task.html Tuning set"
P13-4034,P13-1031,0,0.046656,"orate `1 /`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). 1 wt+1 = arg min ||w − wt ||2 + Cξi w 2 0 s.t. ∀y ∈ Y(xi ), δfi (y 0 ) ≥ ∆i (y 0 ) − ξi where Y(xi ) is the space of possible structured outputs we are able to produce from xi , and C is a regularization parameter that controls the size of the update. In practice, we can define Y(xi ) to be the k-best output. With a passive-aggressive (PA) update, the ∀y 0 constraint above can be approximated by selecting the single most violated constraint, which maximizes y 0 ← arg maxy∈Y(xi ) w&gt; f (xi , y) + ∆i (y). This optimization problem is attractive because"
P13-4034,W12-3154,0,0.0153843,"in Table 2, in order to test the effectiveness and scalability of our system. First, we used the standard development set from WMT12, consisting of 3,003 sentences from news domain. In order to show the scaling characteristics of our approach, we then used larger portions of the training bitext directly to tune parameters. In order to avoid overfitting, we used a jackknifing method to split the training data into n = 10 folds, and 203 Acknowledgments Therefore, we are actually comparing a smaller indomain tuning set with a larger out-of-domain set. While this domain adaptation is problematic (Haddow and Koehn, 2012), the ability to discriminatively tune on larger sets remains highly desirable. In terms of running time, we observe that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances."
P13-4034,W11-2130,0,0.0169494,"that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances. Comparing the wall clock time of parallelization with Hadoop to the standard mode of 10–20 learner parallelization (Haddow et al., 2011; Chiang et al., 2009), for the small 25k feature setting, after one iteration, which takes 4625 seconds using 15 learners on our PBS cluster, the tuning score is 19.5 B LEU, while in approximately the same time, we can perform five iterations with Hadoop and obtain 30.98 B LEU. While this is not a completely fair comparison, as the two clusters utilize different resources and the number of learners, it suggests the practical benefits that Hadoop can provide. Although increasing the number of learners on our PBS cluster to the number of mappers used in Hadoop would result in roughly equivalent"
P13-4034,P05-1012,0,0.0685204,"uch as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin struc"
P13-4034,P02-1040,0,0.0871422,"lization of MIRA with Hadoop for structured learning. While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed. We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol (§2.2). The learner only requires k-best output with feature vectors, as well as the specification of a cost function. Standard automatic evaluation metrics for MT, such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006), have already been implemented. Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning se"
P13-4034,P07-1096,0,0.02368,"ed learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a"
P13-4034,P12-1002,0,0.047525,"odel can produce, to stand in for the correct output in optimization. Our system was developed to handle both cases, with the decoder providing the k-best list to the learner, specifying whether to perform costaugmented selection. Sparse Features While utilizing sparse features is a primary motivation for performing large-scale discriminative training, which features to use and how to learn their weights can have a large impact on the potential benefit. To this end, we incorporate `1 /`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). 1 wt+1 = arg min ||w − wt ||2 + Cξi w 2 0 s.t. ∀y ∈ Y(xi ), δfi (y 0 ) ≥ ∆i (y"
P13-4034,2006.amta-papers.25,0,0.0178616,"structured learning. While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed. We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol (§2.2). The learner only requires k-best output with feature vectors, as well as the specification of a cost function. Standard automatic evaluation metrics for MT, such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006), have already been implemented. Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning sets for SMT practical. We prese"
P13-4034,P02-1062,0,\N,Missing
P14-1105,D10-1111,0,0.0321643,"campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intrasentential contextual influences of the kind seen in Figure 1. Other approaches on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently,"
P14-1105,J92-4003,0,0.211075,"th liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing a list of the top 100 sticky bigrams for each category, ranked by loglikelihood ratio, and selecting another subset from the original data that included only sentences containing at least one sticky bigram, we take the union of the two subsets. Finally, we balance the resulting dataset s"
P14-1105,P13-1162,0,0.282011,"; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, Mexico, housework, speculated, consensus, lawyer, pharmaceuticals, ruthless, deadly, Clinton, redistribution 3 prize individual li"
P14-1105,N12-1085,1,0.308395,"ences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s author, where position is either liberal or conservative.5 We used the Crowdflower crowdsourcing platform (crowdflower.com), which has previously been used for subsentential sentiment annotation (Sayeed et al., 2012), to obtain human annotations of the filtered IBC dataset for political bias on both the sentence and phrase level. While members of the Crowdflower workforce are certainly not experts in political science, our simple task and the ubiquity of political bias allows us to acquire useful annotations. Crowdflower Task First, we parse the filtered IBC sentences using the Stanford constituency parser (Socher et al., 2013a). Because of the expense of labeling every node in a sentence, we only label one path in each sentence. The process for selecting paths is as follows: first, if any paths contain o"
P14-1105,D11-1136,0,0.00970619,"also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned class labels of “neutral” or “biased” to 200 sentences, and selected typical partisan unigrams to represent the “biased” class. DUALIST labels 11,555 sentences as politically biased, 5,434 of which come from conservative authors and 6,121 of which come from liberal authors. 3.2.1 Annotating the IBC For purposes of annotation, we define the task of political ideology detection as identifying, if possible, the political position of a given sentence’s auth"
P14-1105,D13-1010,0,0.0532505,"the liberal-to-conservative switch. In D, negation confuses our model. the sake of simplicity. For example, Gentzkow and Shapiro (2010) derive a “slant index” to rate the ideological leaning of newspapers. A newspaper’s slant index is governed by the frequency of use of partisan collocations of 2-3 tokens. Similarly, authors have relied on simple models of language when leveraging inferred ideological positions. E.g., Gerrish and Blei (2011) predict the voting patterns of Congress members based on bagof-words representations of bills and inferred political leanings of those members. Recently, Sim et al. (2013) have proposed a model to infer mixtures of ideological positions in documents, applied to understanding the evolution of ideological rhetoric used by political candidates during the campaign cycle. They use an HMM -based model, defining the states as a set of fine-grained political ideologies, and rely on a closed set of lexical bigram features associated with each ideology, inferred from a manually labeled ideological books corpus. Although it takes elements of discourse structure into account (capturing the“burstiness” of ideological terminology usage), their model explicitly ignores intras"
P14-1105,N09-1057,1,0.545424,"sentences in the IBC, most of which have no noticeable political bias. Therefore we use the filtering procedure outlined in Section 3.1.1 to obtain a subset of 55,932 sentences. Compared to our final Convote dataset, an even larger percentage of the IBC sentences exhibit no noticeable political bias.4 Because our goal is to distinguish between liberal and conservative 2 Many sentences in Convote are variations on “I think this is a good/bad bill”, and there is also substantial parliamentary boilerplate language. 3 While Kill verbs are not a category in LIWC, Yano et al. (2010) adopted it from Greene and Resnik (2009) and showed it to be a useful predictor of political bias. It includes words such as “slaughter” and “starve”. 4 This difference can be mainly attributed to a historical topics in the IBC (e.g., the Crusades, American Civil War). In Convote, every sentence is part of a debate about 2005 political policy. bias, instead of the more general task of classifying sentences as “neutral” or “biased”, we filter the dataset further using DUALIST (Settles, 2011), an active learning tool, to reduce the proportion of neutral sentences in our dataset. To train the DUALIST classifier, we manually assigned cl"
P14-1105,D11-1014,0,0.786987,"e sentence in Figure 1 includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here"
P14-1105,D13-1137,0,0.00652278,"data; the discrepancy between categorical predictions and annotations is measured 1114 through the cross-entropy loss. We optimize the model parameters to minimize the cross-entropy loss over all sentences in the corpus. The crossentropy loss of a single sentence is the sum over the true labels yi in the sentence, `(ˆ ys ) = k X yp ∗ log(ˆ yp ). (4) p=1 This induces a supervised objective function over all sentences: a regularized sum over all node losses normalized by the number of nodes N in the training set, C= 1 N N X `(predi ) + i λ kθk2 . 2 (5) We use L - BFGS with parameter averaging (Hashimoto et al., 2013) to optimize the model parameters θ = (WL , WR , Wcat , We , b1 , b2 ). The gradient of the objective, shown in Eq. (6), is computed using backpropagation through structure (Goller and Kuchler, 1996), N ∂C 1 X ∂`(ˆ yi ) = + λθ. ∂θ N ∂θ (6) i 2.2 Initialization When initializing our model, we have two choices: we can initialize all of our parameters randomly or provide the model some prior knowledge. As we see in Section 4, these choices have a significant effect on final performance. Random The most straightforward choice is to initialize the word embedding matrix We and composition matrices W"
P14-1105,P13-1088,0,0.0090081,"ove further when given additional phrase-level annotations. RNNs are quantitatively more effective than existing methods that use syntactic and semantic features separately, and we also illustrate how our model correctly identifies ideological bias in complex syntactic constructions. 2 Recursive Neural Networks Recursive neural networks (RNNs) are machine learning models that capture syntactic and semantic composition. They have achieved state-of-the-art performance on a variety of sentence-level NLP tasks, including sentiment analysis, paraphrase detection, and parsing (Socher et al., 2011a; Hermann and Blunsom, 2013). RNN models represent a shift from previous research on ideological bias detection in that they do not rely on hand-made lexicons, dictionaries, or rule sets. In this section, we describe a supervised RNN model for bias detection and highlight differences from previous work in training procedure and initialization. 2.1 ological bias becomes identifiable only at higher levels of sentence trees (as verified by our annotation, Figure 4), models relying primarily on wordlevel distributional statistics are not desirable for our problem. The basic idea behind the standard RNN model is that each wor"
P14-1105,P13-1045,0,0.411506,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,D13-1170,0,0.148607,"includes phrases typically associated with conservatives, such as “small businesses” and “death tax”. When we take more of the structure into account, however, we find that scare quotes and a negative propositional attitude (a lie about X) yield an evident liberal bias. Existing approaches toward bias detection have not gone far beyond “bag of words” classifiers, thus ignoring richer linguistic context of this kind and often operating at the level of whole documents. In contrast, recent work in sentiment analysis has used deep learning to discover compositional effects (Socher et al., 2011b; Socher et al., 2013b). Building from those insights, we introduce a recursive neural network (RNN) to detect ideological bias on the sentence level. This model requires 1113 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1113–1122, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics pe = so-called climate change xe= WL xd= wd = so-called WR pc = climate change xc= WL xa= wa = climate WR xb= wb = change Figure 2: An example RNN for the phrase “socalled climate change”. Two d-dimensional word vectors (here, d = 6) are composed"
P14-1105,W06-1639,0,0.0602891,"wo terms are highly correlated (e.g., a member of the Republican party likely agrees with conservative stances on most issues), they are not identical. For example, a moderate Republican might agree with the liberal position on increased gun control but take conservative positions on other issues. To avoid conflating partisanship and ideology we create a new dataset annotated for ideological bias on the sentence and phrase level. In this section we describe our initial dataset (Convote) and explain the procedure we followed for creating our new dataset (IBC).1 3.1 Convote The Convote dataset (Thomas et al., 2006) consists of US Congressional floor debate transcripts from 2005 in which all speakers have been labeled with their political party (Democrat, Republican, or independent). We propagate party labels down from the speaker to all of their individual sentences and map from party label to ideology label (Democrat → liberal, Republican → conservative). This is an expedient choice; in future work we plan to make use of work in political science characterizing candidates’ ideological positions empirically based on their behavior (Carroll et al., 2009). While the Convote dataset has seen widespread use"
P14-1105,J04-3002,0,0.0126291,"s on the document level use topic models to analyze bias in news articles, blogs, and political speeches (Ahmed and Xing, 2010; Lin et al., 2008; Nguyen et al., 2013). 6.2 Subjectivity Detection Detecting subjective language, which conveys opinion or speculation, is a related NLP problem. While sentences lacking subjective language may contain ideological bias (e.g., the topic of the sentence), highly-opinionated sentences likely have obvious ideological leanings. In addition, sentiment and subjectivity analysis offers methodological approaches that can be applied to automatic bias detection. Wiebe et al. (2004) show that low-frequency words and some collocations are a good indicators of subjectivity. More recently, Recasens et al. (2013) detect biased words in sentences using indicator features for bias cues such as hedges and factive verbs in addition to standard bag-of-words and part-of-speech features. They show that this type of linguistic information dramatically improves performance over several standard baselines. Greene and Resnik (2009) also emphasize the connection between syntactic and semantic relationships in their work on “implicit sentiment”, 1120 n 1 Most conservative n-grams Salt, M"
P14-1105,H05-1044,0,0.0342694,"“be used as an instrument to achieve charitable or social ends” reflects a liberal ideology, which the model predicts correctly. However, our model is unable to detect the polarity switch when this phrase is negated with “should not”. Since many different issues are discussed in the IBC, it is likely that our dataset has too few examples of some of these issues for the model to adequately learn the appropriate ideological positions, and more training data would resolve many of these errors. 6 Related Work A growing NLP subfield detects private states such as opinions, sentiment, and beliefs (Wilson et al., 2005; Pang and Lee, 2008) from text. In general, work in this category tends to combine traditional surface lexical modeling (e.g., bag-of-words) with hand-designed syntactic features or lexicons. Here we review the most salient literature related to the present paper. 6.1 Automatic Ideology Detection Most previous work on ideology detection ignores the syntactic structure of the language in use in favor of familiar bag-of-words representations for 1119 A B Thus , the harsh made worse by of free-market the implementing ideology conditions for farmers caused by a number of factors , , have created"
P14-1105,W10-0723,1,0.935118,"al., 2009). While the Convote dataset has seen widespread use for document-level political classification, we are unaware of similar efforts at the sentence level. 3.1.1 Biased Sentence Selection The strong correlation between US political parties and political ideologies (Democrats with liberal, Republicans with conservative) lends confidence that this dataset contains a rich mix of ideological 1115 1 Available at http://cs.umd.edu/˜miyyer/ibc statements. However, the raw Convote dataset contains a low percentage of sentences with explicit ideological bias.2 We therefore use the features in Yano et al. (2010), which correlate with political bias, to select sentences to annotate that have a higher likelihood of containing bias. Their features come from the Linguistic Inquiry and Word Count lexicon (LIWC) (Pennebaker et al., 2001), as well as from lists of “sticky bigrams” (Brown et al., 1992) strongly associated with one party or another (e.g., “illegal aliens” implies conservative, “universal healthcare” implies liberal). We first extract the subset of sentences that contains any words in the LIWC categories of Negative Emotion, Positive Emotion, Causation, Anger, and Kill verbs.3 After computing"
P14-1106,P96-1041,0,0.134806,"4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on t"
P14-1106,N13-1003,0,0.0233916,"ituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also disc"
P14-1106,D08-1024,1,0.927619,"trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resnik (2008), hereafter MR08. The syntactic soft constraint features include both MR08 exact-matching and crossboundary constraints (denoted XP= and XP+). Since the syntactic parses of the tuning and test data contain 29 types of constituent labels and 35 types of POS tags, we have 29 types of XP+ features and 64 types of XP= features. 4.2 Model Training To train the syntactic and semantic reordering models, we use a gold alignment dataset.2 It contains 7,870 sentences with 191,364 Chinese words and 261,399 English words. We first run syn1 http://www.itl.nist.gov/iad/mig//tests/mt This dataset includes LD"
P14-1106,J07-2003,0,0.490023,"rove translation quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous"
P14-1106,W13-2258,0,0.0235223,"uments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture dif"
P14-1106,I11-1005,0,0.0321974,"Missing"
P14-1106,N03-1017,0,0.0397449,"Missing"
P14-1106,W04-3250,0,0.0690769,"5.2 35.6 34.4 34.5 34.5 35.5 35.6 36.0 36.0 35.9 35.8 35.8 35.8 36.1 MT03 36.1 36.9‡ 37.2‡ 37.1‡ 36.7‡ 36.7‡ 37.0‡ 37.3‡ 37.4 38.2‡ 38.1‡ 38.2‡ 37.6† 37.4 37.6† 38.4‡ Test MT05 32.3 33.6‡ 33.7‡ 33.6‡ 33.0‡ 33.1‡ 33.6‡ 33.7‡ 34.2 35.0‡ 34.8‡ 35.3‡ 34.7‡ 34.5† 34.7‡ 35.2‡ MT08 27.4 28.4‡ 28.6‡ 28.8‡ 27.8† 27.8‡ 27.7† 29.0‡ 28.7 29.2‡ 29.2‡ 29.5‡ 28.7 28.8 28.8 29.5‡ Avg. 31.9 33.0 33.2 33.1 32.5 32.5 32.8 33.3 33.4 34.1 34.0 34.3 33.7 33.6 33.7 34.4 Table 5: System performance in BLEU scores. ‡/†: significant over baseline or MR08 at 0.01 / 0.05, respectively, as tested by bootstrap resampling (Koehn, 2004) shown in the rows of “+ sem-reorder” in Table 5. Here we observe: • The semantic reordering models also achieve significant gain of 0.8 BLEU on average over the baseline system, demonstrating the effectiveness of PAS-based reordering. However, the gain diminishes to 0.3 BLEU on the MR08 system. • The syntactic reordering models outperform the semantic reordering models on both the baseline and MR08 systems. Finally, we integrate both the syntactic and semantic reordering models into the final system. The two models collectively achieve a gain of up to 1.4 BLEU over the baseline and 1.0 BLEU o"
P14-1106,P10-1146,0,0.0294091,"are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word lev"
P14-1106,C10-1071,0,0.0186897,"vement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous"
P14-1106,P05-1066,0,0.0612232,"(e.g., 34.9 vs. 34.3). To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translati"
P14-1106,D13-1049,0,0.0157089,"mantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008)"
P14-1106,P10-4002,1,0.934219,"on quality. Rather than introducing reordering models on either the word level or the translation phrase level, we propose a unified approach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the targe"
P14-1106,P13-4034,1,0.838779,"K Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semanti"
P14-1106,P07-1091,0,0.0254148,"we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with"
P14-1106,P10-1113,1,0.882959,"Missing"
P14-1106,N13-1060,1,0.88395,"Missing"
P14-1106,P13-1032,0,0.0202659,"reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the target side, while the semantic reordering models work with PAS. Experiments on ChineseEnglish translation show that the reordering approach can significantly improve a state-of-the-art hierarchical phrase-based translation system. We have also discussed the difference"
P14-1106,C10-1081,0,0.0215185,". (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and t"
P14-1106,D11-1079,0,0.0382431,"Missing"
P14-1106,P08-1114,1,0.9248,"roach to modeling reordering on the linguistic unit level, e.g., syntactic constituents and semantic roles. The reordering unit falls into multiple granularities, from single words to more complex constituents and semantic roles, and often crosses translation phrases. To show the effectiveness of our reordering models, we integrate both syntactic constituent reordering models and semantic role reordering models into a state-ofthe-art HPB system (Chiang, 2007; Dyer et al., 2010). We further contrast it with a stronger baseline, already including fine-grained soft syntactic constraint features (Marton and Resnik, 2008; Chiang et al., 2008). The general ideas, however, are applicable to other translation models, e.g., phrase-based model, as well. Our syntactic constituent reordering model considers context free grammar (CFG) rules in the source language and predicts the reordering of their elements on the target side, using word alignment information. Due to the fact that a constituent, especially a long one, usually maps into multiple discontinuous blocks in the target language, there is more than one way to describe the monotonicity or swapping patterns; we therefore design two reordering models: one is b"
P14-1106,N10-1127,0,0.018131,"ing. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu"
P14-1106,P11-1065,0,0.0529872,"Missing"
P14-1106,C10-1043,0,0.0197319,"e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT d"
P14-1106,D13-1053,0,0.014297,"ed binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Although employing linguistically motivated labels in SCFG is capable of capturing constituent reorderings (Chiang, 2010; Mylon1130 akis and Sima’an, 2011), the rules are sparser than SCFG with nameless non-terminals (i.e., Xs) and soft constraints. Ge (2010) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation"
P14-1106,P13-1156,0,0.0178749,"the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiaw"
P14-1106,P00-1056,0,0.192489,"ic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03,"
P14-1106,J05-1004,0,0.00957919,"g model takes a CFG rule (e.g., VP → VP PP PP) and models the reordering of the constituents on the left hand side by examining their translation or visit order according to the target language. For the semantic reordering model, it takes a PAS and models its reordering on the target side. Figure 1 shows an example of a PAS where the predicate (Pre) has two core arguments (A0 and A1) and one adjunct (TMP). Note that we refer all core arguments, adjuncts, and predicates as semantic roles; thus we say the PAS in Figure 1 has 4 roles. According to the annotation principles in (Chinese) PropBank (Palmer et al., 2005; Xue and Palmer, 2009), all the roles in a PAS map to a corresponding constituent in the parse tree, and these constituents (e.g., NPs and VBD in Figure 1) do not overlap with each other. Next, we use a CFG rule to describe our syntactic reordering model. Treating the two forms of reorderings in a unified way, the semantic reordering model is obtainable by regarding a PAS as a CFG rule and considering a semantic role as a constituent. Because the translation of a source constituent might result in multiple discontinuous blocks, there can be several ways to describe or group the reordering pat"
P14-1106,P12-1095,0,0.020195,"ted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is un"
P14-1106,P02-1040,0,0.0905028,"et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word"
P14-1106,N09-1028,0,0.021861,"owever, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another ap"
P14-1106,N07-1051,0,0.0385816,"ey smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of the system. We use NIST MT 06 dataset (1664 sentence pairs) for tuning, and NIST MT 03, 05, and 08 datasets (919, 1082, and 1357 sentence pairs, respectively) for evaluation.1 We use BLEU (Papineni et al., 2002) for both tuning and evaluation. To obtain syntactic parse trees and semantic roles on the tuning and test datasets, we first parse the source sentences with the Berkeley Parser (Petrov and Klein, 2007), trained on the Chinese Treebank 7.0 (Xue et al., 2005). We then pass the parses to a Chinese semantic role labeler (Li et al., 2010), trained on the Chinese PropBank 3.0 (Xue and Palmer, 2009), to annotate semantic roles for all verbal predicates (partof-speech tag VV, VE, or VC). Our basic baseline system employs 19 basic features: a language model feature, 7 translation model features, word penalty, unknown word penalty, the glue rule, date, number and 6 passthrough features. Our stronger baseline employs, in addition, the fine-grained syntactic soft constraint features of Marton and Resni"
P14-1106,P09-1037,1,0.840115,"utput and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reordering models without syntactic information in HPB (Setiawan et al., 2009; Huck et al., 2013; Nguyen and Vogel, 2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patter"
P14-1106,P13-1124,0,0.0190784,"2013). The non-syntaxbased reordering approach models the reordering of translation words/phrases while the syntaxbased approach models the reordering of syntactic constituents. Although there are overlaps between translation phrases and syntactic constituents, it is reasonable to think that the two re4 Note that they obtained the translation order of source word pairs by predicting the reordering of adjacent constituents, which was quite close to our work. ordering approaches can work together well and even complement each other, as the linguistic patterns they capture differ substantially. Setiawan et al. (2013) modeled the orientation decisions between anchors and two neighboring multi-unit chunks which might cross phrase or rule boundaries. Last, we also note that recent work on nonsyntax-based reorderings in (flat) phrase-based models (Cherry, 2013; Feng et al., 2013) can also be potentially adopted to hpb models. 7 Conclusion and Future Work In this paper, we have presented a unified reordering framework to incorporate soft linguistic constraints (of syntactic or semantic nature) into the HPB translation model. The syntactic reordering models take CFG rules and model their reordering on the targe"
P14-1106,I05-3027,0,0.0350418,"i with a semantic role Ri . Algorithm 1 therefore permits a unified treatment of syntactic and PAS-based reordering, even though it is expressed in terms of syntactic reordering here for ease of presentation. 4 Experiments We have presented our unified approach to incorporating syntactic and semantic soft reordering constraints in an HPB system. In this section, we test its effectiveness in Chinese-English translation. 4.1 Experimental Settings For training we use 1.6M sentence pairs of the non-UN and non-HK Hansards portions of NIST MT training corpora, segmented with the Stanford segmenter (Tseng et al., 2005). The English data is lowercased, tokenized and aligned with GIZA++ (Och and Ney, 2000) to obtain bidirectional alignments, which are symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We train a 4-gram LM on the English side of the corpus with 600M additional words from non-NYT and non-LAT, randomly selected portions of the Gigaword v4 corpus, using modified Kneser-Ney smoothing (Chen and Goodman, 1996). We use the HPB decoder cdec (Dyer et al., 2010), with Mr. Mira (Eidelman et al., 2013), which is a k-best variant of MIRA (Chiang et al., 2008), to tune the parameters of"
P14-1106,P12-1096,0,0.0135225,"6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reorderings and penalize bad ones. Marton and Resnik (2008) employed soft syntactic constraints with weighted binary features and no MaxEnt model. They did not explicitly target reordering (beyond applying constraints on HPB rules). Althou"
P14-1106,P13-1111,0,0.0135279,"level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering"
P14-1106,P09-1054,0,0.0453873,"Missing"
P14-1106,C10-1126,0,0.020213,"33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted features in the SMT decoder to reward good reord"
P14-1106,D07-1077,0,0.0341675,"To our surprise, however, the improvement achieved by gold semantic reordering types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during dec"
P14-1106,N09-2004,0,0.0214349,"ctivity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translation output and reordered arguments to maximize the cross-lingual match of the semantic frames between the source sentence and the target translation. To our knowledge, their semantic reordering models were PAS-specific. In contrast, our model is universal and can be easily adopted to model the reordering of other linguistic units (e.g., syntactic constituents). Moreover, we have studied the effectiveness of the semantic reordering model in different scenarios. Non-syntax-based reorderings in HPB: Recently we have also seen work on lexicalized reorde"
P14-1106,I11-1004,0,0.014308,"10) presented a syntaxdriven maximum entropy reordering model that predicted the source word translation order. Gao et al. (2011) employed dependency trees to predict the translation order of a word and its head word. Huang et al. (2013) predicted the translation order of two source words.4 Our work, which shares this approach, differs from their work primarily in that our syntactic reordering models are based on the constituent level, rather than the word level. Semantics-based reordering: Semanticsbased reordering has also seen an increase in activity recently. In the pre-ordering approach, Wu et al. (2011) automatically learned pre-ordering rules from PAS. In the soft constraint or reordering model approach, Liu and Gildea (2010) modeled the reordering/deletion of source-side semantic roles in a tree-to-string translation model. Xiong et al. (2012) and Li et al. (2013) predicted the translation order between either two arguments or an argument and its predicate. Instead of decomposing a PAS into individual units, Zhai et al. (2013) constructed a classifier for each source side PAS. Finally in the post-processing approach category, Wu and Fung (2009) performed semantic role labeling on translati"
P14-1106,C04-1073,0,0.0529703,"types is still small (e.g., 33.9 vs. 33.4), suggesting that the potential improvement of semantic reordering models is much more limited. And we again see that the improvement achieved by semantic reordering models is limited in the presence of the syntactic reordering models. 6 Related Work Syntax-based reordering: Some previous work pre-ordered words in the source sentences, so that the word order of source and target sentences is similar. The reordering rules were either manually designed (Collins et al., 2005; Wang et al., 2007; Xu et al., 2009; Lee et al., 2010) or automatically learned (Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010; Khalilov and Sima’an, 2011; Lerner and Petrov, 2013), using syntactic parses. Li et al. (2007) focused on finding the n-best pre-ordered source sentences by predicting the reordering of sibling constituents, while Yang et al. (2012) obtained word order by using a reranking approach to reposition nodes in syntactic parse trees. Both are close to our work; however, our model generates reordering features that are integrated into the log-linear translation model during decoding. Another approach in previous work added soft constraints as weighted feature"
P15-1139,P12-2054,1,0.835429,"+λ · ψˆk,j new , (6) PJk −d,n 0 j 0 =1 ηk,j Nd,k,j 0 where µa,k,j = ( + ηk,j )/Nd,k,· for an existing frame j, and for a newly created frame j new , we have µa,k,j new = P −d,n ( Jj 0k=1 ηk,j 0 Nd,k,j 0 +ηk,j new )/Nd,k,· , where ηk,j new is drawn from the Gaussian prior N (0, γ). Here, the estimated global probability of choosing a frame j of issue k is ψˆk,j . Sampling Issue Topics In the generative process of HIPTM, the topic φk of issue k (1) generates tokens in the bill text and (2) provides the Dirichlet priors of the issue’s frames. Rather than collapsing multinomials and factorizing (Hu and Boyd-Graber, 2012), we follow Ahmed et al. (2013) and sample ˜ k + βφ? ) φˆk ∼ Dir(mk + n (7) k where mk ≡ (Mk,1 , Mk,2 , · · · , Mk,V ) is the token count vector from the bill text assigned to each ˜k,1 , N ˜k,2 , · · · , N ˜k,V ) ˜ k ≡ (N issue. The vector n denotes the token counts propagated from words assigned to topics that are associated with frames of issue k, approximated using minimal or maximal path assumptions (Cowans, 2006; Wallach, 2008). Sampling Frame Proportions Following the direct assignment method described in Teh et al. (2006), we sample the global frame proportion as ψˆk ≡ (ψˆk,1 , ψˆk,2 ,"
P15-1139,D14-1182,1,0.825569,"th stratified sampling, which preserves the ratio of the two classes in both the training and test sets. We report performance using AUC - ROC (Lusted, 1971) using SVMlight (Joachims, 1999).9 After preprocessing, our vocabulary contains 5,349 unique word types. Membership from Votes and Text. First, given the votes and text of all the legislators, we run HIPTM for 1,000 iterations with a burn-in period of 500 iterations. After burning in, we keep the sampled state of the model after every fifty iterations. The feature values are obtained by averaging over the ten stored models as suggested in Nguyen et al. (2014). Each legislator a is represented by a vector concatenating: • K-dimensional ideal point vector estimated from both votes and text ua,k • K-dimensional vector, estimating the ideal point using only text ηkT ψˆa,k • B probabilities estimating a’s votes on B bills P ˆb,k ua,k + yb ) Φ(xb K ϑ k=1 Predicting Tea Party Membership To quantitatively evaluate the effectiveness of HIPTM in capturing “Tea Partiness”, we predict Tea Party Caucus membership of legislators given their votes and text. This examines (1) how effective the baseline features extracted from the votes and text are in predicting"
P15-1139,N15-1076,1,\N,Missing
P15-2072,I13-1191,0,0.00574217,"nomic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover,"
P15-2072,P14-1105,1,0.122942,"of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control"
P15-2072,J08-4004,0,0.126671,"wspapers (Smith et al., 2013; Smith et al., 2014). While not the same as framing, identifying this sort of text reuse is an important step towards analyzing the “media packages” that social scientists associate with framing. Immigration Smoking Same-sex marriage 0.8 0.6 0.4 0.2 0.0 Stage 1 0 10 Stage 2 20 Stage 3 30 40 Round Figure 4: Chance-corrected inter-annotator agreement on the primary frame. Marker size indicates the number of annotations being compared; α = 1 indicates perfect agreement. ric has been previously recommended for tasks in computational linguistics that involve unitizing (Artstein and Poesio, 2008). For a more complete explanation, see Krippendorff (2004). The pattern of αU values across rounds is very similar to that shown in Figure 4, but not surprisingly, average levels of agreement are much lower. Arguably, this agreement statistic is overly harsh for our purposes. We do not necessarily expect annotators to agree perfectly about where to start and end each annotated span, or how many spans to annotate per article, and our codebook and guidelines offer relatively little guidance on these lowlevel decisions. Nevertheless, it is encouraging that in all cases, average agreement is great"
P15-2072,P10-2047,0,0.0327642,"Missing"
P15-2072,N15-1171,0,0.282768,"decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized), much variation is due to valid differen"
P15-2072,W06-2915,0,0.018161,"Missing"
P15-2072,P15-1139,1,0.448476,"e. Theories of framing posit that these decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized)"
P15-2072,W12-3809,0,0.194439,"milar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover, we emphasize that framing is an important feature of even seemingly neutral or objective language. A different but equally relevant line of work has focused on text re-use. L"
P15-2072,Q14-1025,0,0.0177153,"he lengths of the parts which do not overlap.7 As with the more common α statistic, αU is a chance-corrected agreement metric scaled such that 1 represents perfect agreement and 0 represents the level of chance. This metInter-annotator Agreement Because our annotation task is complex (selecting potentially overlapping text spans and labeling them), there is no single comprehensive measure of inter-annotator agreement. The simplest aspect of the annotations to compare is the choice of primary frame, which we measure using Krippendorff’s α (Krippendorff, 2012).5 accommodates missing values. See Passonneau and Carpenter (2014) for additional details. 6 Note that this is not a controlled experiment on annotation procedures, but rather a difference observed between two stages of an evolving process. 7 For example, in the example shown in Figure 2, the amount of disagreement on the two Cultural identity annotations would be the square of the length (in characters) of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s"
P15-2072,D13-1010,1,0.765748,"th (in characters) of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents ma"
P15-2072,D10-1028,1,0.895827,"Missing"
P15-2072,P15-1157,0,0.419998,"g posit that these decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized), much variation is"
P15-2072,N12-1072,0,0.0510175,"nfluence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality,"
P15-2072,J04-3002,0,0.0348045,"ippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover, we emphasize that framing is an important feature of even seemingly neutral or objective language. A different but equally relevant line of work has focus"
P15-2072,D10-1111,0,\N,Missing
P16-1065,D10-1005,1,0.838871,"Downstream models, on the contrary, generates topics and supervisory data simultaneously, which turns unsupervised topic models to (semi-)supervised ones. Supervisory data, like labels of documents and links between documents, can be generated from either a maximum likelihood estimation approach (McAuliffe and Blei, 2008; Chang and 6 We omit the comparison of WSBM with other models, because this has been done by Aicher et al. (2014). In addition, WSBM is a probabilistic method while SCC is deterministic. They are not comparable quantitatively, so we compare them qualitatively. 693 Blei, 2010; Boyd-Graber and Resnik, 2010) or a maximum entropy discrimination approach (Zhu et al., 2012; Yang et al., 2015). In block detection literature, stochastic block model (Holland et al., 1983; Wang and Wong, 1987, SBM) is one of the most basic generative models dealing with binary-weighted edges. SBM assumes that each node belongs to only one block and each link exists with a probability that depends on the block assignments of its connecting nodes. It has been generalized for degreecorrection (Karrer and Newman, 2011), bipartite structure (Larremore et al., 2014), and categorial values (Guimer`a and Sales-Pardo, 2013), as"
P16-1065,D07-1109,1,0.777842,"blocks in the C ORA dataset identified by WSBM , designated Blocks 1 and 2. Some statistics are given in Table 4. The two blocks are very sparsely connected, but comparatively quite densely connected inside either block. The two blocks’ topic distributions also reveal their differences: abstracts in Block 1 mainly focus on learning theory (learn, algorithm, bound, result, etc.) and MCMC (markov, chain, distribution, converge, etc.). Abstracts in Block 2, however, have higher 6 Related Work Topic models are widely used in information retrieval (Wei and Croft, 2006), word sense disambiguation (Boyd-Graber et al., 2007), dialogue segmentation (Purver et al., 2006), and collaborative filtering (Marlin, 2003). Topic models can be extended in either upstream or downstream way. Upstream models generate topics conditioned on supervisory information (Daum´e III, 2009; Mimno and McCallum, 2012; Li and Perona, 2005). Downstream models, on the contrary, generates topics and supervisory data simultaneously, which turns unsupervised topic models to (semi-)supervised ones. Supervisory data, like labels of documents and links between documents, can be generated from either a maximum likelihood estimation approach (McAuli"
P16-1065,P09-2074,0,0.0549227,"Missing"
P16-1065,W04-3243,0,0.0598077,"tion gets higher. Finally, only the NN topic dominates the two documents when LBH - RTM is applied (Figure 7(e)). LCH - RTM gives the highest proportion to the NN topic (Figure 7(b)). However, the NN topic 5.3 Topic Quality Results We use an automatic coherence detection method (Lau et al., 2014) to evaluate topic quality. Specifically, for each topic, we pick out the top n words and compute the average association score of each pair of words, based on the held-out documents in development and test sets. We choose n = 25 and use Fisher’s exact test (Upton, 1992, FET) and log likelihood ratio (Moore, 2004, LLR) as the association measures (Table 3). The main advantage of these measures is that they are robust even when the reference corpus is not large. Coherence improves with WSBM and maxmargin learning, but drops a little when adding lexical weights except the FET score on the W E B KB dataset, because lexical weights are intended to improve link prediction performance, not topic quality. Topic quality of LBH-RTM is also better than that of LCH-RTM, suggesting that WSBM benefits topic quality more than SCC. 692 Parallel Computing Optimization-2 Algorithm Bound Knowledge Base Belief Network O"
P16-1065,P14-1110,1,0.874468,"rvedi et al., 2012). edges in the experiment, it would be straightforward to adapt to model both directed/undirected and binary/nonnegative real weight edges. We are also interested in modeling changing topics and vocabularies (Blei and Lafferty, 2006; Zhai and Boyd-Graber, 2013). In the spirit of treating links probabilistically, we plan to explore application of the model in suggesting links that do not exist but should, for example in discovering missed citations, marking social dynamics (Nguyen et al., 2014), and identifying topically related content in multilingual networks of documents (Hu et al., 2014). 7 References Acknowledgment This research has been supported in part, under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113. Boyd-Graber is also supported by NSF grants IIS/1320538, IIS /1409287, and NCSE /1422492. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors. Conclusions and Future Work Christopher Aicher, Abigail Z. Jacobs, and Aaron Clauset. 2014. Learning latent block structure in weighted networks. Journal of Complex Networks. We introduce LBH-RTM, a discr"
P16-1065,P06-1003,0,0.103916,"Missing"
P16-1065,E14-1056,0,0.0324056,"(Table 2), except LCH-RTM, which is consistent with our PLR results. In Figure 7, we also show the proportions of topics that dominate the two documents according to the various models. There are multiple topics dominating K and A according to RTM (Figure 7(a)). As the model gets more sophisticated, the NN topic proportion gets higher. Finally, only the NN topic dominates the two documents when LBH - RTM is applied (Figure 7(e)). LCH - RTM gives the highest proportion to the NN topic (Figure 7(b)). However, the NN topic 5.3 Topic Quality Results We use an automatic coherence detection method (Lau et al., 2014) to evaluate topic quality. Specifically, for each topic, we pick out the top n words and compute the average association score of each pair of words, based on the held-out documents in development and test sets. We choose n = 25 and use Fisher’s exact test (Upton, 1992, FET) and log likelihood ratio (Moore, 2004, LLR) as the association measures (Table 3). The main advantage of these measures is that they are robust even when the reference corpus is not large. Coherence improves with WSBM and maxmargin learning, but drops a little when adding lexical weights except the FET score on the W E B"
P16-1065,D15-1030,1,0.886976,"rs only pairwise document relationships, failing to capture network structure at the level of groups or blocks of documents. We propose a new joint model that makes fuller use of the rich link structure within a document network. Specifically, our model embeds the weighted stochastic block model (Aicher et al., 2014, WSBM) to identify blocks in which documents are densely connected. WSBM basically categorizes each item in a network probabilistically as belonging to one of L blocks, by reviewing its connections with each block. Our model can be viewed as a principled probabilistic extension of Yang et al. (2015), who identify blocks in a document network deterministically as strongly connected components (SCC). Like them, we assign a distinct Dirichlet prior to each block to capture its topical commonalities. Jointly, a linear regression model with a discriminative, max-margin objective function (Zhu et al., 2012; Zhu et al., 2014) is trained to reconstruct the links, taking into account the features of documents’ topic and word distributions (Nguyen et al., 2013), block assignments, and inter-block link rates. We validate our approach on a scientific paper abstract dataset and collection of webpages"
P16-1177,D14-1110,0,0.0254243,"e-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) outperforms the pre-tra"
P16-1177,P15-2114,0,0.12848,"een their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho,"
P16-1177,N10-1145,0,0.131137,"and h2n , as additional features: hsub = |h1n − h2n | hdot = h1n h2n , (10) where hsub and hdot capture the element-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a wor"
P16-1177,P12-1092,0,0.157429,"mantic retrieval tasks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, co"
P16-1177,P15-1162,1,0.379487,"Missing"
P16-1177,D14-1218,0,0.0343004,"Missing"
P16-1177,D15-1106,0,0.119472,"e, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various forms. For example, the context of a current sentence in a document could be either its neighboring sentences (Lin et al., 2015; Wang and Cho, 2015), topics associated with the sentence (Mikolov and Zweig, 2012; Le and Mikolov, 2014), the document that contains the sentence (Huang et al., 2012), as well as their combinations (Ji et al., 2016). It is important to integrate context into neural networks because these models are often trained with only local information about their individual inputs. For example, recurrent and recursive neural networks only use local information about previously seen words in a sentence to predict the next word or composition.1 On the other hand, context information (such as topical infor"
P16-1177,D14-1113,0,0.0149496,"ection 3). The state-of-the-art model for this task is a semi-supervised approach (Rothe and Sch¨utze, 2015). This model use resources like WordNet 1886 to compute embeddings for different senses of words. Given a pair of target words and their context (neighboring words and sentences), this model represents each target word as the average of its sense embeddings weighted by cosine similarity to the context. The cosine similarity between the representations of words in a pair is then used to determine their semantic similarity. Also, the Skip-gram model (Mikolov et al., 2013a) is extended in (Neelakantan et al., 2014; Chen et al., 2014) to learn contextual word pair similarity in an unsupervised way. Table 2 shows the performance of different models on the SCWS dataset. SAE, CSAE-LC, CSAE-LGC show the performance of our pairwise autoencoders without context, with local context, and with local and global context, respectively. In case of CSAE-LGC, we concatenate local and global context to create context vectors. CSAELGC performs significantly better than the baselines, including the semi-supervised approach in Rothe and Sch¨utze (2015). It is also interesting that SAE (without any context information) out"
P16-1177,Q15-1022,0,0.00399514,"performances initially improve. The CSAE-LGC model that uses both local and global context benefits more from greater number of hidden layers than CSAE-LC that only uses local context. We attribute this to the use of global context in CSAE-LGC that leads to more accurate representations of words in their context. We also note that with just a single hidden layer, CSAE-LGC largely improves the performance as compared to CSAE-LC. 6 Related Work Representation learning models have been effective in many tasks such as language modeling (Bengio et al., 2003; Mikolov et al., 2013b), topic modeling (Nguyen et al., 2015), paraphrase detection (Socher et al., 2011), and ranking tasks (Yih et al., 2013). We briefly review works that use context information for text representation. Huang et al. (2012) presented an RNN model that uses document-level context information to construct more accurate word representations. In particular, given a sequence of words, the approach uses other words in the document as external (global) knowledge to predict the next word in the sequence. Other approaches have also modeled context at the document level (Lin et al., 2015; Wang and Cho, 2015; Ji et al., 2016). Ji et al. (2016) p"
P16-1177,D14-1162,0,0.108443,"Missing"
P16-1177,P15-1173,0,0.0245374,"Missing"
P16-1177,D13-1044,0,0.01333,"h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant quest"
P16-1177,P14-1068,0,0.0298506,"ks and a contextual word similarity task. For retrieval, our unsupervised approach that merely ranks inputs with respect to the cosine similarity between their hidden representations shows comparable performance with the state-of-the-art supervised models and in some cases outperforms them. 1 Introduction Representation learning algorithms learn representations that reveal intrinsic low-dimensional structure in data (Bengio et al., 2013). Such representations can be used to induce similarity between textual contents by computing similarity between their respective vectors (Huang et al., 2012; Silberer and Lapata, 2014). Recent research has made substantial progress on semantic similarity using neural networks (Rothe and Sch¨utze, 2015; Dos Santos et al., 2015; Severyn and Moschitti, 2015). In this work, we focus our attention on deep autoencoders and extend these models to integrate sentential or document context information about their inputs. We represent context information as low dimensional vectors that will be injected to deep autoencoders. To the best of our knowledge, this is the first work that enables integrating context into autoencoders. In representation learning, context may appear in various"
P16-1177,D12-1087,0,0.00457205,"ncoder network. Each decoder layer tries to recover the input of its corresponding encoder layer. As such, the weights are initially symmetric and the decoder weights do need to be learned. After the training is complete, the hidden layer hn contains a context-sensitive representation of the inputs x and cx . 2.3 Context Information Context is task and data dependent. For example, a sentence or document that contains a target word forms the word’s context. When context information is not readily available, we use topic models to determine such context for individual inputs (Blei et al., 2003; Stevens et al., 2012). In particular, we use Non-Negative Matrix Factorization (NMF) (Lin, 2007): Given a training set with n instances, i.e., X ∈ Rv×n , where v is the size of a global vocabulary and the scalar k is the number of topics in the dataset, we learn the topic matrix D ∈ Rv×k and context matrix C ∈ Rk×n using the following sparse coding algorithm: min kX − DCk2F + µkCk1 , D,C s.t. (8) D ≥ 0, C ≥ 0, where each column in C is a sparse representation of an input over all topics and will be used as global context information in our model. We obtain context vectors for test instances by transforming them ac"
P16-1177,D07-1003,0,0.0108977,"quation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context from Huang et al. (2012); “qAns” a TREC QA dataset with ground-truth labels for semantically relevant questions and (single-sentence) answers from Wang et al. (2007); and “qSim” a community QA dataset crawled from Stack Exchange with ground-truth labels for semantically equivalent questions from Dos Santos et al. (2015). Table 1 shows statistics of these datasets. To enable direct comparison with previous work, we use the same training, development, and test data provided by Dos Santos et al. (2015) and Wang et al. (2007) for qSim and qAns respectively and the entire data of SCWS (in unsupervised setting). We consider local and global context for target words in SCWS. The local context of a target word is its ten neighboring words (five before and five af"
P16-1177,N13-1106,0,0.0321806,"Missing"
P16-1177,P13-1171,0,0.0498837,"ment-wise difference and similarity (in terms of the sign of elements in each dimension) between h1n and h2n , respectively. We expect elements in hsub to be small for semantically similar and relevant inputs and large otherwise. Similarly, we expect elements in hdot to be positive for relevant inputs and negative otherwise. We can use any task-specific feature as additional features. This includes features from the 1885 minimal edit sequences between parse trees of the input pairs (Heilman and Smith, 2010; Yao et al., 2013), lexical semantic features extracted from resources such as WordNet (Yih et al., 2013), or other features such as word overlap features (Severyn and Moschitti, 2015; Severyn and Moschitti, 2013). We can also use additional features (Equation 10), computed for BOW representations of the inputs x1 and x2 . Such additional features improve the performance of our and baseline models. 4 Experiments In this Section, we use t-test for significant testing and asterisk mark (*) to indicate significance at α = 0.05. 4.1 Data and Context Information We use three datasets: “SCWS” a word similarity dataset with ground-truth labels on similarity of pairs of target words in sentential context"
P90-1029,P82-1007,0,0.062856,"lieve this restriction to be typical in NL systems. Most approaches treat this as an inference problem. It can be visualized as finding a relation between two nominal notions faculty member and phone number [1,2]. One such path uses the relation OFFICE(PERSON, ROOM) followed by the relation PHONE(ROOM,PHONE-NUMBER). A general heuristic is to use the shortest path. Computing hidden joins complicates the search space in searching for a solution among the underlying services, as can be seen in the architectures proposed, e.g., [1,4, 9]. 4.3. C o l l a p s e of information. It has long been noted [5] that a complex relation may be represented in a boolean field in a data base, such as the boolean field of the Navy Blue file which for a given vessel was T/F depending on whether there was a doctor onboard the vessel. There was no information about doctors in the data base, except for that field. In a medical data base, a In contrast to the typical approach where one 231 infers the hidden join as needed, we believe such joins are normally anticipatable, and provide support in our lexical definition tools (KNACQ) for specifying them. In KNACQ [15], a knowledge engineer, data base administrato"
P90-1029,P86-1036,0,0.0254745,"ed not derive programs for terms that it does not already know. For the example Inherent in the collectio/: of services covering a DNF expression is the data flow that combines the services into a program to fulfill the DNF request. The next step in the formulatior, process is data flow analysis to extract the data ~low graph corresponding to an abstract program fulfillin~ the request. &apos;Thedistancefunctiontakesanyphysicalobjectsas its arguments and looksuptheirlocation. 230 above, the system should b e expected to respond I don&apos;t know how to compute square root. similar phenomenon was noticed [11]; patient records contained a T/F field depending on whether the patient&apos;s mother had had melanoma, though there was no other information on the patient&apos;s mother or her case of melanoma. By making that assumption, we know that all concepts and relations in the domain model, that is, all primitives appearing in WML as input to the MUS component, have a translation specified by the applications programmer to a composition of underlying services. As stated in Section 2, we further restrict the goals of the MUS component to synthesize programs of a simple structure: acyclic data flow graphs of ser"
P90-1029,P89-1024,1,0.547201,"ology does insulate them from the underlying implemantation idiosyncrasies of one application will expect that our models of language and understanding will extend to simultaneous access of several applications. 2. Scope of the Problem Our view of access to multiple underlying systems is given in Figure 2. As implied in the graphical representation, the user&apos;s request, whatever its modality, is translated into an internal representation of the meaning of what the user needs. We initially explored a first-order logic for this purpose; however, in Janus [13] we have adopted an intensional logic [3, 14] to investigate whether intensional logic offers 227 more appropriate representations for applications more complex than databases, e.g., simulations and other calculations in hypothetical situations. From the statement of what the user needs, we next derive a statement of how to fulfill that need, an execution p/an composed of abstract commands. The execution plan takes the form of a limited class of data flow graphs for a virtual machine that includes the capabilities of all of the application systems. At the level of that virtual machine, specific commands to specific underlying systems are"
P90-1029,H89-1013,1,0.910681,"ing Cases Here we present several well-known challenging classes of problems in translating from logical form to programs. T)))) (OSGP- ENTITY-OVERALL-READINESS-OF ?JX699 C1))))) TiME WORLD)) 4.1. Deriving procedures from descriptions. The challenge is to find a compromise between arbitrary program synthesis and a useful class of program derivation problems. Suppose the user asks for the square root of a value, when the system does not know the meaning of square root, as in Find the square root of the sum of the squares of the residuals. Various knowledge acquisition techniques, such as KNACQ [15], would allow a user to provide syntactic and semantic information for the unknown phrase to be defined. Square root could be defined as a function that computes the number that when multiplied times itseff is the same as the input. However, that is a descriptive definition of square root without any indication of how to compute it. One still must synthesize a program that computes square root; in fact, in early literature on automatic programming and rigorous approaches to developing programs, deriving a program to compute square root was often used as an example problem. (#s (CONTEXT :OPERAT"
P90-1029,H86-1008,0,\N,Missing
P92-1053,P90-1034,0,0.0607191,"Missing"
P92-1053,H92-1021,0,\N,Missing
P99-1068,J90-2002,0,0.313221,"1998) is a languageindependent system for automatic discovery of text in parallel translation on the World Wide Web. This paper extends the preliminary STRAND results by adding automatic language identification, scaling up by orders of magnitude, and formally evaluating performance. The most recent end-product is an automatically acquired parallel corpus comprising 2491 English-French document pairs, approximately 1.5 million words per language. 1 Introduction Text in parallel translation is a valuable resource in natural language processing. Statistical methods in machine translation (e.g. (Brown et al., 1990)) typically rely on large quantities of bilingual text aligned at the document or sentence level, and a number of approaches in the burgeoning field of crosslanguage information retrieval exploit parallel corpora either in place of or in addition to mappings between languages based on information from bilingual dictionaries (Davis and Dunning, 1995; Landauer and Littman, 1990; Hull and Oard, 1997; Oard, 1997). Despite the utility of such data, however, sources of bilingual text are subject to such limitations as licensing restrictions, usage fees, restricted domains or genres, and dated text ("
P99-1068,J96-2004,0,0.0257748,"Missing"
P99-1068,resnik-1998-parallel,1,0.89134,"call (Section 3). Second, the algorithm is scaled up more seriously to generate large numbers of parallel documents, this time for English and French, and again subjected to formal evaluation (Section 4). The concrete end result reported here is an automatically acquired English-French parallel corpus of Web documents comprising 2491 document pairs, approximately 1.5 million words per language (without markup), containing little or no noise. STRA N D (Structural 2 STRAND Translation Preliminaries This section is a brief summary of the STRAND system and previously reported preliminary results (Resnik, 1998). The STRAND architecture is organized as a pipeline, beginning with a candidate generation stage that (over-)generates candidate pairs of documents that might be parallel translations. (See Figure 1.) The first implementation of the generation stage used a query to the Altavista search engine to generate pages that could be viewed as ""parents"" of pages in parM]el transla528 tion, by asking for pages containing one portion of anchor text (the readable material in a hyperlink) containing the string ""English"" within a fixed distance of another anchor text containing the string ""Spanish"". (The ma"
resnik-1998-parallel,J93-1004,0,\N,Missing
resnik-1998-parallel,W96-0201,0,\N,Missing
resnik-1998-parallel,W97-0311,0,\N,Missing
resnik-1998-parallel,A97-1050,1,\N,Missing
resnik-1998-parallel,J90-2002,0,\N,Missing
resnik-1998-parallel,H91-1026,0,\N,Missing
S01-1014,H01-1060,1,0.704366,"r {St, s2, ... , SNw}· For each instance, we select the sense for which the SVM classifier&apos;s response is most strongly &quot;yes&quot; (or, equivalently, most weakly &quot;no&quot;). 3 SENSEVAL-2 Results In our preliminary efforts we were not surprised to find that sparseness of data was a problem. Although we expect that some improvements may be obtained by collapsing across word variants - e.g. via morphological equivalence classes or stemming - we also plan to focus our efforts on semantic expansion, using document expansion techniques we have developed in our research on cross-language information retrieval (Levow et al., 2001). We have implemented a variant of the architecture in which training contexts are used as queries to a comparable corpus in order to retrieve related documents. The features from these documents are then added to the context representations, providing semantically enhanced feature vectors. Evaluation of this approach using SENSEVAL data is in progress. Table 1 shows the performance of UMD&apos;s supervised sense tagger (UMD-SST) for the lexical sample tasks in four languages. The figures for English, Spanish, and Swedish are official SENSEVAL-2 results; the figures for Basque are unofficial result"
S01-1014,H01-1033,1,0.812522,"available for all languages, and the predominance of lexical selection tasks (rather than all- words tasks) suggested adopting a disambiguation approach capable of exploiting manually annotated training data. These considerations motivated a system design based on supervised learning, where senses to be predicted did not need to be treated as part of a semantic hierarchy. Our design was also motivated by the role of semantic selection techniques in our longer term research agenda. In the context of our group&apos;s work on cross-language information retrieval and machine translation applications (Resnik et al., 2001; Cabezas et al., 2001), lexical selection - that is, choosing the right target-language System Architecture UMD&apos;s system follows the classic supervised learning paradigm that, for WSD, is perhaps best exemplified by Yarowsky&apos;s (1993) work. Each word in the vocabulary is considered an independent classification problem. First, annotated training instances for the ambiguous word are analyzed so that each instance can be represented as a collection of feature-value pairs labeled with the correct category. Then, these data are used for parameter estimation within a supervised learning framework i"
S01-1014,W97-0209,1,0.791254,"of the opportunity to construct an architecture that will support both tasks. In the sections that follow, we lay out our system architecture, briefly summarize our SENSEVAL-2 results, and discuss our plans for future work. 1 2 Introduction The SENSEVAL-2 exercise provided an unprecedented opportunity to explore word sense disambiguation (WSD) in a common evaluation framework for a large number of languages. In past work, we have focused on unsupervised methods for English, taking advantage of the WordN et hierarchy and sometimes also selectional preferences between predicates and arguments (Resnik, 1997; Resnik, 1999). In the current exercise, however, WordNet-like sense hierarchies were not necessarily going to be available for all languages, and the predominance of lexical selection tasks (rather than all- words tasks) suggested adopting a disambiguation approach capable of exploiting manually annotated training data. These considerations motivated a system design based on supervised learning, where senses to be predicted did not need to be treated as part of a semantic hierarchy. Our design was also motivated by the role of semantic selection techniques in our longer term research agenda."
S01-1014,H93-1052,0,0.0769688,"Missing"
S07-1071,N06-2015,0,0.0357647,"or manually-annotated data. ∑i m i j ∑i, j mi j mi j P(wi |c j ) = ∑i m i j P(c j ) = (2) (3) 329 c pmi = argmax c j ∈C ∑ PMI(wi , c j ) (4) (5) (6) (7) wi ∈W Note that this PMI-based classifier does not capitalize on prior probabilities of the different senses. 4 Data 4.1 English Lexical Sample Task The English Lexical Sample Task training and test data (Pradhan et al., 2007) have 22281 and 4851 instances respectively for 100 target words (50 nouns and 50 verbs). WordNet 2.1 is used as the sense inventory for most of the target words, but certain words have one or more senses from OntoNotes (Hovy et al., 2006). Many of the finegrained senses are grouped into coarser senses. Our approach relies on representing a sense with a number of near-synonymous words, for which a thesaurus is a natural source. Even though the approach can be ported to WordNet4 , there was no easy 4 The synonyms within a synset, along with its one-hop neighbors and all its hyponyms, can represent that sense. W ORDS all nouns only verbs only BASELINE 27.8 25.6 29.2 T RAINING DATA PMI- BASED NA¨I VE BAYES 41.4 50.8 43.4 53.6 38.4 44.5 P RIOR 37.4 18.1 58.9 T EST DATA L IKELIHOOD NA¨I VE BAYES 49.4 52.1 49.6 49.7 49.1 54.7 Table 1"
S07-1071,O97-1002,0,0.0672605,"to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (if it is large enough—a few million words) or from another large text with a distribution of senses similar to the target text. ... .. . 327 Figure 1: The cross-lingual candidate senses of Chiand . nese words distributional concept-distance measures performed better than all WordNet-based measures as well, except for the Jiang and Conrath (1997) measure. 2.2 Cross-lingual word–category co-occurrence matrix Given a Chinese word wch in context, we use a Chinese–English bilingual lexicon to determine its different possible English translations. Each English translation wen may have one or more possible coarse senses, as listed in an English thesaurus. These English thesaurus concepts (cen ) will be referred to as cross-lingual candidate senses of the Chinese word wch .2 Figure 1 depicts examples. We create a cross-lingual word–category cooccurrence matrix (CL-WCCM) with Chinese word types wch as one dimension and English thesaurus conce"
S07-1071,S07-1004,0,0.323552,"e a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of"
S07-1071,S07-1009,0,0.189256,"the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical"
S07-1071,P04-1036,0,0.0308951,"he words in it. For each word, the strength of association of each of the words in its context (±5 words) with each of its senses is summed. The sense that has the highest cumulative association is chosen as the intended sense. A new bootstrapped WCCM is created such that each cell mi j , corresponding to word wen i and concept cen , is populated with the number of times wen j i en co-occurs with any word used in sense c j . Mohammad and Hirst (2006a) used the DPCs created from the bootstrapped WCCM to attain near-upper-bound results in the task of determining word sense dominance. Unlike the McCarthy et al. (2004) dominance system, this approach can be applied to much smaller target texts (a few hundred sentences) without the need for a large similarly-sense-distributed text1 . Mohammad and Hirst (2006b) used the DPC-based monolingual distributional measures of concept-distance to rank word pairs by their semantic similarity and to correct real-word spelling errors, attaining markedly better results than monolingual distributional measures of word-distance. In the spelling correction task, the 1 The McCarthy et al. (2004) system needs to first generate a distributional thesaurus from the target text (i"
S07-1071,E06-1016,1,0.928947,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,W06-1605,1,0.921315,"stributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word-sense classifiers. Mohammad and Hirst (2006a) proposed a way to determine the strength of association between a sense or concept and co-occurring words—the distributional profile of a concept (DPC)—without the use of manually annotated data. We implemented an unsupervised na¨ıve Bayes word sense classifier using these DPCs that was best or within one percentage point of the best unsupervised systems in the Multilingual Chinese– English Lexical Sample Task (task #5) and the English Lexical Sample Task (task #17). We also created a simple PMI-based classifier to attempt the English Lexical Substitution Task (task #10); however, its perfo"
S07-1071,D07-1060,1,0.850175,"sible substitutes for a given target word is usually contingent on its intended sense. Therefore, word sense disambiguation is expected to be useful in lexical substitution. We used the PMI-based classier to determine the intended sense. 326 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 326–333, c Prague, June 2007. 2007 Association for Computational Linguistics The objective of the Multilingual Chinese– English Lexical Sample Task (Jin et al., 2007) is to select from a given list a suitable English translation of a Chinese target word in context. Mohammad et al. (2007) proposed a way to create cross-lingual distributional profiles of a concepts (CL-DPCs)— the strengths of association between the concepts of one language and words of another. For this task, we mapped the list of English translations to appropriate thesaurus categories and used an implementation of a CL-DPC–based unsupervised na¨ıve Bayes classifier to identify the intended senses (and thereby the English translations) of target Chinese words. 2 Distributional profiles of concepts In order to determine the strength of association between a sense of the target word and its co-occurring words,"
S07-1071,S07-1016,0,0.332879,"poor. 1 Introduction Determining the intended sense of a word is potentially useful in many natural language tasks including machine translation and information retrieval. The best approaches for word sense disambiguation are supervised and they use words that co-occur with the target as features. These systems rely on senseannotated data to identify words that are indicative of the use of the target in each of its senses. However, only limited amounts of senseannotated data exist and it is expensive to create. In our previous work (Mohammad and Hirst, 2006a), The English Lexical Sample Task (Pradhan et al., 2007) is a traditional word sense disambiguation task wherein the intended (WordNet) sense of a target word is to be determined from its context. We manually mapped the WordNet senses to the categories in a thesaurus and the DPC-based na¨ıve Bayes classifier was used to identify the intended sense (category) of the target words. The object of the Lexical Substitution Task (McCarthy and Navigli, 2007) is to replace a target word in a sentence with a suitable substitute that preserves the meaning of the utterance. The list of possible substitutes for a given target word is usually contingent on its i"
S07-1071,C92-2070,0,0.588737,"Toronto Toronto, ON M5S 3G4 Canada smm@cs.toronto.edu Philip Resnik Graeme Hirst Dept. of Computer Science Dept. of Linguistics and UMIACS University of Maryland University of Toronto College Park, MD 20742 Toronto, ON M5S 3G4 USA Canada gh@cs.toronto.edu resnik@umiacs.umd.edu Abstract we proposed an unsupervised approach to determine the strength of association between a sense or concept and its co-occurring words—the distributional profile of a concept (DPC)—relying simply on raw text and a published thesaurus. The categories in a published thesaurus were used as coarse senses or concepts (Yarowsky, 1992). We now show how distributional profiles of concepts can be used to create an unsupervised na¨ıve Bayes word-sense classifier. We also implemented a simple classifier that relies on the pointwise mutual information (PMI) between the senses of the target and co-occurring words. These DPC-based classifiers participated in three SemEval 2007 tasks: the English Lexical Sample Task (task #17), the English Lexical Substitution Task (task #10), and the Multilingual Chinese– English Lexical Sample Task (task #5). Words in the context of a target word have long been used as features by supervised word"
S16-1184,W13-2322,0,0.0373079,"ms in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the sentence “I read a book, called Stories from Nature, about the forest.” that control a policy that takes actions in this search space. AMR (Banarescu et al., 2013), in turn, is a structured semantic representation which is a rooted, directed, acyclic graph. The nodes of this graph represent concepts in the given sentence and the edges represent relations between these concepts. As such, the task of predicting AMRs can be naturally placed in the L2S framework. This allows us to model the learning of concepts and relations in a unified setting which aims to minimize the loss over the entire predicted structure. In the next section, we briefly review DAGGER and explain its various components with respect to our AMR parsing task. Section 3 describes our mai"
S16-1184,P13-2131,0,0.0685972,"ed root croot and the root of a component, we get the k-best list (as described in section 3.2) between them and choose the most frequent edge from it. Dataset BOLT DF MT Broadcast conversation Weblog and WSJ BOLT DF English Guidelines AMRs 2009 Open MT Proxy reports Weblog Xinhua MT Training 1061 214 0 6455 689 204 6603 866 741 Dev 133 0 100 210 0 0 826 0 99 Test 133 0 100 229 0 0 823 0 86 Table 4: Dataset statistics. All figures represent number of sentences. bit machine learning library (Langford et al., 2007; Daum´e III et al., 2014). The evaluation of predicted AMRs is done using Smatch (Cai and Knight, 2013) 1 , which compares two AMRs using precision, recall and F1 . Our system obtained a Smatch F1 score of 0.46 with a P recision of 0.51 and a Recall of 0.43 on the test set in the Shared Task (We made a tokenization error during the actual semeval submission and so reported an F1 score of 0.44 instead). The mean F1 score of all systems submitted to the shared task was 0.55 and the standard deviation was 0.06. 5 3.6 Acyclicity The post-processing step described in the previous section ensures that the predicted AMRs are rooted, connected, graphs. However, an AMR, by definition, is also acyclic. W"
S16-1184,P04-1015,0,0.0735607,"tasks are treated as a sequence of predictions. Using Learning to Search, we add past predictions as features for future predictions, and define a combined loss over the entire AMR structure. ARG1 ARG0 book i topic name name op1 forest op3 op2 Nature Stories from 1 Introduction This paper describes our submission to the Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016. The goal of the task is to generate AMRs automatically for English sentences. We develop a novel technique for AMR parsing that uses Learning to Search (L2S) (Ross et al., 2011; Daum´e III et al., 2009; Collins and Roark, 2004). L2S is a family of approaches that solves structured prediction problems. These algorithms have proven to be highly effective for problems in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the se"
S16-1184,de-marneffe-etal-2006-generating,0,0.0498589,"Missing"
S16-1184,D13-1152,1,0.899758,"Missing"
S16-1184,D14-1225,0,0.0312202,"n This paper describes our submission to the Abstract Meaning Representation (AMR) Parsing Shared Task at SemEval 2016. The goal of the task is to generate AMRs automatically for English sentences. We develop a novel technique for AMR parsing that uses Learning to Search (L2S) (Ross et al., 2011; Daum´e III et al., 2009; Collins and Roark, 2004). L2S is a family of approaches that solves structured prediction problems. These algorithms have proven to be highly effective for problems in NLP like part-of-speech tagging, named entity recognition (Daum´e III et al., 2014), coreference resolution (Ma et al., 2014), and dependency parsing (He et al., 2013). Briefly, L2S attempts to do structured prediction by (1) decomposing the production of the structured output in terms of an explicit search space (states, actions, etc.); and (2) learning hypotheses ∗ The first two authors contributed equally to this work. Figure 1: AMR graph for the sentence “I read a book, called Stories from Nature, about the forest.” that control a policy that takes actions in this search space. AMR (Banarescu et al., 2013), in turn, is a structured semantic representation which is a rooted, directed, acyclic graph. The nodes of"
W04-0821,P04-1037,1,0.893578,"performance slightly. Although we have not yet reached any firm conclusions, we conjecture that value potentially added by these features may have been offset by the expansion in the size of the feature space; in future work we plan to explore feature selection and alternative learning frameworks. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assumptions: it assumes that senses and words have certai"
W04-0821,S01-1014,1,0.927365,"oitation of real-world bilingual text as a resource for unsupervised sense tagging. We validated the portability of our supervised disambiguation approach by applying it in seven tasks (English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples), and we experimented with a new unsupervised algorithm for sense modeling using parallel corpora. 1 Supervised Sense Tagging for Lexical Samples 1.1 Tagging Framework For the English, Basque, Catalan, Chinese, Romanian, Spanish, and “multilingual” lexical samples, we employed the UMD-SST system developed for S ENSEVAL -2 (Cabezas et al., 2001); we refer the reader to that paper for a detailed system description. Briefly, UMD-SST takes a supervised learning approach, treating each word in a task’s vocabulary as an independent problem of classification into that word’s sense inventory. Each training and test item is represented as a weighted feature vector, with dimensions corresponding to properties of the context. As in S ENSEVAL -2, our system supported the following kinds of features:  Local context. For each = 1, 2, and 3, and for each word  in the vocabulary, there is a feature  representing the presence of word   at"
W04-0821,P02-1033,1,0.859288,"tional features did not help and may have hurt performance slightly. Although we have not yet reached any firm conclusions, we conjecture that value potentially added by these features may have been offset by the expansion in the size of the feature space; in future work we plan to explore feature selection and alternative learning frameworks. 2 Unsupervised Sense Tagging using Bilingual Text 2.1 Probabilistic Sense Model For the past several years, the University of Maryland group has been exploring unsupervised approaches to word sense disambiguation that take advantage of parallel corpora (Diab and Resnik, 2002; Diab, 2003). Recently, Bhattacharya et al. (2004) (in a UMD/Montreal collaboration) have developed a variation on this bilingual approach that is inspired by the central insight of Diab’s work, but recasts it in a probabilistic framework. A generative model, it is a variant of the graphical model of Bengio and Kermorvant (2003), which groups semantically related words from the two languages into “senses”; translations are generated by probabilistically choosing a sense and then words from the sense. Briefly, the model of Bhattacharya et al. uses probabilistic analysis and independence assump"
W04-0821,N03-1019,0,0.0288725,"c model results (fine-grained) on the S ENSEVAL -2 English allwords task United Nations Proceedings, and newswire translations from FBIS (the Foreign Broadcast Information Service). French-English: a set of 1,008,591 sentence pairs from the Europarl corpus (Koehn, 2003) Chinese-English: a set of 440,223 sentence pairs from FBIS. In order to tag new test sentences, we used machine translation from English test items into each of Spanish, French, and Chinese. We used Systran for Spanish and French, and for Chinese we used an implementation of the alignment template framework for statistical MT (Kumar and Byrne, 2003). Once having obtained the translations for test sentences, we used GIZA++ to create word-level alignments within which translation pairs could be identified. We used the probabilistic model only for WSD of nouns, where nouns were identified using an automatic part-of-speech tagger. For other parts of speech, we used the first-listed WordNet sense. Time limitations prevented us from completing S ENSEVAL -3 runs in time for this writing. Table 4 shows the performance of the system on the S ENSEVAL -2 English all-words task. This performance level places the approach in the middle group of perfo"
W04-0821,H01-1060,1,0.798826,"English: Grammatical context. We use a syntactic dependency parser (Lin, 1998) to produce, for each word to be disambiguated, features identifying relevant syntactic relationships in the sentence where it occurs. For example, in the sentence The U.S. government announced a new visa waiver policy, the word government would have syntactic features like DET: THE, MOD :U.S., and SUBJ - OF : ANNOUNCED. Expanded context. In information retrieval, we and other researchers have found that it can be useful to expand the representation of a document to include informative words from similar documents (Levow et al., 2001). In a similar spirit, we create a set of expandedcontext features  by (a) treating the WSD context as a bag of words, (b) issuing it as a query to a standard information retrieval system that has indexed a large collection of documents, and (c) including the nonstopword vocabulary of the top  documents returned. So, for example, in a context containing the sentence The U.S. government announced a new visa waiver policy, the query might retrieve news articles like “US to Extend Fingerprinting to Europeans, Japanese” (Bloomberg.com, April 2, 2004), leading to the addition of features li"
W04-0821,J03-1002,0,0.00617187,"Missing"
W05-0812,J93-2003,0,0.0182485,"Missing"
W05-0812,P03-1012,0,0.263489,"distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a dependency parse of eM 1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1 We ignore the sentence length probability p(M|N), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabilities, and normalization"
W05-0812,W03-0305,0,0.0933461,"Missing"
W05-0812,W03-0301,0,0.129972,"Missing"
W05-0812,C00-2163,0,0.487721,"th English word eai . This representation introduces 83 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0)"
W05-0812,P00-1056,0,0.734476,"th English word eai . This representation introduces 83 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 83–86, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 an asymmetry into the model because it constrains each French word to correspond to exactly one English word, while each English word is permitted to correspond to an arbitrary number of French words. Although the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0)"
W05-0812,J03-1002,0,0.0274411,"that this varied depending on the characteristics of the corpus and the type of annotation (in particular, whether the annotation set included probable alignments). The results are summarized in Table 2. It shows results with our HMM model using both Equations 2 and 4 as our distortion model, which represent the unlimited and limited resource tracks, respectively. It also includes a comparison with IBM Model 4, for which we use a training sequence of IBM Model 1 (5 iterations), HMM (6 iterations), and IBM Model 4 (5 iterations). This sequence performed well in an evaluation of the IBM Models (Och and Ney, 2003). For comparative purposes, we show results of applying both P(f|e) and P(e|f) prior to symmetrization, along with results of symmetrization. Comparison of the asymmetric and symmetric results largely supports the hypothesis presented in Section 2.3, as our system generally produces much better recall than IBM Model 4, while offering a competitive precision. Our symmetrized results usually produced higher recall and precision, and lower alignment error rate. We found that the largest gain in performance came from the improved initialization. The combined distortion model (Equation 4), which pr"
W05-0812,E99-1010,0,0.022368,"i , eai−1 ), T (eai−1 )) (3) Since both the surface distortion and tree distortion models represent p(ai |ai−1 ), we can combine them using linear interpolation as in Equation 4. 2.1 The Tree Distortion Model Equation 1 is adequate in practice, but we can improve it. Numerous parameterizations have been proposed for the distortion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a"
W05-0812,W02-1012,0,0.197458,"Missing"
W05-0812,C96-2141,0,0.547554,"hough the resulting set of links may still be relatively accurate, we can symmetrize by combining it with the set produced by applying the complementary model P(e|f) to the same data (Och and Ney, 2000b). Making a few independence assumptions we arrive at the decomposition in Equation 1. 1 M P(f, a|e) = ∏ d(ai |ai−1 ) · t( fi |eai ) (1) i=1 We refer to d(ai |ai−1 ) as the distortion model and t( f i |eai ) as the translation model. Conveniently, Equation 1 is in the form of an HMM, so we can apply standard algorithms for HMM parameter estimation and maximization. This approach was proposed in Vogel et al. (1996) and subsequently improved (Och and Ney, 2000a; Toutanova et al., 2002). I1 very2 much3 doubt4 that5 τ(I1 , very2 ) = (1, 2, 0) τ(very2 , I1 ) = (2, 1, 1) τ(I1 , doubt4 ) = (1, 0, 0) τ(that5 , I1 ) = (1, 1, 1) Figure 2: Example of tree distances in a sentence from the Romanian-English development set. if i &gt; k; 0 otherwise} is simply a binary indicator of the linear relationship of the words within the surface string. Tree distance is illustrated in Figure 2. In our tree distortion model, we condition on the tree distance and the part of speech T (ei−1 ), giving us Equation 3. d(ai |ai−1 ) = p"
W05-0812,P01-1067,0,0.122892,"ion model. In our surface distortion model, it depends only on the distance ai − ai−1 and an automatically determined word class C(eai−1 ) as shown in Equation 2. It is similar to (Och and Ney, 2000a). The word class C(eai−1 ) is assigned using an unsupervised approach (Och, 1999). d(ai |ai−1 ) = p(ai |ai − ai−1,C(eai−1 )) (2) The surface distortion model can capture local movement but it cannot capture movement of structures or the behavior of long-distance dependencies across translations. The intuitive appeal of capturing richer information has inspired numerous alignment models (Wu, 1995; Yamada and Knight, 2001; Cherry and Lin, 2003). However, we would like to retain the simplicity and good performance of the HMM Model. We introduce a distortion model which depends on the tree distance τ(ei , ek ) = (w, x, y) between each pair of English words ei and ek . Given a dependency parse of eM 1 , w and x represent the respective number of dependency links separating ei and ek from their closest common ancestor node in the parse tree. 2 The final element y = {1 1 We ignore the sentence length probability p(M|N), which is not relevant to word alignment. We also omit discussion of HMM start and stop probabili"
W05-0812,P04-1066,0,\N,Missing
W07-0716,P02-1038,0,0.0853391,"In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences a"
W07-0716,W05-0909,0,0.06195,",nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can b"
W07-0716,P05-1074,0,0.601747,"Missing"
W07-0716,N03-1003,0,0.572191,"Missing"
W07-0716,N06-1003,0,0.538654,"Missing"
W07-0716,P00-1056,0,0.104185,"The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexical re-orderings, but 121 phrasal re-orderings, as well. Each φ(¯ e, f¯, X) denotes a feature function defined on the pair of hierarchical phrases.1 Feature functions represent conditional and joint co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to"
W07-0716,J04-4002,0,0.0597953,"nt co-occurrence probabilities over the hierarchical paraphrase pair. The Hiero framework includes methods to learn grammars and feature values from unannotated parallel corpora, without requiring syntactic annotation of the data. Briefly, training a Hiero model proceeds as follows: • GIZA++ (Och and Ney, 2000) is run on the parallel corpus in both directions, followed by an alignment refinement heuristic that yields a many-to-many alignment for each parallel sentence. • Initial phrase pairs are identified following the procedure typically employed in phrase based systems (Koehn et al., 2003; Och and Ney, 2004). • Grammar rules in the form of equation (2) are induced by “subtracting” out hierarchical phrase pairs from these initial phrase pairs. • Fractional counts are assigned to each produced rule: c(X → h¯ e, f¯i) = m X 1 j=1 njr (3) where m is the number of initial phrase pairs that give rise to this grammar rule and njr is the number of grammar rules produced by the j th initial phrase pair. • Feature functions φk1 (f¯, e¯, X) are calculated for each rule using the accumulated counts. Once training has taken place, minimum error rate training (Och, 2003) is used to tune the parameters λi . Fina"
W07-0716,P03-1021,0,0.0625549,"on Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation quality measure—relies on data sets in which source language sentences are paired with (sets of) reference translations. It is widely agreed that, at least for the widely used BLEU criterion, which is based on n-gram overlap between hypotheses and reference translations, the criterion is most accurate when computed with as many distinct reference translations as possible. Intuitively this makes sense: if there"
W07-0716,N03-1024,0,0.450807,"Missing"
W07-0716,P02-1040,0,0.118446,"ion Nitin Madnani, Necip Fazil Ayan, Philip Resnik & Bonnie J. Dorr Institute for Advanced Computer Studies University of Maryland College Park, MD, 20742 {nmadnani,nfa,resnik,bonnie}@umiacs.umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper,"
W07-0716,W04-3219,0,0.482508,"Missing"
W07-0716,J07-2003,0,0.20005,"ique for paraphrasing, designed with the application to parameter tuning in mind. Section 4 presents evaluation results using a state of the art statistical MT system, demonstrating that half the human reference translations in a standard 4-reference tuning set can be replaced with automatically generated paraphrases, with no significant decrease in MT system performance. In Section 5 we discuss related work, and in Section 6 we summarize the results and discuss plans for future research. 2 Translation Framework The work described in this paper makes use of the Hiero statistical MT framework (Chiang, 2007). Hiero is formally based on a weighted synchronous context-free grammar (CFG), containing synchronous rules of the form X → h¯ e, f¯, φk1 (f¯, e¯, X)i (2) where X is a symbol from the nonterminal alphabet, and e¯ and f¯ can contain both words (terminals) and variables (nonterminals) that serve as placeholders for other phrases. In the context of statistical MT, where phrase-based models are frequently used, these synchronous rules can be interpreted as pairs of hierarchical phrases. The underlying strength of a hierarchical phrase is that it allows for effective learning of not only the lexic"
W07-0716,2006.amta-papers.25,1,0.912501,".umd.edu Abstract model parameters relative to a measure of translation quality. This has become much more standard than optimizing the conditional probability of the training data given the model (i.e., a maximum likelihood criterion), as was common previously. Och showed that system performance is best when parameters are optimized using the same objective function that will be used for evaluation; BLEU (Papineni et al., 2002) remains common for both purposes and is often retained for parameter optimization even when alternative evaluation measures are used, e.g., (Banerjee and Lavie, 2005; Snover et al., 2006). Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically"
W07-0716,W03-1608,0,0.16138,"Missing"
W07-0716,strassel-etal-2006-integrated,0,0.0201083,"count as possible. To do otherwise is to risk the possibility that the criterion might judge good translations to be poor when they fail to match the exact wording within the reference translations that have been provided. This reliance on multiple reference translations creates a problem, because reference translations are labor intensive and expensive to obtain. A common source of translated data for MT research is the Linguistic Data Consortium (LDC), where an elaborate process is undertaken that involves translation agencies, detailed translation guidelines, and quality control processes (Strassel et al., 2006). Some 120 Proceedings of the Second Workshop on Statistical Machine Translation, pages 120–127, c Prague, June 2007. 2007 Association for Computational Linguistics efforts have been made to develop alternative processes for eliciting translations, e.g., from users on the Web (Oard, 2003) or from informants in lowdensity languages (Probst et al., 2002). However, reference translations for parameter tuning and evaluation remain a severe data bottleneck for such approaches. Note, however, one crucial property of reference translations: they are paraphrases, i.e., multiple expressions of the same"
W07-0716,N03-1017,0,0.254743,"nslations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality. 1 Introduction Viewed at a very high level, statistical machine translation involves four phases: language and translation model training, parameter tuning, decoding, and evaluation (Lopez, 2007; Koehn et al., 2003). Since their introduction in statistical MT by Och and Ney (2002), log-linear models have been a standard way to combine sub-models in MT systems. Typically such a model takes the form X λi φi (f¯, e¯) (1) i where φi are features of the hypothesis e and λi are weights associated with those features. Selecting appropriate weights λi is essential in order to obtain good translation performance. Och (2003) introduced minimum error rate training (MERT), a technique for optimizing log-linear Minimum error rate training—and more generally, optimization of parameters relative to a translation qualit"
W07-0716,P06-1091,0,0.0340642,"we have an example input and a single labeled, correct output. However, this output is chosen from a space in which the number of possible outputs is exponential in the input size, and in which there are many good outputs in this space (although they are vastly outnumbered by the bad outputs). Various discriminative learning methods have attempted to deal with the first of these issues, often by restricting the space of examples. For instance, some max-margin methods restrict their computations to a set of examples from a “feasible set,” where they are expected to be maximally discriminative (Tillmann and Zhang, 2006). The present approach deals with the second issue: in a learning problem where the use of a single positive example is likely to be highly biased, how can we produce a set of positive examples that is more representative of the space of correct outcomes? Our method exploits alternative sources of information to produce new positive examples that are, we hope, reasonably likely to represent a consensus of good examples. Quite a bit of work has been done on paraphrase, 6 We anticipate doing significance tests for differences in TER in future work. some clearly related to our technique, although"
W07-0716,W04-3250,0,0.0352334,"d in expanding the reference set via paraphrase: • Expanded (4H + 4P): This is the same as Condition 2, but using all four human references. Note that since we have only four human references per item, this fourth condition does not permit comparison with an upper bound of eight human references. Table 4 shows BLEU and TER scores on the test set for all four conditions.5 If only two human references were available (simulated by using only two of the available four), expanding to four using paraphrases would yield a clear improvement. Using bootstrap resampling to compute confidence intervals (Koehn, 2004), we find that the improvement in BLEU score is statistically significant at p < .01. Equally interesting, expanding the number of reference translations from two to four using paraphrases yields performance that approaches the upper bound obtained by doing MERT using all four human reference translations. The difference in BLEU between conditions 2 and 3 is not significant. Finally, our fourth condition asks whether it is possible to improve MT performance given the typical four human reference translations used for MERT in most statistical MT systems, by adding a paraphrase to each one for a"
W07-0716,2006.iwslt-papers.3,0,0.0336729,"Missing"
W07-0716,N06-1057,0,0.159707,"Missing"
W09-0426,P01-1008,0,0.0294189,"ion baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model for lattice system, we segmented the training data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to th"
W09-0426,N06-1003,0,0.022021,"ibed in the previous section. German Hungarian Condition baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model for lattice system, we segmented the training data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likel"
W09-0426,J07-2003,0,0.14396,"inning with a convention hierarchical phrase-based system, we found benefits for using word segmentation lattices as input, explicit generation of beginning and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseli"
W09-0426,P07-1090,1,0.847739,"ed to model the phrases being moved. The feature assesses the quality of a reordering by looking at the phrase alignment between pairs of Table 2: Impact of modeling sentence boundaries. 3.3 Dominance feature Source language paraphrases In order to deal with the sparsity associated with a rich source language morphology and limitedsize parallel corpora (bitexts), we experimented with a novel approach to paraphrasing out-ofvocabulary (OOV) source language phrases in 147 Source function words. In our experiments, we treated the 128 most frequent words in the corpus as function words, similar to Setiawan et al. (2007). Due to space constraints, we will discuss the details in another publication. As Table 3 reports, the use of this feature yields positive results. Source German Hungarian Condition baseline +dom baseline +dom BLEU TER 21.6 22.2 12.8 12.6 60.1 59.8 70.4 70.0 German Hungarian BLEU TER 22.2 22.6 12.6 12.8 59.8 59.4 70.0 69.8 Table 4: Performance of maximum derivation vs. MBR decoders. 4 Conclusion Table 5 summarizes the impact on the dev-test set of all features included in the University of Maryland system submission. Table 3: Impact of alignment dominance feature. Condition 3.5 Decoder Max-D"
W09-0426,J93-1003,0,0.094686,"e onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to the grammar in order to provide mappings to English for OOV Hungarian phrases. This attempt at monolingually-derived sourceside paraphrasing did not yield improvements over baseline. Preliminary analysis suggests that"
W09-0426,P08-1115,1,0.844708,". 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive compounding, we employ word segmentation lattices, which are word lattices that encode alternative possible segmentations of compound words. Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008). To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algor"
W09-0426,2006.amta-papers.25,0,0.027205,"from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning: − BLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric HTER. In this paper, we report uncased TER and BLEU individually. 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive compounding, we emplo"
W09-0426,E03-1076,0,0.0193663,"several morphemes to form a single orthographic token. To deal with productive compounding, we employ word segmentation lattices, which are word lattices that encode alternative possible segmentations of compound words. Doing so enables us to use possibly inaccurate approaches to guess the segmentation of compound words, allowing the decoder to decide which to use during translation. This is a further development of our general source-lattice approach to decoding (Dyer et al., 2008). To construct the segmentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test dat"
W09-0426,I08-1066,0,0.0257269,"first word, meaning that the exact computation must deferred, which makes pruning a challenge. In typical CKY decoders, the beginning and ends of the sentence (which often have special characteristics) are not conclusively determined until the whole sentence has been translated and the probabilities for the beginning and end sentence probabilities can be added. However, by this point it is often the case that a possibly better sentence beginning has been pruned away. To address this, we explicitly generate beginning and end sentence markers as part of the translation process, as suggested by Xiong et al. (2008). The results of doing this are shown in Table 2. Source German Hungarian Condition baseline +boundary baseline +boundary BLEU TER 21.3 21.6 12.3 12.8 59.9 60.1 70.4 70.4 3.4 Although our baseline hierarchical system permits long-range reordering, it lacks a mechanism to identify the most appropriate reordering for a specific sentence translation. For example, when the most appropriate reordering is a long-range one, our baseline system often also has to consider shorter-range reorderings as well. In the worst case, a shorter-range reordering has a high probability, causing the wrong reorderin"
W09-0426,W08-0318,0,0.0155767,"ing the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative frequency P (¯ e|f¯), the “lexical” translation probabilities Plex (¯ e|f¯) and Plex (f¯|¯ e), a rule coun"
W09-0426,N04-1022,0,0.0472478,"garian BLEU TER 22.2 22.6 12.6 12.8 59.8 59.4 70.0 69.8 Table 4: Performance of maximum derivation vs. MBR decoders. 4 Conclusion Table 5 summarizes the impact on the dev-test set of all features included in the University of Maryland system submission. Table 3: Impact of alignment dominance feature. Condition 3.5 Decoder Max-D MBR Max-D MBR Minimum Bayes risk decoding baseline +lattices +boundary +dom +MBR Although during minimum error training we assume a decoder that uses the maximum derivation decision rule, we find benefits to translating using a minimum risk decision rule on a test set (Kumar and Byrne, 2004). This seeks the translation E of the input lattice F that has the least expected loss, measured by some loss function L: German Hungarian BLEU TER BLEU TER 20.8 21.3 21.6 22.2 22.6 60.7 59.9 60.1 59.8 59.4 11.0 12.3 12.8 12.6 12.8 71.1 70.4 70.4 70.0 69.8 Table 5: Summary of all features Acknowledgments 0 ˆ = arg min EP (E|F ) [L(E, E )] E E0 X = arg min P (E|F)L(E, E 0 ) 0 E (1) This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-2-001, and the Army Research Laboratory. Any opinions, findings, conclusions or recommen"
W09-0426,D07-1104,0,0.0211523,"rs, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative frequency P (¯ e|f¯), the “lexical” translation probabilities Plex (¯ e|f¯) and Plex (f¯|¯ e), a rule count, a target language word count, the target (English) language model P (eI1 ), and a “passthrough” penalty for passing a source language word to the target side.1 The rule feature values were computed online during decoding using the suffix array method described by Lopez (2007). Introduction For the shared translation task of the Fourth Workshop on Machine Translation (WMT09), we focused on two tasks: German to English and Hungarian to English translation. Despite belonging to different language families, German and Hungarian have three features in common that complicate translation into English: 1. productive nouns), compounding (especially Baseline system of 2. rich inflectional morphology, 3. widespread mid- to long-range word order differences with respect to English. 1 The “pass-through” penalty was necessary since the English language modeling data contained a"
W09-0426,C08-1064,0,0.012091,"and end of sentence markers, minimum Bayes risk decoding, and incorporation of a feature scoring the alignment of function words in the hypothesized translation. We also explored the use of monolingual paraphrases to improve coverage, as well as co-training to improve the quality of the segmentation lattices used, but these did not lead to improvements. 1 2 Our translation system makes use of a hierarchical phrase-based translation model (Chiang, 2007), which we argue is a strong baseline for these language pairs. First, such a system makes use of lexical information when modeling reordering (Lopez, 2008), which has previously been shown to be useful in German-to-English translation (Koehn et al., 2008). Additionally, since the decoder is based on a CKY parser, it can consider all licensed reorderings of the input in polynomial time, and German and Hungarian may require quite substantial reordering. Although such decoders and models have been common for several years, there have been no published results for these language pairs. The baseline system translates lowercased and tokenized source sentences into lowercased target sentences. The features used were the rule translation relative freque"
W09-0426,D08-1076,0,0.0484393,"mentation lattices, we define a log-linear model of compound word segmentation inspired by Koehn and Knight (2003), making use of features including number of morphemes hypothesized, frequency of the segments as free-standing morphemes in a training corpus, and letters in each segment. To tune the model parameters, we selected a set of compound words from a subset of the German development set, manually created a linguistically plausible segmentation of these words, and used this to select the parameters of the log-linear model using a lattice minimum error training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low-probability segmentation paths (Sixtus and Ortmanns, 1999). We then concatenated the lattices in each sentence. Automatic evaluation metric TER Experimental variations Forest minimum error training Source To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the targe"
W09-0426,J03-1002,0,0.0041536,"omena are poorly addressed with conventional approaches to statistical machine Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 145–149, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 145 2.1 3 Training and development data This section describes the experimental variants explored. To construct the translation suffix arrays used to compute the translation grammar, we used the parallel training data provided. The preprocessed training data was filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) in both directions and symmetrized using the grow-diag-final-and heuristic. We trained a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we"
W09-0426,P03-1021,0,0.0131462,"r training algorithm to minimize WER (Macherey et al., 2008). We reused the same features and weights to create the Hungarian lattices. For the test data, we created a lattice of every possible segmentation of any word 6 characters or longer and used forward-backward pruning to prune out low-probability segmentation paths (Sixtus and Ortmanns, 1999). We then concatenated the lattices in each sentence. Automatic evaluation metric TER Experimental variations Forest minimum error training Source To tune the feature weights of our system, we used a variant of the minimum error training algorithm (Och, 2003) that computes the error statistics from the target sentences from the translation search space (represented by a packed forest) that are exactly those that are minimally discriminable by changing the feature weights along a single vector in the dimensions of the feature space (Macherey et al., 2008). The loss function we used was the linear combination of TER and BLEU described in the previous section. German Hungarian Condition baseline lattice baseline lattice BLEU TER 20.8 21.3 11.0 12.3 60.7 59.9 71.1 70.4 Table 1: Impact of compound segmentation lattices. To build the translation model f"
W09-0426,P02-1040,0,0.0782984,"a 5-gram language model from the provided English monolingual training data and the non-Europarl portions of the parallel training data using modified Kneser-Ney smoothing as implemented in the SRI language modeling toolkit (Kneser and Ney, 1995; Stolcke, 2002). We divided the 2008 workshop “news test” sets into two halves of approximately 1000 sentences each and designated one the dev set and the other the dev-test set. 2.2 3.1 Since the official evaluation criterion for WMT09 is human sentence ranking, we chose to minimize a linear combination of two common evaluation metrics, BLEU and TER (Papineni et al., 2002; Snover et al., 2006), during system development and tuning: − BLEU 2 Although we are not aware of any work demonstrating that this combination of metrics correlates better than either individually in sentence ranking, Yaser Al-Onaizan (personal communication) reports that it correlates well with the human evaluation metric HTER. In this paper, we report uncased TER and BLEU individually. 2.3 Word segmentation lattices Both German and Hungarian have a large number of compound words that are created by concatenating several morphemes to form a single orthographic token. To deal with productive"
W09-0426,P99-1067,0,0.0414102,"data using the onebest split predicted by the segmentation model, 146 and word aligned this with the English side. This variant version of the training data was then concatenated with the baseline system’s training data. 3.1.1 our Hungarian-English system, using monolingual contextual similarity rather than phrase-table pivoting (Callison-Burch et al., 2006) or monolingual bitexts (Barzilay and McKeown, 2001; Dolan et al., 2004). Distributional profiles for source phrases were represented as context vectors over a sliding window of size 6, with vectors defined using log-likelihood ratios (cf. Rapp (1999), Dunning (1993)) but using cosine rather than cityblock distance to measure profile similarity. The 20 distributionally most similar source phrases were treated as paraphrases, considering candidate phrases up to a width of 6 tokens and filtering out paraphrase candidates with cosine similarity to the original of less than 0.6. The two most likely translations for each paraphrase were added to the grammar in order to provide mappings to English for OOV Hungarian phrases. This attempt at monolingually-derived sourceside paraphrasing did not yield improvements over baseline. Preliminary analysi"
W09-0426,C04-1051,0,\N,Missing
W09-0426,P07-1065,0,\N,Missing
W09-0426,P06-1002,0,\N,Missing
W09-0426,N03-1017,0,\N,Missing
W09-0426,P10-4002,1,\N,Missing
W09-0426,P07-1019,0,\N,Missing
W09-0426,N09-1046,1,\N,Missing
W09-0426,P09-1019,1,\N,Missing
W10-0723,J93-1003,0,0.105507,"entences this way so that the collection has variety, while including enough examples for individual categories. Our goal was to gather at least 1,000 annotated sentences; ultimately we collected 1,041. The categories are as follows. “Sticky” partisan bigrams. One likely indicator of bias is the use of terms that are particular to one side or the other in a debate (Monroe et al., 2008). In order to identify such terms, we independently created two lists of “sticky” (i.e., strongly associated) bigrams in liberal and conservative subcorpora, measuring association using the log-likelihood ratio (Dunning, 1993) and omitting bigrams containing stopwords.11 We identified a bigram as “liberal” if it was among the top 1,000 bigrams from the liberal blogs, as measured by strength of association, and was also not among the top 1,000 bigrams on the conservative side. The reverse definition yielded the “conservative” bigrams. The resulting liberal list contained 495 bigrams, and the conservative list contained 539. We then manually filtered cases that were clearly remnant HTML tags and other markup, arriving at lists of 433 and 535, respectively. Table 1 shows the strongest weighted bigrams. As an example,"
W10-0723,N09-1057,1,0.839043,"egories of words from Pennebaker’s LIWC dictionary: Negative Emotion, Positive Emotion, Causation, and Anger.12 The following is one example of a biased sentence in our dataset that matched these lexicons, in this case the Anger category; the match is in bold. A bunch of ugly facts are nailing the biggest scare story in history. The five most frequent matches in the corpus for each category are as follows.13 Negative Emotion: war attack* problem* numb* argu* Positive Emotion: like well good party* secur* Causation: how because lead* make why Anger: war attack* argu* fight* threat* Kill verbs. Greene and Resnik (2009) discuss the relevance of syntactic structure to the perception of sentiment. For example, their psycholinguistic experiments would predict that when comparing Millions of people starved under Stalin (inchoative) with Stalin starved millions of people (transitive), the latter will be perceived as more negative toward Stalin, because the transitive syntactic frame tends to be connected with semantic properties such as intended action by the subject and change of state in the object. “Kill verbs” provide particularly strong examples of such phenomena, because they exhibit a large set of semantic"
W10-0723,P99-1032,0,0.110884,"Missing"
W10-0723,N07-1033,0,0.0141441,"a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. 1 • The nature of the bias (very liberal, moderately liberal, moderately conservative, very conservative, biased but not sure which direction); and • Which words in the sentence give away the author’s bias, similar to “rationale” annotations in Zaidan et al. (2007). For example, a participant might identify a moderate liberal bias in this sentence, Introduction Bias and framing are central topics in the study of communications, media, and political discourse (Scheufele, 1999; Entman, 2007), but they have received relatively little attention in computational linguistics. What are the linguistic indicators of bias? Are there lexical, syntactic, topical, or other clues that can be computationally modeled and automatically detected? Here we use Amazon Mechanical Turk (MTurk) to engage in a systematic, empirical study of linguistic indicators of bias in the"
W10-0730,N09-1057,1,0.929182,"e) and captures how much “action” takes place in a sentence. Such notions of Transitivity are not apparent from surface features alone; identical syntactic constructions can have vastly different Transitivity. This well-established linguistic theory, however, is not useful for real-world applications without a Transitivity-annotated corpus. Given such a substantive corpus, conventional machine learning techniques could help determine the Transitivity of verbs within sentences. Transitivity has been found to play a role in what is called “syntactic framing,” which expresses implicit sentiment (Greene and Resnik, 2009). 1 We use capital “T” to differentiate from conventional syntactic transitivity throughout the paper. Table 1 shows the subset of the Hopper-Thompson dimensions of Transitivity used in this study. We excluded noun-specific aspects as we felt that these were well covered by existing natural language processing (NLP) approaches (e.g. whether the object / subject is person, abstract entity, or abstract concept is handled well by existing named entity recognition systems) and also excluded aspects which we felt had significant overlap with the dimensions we were investigating (e.g. affirmation an"
W10-0730,P03-1054,0,0.00337267,"and that raters were less confident about their answers, prompting more hedging and a flat distribution. 3.4 Predicting Transitivity We also performed an set of initial experiments to investigate our ability to predict Transitivity values for held out data. We extracted three sets of features from the sentences: lexical features, syntactic features, and features derived from WordNet (Miller, 1990). Lexical Features A feature was created for each word in a sentence after being stemmed using the Porter stemmer (Porter, 1980). Syntactic Features We parsed each sentence using the Stanford Parser (Klein and Manning, 2003) and used heuristics to identify cases where the main verb is transitive, where the subject is a nominalization (e.g. “running”), or whether the sentence is passive. If any of these constructions appear in the sentence, we generate a corresponding feature. These represent features identified by Greene and Resnik (2009). WordNet Features For each word in the sentence, we extracted all the possible senses for each word. If any possible sense was a hyponym (i.e. an instance of) one of: artifact, living thing, abstract entity, location, or food, we added a feature corresponding to that top level s"
W10-0735,N06-1003,0,0.0316767,"tial advantage for translation systems, namely that some variations may be more easily translated than others depending on the training data that was given to the system, and we can improve translation quality by allowing a system to take best advantage of the variations it knows about, at the subsentential level, just as the systems described above can take advantage of alternative segmentations. Paraphrase lattices provide a way to make this hypothesis operational. This idea is a variation on the uses of paraphrase in translation introduced by Callison-Burch and explored by others, as well (Callison-Burch et al., 2006; Madnani et al., 2007; Callison-Burch, 2008; Marton et al., 2009). These authors have shown that performance improvements can be gained by exploiting paraphrases using phrase pivoting. We have investigated using pivoting to create exhaustive paraphrase lattices, and we have also investigated defining upper bounds by eliciting human sub-sentential paraphrases using Mechanical Turk. Unfortunately, in both cases, we have found the size of the paraphrase lattice prohibitive: there are 217 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk"
W10-0735,D08-1021,0,0.0288325,"some variations may be more easily translated than others depending on the training data that was given to the system, and we can improve translation quality by allowing a system to take best advantage of the variations it knows about, at the subsentential level, just as the systems described above can take advantage of alternative segmentations. Paraphrase lattices provide a way to make this hypothesis operational. This idea is a variation on the uses of paraphrase in translation introduced by Callison-Burch and explored by others, as well (Callison-Burch et al., 2006; Madnani et al., 2007; Callison-Burch, 2008; Marton et al., 2009). These authors have shown that performance improvements can be gained by exploiting paraphrases using phrase pivoting. We have investigated using pivoting to create exhaustive paraphrase lattices, and we have also investigated defining upper bounds by eliciting human sub-sentential paraphrases using Mechanical Turk. Unfortunately, in both cases, we have found the size of the paraphrase lattice prohibitive: there are 217 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 217–221, c Los Angeles, California,"
W10-0735,P08-1115,1,0.116208,"tion is familiar to most statistical MT researchers in the form of preprocessing choices — for example, one segmentation of a Chinese sentence might yield better translations than another.1 Over the past several years, MT frameworks have been developed that permit all the alternatives to be used as input, represented efficiently as a confusion network, lattice, or forest, rather than forcing selection of a single input representation. This has improved performance when applied to phenomena including segmentation, morphological analysis, and more recently source langage word order (Dyer, 2007; Dyer et al., 2008; Dyer and Resnik, to appear). We have begun to explore the application of the same key idea beyond low-level processing phenomena such as segmentation, instead looking at alternative expressions of meaning. For example, consider translating The 1 Chinese is written without spaces, so most MT systems need to segment the input into words as a preprocessing step. These examples illustrate lexical variation, as well as syntactic differences, e.g. whether the attacking or the increasing serves as the main verb. We hypothesize that variation of this kind holds a potential advantage for translation"
W10-0735,W07-0729,0,0.11066,"This observation is familiar to most statistical MT researchers in the form of preprocessing choices — for example, one segmentation of a Chinese sentence might yield better translations than another.1 Over the past several years, MT frameworks have been developed that permit all the alternatives to be used as input, represented efficiently as a confusion network, lattice, or forest, rather than forcing selection of a single input representation. This has improved performance when applied to phenomena including segmentation, morphological analysis, and more recently source langage word order (Dyer, 2007; Dyer et al., 2008; Dyer and Resnik, to appear). We have begun to explore the application of the same key idea beyond low-level processing phenomena such as segmentation, instead looking at alternative expressions of meaning. For example, consider translating The 1 Chinese is written without spaces, so most MT systems need to segment the input into words as a preprocessing step. These examples illustrate lexical variation, as well as syntactic differences, e.g. whether the attacking or the increasing serves as the main verb. We hypothesize that variation of this kind holds a potential advanta"
W10-0735,W07-0716,1,0.729283,"n systems, namely that some variations may be more easily translated than others depending on the training data that was given to the system, and we can improve translation quality by allowing a system to take best advantage of the variations it knows about, at the subsentential level, just as the systems described above can take advantage of alternative segmentations. Paraphrase lattices provide a way to make this hypothesis operational. This idea is a variation on the uses of paraphrase in translation introduced by Callison-Burch and explored by others, as well (Callison-Burch et al., 2006; Madnani et al., 2007; Callison-Burch, 2008; Marton et al., 2009). These authors have shown that performance improvements can be gained by exploiting paraphrases using phrase pivoting. We have investigated using pivoting to create exhaustive paraphrase lattices, and we have also investigated defining upper bounds by eliciting human sub-sentential paraphrases using Mechanical Turk. Unfortunately, in both cases, we have found the size of the paraphrase lattice prohibitive: there are 217 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 217–221, c Los"
W10-0735,D09-1040,1,0.161505,"e more easily translated than others depending on the training data that was given to the system, and we can improve translation quality by allowing a system to take best advantage of the variations it knows about, at the subsentential level, just as the systems described above can take advantage of alternative segmentations. Paraphrase lattices provide a way to make this hypothesis operational. This idea is a variation on the uses of paraphrase in translation introduced by Callison-Burch and explored by others, as well (Callison-Burch et al., 2006; Madnani et al., 2007; Callison-Burch, 2008; Marton et al., 2009). These authors have shown that performance improvements can be gained by exploiting paraphrases using phrase pivoting. We have investigated using pivoting to create exhaustive paraphrase lattices, and we have also investigated defining upper bounds by eliciting human sub-sentential paraphrases using Mechanical Turk. Unfortunately, in both cases, we have found the size of the paraphrase lattice prohibitive: there are 217 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 217–221, c Los Angeles, California, June 2010. 2010 Associ"
W10-0735,P02-1050,1,\N,Missing
W10-0735,N04-1021,0,\N,Missing
W10-1707,N03-1017,0,0.00664368,"ith a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We u"
W10-1707,N04-1022,0,0.033198,"gmented the training data using the 1-best segmentation predicted by the segmentation model, and word aligned this with the English side. This version of the parallel corpus was concatenated with the original training parallel corpus. 3 Language Model RandLM SRILM TER 22.4 23.1 69.1 68.0 Table 1: Impact of language model on translation 3.2 Experimental variation Minimum Bayes risk decoding During minimum error rate training, the decoder employs a maximum derivation decision rule. However, upon exploration of alternative strategies, we have found benefits to using a minimum risk decision rule (Kumar and Byrne, 2004), wherein we want the translation E of the input F that has the least expected loss, again as measured by some loss function L: This section describes the experiments we performed in attempting to assess the challenges posed by current methods and our exploration of new ones. 3.1 BLEU Bloom filter language model Language models play a crucial role in translation performance, both in terms of quality, and in terms of practical aspects such as decoder memory usage and speed. Unfortunately, these two concerns tend to trade-off one another, as increasing to a higher-order more complex language mod"
W10-1707,J03-1002,0,0.00355772,"ch with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demon"
W10-1707,P06-1002,0,0.0129619,"overhead. The RandLM provides a median between the two extremes: reduced memory and (relatively) fast decoding at the price of somewhat decreased translation quality. Since we are using a relatively large beam of 1000 candidates for decoding, the time presented in Table 3 does not represent an accurate basis for comparison of cdec to other decoders, which should be done using the results presented in Dyer et al. (2010). We also tried one other grammar extraction configuration, which was with so-called ‘loose’ phrase extraction heuristics, which permit unaligned words at the edges of phrases (Ayan and Dorr, 2006). When decoded using the SRILM and MBR, this achieved the best performance for our system, with a BLEU score of 23.6 and TER of 67.7. 4 Conclusion We presented the University of Maryland hierarchical phrase-based system for the WMT2010 shared translation task. Using cdec, we experimented with a number of methods that are shown above to lead to improved German-to-English translation quality over our baseline according to BLEU and TER evaluation. These include methods to directly address German morphological complexity, such as appropriate feature functions, segmentation lattices, and a model fo"
W10-1707,P03-1021,0,0.0385487,"ng of all possible segmentations for every word consisting of more than 6 letters was created, and the paths were weighted by the posterior probability assigned by the segmentation model. Then, max-marginals were computed using the forward-backward algorithm and used to prune out paths that were greater than a factor of 2.3 from the best path, as recommended by Dyer Viterbi envelope semiring training To optimize the feature weights for our model, we use Viterbi envelope semiring training (VEST), which is an implementation of the minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. VEST reinterprets MERT within a semiring framework, which is a useful mathematical abstraction for defining two general operations, addition (⊕) and multiplication (⊗) over a set of values. Formally, a semiring is a 5-tuple (K, ⊕, ⊗, 0, 1), where addition must be commu2 This algorithm is equivalent to the hypergraph MERT algorithm described by Kumar et al. (2009). 3 The reference segmentation lattices used for training are available in the cdec distribution. 73 (2009).4 To create the translation model for lattice input, we segmented the training d"
W10-1707,P96-1041,0,0.088436,"rl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method (Koehn et al., 2003). We constructed a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the provided English monolingual training data and the non-Europarl portions of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). Since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, and have previously been demonstrated to significantly improve performance (Dyer et al., 2009), we explicitly annotate beginning and end of sentence markers as part of our translation process. We used the 2525 sentences in newstest2009 as our dev set on which we tuned the feature weights, and report results on the 2489 sentences of the news-test2010 test set. 2.2 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of pr"
W10-1707,P02-1040,0,0.0920472,"icted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized b"
W10-1707,J07-2003,0,0.0447776,"rchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method"
W10-1707,2006.amta-papers.25,0,0.0211012,"ls which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to create our RandLM.5 Table 1 shows the comparison of SRILM and RandLM with respect to performance on BLEU and TER (Snover et al., 2006) on the test set. b = arg min EP (E|F ) [L(E, E 0 )] E E0 X = arg min P (E|F )L(E, E 0 ) 0 E E Using our system, we generate a unique 500best list of translations to approximate the posterior distribution P (E|F ) and the set of possible translations. Assuming H(E, F ) is the weight of the decoder’s current path, this can be written as: P (E|F ) ∝ exp αH(E, F ) where α is a free parameter which depends on the models feature functions and weights as well as pruning method employed, and thus needs to be separately empirically optimized on a held out development set. For this submission, we used"
W10-1707,W09-0426,1,0.791239,"hrase-based translation model, which is formally based on the notion of a synchronous context-free grammar (SCFG) (Chiang, 2007). These grammars contain pairs of CFG rules with aligned nonterminals, and by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting"
W10-1707,P10-4002,1,0.884705,"ntroducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to be well suited for German-English translation, as they are able to both exploit lexical information for and efficiently compute all possible reorderings using a CKY-based decoder (Dyer et al., 2009). Our system is implemented within cdec, an efficient and modular open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs (Dyer et al., 2010).1 cdec’s modular framework facilitates seamless integration of a translation model with different language models, pruning strategies and inference algorithms. As input, cdec expects a string, lattice, or context-free forest, and uses it to generate a hypergraph representation, which represents the full translation forest without any pruning. The forest can now be rescored, by intersecting it with a language model for instance, to obtain output translations. The above capabilities of cdec allow us to perform the experiments described below, which would otherwise be quite cumbersome to carry o"
W10-1707,P07-1065,0,0.0232787,"ch as decoder memory usage and speed. Unfortunately, these two concerns tend to trade-off one another, as increasing to a higher-order more complex language model improves performance, but comes at the cost of increased size and difficulty in deployment. Ideally, the language model will be loaded into memory locally by the decoder, but given memory constraints, it is entirely possible that the only option is to resort to a remote language model server that needs to be queried, thus introducing significant decoding speed delays. One possible alternative is a randomized language model (RandLM) (Talbot and Osborne, 2007). Using Bloom filters, which are a randomized data structure for set representation, we can construct language models which significantly decrease space requirements, thus becoming amenable to being stored locally in memory, while only introducing a quantifiable number of false positives. In order to assess what the impact on translation quality would be, we trained a system identical to the one described above, except using a RandLM. Conveniently, it is possible to construct a RandLM directly from an existing SRILM, which is the route we followed in using the SRILM described in Section 2.1 to"
W10-1707,N09-1046,1,0.817241,"sults on the 2489 sentences of the news-test2010 test set. 2.2 2.3 Compound segmentation lattices To deal with the aforementioned problem in German of productive compounding, where words are formed by the concatenation of several morphemes and the orthography does not delineate the morpheme boundaries, we utilize word segmentation lattices. These lattices serve to encode alternative ways of segmenting compound words, and as such, when presented as the input to the system allow the decoder to automatically choose which segmentation is best for translation, leading to markedly improved results (Dyer, 2009). In order to construct diverse and accurate segmentation lattices, we built a maximum entropy model of compound word splitting which makes use of a small number of dense features, such as frequency of hypothesized morphemes as separate units in a monolingual corpus, number of predicted morphemes, and number of letters in a predicted morpheme. The feature weights are tuned to maximize conditional log-likelihood using a small amount of manually created reference lattices which encode linguistically plausible segmentations for a selected set of compound words.3 To create lattices for the dev and"
W10-1707,P07-1019,0,0.0279095,"The hierarchical phrase-base translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). 2.1 nicative and associative, multiplication must be associative and must distribute over addition, and an identity element exists for both. For VEST, having K be the set of line segments, ⊕ be the union of them, and ⊗ be Minkowski addition of the lines represented as points in the dual plane, allows us to compute the necessary MERT line search with the INSIDE algorithm.2 The error function we use is BLEU (Papineni et al., 2002), and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. During decoding of the test set, we raise the cube pruning limit to 1000 candidates at each node. Data preparation In order to extract the translation grammar necessary for our model, we used the provided Europarl and News Commentary parallel training data. The lowercased and tokenized training data was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized by combining both into a single alignment using the grow-diagfinal-and method"
W11-2140,P96-1041,0,0.0567336,"ch we extracted our grammar consisted of 123,609 sentence pairs, which was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in either direction and symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the English monolingual News Commentary and News Crawl language modeling training data provided for the shared task and the English portion of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We have previously found that since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, explicitly annotating beginning and end of sentence markers as part of our translation process leads to significantly improved performance (Dyer et al., 2009). A further difficulty of the task stems from the fact that there are two versions of the SMS test set, a raw version, which contains the original messages, and a clean version which was post-edited by humans. As the evaluation of the task will consist of translating"
W11-2140,J07-2003,0,0.302493,"a lowdensity language, but additionally, with noisy, realworld data in a domain which has thus far received relatively little attention in statistical machine translation. We were especially interested in this task because of the unique set of challenges that it poses for existing translation systems. We focused our research effort on techniques to better utilize the limited available training resources, as well as ways in System Overview Our baseline system is based on a hierarchical phrase-based translation model, which can formally be described as a synchronous context-free grammar (SCFG) (Chiang, 2007). Our system is implemented in cdec, an open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs. (Dyer et al., 2010). 1 SCFG grammars contain pairs of CFG rules with aligned nonterminals, where by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to produce state-of-the-art translation for most language pairs, as they are capable of both exploiting lexical information fo"
W11-2140,W09-0426,1,0.845646,"ning, and decoding with a number of different translation models, including SCFGs. (Dyer et al., 2010). 1 SCFG grammars contain pairs of CFG rules with aligned nonterminals, where by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to produce state-of-the-art translation for most language pairs, as they are capable of both exploiting lexical information for and efficiently computing all possible reorderings using a CKY-based decoder (Dyer et al., 2009). 1 http://cdec-decoder.org 344 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 344–350, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics One benefit of cdec is the flexibility allowed with regard to the input format, as it expects either a string, lattice, or context-free forest, and subsequently generates a hypergraph representing the full translation forest without any pruning. This forest can now be rescored, by intersecting it with a language model for instance, to obtain output translations. These capabilities of cdec a"
W11-2140,P10-4002,1,0.858657,"re especially interested in this task because of the unique set of challenges that it poses for existing translation systems. We focused our research effort on techniques to better utilize the limited available training resources, as well as ways in System Overview Our baseline system is based on a hierarchical phrase-based translation model, which can formally be described as a synchronous context-free grammar (SCFG) (Chiang, 2007). Our system is implemented in cdec, an open source framework for aligning, training, and decoding with a number of different translation models, including SCFGs. (Dyer et al., 2010). 1 SCFG grammars contain pairs of CFG rules with aligned nonterminals, where by introducing these nonterminals into the grammar, such a system is able to utilize both word and phrase level reordering to capture the hierarchical structure of language. SCFG translation models have been shown to produce state-of-the-art translation for most language pairs, as they are capable of both exploiting lexical information for and efficiently computing all possible reorderings using a CKY-based decoder (Dyer et al., 2009). 1 http://cdec-decoder.org 344 Proceedings of the 6th Workshop on Statistical Machi"
W11-2140,2007.iwslt-1.28,0,0.0243498,"Missing"
W11-2140,N09-1046,0,0.01664,"quality translation, it may be of use to be able to utilize both segmentations and allow the decoder to learn the appropriate one. In previous work, word segmentation lattices have been used to address the problem of productive compounding in morphologically rich languages, such as German, where morphemes are combined to make words but the orthography does not delineate the morpheme boundaries. These lattices encode alternative ways of segmenting compound words, and allow the decoder to automatically choose which segmentation is best for translation, leading to significantly improved results (Dyer, 2009). As opposed to building word segmentation lattices from a linguistic morphological analysis of a compound word, we propose to utilize the lattice to encode all alternative ways of segmenting a word as presented to us in either the clean or raw versions of a sentence. As the task requires us to produce separate clean and raw output on the test set, we tune one system on a lattice built from the clean and raw dev set, and use the single system to decode both the clean and raw test set separately. Table 5 presents the results of using segmentation lattices. 3.5 Raw-to-Clean Transformation Lattic"
W11-2140,P07-1019,0,0.0247353,"s used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-based translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). To optimize the feature weights for our model, we used an implementation of the hypergraph minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. The error function we used was BLEU (Papineni et al., 2002), and the decoder was configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. 2.1 Data Preparation The SMS messages were originally translated by English speaking volunteers for the purpose of providing first responders with information and locations requiring their assistance. As such, in order to create a suitable parallel training corpus from which to extract a translation grammar, a number of steps had to be taken in addition to lowercasing and tokenizing both sides of training data. Many of the English translations had additional notes sections that were added by the translator to the messages with either personal notes"
W11-2140,N03-1017,0,0.0044238,"lent and had to be preserved as they were. Since the total amount of HaitianEnglish parallel data provided is quite limited, we found additional data and augmented the available set with data gathered by the CrisisCommons group and made it available to other WMT participants. The combined training corpus from which we extracted our grammar consisted of 123,609 sentence pairs, which was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in either direction and symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the English monolingual News Commentary and News Crawl language modeling training data provided for the shared task and the English portion of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We have previously found that since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the context of the model, explicitly annotating beginning and end of sentence markers as part of our translation process leads to sign"
W11-2140,D07-1104,0,0.0156372,"carry out in another system. The set of features used in our model were the rule translation relative frequency P (e|f ), a target n-gram language model P (e), lexical translation probabilities Plex (e|f ) and Plex (f |e), a count of the total number of rules used, a target word penalty, and a count of the number of times the glue rule is used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-based translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). To optimize the feature weights for our model, we used an implementation of the hypergraph minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. The error function we used was BLEU (Papineni et al., 2002), and the decoder was configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. 2.1 Data Preparation The SMS messages were originally translated by English speaking volunteers for the purpose of providing first responders with information and locations requiring their assistance. As"
W11-2140,J03-1002,0,0.00439587,"removed. Furthermore, the anonymization of the data 345 resulted in tokens such as firstname and phonenumber which were prevalent and had to be preserved as they were. Since the total amount of HaitianEnglish parallel data provided is quite limited, we found additional data and augmented the available set with data gathered by the CrisisCommons group and made it available to other WMT participants. The combined training corpus from which we extracted our grammar consisted of 123,609 sentence pairs, which was then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in either direction and symmetrized using the grow-diag-final-and method (Koehn et al., 2003). We trained a 5-gram language model using the SRI language modeling toolkit (Stolcke, 2002) from the English monolingual News Commentary and News Crawl language modeling training data provided for the shared task and the English portion of the parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We have previously found that since the beginnings and ends of sentences often display unique characteristics that are not easily captured within the con"
W11-2140,P03-1021,0,0.0156839,"ranslation probabilities Plex (e|f ) and Plex (f |e), a count of the total number of rules used, a target word penalty, and a count of the number of times the glue rule is used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-based translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). To optimize the feature weights for our model, we used an implementation of the hypergraph minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. The error function we used was BLEU (Papineni et al., 2002), and the decoder was configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. 2.1 Data Preparation The SMS messages were originally translated by English speaking volunteers for the purpose of providing first responders with information and locations requiring their assistance. As such, in order to create a suitable parallel training corpus from which to extract a translation grammar, a number of steps had to be taken in addition to lowercasing"
W11-2140,P02-1040,0,0.0897142,"d, a target word penalty, and a count of the number of times the glue rule is used. The number of non-terminals allowed in a synchronous grammar rule was restricted to two, and the non-terminal span limit was 12 for non-glue grammars. The hierarchical phrase-based translation grammar was extracted using a suffix array rule extractor (Lopez, 2007). To optimize the feature weights for our model, we used an implementation of the hypergraph minimum error rate training (MERT) algorithm (Dyer et al., 2010; Och, 2003) for training with an arbitrary loss function. The error function we used was BLEU (Papineni et al., 2002), and the decoder was configured to use cube pruning (Huang and Chiang, 2007) with a limit of 100 candidates at each node. 2.1 Data Preparation The SMS messages were originally translated by English speaking volunteers for the purpose of providing first responders with information and locations requiring their assistance. As such, in order to create a suitable parallel training corpus from which to extract a translation grammar, a number of steps had to be taken in addition to lowercasing and tokenizing both sides of training data. Many of the English translations had additional notes sections"
W11-2148,W10-0735,1,0.897777,"Missing"
W11-2148,2004.eamt-1.4,0,0.0610774,"Missing"
W11-2148,P02-1050,1,0.741704,"an often identify when some part of a sentence doesn’t make sense, or when one sentence seems more fluent or plausible than another. Sometimes rather than identifying errors, it is easier to suggest an entirely new translation candidate based on the information available on the target side, a variant of monolingual post-editing (Callison-Burch et al., 2004). Any new translation candidates are then backtranslated into Haitian Creole, and any spans marked as translation errors are projected back to identify the corresponding spans in the source sentence, using word alignments as the bridge (cf. Hwa et al. (2002), Yarowsky et al. (2001)).2 The Haitian Creole speakers can then: • Rephrase the entire source sentence (cf. (Morita and Ishida, 2009)) • “Explain” spans marked as errors • Vote candidates up or down (based on the backtranslation) Source speakers can “explain” error spans by offering a different way of phrasing that piece of the source sentence (Resnik et al., 2010), in order to produce a new source sentence, or by annotating the spans with images (e.g. via Google image search) or Web links (e.g. to Wikipedia). The protocol then continues: new source sentences created via partial1 For the work"
W11-2148,2010.amta-workshop.1,0,0.0568659,"Missing"
W11-2148,D10-1013,1,0.844688,"new translation candidates are then backtranslated into Haitian Creole, and any spans marked as translation errors are projected back to identify the corresponding spans in the source sentence, using word alignments as the bridge (cf. Hwa et al. (2002), Yarowsky et al. (2001)).2 The Haitian Creole speakers can then: • Rephrase the entire source sentence (cf. (Morita and Ishida, 2009)) • “Explain” spans marked as errors • Vote candidates up or down (based on the backtranslation) Source speakers can “explain” error spans by offering a different way of phrasing that piece of the source sentence (Resnik et al., 2010), in order to produce a new source sentence, or by annotating the spans with images (e.g. via Google image search) or Web links (e.g. to Wikipedia). The protocol then continues: new source sentences created via partial1 For the work reported here, we used Google Translate as the MT component via the Google Translate Research API. 2 The Google Translate Research API provides alignments with its hypotheses. 400 or full-sentence paraphrase pass back through MT to the English side, and any explanatory annotations are projected back to the corresponding spans in the English candidate translations ("
W11-2148,H01-1035,0,0.0199265,"hen some part of a sentence doesn’t make sense, or when one sentence seems more fluent or plausible than another. Sometimes rather than identifying errors, it is easier to suggest an entirely new translation candidate based on the information available on the target side, a variant of monolingual post-editing (Callison-Burch et al., 2004). Any new translation candidates are then backtranslated into Haitian Creole, and any spans marked as translation errors are projected back to identify the corresponding spans in the source sentence, using word alignments as the bridge (cf. Hwa et al. (2002), Yarowsky et al. (2001)).2 The Haitian Creole speakers can then: • Rephrase the entire source sentence (cf. (Morita and Ishida, 2009)) • “Explain” spans marked as errors • Vote candidates up or down (based on the backtranslation) Source speakers can “explain” error spans by offering a different way of phrasing that piece of the source sentence (Resnik et al., 2010), in order to produce a new source sentence, or by annotating the spans with images (e.g. via Google image search) or Web links (e.g. to Wikipedia). The protocol then continues: new source sentences created via partial1 For the work reported here, we used"
W11-2148,2010.amta-workshop.3,1,\N,Missing
W13-2214,P07-1019,0,0.0376821,"rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012) with a single passive-aggressive update for each sentence (Eidelman, 2012). Compound segmentation lattices As German is a morphologically rich language with productive compounding, we use word segmentation lattices as input for the German translation task. These lattices encode alternative segmentations of compound words, allowing the decoder to automatically choose which segmentation is best. We use a m"
W13-2214,2006.amta-papers.25,0,0.0284509,"ng a weighted average to distribute as initial weights for the next iteration. We constructed a 5-gram language model using SRILM (Stolcke, 2002) from the provided English monolingual training data and parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996), which was binarized using KenLM (Heafield, 2011). The sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012)"
W13-2214,N03-1017,0,0.00610473,", context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl and News Commentary parallel training data to create the translation grammar necessary for our model. For Russian, we additionally used the Common Crawl and Yandex data. The data were lowercased and tokenized, then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized sing the grow-diag-final-and method (Koehn et al., 2003). 3 We approximate corpus B LEU by scoring sentences using a pseudo-document of previous 1-best translations (Chiang et al., 2009). 129 en de Dev 75k 74k Test 74k 73k Test2 27k 26k 5k 132k 133k 10k 255k 256k 25k 634k 639k 50k 1258k 1272k ru en 16 108k 16 77k Tune ↑B LEU 22.38 23.86 30.18 32.40 Test ↑B LEU 22.69 23.01 29.89 30.81 Test2 24k 25k 15k 350k 371k 24 ↓TER 60.61 59.89 49.05 48.40 λ=0.01 23.8 23.6 λ=0.1 23.4 BLEU de-en +sparse ru-en +sparse # features Test 24k 27k Table 2: Corpus statistics in tokens for Russian. Table 1: Corpus statistics in tokens for German. Set Dev 46k 50k 23.2 23 2"
W13-2214,D07-1104,0,0.0170173,"ights and decoding and updating parameters on a shard of the data. This is followed by a reduce phase, with a single reducer collecting final weights from all mappers and computing a weighted average to distribute as initial weights for the next iteration. We constructed a 5-gram language model using SRILM (Stolcke, 2002) from the provided English monolingual training data and parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996), which was binarized using KenLM (Heafield, 2011). The sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with"
W13-2214,J03-1002,0,0.0058822,"and nonterminals), target bigrams, lexical insertions and deletions (for the top 150 unaligned words from the training data), context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl and News Commentary parallel training data to create the translation grammar necessary for our model. For Russian, we additionally used the Common Crawl and Yandex data. The data were lowercased and tokenized, then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized sing the grow-diag-final-and method (Koehn et al., 2003). 3 We approximate corpus B LEU by scoring sentences using a pseudo-document of previous 1-best translations (Chiang et al., 2009). 129 en de Dev 75k 74k Test 74k 73k Test2 27k 26k 5k 132k 133k 10k 255k 256k 25k 634k 639k 50k 1258k 1272k ru en 16 108k 16 77k Tune ↑B LEU 22.38 23.86 30.18 32.40 Test ↑B LEU 22.69 23.01 29.89 30.81 Test2 24k 25k 15k 350k 371k 24 ↓TER 60.61 59.89 49.05 48.40 λ=0.01 23.8 23.6 λ=0.1 23.4 BLEU de-en +sparse ru-en +sparse # features Test 24k 27k"
W13-2214,P03-1021,0,0.0277435,"e and target words, passing an untranslated source word to the target side, singleton rule and source side, as well as counts for arity-0,1, or 2 SCFG rules, the total number of rules used, and the number of times the glue rule is used. 2.1 3 Evaluation This section describes the experiments we conducted in moving towards a better understanding of the benefits and challenges posed by large-scale high-dimensional discriminative tuning. 3.1 Data preparation Sparse Features The ability to incorporate sparse features is the primary reason for the recent move away from Minimum Error Rate Training (Och, 2003), as well as for performing large-scale discriminative training. We include the following sparse Boolean feature templates in our system in addition to the aforementioned baseline features: rule identity (for every unique rule in the grammar), rule shape (mapping rules to sequences of terminals and nonterminals), target bigrams, lexical insertions and deletions (for the top 150 unaligned words from the training data), context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl"
W13-2214,P02-1040,0,0.0990699,"e sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012) with a single passive-aggressive update for each sentence (Eidelman, 2012). Compound segmentation lattices As German is a morphologically rich language with productive compounding, we use word segmentation lattices as input for the German translation task. These lattices encode alternative segmentations of compound words, allowi"
W13-2214,P12-1002,0,0.0477022,"to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and RussianEnglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data. 1 Baseline system Introduction The adoption of discriminative learning methods for SMT that scale easily to handle sparse and lexicalized features has been increasing in the last several years (Chiang, 2012; Hopkins and May, 2011). However, relatively few systems take full advantage of the opportunity. With some exceptions (Simianer et al., 2012), most still rely on tuning a handful of common dense features, along with at most a few thousand others, on a relatively small development set (Cherry and Foster, 2012; Chiang et al., 2009). While more features tuned on more data usually results in better performance for other NLP tasks, this has not necessarily been the case for SMT. System design To efficiently encode the information that the learner and decoder require (source sentence, reference translation, grammar rules) in a manner amenable to MapReduce, i.e. avoiding dependencies on “side data” and large transfers across the network,"
W13-2214,N13-1025,0,\N,Missing
W13-2214,D08-1024,1,\N,Missing
W13-2214,W12-3160,1,\N,Missing
W13-2214,P13-1031,0,\N,Missing
W13-2214,W11-2123,0,\N,Missing
W13-2214,N09-1025,0,\N,Missing
W13-2214,P10-4002,1,\N,Missing
W13-2214,N12-1047,0,\N,Missing
W13-2214,N09-1046,0,\N,Missing
W13-2214,P13-4034,1,\N,Missing
W13-2214,D11-1125,0,\N,Missing
W15-1207,W14-3207,0,0.278976,", Leonardo Claudino1,4 , Thang Nguyen3 1 Computer Science, 2 Linguistics, 3 iSchool, and 4 UMIACS, University of Maryland {resnik,armstrow}@umd.edu {claudino,daithang}@cs.umd.edu 1 Introduction The 2015 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on classification of a sample of Twitter users according to three mental health categories: users who have self-reported a diagnosis of depression, users who have self-reported a diagnosis of post-traumatic stress disorder (PTSD), and control users who have done neither (Coppersmith et al., 2015; Coppersmith et al., 2014). Like other shared tasks, the goal here was to assess the state of the art with regard to a challenging problem, to advance that state of the art, and to bring together and hopefully expand the community of researchers interested in solving it. The core problem under consideration here is the identification of individuals who suffer from mental health disorders on the basis of their online language use. As Resnik et al. (2014) noted in their introduction to the first ACL Workshop on Computational Linguistics and Clinical Psychology, few social problems are more costly than problems of mental"
W15-1207,W15-1204,0,0.147192,"2,4 , William Armstrong1,4 , Leonardo Claudino1,4 , Thang Nguyen3 1 Computer Science, 2 Linguistics, 3 iSchool, and 4 UMIACS, University of Maryland {resnik,armstrow}@umd.edu {claudino,daithang}@cs.umd.edu 1 Introduction The 2015 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on classification of a sample of Twitter users according to three mental health categories: users who have self-reported a diagnosis of depression, users who have self-reported a diagnosis of post-traumatic stress disorder (PTSD), and control users who have done neither (Coppersmith et al., 2015; Coppersmith et al., 2014). Like other shared tasks, the goal here was to assess the state of the art with regard to a challenging problem, to advance that state of the art, and to bring together and hopefully expand the community of researchers interested in solving it. The core problem under consideration here is the identification of individuals who suffer from mental health disorders on the basis of their online language use. As Resnik et al. (2014) noted in their introduction to the first ACL Workshop on Computational Linguistics and Clinical Psychology, few social problems are more cost"
W15-1207,N15-1076,1,0.703778,"Psych Hackathon data (Coppersmith, 2015). 2 System Overview In our system, we build on a fairly generic supervised classification approach, using SVM with a linear or RBF kernel and making use of baseline lexical features with TF-IDF weighting. 2.1 Variations explored The innovations we explore center on using topic models to develop features that capture latent structure in the dataset, going beyond “vanilla” latent Dirichlet allocation (Blei et al., 2003) to include supervised LDA (Blei and McAuliffe, 2008, sLDA) as well as a supervised variant of the “anchor” algorithm (Arora et al., 2013; Nguyen et al., 2015, sAnchor). Putting together various combinations in our experimentation — linear vs. RBF kernel, big vs. small vocabulary, and four feature configurations (namely sLDA, sAnchor, lexical TF-IDF, and all combined), we evaluated a total of 16 systems for each of the three shared tasks (discriminating depression vs. controls, depression vs. PTSD, and PTSD vs. controls) for a total of 48 systems in all. In general below, systems are named simply by concatenating the relevant elements of the description. For example, combobigvocabSVMlinear 1 is the name of the system that uses (a) an SVM with linea"
W15-1207,D13-1133,1,0.43679,"s. To take full advantage of the shared task’s labeled training data in a topic modeling setting, we opted to use supervised topic models (sLDA, introduced by Blei and McAuliffe (2008)), as a method for gaining both clinical insight and predictive ability. However, initial exploration with the training dataset provided noisy topics of variable quality, many of which seemed intuitively unlikely to be useful as predictive features in the mental health domain. Therefore we incorporated an informed prior based on a model that we knew to capture relevant latent structure. Specifically, we followed Resnik et al. (2013) in building a 50-topic model by running LDA on stream-of-consciousness essays collected by Pennebaker and King (1999) — a young population that seems likely to be similar to many authors in the Twitter dataset. These 50 topics were used as informed priors for sLDA. Tables 3 to 5 show the top words in the sLDA topics with the 5 highest and 5 lowest Z-normalized regression scores, sLDA having been run with parameters: number of topics (k) = 50, documenttopic Dirichlet hyper-parameter (α) = 1, topic-word Dirichlet hyper-parameter (β) = 0.01, Gaussian variance for document responses (ρ) = 1, Gaus"
W15-1207,W15-1212,1,0.768189,"tigma, an inability of people to recognize symptoms and report them accurately, and the lack of access to clinicians. Language technology has the potential to make a real difference by offering low-cost, unintrusive methods for early screening, i.e. to identify people who should be more thoroughly evaluated by a professional, and for ongoing monitoring, i.e. to help providers serve their patients better over the long-term continuum of care (Young et al., 2014). This paper summarizes the University of Maryland contribution to the CLPsych 2015 shared task. More details of our approach appear in Resnik et al. (2015), where we also report results on preliminary experimentation using the CLPsych Hackathon data (Coppersmith, 2015). 2 System Overview In our system, we build on a fairly generic supervised classification approach, using SVM with a linear or RBF kernel and making use of baseline lexical features with TF-IDF weighting. 2.1 Variations explored The innovations we explore center on using topic models to develop features that capture latent structure in the dataset, going beyond “vanilla” latent Dirichlet allocation (Blei et al., 2003) to include supervised LDA (Blei and McAuliffe, 2008, sLDA) as we"
W15-1212,W14-3207,0,0.411088,"recent surge of interest in finding accessible, cost effective, non-intrusive methods to detect depression and other mental disorders. Continuing a line of thought pioneered by Pennebaker and colleagues (Pennebaker and King, 1999; Rude et al., 2004, and others), researchers have been developing methods for identifying relevant signal in people’s language use, which could potentially provide inexpensive early detection of individuals who might require a specialist’s evaluation, on the basis of their naturally occurring linguistic behavior, e.g. (Neuman et al., 2012; De Choudhury et al., 2013; Coppersmith et al., 2014). Critical mass for a community of interest on these topics has been building within the computational linguistics research community (Resnik et al., 2014). To date, however, the language analysis methods used in this domain have tended to be fairly simple, typically including words or n-grams, manually defined word categories (e.g., Pennebaker’s LIWC lexicon, Pennebaker and King (1999)), and “vanilla” topic models (Blei et al., 2003, latent Dirichlet allocation (LDA)). This stands in contrast to other domains of computational social science in which more sophisticated models have been develop"
W15-1212,W15-1204,0,0.102468,"Missing"
W15-1212,N09-1031,0,0.0192038,". To assess the ability of the models/features and how they compare to baseline methods, we trained a linear support vector regression (SVR) model on the union of the Twitter train and dev sets, evaluated on the test set. We chose regression over classification despite having binary labels in our data in order to more easily evaluate precision at various levels of recall, which can be done simply by thresholding the predicted value at different points in order to obtain different recall levels. In addition, SVR has been shown to be an adequate choice to other similar text regression problems (Kogan et al., 2009), and in future analyses the use of the linear kernel will allow us to further see the contributions of each feature from the weights assigned by the regression model. We follow standard practice in using unigram features and LIWC categories as baseline feature sets, and we also use topic posteriors from a 50topic LDA model built on the Twitter training data.5 5 Not to be confused with the LDA model built using the stream-of-consciousness dataset in Section 3.1, which was used 104 As shown in Table 7, we evaluated alternative models/feature sets by fixing the percentage of recalled (correctly"
W15-1212,N15-1076,1,0.743604,"latent Dirichlet allocation (LDA)). This stands in contrast to other domains of computational social science in which more sophisticated models have been developed for some time, including opinion analysis (Titov and McDonald, 2008), analysis of the scientific literature (Blei and Lafferty, 2007), and computational political science (Grimmer, 2010). In this paper, we take steps toward employing more sophisticated models in the analysis of linguistic signal for detecting depression, providing promising results using supervised LDA (Blei and McAuliffe, 2007) and supervised anchor topic models (Nguyen et al., 2015), and beginning some initial exploration of a new supervised nested LDA model (SNLDA). 2 Data Our primary experimental dataset is the Twitter collection created by Coppersmith et al. (2014) 99 Proceedings of the 2nd Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 99–107, c Denver, Colorado, June 5, 2015. 2015 Association for Computational Linguistics and used in the CLPsych Hackathon (Coppersmith, 2015). The raw set contains roughly 3 million tweets from about 2,000 twitter users, of which roughly 600 self-identify as having been"
W15-1212,D13-1133,1,0.743591,"ving a regression value of 1 and those from the control subset as having a regression value of -1. In building some of our models, we also use a collection of 6,459 stream-of-consciousness essays collected between 1997 and 2008 by Pennebaker and King (1999), who asked students to think about their thoughts, sensations, and feelings in the moment and “write your thoughts as they come to you”. As discussed in Section 3.1, running LDA on this dataset provides informative priors for S LDA’s learning process on the Twitter training data. The student essays average approximately 780 words each, and Resnik et al. (2013) showed that unsupervised topic models based on this dataset can produce very clean, interpretable topical categories, a number of which were viewed by a clinician as relevant in the assessment of depression, including, for example, “vegetative” symptoms (particularly related to sleep and energy level), somatic symptoms (physical discomfort, e.g. headache, itching, digestive problems), and situational factors such as homesickness. For uniformity, we preprocessed the stream-ofconsciousness dataset with the same tools as the Twitter set.1 We created a shared vocabulary for our models by taking t"
W15-1212,W15-1207,1,0.72997,"Missing"
W15-1212,W14-3214,0,0.369338,"Missing"
W15-1212,P11-1026,1,\N,Missing
W15-3003,W11-2123,0,0.0158756,"n the WIT3 TED Chinese-English corpus (Cettolo et al., 2012), a good example of a subdomain with little available training data. We used the IWSLT dev2010 and test2010 sets (also from WIT3 ) for tuning and evaluation. The larger pool from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively comp"
W15-3003,2014.eamt-1.7,0,0.0214638,"LDC2013E125 LDC2013E132 LDC2013E83 LDC2013T03 LDC2013T05 LDC2013T07 LDC2013T11 LDC2013T16 LDC2014E08 LDC2014E111 LDC2014E50 LDC2014E69 LDC2014E99 LDC2014T04 LDC2014T11. 2 The Stanford NLP tools use the Penn tagsets, which comprise 43 tags for English and 35 for Chinese. 60 TED vocab LDC vocab Joint vocab LDC minus singletons Baseline selection vocab English 49,323 458,570 470,154 243,882 257,744 Chinese 64,616 714,628 729,283 373,381 388,927 Joint vocab Vocab with count ≥ 10 POS tags Hybrid vocab previously found that setting the threshold to 10 is slightly better than a minimum count of 20 (Axelrod, 2014), and varying the threshold further is a topic for future work; see Section 5. (most domain-like) to highest score (least domainlike). For each of those ranked pools, we consider increasingly larger subsets of the data: the best n = 50K, the best n = 100K, and so on. The largest subset we consider consists of the best n = 4M sentence pairs out of the 6M available. 4.2 4.2.1 Results Intrinsic Evaluation As noted, each of the bilingual Moore-Lewis method and our hybrid word/POS variation produces a version of the additional training pool in which sentences are ranked by relevance. We then select"
W15-3003,W13-2233,0,0.0717906,"Missing"
W15-3003,D11-1033,1,0.879115,"the undue effects of infrequent words. The proposal can be realized straightforwardly: after part-of-speech tagging the in-domain and pool corpora, we identify all words that appear infrequently in either one of the two corpora, and replace each of their word tokens with its POS tag. (1) where Hm (s) is the per-word cross entropy of s according to language model m. Lower scores for cross-entropy difference indicate more relevant sentences, i.e. those that are most like the target domain and unlike the full pool average. In bilingual settings, the bilingual Moore-Lewis criterion, introduced by Axelrod et al. (2011), combines the 59 cal machine translation as a downstream task. Relevance computation, sentence ranking and subset selection then proceed as usual according to the Moore-Lewis or bilingual Moore-Lewis criterion. As an example, consider again the phrases “an earthquake in Port-au-Prince” and “an earthquake in Kodari”, and suppose that the words an, in, and earthquake are above-threshold in frequency. Our hybrid word/POS representation for both sentences would be the same: “an earthquake in NNP”. Our approach differs from the standard data selection method most significantly in its handling of r"
W15-3003,N03-2003,1,0.666232,"Missing"
W15-3003,P10-2041,0,0.202258,"system. The catch, of course, is that any large data pool can be expected to contain sentences that are at best irrelevant to the domain, and at worst detrimental: the goals of fidelity (matching in-domain data as closely as possible) and broad coverage are often at odds (Gasc´o et al., 2012). As a result, much work has focused on fidelity. Mirkin and Besacier (2014) survey the difficulties of increasing coverage while minimizing impact on model performance. We build on the standard approach for data selection in language modeling, which uses crossentropy difference as the similarity metric (Moore and Lewis, 2010). The Moore-Lewis procedure first trains an in-domain language model (LM) on the in-domain data, and another LM on the full pool of general data. It assigns to each full-pool sentence s a cross-entropy difference score, HLMIN (s) − HLMP OOL (s), (2) 3 Our Approach: Abstracting Away Words in the Long Tail Our approach is motivated by the observation that domain mismatches can have a strong register component, and this comprises both lexical and syntactic differences. We are inspired, as well, by work in stylometry, observing that attempts to quantify differences between text datasets try to lea"
W15-3003,P02-1040,0,0.0918678,"e, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary co"
W15-3003,D08-1024,1,0.706303,"inearly with the amount of selection data. By contrast, our proposed method appears to asymptotically approach full in-domain vocabulary coverage, particularly for Chinese. Similarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smalle"
W15-3003,2006.amta-papers.25,0,0.0215726,"ilarly, Figure 1 shows that our hybrid method also increases more rapidly to asymptotically approach full in-domain vocabulary coverage as well. 4.2.2 Extrinsic Evaluation Improved vocabulary coverage is a positive result, but we are also interested in downstream application performance. Accordingly, we trained SMT systems using cdec (Dyer et al., 2010) on subsets of selected data. All SMT systems were tuned using MIRA (Chiang et al., 2008) on the dev2010 data from IWSLT (Federico et al., 2011), and then evaluated on the test2010 IWSLT test set using both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). To isolate the impact of the data selection method, we present results just using the selected data, without the combining with the in-domain data into a multi-model sys4.2.3 Selection Model Size The resulting translation system sizes conform with prior work: selecting smaller subsets yields smaller downstream MT systems. For example, an MT system trained on 1M selected sentences is ∼2.3GB in size, a factor of 5 smaller than the 11.3GB baseline MT system trained on all 6M sentences. In addition, we observe a healthy re62 Figure 1: Percentage of TED vocabulary covered vs. number of selected s"
W15-3003,W13-2803,0,0.0193944,"cally a range of values for n is considered, selecting the n that performs best on held-out indomain data. While shown to be effective, however, wordbased scores may not capture all facets of relevance. The strategy of a hybrid word/POS representation was first explored by Bulyko et al. (2003), who used class-dependent weights for mixing multi-source language models. The classes were a combination of the 100 most frequent words and POS tags. Bisazza and Federico (2012) target in-domain coverage by using a hybrid word/POS representation to train an additional LM for decoding in an MT pipeline. Toral (2013) uses a hybrid word/class representation for data selection for language modeling; he replaces all named entities with their type (e.g. ’person’, ’organization’), and experiments with also lemmatizing the remaining words. Related Work Data selection is a widely-used variant of domain adaptation that requires quantifying the relevance to the domain of the sentences in a pooled corpus of additional data. The pool is sorted by relevance score, the highest ranked portion is kept, and the rest discarded.This process – also known as “rank-and-select” in language modeling (Sethy et al., 2009) – ident"
W15-3003,N03-1033,0,0.0161681,"from which we selected data was constructed from an aggregation of 47 LDC Chinese-English parallel datasets.1 Table 1 contains the corpus statistics for the task and pool bilingual corpora. Corpus TED (task) LDC (pool) Sentences 145,901 6,025,295 Vocab (En) 49,323 458,570 Vocab (Zh) 64,616 714,628 Table 1: Chinese-English Parallel Data. We used the KenLM toolkit (Heafield, 2011) to build all language models used in this work (i.e., both for data selection and for the MT systems used for extrinsic evaluation). In all cases the models were 4-gram LMs. We used the Stanford part-of-speech tagger (Toutanova et al., 2003) when constructing our hybrid representations, to generate the POS tags for each of the English and Chinese sides of the corpora.2 We consider three ways of applying data selection using the standard (fully lexicalized) corpus representation and our hybrid representation. The first two use the monolingual MooreLewis method (Equation 1) to respectively compute relevance scores using the English (output) side and the Chinese (input) side of the parallel corpora. The third uses bilingual Moore-Lewis (Equation 2) to compute the bilingual score over both sides. Each of these three variants produces"
W15-3003,E12-1045,0,\N,Missing
W15-3003,E12-1016,0,\N,Missing
W15-3003,P10-4002,1,\N,Missing
W15-3003,2014.amta-researchers.23,0,\N,Missing
W15-3003,2011.iwslt-evaluation.1,0,\N,Missing
W15-3003,2012.eamt-1.60,0,\N,Missing
W16-0319,P11-1026,0,\N,Missing
W16-0319,W15-1207,1,\N,Missing
W16-0321,P14-5010,0,0.0126145,"Missing"
W16-2524,1994.tc-1.5,0,0.766048,"Missing"
W16-2524,W13-2322,0,0.0160883,"portance of incorporating broad structural diversity in the dataset construction. Semantic characteristics There are many types of semantic information that we might probe for with this method. For our purposes here, we are going to focus on two basic types, which are understood in linguistics to be fundamental components of meaning, and which have clear ties to common downstream applications: semantic role and scope. The importance of semantic role information is well-recognized both in linguistics and in NLP— for the latter in the form of tasks such as abstract meaning representation (AMR) (Banarescu et al., 2013). Similarly, the concept of scope is critical to many key linguistic phenomena, including negation—the importance of which is widely acknowledged in NLP, in particular for applications such as sentiment analysis (Blunsom et al., 2013; Iyyer et al., 2015). Both of these information types are of course critical to computing entailment. 3.3 Example classification tasks Once we have identified semantic information of interest, we can design classification tasks to target this information. We illustrate with two examples. Semantic role If a sentence representation has captured semantic roles, a rea"
W16-2524,S14-2055,0,0.0626242,"Missing"
W16-2524,D11-1037,0,0.0512209,"increase complexity and variation, and to make sentences better reflect natural text. More detailed discussion of considerations for construction of the actual dataset is given in Section 5. 135 3.2 umn contains the string the school recommended, though school is not the agent of recommended— rather, the agent of recommended is located at the beginning of the sentence. We believe that incorporation of such long-distance dependencies is critical for assessing whether systems are accurately capturing semantic roles across a range of naturally-occurring sentence structures (Rimell et al., 2009; Bender et al., 2011). This example task can obviously be extended to other relations and other entities/events as desired, with training and test data adjusted accordingly. We will remain agnostic here as to the optimal method of selecting relations and entities/events for classification tasks; in all likelihood, it will be ideal to choose and test several different combinations, and obtain an average accuracy score. Note that if we structure our task as we have suggested here—training and testing only on sentences containing certain selected lexical items— then the number of examples at our disposal (both positi"
W16-2524,marelli-etal-2014-sick,0,0.0330522,"Missing"
W16-2524,D14-1162,0,0.103327,"Missing"
W16-2524,W13-3209,0,0.0286009,"us on two basic types, which are understood in linguistics to be fundamental components of meaning, and which have clear ties to common downstream applications: semantic role and scope. The importance of semantic role information is well-recognized both in linguistics and in NLP— for the latter in the form of tasks such as abstract meaning representation (AMR) (Banarescu et al., 2013). Similarly, the concept of scope is critical to many key linguistic phenomena, including negation—the importance of which is widely acknowledged in NLP, in particular for applications such as sentiment analysis (Blunsom et al., 2013; Iyyer et al., 2015). Both of these information types are of course critical to computing entailment. 3.3 Example classification tasks Once we have identified semantic information of interest, we can design classification tasks to target this information. We illustrate with two examples. Semantic role If a sentence representation has captured semantic roles, a reasonable expectation would be extractability of the entity-event relations contained in the sentence meaning. So, for instance, we might choose professor as our entity, recommend as our event, and AGENT as our relation—and label sente"
W16-2524,P16-1030,0,0.0348798,"lly reduce the likelihood that systems will have been trained on the phrases encountered during the classification evaluation—while remaining agnostic as to the particulars of those systems’ training data. It would be useful, in this case, to have annotation of the plausibility levels of our sentences, in order to ascertain whether performance is in fact aided by the presence of phrases that may reasonably have occurred during composition training. Possible approaches to estimating plausibility without human annotation include using n-gram statistics on simple argument/predicate combinations (Rashkin et al., 2016) or making use of selectional preference modeling (Resnik, 1996; Erk, 2007; S´eaghdha, 2010). A final note: learning low-dimensional vector representations for sentences is bound to require a trade-off between the coverage of encoded information and the accessibility of encoded information—some semantic characteristics may be easily extractable at the cost of others. We have not, in this proposal, covered all semantic characteristics of interest, but it will ultimately be valuable to develop a broad-coverage suite of classification tasks for relevant information types, to obtain an assessment"
W16-2524,P16-2059,1,0.882415,"Missing"
W16-2524,D09-1085,0,0.0607694,"nents can be added to increase complexity and variation, and to make sentences better reflect natural text. More detailed discussion of considerations for construction of the actual dataset is given in Section 5. 135 3.2 umn contains the string the school recommended, though school is not the agent of recommended— rather, the agent of recommended is located at the beginning of the sentence. We believe that incorporation of such long-distance dependencies is critical for assessing whether systems are accurately capturing semantic roles across a range of naturally-occurring sentence structures (Rimell et al., 2009; Bender et al., 2011). This example task can obviously be extended to other relations and other entities/events as desired, with training and test data adjusted accordingly. We will remain agnostic here as to the optimal method of selecting relations and entities/events for classification tasks; in all likelihood, it will be ideal to choose and test several different combinations, and obtain an average accuracy score. Note that if we structure our task as we have suggested here—training and testing only on sentences containing certain selected lexical items— then the number of examples at our"
W16-2524,P07-1028,0,0.0244426,"ed during the classification evaluation—while remaining agnostic as to the particulars of those systems’ training data. It would be useful, in this case, to have annotation of the plausibility levels of our sentences, in order to ascertain whether performance is in fact aided by the presence of phrases that may reasonably have occurred during composition training. Possible approaches to estimating plausibility without human annotation include using n-gram statistics on simple argument/predicate combinations (Rashkin et al., 2016) or making use of selectional preference modeling (Resnik, 1996; Erk, 2007; S´eaghdha, 2010). A final note: learning low-dimensional vector representations for sentences is bound to require a trade-off between the coverage of encoded information and the accessibility of encoded information—some semantic characteristics may be easily extractable at the cost of others. We have not, in this proposal, covered all semantic characteristics of interest, but it will ultimately be valuable to develop a broad-coverage suite of classification tasks for relevant information types, to obtain an assessment that is both fine-grained and comprehensive. This kind of holistic assessm"
W16-2524,P10-1045,0,0.0501266,"Missing"
W16-2524,P15-1150,0,0.0282097,"ssification tasks for targeting this information. 1 Introduction Sentence-level meaning representations, when formed from word-level representations, require a process of composition. Central to evaluation of sentence-level vector representations, then, is evaluating how effectively a model has executed this composition process. In assessing composition, we must first answer the question of what it means to do composition well. On one hand, we might define effective composition as production of sentence representations that allow for high performance on a task of interest (Kiros et al., 2015; Tai et al., 2015; Wieting et al., 2015; Iyyer et al., 2015). A limitation of such an approach is that it is likely to produce overfitting to the characteristics of the particular task. Alternatively, we might define effective composition as generation of a meaning representation that makes available all of the information that we would expect to be extractable from the meaning of the input sentence. For instance, in a representation of the sentence “The dog didn’t bark, but chased the cat”, we would expect to be able to extract the information that there is an event of chasing, that a dog is doing the chasing"
W16-2524,D15-1002,0,0.0295278,"Missing"
W16-2524,P15-1162,0,0.00888502,"Missing"
W18-0603,W16-0311,0,0.1294,"Missing"
W18-0603,W17-1612,0,0.0615839,"ne is that we have so far limited ourselves to Reddit, which may have particular characteristics that fail to generalize; in particular, evidence suggests that users show different behavior when posting anonymously, with both positive and negative implications (Christopherson, 2007; De Choudhury and De, 2014). A second limitation is that, without health records, outcomes, or even self-report questionnaires from the users whose postings were asDataset Availability and Ethical Considerations The research we report was approved by the University of Maryland’s Institutional Review Board (IRB). As Benton et al. (2017) discuss, human subjects research using previously existing data falls into a category exempted from the requirement of full IRB review as long as the data are either from publicly available sources or they do not provide a way to recover the identity of the subjects. In our case, the data are publicly available and from a site where users are anonymous. As an extra precaution we replace Reddit usernames with numeric identifiers. Benton et al. (2017) point out that even exempt research needs to be reviewed by an IRB to make an exemption determination. In addition, they discuss the importance o"
W18-0603,S17-2093,0,0.0173585,"Portability and Accountability Act, or HIPAA. Resnik (2017) has argued that, owing to the fact that the law was written without anticipating the importance of large scale, community-wide research datasets, the state of the art in clinical natural language processing is significantly behind the state of the art in other domains. For example, the widely used Enron email corpus contains 1.2 million emails (Klimt and Yang, 2004); in contrast, the SemEval-2017 Clinical TempEval shared task used 400 manually de-identified clinical notes and pathology reports from cancer patients at the Mayo Clinic (Bethard et al., 2017). 3 https://www.reddit.com/r/datasets/ 26 3.1 In order to determine user-level risk, we consider a user to have the highest risk associated with any of their annotation units. We defined a four-way categorization of risk adapting Corbitt-Hall et al. (2016) (who provided lay definitions based on risk categories in Joiner et al. (1999)): (a) No Risk (or “None”): I don’t see evidence that this person is at risk for suicide; (b) Low Risk: There may be some factors here that could suggest risk, but I don’t really think this person is at much of a risk of suicide; (c) Moderate Risk: I see indication"
W18-0603,P16-1191,0,0.0146585,"tional Neural Networks Assessment Classifier We conduct screening experiments looking at evidence within t days before the “signal” (i.e. the first SuicideWatch post), where t could be 1, 2, 5, or 7 days. For control users, a random post is chosen as the point from which t is determined. A user’s relevant posts for screening, from which the In additional to our baseline classifier for the assessment task, we explored using a convolutional neural network (CNN), since CNNs are effective in many NLP tasks, especially text classification problems like sentence-level sentiment analysis (Kim, 2014; Flekova and Gurevych, 2016). We adopt a similar CNN architecture to the one introduced in Kim (2014) due to its popularity and ease of scalability to multiple tasks and strong results on many datasets. Figure 1 depicts the structure of our CNN architecture, where the input of the network is the concatenation of all user’s posts and 17 We also experimented with logistic regression and XGBoost, with substantially inferior results. 18 For these experiments logistic regression and XGBoost had performance very similar to SVM. 4.4 Screening 30 comprehensive discussion of crowdsourcing, using CrowdFlower, as a means for obtain"
W18-0603,W14-3207,0,0.55779,"(Reddit, 2018). Since users might have chosen to include potentially identifying information in their usernames, we go a step further and replace usernames with unique numeric identifiers.6 We discuss privacy and other issues further in Section 6. based on the same detailed instructions. We evaluated levels of inter-rater agreement within and across groups and also looked at differences between groups. In addition, we present initial automatic risk-level classification and screening results for SuicideWatch data using machine learning. 2 Dataset Our approach to data collection is inspired by Coppersmith et al. (2014), who introduced an innovative way to solve for the absence of clinical ground truth when studying mental health in social media. Their approach is to identify users who have produced an overt signal, in social media, indicating they might be a positive instance of the relevant condition, and then manually assessing the signal to filter out candidates for which the signal does not appear genuine. They applied this on Twitter by seeking variations of the statement I have been diagnosed with X, (where X is depression, PTSD, or other conditions), and then manually filtering tweets for which the s"
W18-0603,W14-3213,0,0.0395626,"ing that there is clear value in the crowdsourced annotations. Table 4 shows a confusion matrix measuring crowdsourcers’ consensus against the all-experts consensus, and it appears that most of the errors involve erring on the side of caution, misclassifying more than half of the low-risk users as having higher risk, and misclassifying a large number of All Experts Crowdflower None Low Moderate Severe None Low Moderate Severe 29 11 6 1 1 13 11 1 1 20 47 8 5 6 51 34 Table 4: All Experts vs. Crowdsourcers 12 Performance differences between experts and nonexperts require more study. For example, Homan et al. (2014) found that two novice annotators were more likely to assign their expert’s “low distress” tweets to the “no distress” category. Conversely, on a related but coarser-grained categorization task, Liu et al. (2017) find “some evidence that multiple crowdsourcing workers, when they reach high inter-annotator agreement, can provide reliable quality of annotations”. Short Experts 10 We conjecture that, with fewer jobs left available, annotators were less inclined to go through the detailed instructions and test because there was less for them to get paid for. 11 See Confidence Score https://success"
W18-0603,Q14-1025,0,0.0200369,"eliability cutoff for chance-corrected agreement (> 0.8, Krippendorff (2004)), which is to our knowledge the first result demonstrating inter-rater reliability by clinical experts for suicide risk based on social media postings. Inter-rater reliability for the pair receiving short instructions was substantially lower (0.768), demonstrating the value of our detailed rubric based on explicitly identified risk factors. We generated consensus user-level labels based on the expert annotations using a well known model for inferring true labels from multiple noisy annotations (Dawid and Skene, 1979; Passonneau and Carpenter, 2014), including consensus for the pairs receiving long instructions (Long Experts), short instructions (Short Experts), and consensus among all four experts. Table 2 summarizes the data, partitioning categories according to the allexperts consensus. Krippendorff α exp L1 exp L2 exp S1 exp S2 exp exp exp exp 1 - 0.837 1 - 0.804 0.808 1 - 0.823 0.831 0.768 1 L1 L2 S1 S2 Table 1: Krippendorff’s α pairwise among experts 9 Random selection was from the set of crowdsourceannotated users obtained in Section 3.2, ensuring that all expert annotations would be accompanied by crowdsourced annotations. Recall"
W18-0603,D14-1181,0,0.0058191,"Missing"
W18-0603,W16-0312,0,0.474076,"t, Cornell University, Ithaca, NY 4 Stanford Center for Population Health Sciences, Stanford University, Stanford, CA {shing,srnair,hal,resnik}@umd.edu ayah.zirikly@nih.gov, mdf224@cornell.edu Abstract who post material indicating imminent risk and the need for intervention. An emerging subset of the artificial intelligence and language technology communities has been making progress on automated methods that analyze online postings to flag mental health conditions, with the goal of being able to screen or monitor for suicide risk and other conditions (Calvo et al., 2017; Resnik et al., 2014; Milne et al., 2016; Milne, 2017). Some sites have been taking advantage of these methods to add automation to their moderation, in the form of a pipeline from algorithmic risk assessment to human moderator review to preventive action. With all of these technology-driven developments taking place so quickly, it is easy to forget that clinician assessment of suicidality from online writing is a new and largely unstudied problem. To what extent is level of suicide risk discernable from online postings? How are traditional training and experience in assessment brought to bear in the absence of interaction with the"
W18-0603,D08-1027,0,0.215817,"Missing"
W18-0603,N16-1174,0,0.0103924,"tap the potential of the dataset. For example, metadata associated with posts includes potentially valuable temporal information (Coppersmith et al., 2015), and we also have not yet explored the value of the annotators’ selecting the post that most strongly supports their judgment. In addition, the classification results here are just an initial exploration of the problem; for example, we plan to follow Vioul`es et al. (2018) in exploring hierarchical rather than four-way classification, which yielded substantial improvements, and we are exploring the role of hierarchical attention networks (Yang et al., 2016) as a way to cut through noise to identify the most relevant signals. We look forward to other researchers joining us in order to foster more rapid progress. sessed, we cannot validate clinician assessments; nor are we able to provide clinical evidence for improved validity using the detailed assessment instructions. Outcomes data would clearly be preferable if it were available; for example, Pokorny (1983) and Goldstein et al. (1991) attempt prediction of suicide using a wide range of variables and clinical measures for thousands of psychiatric inpatients. However, outcomes data are very diff"
W18-0603,D17-1322,0,0.190234,"Missing"
W18-0603,W16-0321,1,0.891999,"Missing"
W18-0604,W14-3207,0,0.0252189,"g it easier to draw comparisons between system performances. We take the measurement error from literature on the reliability of the adult psychological distress measure (rmeas1 = 0.77; Ploubidis et al. (2017)) and of similar, languagebased prediction measures (rmeas2 = 0.70; Park et al. (2015)). The metric is thus: Innovation Challenge: Future Language Generation One of the limitations of traditional psychological assessments is that they typically only capture one or a few psychological factors. In contrast, language has been shown to capture many aspects of an individual (Pennebaker, 2011; Coppersmith et al., 2014; Schwartz and Ungar, 2015; Kern et al., 2016), making language-based assessments an attractive compliment or alternative. Language generation tools for mental health could indicate whether an individual is likely to produce signs of mental distress in future, e.g. “I want to end my life.” Should language generation tools be adequately reliable and valid indicators of future mental health states, these tools could serve as a means of identifying individuals who could be targeted for early intervention or preventative treatments. The Innovation Challenge is a difficult rdis = √ rP earson rmeas1"
W18-0604,W14-3348,0,0.0122096,"as the average embedding for all words in the document — and measure the cosine similarity between the generated essay’s embedding and that of the actual essay. The word-level embeddings are Word2Vec (Mikolov et al., 2013) embeddings learned from the age 50 essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not individual documents as we do here. Secon"
W18-0604,W16-6203,0,0.150517,".edu Abstract and easy to conduct at scale. Further, whereas traditional risk assessments are typically limited to capturing one or a few psychological factors, language analysis has the advantage of being theoretically unlimited in what it can capture. By evaluating the relationship between linguistic markers and lifetime health outcomes, such research may provide benefits for intake assessment, monitoring, and preventative care. Computational linguistics has now shown strong potential for aiding in mental health assessment and treatment. With few exceptions (e.g. De Choudhury et al. (2016), Sadeque et al. (2016)), work thus far from the NLP community has focused on predicting current mental health from language, and most exceptions have still only looked at the short-term future. While such research is valuable, predictions about the long-term future can aid with another class of applications: the understanding of early life markers and development of preventative care. Here we describe the CLPsych 2018 shared task, the purpose of which is to evaluate multiple methods for analyzing linguistic markers as a signal for current and future psychological outcomes (i.e. risk assessment). We present three ta"
W18-0604,P17-4012,0,0.0230066,"Missing"
W18-0604,W09-0441,0,0.0150507,"ument-level embeddings — computed as the average embedding for all words in the document — and measure the cosine similarity between the generated essay’s embedding and that of the actual essay. The word-level embeddings are Word2Vec (Mikolov et al., 2013) embeddings learned from the age 50 essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not in"
W18-0604,W17-3110,1,0.759461,"in individuals by analyzing social media signals up to a year in advance of its reported onset. Similarly, De Choudhury et al. (2016) aims to identify individuals who are likely to engage in suicidal ideation in the future. Sadeque et al. (2016) predict whether posters on a mental health forum will leave the forum within a particular (one, six, or twelve month) time frame. In addition to these cases, some have used temporal information within cross-sectional analyses. Zirikly et al. (2016), for example, use timestamp data to help classify the severity levels of posts to a mental health forum. Loveys et al. (2017) explore mental health within the context of micropatterns, or sequences of posts occurring within a small time frame. The goal of this shared task is to predict mental health not only at the time of writing, but years or decades into the future. 2 Figure 1: Example of an essay written by an NCDS participant at age 11, imagining where they saw themself at age 25. dents displayed characteristics such as “does not know what to do with himself, can never stick at anything long” or “miserable, depressed, seldom smiles”. For the purposes of the shared task, we focused on the total BSAG score, as we"
W18-0604,W16-0321,1,0.892183,"Missing"
W18-0604,P02-1040,0,0.101515,"essay training set; words that appeared less than ten times were replaced with an out-of-vocabulary token. This approach is similar to that of Garten et al. (2017), which uses embeddings to capture semantic similarity when applying psychological lexica. It’s also similar in motivation to metrics like TERp (Snover et al., 2009) and METEOR (Denkowski and Lavie, 2014) which leverage semantic similarity for evaluating language generation. For this metric, we report the average cosine similarity across all essays. Innovation Challenge To evaluate the Innovation Challenge we compute the BLEU Score (Papineni et al., 2002), a measure commonly used for evaluating machine translation models, between the generated age 50 essay and the actual essay. We then report the average BLEU score across all documents. However, BLEU is not a perfect metric for this task. First, it was intended to be used to compare entire corpora, not individual documents as we do here. Second, this score was designed for machine translation, which our task is not. Instead, we are trying to predict a person’s response to an open-ended prompt, based on their response to a similar prompt forty years prior. For these reasons, we employ a second"
W19-3003,W14-3207,0,0.358731,"otation Source dataset • Logistics includes, e.g., talking about methods of attempting suicide (even if not planning), or having access to lethal means like firearms; We derived our shared task data from the dataset introduced by Shing et al. (2018). Shing et al. began with a collection intended to contain essentially every publicly available Reddit posting from its beginning in 2005 into summer 2015, and identified a subset of users potentially at risk by extracting all users who had posted to the r/SuicideWatch subreddit.2 The process was analogous to the data collection method pioneered by Coppersmith et al. (2014) for a variety of mental health conditions, where an explicit signal for candidate (potentially relevant) Twitter users was defined by specifying a self-report pattern, e.g. I have been diagnosed with [condition], and then matching posts were reviewed manually to identify candidates where the signal does not appear genuine, such as sarcastic or joking references. For the suicidality dataset, posting on SuicideWatch constituted the signal, and Shing et al. (2018) collected 11,129 candidate users on SuicideWatch, accounting for a total of 1,556,194 posts across Reddit, along with a comparable nu"
W19-3003,W19-3024,0,0.0636659,"Missing"
W19-3003,W19-3022,0,0.152523,"es if applicable) in Sections 4.2 and 4.3. In section 6, we provide more details about the top systems per task. • Task C is about screening. This task simulates a scenario in which someone has opted in to having their social media monitored (e.g., a new mother at risk for postpartum depression, a veteran returning from a deployment, a patient whose therapist has suggested it) and the goal is to identify whether they are at risk even if they have not explicitly presented with a problem. Here predictions are made only from users posts that are not on SuicideWatch. Team Affective Computing ASU (Ambalavanan et al., 2019) CAMH † Chen et al. (2019) CLaC (Mohammadi et al., 2019) CMU (Allen et al., 2019) IBM data science (Morales et al., 2019) IDLab (Bitew et al., 2019) JXUFE † SBU-HLAB (Matero et al., 2019) TsuiLab (Ruiz et al., 2019) TTU (Iserman et al., 2019) UniOvi-WESO (Hevia et al., 2019) uOttawa † USI-UPF (R´ıssola et al., 2019) For all tasks, we provided participating teams with training and test data using an 80-20 split. In order to keep the original labels’ distribution in the split, we applied the proportional training/test split separately for each label. The statistics of the data are shown in Table"
W19-3003,S17-2126,0,0.0225039,"sts to create separate language models. Some teams used models that were pre-trained on Wikipedia and some other large corpora as-is in their system. The most commonly used neural architecture is convolutional neural networks (CNN) on the user or post level, where in the latter case an aggregation step is needed to produce the final outcome. Other frequently employed architectures were long-short term memory (LSTM) networks or recurrent neural networks (RNNs) and LSTMs with an attention mechanism. Some teams experimented with multichannel neural networks in a multi-task learning setting. sis (Baziotis et al., 2017), a tool set that is tailored for social media data. Iserman et al. (2019) applied two-stage error spelling correction using edit distance from augmented dictionary entries. 4.2 Approaches 4.3 Model inputs The submitted systems used a wide range of input representations on the post and user level. We can distinguish several main categories: • Embeddings on the word, sentence or document (post/posts) level. In addition to GloVe and word2vec, we mostly see the more recently introduced contextualized embedding techniques such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). • Lexicon"
W19-3003,W17-1612,0,0.408007,"oughts, or worried about someone who may be at risk”. Henceforth we refer to it simply as SuicideWatch. 3 It is worth noting that, subsequent to Shing et al.’s collection and annotation, Gaffney and Matias (2018) reported on an analysis showing that the widely used Baumgartner Reddit collection, which Shing et al. had used as their start25 (d) Severe Risk: I believe this person is at high risk of attempting suicide in the near future. created specifically for anonymous posting, discussions on the platform nonetheless need to be viewed as sensitive and subject to careful ethical consideration (Benton et al., 2017; Chancellor et al., 2019). For that reason, a number of steps were taken to further remove identifying information from the dataset for the shared task. First, although Reddit is a site for anonymous discussion, it is possible for users to put identifying information in their self-selected user names; although most select names like awesomeprogrammer, in principle nothing on the site would prevent someone from naming herself mary-smith-UMDsophomore-born7July2002. Therefore the dataset replaces the self-selected user names with arbitrary numeric identifiers for the user id. Second, automatic p"
W19-3003,W19-3019,0,0.0346957,"Missing"
W19-3003,W19-3017,0,0.0426913,"Missing"
W19-3003,D18-2029,0,0.0146598,"end of the pipeline, where it acts as a metaclassifier on top of a set of CNN, Bi-LSTM, BiRNN and Bi-GRU neural networks. However, for both of those tasks, the primary runs do not generate the best unofficial macro-F1 score on the test set: a different variation on the CLaC approach, in which SVM uses as input both the neural features and the predicted class probabilities from the SVM, yields the best macro-F1 score, 0.533 for task A as compared with 0.481 for the primary system. On the other hand, the CAMH system, which uses a stacked parallel CNN with LIWC and a universal sentence encoder (Cer et al., 2018), produced the best unofficial F1 score for task C: 0.278 as compared to 0.268 for the CLaC primary system. For task B the best official score is 0.457, obtained by the HLAB team, where the system used logistic regression with features from SuicideWatch and non-SuicideWatch language that were processed separately. The best unofficial F1 score (0.504) is also obtained by HLAB system, using BERT features generated separately from SuicideWatch and non-SuicideWatch posts. 6 that the set of available posts excludes SuicideWatch and other mental health subreddits. However, the overall F1 score is lo"
W19-3003,W19-3025,0,0.0170208,"e-trained on Wikipedia and some other large corpora as-is in their system. The most commonly used neural architecture is convolutional neural networks (CNN) on the user or post level, where in the latter case an aggregation step is needed to produce the final outcome. Other frequently employed architectures were long-short term memory (LSTM) networks or recurrent neural networks (RNNs) and LSTMs with an attention mechanism. Some teams experimented with multichannel neural networks in a multi-task learning setting. sis (Baziotis et al., 2017), a tool set that is tailored for social media data. Iserman et al. (2019) applied two-stage error spelling correction using edit distance from augmented dictionary entries. 4.2 Approaches 4.3 Model inputs The submitted systems used a wide range of input representations on the post and user level. We can distinguish several main categories: • Embeddings on the word, sentence or document (post/posts) level. In addition to GloVe and word2vec, we mostly see the more recently introduced contextualized embedding techniques such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). • Lexicon-based features. Teams used dictionaries mainly to capture emotions repres"
W19-3003,W19-3018,0,0.0645536,"Missing"
W19-3003,W19-3020,0,0.0606997,"Missing"
W19-3003,W19-3005,0,0.280709,"Missing"
W19-3003,D14-1121,0,0.0621931,"tence or document (post/posts) level. In addition to GloVe and word2vec, we mostly see the more recently introduced contextualized embedding techniques such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). • Lexicon-based features. Teams used dictionaries mainly to capture emotions represented in the user’s posts. Examples of dictionaries used are NRC (Mohammad, 2017) and LIWC (Tausczik and Pennebaker, 2010). These features were generally represented as the normalized count of post per category. Other lexicons were employed to capture user-level features including age and gender (Sap et al., 2014), and assessment of the Big-5 personality traits (Schwartz et al., 2013). 5 5.1 Metrics The official metric used in this shared task is the macro-averaged F1 score. This metric was also used in previous CLPsych shared tasks that classified online posts into one of four labels (Milne et al., 2016; Milne, 2017); as a way of defining a single figure of merit, macro-averaging treats each class as contributing equally to performance, which helps avoid performance on a single class dominating the result when there is class imbalance (cf. Table 1). In addition, we adopt two metrics introduced in thos"
W19-3003,W16-0312,0,0.537043,"s well as information from wearables and other technologies. The platform has been adapted by their collaborators for research on other mental health topics, as well; for example, UMD.OurDataHelps.org collects data donations for a project focused on schizophrenia. 24 Proceedings of the Sixth Workshop on Computational Linguistics and Clinical Psychology, pages 24–33 c Minneapolis, Minnesota, June 6, 2019. 2019 Association for Computational Linguistics 2.2 designed for social media, similar in spirit to previous CLPsych shared tasks on four-way assessment of crisis risk in a peer support forum (Milne et al., 2016; Milne, 2017). In order to address ethical access to and sharability of data, we focus on materials collected from Reddit, where posts are public and anonymous, and further de-identified by us; see Section 2. A limitation of the tasks is that we lack information about actual outcomes (suicide attempts or competions); we instead use human annotations of risk level as a starting point. In that regard this year’s exercise can be viewed at minimum as establishing face validity for the idea of extracting meaningful signal related to suicidality from Reddit posts, and more optimistically as a step"
W19-3003,W18-0603,1,0.558451,"Missing"
W19-3003,W19-3004,0,0.375579,"e provide more details about the top systems per task. • Task C is about screening. This task simulates a scenario in which someone has opted in to having their social media monitored (e.g., a new mother at risk for postpartum depression, a veteran returning from a deployment, a patient whose therapist has suggested it) and the goal is to identify whether they are at risk even if they have not explicitly presented with a problem. Here predictions are made only from users posts that are not on SuicideWatch. Team Affective Computing ASU (Ambalavanan et al., 2019) CAMH † Chen et al. (2019) CLaC (Mohammadi et al., 2019) CMU (Allen et al., 2019) IBM data science (Morales et al., 2019) IDLab (Bitew et al., 2019) JXUFE † SBU-HLAB (Matero et al., 2019) TsuiLab (Ruiz et al., 2019) TTU (Iserman et al., 2019) UniOvi-WESO (Hevia et al., 2019) uOttawa † USI-UPF (R´ıssola et al., 2019) For all tasks, we provided participating teams with training and test data using an 80-20 split. In order to keep the original labels’ distribution in the split, we applied the proportional training/test split separately for each label. The statistics of the data are shown in Tables 1 and 2. Note the large number of posts in tasks B and"
W19-3003,W19-3023,0,0.0681832,"Missing"
W19-3003,N18-1202,0,0.0242716,"in a multi-task learning setting. sis (Baziotis et al., 2017), a tool set that is tailored for social media data. Iserman et al. (2019) applied two-stage error spelling correction using edit distance from augmented dictionary entries. 4.2 Approaches 4.3 Model inputs The submitted systems used a wide range of input representations on the post and user level. We can distinguish several main categories: • Embeddings on the word, sentence or document (post/posts) level. In addition to GloVe and word2vec, we mostly see the more recently introduced contextualized embedding techniques such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). • Lexicon-based features. Teams used dictionaries mainly to capture emotions represented in the user’s posts. Examples of dictionaries used are NRC (Mohammad, 2017) and LIWC (Tausczik and Pennebaker, 2010). These features were generally represented as the normalized count of post per category. Other lexicons were employed to capture user-level features including age and gender (Sap et al., 2014), and assessment of the Big-5 personality traits (Schwartz et al., 2013). 5 5.1 Metrics The official metric used in this shared task is the macro-averaged F1 score. This"
W19-3003,W19-3021,0,0.17912,"Missing"
W93-0307,C90-3029,0,0.106126,"Missing"
W93-0307,J87-3005,0,0.149145,"Missing"
W93-0307,H93-1054,1,0.817587,"Missing"
W93-0307,P84-1054,0,0.0409969,"Missing"
W93-0307,P90-1004,0,0.0866302,"Missing"
W93-0307,J82-3004,0,0.214561,"Missing"
W93-0307,J93-2006,0,\N,Missing
W93-0307,P91-1030,0,\N,Missing
W93-0307,J92-4003,0,\N,Missing
W95-0105,W94-0102,0,0.0417609,"occurrence with words or with other senses. Unfortunately, there are few corpora annotated with word sense information, and computing reliable statistics on word senses rather than words will require more data, rather than less. 1 Furthermore, one widely available example of a large, manually sense-tagged corpus - - the WordNet group&apos;s annotated subset of the Brown corpus 2 - - vividly illustrates the difficulty in obtaining suitable data. 1Actually,this depends on the fine-grainednessof sense distinctions;clearlyone could annotatecorpora with very high level semantic distinctionsFor example, Basili et al. (1994) take such a coarse-grainedapproach, utilizing on the order of 10 to 15 semantictags for a given domain. I assumethroughoutthis paper that finer-graineddistinctionsthan that are necessary. 2Available by anonymous ftp to clarity.princeton.edu as p u b / w n l . 4seracor. t a r . Z; Word_Netis described by Miller et al. (1990). 54 It is quite small, by current corpus standards (on the order of hundreds of thousands of words, rather than millions or tens of millions); the direct annotation methodology used to create it is labor intensive (Marcus et al. (1993) found that direct annotation takes tw"
W95-0105,P91-1047,0,0.0341198,"Missing"
W95-0105,J92-4003,0,0.377513,"as the output of distributional clustering algorithms. Disambiguation is performed with respect to WordNet senses, which are fairly fine-gained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented. 1 Introduction Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g. (Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtltze, 1993)). However, for many tasks, one is interested in relationships among word senses, not words. Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a ""semantically sticky"" group of words. As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so. Yet a computational system has no choice but to consider o"
W95-0105,P89-1010,0,0.0504722,"overed by WordNet are ignored. Although the WordNet noun taxonomy has multiple root nodes, a single, ""virtual"" root node is assumed to exist, with the original root nodes as its children. Note that by equations (1) through (3), if two senses have the virtual root node as their only upper bound then their similarity value is 0. Example. The following table shows the semantic similarity computed for several word pairs, in each case shown with the most informative subsumer. 6 Probabifities were estimated using the Penn Treebank version of the Brown corpus. The pairs come from an example given by Church and Hanks (1989), illustrating the words that human subjects most frequently judged as being associated with the word doctor. (The word sick also appeared on the list, but is excluded here because it is not a noun.) Word 1 doctor doctor doctor doctor doctor doctor doctor Word 2 nurse lawyer man medicine hospital health sickness Similarity 9.4823 7.2240 2.9683 1.0105 1.0105 0.0 0.0 Most Informative Subsumer (health professional) (professional person} (person, individual) &lt;entity} &lt;entity} virtual root virtual root Doctors are minimally similar to medicine and hospitals, since these things are all instances of"
W95-0105,W93-0106,0,0.07265,"Missing"
W95-0105,J93-2004,0,0.0285021,"Missing"
W95-0105,H93-1053,0,0.0387849,"Missing"
W95-0105,P93-1024,0,0.50829,"formed with respect to WordNet senses, which are fairly fine-gained; however, the method also permits the assignment of higher-level WordNet categories rather than sense labels. The method is illustrated primarily by example, though results of a more rigorous evaluation are also presented. 1 Introduction Word groupings useful for language processing tasks are increasingly available, as thesauri appear on-line, and as distributional techniques become increasingly widespread (e.g. (Bensch and Savitch, 1992; Brill, 1991; Brown et al., 1992; Grefenstette, 1994; McKcown and Hatzivassiloglou, 1993; Pereira et al., 1993; Schtltze, 1993)). However, for many tasks, one is interested in relationships among word senses, not words. Consider, for example, the cluster containing attorney, counsel, trial, court, and judge, used by Brown et al. (1992) to illustrate a ""semantically sticky"" group of words. As is often the case where sense ambiguity is involved, we as readers impose the most coherent interpretation on the words within the group without being aware that we are doing so. Yet a computational system has no choice but to consider other, more awkward possibilities - - for example, this cluster might be captur"
W95-0105,C92-2070,0,0.563907,"Missing"
W97-0209,H92-1046,0,0.139807,"Missing"
W97-0209,H93-1054,1,0.897808,"4), it takes as its starting point the assumption that senseannotated training text is not available. Motivating this assumption is not only the limited availability of such text at present, but skepticism that the situation will change any time soon. In marked contrast to annotated training material for partof-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas part-of-speech tag sets tend to differ in the details); m m m m m mm Selectional Preference as Statistical Association The treatment of selectionalpreference used here is that proposed by Resnik (1993a; 1996), combining statisticaland knowledge-based methods. The basis of the approach is a probabilistic model capturing the co-occurrence behavior of predicates and conceptual classes in the taxonomy. The intuitionis illustrated in Figure 1. The prior distribution PrR(c) captures the probability of a class occurring as the argument in predicate-argument relation R, regardless of the identity of the predicate. For example, given the verb-subject relationship,the prior probability for (person) tends to be significantlyhigher than the prior probability for (insect). However, once the identity of"
W97-0209,C94-2123,0,0.0191172,"pected to have higher posterior probabilities,compared to their priors, as is the case for (insect) in Figure 1. Formally, selectional association is defined as: Am(p, c) -- 1 Pr(c[p) Pr(c[p) log Pr(c) &quot; This model of selectional preference has turned out to make reasonable predictions about human judgments of argument plausibilityobtained by psycholinguistic methods (Resnik, 1993a). Closely related proposals have been applied in syntactic disambiguation (Resnik, 1993b; Lauer, 1994) and to automatic acquisition of more KatzFodoresque selection restrictionsin the form of weighted disjunctions (Ribas, 1994). The selectional association has also been used recently to explore apparent cases of syntactic optionality (Paola Merlo, personal communication). 3 Estimation Issues If taxonomic classes were labeled explicitly in a training corpus, estimation of probabilities in the model would be fairly straightforward. But since text corpora contain words, not classes, it is necessary to treat each occurrence of a word in an argument position as if it might represent any of the conceptual classes to which it belongs, and assign frequency counts accordingly. At present, this is done by distributing the &quot;cr"
W97-0209,C92-2070,0,0.436351,"conceptual classes containing an observed argument. Formally, given a predicate-argument relationship R (for example, the verb-object relationship), a predicate p, and a conceptual class c, freqR(p,c) ~ where countR(p, w) is the number of times word w was observed as the argument of p with respect to R, and classes(w) is the number of taxonomic classes to which w belongs. Given the frequencies, probabilitiesare currently estimated using m a x i m u m likelihood; the use of word classes is itselfa form of smoothing (cf. Pereira et al. (1993)).I This estimation method is similarto that used by Yarowsky (1992) for Roget&apos;s thesaurus categories, and works for similar reasons. As an example, consider two instances of the verb-object relationshipin a training corpus, drink coffee and drink wine. Coffee has 2 senses in the WordNet 1.4 noun taxonomy, and belongs to 13 classes in all, and wine has 2 senses and belongs to a total of 16 classes. This means that the observed countverb_obj(drink, coffee) = 1 will be distributed by adding 1-~ to the joint frequency with drink for each of the 13 classes containing coffee. Similarly, the joint frequency with drink will be incremented by ~ for each of the 16 clas"
W97-0209,H93-1052,0,0.183247,"Missing"
W97-0209,P94-1052,0,0.0655372,"Missing"
W97-0209,H93-1061,0,0.0877715,"= = { c l c is an ancestor ofsi} max AR(p,c) cEC~ and assign as as the score for sense st. The simplest way to use the resulting scores, following Miller et al. (1994), is as follows: if n has only one sense, select it; otherwise select the sense st for which at is greatest, breaking ties by random choice. 5 Evaluation Task and materials. Test and training materials were derived from the Brown corpus of American English, all of which has been parsed and manually verified by the Penn T~eebank project (Marcus et al., 1993) and parts of which have been manually sense-tagged by the WordNet group (Miller et al., 1993). A parsed, sense-tagged corpus was obtained by mergingthe WordNet sense-tagged corpus (approximately 200,000 words of source text from the Brown corpus, distributed across genres) with the corresponding Penn Treebank parses,a The rest of the Brown corpus (approximately 800,000 words of source text) remained as a parsed, but not sensetagged, training set. 3(1) Written message, (2) varsity letter, (3) alphabetic character. 3The merge was mostly automatic, requiring manual intervention for only 3 of 103 files. The test set for the verb-object relationshipwas constructed by firsttraining a select"
W97-0209,H94-1046,0,0.375257,"ext of Mary drank burgundy, because the verb drink specifies the selection restriction +LIQUID for its direct objects. Problems with this approach arise, however, as soon as the domain of interest becomes too large or too rich to specify semantic features and selection restrictions accurately by hand. This paper concerns the use of selectional constraints for automatic sense disambiguation in such broad-coverage settings. The approach combines statistical and knowledge-based methods, but unlike many recent corpus-based approaches to sense disambiguation (¥arowsky, 1993; Bruce and Wiebe, 1994; Miller et al., 1994), it takes as its starting point the assumption that senseannotated training text is not available. Motivating this assumption is not only the limited availability of such text at present, but skepticism that the situation will change any time soon. In marked contrast to annotated training material for partof-speech tagging, (a) there is no coarse-level set of sense distinctions widely agreed upon (whereas part-of-speech tag sets tend to differ in the details); m m m m m mm Selectional Preference as Statistical Association The treatment of selectionalpreference used here is that proposed by Re"
W97-0209,P93-1024,0,0.0693503,"e by distributing the &quot;credit&quot; for an observation uniformly across all the conceptual classes containing an observed argument. Formally, given a predicate-argument relationship R (for example, the verb-object relationship), a predicate p, and a conceptual class c, freqR(p,c) ~ where countR(p, w) is the number of times word w was observed as the argument of p with respect to R, and classes(w) is the number of taxonomic classes to which w belongs. Given the frequencies, probabilitiesare currently estimated using m a x i m u m likelihood; the use of word classes is itselfa form of smoothing (cf. Pereira et al. (1993)).I This estimation method is similarto that used by Yarowsky (1992) for Roget&apos;s thesaurus categories, and works for similar reasons. As an example, consider two instances of the verb-object relationshipin a training corpus, drink coffee and drink wine. Coffee has 2 senses in the WordNet 1.4 noun taxonomy, and belongs to 13 classes in all, and wine has 2 senses and belongs to a total of 16 classes. This means that the observed countverb_obj(drink, coffee) = 1 will be distributed by adding 1-~ to the joint frequency with drink for each of the 13 classes containing coffee. Similarly, the joint f"
W97-0213,W96-0208,0,0.0238627,"e several observations about the state of the art in automatic word sense disambiguation. Motivated by these observations, we offerseveral specificproposals to the community regarding improved evaluation criteria,common training and testing resources, and the definition of sense inventories. 1 In contrast,most previous work in word sense disambiguation has tended to use differentsets of polysemous words, differentcorpora and differentevaluation metrics. Some clustersof studies have used common testsuites,most notably the 2094-word Hne data of Leacock et al. (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Also, researchers have tended to keep their evaluation data and procedures somewhat standard across theirown studies for internally consistent comparison. Nevertheless,there are nearly as many test suitesas there are researchers in this field. Introduction Word sense disambiguation (WSD) is perhaps the great open problem at the lexical level of natural language processing. If one requires part-of-speech tagging for some task, it is now possible to obtain high performance off the shelf; if one needs morphological analysis, softwa"
W97-0213,J92-4003,0,0.00751697,"Missing"
W97-0213,P94-1020,0,0.0426629,"is small and fairly standard. • Context outside the current sentence has little influence. • The within-sentence dependencies are very local. • Prior (decontextuaUzed) probabilities dominate in many cases. • The task can generally be accomplished successfully using only tag-level models without lexical sensitivities besides the priors. • Standard annotated corpora of adequate size have long been available. Table 1: Some properties of the POS tagging task. In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (¥arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al., 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word. 3 where N is the number of test instances and Pr~t is the probability assigned by the algorithm A to the correct sense, c.sl of polysemous word wi in contexti. Crucially, given the hypothetical case ab"
W97-0213,A88-1019,0,0.0110063,"or manual annotation. Given the data requirements for supervised learning algorithms and the current paucity of such data, we believe that unsupervised and minimally supervised methods offer the primary near-term hope for broad-coverage sense tagging. However, we see strong future potential for supervised algorithms using many types of aligned bilingual corpora for many types of sense distinctions. Observation 4. T h e field has narrowed d o w n approaches, b u t only a little. In the area of partof-speech tagging, the noisy channel model dominates (e.g. (Bald and Mercer, 1976; Jelinek, 1985; Church, 1988)), with transformational role-based methods (Brill, 1993) and grammatico-statistical hybrids (e.g. (Tapanainen and Voutilainen, 1994)) also having a presence. Regardless of which of these approaches one takes, there seems to be consensus on what makes part-of-speech tagging successful: • The inventory of tags is small and fairly standard. • Context outside the current sentence has little influence. • The within-sentence dependencies are very local. • Prior (decontextuaUzed) probabilities dominate in many cases. • The task can generally be accomplished successfully using only tag-level models w"
W97-0213,H94-1046,0,0.0753447,"Missing"
W97-0213,W96-0306,0,0.0500962,"Missing"
W97-0213,C96-1055,0,0.025232,"Missing"
W97-0213,A94-1008,0,0.0206725,"uch data, we believe that unsupervised and minimally supervised methods offer the primary near-term hope for broad-coverage sense tagging. However, we see strong future potential for supervised algorithms using many types of aligned bilingual corpora for many types of sense distinctions. Observation 4. T h e field has narrowed d o w n approaches, b u t only a little. In the area of partof-speech tagging, the noisy channel model dominates (e.g. (Bald and Mercer, 1976; Jelinek, 1985; Church, 1988)), with transformational role-based methods (Brill, 1993) and grammatico-statistical hybrids (e.g. (Tapanainen and Voutilainen, 1994)) also having a presence. Regardless of which of these approaches one takes, there seems to be consensus on what makes part-of-speech tagging successful: • The inventory of tags is small and fairly standard. • Context outside the current sentence has little influence. • The within-sentence dependencies are very local. • Prior (decontextuaUzed) probabilities dominate in many cases. • The task can generally be accomplished successfully using only tag-level models without lexical sensitivities besides the priors. • Standard annotated corpora of adequate size have long been available. Table 1: Som"
W97-0213,H93-1051,0,0.150301,"Missing"
W97-0213,C92-2070,0,0.0567223,"word sense disambiguation. Motivated by these observations, we offerseveral specificproposals to the community regarding improved evaluation criteria,common training and testing resources, and the definition of sense inventories. 1 In contrast,most previous work in word sense disambiguation has tended to use differentsets of polysemous words, differentcorpora and differentevaluation metrics. Some clustersof studies have used common testsuites,most notably the 2094-word Hne data of Leacock et al. (1993), shared by Lehman (1994) and Mooney (1996) and evaluated on the system of Gale, Church and Yarowsky (1992). Also, researchers have tended to keep their evaluation data and procedures somewhat standard across theirown studies for internally consistent comparison. Nevertheless,there are nearly as many test suitesas there are researchers in this field. Introduction Word sense disambiguation (WSD) is perhaps the great open problem at the lexical level of natural language processing. If one requires part-of-speech tagging for some task, it is now possible to obtain high performance off the shelf; if one needs morphological analysis, software and lexical data are not too hard to find. In both cases, per"
W97-0213,H93-1052,0,0.0276563,"ependencies are very local. • Prior (decontextuaUzed) probabilities dominate in many cases. • The task can generally be accomplished successfully using only tag-level models without lexical sensitivities besides the priors. • Standard annotated corpora of adequate size have long been available. Table 1: Some properties of the POS tagging task. In contrast, approaches to WSD attempt to take advantage of many different sources of information (e.g. see (McRoy, 1992; Ng and Lee, 1996; Bruce and Wiebe, 1994)); it seems possible to obtain benefit from sources ranging from local collocational clues (Yarowsky, 1993) to membership in semantically or topically related word classes (¥arowsky, 1992; Resnik, 1993) to consistency of word usages within a discourse (Gale et al., 1992); and disambignation seems highly lexically sensitive, in effect requiring specialized disamhignators for each polysemous word. 3 where N is the number of test instances and Pr~t is the probability assigned by the algorithm A to the correct sense, c.sl of polysemous word wi in contexti. Crucially, given the hypothetical case above, the sense disambiguation algorithm in System 1 would get much of the credit for assigning high probabi"
W97-0217,H93-1051,0,0.174088,"Missing"
W97-0217,W96-0208,0,0.0409062,"Missing"
W97-0217,W97-0213,1,0.90538,"Missing"
