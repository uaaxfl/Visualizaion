2020.bea-1.9,2020.lrec-1.883,1,0.845269,"Missing"
2020.bea-1.9,W11-2308,1,0.842233,"Missing"
2020.bea-1.9,W18-4606,0,0.0372641,"of grammatical structures’ competence with respect to predefined grades, such as the Common European Framework of Reference for Languages (CEFRL) (Zilio et al., 2018). Given the difficulty of defining a unique indicator of linguistic complexity in the context of L2 language development, a great variety of features from all linguistic levels have been used as input for supervised classification systems trained on authentic learner data for different L2s. Such is the case e.g. of Hancke and Meurers (2013) and Vajjala and L˙eo (2014), dealing with L2 German and L2 Estonian, respectively, and of Pilán and Volodina (2018), who also provided a features analysis focused on predictive features extracted from both receptive and productive texts in Swedish L2. This paper adopts this framework and presents an innovative NLP-based stylometric approach to model writing development in learners of Spanish as a second and Heritage language. Our approach relies on a wide set of linguistically motivated features extracted from students’ essays, which have already been shown relevant for a number of tasks related to modelling the ‘form’ of a text rather than the content. While the majority of previous studies on the evoluti"
2020.bea-1.9,W11-0314,1,0.840214,"Missing"
2020.bea-1.9,W14-1820,1,0.845606,"Missing"
2020.bea-1.9,P05-1025,1,0.579616,"arners, in both the first and second language context. The empirical evidence acquired from learner corpora, complemented with the increased reliability of linguistic features extracted by computational tools and machine learning approaches, has promoted a better understanding of learners’ language properties and how they change across time and increasing proficiency level (Crossley, 2020). A first line of research has focused on providing automatic ways of operationalizing sophisticated metrics of language development to alleviate the laborious manual computation of these metrics by experts (Sagae et al., 2005; Lu, 2009). A second line of research has taken the more challenging step of implementing completely data-driven approaches, which use a variety of linguistic features extracted from texts to automatically assign a learner’s language production to a given developmental level (Lubetich and Sagae, 2014). A great amount of work has been carried out in the field of second language acquisition where 92 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–101 c July 10, 2020. 2020 Association for Computational Linguistics the best of our knowled"
2020.bea-1.9,W01-0521,0,0.122749,"duction in a L2 (Cimino et al., 2018). Interestingly, for all mentioned tasks the set of linguistic features plays a very important role in the classification not only of a whole document but also of each single sentence. This is the reason why, as reported in the following sections, we modelled the prediction of the development of writing skills both as document and sentence classification tasks. Although we used a state–of–the art pipeline, it is well-acknowledged that the accuracy of statistical parsers decreases when tested against texts of a different typology from that used in training (Gildea, 2001). In this respect, learners’ data are particularly challenging for general–purpose text analysis tools since they can exhibit deviation from correct and standard language; for instance, missing or anomalous use of punctuation (especially in 1st grade prompts) already impacts on the coarsest levels of text processing, i.e. sentence splitting, and thus may affect all subsequent levels of annotation. F L) Ccl = blog2 f freq(M req(CL) c, where MFW and MFL are the most frequent word form/lemma in the corpus and CW and CL are the considered ones. A first overview of how and to what extent all these"
2020.bea-1.9,L16-1680,0,0.0122333,"uency for each word form/lemma was computed exploiting the Spanish Wikipedia (dump of March 2020) using the FW) following measures: Ccw = blog2 f freq(M req(CW ) c, Linguistic Features The set of linguistic features considered as predictors of L2 written competence evolution is based on those described in Brunato et al. (2020). It includes a wide range of text properties, from raw text features, to lexical, morpho-syntactic and syntactic properties, which were extracted from different levels of linguistic annotation. For this purpose, the COWS-L2H Corpus was automatically parsed using UDPipe (Straka et al., 2016) trained on the Spanish Universal Dependency Treebank (GSD section), version 2.5. We rely on these features since it has been shown that they have a high predictive power for several tasks all aimed at modelling the linguistic form of documents. This is the case for example of the automatic readability assessment task (Dell’Orletta et al., 2011a), of the automatic classification of the textual genre of documents (Cimino et al., 2017), or also of the automatic identification of the L1 of a writer based on his/her language production in a L2 (Cimino et al., 2018). Interestingly, for all mentione"
2020.bea-1.9,C14-1203,1,0.327745,"age properties and how they change across time and increasing proficiency level (Crossley, 2020). A first line of research has focused on providing automatic ways of operationalizing sophisticated metrics of language development to alleviate the laborious manual computation of these metrics by experts (Sagae et al., 2005; Lu, 2009). A second line of research has taken the more challenging step of implementing completely data-driven approaches, which use a variety of linguistic features extracted from texts to automatically assign a learner’s language production to a given developmental level (Lubetich and Sagae, 2014). A great amount of work has been carried out in the field of second language acquisition where 92 th Proceedings of the 15 Workshop on Innovative Use of NLP for Building Educational Applications, pages 92–101 c July 10, 2020. 2020 Association for Computational Linguistics the best of our knowledge, the first data–driven study which uses linguistic features from student data to model the evolution of written language competence in Spanish as a Second Language (SSL); (ii) we show that it is possible to automatically predict the relative order of two essays written by the same student at differe"
2020.bea-1.9,W14-3509,0,0.0681894,"Missing"
2020.bea-1.9,L18-1650,0,0.0261058,"ifornia Davis alessio.miaschi@phd.unipi.it, {ssdavidson,sagae,chsanchez}@ucdavis.edu, {dominique.brunato,felice.dellorletta,giulia.venturi}@ilc.cnr.it Abstract the study of L2 writings is seen as a proxy of language ability development (Crossley, 2020). In this respect, much related work is devoted to predicting the degree of second language proficiency according to expert–based evaluation (Crossley and McNamara, 2012) or to modelling the evolution of grammatical structures’ competence with respect to predefined grades, such as the Common European Framework of Reference for Languages (CEFRL) (Zilio et al., 2018). Given the difficulty of defining a unique indicator of linguistic complexity in the context of L2 language development, a great variety of features from all linguistic levels have been used as input for supervised classification systems trained on authentic learner data for different L2s. Such is the case e.g. of Hancke and Meurers (2013) and Vajjala and L˙eo (2014), dealing with L2 German and L2 Estonian, respectively, and of Pilán and Volodina (2018), who also provided a features analysis focused on predictive features extracted from both receptive and productive texts in Swedish L2. This"
2020.coling-main.65,Q19-1004,0,0.0604358,"(ii) we showed that contextualized representations tend to lose their precision in encoding a wide range of linguistic properties after a fine-tuning process; (iii) we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang a"
2020.coling-main.65,I17-1001,0,0.061881,"Missing"
2020.coling-main.65,P18-2003,0,0.0501628,"(Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and th"
2020.coling-main.65,2020.lrec-1.883,1,0.676715,"Missing"
2020.coling-main.65,W19-4828,0,0.0228971,"Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less stud"
2020.coling-main.65,P18-1198,0,0.184472,"Ms (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for synt"
2020.coling-main.65,W14-1820,1,0.849302,"Missing"
2020.coling-main.65,N19-1423,0,0.217206,"ds to lose this information when trained on specific downstream tasks. We also find that BERT’s capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence. 1 Introduction Neural Language Models (NLMs) have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition o"
2020.coling-main.65,D19-1275,0,0.0110547,"techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of"
2020.coling-main.65,N19-1419,0,0.0387266,"en representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. Liu et al. (2019), instead, quantified differences in the transferability of individual layers"
2020.coling-main.65,N19-1357,0,0.0229686,"ks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they make when solving specific downstream tas"
2020.coling-main.65,P19-1356,0,0.0412186,"Missing"
2020.coling-main.65,D19-1445,0,0.109142,"s its ability to solve NLI downstream tasks: the more BERT stores information about these features, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even"
2020.coling-main.65,N16-1082,0,0.0270966,"es after a fine-tuning process; (iii) we showed that the linguistic knowledge stored in the contextualized representations of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs"
2020.coling-main.65,W19-4825,0,0.0300261,"arstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied. In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT (Devlin et al., 2019), and how these properties affect its predictions when solving a specific downstream task. We defined three research questions aimed at understanding: (i) what kind of linguistic properties are already encoded in a pre-trained version of BERT and where across its 12 layers; (ii) how the knowledg"
2020.coling-main.65,N19-1112,0,0.126438,"2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. Liu et al. (2019), instead, quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. 3 Our Approach To probe the linguistic knowledge encoded by BERT and understand how it affects its predictions in several classification problems, we relied on a suite of 68 probing tasks, each of which corresponds to a distinct feature capturing lexical, morpho–syntactic and syntactic properties of a sentence. Specifically, 746 Le"
2020.coling-main.65,C14-1203,0,0.0121555,"different This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 745 Proceedings of the 28th International Conference on Computational Linguistics, pages 745–756 Barcelona, Spain (Online), December 8-13, 2020 L1 languages. Particularly relevant for our study, is that multi-level linguistic features have been shown to have a highly predictive role in tracking the evolution of learners’ linguistic competence across time and developmental levels, both in first and second language acquisition scenarios (Lubetich and Sagae, 2014; Miaschi et al., 2020). Given the strong informative power of these features to encode a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classify"
2020.coling-main.65,W17-5007,0,0.0213566,"code a variety of language phenomena across stages of acquisition, we assume that they can be also helpful to dig into the issues of interpretability of NLMs. In particular, we would like to investigate whether features successfully exploited to model the evolution of language competence can be similarly helpful in profiling how the implicit linguistic knowledge of a NLM changes across layers and before and after tuning on a specific downstream task. We chose the NLI task, i.e. the task of automatically classifying the L1 of a writer based on his/her language production in a learned language (Malmasi et al., 2017). As shown by Cimino et al. (2018), linguistic features play a very important role when NLI is tackled as a sentence–classification task rather than as a traditional document–classification task. This is the reason why we considered the sentence-level NLI classification as a task particularly suitable for probing the NLM linguistic knowledge. Finally, we investigated whether and which linguistic information encoded by BERT is involved in discriminating the sentences correctly or incorrectly classified by the fine-tuned models. To this end, we tried to understand if the linguistic knowledge tha"
2020.coling-main.65,D18-1151,0,0.0284819,"guage Models (NLMs) have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Beli"
2020.coling-main.65,2020.repl4nlp-1.15,1,0.530254,"Missing"
2020.coling-main.65,2020.bea-1.9,1,0.810718,"Missing"
2020.coling-main.65,N18-1202,0,0.048471,"e of linguistic characteristics, but it tends to lose this information when trained on specific downstream tasks. We also find that BERT’s capacity to encode different kind of linguistic properties has a positive influence on its predictions: the more it stores readable linguistic information of a sentence, the higher will be its capacity of predicting the expected label assigned to that sentence. 1 Introduction Neural Language Models (NLMs) have become a central component in NLP systems over the last few years, showing outstanding performance and improving the state-of-the-art on many tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019;"
2020.coling-main.65,W18-5431,0,0.0240308,"ions of BERT positively affects its ability to solve NLI downstream tasks: the more BERT stores information about these features, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney"
2020.coling-main.65,N19-1329,0,0.019033,"res, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. ("
2020.coling-main.65,silveira-etal-2014-gold,0,0.150192,"Missing"
2020.coling-main.65,W18-6304,0,0.0270087,"the-art on many tasks (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019). However, the introduction of such systems has come at the cost of interpretability and, consequently, at the cost of obtaining meaningful explanations when automated decisions take place. Recent work has begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they make when solving"
2020.coling-main.65,P19-1452,0,0.115481,"a even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the analysis and interpretation of attention mechanisms (Tang et al., 2018; Jain and Wallace, 2019; Clark et al., 2019) and on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. Probing models trained on different contextual representations provided evidences that such models are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). However, the way in which this knowledge affects the decisions they make when solving specific downstream tasks has been less studied. In this paper, we extended prior work by studying the linguistic properties encoded by one of the most prominent NLM, BERT (Devlin et al., 2019), and how these properties affect its predictions when solving a specific downstream task. We defined three research questions aimed at understanding: (i) what kind of linguistic properties"
2020.coling-main.65,W19-4808,0,0.0113016,"NLI downstream tasks: the more BERT stores information about these features, the higher will be its capacity of predicting the correct label. 2 Related Work In the last few years, several methods have been devised to obtain meaningful explanations regarding the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extractio"
2020.coling-main.65,W18-5448,0,0.0622513,", 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019), studying correlations between representations (Saphra and Lopez, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019; Miaschi and Dell’Orletta, 2020). These latter studies demonstrated that NLMs are able to encode a variety of language properties in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic prope"
2020.evalnlgeval-1.5,W05-0909,0,0.0610457,"ed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content"
2020.evalnlgeval-1.5,E09-1014,0,0.0607084,"Missing"
2020.evalnlgeval-1.5,2020.lrec-1.828,1,0.810698,"Missing"
2020.evalnlgeval-1.5,E14-1074,0,0.0600624,"Missing"
2020.evalnlgeval-1.5,P17-4012,0,0.0122863,"ifferent frameworks with different takes on the same problem: (a) as a true translation task, where given a headline in one style, the model learns to generate a new headline in the target style; (b) as a summarisation task, where headlines are viewed as an extreme case of summarisation and generated from the article. We exploit article-headline generators trained on opposite sources to do the transfer. This approach does not in principle require parallel data for training. For the translation approach (S2S), we train a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as"
2020.evalnlgeval-1.5,W19-8643,1,0.892211,"Missing"
2020.evalnlgeval-1.5,W04-1013,0,0.0256706,", 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evalua"
2020.evalnlgeval-1.5,N19-1049,0,0.0483085,"Missing"
2020.evalnlgeval-1.5,N18-2012,0,0.0472247,"Missing"
2020.evalnlgeval-1.5,D17-1238,0,0.0311446,"Missing"
2020.evalnlgeval-1.5,P02-1040,0,0.114378,"nsfer (Fu et al., 2018; Mir et al., 2019; Luo et al., 38 Proceedings of the 1st Workshop on Evaluating NLG Evaluation, pages 38–43, Online (Dublin, Ireland), December 2020. 2019), can shed light on differences between models. We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’"
2020.evalnlgeval-1.5,N18-1012,0,0.0198065,"summarisation system SUM does better at content preservation (HH and AH) than S2S1. However, its scores on the main classifier are worse in both transfer directions, as well as on average. The average compliancy score is higher for S2S1. In summary, for data which is not strongly aligned, our methods suggest that style transfer is better when conceived as a translation task. BLEU is higher for SUM, but the overall extremely low scores across the board suggest that it might not be a very informative metric for this setup, although commonly used to assess content preservation in style transfer (Rao and Tetreault, 2018). Our HH and AH classifiers appear more indicative in this respect, and ROUGE scores seem to correlate a bit more with them, when compared to BLEU. It remains to be investigated whether BLEU, ROUGE, and our content-checking classifiers do in fact measure something similar or not. With better-aligned data (bottom panel), the picture is more nuanced. Here, the main comparison is between two systems trained on strongly aligned data, one of which (S2S2) has additional, weakly aligned data. The overall compliancy score suggests that this improves style transfer (and this system is also the top perf"
2020.evalnlgeval-1.5,J18-3002,0,0.0235647,"Missing"
2020.evalnlgeval-1.5,J09-4008,0,0.0932183,"Missing"
2020.evalnlgeval-1.5,W02-2113,0,0.189512,"Missing"
2020.evalnlgeval-1.5,P17-1099,0,0.0166246,"ain a supervised BiLSTM sequence-to-sequence model with attention from OpenNMT (Klein et al., 2017) 1 Note that all sets also always contain the headlines’ respective full articles, though these are not necessarily used. to map the headline from left-wing to right-wing, and viceversa. Since the model needs parallel data, we exploit the aligned headlines for training. We experiment with three differently composed training sets, varying not only in size, but also in the strength of the alignment, as shown in Figure 1b. For the summarisation approach (SUM), we use two pointer-generator networks (See et al., 2017), which include a pointing mechanism able to copy words from the source as well as pick them from a fixed vocabulary, thereby allowing better handling of out-of-vocabulary words. ability to reproduce novel words. One model is trained on the la Repubblica portion of the training set, the other on Il Giornale. In a style transfer setting we use these models as follows: Given a headline from Il Giornale, for example, the model trained on la Repubblica can be run over the corresponding article from Il Giornale to generate a headline in the style of la Repubblica, and vice versa. To train the model"
2020.evalnlgeval-1.5,2020.acl-main.704,0,0.0162999,". We also add some untrained automatic metrics for evaluation. As observed above, the fact that humans cannot perform this task reliably makes it impossible to choose such metrics based on good correlations with human judgement (Celikyilmaz et al., 2020). Therefore, relying on previous work, we compare the insights gained from our classifiers with those obtained from BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004), since they are commonly used metrics to assess performance for content preservation and summarisation. Other common metrics such as METEOR (Banerjee and Lavie, 2005) and BLEURT (Sellam et al., 2020), which in principle would be desirable to use, are not applicable to our use case as they require resources not available for Italian. More specifically, we train a classifier which, given a headline coming from one of two newspapers with distinct ideological leanings and in-house styles, can identify the provenance of the headline with high accuracy. We use this (the ‘main’ classifier) to evaluate the success of a model in regenerating a headline from one newspaper, in the style of the other. We add two further consistency checks, both of which aim at content assessment, and are carried out"
2020.lrec-1.114,W13-2308,1,0.803707,"nce of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,573 103,005 3,487 – 2,445 3,050 14,659 Table 2: For each genre, number of tokens manually revis"
2020.lrec-1.114,P05-1045,0,0.00891135,"ry (De Mauro, 2000): following Giuliani et al. (2005) who demonstrate the stability and conservativeness of the Italian lexicon throughout the centuries, we took the Basic Italian Dictionary as a reference, starting from the assumption that the most important areas of experience continue to be mostly indicated by a stable nucleus of words in use for at least seven centuries. 3.2. Named Entities Annotation The Named Entity annotation of VGG-Silver was carried out with an adaption of the CoLingLab Named Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian w"
2020.lrec-1.114,magnini-etal-2006-cab,0,0.0845611,"d Entity Recognizer (NER) described in Passaro and Lenci (2014) and based on the Stanford NER (Finkel et al., 2005). Our NE label set was composed of three tags: PER for people’s proper names (e.g., Luigi Cadorna), LOC for locations (e.g., passo di Monte Croce), and ORG for organizations, including military formations (e.g., 2◦ Reggimento Bersaglieri) (see Figure 1). To annotate VGG-Silver we concatenated two distinct NER models, both trained on the same dataset. In order to adapt the NER to the Italian variety represented in the VGG corpus, the training 914 data consisted of the ICAB corpus (Magnini et al., 2006), in which the Geo-Political Entities (GPE) were converted into the LOC tag, and the Italian war bulletins of World War I (Boschetti et al., 2014), in which the military formations originally tagged as MIL were converted into ORG. Moreover, we added three texts from VGG (a sample from Mussolini’s diary, a parliamentary report and a speech by the politician Claudio Treves), whose NEs were manually annotated. Only the second model was also trained on a gazetteer including person names collected from the online Italian Treccani Encyclopedia,4 WWI military organization names gathered from Wikipedi"
2020.lrec-1.114,onelli-etal-2006-diacoris,0,0.0245846,"reover, they are not economically attractive, due to their limited use to develop downstream applications with a large-scale economic impact. Still, historical corpora represent an invaluable asset in the era of Digital Humanities, given the growing interest in applying quantitative and computational methods to diachronic linguistics and historical text analysis (Tahmasebi et al., 2019). Italian makes not exception to this trend, as diachronic corpora are still few, among which it is worth pointing out the Corpus OVI dell’Italiano antico and the Corpus TLIO (by OVI– CNR), the DiaCORIS corpus (Onelli et al., 2006) and the MIDIA corpus (Gaeta et al., 2013). One major shortcoming of such resources is the extremely large timespan they cover in comparison to their limited size. The project Voci della Grande Guerra (“Voices of the Great War”) (in short, VGG)1 aimed at filling this gap by creating the largest digital corpus to date of Italian texts at the time of World War I (WWI). The corpus includes a selection of texts representative of different textual genres and registers, including popular Italian. VGG texts have been 1 http://www.vocidellagrandeguerra.it/ automatically annotated with state-of-the-art"
2020.lrec-1.114,L16-1680,0,0.0216851,"cated venture (Piotrowski, 2012), due to e.g. the absence of standardized spelling, the occurrence of historical variants of words as well as peculiar syntactic structures. For these reasons, contemporary tools for linguistic analysis are generally not suitable for processing historical texts and need to be specialized with respect to the peculiarities of the historical variety of language to be processed. The annotation methodology we have employed for the annotation of the VGG corpus was articulated in the following steps: 1. automatic annotation of representative sample texts using UDPipe (Straka et al., 2016) trained on the Italian Universal Dependency Treebank (IUDT), version 2.0 (Bosco et al., 2013); 913 Text genre Diary (Gadda, Martini, Sonnino) Discourse (D’Annunzio, Morgari, Salandra, Salvemini, Treves, Turati; dichiarazioni del Partito Socialista) Essay (Croce, Gemelli, Gentile) Letters (Fontana, Monteleone, Monti, Procacci, Raviele) Memoir (Cadorna, Jahier, Monelli, Prezzolini, Soffici) Report (Comitati Segreti della Camera dei Deputati) Total Tok. + Lemm. + Meta–ling. 93,287 Morpho–synt. 49,868 Dep. Syntax 3,050 52,734 7,792 2,627 17,876 95,248 157,812 83,122 500,079 9,524 5,310 22,938 7,5"
2020.lrec-1.828,W10-4201,0,0.222329,"2018). Similarly, humans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful g"
2020.lrec-1.828,E09-1014,0,0.238704,"mans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful given that human judgemen"
2020.lrec-1.828,E14-1074,0,0.528122,"Missing"
2020.lrec-1.828,W17-4912,0,0.0154511,"ish (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017; Hu et al., 2017; Keskar et al., 2019, e.g.), and on attempts to define best practices for human and automatic evaluation (van der Lee et al., 2019), reliable and shared evaluation metrics and strategies concerning style-aware generation are still lacking (Fu et al., 2018). As a contribution to this aspect, we develop style-aware headline generation models, and discuss an evaluation strategy based on text classification, which is particularly useful given that human judgement for this task is found to be unreliable. While the strategy of using classification as evaluation is in itself not new"
2020.lrec-1.828,P16-2051,0,0.0413101,"Missing"
2020.lrec-1.828,N18-1169,0,0.0245404,"judgement. Besides the works mentioned in the Introduction to frame the problem, we will not discuss further related work on style modelling or summarisation. Rather, we concentrate on discussing previous works that make use of automatic classification for the evaluation of NLG systems, also to show in what sense our approach differ from existing ones. Using a classifier to assess the goodness of generated texts in connection to a broad definition of style-aware generation has been used in several previous works (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (20"
2020.lrec-1.828,D16-1230,0,0.0169047,", which are a prime tool to capture attention and make clear statements about the newspaper’s position over a certain event. Can this newspaper-specific style be distinguished? And is it preserved in automatically generated headlines? To answer such questions, we train newspaper-specific headline generation models, and evaluate how style-compliant the generated headline is for a given newspaper. How such evaluation can be performed though is yet another research question of its own. Evaluating generated text just using standard metrics based on lexical overlap is normally not accurate enough (Liu et al., 2016). In machine translation, for example, the decisive, final system evaluation is typically human-based, as the lexically-based BLEU score is not exhaustive. Automatic evaluation strategies are still used because human evaluation is expensive, not always available, and complex to include in highly iterative developments. However, human evaluation is not always a decisive and accurate strategy, since there might be aspects of text that for people are not so easy to grasp. For example, in profiling, where differently from the assessment of the goodness of translated text, evaluation can be perform"
2020.lrec-1.828,W15-0620,0,0.0118382,"t always a decisive and accurate strategy, since there might be aspects of text that for people are not so easy to grasp. For example, in profiling, where differently from the assessment of the goodness of translated text, evaluation can be performed against discrete gold labels, several studies found that humans are definitely not better than machines in identifying the gender of a writer (Koppel et al., 2002; Flekova et al., 2016; van der Goot et al., 2018). Similarly, humans failed to outperform automatic systems in recognising the native language of nonEnglish speakers writing in English (Malmasi et al., 2015). Baroni and Bernardini (2005) also find that seven out of ten subjects, including professional translators, performed worse than a simple SVM at the task of telling apart original from translated texts. More generally, Gatt and Krahmer (2018) have observed that it is difficult to ascertain if readers can perceive subtle stylistic variations, and past human-based evaluations of style have indeed shown very low inter-rater agreement (Belz and Kow, 2010; Cahill and Forst, 2009; Dethlefs et al., 2014). In spite of a recent surge of works focusing on style in generation (Ficler and Goldberg, 2017;"
2020.lrec-1.828,D15-1221,0,0.0467534,"Missing"
2020.lrec-1.828,P18-1080,0,0.0137694,"overcome the limitation of unreliable human judgement. Besides the works mentioned in the Introduction to frame the problem, we will not discuss further related work on style modelling or summarisation. Rather, we concentrate on discussing previous works that make use of automatic classification for the evaluation of NLG systems, also to show in what sense our approach differ from existing ones. Using a classifier to assess the goodness of generated texts in connection to a broad definition of style-aware generation has been used in several previous works (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation"
2020.lrec-1.828,N18-1012,0,0.0666406,"the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (2018) compared the automatic classification-based evaluation with human evaluation. They find a high correlation between human and automatic evaluation in two out of their three data-sets, showing the validity of the automatic approach. However, the task of sentiment analysis, though subjective, is not too hard for humans, who are usually able to perceive sentiment encapsulated in text. Rao and Tetreault (2018) also exploited human and automatic classification as benchmarks for a machine translation system that translates formal texts into informal texts and vice-versa. Also in this case, usually text register is something that humans are quite able to grasp. Our work differs from the above in at least two respects. One is that we want to evaluate the capabilities of an NLG system to learn (different) stylistics aspects from (different) training data sets, rather than evaluating the capabilities of style transfer systems mostly based on lexical substituFigure 1: Red: generation task. Blue: classific"
2020.lrec-1.828,W16-5603,0,0.0161894,"rks (Hu et al., 2017; Tian et al., 2018; Prabhumoye et al., 2018; John et al., 2018; Li et al., 2018, e.g.). However, these works tend to focus on sentiment aspects (transforming a positive review into a negative one, for example), which are usually mostly associated to a lexical problem (only a small part of style). Indeed, the problem of style transfer is usually addressed within the Variational Autoencoder framework and/or trough lexical substitution. Lexical substitution was also the key element of a system developed for obfuscating gender-related stylistics aspects in social media texts (Reddy and Knight, 2016), where a classificationbased evaluation was used. In addition, Li et al. (2018) compared the automatic classification-based evaluation with human evaluation. They find a high correlation between human and automatic evaluation in two out of their three data-sets, showing the validity of the automatic approach. However, the task of sentiment analysis, though subjective, is not too hard for humans, who are usually able to perceive sentiment encapsulated in text. Rao and Tetreault (2018) also exploited human and automatic classification as benchmarks for a machine translation system that translat"
2020.lrec-1.828,D15-1044,0,0.0129556,"er-specific style. Importantly, we also observe that humans aren’t reliable judges for this task, since although familiar with the newspapers, they are not able to discern their specific styles even in the original human-written headlines. The utility of automatic evaluation goes therefore beyond saving the costs and hurdles of manual annotation, and deserves particular care in its design. Keywords: Natural Language Generation, Stylistic variations, Evaluation 1. Introduction Automatic headline generation is conceptually a simple task which can be conceived as a form of extreme summarisation (Rush et al., 2015): given an article, or a portion of it, generate its headline. Generation of headlines though is not just a matter of summarising the content. Different newspapers report the news in different ways, depending on their policies and strategies. For example, they might exhibit some topic-biases, such as writing more about gossip vs more about politics. But even when reporting on the same topics, they might exhibit specific stylistic features related to word choices, word order, punctuation usage, etc. This might be even more evident when newspapers are positioned at opposite ends of the political"
2020.lrec-1.828,P17-1099,0,0.0664172,"eadlines and verify whether it is able to correctly classify their source. Figure 1 shows an overview of the approach. 6710 3.1. Generation Models classifier human As the focus of this contribution is not on making the best model for headline generation, rather on evaluation strategies, we leverage existing implementations of sequence-tosequence networks. More specifically, we experiment with the following three models: • Sequence-to-Sequence with Attention (S2S) We used a sequence-to-sequence model (Sutskever et al., 2014) with attention (Bahdanau et al., 2014) with the configuration used by See et al. (2017) but we used a bidirectional instead of a unidirectional layer. This choice applies to all the models we used. The final configuration is 1 bidirectional encoder-decoder layer with 256 LSTM cells each, no dropout and shared embeddings with size 128; the model is optimised with Adagrad with learning rate 0.15 and gradient clipped (Mikolov, 2012) to a maximum magnitude of 2. • Pointer Generator Network (PN) The basic architecture is a sequence-to-sequence model, but the hybrid pointer-generator network uses a pointing mechanism (See et al., 2017) that lets it copy words from the source text, and"
2020.lrec-1.828,P18-2061,1,0.868035,"Missing"
2020.lrec-1.828,W19-8643,0,0.0947261,"Missing"
2020.lrec-1.883,W13-1727,1,0.797509,"Missing"
2020.lrec-1.883,W11-2308,1,0.881623,"Missing"
2020.lrec-1.883,W14-1820,1,0.81729,"Missing"
2020.lrec-1.883,W17-0216,0,0.0645813,"Missing"
2020.lrec-1.883,L16-1680,0,0.0298962,"etical studies and applicative scenarios in which they were successfully used for text and author profiling. 2. Profiling–UD 1 Profiling–UD is a web–based application inspired to the methodology initially presented in Montemagni (2013) and successfully tested in different case studies (some of which are reported in section 3.), that performs linguistic profiling of a text, or a large collection of texts, for multiple languages. The tool implements a two–stage process: linguistic annotation and linguistic profiling. The first step, linguistic annotation, is automatically carried out by UDPipe (Straka et al., 2016), a state–of–the–art pipeline available for nearly all languages included in the Universal Dependencies (UD) initiative, which carries out basic pre-processing steps, i.e. sentence splitting and tokenization, POS tagging, lemmatization and dependency parsing. In the second step, a set of about 130 features representative of the linguistic structure underlying the text are extracted from the output of the different levels of linguistic annotation. These features capture a wide number of linguistic phenomena ranging from superficial, morpho–syntactic and syntactic properties, which were proven t"
2020.lrec-1.883,W13-1706,0,0.0295363,"is a type of authorship attribution problem. Instead of identifying a particular author from among a 5 For the purpose of these experiments the author considered word and lemma n-grams as lexical features. closed list of suspects, we wish to identify an author class, namely, those authors who share a particular native language” (Koppel et al., 2005). Koppel’s definition highlights how profiling authors according to their native language can be seen as a process of detection of fingerprints of groups of authors. Since the organization of the First Shared Task on Native Language Identification (Tetreault et al., 2013), stylistic characteristics of L2 writings have been used to model L1 features and predict the native language of the writer of a given document. Also in this scenario, Profiling– UD features turned out to be effective in i) classifying the L1 of the writer and ii) reconstructing the linguistic profile of L1 starting from L2 productions (Cimino et al., 2013). For instance, the authors highlighted that L1s belonging to the same language family (e.g. Japanese and Korean), or contact languages (e.g. Hindi and Telugu), show closer distributions of features. The authors carried out an additional an"
2020.repl4nlp-1.15,Q19-1004,0,0.0236853,"a contextual (BERT) and a contextual-independent (Word2vec) Neural Language Model; (ii) we evaluate the best method for obtaining sentence-level representations from BERT and Word2vec according to a wide spectrum of probing tasks; (iii) we compare the results obtained by BERT and Word2vec according to the different combining methods; (iv) we study whether BERT is able to encode sentence-level properties within its single word representations. 2 Related Work In the last few years, several methods have been devised to open the black box and understand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstra"
2020.repl4nlp-1.15,I17-1001,0,0.136797,"Missing"
2020.repl4nlp-1.15,P18-2003,0,0.0553103,"ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and th"
2020.repl4nlp-1.15,2020.lrec-1.883,1,0.819071,"Missing"
2020.repl4nlp-1.15,N19-1423,0,0.0562699,"nowledge encoded in its aggregated sentence representations is still comparable to that of a contextualindependent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations. 1 Introduction Distributional word representations (Mikolov et al., 2013) trained on large-scale corpora have rapidly become one of the most prominent component in modern NLP systems. In this context, the recent development of context-dependent embeddings (Peters et al., 2018; Devlin et al., 2019) has shown that such representations are able to achieve state-ofthe-art performance in many complex NLP tasks. However, the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex. Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the definition of probing models trained to predict simple"
2020.repl4nlp-1.15,D19-1006,0,0.0227204,"compared a Single Word Embedding-based model (SWEM-based) with existing recurrent and convolutional networks using a suite of 17 NLP datasets, demonstrating that simple pooling operations over SWEM-based representations exhibit comparable or even superior performance in the majority of cases considered. On the contrary, Joshi et al. (2019) showed that, in the context of three different classification problems in health informatics, context-based representations are a better choice than word-based representations to create vectors. Focusing instead on the geometry of the representation space, Ethayarajh (2019) first showed that the contextualized word representations of ELMo, BERT and GPT-2 produce more context specific representations in the upper layers and then proposed a method for creating a new type of static embedding that outperforms GloVe and FastText on many benchmarks, by simply taking the first principal component of contextualized representations in lower layers of BERT. Differently from those latter work, our aim is to investigate the implicit linguistic knowledge encoded in pre-trained contextual and contextualindependent models both at sentence and word levels. 3 Our Approach We stu"
2020.repl4nlp-1.15,D19-1275,0,0.0319677,"information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al"
2020.repl4nlp-1.15,N19-1419,0,0.0420396,"o and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. Liu et al. (2019), instead, quantified differences in the transferability of individual layers"
2020.repl4nlp-1.15,P19-1356,0,0.0837161,"Missing"
2020.repl4nlp-1.15,W19-5015,0,0.0212322,"eddings are particularly effective and encode a wide amount of information regarding sentence length, while LSTM auto-encoders are very effective at capturing word order and word content. Similarly, but focused on the resolution of specific downstream tasks, Shen et al. (2018) compared a Single Word Embedding-based model (SWEM-based) with existing recurrent and convolutional networks using a suite of 17 NLP datasets, demonstrating that simple pooling operations over SWEM-based representations exhibit comparable or even superior performance in the majority of cases considered. On the contrary, Joshi et al. (2019) showed that, in the context of three different classification problems in health informatics, context-based representations are a better choice than word-based representations to create vectors. Focusing instead on the geometry of the representation space, Ethayarajh (2019) first showed that the contextualized word representations of ELMo, BERT and GPT-2 produce more context specific representations in the upper layers and then proposed a method for creating a new type of static embedding that outperforms GloVe and FastText on many benchmarks, by simply taking the first principal component of"
2020.repl4nlp-1.15,D19-1445,0,0.0501347,"T and Word2vec according to the different combining methods; (iv) we study whether BERT is able to encode sentence-level properties within its single word representations. 2 Related Work In the last few years, several methods have been devised to open the black box and understand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2"
2020.repl4nlp-1.15,N16-1082,0,0.0564417,"ng sentence-level representations from BERT and Word2vec according to a wide spectrum of probing tasks; (iii) we compare the results obtained by BERT and Word2vec according to the different combining methods; (iv) we study whether BERT is able to encode sentence-level properties within its single word representations. 2 Related Work In the last few years, several methods have been devised to open the black box and understand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 20"
2020.repl4nlp-1.15,W19-4825,0,0.0644884,"Missing"
2020.repl4nlp-1.15,N19-1112,0,0.0453813,"2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of probing tasks, Tenney et al. (2019a) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline: POS tagging, parsing, NER, semantic roles and then coreference. Liu et al. (2019), instead, quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. Closer to our study, Adi et al. (2016) proposed a method for analyzing and comparing different sentence representations and different dimensions, exploring the effect of the dimensionality on the resulting representations. In particular, they showed that sentence representations based on averaged Word2vec embeddings are particularl"
2020.repl4nlp-1.15,D18-1151,0,0.0455081,"nent component in modern NLP systems. In this context, the recent development of context-dependent embeddings (Peters et al., 2018; Devlin et al., 2019) has shown that such representations are able to achieve state-ofthe-art performance in many complex NLP tasks. However, the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex. Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. In particular, those work provided evidences that contextualized Neural Language Models (NLMs) are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). Despite this, less study focused on the analysis and the comparison of"
2020.repl4nlp-1.15,N18-1202,0,0.060995,"uence, the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextualindependent model. We also find that BERT is able to encode sentence-level properties even within single-word embeddings, obtaining comparable or even superior results than those obtained with sentence representations. 1 Introduction Distributional word representations (Mikolov et al., 2013) trained on large-scale corpora have rapidly become one of the most prominent component in modern NLP systems. In this context, the recent development of context-dependent embeddings (Peters et al., 2018; Devlin et al., 2019) has shown that such representations are able to achieve state-ofthe-art performance in many complex NLP tasks. However, the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex. Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the definition of probing models tra"
2020.repl4nlp-1.15,W18-5431,0,0.0480681,"re the results obtained by BERT and Word2vec according to the different combining methods; (iv) we study whether BERT is able to encode sentence-level properties within its single word representations. 2 Related Work In the last few years, several methods have been devised to open the black box and understand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2"
2020.repl4nlp-1.15,P18-1041,0,0.0286647,"increase in task-specificity. Closer to our study, Adi et al. (2016) proposed a method for analyzing and comparing different sentence representations and different dimensions, exploring the effect of the dimensionality on the resulting representations. In particular, they showed that sentence representations based on averaged Word2vec embeddings are particularly effective and encode a wide amount of information regarding sentence length, while LSTM auto-encoders are very effective at capturing word order and word content. Similarly, but focused on the resolution of specific downstream tasks, Shen et al. (2018) compared a Single Word Embedding-based model (SWEM-based) with existing recurrent and convolutional networks using a suite of 17 NLP datasets, demonstrating that simple pooling operations over SWEM-based representations exhibit comparable or even superior performance in the majority of cases considered. On the contrary, Joshi et al. (2019) showed that, in the context of three different classification problems in health informatics, context-based representations are a better choice than word-based representations to create vectors. Focusing instead on the geometry of the representation space,"
2020.repl4nlp-1.15,silveira-etal-2014-gold,0,0.100338,"Missing"
2020.repl4nlp-1.15,P19-1452,0,0.170826,"ations more complex. Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. In particular, those work provided evidences that contextualized Neural Language Models (NLMs) are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). Despite this, less study focused on the analysis and the comparison of contextual and non-contextual NLMs according to their ability to encode implicit linguistic properties in their representations. In this paper we perform a large number of probing experiments to analyze and compare the implicit knowledge stored by a contextual and a non-contextual model within their inner representations. In particular, we define two research questions, aimed at understanding: ("
2020.repl4nlp-1.15,W19-4808,0,0.0412102,"g to the different combining methods; (iv) we study whether BERT is able to encode sentence-level properties within its single word representations. 2 Related Work In the last few years, several methods have been devised to open the black box and understand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the rep"
2020.repl4nlp-1.15,D19-1286,0,0.0234611,"this context, the recent development of context-dependent embeddings (Peters et al., 2018; Devlin et al., 2019) has shown that such representations are able to achieve state-ofthe-art performance in many complex NLP tasks. However, the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex. Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties (Marvin and Linzen, 2018; Goldberg, 2019; Warstadt et al., 2019). Much of this work focused on the definition of probing models trained to predict simple linguistic properties from unsupervised representations. In particular, those work provided evidences that contextualized Neural Language Models (NLMs) are able to capture a wide range of linguistic phenomena (Adi et al., 2016; Perone et al., 2018; Tenney et al., 2019b) and even to organize this information in a hierarchical manner (Belinkov et al., 2017; Lin et al., 2019; Jawahar et al., 2019). Despite this, less study focused on the analysis and the comparison of contextual and non-contextual NLMs accor"
2020.repl4nlp-1.15,W18-5448,0,0.0468739,"nderstand the linguistic information encoded in NLMs (Belinkov and Glass, 2019). They range from techniques to examine the activations of individual neurons (Karpathy et al., 2015; Li et al., 2016; K´ad´ar et al., 2017) to more domain specific approaches, such as interpreting attention mechanisms (Raganato and Tiedemann, 2018; Kovaleva et al., 2019; Vig and Belinkov, 2019) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018; Zhang and Bowman, 2018; Hewitt and Liang, 2019). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Belinkov et al., 2017; Blevins et al., 2018; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). Jawahar et al. (2019) investigated the representations learned at different layers of BERT, showing that lower layer representations are usually better for capturing surface features, while embeddings from higher layers are better for syntactic and semantic properties. Using a suite of pr"
2021.cmcl-1.23,P18-2073,0,0.0141534,"late more with complexity judgments and with the degree of agreement among annotators. We train several regression models, using either explicit linguistic features or contextualized word embeddings, to predict the mean complexity values assigned to sentences in the different contextual windows, as well as their standard deviation. Results show that models leveraging explicit features capturing morphosyntactic and syntactic phenomena perform always better, especially when they have access to features extracted from all contextual sentences. 1 different from complexity, yet somehow related. In Bernardy et al. (2018) speakers were asked to evaluate the degree of acceptability of sentences from Wikipedia, both in their original form and with some grammatical alterations artificially introduced by a process of round-trip machine translation. Results showed that ill-formed sentences are evaluated as more acceptable when presented within context (i.e. along with their preceding or following sentence) rather than in isolation. More closely related to our study is the one by Schumacher et al. (2016) on readability assessment. In that work, authors gathered pairwise evaluations of reading difficulty on sentences"
2021.cmcl-1.23,2020.lrec-1.883,1,0.806476,"Missing"
2021.cmcl-1.23,D18-1289,1,0.691393,"Missing"
2021.cmcl-1.5,N16-1120,0,0.0273811,"School for Advanced Studies (SISSA), Trieste 2 Istituto di Linguistica Computazionale ""Antonio Zampolli"" (ILC-CNR), Pisa ItaliaNLP Lab – italianlp.it gabriele.sarti996@gmail.com {dominique.brunato, felice.dellorletta}@ilc.cnr.it Abstract reflecting both low and high-level complexity features of the input (Rayner, 1998; Hahn and Keller, 2016). Eye-tracking measures have recently contributed to significant improvements across many popular NLP applications (Hollenstein et al., 2019a, 2020) and in particular on tasks related to linguistic complexity such as automatic readability assessment (ARA) (Ambati et al., 2016; Singh et al., 2016; González-Garduño and Søgaard, 2018), obtaining meaningful results for sentence-level classification in easy and hard-to-read categories (Vajjala and Luˇci´c, 2018; Evaldo Leal et al., 2020; Martinc et al., 2021). However, readability levels are conceptually very different from cognitive processing metrics since ARA corpora are usually built in an automated fashion from parallel documents at different readability levels, without explicit evaluations of complexity by target readers (Vajjala and Luˇci´c, 2019). A different approach to complexity assessment that directly acco"
2021.cmcl-1.5,2020.lrec-1.883,1,0.732159,"Missing"
2021.cmcl-1.5,D18-1289,1,0.672891,"Missing"
2021.cmcl-1.5,N19-1423,0,0.0284643,".62 6.17 2.36 .54 .44 .62 .74 .44 .75 2 TFD RMSE R .06 .86 1078 374 2.19 1.77 .88 .92 1.98 .91 2 TRD RMSE R2 .06 .85 540 403 .03 .45 494 435 .86 .89 405 400 .45 .46 435 .90 382 .49 RMSE R .06 .89 1297 532 343 287 .90 .93 302 .93 2 Table 4: Average Root-Mean-Square Error and R2 for complexity predictions of two average baselines, two SVMs relying on explicit features and a pretrained language model with contextualized word embeddings using 5-fold cross-validation. ALBERT learns eye-tracking metrics in a multitask setting over parallel annotations. lightweight yet effective alternative to BERT (Devlin et al., 2019) for obtaining contextual word representations, using its last-layer [CLS] sentence embedding as input for a linear regressor during fine-tuning and testing. We selected the last layer representations, despite having strong evidence on the importance of intermediate representation in encoding language properties, because we aim to investigate how final layers encode complexityrelated competences. Given the availability of parallel eye-tracking annotations, we train ALBERT using multitask learning with hard parameter sharing (Caruana, 1997) on gaze metrics.2 From results in Table 4 we note that"
2021.cmcl-1.5,W19-4820,0,0.0172284,"omputational Linguistics features or contextualized word embeddings, focusing mainly on the neural language model ALBERT (Lan et al., 2020). In this context, we show how both explicit features and learned representations obtain comparable results when predicting complexity scores. Finally, we focus on studying how complexity-related properties are encoded in the representations of ALBERT. This perspective goes in the direction of exploiting human processing data to address the interpretability issues of unsupervised language representations (Hollenstein et al., 2019b; Gauthier and Levy, 2019; Abnar et al., 2019). To this end, we rely on the probing task approach, a recently introduced technique within the area of NLMs interpretability consisting of training diagnostic classifiers to probe the presence of encoded linguistic properties inside contextual representations (Conneau et al., 2018; Zhang and Bowman, 2018). We observe that fine-tuning on online and offline complexity produces a consequent increase in probing performances for complexityrelated features during our probing experiments. This investigation has the specific purpose of studying whether and how learning a new task affects the linguist"
2021.cmcl-1.5,2020.blackboxnlp-1.4,0,0.0159074,"ties inside contextual representations (Conneau et al., 2018; Zhang and Bowman, 2018). We observe that fine-tuning on online and offline complexity produces a consequent increase in probing performances for complexityrelated features during our probing experiments. This investigation has the specific purpose of studying whether and how learning a new task affects the linguistic properties encoded in pretrained representations. In fact, while pre-trained models have been widely studied using probing methods, the effect of fine-tuning on encoded information was seldom investigated. For example, Merchant et al. (2020) found that fine-tuning does not impact heavily the linguistic information implicitly learned by the model, especially when considering a supervised probe closely related to a downstream task. Miaschi et al. (2020) further demonstrated a positive correlation between the model’s ability to solve a downstream task on a specific input sentence and the related linguistic knowledge encoded in a language model. Nonetheless, to our knowledge, no previous work has taken into account sentence complexity assessment as a fine-tuning task for NLMs. Our results suggest that the model’s competencies during"
2021.cmcl-1.5,D19-1050,0,0.0177449,". ©2021 Association for Computational Linguistics features or contextualized word embeddings, focusing mainly on the neural language model ALBERT (Lan et al., 2020). In this context, we show how both explicit features and learned representations obtain comparable results when predicting complexity scores. Finally, we focus on studying how complexity-related properties are encoded in the representations of ALBERT. This perspective goes in the direction of exploiting human processing data to address the interpretability issues of unsupervised language representations (Hollenstein et al., 2019b; Gauthier and Levy, 2019; Abnar et al., 2019). To this end, we rely on the probing task approach, a recently introduced technique within the area of NLMs interpretability consisting of training diagnostic classifiers to probe the presence of encoded linguistic properties inside contextual representations (Conneau et al., 2018; Zhang and Bowman, 2018). We observe that fine-tuning on online and offline complexity produces a consequent increase in probing performances for complexityrelated features during our probing experiments. This investigation has the specific purpose of studying whether and how learning a new task"
2021.cmcl-1.5,2020.coling-main.65,1,0.855352,"Missing"
2021.cmcl-1.5,L16-1262,0,0.023916,"Missing"
2021.cmcl-1.5,D16-1009,0,0.043529,"Missing"
2021.cmcl-1.5,2020.lincr-1.3,0,0.0650993,"Missing"
2021.cmcl-1.5,K19-1050,0,0.130081,"acterizing Linguistic Complexity in Humans and Language Models Gabriele Sarti 1,2 1 Dominique Brunato 2 University of Trieste, International School for Advanced Studies (SISSA), Trieste 2 Istituto di Linguistica Computazionale ""Antonio Zampolli"" (ILC-CNR), Pisa ItaliaNLP Lab – italianlp.it gabriele.sarti996@gmail.com {dominique.brunato, felice.dellorletta}@ilc.cnr.it Abstract reflecting both low and high-level complexity features of the input (Rayner, 1998; Hahn and Keller, 2016). Eye-tracking measures have recently contributed to significant improvements across many popular NLP applications (Hollenstein et al., 2019a, 2020) and in particular on tasks related to linguistic complexity such as automatic readability assessment (ARA) (Ambati et al., 2016; Singh et al., 2016; González-Garduño and Søgaard, 2018), obtaining meaningful results for sentence-level classification in easy and hard-to-read categories (Vajjala and Luˇci´c, 2018; Evaldo Leal et al., 2020; Martinc et al., 2021). However, readability levels are conceptually very different from cognitive processing metrics since ARA corpora are usually built in an automated fashion from parallel documents at different readability levels, without explicit e"
2021.cmcl-1.5,silveira-etal-2014-gold,0,0.0595083,"Missing"
2021.cmcl-1.5,W18-0535,0,0.0338071,"Missing"
2021.cmcl-1.5,W19-4437,0,0.0241454,"Missing"
2021.cmcl-1.5,W18-5448,0,0.0278966,"studying how complexity-related properties are encoded in the representations of ALBERT. This perspective goes in the direction of exploiting human processing data to address the interpretability issues of unsupervised language representations (Hollenstein et al., 2019b; Gauthier and Levy, 2019; Abnar et al., 2019). To this end, we rely on the probing task approach, a recently introduced technique within the area of NLMs interpretability consisting of training diagnostic classifiers to probe the presence of encoded linguistic properties inside contextual representations (Conneau et al., 2018; Zhang and Bowman, 2018). We observe that fine-tuning on online and offline complexity produces a consequent increase in probing performances for complexityrelated features during our probing experiments. This investigation has the specific purpose of studying whether and how learning a new task affects the linguistic properties encoded in pretrained representations. In fact, while pre-trained models have been widely studied using probing methods, the effect of fine-tuning on encoded information was seldom investigated. For example, Merchant et al. (2020) found that fine-tuning does not impact heavily the linguistic"
2021.deelio-1.5,2020.lrec-1.883,1,0.818439,"Missing"
2021.deelio-1.5,P19-1337,0,0.0231957,"020) or to assess whether it matches various human behavioural measures, such as gaze duration during reading (Demberg and Keller, 2008; Goodkind and Bicknell, 2018). With the recent success gained by Neural Language Models (NLMs) across a variety of NLP tasks, the notion of perplexity has started being investigated also to dig into issues related to the interpretability of contextual word representations, with the aim of understanding whether there is a relationship between this metric and the grammatical abilities implicitly encoded by a NLM (Gulordava et al., 2018; Marvin and Linzen, 2018; Kuncoro et al., 2019). In this context, Hu et al. (2020) and Warstadt et al. (2020) observed a dissociation between the perplexity of a NLM and its performance on targeted syntactic assessments probing the model’s ability to encode a range of subtle syntactic phenomena. 2 Our Approach We defined two sets of experiments. The first consists in investigating the relationship between BERT and GPT-2 sentence-level perplexity (PPL) scores. To do so, we first computed BERT and GPT-2 PPL scores for sentences contained in the English Universal Dependencies (UD) treebank (Nivre et al., 2016) and we assessed their corre40 Pr"
2021.deelio-1.5,2020.acl-main.176,0,0.025146,"exploit linguistic features capturing a wide set of morpho-syntactic and syntactic phenomena showing how they contribute to predict the perplexity of the two NLMs. 1 Introduction and Motivation Perplexity is one of the most standard metrics to assess the quality of a language model. It is also used in different scenarios, such as to classify formal and colloquial tweets (González, 2015), to detect the boundaries between varieties belonging to the same language family (Gamallo et al., 2017), to identify speech samples produced by subjects with cognitive and/or language diseases e.g. dementia, (Cohen and Pakhomov, 2020) or to assess whether it matches various human behavioural measures, such as gaze duration during reading (Demberg and Keller, 2008; Goodkind and Bicknell, 2018). With the recent success gained by Neural Language Models (NLMs) across a variety of NLP tasks, the notion of perplexity has started being investigated also to dig into issues related to the interpretability of contextual word representations, with the aim of understanding whether there is a relationship between this metric and the grammatical abilities implicitly encoded by a NLM (Gulordava et al., 2018; Marvin and Linzen, 2018; Kunc"
2021.deelio-1.5,D18-1151,0,0.0297335,"a, (Cohen and Pakhomov, 2020) or to assess whether it matches various human behavioural measures, such as gaze duration during reading (Demberg and Keller, 2008; Goodkind and Bicknell, 2018). With the recent success gained by Neural Language Models (NLMs) across a variety of NLP tasks, the notion of perplexity has started being investigated also to dig into issues related to the interpretability of contextual word representations, with the aim of understanding whether there is a relationship between this metric and the grammatical abilities implicitly encoded by a NLM (Gulordava et al., 2018; Marvin and Linzen, 2018; Kuncoro et al., 2019). In this context, Hu et al. (2020) and Warstadt et al. (2020) observed a dissociation between the perplexity of a NLM and its performance on targeted syntactic assessments probing the model’s ability to encode a range of subtle syntactic phenomena. 2 Our Approach We defined two sets of experiments. The first consists in investigating the relationship between BERT and GPT-2 sentence-level perplexity (PPL) scores. To do so, we first computed BERT and GPT-2 PPL scores for sentences contained in the English Universal Dependencies (UD) treebank (Nivre et al., 2016) and we as"
2021.deelio-1.5,2020.acl-main.490,0,0.0399239,"Missing"
2021.deelio-1.5,silveira-etal-2014-gold,0,0.0584765,"Missing"
2021.deelio-1.5,2020.tacl-1.25,0,0.0272424,"l measures, such as gaze duration during reading (Demberg and Keller, 2008; Goodkind and Bicknell, 2018). With the recent success gained by Neural Language Models (NLMs) across a variety of NLP tasks, the notion of perplexity has started being investigated also to dig into issues related to the interpretability of contextual word representations, with the aim of understanding whether there is a relationship between this metric and the grammatical abilities implicitly encoded by a NLM (Gulordava et al., 2018; Marvin and Linzen, 2018; Kuncoro et al., 2019). In this context, Hu et al. (2020) and Warstadt et al. (2020) observed a dissociation between the perplexity of a NLM and its performance on targeted syntactic assessments probing the model’s ability to encode a range of subtle syntactic phenomena. 2 Our Approach We defined two sets of experiments. The first consists in investigating the relationship between BERT and GPT-2 sentence-level perplexity (PPL) scores. To do so, we first computed BERT and GPT-2 PPL scores for sentences contained in the English Universal Dependencies (UD) treebank (Nivre et al., 2016) and we assessed their corre40 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Worksh"
2021.deelio-1.6,W19-4814,0,0.0312355,"Missing"
2021.deelio-1.6,Q19-1004,0,0.0189971,"ormation is encoded within contextualized internal representations, e.g. defining probing classifier trained to predict a variety of language phenomena (Conneau et al., 2018a; Hewitt and Manning, 2019; Tenney et al., 2019a). In line with this latter approach and with recent works aimed at investigating how the information is arranged within neural models representations (Baan et al., 2019; Dalvi et al., 2019; Lakretz 2 Related work In the last few years, a number of recent works have explored the inner mechanism and the linguistic knowledge implicitly encoded in Neural Language Models (NLMs) (Belinkov and Glass, 2019). The most common approach is based on 48 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57 Online, June 10, 2021. ©2021 Association for Computational Linguistics the development of probes, i.e. supervised models trained to predict simple linguistic properties using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018b; Zhang and Bowman, 2018; Miaschi et al., 2020). These latter studies demonstrated that NLMs are able to encode a wide range"
2021.deelio-1.6,P18-2003,0,0.0178325,"s of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57 Online, June 10, 2021. ©2021 Association for Computational Linguistics the development of probes, i.e. supervised models trained to predict simple linguistic properties using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018b; Zhang and Bowman, 2018; Miaschi et al., 2020). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Blevins et al., 2018; Jawahar et al., 2019; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). For instance, Liu et al. (2019) quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. Other works also investigated the importance of individual neurons within models representations (Qian et al., 2016; Bau et al., 2019; Baan et al., 2019). Dalvi et al. (2019) pr"
2021.deelio-1.6,P18-1198,0,0.0260806,"back of these approaches, despite the outstanding performances, is the lack of interpretability. In fact, high dimensional representations do not allow for any insight of the type of linguistic properties encoded in these models. Therefore this implicit knowledge can only be determined a posteriori, by designing tasks that require a specific linguistic skill to be tackled (Linzen and Baroni, 2020) or by investigating to what extent certain information is encoded within contextualized internal representations, e.g. defining probing classifier trained to predict a variety of language phenomena (Conneau et al., 2018a; Hewitt and Manning, 2019; Tenney et al., 2019a). In line with this latter approach and with recent works aimed at investigating how the information is arranged within neural models representations (Baan et al., 2019; Dalvi et al., 2019; Lakretz 2 Related work In the last few years, a number of recent works have explored the inner mechanism and the linguistic knowledge implicitly encoded in Neural Language Models (NLMs) (Belinkov and Glass, 2019). The most common approach is based on 48 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration"
2021.deelio-1.6,2020.lrec-1.883,1,0.802342,"Missing"
2021.deelio-1.6,N19-1423,0,0.0382326,"ed a different study, based on Lasso regression, aimed at understanding how the information encoded by BERT sentence-level representations is arranged within its hidden units. Using a suite of several probing tasks, we showed the existence of a relationship between the implicit knowledge learned by the model and the number of individual units involved in the encodings of this competence. Moreover, we found that it is possible to identify groups of hidden units more relevant for specific linguistic properties. 1 Introduction The rise of contextualized word representations (Peters et al., 2018; Devlin et al., 2019) has led to significant improvement in several (if not every) NLP tasks. The main drawback of these approaches, despite the outstanding performances, is the lack of interpretability. In fact, high dimensional representations do not allow for any insight of the type of linguistic properties encoded in these models. Therefore this implicit knowledge can only be determined a posteriori, by designing tasks that require a specific linguistic skill to be tackled (Linzen and Baroni, 2020) or by investigating to what extent certain information is encoded within contextualized internal representations,"
2021.deelio-1.6,D16-1079,0,0.0485643,"Missing"
2021.deelio-1.6,N19-1419,0,0.361634,"s, despite the outstanding performances, is the lack of interpretability. In fact, high dimensional representations do not allow for any insight of the type of linguistic properties encoded in these models. Therefore this implicit knowledge can only be determined a posteriori, by designing tasks that require a specific linguistic skill to be tackled (Linzen and Baroni, 2020) or by investigating to what extent certain information is encoded within contextualized internal representations, e.g. defining probing classifier trained to predict a variety of language phenomena (Conneau et al., 2018a; Hewitt and Manning, 2019; Tenney et al., 2019a). In line with this latter approach and with recent works aimed at investigating how the information is arranged within neural models representations (Baan et al., 2019; Dalvi et al., 2019; Lakretz 2 Related work In the last few years, a number of recent works have explored the inner mechanism and the linguistic knowledge implicitly encoded in Neural Language Models (NLMs) (Belinkov and Glass, 2019). The most common approach is based on 48 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architec"
2021.deelio-1.6,P19-1356,0,0.0231851,"ide Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57 Online, June 10, 2021. ©2021 Association for Computational Linguistics the development of probes, i.e. supervised models trained to predict simple linguistic properties using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018b; Zhang and Bowman, 2018; Miaschi et al., 2020). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Blevins et al., 2018; Jawahar et al., 2019; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). For instance, Liu et al. (2019) quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. Other works also investigated the importance of individual neurons within models representations (Qian et al., 2016; Bau et al., 2019; Baan et al., 2019). Dalvi et al. (2019) proposed two methods, Li"
2021.deelio-1.6,silveira-etal-2014-gold,0,0.0399325,"Missing"
2021.deelio-1.6,N19-1002,0,0.0228654,"esentations (Qian et al., 2016; Bau et al., 2019; Baan et al., 2019). Dalvi et al. (2019) proposed two methods, Linguistic Correlations Analysis and Cross-model correlation analysis, to study whether specific dimensions learned by end-to-end neural models are responsible for specific properties. For instance, they showed that open class categories such as verbs and location are much more distributed across the network compared to closed class categories (e.g. coordinating conjunction) and also that the model recognizes a hierarchy of linguistic proprieties and distributes neurons based on it. Lakretz et al. (2019), instead, proposed a detailed study of the inner mechanism of number tracking in LSTMs at single neuron level, showing that long distance number information (from the subject to the verb) is largely managed by two specific units. Differently from those latter work, our aim was to combine previous approaches based on probes and on the study on individual units in order to propose an in-depth investigation on the organization of linguistic competence within NLM contextualized representations. 3 which weights within sentence-level BERT internal representations can be set to zero, in order to und"
2021.deelio-1.6,P19-1452,0,0.0204746,"performances, is the lack of interpretability. In fact, high dimensional representations do not allow for any insight of the type of linguistic properties encoded in these models. Therefore this implicit knowledge can only be determined a posteriori, by designing tasks that require a specific linguistic skill to be tackled (Linzen and Baroni, 2020) or by investigating to what extent certain information is encoded within contextualized internal representations, e.g. defining probing classifier trained to predict a variety of language phenomena (Conneau et al., 2018a; Hewitt and Manning, 2019; Tenney et al., 2019a). In line with this latter approach and with recent works aimed at investigating how the information is arranged within neural models representations (Baan et al., 2019; Dalvi et al., 2019; Lakretz 2 Related work In the last few years, a number of recent works have explored the inner mechanism and the linguistic knowledge implicitly encoded in Neural Language Models (NLMs) (Belinkov and Glass, 2019). The most common approach is based on 48 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57 On"
2021.deelio-1.6,N19-1112,0,0.0183757,"1 Association for Computational Linguistics the development of probes, i.e. supervised models trained to predict simple linguistic properties using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018b; Zhang and Bowman, 2018; Miaschi et al., 2020). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Blevins et al., 2018; Jawahar et al., 2019; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). For instance, Liu et al. (2019) quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. Other works also investigated the importance of individual neurons within models representations (Qian et al., 2016; Bau et al., 2019; Baan et al., 2019). Dalvi et al. (2019) proposed two methods, Linguistic Correlations Analysis and Cross-model correlation analysis, to study whether specific dimensions learned by end-to-end neural models ar"
2021.deelio-1.6,2020.coling-main.65,1,0.778715,"Missing"
2021.deelio-1.6,W18-5448,0,0.0267852,"inner mechanism and the linguistic knowledge implicitly encoded in Neural Language Models (NLMs) (Belinkov and Glass, 2019). The most common approach is based on 48 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 48–57 Online, June 10, 2021. ©2021 Association for Computational Linguistics the development of probes, i.e. supervised models trained to predict simple linguistic properties using the contextual word/sentence embeddings of a pre-trained model as training features (Conneau et al., 2018b; Zhang and Bowman, 2018; Miaschi et al., 2020). These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner (Blevins et al., 2018; Jawahar et al., 2019; Tenney et al., 2019b) and even to support the extraction of dependency parse trees (Hewitt and Manning, 2019). For instance, Liu et al. (2019) quantified differences in the transferability of individual layers between different models, showing that higher layers of RNNs (ELMo) are more task-specific (less general), while transformer layers (BERT) do not exhibit this increase in task-specificity. Othe"
2021.deelio-1.6,L16-1262,0,0.0668687,"Missing"
2021.gem-1.2,2021.ccl-1.108,0,0.0250209,"Missing"
2021.gem-1.2,2020.lrec-1.883,1,0.75383,"Missing"
2021.gem-1.2,N19-1423,0,0.00849659,"by the classifierbased reward used in style transfer tasks (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Sancheti et al., 2020), we reward the model to push its classification confidence. We evaluate the new perception-enhanced models in comparison with the original GePpeTto by running both an automatic as well as a human evaluation on output generated by the various models. Lastly, we conduct a linguistic analysis to highlight which linguistic characteristics are more commonly found in human- and machine-perceived text. Introduction Pre-trained language models, such as the BERT (Devlin et al., 2019) and the GPT (Radford et al., 2018, 2019) families, are nowadays the core component of NLP systems. These models, based on the Transformer (Vaswani et al., 2017) and trained using huge amounts of crawl data (which can contain substantial noise), have been shown to produce high quality text, more often than not judged as human-written (Radford et al., 2019; De Mattei et al., 2020; Brown et al., 2020). Existing evaluations of GPT-2 models (Ippolito et al., 2020; De Mattei et al., 2020) have shown that while generated sentences were ranked lower in human perception than gold sentences, many gold"
2021.gem-1.2,N19-1320,0,0.0282807,"applications, as for example human-machine interaction in dialogues; the other is that it opens the opportunity to investigate what linguistic aspects make a text more humanly-perceived. We run our experiments on Italian, using GePpeTto (De Mattei et al., 2020) as pre-trained model. First, we collect human judgements on gold texts and texts generated by GePpeTto in terms of how they are perceived (human or automatically produced). We then fine-tune GePpeTto with this perceptionlabelled data. In addition, inspired by the classifierbased reward used in style transfer tasks (Lample et al., 2019; Gong et al., 2019; Luo et al., 2019; Sancheti et al., 2020), we reward the model to push its classification confidence. We evaluate the new perception-enhanced models in comparison with the original GePpeTto by running both an automatic as well as a human evaluation on output generated by the various models. Lastly, we conduct a linguistic analysis to highlight which linguistic characteristics are more commonly found in human- and machine-perceived text. Introduction Pre-trained language models, such as the BERT (Devlin et al., 2019) and the GPT (Radford et al., 2018, 2019) families, are nowadays the core comp"
2021.gem-1.2,2020.emnlp-demos.6,0,0.0193228,"Missing"
2021.gem-1.2,N19-1169,0,0.0282901,"GePpeTto is able to produce text which is much closer to human quality rather than to the text generated by other baseline models. Still, real human-produced text is recognised as such more often than GePpeTto’s output. k where φ are the parameters of GePpeTto, and Rk is the reward of the kth sequence y s sampled from the distribution of model’s outputs at each time step in decoding. The framework can be trained endto-end by combining the policy gradient with the cross entropy loss of the base model. 4 We run both a human and an automatic evaluation, in line with Ippolito et al. (2020)’s and Hashimoto et al. (2019)’s suggestions in terms of evaluation’s diversity and quality. For the automatic evaluation, we train a regressor on the perception-labelled data (with the original 1–5 values) adding a dropout (Srivastava et al., 2014) and a dense layer on the top of UmBERTo. We use Adam (Kingma and Ba, 2015) with initial learning rate is 1e-5, and set the batch size to 16. We calculate the correlation of the regressor’s scores with human judgements over each single data point in the test set (N=1400), and observe good scores (Pearson=0.54 (p &lt; 10−4 ) and RMSE=0.75). For the human evaluation, we assign to eac"
bosco-etal-2010-comparing,W06-2920,0,\N,Missing
bosco-etal-2010-comparing,W08-1004,0,\N,Missing
bosco-etal-2010-comparing,P07-1122,1,\N,Missing
bosco-etal-2010-comparing,P09-1008,0,\N,Missing
bosco-etal-2010-comparing,D07-1096,1,\N,Missing
D07-1119,W06-2922,1,0.638257,"rministic incremental processing of the input sentence and by adding specific rules for handling non-projective dependencies. The parser also performs dependency labeling within a single processing step. The parser is modular and can use several learning algorithms. The submitted runs used a second order Average Perceptron, derived from the multiclass perceptron of Crammer and Singer (2003). No additional resources were used. No preprocessing or post-processing was used, except stemming for English, by means of the Snowball stemmer (Porter 2001). 3 Deterministic Classifier-based Parsing DeSR (Attardi, 2006) is an incremental deterministic classifier-based parser. The parser constructs dependency trees employing a deterministic bottom-up algorithm which performs Shift/Reduce actions while analyzing input sentences in left-toright order. Using a notation similar to (Nivre and Scholz, 2003), the state of the parser is represented by a 1112 Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 1112–1118, c Prague, June 2007. 2007 Association for Computational Linguistics quadruple 〈S, I, T, A〉, where S is the stack of past tokens, I is the list of (remaining) input tokens, T is a sta"
D07-1119,N07-1049,1,0.358664,"Missing"
D07-1119,W07-2217,1,0.801369,"aining SVMs on large data sets like those arising from a big training corpus was too computationally expensive, forcing them to resort to partitioning the training data (by POS) and to learn several models. Our implementation of the perceptron algorithm uses sparse data structures (hash maps) so that it can handle efficiently even large feature spaces in a single model. For example the feature space for the 2nd-order model for English contains over 21 million. Parsing unseen data can be performed at tens of sentences per second. More details on such aspects of the DeSR parser can be found in (Ciaramita and Attardi 2007). 6 Tuning The base parser was tuned on several parameters to optimize its accuracy as follows. 6.1 Feature Selection Given the different characteristics of languages and corpus annotations, it is worth while to select a different set of features for each language. For example, certain corpora do not contain lemmas or morphological information so lexical information will be useful. Vice versa, when lemmas are present, lexical information might be avoided, reducing the size of the feature set. We performed a series of feature selection experiments on each language, starting from a fairly compre"
D07-1119,W07-2416,0,0.0184986,"Missing"
D07-1119,W04-3111,0,0.00779651,"ed from looking at the unlabeled collection supplied for training in the domain was the presence of truncated terms like goin (for going), d (for did), etc. However none of these unusually spelled words appeared in the test set, so a normal English parser performed reasonably well on this task. Because of certain inconsistencies in the annotation guidelines, the organizers decided to make this task optional and hence we submitted just the parse produced by the parser trained for English. For the second adaptation task we were given a large collection of unlabeled data in the chemistry domain (Kulick et al, 2004) as well as a test set of 5000 tokens (200 sentences) to parse (english_pchemtbtb_test.conll). There were three sets of unlabeled documents: we chose the smallest (unlab1) consisting of over 300,000 tokens (11663 sentences). unlab1 was tokenized, POS and lemmas were added using our version of TreeTagger (Schmid, 1994), and lemmas replaced with stems, which had turned out to be more effective than lemmas. We call this set pchemtb_unlab1.conll. 1116 We trained the DeSR parser on English using english_ptb_train.conll, the WSJ PTB collection provided for CoNLL 2007. This consists of WSJ sections 0"
D07-1119,H05-1066,0,0.113013,"Missing"
D07-1119,P05-1013,0,0.015113,"the current configuration. Perceptron Learning and 2nd-Order Feature Maps The software architecture of the DeSR parser is modular. Several learning algorithms are available, including SVM, Maximum Entropy, MemoryBased Learning, Logistic Regression and a few variants of the perceptron algorithm. We obtained the best accuracy with a multiclass averaged perceptron classifier based on the ultraconservative formulation of Crammer and Singer (2003) with uniform negative updates. The classifier function is: F ( x) = arg max{α k ⋅ x} k 4 Non-Projective Relations For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective. A post-processing step is then required to restore the arcs to the proper heads. In DeSR non-projective dependencies are handled in a single step by means of the following additional parsing rules, slightly different from those in (Attardi, 2006): Right2d Left2 d Right3d Left3 d Extract Insert 〈s1|s2|S, n|I, T, A〉 〈 S, s1|n|I, T, A∪{(s2, d, n)}〉 〈s1|s2|S, n|I, T, A〉 〈s2|S, s1|I, T, A∪{(n, d, s2)}〉 〈s1|s2|s3|S, n|I, T,"
D07-1119,C04-1010,0,0.0818874,"Missing"
D07-1119,W03-3023,0,0.102276,"re vector x into a higher dimensional feature space consisting of all unordered feature pairs: Φ(x) = 〈xixj |i = 1, …, d, j = i, …, d〉 In other words we expand the original representation in the input space with a feature map that generates all second-order feature combinations from each observation. We call this the 2 nd-order model, where the inner products are computed as αk ⋅ Φ(x), with αk a vector of dimension d(d+1)/2. Applying a linear perceptron to this feature space corresponds to simulating a polynomial kernel of degree two. A polynomial kernel of degree two for SVM was also used by Yamada and Matsumoto (2003). However, training SVMs on large data sets like those arising from a big training corpus was too computationally expensive, forcing them to resort to partitioning the training data (by POS) and to learn several models. Our implementation of the perceptron algorithm uses sparse data structures (hash maps) so that it can handle efficiently even large feature spaces in a single model. For example the feature space for the 2nd-order model for English contains over 21 million. Parsing unseen data can be performed at tens of sentences per second. More details on such aspects of the DeSR parser can"
D07-1119,W04-2407,0,\N,Missing
D07-1119,J93-2004,0,\N,Missing
D07-1119,D07-1096,0,\N,Missing
D16-1034,W03-1004,0,0.0311988,"n et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods taking into account also the order in which information is presented (Bott and Saggion, 2011). Differently from these methods, our approach 351 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 351–361, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics contains two important novelties: the typology of the starting data and consequently the methodology developed to build the complex–simple aligned corpus. To overcome the scarcity of large parallel corpor"
D16-1034,N03-1003,0,0.109676,"nd to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sent"
D16-1034,W11-1603,0,0.0728284,"at can be easily extracted from the web making it suitable also for less–resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification. 1 Introduction The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sent"
D16-1034,W14-1206,0,0.0975131,"Missing"
D16-1034,W15-1604,1,0.915744,"Missing"
D16-1034,P11-1020,0,0.0158717,"–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006;"
D16-1034,P15-1118,0,0.0184498,"ing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods taking into account also the order in which information is presented (Bott and Saggion, 2011). Differently from these methods, our approach 351 Proceedings of the 2016 Conference on Empirical Methods in Natural"
D16-1034,P11-2117,0,0.507092,"site for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monoli"
D16-1034,W11-2308,1,0.881025,"Missing"
D16-1034,C04-1051,0,0.0684342,"s preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and El"
D16-1034,P13-1158,0,0.0239739,"ent PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods tak"
D16-1034,N13-1092,0,0.0822218,"Missing"
D16-1034,klerke-sogaard-2012-dsim,0,0.0193123,"guage making available the biggest Italian corpus for automatic text simplification. 1 Introduction The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sent"
D16-1034,W14-3505,0,0.0161026,"omatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods taking into account also the order in which information is presented (Bott and Saggion, 2011). Differently from these methods, our approach 351"
D16-1034,W14-0406,1,0.893826,"Missing"
D16-1034,D09-1040,0,0.0200169,"resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different scenarios such as e.g. paraphrase detection (Ganitkevitch et al., 2013; Barzilay and Lee, 2003; Dolan et al., 2004) and evaluation (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods taking into account also the order in which information is presented (Bott and Saggion, 2011). Diff"
D16-1034,E06-1021,0,0.0266083,"on (Chen and Dolan, 2011), question answering (Fader et al., 2013), textual entailment (Bosma and Callison-Burch, 2007), machine translation (Marton et al., 2009), short answer scoring (Koleva et al., 2014), domain adaptation of dependency parsing (Choe and McClosky, 2015). Specifically in ATS, these techniques are typically applied to already existing parallel corpora; in this case the task of aligning the original sentence to its corresponding simple version can be tackled by applying similarity metrics that consider the TF/IDF score of the words in the sentence (Barzilay and Elhadad, 2003; Nelken and Shieber, 2006; Coster and Kauchak, 2011) or methods taking into account also the order in which information is presented (Bott and Saggion, 2011). Differently from these methods, our approach 351 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 351–361, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics contains two important novelties: the typology of the starting data and consequently the methodology developed to build the complex–simple aligned corpus. To overcome the scarcity of large parallel corpora of complex and simple te"
D16-1034,E14-1076,0,0.0205128,"method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less–resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification. 1 Introduction The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corp"
D16-1034,D11-1038,0,0.260738,"ticularly suitable for text simplification. The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less–resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification. 1 Introduction The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques t"
D16-1034,P12-1107,0,0.0584628,"Missing"
D16-1034,Q15-1021,0,0.039771,"sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application of data–driven techniques to In this paper we present PaCCSS–IT, a Parallel Corpus of Complex–Simple Aligned Sentences for ITalian. To build the resource we developed a new approach for automatically acquiring a large corpus of paired sentences containing structural transformations which can be used as a developmental resource for text simplification systems. The proposed approach relies on monolingual sentence alignment techniques which have been exploited in different s"
D16-1034,C10-1152,0,0.436956,"formations and particularly suitable for text simplification. The method requires a wide amount of texts that can be easily extracted from the web making it suitable also for less–resourced languages. We test it on the Italian language making available the biggest Italian corpus for automatic text simplification. 1 Introduction The availability of monolingual parallel corpora is a prerequisite for research on automatic text simplification (ATS), i.e. the task of reducing sentence complexity by preserving the original meaning. This has been recently shown for different languages, e.g. English (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014), Spanish (Bott and Saggion, 2011; Bott and Saggion, 2014), French (Brouwers et al., 2014), Portuguese (Caseli et al., 2009), Danish (Klerke and Søgaard, 2012), Italian (Brunato et al., 2015). While English can rely on large datasets like the well-known Parallel Wikipedia Simplification corpus (Coster and Kauchak, 2011; Zhu et al., 2010) and, more recently, the Newsela corpus (Xu et al., 2015), for other languages similar resources are difficult to acquire and tend to be very small, thus preventing the application"
D18-1289,bautista-saggion-2014-numerical,0,0.0306135,"ian rs =0.59, English 0.54) and similarly ranked in the two languages at 3rd position. The distribution of verbs num pers makes the sentence harder only for Italian; this is possibly related to the higher complexity of verbal morphology since the 3rd person verb in impersonal structures might increase the ambiguity of the sentence with respect to the referent. Only in English, sentence complexity is affected by the distribution of cardinal numbers (cpos NUM) and the dependency type “numeric modifier” (dep num), in line with the difficulty of numerical information shown in readability studies (Bautista and Saggion, 2014). Conversely, the verbal arity and the relative ordering of subjects with respect to the verb have a lower position in the negative ranking, suggesting that these features make a sentence easier: this might be due to a more fixed predicate-argument structure and word order in this language. If we focus on sentences of the same length, features considered as a proxy of lexical complexity are in the top positions in both languages. It is the case of the average word length (char tok) and the lexical density (lex density) only for English. Interestingly, while for English the majority of features"
D18-1289,D16-1034,1,0.853864,"experiments. In addition, crowdsourcing reaches a 2690 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2690–2699 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics broader population, in terms of age, education, profession etc. and it is thus more suitable to catch the “layman” intuition of sentence complexity. For these reasons, this method has been used in recent works in the field of readability and text simplification; it is the case of Lasecki et al. (2015); Clercq et al. (2013); Brunato et al. (2016) where the crowd was asked to evaluate the level of complexity or the degree of informativeness of simplified sentences compared to the original one. In our study, we adopted a similar perspective relying on a crowdsourcing approach to collect a wide resource containing multiple annotations of sentence complexity given by humans. Unlike traditional studies which typically assess either lexical or structural complexity phenomena, we focused on the analysis of a wide set of linguistic features to investigate how all contribute to human perception of sentence complexity. This choice is also motiv"
D18-1289,W15-2115,0,0.0279256,"essing difficulties are e.g. word frequency, age of acquisition, root frequency effect, orthographic neighbourhood frequency. At syntactic level, a well-studied measure of sentence complexity takes into account dependency length (Gibson, 1998, 2000), which has been used to explain a wide range of psycholinguistic phenomena, such as the subject/object relative clauses asymmetry or the garden path effect in main verb/reduced-relative ambiguities (Gordon et al., 2001; Staub et al., 2010), as well as variations in word order patterns (Gildea and Temperley, 2010), also in a diachronic perspective (Gulordava and Merlo, 2015). Alternatively, processing difficulty has been explained in terms of surprisal (Hale, 2001). Computational models to calculate lexical and syntactic surprisal have been developed by e.g. Roark et al. (2009) using a broadcoverage probabilistic PCFG parser and Demberg and Keller (2009), who introduced Prediction Theory, which aims at unifying Dependency Length Theory with syntactic surprisal, by making use of a psycholinguistically-motivated version of treeadjoining grammar. Unlike more conventional studies on human sentence processing carried out in experimental settings, in this study we rely"
D18-1289,N01-1021,0,0.172266,"ourhood frequency. At syntactic level, a well-studied measure of sentence complexity takes into account dependency length (Gibson, 1998, 2000), which has been used to explain a wide range of psycholinguistic phenomena, such as the subject/object relative clauses asymmetry or the garden path effect in main verb/reduced-relative ambiguities (Gordon et al., 2001; Staub et al., 2010), as well as variations in word order patterns (Gildea and Temperley, 2010), also in a diachronic perspective (Gulordava and Merlo, 2015). Alternatively, processing difficulty has been explained in terms of surprisal (Hale, 2001). Computational models to calculate lexical and syntactic surprisal have been developed by e.g. Roark et al. (2009) using a broadcoverage probabilistic PCFG parser and Demberg and Keller (2009), who introduced Prediction Theory, which aims at unifying Dependency Length Theory with syntactic surprisal, by making use of a psycholinguistically-motivated version of treeadjoining grammar. Unlike more conventional studies on human sentence processing carried out in experimental settings, in this study we rely on crowdsourcing methods to investigate how people perceive sentence complexity. The reliab"
D18-1289,W14-0406,1,0.834034,"Missing"
D18-1289,W17-5007,0,0.0169555,"ect a wide resource containing multiple annotations of sentence complexity given by humans. Unlike traditional studies which typically assess either lexical or structural complexity phenomena, we focused on the analysis of a wide set of linguistic features to investigate how all contribute to human perception of sentence complexity. This choice is also motivated by previous studies focused on the “form” of a text all related to the assessment of complexity, e.g. readability assessment (Collins-Thompson, 2015), first language acquisition (Sagae et al., 2005) and Native Language Identification (Malmasi et al., 2017). 2 Our Contributions Our contribution to the study of sentence complexity is multiple: • we address two research questions aimed to investigate the role played by a set of linguistic phenomena in characterizing a) the agreement among annotators when they rated the sentences independently from the assigned score and b) the human perception of complexity. • we introduce a new crowdsourcing-based method to assess how people perceive sentence complexity and we test it for two languages; • we collect two corpora of sentences annotated by humans with a judgment of complexity; The two research quest"
D18-1289,de-marneffe-etal-2006-generating,0,0.0254539,"Missing"
D18-1289,P13-2017,0,0.047015,"Missing"
D18-1289,W10-0719,0,0.0210803,"coverage probabilistic PCFG parser and Demberg and Keller (2009), who introduced Prediction Theory, which aims at unifying Dependency Length Theory with syntactic surprisal, by making use of a psycholinguistically-motivated version of treeadjoining grammar. Unlike more conventional studies on human sentence processing carried out in experimental settings, in this study we rely on crowdsourcing methods to investigate how people perceive sentence complexity. The reliability of crowdsourced data for linguistics and computational linguistics research is well acknowledged as shown in the survey by Munro et al. (2010) proving that the quality of findings obtained from the crowd is comparable, if not higher, to controlled laboratory experiments. In addition, crowdsourcing reaches a 2690 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2690–2699 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics broader population, in terms of age, education, profession etc. and it is thus more suitable to catch the “layman” intuition of sentence complexity. For these reasons, this method has been used in recent works in the field of"
D18-1289,D09-1034,0,0.0256748,"endency length (Gibson, 1998, 2000), which has been used to explain a wide range of psycholinguistic phenomena, such as the subject/object relative clauses asymmetry or the garden path effect in main verb/reduced-relative ambiguities (Gordon et al., 2001; Staub et al., 2010), as well as variations in word order patterns (Gildea and Temperley, 2010), also in a diachronic perspective (Gulordava and Merlo, 2015). Alternatively, processing difficulty has been explained in terms of surprisal (Hale, 2001). Computational models to calculate lexical and syntactic surprisal have been developed by e.g. Roark et al. (2009) using a broadcoverage probabilistic PCFG parser and Demberg and Keller (2009), who introduced Prediction Theory, which aims at unifying Dependency Length Theory with syntactic surprisal, by making use of a psycholinguistically-motivated version of treeadjoining grammar. Unlike more conventional studies on human sentence processing carried out in experimental settings, in this study we rely on crowdsourcing methods to investigate how people perceive sentence complexity. The reliability of crowdsourced data for linguistics and computational linguistics research is well acknowledged as shown in"
D18-1289,P05-1025,0,0.0129678,"perspective relying on a crowdsourcing approach to collect a wide resource containing multiple annotations of sentence complexity given by humans. Unlike traditional studies which typically assess either lexical or structural complexity phenomena, we focused on the analysis of a wide set of linguistic features to investigate how all contribute to human perception of sentence complexity. This choice is also motivated by previous studies focused on the “form” of a text all related to the assessment of complexity, e.g. readability assessment (Collins-Thompson, 2015), first language acquisition (Sagae et al., 2005) and Native Language Identification (Malmasi et al., 2017). 2 Our Contributions Our contribution to the study of sentence complexity is multiple: • we address two research questions aimed to investigate the role played by a set of linguistic phenomena in characterizing a) the agreement among annotators when they rated the sentences independently from the assigned score and b) the human perception of complexity. • we introduce a new crowdsourcing-based method to assess how people perceive sentence complexity and we test it for two languages; • we collect two corpora of sentences annotated by hu"
D18-1289,simi-etal-2014-less,0,0.0221433,"Missing"
dellorletta-etal-2006-searching,bel-etal-2000-simple,1,\N,Missing
dellorletta-etal-2014-t2k,N09-2066,1,\N,Missing
dellorletta-etal-2014-t2k,W09-1119,0,\N,Missing
dellorletta-etal-2014-t2k,P04-1026,0,\N,Missing
dellorletta-etal-2014-t2k,W03-0419,0,\N,Missing
dellorletta-etal-2014-t2k,W06-2922,0,\N,Missing
dellorletta-etal-2014-t2k,D07-1096,0,\N,Missing
dellorletta-etal-2014-t2k,magnini-etal-2006-cab,0,\N,Missing
L16-1014,berkling-etal-2014-database,0,0.0231897,"the years and types of schools (e.g. private vs. public schools, urban vs. rural). Or by McNamara et al. (2010) who collected 120 essays written by U.S. undergraduate students that were manually evaluated to investigate linguistic factors (e.g. syntactic complexity and lexical diversity) related to the level of student writing quality. If great attention has been payed so far to the construction of corpora of written essays to study English language development of L1 learners, little work has been carried out for other languages. The KoKo corpus (Abel et al., 2014) and the corpus collected by Berkling et al. (2014) represent two main exceptions for the German language. The former is a collection of authentic texts (for a total of 716,000 tokens) written by 1,319 German–speaking students attendhttp://www.cs.rochester.edu/∼tetreaul/naacl-bea10.html 88 ing the last year of secondary school, linguistically annotated using a battery of linguistic annotation tools and manually annotated for background information and errors. It was built to get insight into pupils writing competencies and difficulties. The latter is a corpus of essays collected via elicitation and written by 1,730 students (for a total of 159"
L16-1014,J93-2001,0,0.682489,"Missing"
L16-1014,boyd-etal-2014-merlin,0,0.0312104,"rces for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures"
L16-1014,brooke-hirst-2012-measuring,0,0.0232338,"ilt in the framework of an interdisciplinary study jointly carried out by computational linguistics and experimental pedagogists and aimed at tracking the development of written language competence over the years and students’ background information. Keywords: Italian Learner Corpus, Diachronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorn"
L16-1014,W13-1703,0,0.0290619,"ritten language competence over the years and students’ background information. Keywords: Italian Learner Corpus, Diachronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of langu"
L16-1014,R13-1025,1,0.905929,"Missing"
L16-1014,dickinson-ledbetter-2012-annotating,0,0.0299036,"an be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012)."
L16-1014,abel-etal-2014-koko,0,0.0770918,"the relative rate of progress in writing over the years and types of schools (e.g. private vs. public schools, urban vs. rural). Or by McNamara et al. (2010) who collected 120 essays written by U.S. undergraduate students that were manually evaluated to investigate linguistic factors (e.g. syntactic complexity and lexical diversity) related to the level of student writing quality. If great attention has been payed so far to the construction of corpora of written essays to study English language development of L1 learners, little work has been carried out for other languages. The KoKo corpus (Abel et al., 2014) and the corpus collected by Berkling et al. (2014) represent two main exceptions for the German language. The former is a collection of authentic texts (for a total of 716,000 tokens) written by 1,319 German–speaking students attendhttp://www.cs.rochester.edu/∼tetreaul/naacl-bea10.html 88 ing the last year of secondary school, linguistically annotated using a battery of linguistic annotation tools and manually annotated for background information and errors. It was built to get insight into pupils writing competencies and difficulties. The latter is a corpus of essays collected via elicitatio"
L16-1014,C14-1203,0,0.0136448,"English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP"
L16-1014,W13-3601,0,0.170421,"hronic Evolution of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has"
L16-1014,W14-1701,0,0.0216362,"of Written Language Competence, Error Annotation foreign language (L2). Corpora enriched with this kind of information can offer insight into learners’ development of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed fo"
L16-1014,W07-1001,0,0.0641425,"Missing"
L16-1014,N13-1084,0,0.023759,"Missing"
L16-1014,P05-1025,0,0.0467593,"umber of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of t"
L16-1014,P12-2019,0,0.0211331,"kinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA) arrived in 2016 at its eleventh edition1 , language technolo"
L16-1014,P05-1065,0,0.0334174,"or different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012). NLP–based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013), and to select reading material that are appropriate for students’ reading proficiency considered a fundamental component of language competency (Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009). As witnessed by the increasing success of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA) arrived in 2016 at its eleventh edition1 , language technologies have been also exploited in educational settings to design and develop educational applications such as for instance Intelligent Computer–Assisted Language Learning systems (ICALL) (Granger, 2003) or Automatic Essay Scoring systems (Attali and Burstein, 2006). For all these studies and applications, the availability of electronically accessible corpora of student essays is of"
L16-1014,W15-1614,0,0.0222346,"t of competencies and difficulties (Deane and Quinlan, 2010), they can be used to investigate the characteristics of interlanguage (Brooke and Hirst, 2012) or as reference resources for automatic error detection and correction tasks. The latter is the case of the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) exploited during 2013 and 2014 editions of the “Shared Task on Grammatical Error Correction” (Ng et al., 2013; Ng et al., 2014). Interestingly, corpora of L2 learners annotated for errors have been built for a number of languages other than English, e.g. for Arabic as L2 (Zaghouani et al., 2015), German (Lüdeling et al., 2005), Hungarian (Dickinson and Ledbetter, 2012), Basque (Aldabe et al., 2005), Czech and Italian (Andorno and Rastelli, 2009; Boyd et al., 2014). 1. Introduction Over the last ten years, language technologies have been successfully exploited to study the development of language learning processes. A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014), to measure the developmental"
L18-1719,W13-2308,1,0.758772,"e from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identifying annotation inconsistencies, and thus for also detecting annotation errors (Alzetta et al., 2018). Tusa et al. (2016) represent the first attempt to exploit the plausibility score returned by this class of algorithms to acquire linguistic evidence, i.e. to infer the prototypicality degree of specific linguistic constructions. The experiment was carried out against the Italian Universal Dependency Treebank (IUDT) (Bosco et al., 2013) with promising results: the plausibility-based ranking of dependencies corresponding to specific syntactic constructions turned out to closely reflect their linguistic “markedness” degree. In what follows, we focus on word pattern variation across three different languages, English, Italian and Spanish. This goal is pursued by applying a plausibility assessment algorithm against the UD treebanks available for these languages. Achieved results have been compared with the threefold aim of: i) reconstructing the frequency distributions of different word order patterns, with particular attention"
L18-1719,W11-0314,1,0.817185,"lity of automatically produced syntactic annotations could be used to acquire useful quantitative typological evidence. In fact, the result of these algorithms is typically driven by linguistic properties 2 Available online at http://wals.info 4540 http://universaldependencies.org/ characterizing the language being processed: by comparing the results achieved against different languages, it is possible to acquire information concerning typological similarities and differences. This kind of algorithms operate at the level of either the whole syntactic tree (cfr. for example Dell’Orletta et al. (2011) and Reichart and Rappoport (2009)), or individual dependencies (see, among others, Dell’Orletta et al. (2013) and Che et al. (2014)). Given the focus of this study on specific constructions, we selected the class of algorithms operating at the level of individual dependencies, and in particular on those ranking dependencies by decreasing plausibility of annotation. These algorithms, originally meant to discern reliable from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identi"
L18-1719,W15-2112,0,0.0257473,"n type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The goal in developing UD was not only to support compar"
L18-1719,W15-2115,0,0.138533,"scientific papers. But impact and role of this information type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The"
L18-1719,K15-1025,0,0.345505,"scientific papers. But impact and role of this information type cannot be limited to the descriptive level. Typological evidence inferred from linguistically annotated corpora for dif1 ferent languages can significantly contribute to model linguistic variation within and across languages. Word order variation represents a widely investigated topic of the typological literature whose recent developments include finegrained studies based on a wide range of features and their frequency distributions typically acquired from annotated corpora (O’Horan et al., 2016). To mention only a few, see e.g. Gulordava and Merlo (2015a), Gulordava and Merlo (2015b) Futrell et al. (2015), Merlo (2016). More recently, such an approach to typological studies has also been prompted by the availability of multi–lingual treebanks such as those designed and constructed within the Universal Dependencies project2 for over 50 languages. Universal Dependencies (UD) is a framework for cross– linguistically consistent treebank annotation aiming to capture similarities as well as idiosyncrasies among typologically different languages (e.g., morphologically rich languages, pro–drop languages, and languages featuring clitic doubling). The"
L18-1719,J05-4001,0,0.111724,"different languages (English, Italian and Spanish). Keywords: Linguistic Knowledge Extraction, Dependency Treebanks, Linguistic Typology 1. Introduction The interaction between linguistics and computational linguistics has a long history dating back to the 60’s. In Kuˇcera (1982), it is explicitly stated that “computational linguistics provides important potential tools for the testing of theoretical linguistic constructs and of their power to predict actual language use”. This still appears to represent a key objective, as claimed e.g. by Martin Kay in his ACL Lifetime Award speech in 2005 (Kay, 2005), or by the more recent papers gathered in the Special Issue of the journal “Linguistic Issues in Language Technology” (LiLT) focusing on the relationship between language technology and linguistic insights (Baldwin and Kordoni, 2011). After more than 40 years from the first pioneering studies, the growing availability of linguistic resources such as annotated corpora for many languages combined with the increasing reliability of Natural Language Processing (NLP) methods and tools enables the acquisition of quantitative evidence ranging across different levels of linguistic description which c"
L18-1719,C82-1027,0,0.725667,"Missing"
L18-1719,P13-2017,0,0.0651344,"Missing"
L18-1719,P12-1066,0,0.0126894,"ed, existing typological databases such as WALS have still a partial coverage, and most importantly here, do not always reflect real language use. Methods for automatic induction of typological information are still at the beginning: this paper represents a promising attempt in this area. It is a widely acknowledged fact that word-order affects the automatic analysis of sentences: free–order languages are harder to parse (Gulordava and Merlo, 2015c). Acquired information from real language usage can be used among the “selective sharing parameters” in a cross–lingual transfer parsing scenario (Naseem et al., 2012). Further directions of research include: the application of the methodology to other languages, including typologically distant ones, to reconstruct shades of typological proximity starting from real language usage; the analysis of other dependency relations as well as of more complex structures such as dependency subtrees; the extension of the range of properties expected to influence the preference towards a given word order pattern. Last but not least, we are planning to test the effectiveness of acquired typological information in a cross–lingual parsing scenario. 4547 Acknowledgements Th"
L18-1719,C16-1123,0,0.0604288,"Missing"
L18-1719,W09-1103,0,0.0224685,"omatically produced syntactic annotations could be used to acquire useful quantitative typological evidence. In fact, the result of these algorithms is typically driven by linguistic properties 2 Available online at http://wals.info 4540 http://universaldependencies.org/ characterizing the language being processed: by comparing the results achieved against different languages, it is possible to acquire information concerning typological similarities and differences. This kind of algorithms operate at the level of either the whole syntactic tree (cfr. for example Dell’Orletta et al. (2011) and Reichart and Rappoport (2009)), or individual dependencies (see, among others, Dell’Orletta et al. (2013) and Che et al. (2014)). Given the focus of this study on specific constructions, we selected the class of algorithms operating at the level of individual dependencies, and in particular on those ranking dependencies by decreasing plausibility of annotation. These algorithms, originally meant to discern reliable from unreliable annotations within the automatic output of parsers, have also been applied to manually revised (i.e. “gold”) linguistic annotations with the final aim of identifying annotation inconsistencies,"
L18-1719,silveira-etal-2014-gold,0,0.0308992,"Missing"
L18-1719,L16-1680,0,0.0228477,"constituents. Because of these properties, highly related to the linguistic type each language belongs to, we expect to observe a similar behaviour for typologically close languages and, on the other hand, significant differences in case of typologically distant languages. The corpora used to collect the statistics to build the LISCA models (step 1 in Section 3.1. above) are represented by the English, Italian and Spanish Wikipedia, for a total of around 40 million tokens for each language. The Spanish and English corpora were morpho–syntactically annotated and parsed by the UDPipe pipeline (Straka et al., 2016) trained on the Universal Dependency treebanks, version ˇ 2.0 (J. Nivre and A. Zeljko and A.Lars and et alii, 2017). The Italian corpus was morpho–syntactically tagged using the ILC–POS–Tagger (Dell’Orletta, 2009), and then parsed with UDPipe. LISCA, trained on the models we created earlier, was then applied to the Italian, English and Spanish UD Treebanks in order to assign a plausibility score to each dependency relation. The English Web Treebank (Silveira et al., 2014) contains 16,624 sentences and 254,830 tokens, while the Italian Universal Dependency Treebank (Bosco et al., 2013) contains"
N09-2066,D07-1119,1,0.616982,"Missing"
N09-2066,W06-2920,0,0.0187194,"emma of the predicted grandparent and PHDEP is the predicted dependency label of the predicted head of a token to the predicted grandparent. s0 refers to a token on top of the stack, wi refers to word at the i-th relative position with respect to the current word and parsing direction. This feature model was used for all languages in our tests. We present experiments and comparative error analysis on three representative languages from the CoNLL 2007 shared task (Nivre at al., 2007): Italian, Czech and English. We also report an evaluation on all thirteen languages of the CoNLL-X shared task (Buchholz & Marsi, 2006), for comparison with the results by Nivre and McDonald (2008). Table 2 shows the Labeled Attachment Score (LAS), for the Left-to-right parser (LR ), Right-toLeft (RL ), Reverse Revision parser (Rev2 ), linear parser combination (Comb) and MST parser combination (CombMST). Figure 1 and 2 present the accuracies of the LR and Rev2 parsers for English relative to the dependency length and the length of sentences, respectively. For Czech and Italian the RL parser achieves higher accuracy than the LR parser and the Rev2 parser even higher. The error analysis for Czech showed that the Rev2 parser im"
N09-2066,C96-1058,0,0.0403655,"these errors, although maintaining a quasi linear complexity. The method consists in two steps: first the sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser. Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity. In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum spanning tree (MST) of a graph, which require algorithms of quadratic time complexity (e.g. (Chu & Liu, 1965; Edmonds, 1967)). We introduce an alternative linear combination 261 method. The al"
N09-2066,D07-1097,0,0.217437,"Missing"
N09-2066,D07-1013,0,0.0340873,"Missing"
N09-2066,H05-1066,0,0.0268115,"Missing"
N09-2066,E06-1011,0,0.0344113,"Missing"
N09-2066,P08-1108,0,0.307747,"y using a beam search instead of a greedy algorithm for predicting the next parser transition. We propose a parsing method that allows reducing several of these errors, although maintaining a quasi linear complexity. The method consists in two steps: first the sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser. Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity. In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum spanning tree (MST) of a graph, which require algor"
N09-2066,C04-1010,0,0.0112609,"he sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser. Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity. In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum spanning tree (MST) of a graph, which require algorithms of quadratic time complexity (e.g. (Chu & Liu, 1965; Edmonds, 1967)). We introduce an alternative linear combination 261 method. The algorithm is greedy and works by combining the trees top down. We tested it on the dependency trees produced by thr"
N09-2066,N06-2033,0,0.0148524,"erson (2007) address this accuracy drop by using a beam search instead of a greedy algorithm for predicting the next parser transition. We propose a parsing method that allows reducing several of these errors, although maintaining a quasi linear complexity. The method consists in two steps: first the sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser. Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity. In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum span"
N09-2066,D07-1099,0,0.0233493,"Missing"
N09-2066,W03-3023,0,0.063532,"ists in two steps: first the sentence is parsed by a deterministic Shift/Reduce parser, then a second deterministic Shift/Reduce parser analyzes the sentence in reverse using additional features extracted from the parse trees produced by the first parser. Right-to-left parsing has been used as part of ensemble-based parsers (Sagae & Lavie, 2006; Hall et al., 2007). Nivre and McDonald (2008) instead use hints from one parse as features in a second parse, exploiting the complementary properties of graph-based parsers (Eisner, 1996; McDonald et al., 2005) and transition-based dependency parsers (Yamada & Matsumoto, 2003; Nivre & Scholz, 2004). Also our method uses input from a previous parser but only uses parsers of a single type, deterministic transition-based Shift/Reduce, maintaining an overall linear complexity. In fact both the ensemble parsers and the stacking solution of NivreMcDonald involve the computation of the maximum spanning tree (MST) of a graph, which require algorithms of quadratic time complexity (e.g. (Chu & Liu, 1965; Edmonds, 1967)). We introduce an alternative linear combination 261 method. The algorithm is greedy and works by combining the trees top down. We tested it on the dependenc"
N09-2066,D07-1096,0,\N,Missing
passarotti-dellorletta-2010-improvements,nilsson-nivre-2008-malteval,0,\N,Missing
passarotti-dellorletta-2010-improvements,E06-1011,0,\N,Missing
passarotti-dellorletta-2010-improvements,W09-0306,1,\N,Missing
passarotti-dellorletta-2010-improvements,N09-2066,1,\N,Missing
passarotti-dellorletta-2010-improvements,W06-0604,1,\N,Missing
passarotti-dellorletta-2010-improvements,W06-2920,0,\N,Missing
passarotti-dellorletta-2010-improvements,W06-2922,0,\N,Missing
passarotti-dellorletta-2010-improvements,P05-1013,0,\N,Missing
passarotti-dellorletta-2010-improvements,D07-1096,0,\N,Missing
passarotti-dellorletta-2010-improvements,W07-0905,0,\N,Missing
passarotti-dellorletta-2010-improvements,W07-2218,0,\N,Missing
R13-1025,W06-2922,0,0.0396621,"Missing"
R13-1025,J93-2001,0,0.746893,"Missing"
R13-1025,W01-0521,0,0.0306001,"Missing"
R13-1025,C10-1062,0,0.064829,"Missing"
R13-1025,C96-2123,0,0.116086,"Missing"
R13-1025,W11-2308,1,0.921951,"Missing"
R13-1025,D07-1013,0,0.0629438,"Missing"
R13-1025,W12-5812,1,0.75592,"Missing"
R13-1025,U09-1008,0,0.0744316,"Missing"
R13-1025,W13-1506,0,0.0549089,"Missing"
R13-1025,P04-1026,0,\N,Missing
W05-0509,bartolini-etal-2004-hybrid,1,0.869022,"Missing"
W05-0509,J96-1002,0,0.00498103,"ound way to build a probabilistic model for SOI, which combines different linguistic cues. Given a linguistic context c and an outcome a∈A that depends on c, in the ME framework the conditional probability distribution p(a|c) is estimated on the basis of the assumption that no a priori constraints must be met other than those related to a set of features f j (a,c) of c, whose distribution is derived from the training data. It can be proven that the probability distribution p satisfying the above assumption is the one with the highest entropy, is unique and has the following expone ntial form (Berger et al. 1996): (1) p (a |c) = 1 k f ( a ,c ) αjj ∏ Z (c) j =1 where Z(c) is a normalization factor, f j (a,c) are the values of k features of the pair (a,c) and correspond to the linguistic cues of c that are relevant to predict the outcome a. Features are extracted from the training data and define the constraints that the probabilistic model p must satisfy. The parameters of the distribution a 1 , …, a k correspond to weights associated with the features, and determine the relevance of each feature in the overall model. In the experiments reported below feature weights have been estimated with the Genera"
W05-0509,bel-etal-2000-simple,1,0.836713,"Missing"
W06-0604,W05-0509,1,0.838535,"Missing"
W06-0604,bel-etal-2000-simple,1,0.758385,"Missing"
W08-2138,W06-2922,1,0.69496,"ddress each of these subtasks with separate components without backward feedback between sub-tasks. However, the use of multiple parsers at the beginning of the process, and re-ranking at the end, contribute beneficial stochastic aspects to the system. Figure 1 summarizes the system architecture. We detail the parsing All authors contributed equally to this work. c 2008. ° Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. ∗ DeSRlef t−to−right This model is a version of DeSR (Attardi, 2006), a deterministic classifier-based Shif t/Reduce parser. The parser processes input tokens advancing on the input from left to right with Shif t actions and accumulates processed tokens on a stack with Reduce actions. The parser has been adapted for this year’s shared task and extended with additional classifiers, e.g., Multi Layer Perceptron and multiple SVMs.2 The parser uses the following features: System description ∗ Parsing 1. SPLIT LEMMA: from tokens −1, 0, 1, prev(0), lef tChild(0), rightChild(0) 2. PPOSS: from −2, −1, 0, 1, 2, 3, prev(0), next(−1), lef tChild(−1), lef tChild(0), right"
W08-2138,N07-1049,1,0.890278,"Missing"
W08-2138,W05-1505,0,0.107878,"Missing"
W08-2138,N06-2033,0,0.0610556,"Missing"
W08-2138,P05-1073,0,0.0657299,"or split lemmas. 4. Length of the dependency path. 5. Distance in tokens between p and a. 6. Position of a relative to p: before or after. We implemented two inference strategies: greedy and reranking. The greedy strategy sorts all arguments in a frame Fin in descending order of their scores and iteratively adds each argument to the output frame Fout only if it respects the domain constraints with the other arguments already selected. The only domain constraint we use is that core arguments cannot repeat. 3.3 Reranking of Argument Frames The reranking inference strategy adapts the approach of Toutanova et al. (2005) to the dependency representation with notable changes in candidate selection, feature set, and learning model. For candidate selection we modify Algorithm 1: instead of storing only yˆ for each argument in Fin we store the top k best labels. Then, from the arguments in Fin , we generate the top k frames with the highest score, where the score of a frame is the product of all its argument probabilities, computed as the sof tmax function on the output of the Perceptron. In this set of candidate frames we mark the frame with the highest F1 score as the positive example and all others as negative"
W08-2138,W08-2121,1,\N,Missing
W08-2138,S07-1095,1,\N,Missing
W10-3711,bonin-etal-2010-contrastive,1,0.520112,"Missing"
W11-0314,W06-2922,0,0.056052,"lows: weight((P d, P h, t)) = F ((P d, P h, t)) · F ((P d, X, t)) F ((P d, P h, t)) · F ((X, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P d, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P h, P h2, t2)) F (((P d, P h, t)(P h, P h2, t2))) · , F ((((P d, X, t))(X, P h2, t2))) · where F (x) is the frequency of x in I, X is a variable and (arc1 arc2) represent two consecutive arcs in the tree. 3 The Parsers ULISSE was tested against the output of two really different data–driven parsers: the first–order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. The 1 We set r=0 in the in–domain experiments and r=2 in the out–of–domain experiment reported in Sec 5.3. former is a graph–based parser (following the so– called “all–pairs” approach Buchholz et al. (2006)) where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for a maximum spanning tree in a directed graph. The latter is a Shift–Reduce parser (following a “stepwise” approach, Buchholz et al. (2006)), where the parser is trained and learns the sequence"
W11-0314,W06-2920,0,0.186517,"Missing"
W11-0314,I08-2097,0,0.578412,"studies devoted to detecting reliable parses from the output of a syntactic parser is spreading. They mainly differ with respect to the kind of selection algorithm they exploit. Depending on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of"
W11-0314,C96-2123,0,0.0615079,"ring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause. It is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post–verbal rather than in pre–verbal position (Miller, 1998). Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in accuracy when analysing long distance dependencies. This is in line with Lin (1996) and Gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. Dependency link plausibility (henceforth, ArcPOSFeat): this feature is used to calculate the plausibility of a dependency link given the part–of–speech of the dependent and the head, by also considering the PoS of the head father and the dependency linking the two. 2.2 Comput"
W11-0314,W06-2932,0,0.0512999,"The individual arc weight is computed as follows: weight((P d, P h, t)) = F ((P d, P h, t)) · F ((P d, X, t)) F ((P d, P h, t)) · F ((X, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P d, P h, t)) F (((P d, P h, t)(P h, P h2, t2))) · · F ((P h, P h2, t2)) F (((P d, P h, t)(P h, P h2, t2))) · , F ((((P d, X, t))(X, P h2, t2))) · where F (x) is the frequency of x in I, X is a variable and (arc1 arc2) represent two consecutive arcs in the tree. 3 The Parsers ULISSE was tested against the output of two really different data–driven parsers: the first–order Maximum Spanning Tree (MST) parser (McDonald et al., 2006) and the DeSR parser (Attardi, 2006) using Support Vector Machine as learning algorithm. The 1 We set r=0 in the in–domain experiments and r=2 in the out–of–domain experiment reported in Sec 5.3. former is a graph–based parser (following the so– called “all–pairs” approach Buchholz et al. (2006)) where every possible arc is considered in the construction of the optimal parse tree and where dependency parsing is represented as the search for a maximum spanning tree in a directed graph. The latter is a Shift–Reduce parser (following a “stepwise” approach, Buchholz et al. (2006)), where the parse"
W11-0314,D07-1013,0,0.651407,"lauses: subordination is generally considered to be an index of structural complexity in language. Two distinct features are considered for monitoring this aspect: one measuring the ratio between main and subordinate clauses and the other one focusing on the relative ordering of subordinate clauses with respect to the main clause. It is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses; however, subordinate clauses are easier to process if they occur in post–verbal rather than in pre–verbal position (Miller, 1998). Length of dependency links: McDonald and Nivre (2007) report that statistical parsers have a drop in accuracy when analysing long distance dependencies. This is in line with Lin (1996) and Gibson (1998) who claim that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links, given the memory overhead of very long distance dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. Dependency link plausibility (henceforth, ArcPOSFeat): this feature is used to calculate the plausibility of a dependency link given the part–of–"
W11-0314,J93-2004,0,0.0434808,"Missing"
W11-0314,P06-1043,0,0.225735,"Missing"
W11-0314,C08-1071,0,0.0182086,"dPUPA represents each sentence as a collection of sequences of POSs covering all identified dependency subtrees. In particular, each dependency tree is represented as the set of all subtrees rooted by non–terminal nodes. Each subtree is then represented as the sequence of POS tags of the words in the subtree (reflecting the word order of the original sentence) integrated with the POS of the leftmost and rightmost in the sentence (NULL when there are no neighbors). Figure 1 shows the example of the dependency tree for the sentence I will give you the ball. by Reichart and Rappoport (2007b) and McClosky et al. (2008), small and big treebanks pose different problems in the reliable parses selection. Last but not least, we aimed at demonstrating that ULISSE can be successfully used not only with texts belonging to the same domain as the parser training corpus. For this purpose, ULISSE was tested on a target corpus of Italian legislative texts, whose automatic linguistic analysis poses domain–specific challenges (Venturi, 2010). Out–of–domain experiments are being carried out also for English. Figure 1: Example of dependency tree. If we consider the subtree rooted by give (in the dotted circle), the resultin"
W11-0314,D08-1093,0,0.0234258,"g on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses prod"
W11-0314,P07-1052,0,0.596188,"d semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy a"
W11-0314,P07-1078,0,0.522868,"d semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy a"
W11-0314,W09-1120,0,0.162688,"ed methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser. To our knowledge, Reichart and Rappoport (2009a) are the first to address the task of high qual115 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115–124, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics ity parse selection by resorting to an unsupervised– based method. The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently. For this purpose, their PUPA (POS– based Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed"
W11-0314,W09-1103,0,0.157155,"ed methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of the used parser. To our knowledge, Reichart and Rappoport (2009a) are the first to address the task of high qual115 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 115–124, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics ity parse selection by resorting to an unsupervised– based method. The underlying idea is that syntactic structures that are frequently created by a parser are more likely to be correct than structures produced less frequently. For this purpose, their PUPA (POS– based Unsupervised Parse Assessment Algorithm) uses statistics about POS tag sequences of parsed"
W11-0314,D10-1067,0,0.0119059,"ncy parsers. ULISSE shows a promising performance against the output of two supervised parsers selected for their behavioral differences. In all experiments, ULISSE outperforms all baselines, including dPUPA and Sentence Length (SL), the latter representing a very strong baseline selection method in a supervised scenario, where parsers have a very high performance with short sentences. The fact of carrying out the task of reliable parse selection in a supervised scenario represents an important novelty: however, the unsupervised nature of ULISSE could also be used in an unsupervised scenario (Reichart and Rappoport, 2010). Current direction of research include a careful study of a) the quality score function, in particular for what concerns the combination of individual feature weights, and b) the role and effectivess of the set of linguistic features. This study is being carried out with a specific view to NLP tasks which might benefit from the ULISSE algorithm. This is the case, for instance, of the domain adaptation task in a self–training scenario (McClosky et al., 2006), of the treebank construction process by minimizing the human annotators’ efforts (Reichart and Rappoport, 2009b), of n–best ranking meth"
W11-0314,W07-1001,0,0.0257779,"a set of parsed sentences and it assigns to each dependency tree a score quantifying its reliability. It operates in two different steps: 1) it collects statistics about a set of linguistically–motivated features extracted from a corpus of parsed sentences; 2) it calculates a quality (or reliability) score for each analyzed sentence using the feature statistics extracted from the whole corpus. 2.1 Selection of features The features exploited by ULISSE are all linguistically motivated and rely on the dependency tree structure. Different criteria guided their selection. First, as pointed out in Roark et al. (2007), we needed features which could be reliably identified 116 within the automatic output of a parser. Second, we focused on dependency structures that are widely agreed in the literature a) to reflect sentences’ syntactic and thus parsing complexity and b) to impose a high cognitive load on the parsing of a complete sentence. Here follows the list of features used in the experiments reported in this paper, which turned out to be the most effective ones for the task at hand. Parse tree depth: this feature is a reliable indicator of sentence complexity due to the fact that, with sentences of appr"
W11-0314,D07-1111,0,0.0874791,"e web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches proposed by Reichart and Rappoport (2007a) and Sagae and Tsujii (2007) can be classified as ensemble–based methods. Both select high quality parses by computing the level of agreement among different parser outputs: wheras the former uses several versions of a constituency parser, each trained on a different sample from the training data, the latter uses the parses produced by different dependency parsing algorithms trained on the same data. However, a widely acknowledged problem of both supervised–based and ensemble–based methods is that they are dramatically influenced by a) the selection of the training data and b) the accuracy and the typology of errors of t"
W11-0314,W06-1604,0,0.196462,"erest has been shown in assessing the reliability of automatically produced parses: the selection of high quality parses represents nowadays a key and challenging issue. The number of studies devoted to detecting reliable parses from the output of a syntactic parser is spreading. They mainly differ with respect to the kind of selection algorithm they exploit. Depending on whether training data, machine learning classifiers or external parsers The first is the case of the construction of a machine learning classifier to predict the reliability of parses on the basis of different feature types. Yates et al. (2006) exploited semantic features derived from the web to create a statistical model to detect unreliable parses produced by a constituency parser. Kawahara and Uchimoto (2008) relied on features derived from the output of a supervised dependency parser (e.g. dependency lengths, number of unknown words, number of coordinated conjunctions, etc.), whereas Ravi et al. (2008) exploited an external constituency parser to extract text–based features (e.g. sentence length, unknown words, etc.) as well as syntactic features to develop a supervised predictor of the target parser accuracy. The approaches pro"
W11-0314,W06-1626,0,0.021146,"Missing"
W11-0314,D07-1096,0,\N,Missing
W11-2308,W10-1001,0,0.638024,"ng of documents by reading difficulty (e.g. in returning the results of web queries) to NLP tasks such as automatic document summarization, machine translation as well as text simplification. Again, also the application making use of the readability assessment, which is in turn strictly related to the intended audience of readers, strongly influences the typology of features to be taken into account. Advanced NLP–based readability metrics developed so far typically deal with English, with a few attempts devoted to other languages, namely French (Collins-Thompson and Callan, 2004), Portuguese (Aluisio et al., 2010) and German (Br¨uck, 2008). 75 3 Our Approach Our approach to readability assessment was developed with a specific application in mind, i.e. text simplification, and addresses a specific target audience of readers, namely people characterised by low literacy skills and/or by mild cognitive impairment. Following the most recent approaches, we treat readability assessment as a classification task: in particular, given the available corpora for the Italian language as well as the type of target audience, we resorted to a binary classification aimed at discerning easy–to–read textual objects from"
W11-2308,W06-2922,0,0.0706355,"Missing"
W11-2308,J08-1001,0,0.00807082,"tures from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent approaches esplored the role of these features in readability assessment: this is the case, for instance, of Barzilay and Lapata (2008) or Feng (2010). The last few years have been characterised by approaches based on the combination of features ranging over different linguistic levels, namely lexical, syntactic and discourse (see e.g. Pitler and Nenkova (2008), Kate (2010)). Another important factor determining the typology of features to be considered for assessing readability has to do with the intended audience of readers: it is commonly agreed that reading ease does not follow from intrinsic text properties alone, but it is also affected by the expected audience. Among the studies addressing readability with respect to s"
W11-2308,C10-2032,0,0.217984,"Missing"
W11-2308,E09-1027,0,0.0392423,"Missing"
W11-2308,N07-1058,0,0.764136,"ng approach (Smoothed Unigram model) to predict reading difficulty of short passages and web documents. These approaches can be seen as a generalization of the vocabulary-based approach, aimed at capturing finer-grained and more flexible information about vocabulary usage. If unigram language models help capturing important content information and variation of word usage, they do not cover other types of features which are reported to play a significant role in the assessment of readability. More recently, the role of syntactic features started being investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009): in these studies syntactic structure is tracked through a combination of features from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent"
W11-2308,W08-0909,0,0.0511786,"Missing"
W11-2308,C10-1062,0,0.0799183,"Missing"
W11-2308,C96-2123,0,0.0561653,"ing with respect to the main clause: according to Miller and Weinert (1998), sentences containing subordinate clauses in post–verbal rather than in pre–verbal position are easier to read. Two further features were introduced to capture the depth of embedded subordinate clauses since it is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses: in particular, a) the average depth of ‘chains’ of embedded subordinate clauses and b) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Length of dependency links feature: both Lin (1996) and Gibson (1998) showed that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links. This is also demonstrated in McDonald and Nivre (2007) who claim that statistical parsers have a drop in accuracy when analysing long dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. This feature is the dependency-based counterpart of the ‘phrase length’ feature used for readability assessment by Nenkova (2010) and Feng (2010). 5 The Corpora One challenge in this work was f"
W11-2308,D07-1013,0,0.0152867,"o read. Two further features were introduced to capture the depth of embedded subordinate clauses since it is a widely acknowledged fact that highly complex sentences contain deeply embedded subordinate clauses: in particular, a) the average depth of ‘chains’ of embedded subordinate clauses and b) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Length of dependency links feature: both Lin (1996) and Gibson (1998) showed that the syntactic complexity of sentences can be predicted with measures based on the length of dependency links. This is also demonstrated in McDonald and Nivre (2007) who claim that statistical parsers have a drop in accuracy when analysing long dependencies. Here, the dependency length is measured in terms of the words occurring between the syntactic head and the dependent. This feature is the dependency-based counterpart of the ‘phrase length’ feature used for readability assessment by Nenkova (2010) and Feng (2010). 5 The Corpora One challenge in this work was finding an appropriate corpus. Although a possibly large collection of texts labelled with their target grade level (such as the Weekly Reader for English) would be ideal, we are not aware of any"
W11-2308,D08-1020,0,0.103077,"yntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty of a text. More recent approaches esplored the role of these features in readability assessment: this is the case, for instance, of Barzilay and Lapata (2008) or Feng (2010). The last few years have been characterised by approaches based on the combination of features ranging over different linguistic levels, namely lexical, syntactic and discourse (see e.g. Pitler and Nenkova (2008), Kate (2010)). Another important factor determining the typology of features to be considered for assessing readability has to do with the intended audience of readers: it is commonly agreed that reading ease does not follow from intrinsic text properties alone, but it is also affected by the expected audience. Among the studies addressing readability with respect to specific audiences, it is worth mentioning here: Schwarm and Ostendorf (2005) and Heilman et al. (2007) dealing with language learners, or Feng (2009) focussing on people with mild intellectual disabilities. Interestingly,Heilman"
W11-2308,W07-1001,0,0.0205729,"Missing"
W11-2308,P05-1065,0,0.551298,"ted a similar language modeling approach (Smoothed Unigram model) to predict reading difficulty of short passages and web documents. These approaches can be seen as a generalization of the vocabulary-based approach, aimed at capturing finer-grained and more flexible information about vocabulary usage. If unigram language models help capturing important content information and variation of word usage, they do not cover other types of features which are reported to play a significant role in the assessment of readability. More recently, the role of syntactic features started being investigated (Schwarm and Ostendorf, 2005; Heilman et al., 2007; Petersen and Ostendorf, 2009): in these studies syntactic structure is tracked through a combination of features from ngram (trigram, bigram and unigram) language models and parse trees (parse tree height, number of noun phrases, verb phrases and subordinated clauses or SBARs) with more traditional features. Yet, besides lexical and syntactic complexity features there are other important factors, such as the structure of the text, the definition of discourse topic, discourse cohesion and coherence and so on, playing a central role in determining the reading difficulty o"
W11-2308,W10-1007,0,0.022255,"dentified within the output of NLP tools. Last but not least, as already done by Aluisio et al. (2010) the set of selected syntactic features also includes simplification oriented ones, with the final aim of aligning the readability assessment step with the text simplification process. Another qualifying feature of our approach to readability assessment consists in the fact that we are dealing with two types of textual objects: documents and sentences. The latter represents an important novelty of our work since so far most research focused on readability classification at the document level (Skory and Eskenazi, 2010). When the target application is text simplification, we strongly believe that also assessing readability at the sentence level could be very useful. We know that methods developed so far perform well to characterize the level of an entire document, but they are unreliable for short texts and thus also for single sentences. Sentence-based readability assessment thus represents a further challenge we decided to tackle: in fact, if all sentences occurring in simplified texts can be assumed to be easy-to-read sentences, the reverse does not necessarily hold since not all sentences occurring in co"
W11-2308,N04-1025,0,\N,Missing
W13-1727,J93-2001,0,0.350606,"c form rather than 207 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 207–215, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics the content of texts, such as readability assessment (Dell’Orletta et al., 2011a) or the classification of textual genres (Dell’Orletta et al., 2012). The exploitation of general features qualifying the lexical and grammatical structure of a text, rather than ad hoc features specifically selected for the task at hand, is not the only peculiarity of our approach to NLI. Following Biber (1993), we start from the assumption that “linguistic features from all levels function together as underlying dimensions of variation”. This choice stems from studies on linguistic variation, in particular from Biber and Conrad (2009) who claim that linguistic varieties – called “registers” from a functional perspective – differ “in their characteristic distributions of pervasive linguistic features, not the single occurrence of an individual feature”. This is to say that by carrying out the linguistic analysis of collections of essays each written by different L1 native speakers, we need to quanti"
W13-1727,C12-1025,0,0.321926,"es (Tetreault et al., 2013). The First Shared Task on Native Language Identification (Tetreault et al., 2013) can be seen as an answer to the above mentioned problems. In this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task. Following the guidelines by the Shared Task Organizers based on the previous literature on this topic, Native Language Identification is tackled as a text classification task combining NLP–enabled feature extraction and machine learning: see e.g. Tetreault et al. (2013) and Brooke and Hirst (2012). Interestingly, the same methodological paradigm is shared by other tasks like e.g. author recognition and verification (see e.g. van Halteren (2004), authorship attribution (see Juola (2008) for a survey), genre identification (Mehler et al., 2011) as well as readability assessment (see Dell’Orletta et al. (2011a) for an updated survey), all relying on feature extraction from automatically parsed texts and state–of–the–art machine learning algorithms. Besides obvious differences at the level of the typology of selected linguistic features and of learning techniques, these different tasks sha"
W13-1727,W11-2308,1,0.922176,"Missing"
W13-1727,W12-5812,1,0.813654,"Missing"
W13-1727,C96-2123,0,0.0531665,"ation and includes: a) the distribution of subordinate vs main clauses; b) the average depth of ‘chains’ of embedded subordinate clauses and c) the probability distribution of embedded subordinate clauses ‘chains’ by depth. Similarly to parse tree depth, this set of features can be taken to reflect the structural complexity of sentences and can thus be indicative of specific difficulties of L2 learners. Length of dependency links: measured in terms of the words occurring between the syntactic head and the dependent. This is another feature which reflects the syntactic complexity of sentences (Lin, 1996; Gibson, 1998) and which can be successfully exploited to capture syntactic idiosyncracies of L2 learners due to L1 interferences. 2.4 Other features Two further features have been considered for NLI purposes, which were included in the distributed datasets. For each document, we have also considered i) the English language proficiency level (high, medium, or low) based on human assessment by language specialists, and ii) the topic of the essays. 3 Linguistic Profiling of TOEFL11 Corpus In this section, we illustrate the results of linguistic profiling carried out on the training and developm"
W13-1727,D07-1013,0,0.0509215,"Missing"
W13-1727,U09-1008,0,0.0339103,"ures (Figures 1(a) and 1(b)), both average sentence length and average word length vary significantly across L1s. In particular, if on the one hand the essays written by Arabic and Spanish L1 speakers contain the shortest words and the longest sentences, on the other hand the Hindi and Telugu L1 essays are characterized by the longest words; the L1 Japanese and Korean corpora contain the shortest sentences. Let us focus now on the distribution of unigrams of coarse grained Parts–Of–Speech. If we consider the distributions of determiners and nouns, two features typically used for NLI purposes (Wong and Dras, 2009) which also represent stylistic markers associated with different linguistic varieties (Biber and Conrad, 2009), it can be noticed (see Figures 1(c) and 1(d)) that for Japanese and Korean the essays show the lowest percentage of determiners, while for Hindi and Telugu they are characterized by the highest percentage of nouns. For what concerns syntactic features, we observe that essays by Japanese and Korean speakers are characterized by quite a different distribution with respect to the other L1 corpora. In particular, they show the shallowest parse trees, the shortest dependency links as wel"
W13-1727,C12-1158,0,0.126834,"Missing"
W13-1727,W13-1706,0,0.278211,"e achieved encouraging results, which show that the proposed approach is general–purpose and portable across different tasks, domains and languages. 1 Introduction Since the seminal work by Koppel et al. (2005), within the Computational Linguistics community there has been a growing interest in the NLP–based Native Language Identification (henceforth, NLI) task. However, so far, due to the unavailability of balanced and wide–coverage benchmark corpora and the lack of evaluation standards it has been difficult to compare the results achieved for this task with different methods and techniques (Tetreault et al., 2013). The First Shared Task on Native Language Identification (Tetreault et al., 2013) can be seen as an answer to the above mentioned problems. In this paper, we describe our approach to native language identification and discuss the results we submitted as participants to the First NLI Shared Task. Following the guidelines by the Shared Task Organizers based on the previous literature on this topic, Native Language Identification is tackled as a text classification task combining NLP–enabled feature extraction and machine learning: see e.g. Tetreault et al. (2013) and Brooke and Hirst (2012). In"
W13-1727,P04-1026,0,\N,Missing
W13-1906,P06-1043,0,0.304181,"Missing"
W13-1906,N10-1004,0,0.0154808,"orini, 1993) in order to carry out an inter–domain analysis of the typology of errors made by each parser and demonstrated that by integrating the output of the three parsers they achieved statistically significant performance gains. Three different methods of parser adaptation for the biomedical domain have been proposed by (Lease & Charniak, 2005) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB–trained parser by improving the Part–Of–Speech tagging accuracy and by relying on an external domain–specific lexicon. More recently, (McClosky, Charniak, & Johnson, 2010) and (Plank & van Noord, 2011) devised adaptation methods based on domain similarity measures. In particular, both of them adopted lexical similarity measures to automatically select from an annotated collection of texts those training data which is more relevant, i.e. lexically closer, to adapt the parser to the target domain. In this paper, a new self–training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically– driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a sign"
W13-1906,W06-2932,0,0.0355201,"Missing"
W13-1906,D07-1013,0,0.0533493,"Missing"
W13-1906,J93-2004,0,0.043758,"Missing"
W13-1906,D07-1096,0,0.0401129,"Missing"
W13-1906,P11-1157,0,0.0308721,"Missing"
W13-1906,P07-1078,0,0.0281545,"ved results (Section 6). identical analyses for the same sentence within the output of different parsing models trained on the same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the G"
W13-1906,W09-1120,0,0.0207968,"s and about 18,600 sentences. For testing, we used the subset of Section 23 of WSJ consisting of 5,003 tokens (214 sentences). All corpora have been morpho–syntactically tagged and lemmatized by a customized version In this paper, we address the second scenario with a main novelty: we use an unsupervised approach to select reliable parses from automatically parsed target domain texts to be combined with the gold–training set. Two unsupervised algorithms have been proposed so far in the literature for selecting reliable parses, namely: PUPA (POS– based Unsupervised Parse Assessment Algorithm) (Reichart & Rappoport, 2009) and ULISSE (Unsupervised LInguiStically–driven Selection of dEpendency parses) (Dell’Orletta, Venturi, & Montemagni, 2011). Both algorithms assign a quality score to each parse tree based on statistics collected from a large automatically parsed corpus, with a main difference: whereas PUPA operates on costituency trees and uses statistics about sequences of part–of–speech tags, ULISSE uses statistics about linguistic features checked against dependency–based representations. The self– training strategy presented in this paper is based on an augmented version of ULISSE. The reasons for this ch"
W13-1906,W10-2606,0,0.0910146,"same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHEM) and biomedical"
W13-1906,D07-1111,0,0.0143815,"notated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ at the level of the selection of parse trees to be added to the in–domain gold trees as further training data. Depending on whether or not external supervised classifiers are used to select the parses to be added to the gold–training set, two types of methods are envisaged in the literature. The first is the case, among others, of: (Kawahara & Uchimoto, 2008), using a machine learning classifier to predict the reliability of parses on the basis of different feature types; or (Sagae & Tsujii, 2007), selecting 45 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45–53, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics After introducing the in– and out–domain corpora used in this study (Section 2), we discuss the results of the multi–level linguistic analysis of these corpora carried out (Section 3) with a view to identifying the main features differentiating the biomedical language from ordinary language. In Section 4, the algorithm used to select reliable parses from automatically parsed domain–specific texts"
W13-1906,I05-2038,0,0.104202,"problem in the biomedical domain where, due to the rapidly expanding body of biomedical literature, the need for increasingly sophisticated and efficient biomedical text mining systems is becoming more and more pressing. In particular, the existence of natural language parsers reliably dealing with biomedical texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled within the biomedical NLP community from different perspectives. The development of a domain–specific annotated corpus, i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta, & Tsujii, 2005), played a key role by providing a sound basis for empirical performance evaluation as well as training of parsers. On the other hand, several attempts have been made to adapt general parsers to the biomedical domain. First experiments in this direction are reported in (Clegg & Shepherd, 2005) who first compared the performance of three different parsers against the Genia treebank and a sample of the Penn Treebank A variety of semi–supervised approaches, where unlabeled data is used in addition to labeled training data, have been recently proposed in the literature in order to adapt parsing s"
W13-1906,W06-2922,0,0.0713225,"Missing"
W13-1906,W06-2920,0,0.0307875,"trategy for domain adaptation able to capture reliable parses which are also representative of the syntactic peculiarities of the target domain. 1 http://categorizer.tmit.bme.hu/∼illes/genia ptb/ In order to be fully compliant with the PTB PoS tagset, we changed the PoS label of all punctuation marks. 2 46 of the pos–tagger described in (Dell’Orletta, n.d.) and dependency parsed by the DeSR parser using Multi–Layer Perceptron (MLP) as learning algorithm (Attardi, Dell’Orletta, Simi, & Turian, n.d.), a state–of–the–art linear–time Shift–Reduce dependency parser following a “stepwise” approach (Buchholz & Marsi, 2006). 3 Linguistic analysis of biomedical abstrats vs newspaper articles Figure 1: Average sentence length in biomedical and newspaper corpora. For the specific concerns of this study, we carried out a comparative linguistic analysis of four different corpora, taken as representative of ordinary language and biomedical language. In each case, we took into account a gold (i.e. manually annotated) corpus, and an unlabelled corpus, which was automatically annotated. By comparing the results obtained with respect to gold and automatically annotated texts, we intend to demonstrate the reliability of fe"
W13-1906,W05-1102,0,0.0314551,"texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled within the biomedical NLP community from different perspectives. The development of a domain–specific annotated corpus, i.e. the Genia Treebank (Tateisi, Yakushiji, Ohta, & Tsujii, 2005), played a key role by providing a sound basis for empirical performance evaluation as well as training of parsers. On the other hand, several attempts have been made to adapt general parsers to the biomedical domain. First experiments in this direction are reported in (Clegg & Shepherd, 2005) who first compared the performance of three different parsers against the Genia treebank and a sample of the Penn Treebank A variety of semi–supervised approaches, where unlabeled data is used in addition to labeled training data, have been recently proposed in the literature in order to adapt parsing systems to new domains. Among these approaches, the last few years have seen a growing interest in self–training for domain adaptation, i.e. a method for using automatically annotated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ a"
W13-1906,W11-0314,1,0.722733,"Missing"
W13-1906,W01-0521,0,0.0175701,"hose training data which is more relevant, i.e. lexically closer, to adapt the parser to the target domain. In this paper, a new self–training method for domain adaptation is illustrated, where the selection of reliable parses is carried out by an unsupervised linguistically– driven algorithm, ULISSE. The method has been tested on biomedical texts with results showing a significant improvement with respect to considered baselines, which demonstrates its ability to capture both reliability of parses and domain– specificity of linguistic constructions. 1 Introduction As firstly demonstrated by (Gildea, 2001), parsing systems have a drop of accuracy when tested against domain corpora outside of the data from which they were trained. This is a real problem in the biomedical domain where, due to the rapidly expanding body of biomedical literature, the need for increasingly sophisticated and efficient biomedical text mining systems is becoming more and more pressing. In particular, the existence of natural language parsers reliably dealing with biomedical texts represents the prerequiste for identifying and extracting knowledge embedded in them. Over the last years, this problem has been tackled with"
W13-1906,W07-2416,0,0.0118006,"acts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHEM) and biomedical abstracts (BIO), made of 5,001 tokens (195 sentences) and of 5,017 tokens (200 sentences) respectively. The dependency– based version of Genia includes ∼493k tokens and ∼18k sentences which was generated by converting the PTB version of Genia created by Illes Solt1 using the (Johansson & Nugues, 2007) tool with the -conll2007 option to produce annotations in line with the CoNLL 2007 data set2 . As unlabelled data, we used the datasets distributed in the framework of the CoNLL 2007 Domain Adaptation Track. For CHEM the set of unlabelled data consists of 10,482,247 tokens (396,128 sentences) and for BIO of 9,776,890 tokens (375,421 sentences). For the experiments using Genia as test set, we used the BIO unlabelled data. This was possible due to the fact that both the Genia Treebank and the BIO dataset consist of biomedical abstracts extracted (though using different query terms) from PubMed."
W13-1906,I08-2097,0,0.0182218,"these approaches, the last few years have seen a growing interest in self–training for domain adaptation, i.e. a method for using automatically annotated data from a target domain when training supervised models. Self–training methods proposed so far mainly differ at the level of the selection of parse trees to be added to the in–domain gold trees as further training data. Depending on whether or not external supervised classifiers are used to select the parses to be added to the gold–training set, two types of methods are envisaged in the literature. The first is the case, among others, of: (Kawahara & Uchimoto, 2008), using a machine learning classifier to predict the reliability of parses on the basis of different feature types; or (Sagae & Tsujii, 2007), selecting 45 Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 45–53, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics After introducing the in– and out–domain corpora used in this study (Section 2), we discuss the results of the multi–level linguistic analysis of these corpora carried out (Section 3) with a view to identifying the main features differentiating the biomedica"
W13-1906,I05-1006,0,0.016894,"ain Felice Dell’Orletta, Giulia Venturi, Simonetta Montemagni Istituto di Linguistica Computazionale “Antonio Zampolli” (ILC–CNR) Via G. Moruzzi, 1 – Pisa (Italy) {felice.dellorletta,giulia.venturi,simonetta.montemagni}@ilc.cnr.it Abstract (PTB) (Mitchell P. Marcus & Santorini, 1993) in order to carry out an inter–domain analysis of the typology of errors made by each parser and demonstrated that by integrating the output of the three parsers they achieved statistically significant performance gains. Three different methods of parser adaptation for the biomedical domain have been proposed by (Lease & Charniak, 2005) who, starting from the results of unknown word rate experiments carried out on the Genia treebank, adapted a PTB–trained parser by improving the Part–Of–Speech tagging accuracy and by relying on an external domain–specific lexicon. More recently, (McClosky, Charniak, & Johnson, 2010) and (Plank & van Noord, 2011) devised adaptation methods based on domain similarity measures. In particular, both of them adopted lexical similarity measures to automatically select from an annotated collection of texts those training data which is more relevant, i.e. lexically closer, to adapt the parser to the"
W13-1906,P08-2026,0,0.0180675,"t parsing models trained on the same dataset; or (McClosky, Charniak, & Johnson, 2006), using a discriminative reranker against the output of a n–best generative parser for selecting the best parse for each sentence to be used as further training data. Yet, due to the fact that several supervised classifiers are resorted to for improving the base supervised parser, this class of methods cannot be seen as a genuine istance of self–training. The second type of methods is exemplified, among others, by (Reichart & Rappoport, 2007) who use the whole set of automatically analyzed sentences, and by (McClosky & Charniak, 2008) and (Sagae, 2010) who add different amounts of automatically parsed data without any selection strategy. Note that (McClosky & Charniak, 2008) tested their self–training approach on the Genia Treebank: they self–trained a PTB–trained costituency parser using a random selection of Medline abstracts. 2 Corpora Used domain corpora include i) the two out– domain datasets used for the “Domain Adaptation Track” of the CoNLL 2007 Shared Task (Nivre et al., 2007) and ii) the dependency–based version of the Genia Treebank (Tateisi et al., 2005). The CoNLL 2007 datasets are represented by chemical (CHE"
W14-0406,lenci-etal-2012-lexit,1,0.800962,"ion of a dictionary”.17 The project goal is to study the combinatory properties of Italian words by developing advanced computational linguistics methods for extracting ` distributional information from PAISA. In particular, CombiNet uses a pattern-based approach to extract a wide range of multiword expressions, such as phrasal lexemes, collocations, and usual combinations. POS n-grams ` and then are automatically extracted from PAISA, ranked according to different types of association measures (e.g., pointwise mutual information, log-likelihood ratios, etc.). Extending the LexIt methodology (Lenci et al., 2012), CombiNet also extracts distributional profiles from the parsed ` including the following types of layer of PAISA, information: Figure 1: Dependency diagram Targeted at novice language learners of Italian, a filter for automatically restricting search results to sentences of limited complexity has been integrated into each search component. When activated, search results are automatically filtered based on a combination of the complexity measures introduced in section 4.3. 5.3 Technical details 1. syntactic slots (subject, complements, modiThe PAISA` online interface has been developed in sev"
W14-0406,W06-2920,0,0.0317161,"minated using a black-list that had been manually populated following inspection of earlier corpus versions. 4.1.2 Table 1: Average text length by source In September 2009, the Wikimedia Foundation decided to release the content of their wikis under CC BY-SA8 , so we decided to download the large and varied amount of texts made available through the Italian versions of these websites. This was done using the Wikipedia Extractor9 on official dumps10 of Wikipedia, Wikinews, Wikisource, Wikibooks, Wikiversity and Wikivoyage. The annotated corpus adheres to the standard CoNLL column-based format (Buchholz and Marsi, 2006), is encoded in UTF-8. 4 Corpus Creation 4.1 Collecting and cleaning web data The web pages for PAISA` were selected in two ways: part of the corpus collection was made through CC-focused web crawling, and another part through a targeted collection of documents from specific websites. 4.1.1 Targeted 4.2 Linguistic annotation and tools adaptation The corpus was automatically annotated with lemma, part-of-speech and dependency information, using state-of-the-art annotation tools for Italian. Part-of-speech tagging was performed with the Part-Of-Speech tagger described in Dell’Orletta (2009) and"
W14-0406,P06-1043,0,0.104212,"Missing"
W14-0406,rehm-etal-2008-towards,0,0.0276103,"ikipedia.org/wiki/ Vocabolario_di_alto_uso), together with high frequent function words not contained in those two lists. We used this domain adaptation approach for 39 pecially for the harvested14 subcorpus that was downloaded as described in section 4.1. We therefore carried out some experiments with the ultimate aim to enrich the corpus with metadata about text genre, topic and function, using automated techniques. In order to gain some insights into the com` we first conducted some manposition of PAISA, ual investigations. Drawing on existing literature on web genres (e.g. (Santini, 2005; Rehm et al., 2008; Santini et al., 2010)) and text classification according to text function and topic (e.g. (Sharoff, 2006)), we developed a tentative three-fold taxonomy to be used for text classification. Following four cycles of sample manual annotation by three annotators, categories were adjusted in order to ` web documents better reflect the nature of PAISA’s (cf. (Sharoff, 2010) about differences between domains covered in the BNC and in the web-derived ukWaC). Details about the taxonomy are provided in Borghetti et al. (2011). Then, we started to cross-check whether the devised taxonomy was ` composii"
W14-0406,W11-0314,1,0.903945,"Missing"
W14-0406,W13-1906,1,0.877398,"Missing"
W14-0406,J03-3001,0,0.0172091,"lity, incrementally augmented with new texts and annotation metadata for intelligent indexing and browsing. These requirements brought us to design a resource that was (1) freely available and freely re-publishable, (2) comprehensively covering contemporary common language and cultural content and (3) enhanced with a rich set of automaticallyannotated linguistic information to enable advanced querying and retrieving of data. On top 2 Related Work The world wide web, with its inexhaustible amount of natural language data, has become an established source for efficiently building large corpora (Kilgarriff and Grefenstette, 2003). Tools are available that make it convenient to bootstrap corpora from the web based on mere seed term lists, such as the BootCaT toolkit (Baroni and Bernardini, 2004). The huge corpora created by the WaCky project (Baroni et al., 2009) are an example of such an approach. A large number of papers have recently been published on the harvesting, cleaning and processing of web corpora.1 However, freely available, large, contemporary, linguistically annotated, easily accessible web corpora are still missing for many languages; but cf. e.g. (G´en´ereux et al., 2012) and the Common Crawl Foundation"
W14-1820,W11-2308,1,0.895607,"Missing"
W14-1820,W10-1001,0,0.0901504,"Missing"
W14-1820,W13-1704,0,0.0250712,"rimental setting and discuss achieved results. 2 Besides readability, sentence–based analyses are reported in the literature for related tasks: for instance, in a text simplification scenario by Drnˇ darevi´c et al. (2013), Alu´ısio et al. (2008), Stajner and Saggion (2013) and Barlacchi and Tonelli (2013); or to predict writing quality level by Louis and Nenkova (2013). Sheikha and Inkpen (2012) report the results of both document– and sentence– based classification in the different but related task of assessing formal vs. informal style of a document/sentence. For students learning English, Andersen et al. (2013) made a self–assessment and tutoring system available which was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features"
W14-1820,W14-1213,0,0.0195313,"Missing"
W14-1820,W06-2922,0,0.12205,"Missing"
W14-1820,C10-2032,0,0.0326186,"es in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with respect to readability levels. In both c"
W14-1820,J08-1001,0,0.0376934,"lity assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸"
W14-1820,D12-1043,0,0.0276398,"Missing"
W14-1820,W11-1603,0,0.0304112,".italianlp.it {name.surname}@ilc.cnr.it • Department of Humanities Computing, University of Groningen, The Netherlands ◦ Department of Quantitative Linguistics, University of T¨ubingen, Germany wieling@gmail.com Abstract sentences. Yet, for specific applications, assessing the readability level of individual sentences would be desirable. This is the case, for instance, for text simplification: in current approaches, text readability is typically assessed with respect to the entire document, while text simplification is carried out at the sentence level, as e.g. done in Alu´ısio et al. (2010), Bott and Saggion (2011) and Inui et al. (2003). By decoupling the readability assessment and simplification processes, the impact of simplification operations on the overall readability level of a given text may not always be clear. With sentence–based readability assessment, this is expected to be no longer a problem. Sentence readability assessment thus represents an open issue in the literature which is worth being further explored. To our knowledge, the only attempts in this direction are represented by Dell’Orletta et al. (2011) and Sj¨oholm (2012) for the Italian and Swedish languages respectively, followed mo"
W14-1820,N07-1058,0,0.0351889,"re for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features ranging across different linguistic levels. Starting from Schwarm and Ostendorf (2005) and Heilman et al. (2007), the role of syntactic Background In spite of the acknowledged need of performing readability assessment at the sentence level, so far very few attempts have been made to systematically investigate the issues and challenges concerned with the readability assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homo"
W14-1820,W11-2708,0,0.0598665,"Missing"
W14-1820,W03-1602,0,0.0253891,"@ilc.cnr.it • Department of Humanities Computing, University of Groningen, The Netherlands ◦ Department of Quantitative Linguistics, University of T¨ubingen, Germany wieling@gmail.com Abstract sentences. Yet, for specific applications, assessing the readability level of individual sentences would be desirable. This is the case, for instance, for text simplification: in current approaches, text readability is typically assessed with respect to the entire document, while text simplification is carried out at the sentence level, as e.g. done in Alu´ısio et al. (2010), Bott and Saggion (2011) and Inui et al. (2003). By decoupling the readability assessment and simplification processes, the impact of simplification operations on the overall readability level of a given text may not always be clear. With sentence–based readability assessment, this is expected to be no longer a problem. Sentence readability assessment thus represents an open issue in the literature which is worth being further explored. To our knowledge, the only attempts in this direction are represented by Dell’Orletta et al. (2011) and Sj¨oholm (2012) for the Italian and Swedish languages respectively, followed more recently by Vajjala"
W14-1820,I13-1043,0,0.0218337,"dressed in this study is represented by people characterised by low literacy skills and/or by mild cognitive impairment. The paper is organized as follows: Section 2 describes the background literature, Section 3 introduces our approach to the task, in terms of used corpora, features and learning algorithm. Finally, Sections 4 and 5 describe the experimental setting and discuss achieved results. 2 Besides readability, sentence–based analyses are reported in the literature for related tasks: for instance, in a text simplification scenario by Drnˇ darevi´c et al. (2013), Alu´ısio et al. (2008), Stajner and Saggion (2013) and Barlacchi and Tonelli (2013); or to predict writing quality level by Louis and Nenkova (2013). Sheikha and Inkpen (2012) report the results of both document– and sentence– based classification in the different but related task of assessing formal vs. informal style of a document/sentence. For students learning English, Andersen et al. (2013) made a self–assessment and tutoring system available which was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous r"
W14-1820,D08-1020,0,0.0330528,"(as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with"
W14-1820,P05-1065,0,0.0668192,"was able to assign a quality score for each individual sentence they write: this provides automated feedback on learners’ writing. A further important issue, largely investigated in previous readability assessment studies, is the identification of linguistic factors playing a role in assessing the readability of documents. If traditional readability metrics (see e.g., Kincaid et al. (1975)) typically rely on raw text characteristics, such as word and sentence length, the new NLP–based readability indices exploit wider sets of features ranging across different linguistic levels. Starting from Schwarm and Ostendorf (2005) and Heilman et al. (2007), the role of syntactic Background In spite of the acknowledged need of performing readability assessment at the sentence level, so far very few attempts have been made to systematically investigate the issues and challenges concerned with the readability assessment of sentences (as opposed to documents). The first two studies in this direction focused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal c"
W14-1820,W10-1007,0,0.0180699,"arsing and statistical language modeling) to capture highly complex linguistic features, and used statistical machine learning to build readability assessment tools. A variety of different NLP–based approaches has been proposed so far in the literature, differing at the level of the number of identified readability classes, the typology of features taken into account, the intended audience of the texts under evaluation, or the application within which readability assessment is carried out, etc. Research focused so far on readability assessment at the document level. However, as pointed out by Skory and Eskenazi (2010), methods developed perform well when the task is characterizing the readability level of an entire document, while they are unreliable for short texts, including single In this paper, we tackle the challenge of assessing the readability of individual sentences as a first step towards text simplification. The task is modelled as a classification task, with the final aim of shedding light on two open issues connected with it, namely the reference corpora to be used for training (i.e. collections of sentences classified according to their readability level), and the identification of the most ef"
W14-1820,W12-2206,0,0.0119548,"cused on languages other than English, namely Italian (Dell’Orletta 164 sider all sentences of 2Par as easy–to–read, not all Rep sentences are expected to be difficult–to–read. From this it follows that whereas the internal composition of 2Par is homogeneous at the sentence level, this is not the case for Rep. features in this task was considered, and more recently, the role of discourse features (e.g., discourse topic, discourse cohesion and coherence) has also been taken into account (see e.g., Barzilay and Lapata (2008), Pitler and Nenkova (2008), Kate et al. (2010), Feng et al. (2010) and Tonelli et al. (2012)). Many of these studies also explored the usefulness of features belonging to individual levels of linguistic description in predicting text readability. For example, Feng et al. (2010) systematically evaluated a wide range of features and compared the results of different statistical classifiers trained on different classes of features. Similarly, the correlation between level–specific features has been calculated by Pitler and Nenkova (2008) with respect to human readability judgments, and by Franc¸ois and Fairon (2012) with respect to readability levels. In both cases, the classes of featu"
W14-1820,D07-1013,0,\N,Missing
W14-1820,W05-0509,1,\N,Missing
W14-1820,C10-1062,0,\N,Missing
W14-1820,P11-2034,0,\N,Missing
W14-1820,W13-5608,0,\N,Missing
W15-1604,W06-2922,0,0.0136682,"h linguistic feature, we calculated the Spearman’s correlation between the feature values extracted from the original text and from the simplified version with respect to the selected rules. 5.1 Linguistic Features The set of linguistic features spans across different levels of linguistic analysis and are broadly classifiable into four main classes: raw text, lexical, morpho–syntactic and syntactic features, shortly described below. They were extracted from the corpora automatically tagged by the part–of–speech tagger described in Dell’Orletta (2009) and dependency– parsed by the DeSR parser (Attardi, 2006). Raw text features (Features [1–2] in Table 4) are typically used within traditional readability metrics and include sentence length (average number of words per sentence), and word length (average number of characters per words). Feature [3] refers to the percentage of all unique words (types) on the Basic Italian Vocabulary (BIV) by De Mauro (2000) in the sentence. The BIV includes a list of 7,000 words highly familiar to Italian native speakers. The set of morpho–syntactic features [4–19] ranges from the probability distribution of part–of– speech types, to the lexical density of the text,"
W15-1604,W14-1206,0,0.14631,"Missing"
W15-1604,C96-2183,0,0.770362,"ion extraction. Recently, ATS has been used in educational scenarios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g"
W15-1604,W11-2308,1,0.866329,"Missing"
W15-1604,W01-0521,0,0.340491,"Missing"
W15-1604,W03-1602,0,0.251637,"Missing"
W15-1604,P14-1041,0,0.0369706,"tly, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs; on the other side, data-driven approaches require the least linguistic knowledge but they are not feasible 31 Proceedings of LAW IX - The 9th Linguistic Annotation Workshop, pages 31–41, c Denver, Colorado, June 5, 2015. 2015 Ass"
W15-1604,W10-4213,0,0.0289342,"educational scenarios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who base"
W15-1604,W11-2802,0,0.0496931,"ios and assistive technologies; e.g. for the adaptation of texts to particular readers, like children (De Belder et al., 2010), L2 learners (Petersen and Ostendorf, 2007), people with low literacy skills (Alu´ısio et al., 2008), cognitive disabilities (Bott and Saggion, The purpose of ATS, within both perspectives, is to reduce lexical and syntactic complexity while preserving the original meaning of the text. To this aim, three main approaches have been followed. The more traditional one relies on the use of hand-crafted rules (Chandrasekar et al., 1996; Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system o"
W15-1604,D11-1038,0,0.125334,"Siddharthan, 2002; Siddharthan, 2010; Siddharthan, 2011), which typically cover specific phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs;"
W15-1604,C10-1152,0,0.210003,"c phenomena that are symptoms of linguistic complexity, especially at the syntactic level (e.g. passives, relative clauses, appositions). Recently, the availability of larger parallel corpora, i.e. sentence-aligned corpora consisting of both the original and the simplified version of the same text (e.g. English and Simple English Wikipedia, in short EW and SEW), has allowed a consistent use of machine learning techniques for automatically acquiring simplification rules. This is the approach followed by e.g. Woodsend and Lapata (2011), who based their ATS system on a quasi-synchronous grammar, Zhu et al. (2010), who adapted a Statistical Machine Translation (SMT) algorithm to implement simplification operations on the parse tree, and Narayan and Gardent (2014), who similarly adopted SMT techniques but also combined a deep semantic representation of the sentence. Both hand-written and automatically acquired rules have advantages and shortcomings. While the former can potentially account for the maximum linguistic information, they are extremely costly to develop and tend to cover only a few lexical and syntactic constructs; on the other side, data-driven approaches require the least linguistic knowle"
W15-1604,E14-1076,0,\N,Missing
W15-2618,W14-1207,0,0.0166904,"syntactic features that can be extracted from the output of a syntactic parser. 2.2 Readability metrics developed so far typically deal with English, with few attempts tackling other languages. The most prominent exception is represented by Swedish, for which a quantitative corpus analysis of a collection of radiology reports was carried out as a preliminary step towards the development of a Swedish text simplification tool (Kvist and Velupillai, 2013). Similarly to English, simplification algorithms for Swedish health–related documents were devised by relying on synonym replacement methods (Abrahamsson et al., 2014), or on automatic detection of out– of–dictionary words and abbreviations, or on compound splitting and spelling correction (Grigonyte et al., 2014). Initiatives carried out so far for what concerns Italian are based on traditional readability formulas. This is the case of the ETHIC (Evaluation Tool of Health Information for Consumers) project (Cocchi et al., 2014), aimed at developing an effective tool for biomedical librarians and health information professionals to assess the quality of produced documents and to support them in preparing texts of increasing quality, suitable and comprehensi"
W15-2618,W06-2922,0,0.0987725,"us as the “Italian Informed Consent Corpus” (2IC). Table 1 reports a selection of linguistic features which turned out to strongly characterize the 2IC corpus with respect to the journalistic 2Par and Rep corpora. This analysis is meant to compare domain–specific (i.e. biomedical) and general purpose corpora with the final aim of detecting the main linguistic features characterizing the language used in informed consent forms. The features were extracted from the corpus automatically tagged by the part–of–speech tagger described in Dell’Orletta (2009) and dependency–parsed by the DeSR parser (Attardi, 2006). Starting from raw textual features, it can be noticed that the 2IC corpus is characterized by shorter sentences (calculated as the average number of words per sentence) and longer words (calculated as the average number of characters per word) if compared with the 2Par and Rep corpora. Starting from the assumption underlying traditional readability formulas assuming that longer Consider now the distribution of Parts–Of– Speech across the 2Par, Rep and 2IC corpora. In135 text more difficult–to–read and need to be simplified, as suggested in Barlacchi and Tonelli (2013). formed consents are ch"
W15-2618,W12-2207,0,0.0731553,"Missing"
W15-2618,J93-2001,0,0.279139,") and longer words (calculated as the average number of characters per word) if compared with the 2Par and Rep corpora. Starting from the assumption underlying traditional readability formulas assuming that longer Consider now the distribution of Parts–Of– Speech across the 2Par, Rep and 2IC corpora. In135 text more difficult–to–read and need to be simplified, as suggested in Barlacchi and Tonelli (2013). formed consents are characterized by a high percentage of adjectives, prepositions and nouns, and by a low percentage of verbs: this gives rise to a much higher noun/verb ratio. According to Biber (1993), such different distributions represent significant dimensions of variation across textual genres. In particular, the higher noun/verb ratio reveals that informed consent forms are more informative than newspaper articles (Biber and Conrad, 2009), while the higher occurrence of nouns and prepositions is strongly connected with their presence within embedded complement ‘chains’ governed by a nominal head and including either prepositional complements or nominal and adjectival modifiers. Similarly to Rep articles, health– related documents contain a high percentage of complex nominal constructi"
W15-2618,W11-2308,1,0.925609,"Missing"
W15-2618,W14-1820,1,0.885768,"Missing"
W15-2618,W14-1209,0,0.0662015,"Missing"
W15-2618,J93-2004,0,\N,Missing
W15-2618,dellorletta-etal-2014-t2k,1,\N,Missing
W17-5049,W17-5007,0,0.601724,"has been a growing interest in the NLP–based Native Language Identification (henceforth, NLI) task. However, so far, due to the unavailability of balanced and wide–coverage benchmark corpora and the lack of evaluation standards it has been difficult to compare the results achieved for this task with different methods and techniques (Tetreault et al., 2012). The First Shared Task on Native Language Identification (Tetreault et al., 2013) was the answer to these mentioned problems. In this paper, we describe our approach to the essay track of the 2017 Native Language Identification Shared Task (Malmasi et al., 2017). Participating teams of this task were asked to classify the native language of writers of 1,100 En430 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 430–437 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work type. The main novelty of our approach is the use of a stack of two SVM classifiers, each one operating on morpho–syntactically tagged and dependency parsed texts. The first classifier is a L1 sentence classifier that is aimed at classifying the native language of each sentence of"
W17-5049,W13-1727,1,0.937576,"Missing"
W17-5049,W13-1706,0,0.641115,"the native language (L1) of a writer based on their writing in another language. Since the seminal work by Koppel et al. (2005), within the Computational Linguistics community there has been a growing interest in the NLP–based Native Language Identification (henceforth, NLI) task. However, so far, due to the unavailability of balanced and wide–coverage benchmark corpora and the lack of evaluation standards it has been difficult to compare the results achieved for this task with different methods and techniques (Tetreault et al., 2012). The First Shared Task on Native Language Identification (Tetreault et al., 2013) was the answer to these mentioned problems. In this paper, we describe our approach to the essay track of the 2017 Native Language Identification Shared Task (Malmasi et al., 2017). Participating teams of this task were asked to classify the native language of writers of 1,100 En430 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 430–437 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work type. The main novelty of our approach is the use of a stack of two SVM classifiers, each one operat"
W17-5049,C12-1158,0,0.489662,"18. 1 Introduction Native Language Identification (NLI) is the task of identifying the native language (L1) of a writer based on their writing in another language. Since the seminal work by Koppel et al. (2005), within the Computational Linguistics community there has been a growing interest in the NLP–based Native Language Identification (henceforth, NLI) task. However, so far, due to the unavailability of balanced and wide–coverage benchmark corpora and the lack of evaluation standards it has been difficult to compare the results achieved for this task with different methods and techniques (Tetreault et al., 2012). The First Shared Task on Native Language Identification (Tetreault et al., 2013) was the answer to these mentioned problems. In this paper, we describe our approach to the essay track of the 2017 Native Language Identification Shared Task (Malmasi et al., 2017). Participating teams of this task were asked to classify the native language of writers of 1,100 En430 Proceedings of the 12th Workshop on Innovative Use of NLP for Building Educational Applications, pages 430–437 c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics 2 Related Work type. The main nov"
W17-5049,W12-5812,1,0.815576,"Missing"
W17-5049,W14-1820,1,0.897234,"Missing"
W17-5049,D14-1142,0,0.634654,"l., 2009). Native Language Identification is most commonly tackled as a multi-class supervised classification task combining NLP–enabled feature extraction and machine learning: see e.g. (Tetreault et al., 2012), and (Malmasi and Dras, 2017). Among the different machine learning algorithms used, systems based on Support Vector Machines obtain the best accuracies. However, the most successful approaches made use of classifier ensemble methods to further improve performance. All recent state-of-the-art systems have relied on some form of multiple classifier system. Among the most recent works, (Ionescu et al., 2014) used multiple string kernels learning using only character ngram features, reporting an accuracy of 85.3 on the TOEFL11 test set, 1.7 higher than the 2013 state of the art obtained by (Jarvis et al., 2013) in the first shared task on NLI (Tetreault et al., 2013). More recently, (Malmasi and Dras, 2017) made a systematic examination of ensemble methods. By exploiting a classifier stacking architecture, the authors obtained the current state-of-the-art results on three datasets from different languages. As in these previous works, the system presented in this paper uses a stcked architecture, b"
W17-5049,W13-1714,0,0.727631,"012), and (Malmasi and Dras, 2017). Among the different machine learning algorithms used, systems based on Support Vector Machines obtain the best accuracies. However, the most successful approaches made use of classifier ensemble methods to further improve performance. All recent state-of-the-art systems have relied on some form of multiple classifier system. Among the most recent works, (Ionescu et al., 2014) used multiple string kernels learning using only character ngram features, reporting an accuracy of 85.3 on the TOEFL11 test set, 1.7 higher than the 2013 state of the art obtained by (Jarvis et al., 2013) in the first shared task on NLI (Tetreault et al., 2013). More recently, (Malmasi and Dras, 2017) made a systematic examination of ensemble methods. By exploiting a classifier stacking architecture, the authors obtained the current state-of-the-art results on three datasets from different languages. As in these previous works, the system presented in this paper uses a stcked architecture, but differently from the previous ones combines a sentence and a document classifier and it is able to exploit in a profitable way both local sentence information and global document information. Typically,"
W17-6505,alicante-etal-2012-treebank,0,0.0101725,"freedom, defined as the distance between the actual dependency length of a sentence and its optimal dependency, is a source of complexity which can be inferred both from lower parsing performance and from a trend toward more fixed word orders over time. Comparing the accuracy of dependency parsing on dative alternations in English, German and Russian, Dakota et al. (2015) showed that the larger the number of possible alternative orders to parse the more training data is needed. The effect of data sparness on the automatic analysis of free word order language was also assessed in the study of Alicante et al. (2012) aimed at comparing the performance of constituency and dependency parsing on an Italian treebank. In this paper we want to focus the attention on word order variation from a less–investigated perspective, aimed at assessing the influence of textual genre and linguistic complexity on the preservation of the basic (or unmarked) position of major constituents in the sentence, i.e. subject, object, adjective, adverb and subordinate clause. To this end, we carried out a corpus-based study for Italian – a Subject-Verb-Object (SVO) language – comparing the distribution of head–initial and head–final"
W17-6505,W15-1604,1,0.84417,"Missing"
W17-6505,W15-2112,0,0.0225773,"rted traditional typological classes with large quantitative data, but also provided evidence that a dominant order exists for languages left unspecified with respect to some grammatical relations (e.g. verbobject) in well-established classifications (Haspelmath et al., 2005). A similar methodology has been applied by (Liu, 2010), who conducted a comparative study based on 15 treebanks demonstrating that dependency direction is a reliable index to explain both the syntactic drift from Latin to Romance languages and to classify Romance languages as a distinct sub–group from other languages. In Futrell et al. (2015) a large cross– linguistic analysis was carried out using dependency treebanks for more than 30 languages; the comparative study allowed the authors to confirm the correlation between high order freedom and overt case–marking. Word order variation is generally investigated together with the effect it has on dependency distance, i.e. the distance between words and their parents, typically measured in terms of intervening words. With this respect, data from dependency annotated corpora highlight that, when two or more alternative orders are possible, languages tend to prefer the order that reduc"
W17-6505,W01-0521,0,0.0240769,"rrative Corpus Repubblica (Rep) DueParole (2Par) Educational materials for high-school (AduEdu) Educational materials for primary school (ChilEdu) Scientific articles on specialized topics (ScientArt) Wikipedia articles “Ecology and Environment” portal (WikiArt) Terence&Teacher-original versions (TT orig) Terence&Teacher-simplified versions (TT simp) Tokens 232,908 72,884 47,805 23,192 471,979 204,460 27,833 25,634 Table 1: The corpora used in the study. ever, it is well-known that the accuracy of parsers decreases when tested against texts of a different typology from those used in training (Gildea, 2001). Thus we can assume that the performance of DeSR will probably be worse in the analysis of texts representative of e.g. narrative and scientific writing. Despite this fact, we expect that the distributions of errors will be almost similar, at least when parsing texts of the same domain and language variety, thus allowing us to carry out a reliable internal comparison with respect to the examined syntactic patterns. In addition, the effect of genre variation on the performance of a general– purpose parser is likely to be less strong since all genres here considered contain standard texts, i.e."
W17-6505,W15-2115,0,0.0333086,"Missing"
W17-6505,P15-2078,0,0.0515568,"Missing"
W17-7624,W11-3405,0,0.27017,"rring in the same context, in this case surrounded by identical words”. This methodology has been recently reimplemented and extended by de Marneffe et al. (2017) to detect inconsistencies in the UD treebanks. The idea that the cases where two “parsers predict dependencies different from the gold standard” are “the most likely candidates when looking for errors” was experimented by Volokh and Neumann (2011), who trained two parsers based on completely different parsing algorithms to reproduce the training data (i.e. the Penn Treebank). A similar patternbased approach has been also proposed by Ambati et al. (2011) who complemented their method with a statistical module that, based on contextual features extracted from the Hindi treebank, was in charge of pruning previously identified candidate erroneous dependencies. If all the aforementioned methods exploit corpus-internal evidence to detect inconsistencies within a given treebank, van Noord (2004) and de Kok et al. (2009) use external resources, i.e. they rely on the analysis of large automatically parsed corpora external to the treebank under validation. The underlying idea of these error mining techniques is that sentences with a low parsability sc"
W17-7624,D16-1239,0,0.0410478,"Missing"
W17-7624,bosco-etal-2000-building,0,0.175459,"s corresponding to 325,816 tokens. As de Marneffe et al. (2017) pointed out, UD treebanks represent a good testing bed for error detection techniques: most part of them originate from a conversion process, often combined with merging and cross-corpus harmonization. In particular, IUDT results from the harmonization and merging of smaller dependency–based resources adopting incompatible annotation schemes into the Universal Dependencies annotation formalism, with the final aim of constructing a standard-compliant and bigger resource for the Italian language: the Turin University Treebank (TUT, Bosco et al. (2000)) and ISST–TANL (originating from the ISST corpus, (Montemagni et al., 2003)). For the specific concerns of this study, we focused on the section of IUDT containing newspaper articles, composed by 10,891 sentences, for a total of 154,784 tokens. This choice was aimed at avoiding possible interferences in detecting anomalies due to textual genre variation: in this case, “abnormal” relations do not only include possible errors but also constructions peculiar to a specific genre. The corpus used to collect the statistics to build the LISCA model is represented by the La Repubblica corpus, a colle"
W17-7624,W13-2308,1,0.908629,"Missing"
W17-7624,W09-2609,0,0.0572566,"Missing"
W17-7624,W17-6514,0,0.0341157,"Missing"
W17-7624,P10-1075,0,0.0154589,"contain a parsing error. This paper aims at testing the potential of algorithms developed to measure the reliability of automatically produced dependency relations for detecting erroneously annotated arcs in gold treebanks. In the literature, the result of this type of algorithms varies from a binary classification (correct vs. wrong) as in Che et al. (2014), to the ranking of dependencies on the basis of a quality score reflecting the reliability and plausibily of the automatic analysis (Dell’Orletta et al., 2013). Although these algorithms typically work on corpora automatically annotated (Dickinson, 2010), they have also been tested against corpora with manually revised (i.e. “gold”) annotation: in this case, the typical aim is the identification of errors or simply inconsistencies in the annotation (Dickinson, 2015). In this work, we used an algorithm ranking dependencies by reliability, LISCA (Dell’Orletta et al., 2013), that was applied to a gold treebank to limit the search space for bootstrapping error patterns, i.e. systematic recurring errors (as opposed to random errors). Identified error patterns were then projected against the whole corpus. Like Ambati et al. (2011), here error detec"
W17-7624,P05-1040,0,0.121261,"Missing"
W17-7624,C12-1055,0,0.0502853,"Missing"
W17-7624,nivre-etal-2006-maltparser,0,0.122382,"Missing"
W17-7624,L16-1680,0,0.0784285,"Missing"
W17-7624,P04-1057,0,0.144055,"Missing"
W17-7624,P11-2060,0,0.0268022,"n and Meurers (2003, 2005) and Boyd et al. (2008) proposed a variation n-gram detection method where the source of variation is the so-called variation nucleus, i.e. “a word which has different taggings despite occurring in the same context, in this case surrounded by identical words”. This methodology has been recently reimplemented and extended by de Marneffe et al. (2017) to detect inconsistencies in the UD treebanks. The idea that the cases where two “parsers predict dependencies different from the gold standard” are “the most likely candidates when looking for errors” was experimented by Volokh and Neumann (2011), who trained two parsers based on completely different parsing algorithms to reproduce the training data (i.e. the Penn Treebank). A similar patternbased approach has been also proposed by Ambati et al. (2011) who complemented their method with a statistical module that, based on contextual features extracted from the Hindi treebank, was in charge of pruning previously identified candidate erroneous dependencies. If all the aforementioned methods exploit corpus-internal evidence to detect inconsistencies within a given treebank, van Noord (2004) and de Kok et al. (2009) use external resources"
W17-7624,W03-3023,0,0.164278,"Missing"
W18-6001,W06-2922,0,0.113576,"Missing"
W18-6001,W13-2308,1,0.853195,".org/ 1 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 1–7 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics LISCA on the basis of the evidence acquired from the reference corpus) concentrate in the bottom groups of the ranking, the manual search of error patterns is restricted to the last groups. Detected anomalous annotations include both systematic and random errors. Systematic errors, formalized as error patterns, are looked for in the whole target corpus, matching contexts are manually revised and, if needed, corrected. (Bosco et al., 2013), mainly targets systematic errors, which represent potentially “dangerous” relations providing systematic but misleading evidence to a parser. Note that with systematic errors we refer here to both real errors as well as annotation inconsistencies internal to the treebank, whose origin can be traced back to different annotation guidelines underlying the source treebanks, or that are connected with substantial changes in the annotation guidelines (e.g. from version 1.4 to 2.0). This error detection methodology is based on an algorithm, LISCA (LInguiStically–driven Selection of Correct Arcs) (D"
W18-6001,P05-1040,0,0.0985159,"Missing"
W18-6001,W17-6514,0,0.037778,"Missing"
W18-6001,W11-3405,0,0.0640163,"Missing"
W18-6001,P13-2109,0,0.0231012,"Missing"
W18-6001,K17-3009,0,0.0352933,"Missing"
W18-6001,L18-1711,0,0.0415736,"Missing"
W19-4430,P18-1057,0,0.011823,"thod on different educational materials. We show how comparable results can be obtained considering only textual information. Table 1: Number of concepts, pairs, pairs showing a prerequisite relation for each domain of each dataset and total values considering all domains for each dataset. Another acknowledged limit of the above methods is the need of large annotated datasets. Manual annotation by domain experts is the most commonly adopted strategy to build such resources, regardless the knowledge unit considered (Wang et al., 2016; Pan et al., 2017; Liang et al., 2017; Alzetta et al., 2018; Fabbri et al., 2018), with the notable exception of the crowd-sourcing strategy of Talukdar and Cohen (2012). The dataset we present in this paper is the first dataset annotated with prerequisite relations between concepts for Italian build completely automatically. 3 3.1 Dataset For our experiments on the English language, we relied on the AL-CPL Dataset (Liang et al., 2018b), which is in turn based on the Wiki Concept Map dataset (Wang et al., 2016). The Wiki Concept Map dataset is a manually constructed dataset consisting of binary-labelled concept pairs collected from textbooks on different educational domain"
W19-4430,I17-1088,0,0.167518,"ction 5 we conclude the paper. such as curriculum planning (Agrawal, 2016), course sequencing (Vuong et al., 2011), reading list generation (Gordon et al., 2017), automatic assessment (Wang and Liu, 2016) and domain ontology construction (Zouaq et al., 2007; Larranaga et al., 2014). NLP techniques usually exploit structured information (e.g. hyperlinks, citations, DBpedia structure) combined with content-based information extracted from educational materials, like scientific literature (Gordon et al., 2016; Li et al., 2019), knowledge units in courses (Yang et al., 2015; Chaplot et al., 2016; Pan et al., 2017; Li et al., 2019) or Learning Objects (Gasparetti et al., 2018), often understood as Wikipedia pages (Gasparetti et al., 2015). Talukdar and Cohen (2012) presented the first work on predicting prerequisite structure of concepts using Wikipedia, which eventually became the most widely used resource for this task. They collected a manually annotated dataset of page pairs using crowd-sourcing and then trained a MaxEnt classifier using Wikipedia graph features, page content and edits to reproduce the prerequisite structure between pages. The classifier was tested both in and across domains, obtai"
W19-4430,W17-5029,0,0.0672046,"ampolli” (ILC-CNR), Pisa ItaliaNLP Lab – www.italianlp.it alessio.miaschi@phd.unipi.it, chiara.alzetta@edu.unige.it, {francoalberto.cardillo, felice.dellorletta}@ilc.cnr.it Abstract them to build educational materials such as textbooks, courses or, more in general, learning paths by combining various LOs of the same subject. Being able to give a pedagogical meaning to the content of a set of LOs by ordering them respecting their pedagogical precedence is not trivial: uncovering educational relationship between LOs is a difficult and time consuming practice usually performed by domain experts (Gordon et al., 2017). Among all pedagogical relations, the most fundamental is the prerequisite relation, which best describes pedagogical precedence since it defines what one needs to know before approaching a new content. Previous work in course and LO sequencing and knowledge tracing infers prerequisite relation between LOs based on their metadata and/or students’ preferences and competences (De-Marcos et al., 2009; Vuong et al., 2011; Piech et al., 2015; M´endez et al., 2016). Educational Data Mining methods usually rely also on graph information of ontologies, university programs or Wikipedia graph structure"
W19-4430,P16-1082,0,0.0137309,"analysis in Sec 3.2.1. Experiments, results and incremental training tests are described in Section 4. In Section 5 we conclude the paper. such as curriculum planning (Agrawal, 2016), course sequencing (Vuong et al., 2011), reading list generation (Gordon et al., 2017), automatic assessment (Wang and Liu, 2016) and domain ontology construction (Zouaq et al., 2007; Larranaga et al., 2014). NLP techniques usually exploit structured information (e.g. hyperlinks, citations, DBpedia structure) combined with content-based information extracted from educational materials, like scientific literature (Gordon et al., 2016; Li et al., 2019), knowledge units in courses (Yang et al., 2015; Chaplot et al., 2016; Pan et al., 2017; Li et al., 2019) or Learning Objects (Gasparetti et al., 2018), often understood as Wikipedia pages (Gasparetti et al., 2015). Talukdar and Cohen (2012) presented the first work on predicting prerequisite structure of concepts using Wikipedia, which eventually became the most widely used resource for this task. They collected a manually annotated dataset of page pairs using crowd-sourcing and then trained a MaxEnt classifier using Wikipedia graph features, page content and edits to reprod"
W19-4430,W12-2037,0,0.170104,"on et al., 2017), automatic assessment (Wang and Liu, 2016) and domain ontology construction (Zouaq et al., 2007; Larranaga et al., 2014). NLP techniques usually exploit structured information (e.g. hyperlinks, citations, DBpedia structure) combined with content-based information extracted from educational materials, like scientific literature (Gordon et al., 2016; Li et al., 2019), knowledge units in courses (Yang et al., 2015; Chaplot et al., 2016; Pan et al., 2017; Li et al., 2019) or Learning Objects (Gasparetti et al., 2018), often understood as Wikipedia pages (Gasparetti et al., 2015). Talukdar and Cohen (2012) presented the first work on predicting prerequisite structure of concepts using Wikipedia, which eventually became the most widely used resource for this task. They collected a manually annotated dataset of page pairs using crowd-sourcing and then trained a MaxEnt classifier using Wikipedia graph features, page content and edits to reproduce the prerequisite structure between pages. The classifier was tested both in and across domains, obtaining higher results in terms of accuracy if compared against a random baseline. The same dataset was used by (Liang et al., 2015) to test the RefD metric,"
W19-4430,D15-1193,0,0.0442059,"Missing"
