2021.fever-1.7,P17-1152,0,0.0374165,"of Computer Science and Information Engineering, National Central University, Taiwan ‡ Center for GIS, Research Center for Humanities and Social Sciences, Academia Sinica, Taiwan {inzugi0910, maple3142, thtsai}@g.ncu.edu.tw Abstract of the shared task, Nie et al. (2019) proposed a system consisting of three connected homogeneous networks of document retrieval, sentence selection, and claim verification. Yoneda et al. (2018) proposed a four-stage system that utilizes logistic regression models for the document retrieval and sentence retrieval stages, Enhanced Sequential Inference Model (ESIM) (Chen et al., 2017) for the natural language inference stage, and Multi-Layer Perceptron (MLP) for the aggregation stage. To explore the ability of automatic fact verification systems over both unstructured sentences and structured table-based information, Aly et al. (2021) introduces the Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS) dataset. The shared task in 2021 uses the FEVEROUS dataset and further requires a system to be able to retrieve structured information from Wikipedia as evidence for each claim, which differs from the shared task in 2018. However, these two"
2021.fever-1.7,N19-1423,0,0.0204491,"rily sentences and table cells, from the given 5.4M Wikipedia documents and verify as &quot;SUPPORTS&quot;, &quot;REFUTES&quot;, or &quot;NOT ENOUGH INFO&quot; for each given claim. Systems are evaluated by jointly considering how complete the relevant Wikipedia elements are retrieved and how correct the final verification verdicts are. In this paper, we propose a three-stage system as Figure 1 shows to improve the FEVEROUS baseline in two aspects. First, while the baseline retriever pays attention to literal relevance and word frequency with a combination method of entity matching and TF-IDF, we fine-tune the BERT model (Devlin et al., 2019) to integrate the conAutomatic fact verification has attracted recent research attention as the increasing dissemination of disinformation on social media platforms. The FEVEROUS shared task introduces a benchmark for fact verification, in which a system is challenged to verify the given claim using the extracted evidential elements from Wikipedia documents. In this paper, we propose our 3rd place three-stage system consisting of document retrieval, element retrieval, and verdict inference for the FEVEROUS shared task. By considering the context relevance in the fact extraction and verificatio"
2021.fever-1.7,2021.ccl-1.108,0,0.0425491,"Missing"
2021.fever-1.7,2020.acl-main.441,0,0.0110304,"ional resources. As a result, we set k = 5 for the downstream element retrieval using the BERT model and at the same time experiment with different settings for the downstream element retrieval using the Anserini. Verdict Inference On the third stage of the FEVEROUS shared task, NLI is a task that matches the scenario of classifying the semantic relationship between the claim and the retrieved elements as “SUPPORTS”, “REFUTES”, or “NOT ENOUGH INFO” (NEI). Therefore, we adopt the RoBERTa (Liu et al., 2019) NLI model pre-trained on well-known NLI datasets, including SNLI, MNLI, FEVER-NLI, ANLI (Nie et al., 2020), and experiment on its variants with aggregation method on top of it to fully utilize the semantic information of retrieved elements in the previous stage. For simplicity, we name our RoBERTa NLI model without aggregation as RoBERTa, RoBERTa NLI model with logical aggregation as RoBERTa-LOG, and RoBERTa NLI model with MLP for aggregation as RoBERTa-MLP. Both aggregation methods have been proved effective in 3.2 Element Retrieval Results Table 2 shows the development set results using our Anserini. The retriever of l = 5000 with prepending achieves better performance than the retrievers withou"
2021.fever-1.7,2020.emnlp-main.163,0,0.0173988,"Wikipedia documents. In this paper, we propose our 3rd place three-stage system consisting of document retrieval, element retrieval, and verdict inference for the FEVEROUS shared task. By considering the context relevance in the fact extraction and verification task, our system achieves 0.29 FEVEROUS score on the development set and 0.25 FEVEROUS score on the blind test set, both outperforming the FEVEROUS baseline. 1 Introduction The large-scale dissemination of disinformation on social media platforms intended to mislead or deceive the general population has become a major societal problem (Tan et al., 2020). For example, the widespread disinformation of the Covid-19 vaccine has caused a growth of anti-vaccination sentiment online and led to declining vaccination coverage. As the best way to stop disinformation from going viral online is early verification, recent researchers have put efforts into automatic fact verification systems. To answer the increasing demand for such systems, the FEVER (Fact Extraction and VERification) dataset (Thorne et al., 2018) was introduced and used for the shared task of the FEVER Workshop 2018. It consists of 185,445 annotated claims with a label of &quot;SUPPORTED&quot;, &quot;"
2021.fever-1.7,N18-1074,0,0.0258187,"semination of disinformation on social media platforms intended to mislead or deceive the general population has become a major societal problem (Tan et al., 2020). For example, the widespread disinformation of the Covid-19 vaccine has caused a growth of anti-vaccination sentiment online and led to declining vaccination coverage. As the best way to stop disinformation from going viral online is early verification, recent researchers have put efforts into automatic fact verification systems. To answer the increasing demand for such systems, the FEVER (Fact Extraction and VERification) dataset (Thorne et al., 2018) was introduced and used for the shared task of the FEVER Workshop 2018. It consists of 185,445 annotated claims with a label of &quot;SUPPORTED&quot;, &quot;REFUTED&quot;, or &quot;NOT ENOUGH INFO&quot; as well as sets of evidential sentences from the given pre-processed Wikipedia pages. Among the participated teams ∗ Corresponding author. 60 Proceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER) at EMNLP 2021, pages 60–65 November 10, 2021. ©2021 Association for Computational Linguistics text relevance for finding evidential elements and downstream verdict inference. Second, the baseline predictor"
C12-1092,P97-1048,0,0.114652,"Missing"
C12-1092,C10-1013,0,0.030576,"ue is always positive, which is defined as: PMI Exp (t , t ′) = e PMI (t , t ′) (3) 3.6.2 Association Score between the Opinion Word and Related Words in the Source Language We aim to determine the translation of an opinion word by scoring its association to its related words in the source language. Different related words have different influence on the target word, so added weighting factors are necessary. There are two factors that we consider: dependency distance and feature-opinion association. 1509 Distance weighting has been used in several studies (Beeferman, Berger, & Lafferty, 1997; Brosseau-Villeneuve, Nie, & Kando, 2010; Gao, Zhou, Nie, He, & Chen, 2002) as a means of estimating the association between two words. The exponential model, in which association between two words decreases exponentially when the distance between them increases, is a commonly used approach. We employ Beeferman et al. (1997)’s distance weighting approach. Therefore, the association score of o and si is defined as: AssociationS (o, si )= µ ⋅ e − µ ⋅ distance ( o , si ) (4) where distance(o, si) is the dependency distance (see Section 3.4 for the details) between o and si; and μ is the parameter for decay rate determined by maximum li"
C12-1092,I05-2021,0,0.022741,"n the international nature of the web and online shopping, opinions in a user’s mother language may not be available. Translation of online customer reviews is therefore an important service sought after in many markets. A crucial aspect of customer review translation is translating opinion words, key words that capture the sentiments. At present, machine translation (MT) systems can translate whole sentences or even complete paragraphs. This not a trivial task, however, because opinion words usually have multiple possible translations and the MT systems have low accuracy on polysemous words (Carpuat & Wu, 2005). This paper proposes an unsupervised method of selecting the most appropriate Chinese translation for an opinion word in a given Japanese sentence. Candidate translations are retrieved from a bilingual dictionary. Consider the following Japanese sentence: 綺麗な夜景とともに食事を楽しむことができました。 (I was able to enjoy a nice meal with a beautiful night view.) The target opinion word 綺麗 has three candidate translations: 漂亮 (beautiful), 乾淨 (clean), and 清楚 (clear) in Chinese. In this example, the most appropriate translation is 漂亮 (beautiful). This disambiguation problem is known as Word Translation Disambiguatio"
C12-1092,J90-1003,0,0.30287,"he extra influence of si’s multiple possible translations. In the next two parts, we will introduce the terms AssociationT (t, t&apos;) and AssociationS (o, si) in detail. 3.6.1 Association Score between Translations in the Target Language In this section, we describe how we estimate AssociationT (t, t&apos;). There are many ways to measure the correlation between two words: one simple way is to calculate the mutual information score in the corpus. First, the snippets described in Section 3.3 are split into segments using punctuation. Their similarity is estimated by Pointwise Mutual Information (PMI) (Church & Hanks, 1990), which is defined as: ′ PMI = = Association (t , t ′) log 2 T (t , t ) p (t , t ′) p (t ) ⋅ p (t ′) (2) where p(t) and p(t&apos;) are the probability of word t and t&apos; appearing separately in the corpus, and p(t, t&apos;) is the probability of the co-occurrence of word t and t&apos; in the corpus, which is estimated by the number of co-occurring segments for t and t&apos; divided by the total number of segments. However, using PMI does not yield the expected AssociationT (t, t&apos;) measurements, because the corpus is compiled from the search snippet results of sending all candidate-/related-translation pairs to Goog"
C12-1092,J94-4003,0,0.29537,"anslation, with a focus on the translation of ambiguous words. Unlike in other lexical sample tasks, the sense inventory for CL-WSD is the set of translations from a bilingual dictionary or a parallel corpus instead of human-defined sense labels. There have been several studies that use cross-lingual evidence to deal with the WSD problem: (Chan & Ng, 2005; Chklovski et al., 2004; Ng, Wang, & Chan, 2003). These approaches rely on large parallel corpora to train a WSD classifier. However, for some language pairs (e.g., Japanese-Chinese), such corpora are not available. To overcome this problem, Dagan and Itai (1994) used a bilingual lexicon and statistical data from a monolingual corpus of the target language for disambiguation. Tsunakawa and Kaji (2010) proposed a method for using a bilingual dictionary with a correlation matrix to select an appropriate translation word. An item in the matrix is the correlation score between associated words and candidate translations. The Web is increasingly being used as a data source in a wide range of natural language processing tasks including WSD. Liu and Zhao (2009) presented a fully unsupervised WTD method which selects the maximum sum of Web Bilingual Relatedne"
C12-1092,P03-1058,0,0.0144338,"eld by the Senseval/SemEval workshop (Chklovski, Mihalcea, Pedersen, & Purandare, 2004; Jin, Wu, & Yu, 2007; Lefever & Hoste, 2010). This task provides a framework for the evaluation of systems that perform machine translation, with a focus on the translation of ambiguous words. Unlike in other lexical sample tasks, the sense inventory for CL-WSD is the set of translations from a bilingual dictionary or a parallel corpus instead of human-defined sense labels. There have been several studies that use cross-lingual evidence to deal with the WSD problem: (Chan & Ng, 2005; Chklovski et al., 2004; Ng, Wang, & Chan, 2003). These approaches rely on large parallel corpora to train a WSD classifier. However, for some language pairs (e.g., Japanese-Chinese), such corpora are not available. To overcome this problem, Dagan and Itai (1994) used a bilingual lexicon and statistical data from a monolingual corpus of the target language for disambiguation. Tsunakawa and Kaji (2010) proposed a method for using a bilingual dictionary with a correlation matrix to select an appropriate translation word. An item in the matrix is the correlation score between associated words and candidate translations. The Web is increasingl"
C12-1092,W10-3205,0,0.0172797,"e set of translations from a bilingual dictionary or a parallel corpus instead of human-defined sense labels. There have been several studies that use cross-lingual evidence to deal with the WSD problem: (Chan & Ng, 2005; Chklovski et al., 2004; Ng, Wang, & Chan, 2003). These approaches rely on large parallel corpora to train a WSD classifier. However, for some language pairs (e.g., Japanese-Chinese), such corpora are not available. To overcome this problem, Dagan and Itai (1994) used a bilingual lexicon and statistical data from a monolingual corpus of the target language for disambiguation. Tsunakawa and Kaji (2010) proposed a method for using a bilingual dictionary with a correlation matrix to select an appropriate translation word. An item in the matrix is the correlation score between associated words and candidate translations. The Web is increasingly being used as a data source in a wide range of natural language processing tasks including WSD. Liu and Zhao (2009) presented a fully unsupervised WTD method which selects the maximum sum of Web Bilingual Relatedness (WBR) between a translation and all context words. The WBR is calculated by four association measures based on mixed-language webpage coun"
C12-1092,W04-0802,0,\N,Missing
C12-1092,S07-1004,0,\N,Missing
C12-1092,W02-2016,0,\N,Missing
C12-1092,W09-2413,0,\N,Missing
I08-1037,P02-1051,0,0.273485,"Missing"
I08-1037,P98-1036,0,0.110925,"ries cannot update their contents frequently. Therefore, it is necessary to construct a named entity translation (NET) system. Economic ties between China and Korea have become closer as China has opened its markets further, and demand for the latest news and information from China continues to grow rapidly in Korea. One key way to meet this demand is to retrieve information written in Chinese by using Korean queries, referred to as Korean-Chinese cross-language information retrieval (KCIR). The main challenge involves translating NEs because they are usually the main concepts of queries. In (Chen et al., 1998), the authors romanized Chinese NEs and selected their English transliterations from English NEs extracted from the Web by comparing their phonetic similarities with Chinese NEs. Yaser Al-Onaizan (Al-Onaizan and Knight, 2002) 281 transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora. Unlike the above works, whose target languages are alphabetic, in K-C translation, the target language is Chinese, which uses an ideographic writing system. Korean-Chinese NET is much more diﬃcult than NET considered in prev"
I08-1037,C98-1036,0,\N,Missing
I08-2134,O98-3002,0,0.0223534,"nd their lengths. Word match features are deﬁned as: WM(w = C−pos . . . C−pos+len−1 ) { 1 if w ∈ D = 0 otherwise where len ∈ [1..4] is w’s length and pos ∈ [0..len] is C0 ’s zero-based relative position in w (when pos = len, the previous len characters form a word found in D). If C0 is “會” and “討論會” is found in D, WM(C−2 . . . C0 ) is enabled. 6.1.2 Word Match with Word Frequency (WMWF) Given two diﬀerent words that have the same position and length, WM features cannot differentiate which should have the greater weight. This could cause problems when two matched words of same length overlap. (Chen and Bai, 1998) solved this conﬂict by selecting the word with higher (frequency × length). We utilize this idea to reform the WM features into our WMWF features: WMWFq (w = C−pos . . . C−pos+len−1 ) { 1 if w ∈ D and log feq(w) = q = 0 otherwise where the word frequency is discretized into 10 bins in a logarithmic scale:log feq(w) = min(⌈log2 w’s frequency + 1⌉, 10) thus q[0..10] is the discretized log frequency of w. In this formulation, matching words with higher log frequencies are more likely to be the correct segmentation. Following the above example, if the frequency of “討論會” is 15, then the feature WM"
I08-2134,J93-1003,0,0.145708,"The baseline model cannot identify 熊天平 xiong-tianping (a singer’s name) because 熊天平 is a lowST word and the morphological tendency of 天 平 is not consistent with the recorded one. In English, there is a similar phenomenon called multi-word expressions (MWEs). (Choueka, 1988) regarded MWE as connected collocations: a sequence of neighboring words “whose exact meaning cannot be derived from the meaning or connotation of its components”, which means that MWEs also have low ST. As some pioneers provide MWE identiﬁcation methods which are based on association metrics (AM), such as likelihood ratio (Dunning, 1993). The methods of identifying low-ST words can be divided into two: ﬁltering and merging. The former uses AM to measure the likelihood that a candidate is actually a whole word that cannot be divided. Candidates with AMs lower than the threshold are ﬁltered out. The latter strategy merges character segments in a bottom-up fashion. AMs are employed to suggest the next candidates for merging. Both methods suﬀer from two main drawbacks of AM: dependency on segment length and inability to use relational information between context and tags. In the ﬁrst case, applying AMs to ranking character segmen"
I08-2134,W03-1726,0,0.0428454,"Missing"
I08-2134,W02-1811,0,0.0325156,"g tasks in these languages. Chinese word segmentation (CWS) systems can be built by supervised learning from a labeled data set. However, labeled data sets are expensive to prepare as it involves manual annotation eﬀorts. Therefore, exploiting unlabeled data to improve CWS performance becomes an important research goal. In addition, new word identiﬁcation (NWI) is also very important because they represent the latest information, such as new product names. This paper explores methods of extracting information from both internal and external unlabeled data to augment NWI and CWS. According to (Tseng and Chen, 2002), new 931 where Zx is the normalization that makes the probability of all state sequences sum to one; fk (yt−1 , yt , x, t) is often a binary-valued feature function and λk is its weight. The feature functions can measure any aspect of a state transition, yt−1 → yt , and the entire observation sequence, x, centered at the current position, t. For example, one feature function might have value 1 when yt−1 is the state B, yt is the state I, and is the character “國”. Large positive values for λk indicate a preference for such an event; large negative values make the event unlikely. In our CRF mod"
I08-2134,W03-1728,0,0.0152274,"ent position, t. For example, one feature function might have value 1 when yt−1 is the state B, yt is the state I, and is the character “國”. Large positive values for λk indicate a preference for such an event; large negative values make the event unlikely. In our CRF model, each binary feature is multiplied with all states (yt ) or all state transitions (yt−1 yt ). For simplicity, we omit them in the following discussion. In addition, we use C0 rather than xt to denote the current character. 3 Baseline n-gram Features Character n-gram features have proven their effectiveness in ML-based CWS (Xue and Shen, 2003). We use 4 types of unigram feature functions: C0 , C1 (next character), C−1 (previous character), C−2 (character preceding C−1 ). Furthermore, 6 types of bigram features are used, and are designated here as conjunctions of the previously speciﬁed unigram features, C−2 C−1 , C−1 C0 , C0 C1 , C−3 C−1 , C−2 C0 , and C−1 C1 . 4 New Word Identiﬁcation We mainly focus on improving new word identiﬁcation (NWI) using unlabeled text. Words with high and low ST are discussed separately due to the disparity in their morphological characteristics. However, it is unnecessary for our system to classify wor"
I11-1095,N09-1018,0,0.0410004,"Missing"
I11-1095,N09-1037,0,0.0155525,"more easily. Unfortunately, a separate NILfiltering stage may filter out the entity mention “MIRL” because it is listed as an abbreviation of organization names, such as Mineral Industry Research Laboratory. With a joint inference process we can carry out both tasks simultaneously to avoid this type of error propagation (Poon and Domingos 2007). Joint inference has become popular recently, because they make it possible for features and constraints to be shared among tasks. For example, Che and Liu (2010) created a joint model for word sense disambiguation (WSD) and semantic role labeling, and Finkel and Manning (2009) integrated parsing and named entity recognition in a joint model. In this paper, we use the Markov Logic Network (MLN) (Richardson and Domingos 2006), a joint model which combines first order logic (FOL) and Markov networks, to unify the NIL-filtering and entity disambiguation stages. The model captures the contextual information of the recognized entities for entity disambiguation as well as the constraints when linking an entity mention to a KB entry—for example, an entity mention can only be linked to a database entry when the mention has not been recognized as an NIL. Another advantage of"
I11-1095,C10-1032,0,\N,Missing
I11-1095,E06-1002,0,\N,Missing
I11-1095,C10-1019,0,\N,Missing
I11-1095,C10-1145,0,\N,Missing
I17-3004,P16-1157,0,0.0423388,"Missing"
I17-3004,P14-2096,1,0.58714,"dia entity embedding (CEEE) feature, the construction of which is detailed in the following sections. Wang et al. (2012) attempted to integrate two different encyclopedias, English Wikipedia and Chinese Baidu Baike, into one cross-language encyclopedia. They created over 0.2 million links between the encyclopedias, but their approach requires many manually pre-linked article links and category links to create the pair-wise connectivity graph (PCG) model. Furthermore, in the paper, they do not mention how to verify accuracy of the newly discovered cross-language links. Another relevant work is Wang et al. (2014), which also focuses on linking English Wikipedia and Baidu Baike articles. To select and predict article links, they designed text-related features for an SVM classifier. Their features include bidirectional title matching, title similarity, hypernym translation and English title occurrence. 3 3.1 Cross-Encyclopedia Entity Embedding Model Our model is based on Mikolov et al.’s (2013) skip-gram model. The training objective of the skip-gram model is to maximize the probability of predicting the target word given the context, where the target-context pairs are extracted by sliding a window over"
I17-3004,N15-1104,0,0.0649201,"Missing"
I17-3005,S15-2048,0,0.0312721,", the system retrieves a set of relevant answers and ranks them. Moreover, our system uses a novel reranker to enhance the ranking result of information retrieval. It employs the word2vec model to represent the sentences as vectors. It also uses a sub-category feature, predicted by the knearest neighbor algorithm. Finally, the system returns the top five candidate answers, making online staffs find answers much more efficiently. 1 Currently, several studies have proved the effectiveness of incorporating word embedding features in answer ranking models (Zhou et al., 2015; Zhou and Huang, 2017; Belinkov et al., 2015; Tran et al., 2015). In our system, we derive our reranking model based on two SemEval-2016 studies (AlessandroMoschitti et al., 2016; Mihaylov and Nakov, 2016). We train a word embedding model to transform all questions into distributed vectors (Le and Mikolov, 2014; Dai et al., 2015). Due to every in-database question contains intention labels assigned by Chunghwa telecom customer service staffs, we train a multi-class classifier to identify the input questions intention. Finally, we design a reranking model using the word embedding features and the intention feature. Our system provides a"
I17-3005,S15-2038,0,0.0317342,"a set of relevant answers and ranks them. Moreover, our system uses a novel reranker to enhance the ranking result of information retrieval. It employs the word2vec model to represent the sentences as vectors. It also uses a sub-category feature, predicted by the knearest neighbor algorithm. Finally, the system returns the top five candidate answers, making online staffs find answers much more efficiently. 1 Currently, several studies have proved the effectiveness of incorporating word embedding features in answer ranking models (Zhou et al., 2015; Zhou and Huang, 2017; Belinkov et al., 2015; Tran et al., 2015). In our system, we derive our reranking model based on two SemEval-2016 studies (AlessandroMoschitti et al., 2016; Mihaylov and Nakov, 2016). We train a word embedding model to transform all questions into distributed vectors (Le and Mikolov, 2014; Dai et al., 2015). Due to every in-database question contains intention labels assigned by Chunghwa telecom customer service staffs, we train a multi-class classifier to identify the input questions intention. Finally, we design a reranking model using the word embedding features and the intention feature. Our system provides a web-based interface"
N06-3009,W03-1305,0,0.074184,"Missing"
N06-3009,J05-1004,0,0.0289506,"ally on those highly relevant to the biomedical domain, such as AM-LOC (location), the performance is not satisfactory. We therefore develop a template generation method to create templates that are used as features for identifying these argument types. 3.1 Biomedical Proposition Bank -- BioProp Our biomedical proposition bank, BioProp, is based on the GENIA Treebank (Yuka et al., 2005), which is a 491-abstract corpus annotated with syntactic structures. The semantic annotation in BioProp is added to the proper constituents in a syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to t"
N06-3009,C04-1197,0,0.0326272,"ed to the proper constituents in a syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check the system’s results. Therefore, human effort is greatly reduced. 3.2 Biomedical SRL System -- SEROW Following (Punyakanok et al., 2004), we formulate SRL as a constituent-by-constituent (C-by-C) tagging problem. We use BioProp to train our biomedical SRL system, SEROW (Tsai et al., 2006b), which uses a maximum entropy (ME) machinelearning model. We use the basic features described in (Xue & Palmer, 2004). In addition, we automatically generate templates which can be used to improve classification of biomedical argument types. The details of SEROW system are described in (Tsai et al., 2005) and (Tsai et al., 2006b). 3.3 Experiment and Summary Our experimental results show that a newswire English SRL system that achieves an F-s"
N06-3009,W05-0638,0,0.0274788,"Missing"
N06-3009,W04-3212,0,0.0647162,"Missing"
N06-3009,I05-2038,0,0.031722,"corpus, annotated with verbs and their corresponding semantic roles; (3) build an automatic semantic interpretation model, using the annotated text as a training corpus for machine learning. However, on adjunct arguments, especially on those highly relevant to the biomedical domain, such as AM-LOC (location), the performance is not satisfactory. We therefore develop a template generation method to create templates that are used as features for identifying these argument types. 3.1 Biomedical Proposition Bank -- BioProp Our biomedical proposition bank, BioProp, is based on the GENIA Treebank (Yuka et al., 2005), which is a 491-abstract corpus annotated with syntactic structures. The semantic annotation in BioProp is added to the proper constituents in a syntactic tree. Basically, we adopt the definitions in PropBank (Palmer et al., 2005). For the verbs not in PropBank, such as “phosphorylate”, we define their framesets. Since the annotation is time-consuming, we adopt a semi-automatic approach. We adapt an SRL system trained on PropBank (Wall Street Journal corpus) to the biomedical domain. We first use this SRL system to automatically annotate our corpus, and then human annotators to double check t"
N18-2054,P15-1125,0,0.0318564,"and b. Wang et al.’s (2014) features are as follows: • BM25: w’s title is translated into Chinese and then used as a query to retrieve articles from Baidu Baike with the Lucene search engine. The returned BM25 score corresponding to b is treated as the value of b’s BM25 feature. • Hypernym translation (HT): Supposing the given English title is e and that e’s hypernym is h, this feature is defined as the log frequency of h’s Chinese translation in the candidate Chinese article. Entity Embedding With recent work on word embedding, there has also been more interest in learning entity embedding. Hu et al. (2015) modeled Wikipedia’s category structure in their entity embedding. Li et al. (2016) extended Hu et al.’s (2015) work to include category embedding in addition to entity embedding. They further extended the model by integrating category structure to capture meaningful semantic relationships between entities and categories. Yamada et al. (2016) learned joint embedding for words and entities. Tsai and Roth (2016) proposed a way to learn multilingual embedding of words and entities. They first learned • English title occurrence (ETO): Whether or not w’s title appears in the first sentence of b is"
N18-2054,P14-2096,1,0.818774,"inks. They then translated English terms into Japanese and designed features based on edit distance. Bennacer et al. (2015) used BabelNet (Navigli and Ponzetto, 2012) to select candidate articles with similar semantics in the target language. They then ranked the candidates based on cross-language link similarity. Unlike the above studies, we aim to carry out CLAL between different encyclopedia platforms, English and Baidu Baike; therefore we cannot use existing inter-language links. Because much of the previous Wikipediaonly CLAL research cannot be directly applied to cross-platform linking, Wang et al. (2014) developed an SVM-based approach with content-similarity-based features to link articles in Wikipedia and Baidu Baike. In this work, they relied on Google Translate to translate Chinese terms into English. They then designed title-based and content-based features based on both the English and translated Chinese articles. 2.2 3 Methods Given an article from a knowledge base (KB), CLAL aims to find the article’s corresponding article in another KB of a different language. Corresponding articles are defined as articles describing the same entity in different languages. We base our CLAL system on"
N18-2054,P13-1062,0,0.0601433,"Missing"
N18-2054,N15-1104,0,0.0804005,"Missing"
N18-2054,C16-1252,0,0.0131513,"ed into Chinese and then used as a query to retrieve articles from Baidu Baike with the Lucene search engine. The returned BM25 score corresponding to b is treated as the value of b’s BM25 feature. • Hypernym translation (HT): Supposing the given English title is e and that e’s hypernym is h, this feature is defined as the log frequency of h’s Chinese translation in the candidate Chinese article. Entity Embedding With recent work on word embedding, there has also been more interest in learning entity embedding. Hu et al. (2015) modeled Wikipedia’s category structure in their entity embedding. Li et al. (2016) extended Hu et al.’s (2015) work to include category embedding in addition to entity embedding. They further extended the model by integrating category structure to capture meaningful semantic relationships between entities and categories. Yamada et al. (2016) learned joint embedding for words and entities. Tsai and Roth (2016) proposed a way to learn multilingual embedding of words and entities. They first learned • English title occurrence (ETO): Whether or not w’s title appears in the first sentence of b is regarded as the value of b’s ETO feature. After replicating Wang et al.’s (2014) sy"
N18-2054,K16-1025,0,0.0208182,"tle is e and that e’s hypernym is h, this feature is defined as the log frequency of h’s Chinese translation in the candidate Chinese article. Entity Embedding With recent work on word embedding, there has also been more interest in learning entity embedding. Hu et al. (2015) modeled Wikipedia’s category structure in their entity embedding. Li et al. (2016) extended Hu et al.’s (2015) work to include category embedding in addition to entity embedding. They further extended the model by integrating category structure to capture meaningful semantic relationships between entities and categories. Yamada et al. (2016) learned joint embedding for words and entities. Tsai and Roth (2016) proposed a way to learn multilingual embedding of words and entities. They first learned • English title occurrence (ETO): Whether or not w’s title appears in the first sentence of b is regarded as the value of b’s ETO feature. After replicating Wang et al.’s (2014) system, we add our proposed cross-encyclopedia entity embedding (CEEE) feature, the construction of which is detailed in the following sections. 335 3.1 Cross-Encyclopedia Entity Embedding Model entities tend to share common category labels, and so do cross-lingu"
O08-2010,P98-1036,0,0.0446998,"queries from Chinese to Korean for Chinese-Korean (C-K) information retrieval systems. The main challenge involves translating named entities (such as names of shows, movies and albums) because they are usually the main concepts of queries. Named entity (NE) translation is a challenging task because, although there are many online bilingual dictionaries, they usually lack domain speciﬁc words or NEs. Furthermore, new NEs are emerged everyday, but bilingual dictionaries cannot update their contents frequently. Therefore, it is necessary to construct a named entity translation (NET) system. In [2], the authors romanized Chinese NEs and selected their English transliterations from English NEs extracted from the Web by comparing their phonetic similarities with Chinese NEs. Yaser Al-Onaizan [3] transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora. Chinese-Korean NET is much more difﬁcult than NET considered in previous works because a Chinese NE may not have similar pronunciation to its Korean translation. In this paper, we propose an effective pattern-based NET method which can achieve very high"
O08-2010,P02-1051,0,0.0410436,"are usually the main concepts of queries. Named entity (NE) translation is a challenging task because, although there are many online bilingual dictionaries, they usually lack domain speciﬁc words or NEs. Furthermore, new NEs are emerged everyday, but bilingual dictionaries cannot update their contents frequently. Therefore, it is necessary to construct a named entity translation (NET) system. In [2], the authors romanized Chinese NEs and selected their English transliterations from English NEs extracted from the Web by comparing their phonetic similarities with Chinese NEs. Yaser Al-Onaizan [3] transliterated an NE in Arabic into several candidates in English and ranked the candidates by comparing their counts in several English corpora. Chinese-Korean NET is much more difﬁcult than NET considered in previous works because a Chinese NE may not have similar pronunciation to its Korean translation. In this paper, we propose an effective pattern-based NET method which can achieve very high accuracy. All patterns are automatically generated and weighed, saving considerable human effort. 2 Difﬁculties in Chinese-Korean Named Entity Translation To translate an NE originated from Chinese i"
O08-2010,I08-1037,1,0.217496,"a Korean drama’s name. The Top-1 candidate is “랑만만우” (rang-man-man-u), which is the transliteration of the Chinese characters “浪漫滿屋” based on the Mandarin pronunciation. However, the correct Korean translation is “풀하우스” (Full House), which is the transliteration based on the English pronunciation. These two queries show that different phraseology may rank the incorrect translation candidates higher. However, the correct translations, such as “청일정쟁” and “풀하우스”, are also extracted by our method. For some applications, such as CLIR, the inaccurate ranking does not inﬂuence the performance a lot [7]. 6 Conclusion In this paper, we have demonstrated several advantages of our pattern-based NE translation method. Our pattern-based method achieves higher recall because it extracts NE translations from the Web, which contains most of human knowledge. Even translations of novel NEs can be found. Second, our method can extract most translations for each NE. This feature makes similar effects of query expansion and very helpful for cross-language information retrieval because documents containing frequent or infrequent translations can be retrieved. Finally, the high MAP over all ﬁve domains est"
O08-2010,C98-1036,0,\N,Missing
O11-1007,O97-4005,0,0.676962,"Missing"
O11-1007,W03-1701,0,0.361387,"Missing"
O11-1007,W03-1719,0,0.27383,"Missing"
O11-1007,W03-1726,0,0.197122,"Missing"
O11-1007,J04-1004,0,0.663135,"Missing"
O11-1007,O05-2002,0,0.275958,"Missing"
O11-1007,I05-3017,0,0.525514,"Missing"
O11-1007,W06-0115,0,0.596159,"Missing"
O11-1007,P06-2123,0,0.541354,"Missing"
O11-1007,W10-4126,0,0.456062,"Missing"
O11-1007,I08-4010,0,\N,Missing
O11-1007,W10-4138,1,\N,Missing
O12-4003,I05-3017,0,0.0722342,"Missing"
O12-4003,J04-1004,0,0.0710601,"Missing"
O12-4003,O05-2002,0,0.0454183,"Missing"
O12-4003,Y03-1017,0,0.0850059,"Missing"
O12-4003,O11-1007,1,0.222658,"Missing"
O12-4003,W99-0701,0,0.149609,"Missing"
O12-4003,W96-0213,0,0.570636,"Missing"
O12-4003,W03-1719,0,0.050688,"Missing"
O12-4003,D11-1090,0,0.0591669,"Missing"
O12-4003,I05-1009,0,0.0679627,"Missing"
O12-4003,P06-2123,0,0.0482026,"Missing"
O12-4003,O97-4005,0,\N,Missing
O12-4003,W03-1701,0,\N,Missing
O12-4003,W03-1726,0,\N,Missing
O12-4003,W06-0115,0,\N,Missing
O12-4003,I08-4010,0,\N,Missing
O12-4003,W10-4138,1,\N,Missing
O14-2002,E06-1002,0,0.0452926,"Missing"
O14-2002,D07-1074,0,0.192564,"Missing"
O14-2002,C10-1032,0,0.0572712,"Missing"
O14-2002,N09-1037,0,0.0666793,"Missing"
O14-2002,P12-1055,0,0.0542655,"Missing"
O14-2002,P05-1012,0,0.0883921,"Missing"
O14-2002,C10-1145,0,0.055367,"Missing"
O14-2002,mcnamee-etal-2010-evaluation,0,\N,Missing
O14-2002,W09-1401,0,\N,Missing
O14-2002,J95-2003,0,\N,Missing
O14-4002,P07-1109,0,0.0603302,"Missing"
O14-4002,W06-0120,1,\N,Missing
O17-3003,P14-2131,0,0.0758101,"Missing"
P14-2096,C92-2082,0,0.0386736,"ish article and Chinese candidate pair in testing, the LDA model provides the distribution of the latent topics. Next, we can use entropy to measure the distribution of topics. The entropy of the estimated topic distribution of a related article is expected to be lower than that of an unrelated article. We can calculate the entropy of the distribution as a value for SVM. The entropy is defined as follows: H=− T X novel nsubj It is cop a det num amod prep_by nn 2008 young adult suzanne collins amod nn nn writer American Next, we apply our predefined syntactic patterns to extract the hypernym. (Hearst, 1992) If any pattern matches the structure of the dependency parse tree, the hypernym can be extracted. In the above example, the following pattern is matched: NN nsubj It cop is [target] nn NN In this pattern, the rightmost leaf is the hypernym target. Thus, we can extract the hypernym “novel” from the previous example. The term “novel” is the extracted hypernym of the English article “The Hunger Games”. After extracting the hypernym of the English article, the hypernym is translated into Chinese. The value of this feature in the SVM model is calculated as follows: θ~dj log θ~dj j=1 where T is the"
P14-2096,I11-1029,0,0.0406765,"Missing"
P14-2096,W08-2106,0,0.0191534,"’s title in the first sentence with pronouns. For example, the previous sentence is rewritten as “It is a 2008 young adult novel by American writer Suzanne Collins.” Then, the dependency parser generates the following parse tree: Mixed-language Topic Model Feature (MTM) For a linked English-Chinese article pair, the distribution of words used in each usually shows some convergence. The two semantically corresponding articles often have many related terms, which results in clusters of specific words. If two articles do not describe the same topic, the distribution of terms is often scattered. (Misra et al., 2008) Thus, the distribution of terms is good measurement of article similarity. Because the number of all possible words is too large, we adopt a topic model to gather the words into some latent topics. For this feature, we use the Latent Dirichlet Allocation (LDA) (Blei et al., 2003). LDA can be seen as a typical probabilistic approach to latent topic computation. Each topic is represented by a distribution of words, and each word has a probability score used to measure its contribution to the topic. To train the LDA model, the pair English and Chinese articles are concatenated into a single docu"
W06-0122,W03-0428,0,0.0381044,"feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-based model is that it can avoid the errors sometimes made by a Chinese word segmentor. The remainder of this paper is organized as follows. In the Section 2, we introduce the machine learning models, the features we apply in the machine learning models, and the ensemble methods. In Section 3, we briefly describe the experimental data and the ex"
W06-0122,J01-2002,0,0.0305322,"ered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-ba"
W06-0122,P97-1056,0,0.0675049,"Missing"
W06-0122,J96-1002,0,0.0220881,"tage of the CRF model is that richer feature sets can be considered, because, unlike HMM, it does not make a dependence assumption. However, the obvious drawback of the CRF model is that it needs more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character"
W06-0122,W03-0425,0,0.0160393,"more computing resources, so we can not apply all the features of the model. One possible way to resolve this problem is to effectively combine the results of various individual classifiers trained with different feature sets. In this paper, we use two ensemble methods to combine the results of the classifiers. We also combine the results generated by two machine learning models: maximum entropy (ME) [1] and CRF. One ensemble method is based on the majority vote [3], and the other is the memory based learner [7]. Although the ensemble methods have been applied in some sequence labeling tasks [2],[3], similar work in Chinese named entity recognition is scarce. Our Chinese named entity tagger uses a character-based model. For English named entity tasks, a character-based NER model proposed by Dan Klein [4] proves the usefulness of substrings within words. In Chinese NER, the characterbased model is more straightforward, since there are no spaces between Chinese words and each Chinese character is actually meaningful. Another reason for using a character-based model is that it can avoid the errors sometimes made by a Chinese word segmentor. The remainder of this paper is organized as fo"
W06-0602,H94-1020,0,0.0537987,"Missing"
W06-0602,J93-2004,0,0.0301737,"Missing"
W06-0602,N03-4009,0,0.079393,"fashion. In addition, users can customize the tag set of arguments. Other linguistic information can also be integrated and displayed in Figure 3. The percentage of the 30 biomedical verbs and other verbs in BioProp 3 3.1 Annotation of BioProp Annotation Process After choosing 30 verbs as predicates, we adopted a semi-automatic method to annotate BioProp. The annotation process consists of the following steps: (1) identifying predicate candidates; (2) automatically annotating the biomedical semantic roles with our WSJ SRL system; (3) transforming the automatic tagging results into WordFreak (Morton and LaCivita, 2003) format; and (4) manually correcting the annotation results with the WordFreak annotation tool. We now describe these steps in detail: 8 WordFreak, which is a convenient annotation tool. 4. In the last step, annotators check the predicted semantic roles using WordFreak and then correct or add semantic roles if the predicted arguments are incorrect or missing, respectively. Three biologists with sufficient biological knowledge in our laboratory performed the annotation task after receiving computational linguistic training for approximately three months. Figure 4 illustrates an example of BioPr"
W06-0602,J05-1004,0,0.12019,"Missing"
W06-0602,C04-1197,0,0.0103882,"ystem (called a BIOmedical SeMantIc roLe labEler, BIOSMILE) that is trained on BioProp and employs all the features used in our WSJ SRL system (Tsai et al., 2006). As with POS tagging, chunking, and named entity recognition, SRL can also be formulated as a sentence tagging problem. A sentence can be represented by a sequence of words, a sequence of phrases, or a parsing tree; the basic units of a sentence in these representations are words, phrases, and constituents, respectively. Hacioglu et al. (2004) showed that tagging phrase-byphrase (P-by-P) is better than word-by-word (Wby-W). However, Punyakanok et al. (2004) showed that constituent-by-constituent (C-by-C) tagging is better than P-by-P. Therefore, we use C-by-C tagging for SRL in our BIOSMILE. SRL can be divided into two steps. First, we identify all the predicates. This can be easily accomplished by finding all instances of verbs of interest and checking their part-of-speech (POS) tags. Second, we label all arguments corresponding to each predicate. This is a difficult problem, since the number of arguments and their positions vary according to a verb’s voice (active/passive) and sense, along with many other factors. In BIOSMILE, we employ the ma"
W06-0602,tateisi-tsujii-2004-part,0,0.0163253,"Missing"
W06-0602,W04-3212,0,0.0234413,"Missing"
W06-0602,W05-0620,0,0.0339564,"Missing"
W06-0602,W04-2416,0,0.0459367,"compare the performance of systems trained on BioProp and PropBank in different domains. We construct a new SRL system (called a BIOmedical SeMantIc roLe labEler, BIOSMILE) that is trained on BioProp and employs all the features used in our WSJ SRL system (Tsai et al., 2006). As with POS tagging, chunking, and named entity recognition, SRL can also be formulated as a sentence tagging problem. A sentence can be represented by a sequence of words, a sequence of phrases, or a parsing tree; the basic units of a sentence in these representations are words, phrases, and constituents, respectively. Hacioglu et al. (2004) showed that tagging phrase-byphrase (P-by-P) is better than word-by-word (Wby-W). However, Punyakanok et al. (2004) showed that constituent-by-constituent (C-by-C) tagging is better than P-by-P. Therefore, we use C-by-C tagging for SRL in our BIOSMILE. SRL can be divided into two steps. First, we identify all the predicates. This can be easily accomplished by finding all instances of verbs of interest and checking their part-of-speech (POS) tags. Second, we label all arguments corresponding to each predicate. This is a difficult problem, since the number of arguments and their positions vary"
W06-0602,I05-2038,0,\N,Missing
W06-0602,J03-4003,0,\N,Missing
W06-3308,W04-2416,0,0.0279478,"for argument classification. Then, we illustrate basic features as well as specialized features such as biomedical named entities and argument templates. AM-LOC Semantic Role Labeling on BioProp In this section, we introduce our BIOmedical SeMantIc roLe labEler, BIOSMILE. Like POS tagging, chunking, and named entity recognition, SRL can be formulated as a sentence tagging problem. A sentence can be represented by a sequence of words, a sequence of phrases, or a parsing tree; the basic units of a sentence are words, phrases, and constituents arranged in the above representations, respectively. Hacioglu et al. (2004) showed that tagging phrase by phrase (P-by-P) is better than word by word (W-by-W). Punyakanok et al., (2004) further showed that constituent-by-constituent (Cby-C) tagging is better than P-by-P. Therefore, we choose C-by-C tagging for SRL. The gold standard SRL corpus, PropBank, was designed as an additional layer of annotation on top of the syntactic structures of the Penn Treebank. 59 Maximum Entropy Model The maximum entropy model (ME) is a flexible statistical model that assigns an outcome for each instance based on the instance’s history, which is all the conditioning data that enables"
W06-3308,W04-1204,0,0.0496523,"Missing"
W06-3308,J05-1004,0,0.0276531,"the biomedical domain exists. In this paper, we aim to build such a biomedical SRL system. To achieve this goal we roughly implement the following three steps as proposed by Wattarujeekrit et al., (2004): (1) create semantic roles for each biomedical verb; (2) construct a biomedical corpus annotated with verbs and their corresponding semantic roles (following definitions created in (1) as a reference resource;) (3) build an automatic semantic interpretation model using the annotated text as a training corpus for machine learning. In the first step, we adopt the definitions found in PropBank (Palmer et al., 2005), defining our own framesets for verbs not in PropBank, such as “phosphorylate”. In the second step, we first use an SRL system (Tsai et al., 2005) trained on the Wall Street Journal (WSJ) to automatically tag our corpus. We then have the results double-checked by human annotators. Finally, we add automatically-generated template features to our SRL system to identify adjunct (modifier) arguments, especially those highly relevant to the biomedical domain. 2 Biomedical Proposition Bank As proposition banks are semantically annotated versions of a Penn-style treebank, they provide consistent sem"
W06-3308,C04-1197,0,0.0217708,"medical named entities and argument templates. AM-LOC Semantic Role Labeling on BioProp In this section, we introduce our BIOmedical SeMantIc roLe labEler, BIOSMILE. Like POS tagging, chunking, and named entity recognition, SRL can be formulated as a sentence tagging problem. A sentence can be represented by a sequence of words, a sequence of phrases, or a parsing tree; the basic units of a sentence are words, phrases, and constituents arranged in the above representations, respectively. Hacioglu et al. (2004) showed that tagging phrase by phrase (P-by-P) is better than word by word (W-by-W). Punyakanok et al., (2004) further showed that constituent-by-constituent (Cby-C) tagging is better than P-by-P. Therefore, we choose C-by-C tagging for SRL. The gold standard SRL corpus, PropBank, was designed as an additional layer of annotation on top of the syntactic structures of the Penn Treebank. 59 Maximum Entropy Model The maximum entropy model (ME) is a flexible statistical model that assigns an outcome for each instance based on the instance’s history, which is all the conditioning data that enables one to assign probabilities to the space of all outcomes. In SRL, a history can be viewed as all the informati"
W06-3308,P03-1002,0,0.0987388,"Missing"
W06-3308,tateisi-tsujii-2004-part,0,0.0717963,"Missing"
W06-3308,I05-2038,0,0.0172,"collection of MEDLINE abstracts selected from the search results with the following keywords: human, blood cells, and transcription factors. In the GENIA corpus, the abstracts are encoded in XML format, where each abstract also contains a MEDLINE UID, and the title and content of the abstract. The text of the title and content is segmented into sentences, in which biological terms are annotated with their semantic classes. The GENIA corpus is also annotated with part-ofspeech (POS) tags (Tateisi et al., 2004), and coreferences (Yang et al., 2004). The Penn-style treebank for GENIA, created by Tateisi et al. (2005), currently contains 500 abstracts. The annotation scheme of the GENIA Treebank (GTB), which basically follows the Penn Treebank II (PTB) scheme (Bies et al., 1995), is encoded in XML. However, in contrast to the WSJ corpus, GENIA lacks a proposition bank. We therefore use its 500 abstracts with GTB as our corpus. To develop our biomedical proposition bank, BioProp, we add the proposition bank annotation on top of the GTB annotation. 2.1 Important Argument Types In the biomedical domain, relations are often dependent upon locative and temporal factors (Kholodenko, 2006). Therefore, locative (A"
W06-3308,W04-3212,0,0.0417507,"Missing"
W11-3205,J03-1002,0,0.0025601,"phonology, Korean may insert a specific vowel “ㅡ” [W] between English consonant clusters or behind the last burst stop consonant of the syllable. For instance, the English name entity “Snell” is transliterated as “스 넬” /sW nel/ and “Albert” is transliterated as “앨 버트” /æl b@ th W/. In order to deal with these complex orthography problems, we adopt substring-based method to group characters into substrings. English words are segmented into several substrings and each substring maps to a substring in the target language, Korean. To create training sets of substrings, we use the GIZA++ toolkit (Och and Ney, 2003) to align all the name entity pairs in the training data. The GIZA++ toolkit performs one-to-many alignments, which means that a single symbol in the source language may be aligned to at least one symbol in the target language. To obtain the manyto-many substring alignments, we run GIZA++ on 1 http://www.speech.cs.cmu.edu. /cgi-bin/cmudict 2 http://www.speech.cs.cmu.edu/tools/ lextool.html 33 should belong to one substring, we need only B and I classes in the tag sets. After the English named entities are segmented into substrings, it can be passed into the CRF model we trained in section 2.1."
W11-3205,W09-3520,0,0.0377187,"Missing"
W11-3205,W06-0120,1,0.821174,"ansliteration Because our method is based on the substrings from the transformed training data, we have to segment the unseen English named entities into the substrings before applying CRF testing of our model. For example, we have to segment the English named entity “SHASHI” into four substrings < SH A SH I &gt;. Since the substrings used to train the CRF model are generated by the bidirectional alignments from the training data, we also used CRF to train another model for substring segmentation of English named entities. We adopt the segmentation approach motivated by the Chinese segmentation (Tsai et al., 2006) which treat Chinese segmentation as a tagging problem. The characters in a sentence are tagged in B class if it is the first character of a Chinese word or in I class if it is in a Chinese word but not the first character. Thus, we collect all the substring results from the bidirectional alignments and tag each character in the English named entity in the training data as B class (the first character of the substring) or I class (not the first character of the substring) to create a training data of substring segmentation for CRF. Since each character 2.2 Rule-based Approach We also construct"
W12-4408,N07-1047,0,0.124503,"Missing"
W12-4408,P08-1103,0,0.173033,"e alignments, we construct another alignment configurations to take null consonant into consideration. Consequently, for any Korean syllabic block containing two Korean letters will be converted into three Roman letters with the third one being a predefined Roman letter representing null consonant. We also have two set of parameters for this change, that is x = 2, y = 3 and x = 1 ,y = 3. The reason we increase both y by one is that there are three Korean letters for each word. 2.3 DirecTL-p Training With aligned English-Korean pairs, we can train our transliteration model. We apply DirecTL-p (Jiampojamarn et al., 2008) for our training and testing task. We train the transliteration models with different alignment parameter settings individually mentioned in section 2.2. 2.4 Re-ranking Results Because we train several transliteration models with different alignment parameters, we have to combine the results from different models. Therefore, the re-ranking method is necessary to select the best transliteration result. For re-ranking, we propose two approaches. 1. Web-based re-ranking 2. JLIS-Reranking 2.4.1 Web-based re-ranking The first re-ranking method is based on the occurrence of transliterations in the"
W12-4408,W10-2405,0,0.062521,"Letter-to-phoneme alignment Named entity translation is a key problem in many NLP research fields such as machine translation, cross-language information retrieval, and question answering. Most name entity translation is based on transliteration, which is a method to map phonemes or graphemes from source language into target language. Therefore, named entity transliteration system is important for translation. In the shared task, we focus on English-Korean transliteration. We consider to transform the transliteration task into a sequential labeling problem. We adopt m2m-aligner and DirecTL-p (Jiampojamarn et al., 2010) to do substring mapping and transliteration predicting, respectively. With this approach (Ji3. DirecTL-p training 4. Re-ranking results 2.1 Pre-processing Korean writing system, namely Hangul, is alphabetical. However, unlike western writing system with Latin alphabets, Korean alphabet is composed into syllabic blocks. Each Korean syllabic block represent a syllable which has three components: initial consonant, medial vowel and optionally final consonant. Korean has 14 initial consonants, 10 medial vowels, and 7 final consonants. For instance, the syllabic block “신” (sin) is composed with th"
W12-4408,W10-2409,0,0.0297425,"0.720 0.754 0.737 MRR 0.488 0.494 0.452 0.474 0.563 0.500 MAPref 0.488 0.494 0.452 0.473 0.536 0.500 MRR 0.398 0.484 MAPref 0.397 0.458 Table 2: Results on test data Run Standard (JLIS-Reranking) Non-standard (Web-based reranking) Accuracy 0.398 0.458 DirecTL-p model outputs a file containing the alignment of each result, there are some features in the results that we can use for re-ranking. In our reranking approach, there are three features used in the process: source grapheme chain feature, target grapheme chain feature and syllable consistent feature. These three feature are proposed in (Song et al., 2010). Source grapheme chain feature: This feature can tell us that how the source characters are aligned. Take “A|D|A|M” for example, we will get three chains which are A|D, D|A and A|M. With this feature we may know the alignment in the source language. Target grapheme chain feature: Similar to the above feature, it tell us how the target characters are aligned. Take “NG:A:n|D|A|M” for example, which is the Korean transliteration of ADAM, we will get three chains which are n|D, D|A and A|M. With this feature we may know the alignment in the target language. “n” is the predefined null consonant. S"
W15-3913,W09-3519,0,0.013924,"N D ER &gt;. Since the CMU pronouncing dictionary does not cover all the pronunciation information of the name entities in the training data, we also apply LOGIOS Lexicon Tool to generate the phonemic representations of all other name entities not in the CMU pronouncing dictionary. Fine-grained Segment Algorithm (FSA) Unlike English letters and words, each Hangul block or Chinese character corresponds to a syllable. Some previous approaches have used English letters and Chinese characters/Korean syllabic blocks as the basic alignment units for transliteration (Oh and Choi, 2006; Li et al., 2004; Jia et al., 2009). Other approaches have tried to segment English NEs into syllabic chunks for alignment with Hangul blocks or Chinese characters (Wan and Verspoor, 1998; Jiang et al., 2007; Zhang et al., 2012). We adopt a heuristic syllable segmentation algorithm, namely Fine-grained Segment Algorithm (FSA), proposed by Zhang et al. (2012) with slight modification to syllabify English NEs. Our modified version of the FSA is defined as follows: 2.1.2 Korean Korean writing system, namely Hangul, is alphabetical. However, unlike western writing system with Latin alphabets, Korean alphabet is composed into syllab"
W15-3913,N07-1047,0,0.13338,"al information between the source language term and the transliteration candidate as the similarity score for ranking. 2.1.3 Chinese For Chinese, we treat each Chinese character as a basic alignment unit. Chinese chacters of a Chinese word are segment as each single Chinese character for further alignment processing. For example, the Chinese word “诺克斯” is separated as three character “诺 克 斯”. 2.2 Alignment After generating English, Korean, and Chinese segmented substrings in the previous step, we determine the alignment between each EnglishKorean and English-Chinese pair using the M2Maligner (Jiampojamarn et al., 2007). The M2Maligner is a many-to-many alignment method based on the expectation maximization (EM) algorithm. It allows us to create alignments between substrings of various lengths. During alignment, empty strings (nulls) are only allowed on the target side. 2.3 3 To measure the transliteration models with different segmentation methods and the reranking methods, we construct the following experimental runs: English-Korean (EnKo) Runs: • Run 1: SINGLE + HANGUL • Run 2: SINGLE + ROMAN • Run 3: PHONEME + ROMAN DirecTL+ Training With aligned English-Korean and EnglishChinese pairs, we can train our"
W15-3913,P08-1103,0,0.02433,"nt method based on the expectation maximization (EM) algorithm. It allows us to create alignments between substrings of various lengths. During alignment, empty strings (nulls) are only allowed on the target side. 2.3 3 To measure the transliteration models with different segmentation methods and the reranking methods, we construct the following experimental runs: English-Korean (EnKo) Runs: • Run 1: SINGLE + HANGUL • Run 2: SINGLE + ROMAN • Run 3: PHONEME + ROMAN DirecTL+ Training With aligned English-Korean and EnglishChinese pairs, we can train our transliteration model. We apply DirecTL+ (Jiampojamarn et al., 2008) for training and testing. DirecTL+ is an online discriminative training model for string transduction problems. We individually train the transliteration models with different segmentation methods individually mentioned in section 2.1. 2.4 Results • Run 4: FSA + HANGUL • Run 5: FSA + ROMAN • Run 6: Orthography Similarity Ranking with Run 1 to 5 • Run 7: Web-based Ranking with Run 1 to 5 English-Chinese (EnCh) Runs: Reranking Results • Run 1: FSA + Chinese characters Because we train several transliteration models with different alignment settings, we can combine the results from different mod"
W15-3913,N10-1103,0,0.347046,"§∗ † Department of Computer Science and Information Engineering, National Taiwan University, Taiwan ‡ Department of Computer Science, National Tsinghua University, Taiwan § Department of Computer Science and Information Engineering, National Central University, Taiwan d97023@csie.ntu.edu.tw s102065512@m102.nthu.edu.tw thtsai@csie.ncu.edu.tw Abstract shared task in previous Named Entities Workshops (NEWS). In the shared task for NEWS 2015, we focus on English-Korean and English-Chinese transliteration. We adopt the M2M-aligner and DirecTL+ to map substrings and predict transliteration results. Jiampojamarn et al. (2010) achieved promising results using this approach in the NEWS 2010 transliteration task. The Korean writing system, Hangul, is alphabetic, but Chinese characters are logograms. Because English and Korean use alphabetic writing systems, we apply different grapheme segmentation methods to create several transliteration models. For Chinese, we treat each distinct Chinese character as a basic unit for the alignment step. In order to improve the transliteration performance, we also apply two ranking techniques to select the best transliterations. This paper is organized as follows. In Section 2 we de"
W15-3913,P04-1021,0,0.533557,"< AE L AH G Z AE N D ER &gt;. Since the CMU pronouncing dictionary does not cover all the pronunciation information of the name entities in the training data, we also apply LOGIOS Lexicon Tool to generate the phonemic representations of all other name entities not in the CMU pronouncing dictionary. Fine-grained Segment Algorithm (FSA) Unlike English letters and words, each Hangul block or Chinese character corresponds to a syllable. Some previous approaches have used English letters and Chinese characters/Korean syllabic blocks as the basic alignment units for transliteration (Oh and Choi, 2006; Li et al., 2004; Jia et al., 2009). Other approaches have tried to segment English NEs into syllabic chunks for alignment with Hangul blocks or Chinese characters (Wan and Verspoor, 1998; Jiang et al., 2007; Zhang et al., 2012). We adopt a heuristic syllable segmentation algorithm, namely Fine-grained Segment Algorithm (FSA), proposed by Zhang et al. (2012) with slight modification to syllabify English NEs. Our modified version of the FSA is defined as follows: 2.1.2 Korean Korean writing system, namely Hangul, is alphabetical. However, unlike western writing system with Latin alphabets, Korean alphabet is c"
W15-3913,P98-2220,0,0.771019,"apply LOGIOS Lexicon Tool to generate the phonemic representations of all other name entities not in the CMU pronouncing dictionary. Fine-grained Segment Algorithm (FSA) Unlike English letters and words, each Hangul block or Chinese character corresponds to a syllable. Some previous approaches have used English letters and Chinese characters/Korean syllabic blocks as the basic alignment units for transliteration (Oh and Choi, 2006; Li et al., 2004; Jia et al., 2009). Other approaches have tried to segment English NEs into syllabic chunks for alignment with Hangul blocks or Chinese characters (Wan and Verspoor, 1998; Jiang et al., 2007; Zhang et al., 2012). We adopt a heuristic syllable segmentation algorithm, namely Fine-grained Segment Algorithm (FSA), proposed by Zhang et al. (2012) with slight modification to syllabify English NEs. Our modified version of the FSA is defined as follows: 2.1.2 Korean Korean writing system, namely Hangul, is alphabetical. However, unlike western writing system with Latin alphabets, Korean alphabet is composed into syllabic blocks. Each Korean syllabic block represents a syllable which has three components: initial consonant, medial vowel and optionally final consonant."
W15-3913,W12-4407,0,0.0360392,"Missing"
W15-3913,C98-2215,0,\N,Missing
W17-5808,S13-2057,0,0.0539639,"Missing"
W17-5808,roark-etal-2006-sparseval,0,0.014151,"Missing"
W17-5808,S13-2058,0,0.051468,"Missing"
W17-5808,S13-2105,0,0.0508324,"Missing"
Y09-1009,E06-1002,0,0.0522272,"ized in Medelyan et al. (2009), a survey paper that provides an in-depth description of the creation process and textual structure of Wikipedia pages, and review diverse effort that exploits Wikipedia in many research areas: word sense disambiguation (Mihalcea and Csomai, 2007; Ruiz-Casado et al., 2005; Wu and Weld, 2007), bilingual lexicography (Erdmann et al., 2008; Tyers and Pienaar, 2008), multilingual information retrieval (Potthast et al., 2008), information extraction and question answering (Banko et al., 2007; Toral and Muñoz, 2007; Ferrández et al., 2007), named entities recognition (Bunescu and Paşca, 2006; Cimiano and Volker, 2005; Dakka and Cucerzan, 2007), and ontology construction (Auer et al. 2007; Ponzetto and Strube, 2007; Suchanek et al., 2007). In our work we address a specific knowledge mining problem of identifying and categorizing named entities that appear as the titles in Wikipedia (e.g., identifying and categorizing John Updike and Terrorist as named entities of the types PERSON and COMMUNICATION). In the work on mining meaning from Wikipedia, the most closely related body of research to our work focuses on word sense disambiguation of Wikipedia titles, words and phrases. 75 Miha"
Y09-1009,D07-1074,0,0.0309835,"an in-depth description of the creation process and textual structure of Wikipedia pages, and review diverse effort that exploits Wikipedia in many research areas: word sense disambiguation (Mihalcea and Csomai, 2007; Ruiz-Casado et al., 2005; Wu and Weld, 2007), bilingual lexicography (Erdmann et al., 2008; Tyers and Pienaar, 2008), multilingual information retrieval (Potthast et al., 2008), information extraction and question answering (Banko et al., 2007; Toral and Muñoz, 2007; Ferrández et al., 2007), named entities recognition (Bunescu and Paşca, 2006; Cimiano and Volker, 2005; Dakka and Cucerzan, 2007), and ontology construction (Auer et al. 2007; Ponzetto and Strube, 2007; Suchanek et al., 2007). In our work we address a specific knowledge mining problem of identifying and categorizing named entities that appear as the titles in Wikipedia (e.g., identifying and categorizing John Updike and Terrorist as named entities of the types PERSON and COMMUNICATION). In the work on mining meaning from Wikipedia, the most closely related body of research to our work focuses on word sense disambiguation of Wikipedia titles, words and phrases. 75 Mihalcea (2007) considers the same title (e.g., bar) with"
Y09-1009,I08-1071,0,0.384982,"Missing"
Y09-1009,H05-1059,0,0.0373418,"Missing"
Y09-2036,P07-1009,0,0.0432058,"Missing"
Y09-2036,N09-1067,0,0.0777529,"Missing"
Y09-2049,J97-4001,0,0.0574838,", and statistical-rule-learning-based (Davel and Barnard, 2004). The Copyright 2009 by Yu-Chun Wang and Richard Tzong-Han Tsai 23rd Pacific Asia Conference on Language, Information and Computation, pages 843–850 843 dictionary-based strategy (Zhang et al., 2001) acquires a lot of phonological knowledge from a phonetic dictionary. It looks up each word in a dictionary and retrieve the corresponding pronunciation. The dictionary-based strategy suffers from out-of-vocabulary (OOV) problems severely because it is impossible to account for all novel words. On the contrary, the rule-based strategy (Divay and Vitale, 1997) only requires a smaller set of rules that describe how to convert a grapheme into one or several phonemes. The rules can be applied to all of words without OOV problems. However, for some languages with complex writing systems such as Chinese and Japanese, building the rules is labor-intensive and very difficult to cover most possible situations. The statistical-rule-learning-based approaches (Zhang and Chu, 2002) try to generate a rule set that can determine the phonemes automatically by statistical machine learning models, such as decision tree (DT) models or maximum entropy (ME) models (Gu"
Y09-2049,W96-0213,0,0.0373978,"ith complex writing systems such as Chinese and Japanese, building the rules is labor-intensive and very difficult to cover most possible situations. The statistical-rule-learning-based approaches (Zhang and Chu, 2002) try to generate a rule set that can determine the phonemes automatically by statistical machine learning models, such as decision tree (DT) models or maximum entropy (ME) models (Guiasu and Shenitzer, 1985). Statistical-rule-learning-based methods can achieve satisfiable performance and also apply to many similar natural language processing tasks such as part-of-speech tagging (Ratnaparkhi, 1996) and named entity recognition (Tsai et al., 2004). However, statistical-rule-learning-based strategy requires a sufficient large human-tagged corpus as a training set to have an accurate rule set. Since Korean uses a phonemic writing system, the rule-based grapheme-to-phoneme conversion is not so difficult as Chinese ones. Therefore, we adopt rule-based approach. The advantages of rule-based grapheme-to-phoneme conversion are simple to implement, low efforts of human tagging, low memory consumption and efficient in computation. It is suitable to work on small embedded devices such as mobile ph"
Y09-2049,O04-2004,0,\N,Missing
Y11-1011,O92-1003,0,0.182742,"ven an utterance “[[meta][data]] / is / the / data / of / data” as standard in five boundaries, seven morphemes (notice the case of [[meta][data]]), six words, and five lexicon types, one possible segmentation “meta / data / is / the / data / of / data” results in one boundary error, zero morpheme error, two word errors, and one lexicon type error. Ando and Lee (2003) proposed a constituent-based metric called compatible bracket error, which counts reasonable combinations of morphemes, estimate “[meta][data]” and “[metadata]” as interchangeable. Previous studies also counted word error rates (Chiang et al., 1992; Teahan et al. 2000; Wong and Chan, 1996) and sentence accuracy (Chiang et al., 1992) as complementary metrics of word-based recall and precision. Several other constituent-based metrics exist, and are usually close related to IR. For example, Liu et al. (2008) proposed an evaluation measure “RankPrecision” based on Kendalltau distance, which compared the similarity between the predicted rankings of “Internal Association Strength (IAS)” and the ideally sorted rankings of IAS in descending order. Methodologies resemble IAS, which may be seen as constituent-based measurements, are “Phrase Insep"
Y11-1011,W10-4101,0,0.062982,"Missing"
Y11-1011,I05-3017,0,0.0173179,"tly has become a focus of research during the past two decades. Numerous researchers have urged the application of various algorithms to produce “accurate” tokenized text in which the segmentations are performed to match one of the standards of Chinese corpora, such as Academia Sinica Balanced Corpus (Huang et al., 1997) and Chinese Treebank of University of Pennsylvania (Xia, 1999), and evaluated using various metrics, such as word-based precision (P), recall (R) and their harmonic average, termed F1 measure score (Fscore), popularized by SIGHAN Chinese WS bakeoffs (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006; Jin and Chen, 2007; Zhao and Liu, 2010). Subsequently, WS systems that ∗ This research was supported in part by the National Science Council under grant NSC 100-2631-S-001-001, and the research center for Humanities and Social Sciences under grant IIS-50-23. Ted Kony is appreciated for his editorial assistance. The authors would like to thank anonymous reviewers for their constructive criticisms. Copyright 2011 by Mike Tian-Jian Jiang, Cheng-Wei Shih, Chan-Hung Kuo, Richard Tzong-Han Tsai, and Wen-Lian Hsu 25th Pacific Asia Conference on Language, Information and Computation, pa"
Y11-1011,P08-1016,0,0.0128341,"native metrics were proposed and discussed, including geometric mean average precision (Robertson, 2006) or other popularity-based measurements (Mizzaro, 2008; Yilmaz and Aslam, 2006), but have not been adopted widely. 2.2 Evaluation of Chinese Word Segmentation This study classifies known evaluation metrics of WS into three categories: boundary-based, token-based and constituent-based. Boundary recall is defined as the percentage of correct boundaries identified, while boundary precision is defined as the percentage of identified boundaries that are correct, discounting utterance boundaries (Fleck, 2008; Goldwater et al., 2009; Palmer and Burger, 1997). Token-based recall (R) and precision (P) are defined 101 analogously to their boundary-based counterparts, except the unit of measurement is tokens rather than boundaries, where tokens can be treated as morpheme instances (Ando and Lee, 2003), lexicon types (Fleck, 2008; Goldwater et al., 2009), or word instances. For example, given an utterance “[[meta][data]] / is / the / data / of / data” as standard in five boundaries, seven morphemes (notice the case of [[meta][data]]), six words, and five lexicon types, one possible segmentation “meta /"
Y11-1011,J05-4005,0,0.0184316,"AN Chinese WS bakeoff traditionally apply word-based evaluation metrics such as recall (R), precision (P), and F1-score (F) to both in-vocabulary and 103 out-of-vocabulary data to measure participating system performance. Although these measurements are intuitive and uncomplicated, they still suffer the weakness of not considering incorrect segments that variously influence applications such as web search or text retrieval. This section presents qualitative analysis between word-based WS evaluation metrics and the proposed metrics. Notably, this study follows the naming convention proposed by Gao et al. (2005) and uses “segmentation unit” (segment in short, hereafter) rather than “word” as the conceptual element of qualitative analysis. An input sequence “XYX” and its four possible segmentation gold standards G1, G2, G3 and G4 are “X/Y/X,” “X/YX,” “XY/X” and “XYX”, respectively. Furthermore, four different systems S1, S2, S3, and S4 output “X/Y/X,” “X/YX,” “XY/X,” and “XYX,” respectively. Table 1 lists their recall (R) and precision (P). Table 1: Performance Evaluation Samples of SIGHAN’s Metrics G1: X/Y/X G2: X/YX G3: XY/X G4: XYX S1: X/Y/X R P 3/3 3/3 1/2 1/3 1/2 1/3 0/1 0/3 S2: X/YX R P 1/3 1/2"
Y11-1011,W02-1804,0,0.214166,"ion that WS accuracy is directly related to the effectiveness of follow-up applications. However, this perspective has sometimes failed to win significant support, particularly in the Chinese IR area, because of the absence of strong evidence confirming the causation. As described later in Section 2.3, some previous studies demonstrated a non-monotonic relationship between the performances of WS and IR for Mandarin Chinese, implying that better WS results do not necessarily yield better IR outcomes (Foo and Li, 2004; Kwok, 2000; Peng et al., 2002), while other studies have shown the opposite (He et al., 2002; Nie et al., 2000; Palmer and Burger, 1997). Although these investigations provide valuable information, they remain unable to reach consensus regarding the principles or metrics involved in selecting a proper WS system for IR applications. Developing a credible method of measuring Chinese WS system requires error analysis. However, the mainstream of WS literature overlooked how incorrect identification of segments based on particular standards affects certain type of applications, and instead focused on improving segmentation accuracy. Consequently, this study annotates negative segment (NS)"
Y11-1011,O97-4003,0,0.0754867,"stics demonstrate that TNR and NPV are generally more closely correlated with MAP than are P and R. Keywords: 1 Word segmentation, information retrieval, true negative rate. Introduction Word segmentation (WS) is an essential preparatory work for Chinese text processing applications and consequently has become a focus of research during the past two decades. Numerous researchers have urged the application of various algorithms to produce “accurate” tokenized text in which the segmentations are performed to match one of the standards of Chinese corpora, such as Academia Sinica Balanced Corpus (Huang et al., 1997) and Chinese Treebank of University of Pennsylvania (Xia, 1999), and evaluated using various metrics, such as word-based precision (P), recall (R) and their harmonic average, termed F1 measure score (Fscore), popularized by SIGHAN Chinese WS bakeoffs (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006; Jin and Chen, 2007; Zhao and Liu, 2010). Subsequently, WS systems that ∗ This research was supported in part by the National Science Council under grant NSC 100-2631-S-001-001, and the research center for Humanities and Social Sciences under grant IIS-50-23. Ted Kony is appreciated for his edi"
Y11-1011,D08-1111,0,0.0207896,"one boundary error, zero morpheme error, two word errors, and one lexicon type error. Ando and Lee (2003) proposed a constituent-based metric called compatible bracket error, which counts reasonable combinations of morphemes, estimate “[meta][data]” and “[metadata]” as interchangeable. Previous studies also counted word error rates (Chiang et al., 1992; Teahan et al. 2000; Wong and Chan, 1996) and sentence accuracy (Chiang et al., 1992) as complementary metrics of word-based recall and precision. Several other constituent-based metrics exist, and are usually close related to IR. For example, Liu et al. (2008) proposed an evaluation measure “RankPrecision” based on Kendalltau distance, which compared the similarity between the predicted rankings of “Internal Association Strength (IAS)” and the ideally sorted rankings of IAS in descending order. Methodologies resemble IAS, which may be seen as constituent-based measurements, are “Phrase Inseparability” (Shi and Nie, 2009) and the “Tightness Continuum Measure” (Xu et al. 2010). 2.3 WS-to-IR Performance Relationship The influence of different Chinese WS methods on IR has attracted extensive research attention (Foo and Li, 2004; He et al. 2002; Kwok, 2"
Y11-1011,C02-1148,0,0.147956,"d are selected as the preprocessors for the commonly accepted notion that WS accuracy is directly related to the effectiveness of follow-up applications. However, this perspective has sometimes failed to win significant support, particularly in the Chinese IR area, because of the absence of strong evidence confirming the causation. As described later in Section 2.3, some previous studies demonstrated a non-monotonic relationship between the performances of WS and IR for Mandarin Chinese, implying that better WS results do not necessarily yield better IR outcomes (Foo and Li, 2004; Kwok, 2000; Peng et al., 2002), while other studies have shown the opposite (He et al., 2002; Nie et al., 2000; Palmer and Burger, 1997). Although these investigations provide valuable information, they remain unable to reach consensus regarding the principles or metrics involved in selecting a proper WS system for IR applications. Developing a credible method of measuring Chinese WS system requires error analysis. However, the mainstream of WS literature overlooked how incorrect identification of segments based on particular standards affects certain type of applications, and instead focused on improving segmentation accu"
Y11-1011,W03-1719,0,0.456166,"applications and consequently has become a focus of research during the past two decades. Numerous researchers have urged the application of various algorithms to produce “accurate” tokenized text in which the segmentations are performed to match one of the standards of Chinese corpora, such as Academia Sinica Balanced Corpus (Huang et al., 1997) and Chinese Treebank of University of Pennsylvania (Xia, 1999), and evaluated using various metrics, such as word-based precision (P), recall (R) and their harmonic average, termed F1 measure score (Fscore), popularized by SIGHAN Chinese WS bakeoffs (Sproat and Emerson, 2003; Emerson, 2005; Levow, 2006; Jin and Chen, 2007; Zhao and Liu, 2010). Subsequently, WS systems that ∗ This research was supported in part by the National Science Council under grant NSC 100-2631-S-001-001, and the research center for Humanities and Social Sciences under grant IIS-50-23. Ted Kony is appreciated for his editorial assistance. The authors would like to thank anonymous reviewers for their constructive criticisms. Copyright 2011 by Mike Tian-Jian Jiang, Cheng-Wei Shih, Chan-Hung Kuo, Richard Tzong-Han Tsai, and Wen-Lian Hsu 25th Pacific Asia Conference on Language, Information and"
Y11-1011,I05-3001,0,0.0200073,"nces of accuracy-controlled WS systems in terms of TPR, TNR, PPV, and NPV. The MAP values of the search results retrieved by the original and segmented queries are illustrated in Figure 5. Finally, Table 4 compares correlation coefficients. Table 4: Correlation between search similarities and WS metrics TPR TNR PPV NPV 5 AS 0.742 0.836 0.788 0.800 CityU 0.916 0.944 0.931 0.940 MSR 0.933 0.649 0.901 0.819 PKU 0.615 0.870 0.634 0.692 Discussion Table 4 shows that TNR and NPV are more closely correlated than TPR and PPV, except in the case of MSR. Notably, since Corpus quality assurance process (Sun et al., 2005) and analysis for out-of-vocabulary issue (Li et al., 2005) of MSR use similar definitions of NS, they provide a good demonstration that considering both the PS and NS oriented metric can identify differences between WS systems trained using different standards of corpora. For example, a Sogou query “上海滩” (shang-hai-tan; The bund of Shanghai) were segmented into “上海滩,” “上 海 / 滩” (Shanghai / bund), or “上 / 海 / 滩” (up / sea / bund) using the accuracy-controlled system MSR, while the same query were segmented into “上海滩,” “上海 / 滩,” or “上 / 海滩” (go to / beach) using the accuracy-controlled system P"
Y11-1011,J00-3004,0,0.0558046,"eta][data]] / is / the / data / of / data” as standard in five boundaries, seven morphemes (notice the case of [[meta][data]]), six words, and five lexicon types, one possible segmentation “meta / data / is / the / data / of / data” results in one boundary error, zero morpheme error, two word errors, and one lexicon type error. Ando and Lee (2003) proposed a constituent-based metric called compatible bracket error, which counts reasonable combinations of morphemes, estimate “[meta][data]” and “[metadata]” as interchangeable. Previous studies also counted word error rates (Chiang et al., 1992; Teahan et al. 2000; Wong and Chan, 1996) and sentence accuracy (Chiang et al., 1992) as complementary metrics of word-based recall and precision. Several other constituent-based metrics exist, and are usually close related to IR. For example, Liu et al. (2008) proposed an evaluation measure “RankPrecision” based on Kendalltau distance, which compared the similarity between the predicted rankings of “Internal Association Strength (IAS)” and the ideally sorted rankings of IAS in descending order. Methodologies resemble IAS, which may be seen as constituent-based measurements, are “Phrase Inseparability” (Shi and"
Y11-1011,C96-1035,0,0.107234,"he / data / of / data” as standard in five boundaries, seven morphemes (notice the case of [[meta][data]]), six words, and five lexicon types, one possible segmentation “meta / data / is / the / data / of / data” results in one boundary error, zero morpheme error, two word errors, and one lexicon type error. Ando and Lee (2003) proposed a constituent-based metric called compatible bracket error, which counts reasonable combinations of morphemes, estimate “[meta][data]” and “[metadata]” as interchangeable. Previous studies also counted word error rates (Chiang et al., 1992; Teahan et al. 2000; Wong and Chan, 1996) and sentence accuracy (Chiang et al., 1992) as complementary metrics of word-based recall and precision. Several other constituent-based metrics exist, and are usually close related to IR. For example, Liu et al. (2008) proposed an evaluation measure “RankPrecision” based on Kendalltau distance, which compared the similarity between the predicted rankings of “Internal Association Strength (IAS)” and the ideally sorted rankings of IAS in descending order. Methodologies resemble IAS, which may be seen as constituent-based measurements, are “Phrase Inseparability” (Shi and Nie, 2009) and the “Ti"
Y11-1011,O03-4001,0,0.0441309,"Missing"
Y11-1011,W10-3708,0,0.0820369,"tion, while Section 5 discusses these same results. Finally, Section 6 presents conclusions, along with recommendations for future research. 2 2.1 Related Works Evaluation of Chinese Information Retrieval For various open evaluation tasks of Chinese IR, the test collections of TREC-5, TREC-6, and TREC-9 comprise 28, 26, and 25 queries in simplified Chinese, respectively, while NTCIR-2 comprises 50 queries in traditional Chinese. One problem is that the number of queries is small. Another problem with the TREC data is that the Chinese queries (topic titles) have too many keywords. According to Xu et al. (2010), the Chinese queries have an average length of 12.2 words; in contrast, the average length of English ad-hoc queries in TREC (English topics 251350) is 4.7 words. Even the length of Chinese queries is measured using English translations, the average length still exceeds 7 words. Long queries introduce complex effects whose interactions are difficult to understand. These problems are part of a more general issue of sample pooling bias (Webber and Park, 2009). Common metrics for evaluating IR effectiveness include precision of top-k retrieval results (P@k), mean reciprocal recall (MRR), mean av"
Y11-1011,zhao-etal-2010-large,0,0.0130046,"tional random fields with a 6-tag labeling scheme in bi-directional unigram, bi-gram and pair contexts (Zhao et al., 2006). Four gold standards involving Chinese WS corpus from SIGHAN 2005 WS bakeoffs, including Academia Sinica (AS), City University of Hong Kong (CityU), Microsoft Research (MSR), and Peking University (PKU), are adopted as training data (cf. footnote). By randomly dividing the training data into exponentially smaller parts ranging from 1/2 to 1/16384, WS systems trained using corresponding parts perform with linearly decreasing accuracy in TPR and PPV, just as demonstrated by Zhao et al. (2010). 4.3 Simulation Design For traditional Chinese, NTCIR queries are segmented by the accuracy-controlled WS systems AS and CityU. For simplified Chinese, Sogou queries are segmented by the accuracy-controlled WS systems MSR and PKU. The average segment numbers of segmented NTCIR and Sogou queries are 5.8 and 3.1, respectively. All the segmented tokens of a query are quoted, adhered by white space, and formed a segmented query. The original query and all the segmented queries from different WS systems are forwarded to Google to retrieve the top-100 search results. Search results of original quer"
Y11-1011,W06-0127,0,0.0157467,"contains multiple query strings in a single query, 105 representing academic samples of queries; meanwhile, query logs arranged by a commercial search engine company “Sogou” (Sogou query as in short, hereafter), written in simplified Chinese, comprising 432 consecutive query strings with an average length of 4.36 characters, provide a practical view of end users. 4.2 Accuracy-controlled WS Systems This study implements several WS systems based on the-state-of-the-art approach that uses conditional random fields with a 6-tag labeling scheme in bi-directional unigram, bi-gram and pair contexts (Zhao et al., 2006). Four gold standards involving Chinese WS corpus from SIGHAN 2005 WS bakeoffs, including Academia Sinica (AS), City University of Hong Kong (CityU), Microsoft Research (MSR), and Peking University (PKU), are adopted as training data (cf. footnote). By randomly dividing the training data into exponentially smaller parts ranging from 1/2 to 1/16384, WS systems trained using corresponding parts perform with linearly decreasing accuracy in TPR and PPV, just as demonstrated by Zhao et al. (2010). 4.3 Simulation Design For traditional Chinese, NTCIR queries are segmented by the accuracy-controlled"
Y11-1011,W06-0115,0,\N,Missing
Y11-1011,I08-4010,0,\N,Missing
Y11-1011,W03-1726,0,\N,Missing
Y13-1025,W10-1736,0,0.0588664,"dering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of thi"
Y13-1025,D11-1017,0,0.0256155,"pes of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, P"
Y13-1025,W08-0509,0,0.0341076,"Missing"
Y13-1025,W00-1303,0,0.0510009,"PC is limited to dependency structure and POS tags, for analysis on the causes of reordering errors, we examine parsing errors from these two linguistic categories. In this section, the value of Kendall’s tau measures the word order similarity between Gold-DPC and Auto-DPC. Figure 2: The distribution of Kendall’s tau values for 2, 236 bilingual sentences (Chinese-Japanese) in which the Chinese is from three systems of baseline, Auto-DPC, and Gold-DPC. file, ch-ja.A3.final. The comparison implies how monotonically the Chinese sentences have been reordered to align with Japanese. We use MeCab6 (Kudo and Matsumoto, 2000) to segment Japanese sentences and also filter out sentences with more than 64 tokens. There are 2, 236 valid Chinese-Japanese bilingual sentences in total. Figure 2 shows the distribution of Kendall’s tau from three systems in which the baseline is built up by using ordinary Chinese. In Figure 2, baseline system contains a large numbers of non-monotonic aligned sentences, whereas both Auto-DPC and Gold-DPC increased the amount of sentences that achieved high τ values. Reordering based on gold-tree reduced more percentage of low τ sentences than reordering based on automatically parsed trees."
Y13-1025,C10-1043,0,0.0128481,"ord correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insig"
Y13-1025,D07-1013,0,0.0109541,"noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the benchmark in scenario 2 and"
Y13-1025,gimenez-marquez-2008-towards,0,0.0255363,"Missing"
Y13-1025,J08-1002,0,0.0198695,"he work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the general figure of the upper bound of the reordering method. However, since it is not only time-consuming but also labor-intensive to set up the benchmark in scenario 1, we use the Japanese reference as the b"
Y13-1025,P11-3013,0,0.0134408,"ependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing erro"
Y13-1025,W06-1608,0,0.0117355,"of parsing errors on reordering performance. In this analysis, we borrow this state-of-the-art pre-reordering model for our experiments since it is a rule-based pre-reordering method for a distant language pair based on dependency parsing as well as its extensibility to other language pairs. 2.2 Related Work Although there are studies on analyzing parsing errors and reordering errors, as far as we know, there is not any work on observing the relationship between these two types of errors. One most relevant work to ours is observing the impact of parsing accuracy on a SMT system introduced in Quirk and Corston-Oliver (2006). They showed the general idea that syntax-based SMT models are sensitive to syntactic analysis. However, they did not further analyze concrete parsing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. B"
Y13-1025,W12-4207,0,0.0531,"ons of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. The contribution of this work is two fold"
Y13-1025,I11-1035,0,0.0167424,"nce separately, we quantify the extent of parsing errors that 4 4.1 Preliminary Experiment Gold Data In order to build up gold parse tree sets for comparison, we used the annotated sentences from Chinese Penn Treebank ver. 7.0 (CTB-7) which is a well known corpus that consists of parsed text in five genres. They are Chinese newswire (NS), magazine news (NM), broadcast news (BN), broadcast conversation programs (BC), and web newsgroups, weblogs (NW). We first randomly selected 517 unique sentences (hereinafter set-1) from all five genres in development set of CTB-7 which is split according to (Wang et al., 2011). However, we found that sentences in BC and NW are mainly from spoken language, which tend to have faults like repetitions, incomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out th"
Y13-1025,W13-2806,0,0.0223562,"Missing"
Y13-1025,C04-1073,0,0.0206232,"plore every possible word correspondence due to the combinatorial complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing er"
Y13-1025,N09-1028,0,0.0264066,"complexity. Considering local permutations of words might be effective to translate languages with a similar sentence structure, but these methods have a limited performance when translating sentences from languages with different syntactical structures. An effective technique to translate sentences between distant language pairs is pre-reordering, where words in sentences from the source language are re-arranged with the objective to resemble the word order of the target language. Rearranging rules are automatically extracted (Xia and McCord, 2004; Genzel, 2010), or linguistically motivated (Xu et al., 2009; Isozaki et al., 2010; Han et al., 2012; Han et al., 2013). We work following the latter strategy, where the source sentence is parsed to find its syntactical structure, and linguistically-motivated rules are used in combination with the structure of the sentence to guide the word reordering. The language pair under consideration is Chinese-to-Japanese, which despite their common roots, it is a well known language pair for their different sentence structure. However, syntax-based pre-reordering techniques are sensitive to parsing errors, but insight into their relationship has been elusive. T"
Y13-1025,D09-1121,0,0.0138352,"sing error types that affect task accuracy. Green (2011) explored the effects of noun phrase bracketing in dependency parsing in English, and further on English to Czech machine translation. But the work focused on using noun phrase structure to improve a machine translation framework. In the work of Katz-Brown et al. (2011), they proposed a training method to improve a parser’s performance by using reordering quality to examine the parse quality. But they did not study the relationship between reordering quality and parse quality. There are more works on parsing error analysis. For instance, Hara et al. (2009) defined several types of parsing error patterns on predicate argument relation and tested them with a Headdriven phrase structure grammar (HPSG) (Pollard and Sag, 1994) parser (Miyao and Tsujii, 2008). McDonald and Nivre (2007) explored parsing errors for data-driven dependency parsing by 1 In this work, POS tag definitions follow the POS tag guidelines of the Penn Chinese Treebank v3.0. 2 According to (Han et al., 2013), a Vb includes the head of the Vb (Vb-H) and an optional component (Vb-D). 268 PACLIC-27 influence reordering. Meanwhile, the former measurement shows additionally the genera"
Y13-1025,W11-2907,0,0.0201923,"ubclasses according to the methodology of the reordering method. We then plot the distribution of these parsing errors for various reordering qualities. In Section 5.2, we illustrate these parsing errors with examples. comparing a graph-based parser with a transitionbased parser, which are representing two dominant parsing models. At the same time, Dredze et al. (2007) provided a comparison analysis on differences in annotation guidelines among treebanks which were suspected to be responsible for dependency parsing errors in domain adaptation tasks. Unlike analyzing parsing errors, authors in Yu et al. (2011) focused on the difficulties in Chinese deep parsing by comparing the linguistic properties between Chinese and English. There are also works on reordering error analysis like Han et al. (2012) which examined an existing reordering method and refined it after a detailed linguistic analysis on reordering issues. Although they discovered that parsing errors affect the reordering quality, they did not observe the concrete relationship. On the other hand, Gim´enez and M`arquez (2008) proposed an automatic error analysis method of machine translation output, by compiling a set of metric variants. H"
Y13-1025,I11-1136,0,0.0206914,"ncomplete sentences, corrections, or incorrect sentence segmentation. Therefore, we randomly selected another 2, 126 unique sentences 269 PACLIC-27 set-1 set-2 Total AL Voc. BN 100 797 897 29.8 5.5K BC 100 100 20.0 690 NM 100 578 678 33.5 5K NS 117 751 868 28.4 5.1K NW 100 100 25.9 972 Total 517 2, 126 2, 643 29.8 9.5K M-reordered Gold-DPC Auto-DPC 0.88 0.95 nese sentences. Word alignments are produced by MGIZA++ (Gao and Vogel, 2008). In both scenarios, we carry out the reordering method DPC (See Section 2.1). Auto-parse trees are generated by an unlabeled Chinese dependency parser, Corbit4 (Hatori et al., 2011). Gold trees5 are converted from CTB-7 parsed text which are created by human annotators. More specifically, we refer to auto-parse tree based reordering system as Auto-DPC and to gold-tree based reordering system as Gold-DPC. Baseline system uses unreordered Chinese sentences. Scenario 1 Preliminary observation about the effects of parsing errors on reordering performance is to compare word order similarities between manually reordered Chinese sentences and automatically reordered Chinese sentences from set-1. Table 3 shows the average τ value. For baseline system, the average τ value shows h"
Y13-1025,D07-1112,0,\N,Missing
Y13-1025,P07-1109,0,\N,Missing
