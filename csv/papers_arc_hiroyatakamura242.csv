2021.naacl-industry.32,An Empirical Study of Generating Texts for Search Engine Advertising,2021,-1,-1,3,0.850102,3633,hidetaka kamigaito,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers,0,"Although there are many studies on neural language generation (NLG), few trials are put into the real world, especially in the advertising domain. Generating ads with NLG models can help copywriters in their creation. However, few studies have adequately evaluated the effect of generated ads with actual serving included because it requires a large amount of training data and a particular environment. In this paper, we demonstrate a practical use case of generating ad-text with an NLG model. Specially, we show how to improve the ads{'} impact, deploy models to a product, and evaluate the generated ads."
2021.inlg-1.11,"Generating Racing Game Commentary from Vision, Language, and Structured Data",2021,-1,-1,7,1,5924,tatsuya ishigaki,Proceedings of the 14th International Conference on Natural Language Generation,0,"We propose the task of automatically generating commentaries for races in a motor racing game, from vision, structured numerical, and textual data. Commentaries provide information to support spectators in understanding events in races. Commentary generation models need to interpret the race situation and generate the correct content at the right moment. We divide the task into two subtasks: utterance timing identification and utterance generation. Because existing datasets do not have such alignments of data in multiple modalities, this setting has not been explored in depth. In this study, we introduce a new large-scale dataset that contains aligned video data, structured numerical data, and transcribed commentaries that consist of 129,226 utterances in 1,389 races in a game. Our analysis reveals that the characteristics of commentaries change over time or from viewpoints. Our experiments on the subtasks show that it is still challenging for a state-of-the-art vision encoder to capture useful information from videos to generate accurate commentaries. We make the dataset and baseline implementation publicly available for further research."
2021.inlg-1.42,{G}raph{P}lan: Story Generation by Planning with Event Graph,2021,-1,-1,3,0,5993,hong chen,Proceedings of the 14th International Conference on Natural Language Generation,0,"Story generation is a task that aims to automatically generate a meaningful story. This task is challenging because it requires high-level understanding of the semantic meaning of sentences and causality of story events. Naivesequence-to-sequence models generally fail to acquire such knowledge, as it is difficult to guarantee logical correctness in a text generation model without strategic planning. In this study, we focus on planning a sequence of events assisted by event graphs and use the events to guide the generator. Rather than using a sequence-to-sequence model to output a sequence, as in some existing works, we propose to generate an event sequence by walking on an event graph. The event graphs are built automatically based on the corpus. To evaluate the proposed approach, we incorporate human participation, both in event planning and story generation. Based on the largescale human annotation results, our proposed approach has been shown to provide more logically correct event sequences and stories compared with previous approaches."
2021.findings-emnlp.128,{S}ci{XG}en: A Scientific Paper Dataset for Context-Aware Text Generation,2021,-1,-1,2,0,5993,hong chen,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Generating texts in scientific papers requires not only capturing the content contained within the given input but also frequently acquiring the external information called context. We push forward the scientific text generation by proposing a new task, namely context-aware text generation in the scientific domain, aiming at exploiting the contributions of context in generated texts. To this end, we present a novel challenging large-scale Scientific Paper Dataset for ConteXt-Aware Text Generation (SciXGen), consisting of well-annotated 205,304 papers with full references to widely-used objects (e.g., tables, figures, algorithms) in a paper. We comprehensively benchmark, using state-of-the-arts, the efficacy of our newly constructed SciXGen dataset in generating description and paragraph. Our dataset and benchmarks will be made publicly available to hopefully facilitate the scientific text generation research."
2021.eacl-main.125,Generating Weather Comments from Meteorological Simulations,2021,-1,-1,6,1,10697,soichiro murakami,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"The task of generating weather-forecast comments from meteorological simulations has the following requirements: (i) the changes in numerical values for various physical quantities need to be considered, (ii) the weather comments should be dependent on delivery time and area information, and (iii) the comments should provide useful information for users. To meet these requirements, we propose a data-to-text model that incorporates three types of encoders for numerical forecast maps, observation data, and meta-data. We also introduce weather labels representing weather information, such as sunny and rain, for our model to explicitly describe useful information. We conducted automatic and human evaluations. The results indicate that our model performed best against baselines in terms of informativeness. We make our code and data publicly available."
2021.eacl-main.267,Metric-Type Identification for Multi-Level Header Numerical Tables in Scientific Papers,2021,-1,-1,4,0,10902,lya suadaa,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Numerical tables are widely used to present experimental results in scientific papers. For table understanding, a metric-type is essential to discriminate numbers in the tables. We introduce a new information extraction task, metric-type identification from multi-level header numerical tables, and provide a dataset extracted from scientific papers consisting of header tables, captions, and metric-types. We then propose two joint-learning neural classification and generation schemes featuring pointer-generator-based and BERT-based models. Our results show that the joint models can handle both in-header and out-of-header metric-type identification problems."
2021.eacl-main.296,One-class Text Classification with Multi-modal Deep Support Vector Data Description,2021,-1,-1,4,0,10949,chenlong hu,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"This work presents multi-modal deep SVDD (mSVDD) for one-class text classification. By extending the uni-modal SVDD to a multiple modal one, we build mSVDD with multiple hyperspheres, that enable us to build a much better description for target one-class data. Additionally, the end-to-end architecture of mSVDD can jointly handle neural feature learning and one-class text learning. We also introduce a mechanism for incorporating negative supervision in the absence of real negative data, which can be beneficial to the mSVDD model. We conduct experiments on Reuters and 20 Newsgroup datasets, and the experimental results demonstrate that mSVDD outperforms uni-modal SVDD and mSVDD can get further improvements when negative supervision is incorporated."
2021.acl-long.115,Towards Table-to-Text Generation with Numerical Reasoning,2021,-1,-1,5,0,10902,lya suadaa,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Recent neural text generation models have shown significant improvement in generating descriptive text from structured data such as table formats. One of the remaining important challenges is generating more analytical descriptions that can be inferred from facts in a data source. The use of a template-based generator and a pointer-generator is among the potential alternatives for table-to-text generators. In this paper, we propose a framework consisting of a pre-trained model and a copy mechanism. The pre-trained models are fine-tuned to produce fluent text that is enriched with numerical reasoning. However, it still lacks fidelity to the table contents. The copy mechanism is incorporated in the fine-tuning step by using general placeholders to avoid producing hallucinated phrases that are not supported by a table while preserving high fluency. In summary, our contributions are (1) a new dataset for numerical table-to-text generation using pairs of a table and a paragraph of a table description with richer inference from scientific papers, and (2) a table-to-text generation framework enriched with numerical reasoning."
2020.wnut-1.38,mgsohrab at {WNUT} 2020 Shared Task-1: Neural Exhaustive Approach for Entity and Relation Recognition Over Wet Lab Protocols,2020,-1,-1,4,1,13706,mohammad sohrab,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),0,"We present a neural exhaustive approach that addresses named entity recognition (NER) and relation recognition (RE), for the entity and re- lation recognition over the wet-lab protocols shared task. We introduce BERT-based neural exhaustive approach that enumerates all pos- sible spans as potential entity mentions and classifies them into entity types or no entity with deep neural networks to address NER. To solve relation extraction task, based on the NER predictions or given gold mentions we create all possible trigger-argument pairs and classify them into relation types or no relation. In NER task, we achieved 76.60{\%} in terms of F-score as third rank system among the partic- ipated systems. In relation extraction task, we achieved 80.46{\%} in terms of F-score as the top system in the relation extraction or recognition task. Besides we compare our model based on the wet lab protocols corpus (WLPC) with the WLPC baseline and dynamic graph-based in- formation extraction (DyGIE) systems."
2020.webnlg-1.18,Text-to-Text Pre-Training Model with Plan Selection for {RDF}-to-Text Generation,2020,-1,-1,2,0,14085,natthawut kertkeidkachorn,Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+),0,"We report our system description for the RDFto-Text task in English on the WebNLG 2020 Challenge. Our approach consists of two parts: 1) RDF-to-Text Generation Pipeline and 2) Plan Selection. RDF-to-Text Generation Pipeline is built on the state-of-the-art pretraining model, while Plan Selection helps decide the proper plan into the pipeline."
2020.inlg-1.21,Market Comment Generation from Data with Noisy Alignments,2020,-1,-1,5,0,5926,yumi hamazono,Proceedings of the 13th International Conference on Natural Language Generation,0,"End-to-end models on data-to-text learn the mapping of data and text from the aligned pairs in the dataset. However, these alignments are not always obtained reliably, especially for the time-series data, for which real time comments are given to some situation and there might be a delay in the comment delivery time compared to the actual event time. To handle this issue of possible noisy alignments in the dataset, we propose a neural network model with multi-timestep data and a copy mechanism, which allows the models to learn the correspondences between data and text from the dataset with noisier alignments. We focus on generating market comments in Japanese that are delivered each time an event occurs in the market. The core idea of our approach is to utilize multi-timestep data, which is not only the latest market price data when the comment is delivered, but also the data obtained at several timesteps earlier. On top of this, we employ a copy mechanism that is suitable for referring to the content of data records in the market price data. We confirm the superiority of our proposal by two evaluation metrics and show the accuracy improvement of the sentence generation using the time series data by our proposed method."
2020.coling-main.28,Pointing to Subwords for Generating Function Names in Source Code,2020,-1,-1,3,0,21071,shogo fujita,Proceedings of the 28th International Conference on Computational Linguistics,0,"We tackle the task of automatically generating a function name from source code. Existing generators face difficulties in generating low-frequency or out-of-vocabulary subwords. In this paper, we propose two strategies for copying low-frequency or out-of-vocabulary subwords in inputs. Our best performing model showed an improvement over the conventional method in terms of our modified F1 and accuracy on the Java-small and Java-large datasets."
2020.coling-main.192,Neural text normalization leveraging similarities of strings and sounds,2020,-1,-1,4,0,21285,riku kawamura,Proceedings of the 28th International Conference on Computational Linguistics,0,"We propose neural models that can normalize text by considering the similarities of word strings and sounds. We experimentally compared a model that considers the similarities of both word strings and sounds, a model that considers only the similarity of word strings or of sounds, and a model without the similarities as a baseline. Results showed that leveraging the word string similarity succeeded in dealing with misspellings and abbreviations, and taking into account the sound similarity succeeded in dealing with phonetic substitutions and emphasized characters. So that the proposed models achieved higher F1 scores than the baseline."
2020.coling-main.213,Learning with Contrastive Examples for Data-to-Text Generation,2020,-1,-1,7,0,19017,yui uehara,Proceedings of the 28th International Conference on Computational Linguistics,0,"Existing models for data-to-text tasks generate fluent but sometimes incorrect sentences e.g., {``}Nikkei gains{''} is generated when {``}Nikkei drops{''} is expected. We investigate models trained on contrastive examples i.e., incorrect sentences or terms, in addition to correct ones to reduce such errors. We first create rules to produce contrastive examples from correct ones by replacing frequent crucial terms such as {``}gain{''} or {``}drop{''}. We then use learning methods with several losses that exploit contrastive examples. Experiments on the market comment generation task show that 1) exploiting contrastive examples improves the capability of generating sentences with better lexical choice, without degrading the fluency, 2) the choice of the loss function is an important factor because the performances on different metrics depend on the types of loss functions, and 3) the use of the examples produced by some specific rules further improves performance. Human evaluation also supports the effectiveness of using contrastive examples."
2020.coling-main.465,An empirical analysis of existing systems and datasets toward general simple question answering,2020,-1,-1,4,0,17066,namgi han,Proceedings of the 28th International Conference on Computational Linguistics,0,"In this paper, we evaluate the progress of our field toward solving simple factoid questions over a knowledge base, a practically important problem in natural language interface to database. As in other natural language understanding tasks, a common practice for this task is to train and evaluate a model on a single dataset, and recent studies suggest that SimpleQuestions, the most popular and largest dataset, is nearly solved under this setting. However, this common setting does not evaluate the robustness of the systems outside of the distribution of the used training data. We rigorously evaluate such robustness of existing systems using different datasets. Our analysis, including shifting of training and test datasets and training on a union of the datasets, suggests that our progress in solving SimpleQuestions dataset does not indicate the success of more general simple question answering. We discuss a possible future direction toward this goal."
2020.coling-main.507,A Neural Model for Aggregating Coreference Annotation in Crowdsourcing,2020,-1,-1,2,0,21605,maolin li,Proceedings of the 28th International Conference on Computational Linguistics,0,"Coreference resolution is the task of identifying all mentions in a text that refer to the same real-world entity. Collecting sufficient labelled data from expert annotators to train a high-performance coreference resolution system is time-consuming and expensive. Crowdsourcing makes it possible to obtain the required amounts of data rapidly and cost-effectively. However, crowd-sourced labels can be noisy. To ensure high-quality data, it is crucial to infer the correct labels by aggregating the noisy labels. In this paper, we split the aggregation into two subtasks, i.e, mention classification and coreference chain inference. Firstly, we predict the general class of each mention using an autoencoder, which incorporates contextual information about each mention, while at the same time taking into account the mention{'}s annotation complexity and annotators{'} reliability at different levels. Secondly, to determine the coreference chain of each mention, we use weighted voting which takes into account the learned reliability in the first subtask. Experimental results demonstrate the effectiveness of our method in predicting the correct labels. We also illustrate our model{'}s interpretability through a comprehensive analysis of experimental results."
2020.acl-main.309,An Analysis of the Utility of Explicit Negative Examples to Improve the Syntactic Abilities of Neural Language Models,2020,22,1,2,0,5927,hiroshi noji,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We explore the utilities of explicit negative examples in training neural language models. Negative examples here are incorrect words in a sentence, such as \textit{barks} in *\textit{The dogs barks}. Neural language models are commonly trained only on positive examples, a set of sentences in the training data, but recent studies suggest that the models trained in this way are not capable of robustly handling complex syntactic constructions, such as long-distance agreement. In this paper, we first demonstrate that appropriately using negative examples about particular constructions (e.g., subject-verb agreement) will boost the model{'}s robustness on them in English, with a negligible loss of perplexity. The key to our success is an additional margin loss between the log-likelihoods of a correct word and an incorrect word. We then provide a detailed analysis of the trained models. One of our findings is the difficulty of object-relative clauses for RNNs. We find that even with our direct learning signals the models still suffer from resolving agreement across an object-relative clause. Augmentation of training sentences involving the constructions somewhat helps, but the accuracy still does not reach the level of subject-relative clauses. Although not directly cognitively appealing, our method can be a tool to analyze the true architectural limitation of neural models on challenging linguistic constructions."
2020.aacl-main.10,A Simple and Effective Usage of Word Clusters for {CBOW} Model,2020,-1,-1,4,1,7870,yukun feng,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"We propose a simple and effective method for incorporating word clusters into the Continuous Bag-of-Words (CBOW) model. Specifically, we propose to replace infrequent input and output words in CBOW model with their clusters. The resulting cluster-incorporated CBOW model produces embeddings of frequent words and a small amount of cluster embeddings, which will be fine-tuned in downstream tasks. We empirically show our replacing method works well on several downstream tasks. Through our analysis, we show that our method might be also useful for other similar models which produce word embeddings."
W19-8640,Controlling Contents in Data-to-Document Generation with Human-Designed Topic Labels,2019,0,0,8,0,21311,kasumi aoki,Proceedings of the 12th International Conference on Natural Language Generation,0,"We propose a data-to-document generator that can easily control the contents of output texts based on a neural language model. Conventional data-to-text model is useful when a reader seeks a global summary of data because it has only to describe an important part that has been extracted beforehand. However, because depending on users, it differs what they are interested in, so it is necessary to develop a method to generate various summaries according to users{'} interests. We develop a model to generate various summaries and to control their contents by providing the explicit targets for a reference to the model as controllable factors. In the experiments, we used five-minute or one-hour charts of 9 indicators (e.g., Nikkei225), as time-series data, and daily summaries of Nikkei Quick News as textual data. We conducted comparative experiments using two pieces of information: human-designed topic labels indicating the contents of a sentence and automatically extracted keywords as the referential information for generation."
R19-1059,Discourse-Aware Hierarchical Attention Network for Extractive Single-Document Summarization,2019,0,0,3,1,5924,tatsuya ishigaki,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Discourse relations between sentences are often represented as a tree, and the tree structure provides important information for summarizers to create a short and coherent summary. However, current neural network-based summarizers treat the source document as just a sequence of sentences and ignore the tree-like discourse structure inherent in the document. To incorporate the information of a discourse tree structure into the neural network-based summarizers, we propose a discourse-aware neural extractive summarizer which can explicitly take into account the discourse dependency tree structure of the source document. Our discourse-aware summarizer can jointly learn the discourse structure and the salience score of a sentence by using novel hierarchical attention modules, which can be trained on automatically parsed discourse dependency trees. Experimental results showed that our model achieved competitive or better performances against state-of-the-art models in terms of ROUGE scores on the DailyMail dataset. We further conducted manual evaluations. The results showed that our approach also gained the coherence of the output summaries."
P19-1099,Global Optimization under Length Constraint for Neural Text Summarization,2019,0,5,3,0,25608,takuya makino,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a global optimization method under length constraint (GOLC) for neural text summarization models. GOLC increases the probabilities of generating summaries that have high evaluation scores, ROUGE in this paper, within a desired length. We compared GOLC with two optimization methods, a maximum log-likelihood and a minimum risk training, on CNN/Daily Mail and a Japanese single document summarization data set of The Mainichi Shimbun Newspapers. The experimental results show that a state-of-the-art neural summarization model optimized with GOLC generates fewer overlength summaries while maintaining the fastest processing speed; only 6.70{\%} overlength summaries on CNN/Daily and 7.8{\%} on long summary of Mainichi, compared to the approximately 20{\%} to 50{\%} on CNN/Daily Mail and 10{\%} to 30{\%} on Mainichi with the other optimization methods. We also demonstrate the importance of the generation of in-length summaries for post-editing with the dataset Mainich that is created with strict length constraints. The ex- perimental results show approximately 30{\%} to 40{\%} improved post-editing time by use of in-length summaries."
P19-1202,"Learning to Select, Track, and Generate for Data-to-Text",2019,0,1,9,0,7220,hayate iso,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We propose a data-to-text generation model with two modules, one for tracking and the other for text generation. Our tracking module selects and keeps track of salient information and memorizes which record has been mentioned. Our generation module generates a summary conditioned on the state of tracking module. Our proposed model is considered to simulate the human-like writing process that gradually selects the information by determining the intermediate variables while writing the summary. In addition, we also explore the effectiveness of the writer information for generations. Experimental results show that our proposed model outperforms existing models in all evaluation metrics even without writer information. Incorporating writer information further improves the performance, contributing to content planning and surface realization."
P19-1635,Numeracy-600{K}: Learning Numeracy for Detecting Exaggerated Information in Market Comments,2019,0,5,3,0,8626,chungchi chen,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we attempt to answer the question of whether neural network models can learn numeracy, which is the ability to predict the magnitude of a numeral at some specific position in a text description. A large benchmark dataset, called Numeracy-600K, is provided for the novel task. We explore several neural network models including CNN, GRU, BiGRU, CRNN, CNN-capsule, GRU-capsule, and BiGRU-capsule in the experiments. The results show that the BiGRU model gets the best micro-averaged F1 score of 80.16{\%}, and the GRU-capsule model gets the best macro-averaged F1 score of 64.71{\%}. Besides discussing the challenges through comprehensive experiments, we also present an important application scenario, i.e., detecting exaggerated information, for the task."
K19-1086,A Simple and Effective Method for Injecting Word-Level Information into Character-Aware Neural Language Models,2019,0,0,3,1,7870,yukun feng,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"We propose a simple and effective method to inject word-level information into character-aware neural language models. Unlike previous approaches which usually inject word-level information at the input of a long short-term memory (LSTM) network, we inject it into the softmax function. The resultant model can be seen as a combination of character-aware language model and simple word-level language model. Our injection method can also be used together with previous methods. Through the experiments on 14 typologically diverse languages, we empirically show that our injection method, when used together with the previous methods, works better than the previous methods, including a gating mechanism, averaging, and concatenation of word vectors. We also provide a comprehensive comparison of these injection methods."
D19-5708,A Neural Pipeline Approach for the {P}harma{C}o{NER} Shared Task using Contextual Exhaustive Models,2019,0,0,4,1,13706,mohammad sohrab,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"We present a neural pipeline approach that performs named entity recognition (NER) and concept indexing (CI), which links them to concept unique identifiers (CUIs) in a knowledge base, for the PharmaCoNER shared task on pharmaceutical drugs and chemical entities. We proposed a neural NER model that captures the surrounding semantic information of a given sequence by capturing the forward- and backward-context of bidirectional LSTM (Bi-LSTM) output of a target span using contextual span representation-based exhaustive approach. The NER model enumerates all possible spans as potential entity mentions and classify them into entity types or no entity with deep neural networks. For representing span, we compare several different neural network architectures and their ensembling for the NER model. We then perform dictionary matching for CI and, if there is no matching, we further compute similarity scores between a mention and CUIs using entity embeddings to assign the CUI with the highest score to the mention. We evaluate our approach on the two sub-tasks in the shared task. Among the five submitted runs, the best run for each sub-task achieved the F-score of 86.76{\%} on Sub-task 1 (NER) and the F-score of 79.97{\%} (strict) on Sub-task 2 (CI)."
D19-5727,Coreference Resolution in Full Text Articles with {BERT} and Syntax-based Mention Filtering,2019,0,0,5,0,26538,hailong trieu,Proceedings of The 5th Workshop on BioNLP Open Shared Tasks,0,"This paper describes our system developed for the coreference resolution task of the CRAFT Shared Tasks 2019. The CRAFT corpus is more challenging than other existing corpora because it contains full text articles. We have employed an existing span-based state-of-theart neural coreference resolution system as a baseline system. We enhance the system with two different techniques to capture longdistance coreferent pairs. Firstly, we filter noisy mentions based on parse trees with increasing the number of antecedent candidates. Secondly, instead of relying on the LSTMs, we integrate the highly expressive language model{--}BERT into our model. Experimental results show that our proposed systems significantly outperform the baseline. The best performing system obtained F-scores of 44{\%}, 48{\%}, 39{\%}, 49{\%}, 40{\%}, and 57{\%} on the test set with B3, BLANC, CEAFE, CEAFM, LEA, and MUC metrics, respectively. Additionally, the proposed model is able to detect coreferent pairs in long distances, even with a distance of more than 200 sentences."
W18-6510,Stylistically User-Specific Generation,2018,0,0,2,0,27655,abdurrisyad fikri,Proceedings of the 11th International Conference on Natural Language Generation,0,"Recent neural models for response generation show good results in terms of general responses. In real conversations, however, depending on the speaker/responder, similar utterances should require different responses. In this study, we attempt to consider individual user{'}s information in adjusting the notable sequence-to-sequence (seq2seq) model for more diverse, user-specific responses. We assume that we need user-specific features to adjust the response and we argue that some selected representative words from the users are suitable for this task. Furthermore, we prove that even for unseen or unknown users, our model can provide more diverse and interesting responses, while maintaining correlation with input utterances. Experimental results with human evaluation show that our model can generate more interesting responses than the popular seq2seqmodel and achieve higher relevance with input utterances than our baseline."
W18-6515,Generating Market Comments Referring to External Resources,2018,0,3,7,1,21286,tatsuya aoki,Proceedings of the 11th International Conference on Natural Language Generation,0,"Comments on a stock market often include the reason or cause of changes in stock prices, such as {``}Nikkei turns lower as yen{'}s rise hits exporters.{''} Generating such informative sentences requires capturing the relationship between different resources, including a target stock price. In this paper, we propose a model for automatically generating such informative market comments that refer to external resources. We evaluated our model through an automatic metric in terms of BLEU and human evaluation done by an expert in finance. The results show that our model outperforms the existing model both in BLEU scores and human judgment."
C18-1202,Exploring the Influence of Spelling Errors on Lexical Variation Measures,2018,0,0,3,0,5976,ryo nagata,Proceedings of the 27th International Conference on Computational Linguistics,0,"This paper explores the influence of spelling errors on lexical variation measures. Lexical richness measures such as Type-Token Ration (TTR) and Yule{'}s K are often used for learner English analysis and assessment. When applied to learner English, however, they can be unreliable because of the spelling errors appearing in it. Namely, they are, directly or indirectly, based on the counts of distinct word types, and spelling errors undesirably increase the number of distinct words. This paper introduces and examines the hypothesis that lexical richness measures become unstable in learner English because of spelling errors. Specifically, it tests the hypothesis on English learner corpora of three groups (middle school, high school, and college students). To be precise, it estimates the difference in TTR and Yule{'}s K caused by spelling errors, by calculating their values before and after spelling errors are manually corrected. Furthermore, it examines the results theoretically and empirically to deepen the understanding of the influence of spelling errors on them."
C18-1274,Neural Machine Translation Incorporating Named Entity,2018,0,5,4,0,30905,arata ugawa,Proceedings of the 27th International Conference on Computational Linguistics,0,"This study proposes a new neural machine translation (NMT) model based on the encoder-decoder model that incorporates named entity (NE) tags of source-language sentences. Conventional NMT models have two problems enumerated as follows: (i) they tend to have difficulty in translating words with multiple meanings because of the high ambiguity, and (ii) these models{'}abilitytotranslatecompoundwordsseemschallengingbecausetheencoderreceivesaword, a part of the compound word, at each time step. To alleviate these problems, the encoder of the proposed model encodes the input word on the basis of its NE tag at each time step, which could reduce the ambiguity of the input word. Furthermore,the encoder introduces a chunk-level LSTM layer over a word-level LSTM layer and hierarchically encodes a source-language sentence to capture a compound NE as a chunk on the basis of the NE tags. We evaluate the proposed model on an English-to-Japanese translation task with the ASPEC, and English-to-Bulgarian and English-to-Romanian translation tasks with the Europarl corpus. The evaluation results show that the proposed model achieves up to 3.11 point improvement in BLEU."
S17-2061,{UINSUSKA}-{T}i{T}ech at {S}em{E}val-2017 Task 3: Exploiting Word Importance Levels for Similarity Features for {CQA},2017,0,0,2,0,32291,surya agustian,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"The majority of core techniques to solve many problems in Community Question Answering (CQA) task rely on similarity computation. This work focuses on similarity between two sentences (or questions in subtask B) based on word embeddings. We exploit words importance levels in sentences or questions for similarity features, for classification and ranking with machine learning. Using only 2 types of similarity metric, our proposed method has shown comparable results with other complex systems. This method on subtask B 2017 dataset is ranked on position 7 out of 13 participants. Evaluation on 2016 dataset is on position 8 of 12, outperforms some complex systems. Further, this finding is explorable and potential to be used as baseline and extensible for many tasks in CQA and other textual similarity based system."
P17-2044,{J}apanese Sentence Compression with a Large Training Dataset,2017,8,1,3,0,32567,shun hasegawa,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In English, high-quality sentence compression models by deleting words have been trained on automatically created large training datasets. We work on Japanese sentence compression by a similar approach. To create a large Japanese training dataset, a method of creating English training dataset is modified based on the characteristics of the Japanese language. The created dataset is used to train Japanese sentence compression models based on the recurrent neural network."
P17-1126,Learning to Generate Market Comments from Stock Prices,2017,0,13,6,1,10697,soichiro murakami,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a novel encoder-decoder model for automatically generating market comments from stock prices. The model first encodes both short- and long-term series of stock prices so that it can mention short- and long-term changes in stock prices. In the decoding phase, our model can also generate a numerical value by selecting an appropriate arithmetic operation such as subtraction or rounding, and applying it to the input stock prices. Empirical experiments show that our best model generates market comments at the fluency and the informativeness approaching human-generated reference texts."
I17-2002,Supervised Attention for Sequence-to-Sequence Constituency Parsing,2017,14,3,4,1,3633,hidetaka kamigaito,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"The sequence-to-sequence (Seq2Seq) model has been successfully applied to machine translation (MT). Recently, MT performances were improved by incorporating supervised attention into the model. In this paper, we introduce supervised attention to constituency parsing that can be regarded as another translation task. Evaluation results on the PTB corpus showed that the bracketing F-measure was improved by supervised attention."
I17-1080,Summarizing Lengthy Questions,2017,19,2,2,1,5924,tatsuya ishigaki,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this research, we propose the task of question summarization. We first analyzed question-summary pairs extracted from a Community Question Answering (CQA) site, and found that a proportion of questions cannot be summarized by extractive approaches but requires abstractive approaches. We created a dataset by regarding the question-title pairs posted on the CQA site as question-summary pairs. By using the data, we trained extractive and abstractive summarization models, and compared them based on ROUGE scores and manual evaluations. Our experimental results show an abstractive method using an encoder-decoder model with a copying mechanism achieves better scores for both ROUGE-2 F-measure and the evaluations by human judges."
E17-1112,Analyzing Semantic Change in {J}apanese Loanwords,2017,17,5,1,1,4800,hiroya takamura,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"We analyze semantic changes in loanwords from English that are used in Japanese (Japanese loanwords). Specifically, we create word embeddings of English and Japanese and map the Japanese embeddings into the English space so that we can calculate the similarity of each Japanese word and each English word. We then attempt to find loanwords that are semantically different from their original, see if known meaning changes are correctly captured, and show the possibility of using our methodology in language education."
D17-1246,Distinguishing {J}apanese Non-standard Usages from Standard Ones,2017,11,0,3,1,21286,tatsuya aoki,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We focus on non-standard usages of common words on social media. In the context of social media, words sometimes have other usages that are totally different from their original. In this study, we attempt to distinguish non-standard usages on social media from standard ones in an unsupervised manner. Our basic idea is that non-standardness can be measured by the inconsistency between the expected meaning of the target word and the given context. For this purpose, we use context embeddings derived from word embeddings. Our experimental results show that the model leveraging the context embedding outperforms other methods and provide us with findings, for example, on how to construct context embeddings and which corpus to use."
L16-1011,Discriminative Analysis of Linguistic Features for Typological Study,2016,0,5,1,1,4800,hiroya takamura,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We address the task of automatically estimating the missing values of linguistic features by making use of the fact that some linguistic features in typological databases are informative to each other. The questions to address in this work are (i) how much predictive power do features have on the value of another feature? (ii) to what extent can we attribute this predictive power to genealogical or areal factors, as opposed to being provided by tendencies or implicational universals? To address these questions, we conduct a discriminative or predictive analysis on the typological database. Specifically, we use a machine-learning classifier to estimate the value of each feature of each language using the values of the other features, under different choices of training data: all the other languages, or all the other languages except for the ones having the same origin or area with the target language."
D16-1140,Controlling Output Length in Neural Encoder-Decoders,2016,38,26,4,1,27810,yuta kikuchi,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task."
D16-1210,Unsupervised Word Alignment by Agreement Under {ITG} Constraint,2016,25,3,3,1,3633,hidetaka kamigaito,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
N15-1143,Estimating Numerical Attributes by Bringing Together Fragmentary Clues,2015,8,7,1,1,4800,hiroya takamura,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This work is an attempt to automatically obtain numerical attributes of physical objects. We propose representing each physical object as a feature vector and representing sizes as linear functions of feature vectors. We train the function in the framework of the combined regression and ranking with many types of fragmentary clues including absolute clues (e.g., A is 30cm long) and relative clues (e.g., A is larger than B)."
N15-1150,Context-Dependent Automatic Response Generation Using Statistical Machine Translation Techniques,2015,18,3,3,0,8089,andrew shin,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Developing a system that can automatically respond to a userxe2x80x99s utterance has recently become a topic of research in natural language processing. However, most works on the topic take into account only a single preceding utterance to generate a response. Recent works demonstrate that the application of statistical machine translation (SMT) techniques towards monolingual dialogue setting, in which a response is treated as a translation of a stimulus, has a great potential, and we exploit the approach to tackle the context-dependent response generation task. We attempt to extract relevant and significant information from the wider contextual scope of the conversation, and incorporate it into the SMT techniques. We also discuss the advantages and limitations of this approach through our experimental results."
D15-1143,Hierarchical Back-off Modeling of {H}iero Grammar based on Non-parametric {B}ayesian Model,2015,32,1,3,1,3633,hidetaka kamigaito,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In hierarchical phrase-based machine translation, a rule table is automatically learned by heuristically extracting synchronous rules from a parallel corpus. As a result, spuriously many rules are extracted which may be composed of various incorrect rules. The larger rule table incurs more run time for decoding and may result in lower translation quality. To resolve the problems, we propose a hierarchical back-off model for Hiero grammar, an instance of a synchronous context free grammar (SCFG), on the basis of the hierarchical Pitman-Yor process. The model can extract a compact rule and phrase table without resorting to any heuristics by hierarchically backing off to smaller phrases under SCFG. Inference is efficiently carried out using two-step synchronous parsing of Xiao et al., (2012) combined with slice sampling. In our experiments, the proposed model achieved higher or at least comparable translation quality against a previous Bayesian model on various language pairs; German/French/Spanish/JapaneseEnglish. When compared against heuristic models, our model achieved comparable translation quality on a full size GermanEnglish language pair in Europarl v7 corpus with significantly smaller grammar size; less than 10% of that for heuristic model."
P14-2052,Single Document Summarization based on Nested Tree Structure,2014,18,26,3,1,27810,yuta kikuchi,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Many methods of text summarization combining sentence selection and sentence compression have recently been proposed. Although the dependency between words has been used in most of these methods, the dependency between sentences, i.e., rhetorical structures, has not been exploited in such joint methods. We used both dependency between words and dependency between sentences by constructing a nested tree, in which nodes in the document tree representing dependency between sentences were replaced by a sentence tree representing dependency between words. We formulated a summarization task as a combinatorial optimization problem, in which the nested tree was trimmed without losing important content in the source document. The results from an empirical evaluation revealed that our method based on the trimming of the nested tree significantly improved the summarization of texts."
D14-1017,Unsupervised Word Alignment Using Frequency Constraint in Posterior Regularized {EM},2014,17,2,3,1,3633,hidetaka kamigaito,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Generative word alignment models, such as IBM Models, are restricted to oneto-many alignment, and cannot explicitly represent many-to-many relationships in a bilingual text. The problem is partially solved either by introducing heuristics or by agreement constraints such that two directional word alignments agree with each other. In this paper, we focus on the posterior regularization framework (Ganchev et al., 2010) that can force two directional word alignment models to agree with each other during training, and propose new constraints that can take into account the difference between function words and content words. Experimental results on French-to-English and Japanese-to-English alignment tasks show statistically significant gains over the previous posterior regularization baseline. We also observed gains in Japanese-toEnglish translation tasks, which prove the effectiveness of our methods under grammatically different language pairs."
P13-1083,Part-of-Speech Induction in Dependency Trees for Statistical Machine Translation,2013,38,7,4,1,1486,akihiro tamura,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper proposes a nonparametric Bayesian method for inducing Part-ofSpeech (POS) tags in dependency trees to improve the performance of statistical machine translation (SMT). In particular, we extend the monolingual infinite tree model (Finkel et al., 2007) to a bilingual scenario: each hidden state (POS tag) of a source-side dependency tree emits a source word together with its aligned target word, either jointly (joint model), or independently (independent model). Evaluations of Japanese-to-English translation on the NTCIR-9 data show that our induced Japanese POS tags for dependency trees improve the performance of a forestto-string SMT system. Our independent model gains over 1 point in BLEU by resolving the sparseness problem introduced in the joint model."
P13-1101,Subtree Extractive Summarization via Submodular Maximization,2013,16,20,3,1,25335,hajime morita,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
I13-1078,Construction of Emotional Lexicon Using Potts Model,2013,20,10,2,0,31213,braja patra,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Emotion is an instinctive state of mind aroused by some specific objects or situation. Exchange of textual information is an important medium for communication and contains a rich set of emotional expressions. The computational approaches to emotion analysis in textual data require annotated lexicons with polarity tags. In this paper we propose a novel method for constructing emotion lexicon annotated with Ekmanxe2x80x9fs six basic emotion classes (anger, disgust, fear, happy, sad and surprise). We adopt the Potts model for the probability modeling of the lexical network. The lexical network has been constructed by connecting each pair of words in which one of the two words appears in the gloss of the other. Starting with a small number of emotional seed words, the emotional categories of other words have been determined. With manual checking of top 200 words from each class an average precision of 85.41% has been achieved."
C12-1060,Statistical Mechanical Analysis of Semantic Orientations on Lexical Network,2012,26,0,3,0,43750,takuma goto,Proceedings of {COLING} 2012,0,"Many of the state-of-the-art methods for constructing a polarity lexicon rely on the propagation of polarity on the lexical network. In one of those methods, where the Ising spin model is employed as a probabilistic model, it is reported that the system exhibits the phase transition in the vicinity of the optimal temperature parameter. We provide an analysis of this phenomenon from the viewpoint of statistical mechanics and clarify the underlying mechanism. On the basis of this analysis, we propose a scheme for improving the extraction performance, i.e., by removing the largest eigenvalue component from the weight matrix. Experimental results show that the scheme significantly improves the accuracy of the extraction of the semantic orientations at negligible additional computational cost, outperforming the state-of-the-art algorithms. We also explore the origin of the high classification performance by analyzing eigenvalues of the weight matrix and a linearized model."
C12-1086,Generating {``}{A} for {A}lpha{''} When There Are Thousands of Characters,2012,7,0,3,0,43766,hiroaki kawasaki,Proceedings of {COLING} 2012,0,"The phonetic alphabet enables people to dictate letters of the alphabet accurately by using representative words, i.e., A for Alpha. Japanese kanji (idiographic Chinese characters) vastly outnumber the letters of the Roman alphabet, and thus Japanese requires an explanatory reading like a phonetic alphabet. We call the explanatory reading of a kanji a xe2x80x9cdistinctive explanation.xe2x80x9d Most kanji characters have their homophones, and the role of the distinctive explanations is to enable users to identify a specific kanji character only by listening to the explanation. In this paper, we propose a corpus-based method for automatically generating distinctive explanations for a kanji, in which information about familiarity and homophones of kanji are taken into consideration. Through the kanji-identification experiments, we show that the quality of the explanations generated by the proposed method is higher than that of the manually crafted distinctive explanations."
I11-1093,A Named Entity Recognition Method based on Decomposition and Concatenation of Word Chunks,2011,0,1,2,0.215517,10521,tomoya iwakura,Proceedings of 5th International Joint Conference on Natural Language Processing,0,None
I11-1158,Potts Model on the Case Fillers for Word Sense Disambiguation,2011,14,0,1,1,4800,hiroya takamura,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We propose a new method for word sense disambiguation for verbs. In our method, sense-dependent selectional preference of verbs is obtained through the probabilistic model on the lexical network. The meanfield approximation is employed to compute the state of the lexical network. The outcome of the computation is used as features for discriminative classifiers. The method is evaluated on the dataset of the Japanese word sense disambiguation."
D10-1081,An Efficient Algorithm for Unsupervised Word Segmentation with Branching Entropy and {MDL},2010,27,16,2,0,41342,valentin zhikov,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper proposes a fast and simple unsupervised word segmentation algorithm that utilizes the local predictability of adjacent character sequences, while searching for a least-effort representation of the data. The model uses branching entropy as a means of constraining the hypothesis space, in order to efficiently obtain a solution that minimizes the length of a two-part MDL code. An evaluation with corpora in Japanese, Thai, English, and the CHILDES corpus for research in language development reveals that the algorithm achieves an accuracy, comparable to that of the state-of-the-art methods in unsupervised word segmentation, in a significantly reduced computational time."
R09-1052,Structured Output Learning with Polynomial Kernel,2009,11,2,2,1,25335,hajime morita,Proceedings of the International Conference {RANLP}-2009,0,"We propose a new method which enables the training of a kernelized structured output model. The structured output learning can flexibly represent a problem, and thus is gaining popularity in natural language processing. Meanwhile the polynomial kernel method is effective in many natural language processing tasks, since it takes into account the combination of features. However, it is computationally difficult to simultaneously use both the structured output learning and the kernel method. Our method avoids this difficulty by transforming the kernel function, and enables the kernelized structured output learning. We theoretically discuss the computational complexity of the proposed method and also empirically show its high efficiency and effectiveness through experiments in the task of identifying agreement and disagreement relations between utterances in meetings. Identifying agreement and disagreement relations consists of two mutuallycorrelated problems: identification of the utterance which each utterance is intended for, and classification of each utterance into approval, disapproval or others. We simultaneously use both of the structured output learning and the kernel method in order to take into account this correlation of the two problems."
E09-1089,Text Summarization Model Based on Maximum Coverage Problem and its Variant,2009,19,126,1,1,4800,hiroya takamura,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We discuss text summarization in terms of maximum coverage problem and its variant. We explore some decoding algorithms including the ones never used in this summarization formulation, such as a greedy algorithm with performance guarantee, a randomized algorithm, and a branch-and-bound method. On the basis of the results of comparative experiments, we also augment the summarization model so that it takes into account the relevance to the document cluster. Through experiments, we showed that the augmented model is superior to the best-performing method of DUC'04 on ROUGE-1 without stopwords."
I08-1019,Identifying Cross-Document Relations between Sentences,2008,14,12,2,0,48665,yasunari miyabe,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"A pair of sentences in different newspaper articles on an event can have one of several relations. Of these, we have focused on two, i.e., equivalence and transition. Equivalence is the relation between two sentences that have the same information on an event. Transition is the relation between two sentences that have the same information except for values of numeric attributes. We propose methods of identifying these relations. We first split a dataset consisting of pairs of sentences into clusters according to their similarities, and then construct a classifier for each cluster to identify equivalence relations. We also adopt a xe2x80x9ccoarse-to-finexe2x80x9d approach. We further propose using the identified equivalence relations to address the task of identifying transition relations."
I08-1039,Learning to Shift the Polarity of Words for Sentiment Classification,2008,12,50,2,0,13640,daisuke ikeda,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"We propose a machine learning based method of sentiment classification of sentences using word-level polarity. The polarities of words in a sentence are not always the same as that of the sentence, because there can be polarity-shifters such as negation expressions. The proposed method models the polarity-shifters. Our model can be trained in two different ways: word-wise and sentence-wise learning. In sentence-wise learning, the model can be trained so that the prediction of sentence polarities should be accurate. The model can also be combined with features used in previous work such as bag-of-words and n-grams. We empirically show that our method almost always improves the performance of sentiment classification of sentences especially when we have only small amount of training data."
N07-1037,Extracting Semantic Orientations of Phrases from Dictionary,2007,22,62,1,1,4800,hiroya takamura,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We propose a method for extracting semantic orientations of phrases (pairs of an adjective and a noun): positive, negative, or neutral. Given an adjective, the semantic orientation classification of phrases can be reduced to the classification of words. We construct a lexical network by connecting similar/related words. In the network, each node has one of the three orientation values and the neighboring nodes tend to have the same value. We adopt the Potts model for the probability model of the lexical network. For each adjective, we estimate the states of the nodes, which indicate the semantic orientations of the adjective-noun pairs. Unlike existing methods for phrase classification, the proposed method can classify phrases consisting of unseen words. We also propose to use unlabeled data for a seed set of probability computation. Empirical evaluation shows the effectiveness of the proposed method."
D07-1063,{J}apanese Dependency Analysis Using the Ancestor-Descendant Relation,2007,9,3,2,1,1486,akihiro tamura,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We propose a novel method for Japanese dependency analysis, which is usually reduced to the construction of a dependency tree. In deterministic approaches to this task, dependency trees are constructed by series of actions of attaching a bunsetsu chunk to one of the nodes in the tree being constructed. Conventional techniques select the node based on whether the new bunsetsu chunk and each node in the trees are in a parent-child relation or not. However, tree structures include relations between two nodes other than the parent-child relation. Therefore, we use ancestor-descendant relations in addition to parent-child relations, so that the added redundancy helps errors be corrected. Experimental results show that the proposed method achieves higher accuracy."
P06-1145,Time Period Identification of Events in Text,2006,12,8,3,0,50001,taichi noro,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This study aims at identifying when an event written in text occurs. In particular, we classify a sentence for an event into four time-slots; morning, daytime, evening, and night. To realize our goal, we focus on expressions associated with time-slot (time-associated words). However, listing up all the time-associated words is impractical, because there are numerous time-associated expressions. We therefore use a semi-supervised learning method, the Naive Bayes classifier backed up with the Expectation Maximization algorithm, in order to iteratively extract time-associated words while improving the classifier. We also propose to use Support Vector Machines to filter out noisy instances that indicates no specific time period. As a result of experiments, the proposed method achieved 0.864 of accuracy and outperformed other methods."
E06-1026,Latent Variable Models for Semantic Orientations of Phrases,2006,17,56,1,1,4800,hiroya takamura,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We propose models for semantic orientations of phrases as well as classification methods based on the models. Although each phrase consists of multiple words, the semantic orientation of the phrase is not a mere sum of the orientations of the component words. Some words can invert the orientation. In order to capture the property of such phrases, we introduce latent variables into the models. Through experiments, we show that the proposed latent variable models work well in the classification of semantic orientations of phrases and achieved nearly 82% classification accuracy."
P05-1017,Extracting Semantic Orientations of Words using Spin Model,2005,19,297,1,1,4800,hiroya takamura,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We propose a method for extracting semantic orientations of words: desirable or undesirable. Regarding semantic orientations as spins of electrons, we use the mean field approximation to compute the approximate probability function of the system instead of the intractable actual probability function. We also propose a criterion for parameter selection on the basis of magnetization. Given only a small number of seed words, the proposed method extracts semantic orientations with high accuracy in the experiments on English lexicon. The result is comparable to the best value ever reported."
I05-1038,Classification of Multiple-Sentence Questions,2005,9,25,2,1,1486,akihiro tamura,Second International Joint Conference on Natural Language Processing: Full Papers,0,"Conventional QA systems cannot answer to the questions composed of two or more sentences. Therefore, we aim to construct a QA system that can answer such multiple-sentence questions. As the first stage, we propose a method for classifying multiple-sentence questions into question types. Specifically, we first extract the core sentence from a given question text. We use the core sentence and its question focus in question classification. The result of experiments shows that the proposed method improves F-measure by 8.8% and accuracy by 4.4%."
W04-2408,Modeling Category Structures with a Kernel Function,2004,8,5,1,1,4800,hiroya takamura,Proceedings of the Eighth Conference on Computational Natural Language Learning ({C}o{NLL}-2004) at {HLT}-{NAACL} 2004,0,"We propose one type of TOP (Tangent vector Of the Posterior log-odds) kernel and apply it to text categorization. In a number of categorization tasks including text categorization, negative examples are usually more common than positive examples and there may be several different types of negative examples. Therefore, we construct a TOP kernel, regarding the probabilistic model of negative examples as a mixture of several component models respectively corresponding to given categories. Since each component model of our mixture model is expressed using a one-dimensional Gaussian-type function, the proposed kernel has an advantage in computational time. We also show that the computational advantage is shared by a more general class of models. In our experiments, the proposed kernel used with Support Vector Machines outperformed the linear kernel and the Fisher kernel based on the Probabilistic Latent Semantic Indexing model."
W03-2604,Incorporating Contextual Cues in Trainable Models for Coreference Resolution,2003,-1,-1,3,0,12930,ryu iida,Proceedings of the 2003 {EACL} Workshop on The Computational Treatment of Anaphora,0,None
W02-2028,Two-dimensional Clustering for Text Categorization,2002,13,13,1,1,4800,hiroya takamura,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,"We propose a new method to improve the accuracy of Text Categorization using two-dimensional clustering. In a number of previous probabilistic approaches, texts in the same category are implicitly assumed to be generated from an identical distribution. We empirically show that this assumption is not accurate, and propose a new framework based on two-dimensional clustering to alleviate this problem. In our method, training texts are clustered so that the assumption is more likely to be true, and at the same time, features are also clustered in order to tackle the data sparseness problem. We conduct some experiments to validate the proposed two-dimensional clustering method."
W01-0507,Feature Space Restructuring for {SVM}s with Application to Text Categorization,2001,13,14,1,1,4800,hiroya takamura,Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,0,None
