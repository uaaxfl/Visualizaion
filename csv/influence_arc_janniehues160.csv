2008.iwslt-papers.5,W08-0303,1,0.811017,"till needed. For a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++Toolkit [12] to generate the word alignment. This toolkit implements the IBM- and HMM-models introduced in [13, 14]. They have the advantage of unsupervised training and are well suited for a noisy-channel approach, but introducing additional features into these models is difficult. In contrast, we use the discriminative word alignment model presented in [15], which uses a conditional random field (CRF) to model the alignment matrix. This model is symmetric and no restrictions to the alignment are required. Furthermore, it is easy to use additional knowledge sources and the alignment can be biased towards precision or recall. In the framework, the different aspects of the word alignment are modeled by three groups of features. The first group of features depend only on the source and target words and may therefore be called local features. The lexical features, which represent the lexical translation probability of the words as well as the source"
2008.iwslt-papers.5,E03-1076,0,0.0214995,"arliament Plenary Speeches (EPPS) and News Commentary corpora, as provided by WMT08 [16]. To this we added a smaller corpus of about 660K running words consisting of spoken language expressions in the travel domain, and a corpus of about 100K running words of German lectures held at Universit¨at Karlsruhe (TH) which were transcribed and translated into English, yielding a parallel training corpus of about 1.46M sentence pairs and 36M running words. For machine translation experiments, we applied compound splitting to the German side of the corpus, using the frequency-based method described in [17] which was trained on the corpus itself. Even though this method splits more aggressively and generates shorter and partially erroneous word fragments, it produced better translation quality than the method used for the speech recognizer described in Section 3.3. After tokenization and compound splitting, we performed word alignment, using the GIZA++ toolkit and, for the final system, the method described in the previous secProceedings of IWSLT 2008, Hawaii - U.S.A. tion, and extracted bilingual phrase pairs with the Pharaoh toolkit [18]. 4-gram language models were used in all experiments, es"
2008.iwslt-papers.5,W06-3114,0,0.0570922,"he corpus, using the frequency-based method described in [17] which was trained on the corpus itself. Even though this method splits more aggressively and generates shorter and partially erroneous word fragments, it produced better translation quality than the method used for the speech recognizer described in Section 3.3. After tokenization and compound splitting, we performed word alignment, using the GIZA++ toolkit and, for the final system, the method described in the previous secProceedings of IWSLT 2008, Hawaii - U.S.A. tion, and extracted bilingual phrase pairs with the Pharaoh toolkit [18]. 4-gram language models were used in all experiments, estimated with modified Kneser-Ney smoothing as implemented in the SRILM toolkit [7]. 4.3. Model adaptation The performance of data-driven MT systems depends heavily on a good match between training and test data in terms of domain coverage and speaking style. This applies even more so in our case, as university lectures can go deeply into a very narrow topic and, depending on the lecturer, are often given in a much more informal and conversational style than that found in prepared speeches or even written text. To adapt the MT component t"
2008.iwslt-papers.5,2007.tmi-papers.21,0,0.0126456,"l adaptation are shown in Table 3. 4.4. Front-end In order to get the best translation results, the output of the speech recognizer has to be pre-processed to match the format expected by the translation component. We first perform compound splitting as described in Section 4.2, on top of the compound splitting already done in the speech recognizer. Because the word order in German and English is very different, reordering over a rather limited distance like done in many phrase-based systems does not lead to a good translation quality. We experimented with rule-based reordering as proposed in [19], in which a word lattice is created as a pre-processing step to encode different reorderings and allow somewhat longer distance reordering. The rules to create the lattice are automatically learned from the corpus and the part-of-speech (POS) tags created by the TreeTagger [20]. In the training, POS based reordering patterns were extracted from word alignment information. The context in which a reordering pattern is seen was used as an additional feature. At decoding time, we build a lattice structure for each source utterance as input for our decoder, which contains reordering alternatives c"
2008.iwslt-papers.5,J93-2003,0,\N,Missing
2008.iwslt-papers.5,C96-2141,0,\N,Missing
2008.iwslt-papers.5,J03-1002,0,\N,Missing
2008.iwslt-papers.5,P03-1021,0,\N,Missing
2010.eamt-1.29,W09-0432,0,0.0240657,"n systems to a domain. Some authors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance u"
2010.eamt-1.29,J93-2003,0,0.0160364,". We propose an approach to adapt an SMT system using small amounts of parallel in-domain data by introducing the corpus identifier (corpus id) as an additional target factor. Then we added features to model the generation of the tags and features to judge a sequence of tags. Using this approach we could improve the translation performance in two domains by up to 1 BLEU point when translating from German to English. 1 Introduction Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented in Brown et al. (1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is selected from a similar topic and from a similar genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system using indomain training data. Factored translation models as presented in Koehn and Hoang (2007) are able to tightly integrate additional knowledge into a ph"
2010.eamt-1.29,W07-0717,0,0.112906,". In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using inf"
2010.eamt-1.29,2005.eamt-1.19,1,0.751116,"and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using information retrieval techniques. Factored translation models were introduced by Koehn and Hoang (2007) to enable the straightforward integration of additional annotations at the word-level. This is done by representing a word by a vector of factors for the different types of annotations instead of using only the word token. The additional factors can be used to better judge the generated output as well as to generate the target word from the other factors, if no direct translation of the"
2010.eamt-1.29,D07-1091,0,0.320516,"of large vocabulary tasks. The approach was first presented in Brown et al. (1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is selected from a similar topic and from a similar genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system using indomain training data. Factored translation models as presented in Koehn and Hoang (2007) are able to tightly integrate additional knowledge into a phrase-based statistical machine translation system. In most c 2010 European Association for Machine Translation. cases, the approach is used to incorporate linguistic knowledge, such as morphological, syntactic and semantic information. In contrast, we will use the approach to integrate domain knowledge into the system by introducing a corpus identifier (corpus id) tag. Using the corpus id as a target word factor enables us to adapt the SMT system by introducting two new types of features in the log-linear model in phrase-based SMT sy"
2010.eamt-1.29,W07-0733,0,0.1737,"use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried"
2010.eamt-1.29,N03-1017,0,0.00795716,"this language pair, the biggest parallel corpus available are the Proceedings of the European Parliament (EPPS). The first system we built was designed to translate documents from the news-commentary domain. As test set we used the test set from the WMT Evaluation in 2007. As parallel training data we used the EPPS corpus as well as an in-domain news-commentary corpus with about 1M words. In contrast, the corpus from the EPPS domain has about 39M words. In a preprocessing step we cleaned the data and performed a compound splitting on the German text based on the frequency method described in Koehn et al. (2003). We generated a word alignment for the parallel corpus using a discriminative approach as described in Niehues and Vogel (2008) trained on a set of 500 hand-aligned sentences. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partl"
2010.eamt-1.29,P07-2045,0,0.0125364,"Missing"
2010.eamt-1.29,D09-1074,0,0.0580419,"ranslation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using information retrieval techniques. Factored translation models were introduced by Koehn and Hoang (2007) to enable the straightforward integration of additional annotations at the word-level. This is done by representing a word by a vector of factors for the different types of ann"
2010.eamt-1.29,W09-0435,1,0.835368,"rs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on the translation task of lecture"
2010.eamt-1.29,W08-0303,1,0.867836,"system we built was designed to translate documents from the news-commentary domain. As test set we used the test set from the WMT Evaluation in 2007. As parallel training data we used the EPPS corpus as well as an in-domain news-commentary corpus with about 1M words. In contrast, the corpus from the EPPS domain has about 39M words. In a preprocessing step we cleaned the data and performed a compound splitting on the German text based on the frequency method described in Koehn et al. (2003). We generated a word alignment for the parallel corpus using a discriminative approach as described in Niehues and Vogel (2008) trained on a set of 500 hand-aligned sentences. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-s"
2010.eamt-1.29,2007.tmi-papers.21,0,0.0607317,"es. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on th"
2010.eamt-1.29,2009.mtsummit-posters.17,0,0.0108605,"re proposed to adapt translation systems to a domain. Some authors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improv"
2010.eamt-1.29,D08-1090,0,0.0249194,"thors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation m"
2010.eamt-1.29,W05-0836,1,0.89216,"of-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on the translation task of lectures from German to English. The system was trained on the data from the European Parliament, the news-commentary data, German-English BTEC data and a small amount of translated lectures. The in-domain corpus contained only around 210K words. The system was built similar to the systems in the other experiments except to some small changes due to speech translation. Instead of doing a separate compound splitting, we used the same splitting as it was used by the speech recognizer. Since the output of the speech recognit"
2010.eamt-1.29,C08-1125,0,0.0790346,"only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation m"
2010.iwslt-evaluation.11,W10-1719,1,\N,Missing
2010.iwslt-evaluation.11,D09-1022,0,\N,Missing
2010.iwslt-evaluation.11,W08-1006,0,\N,Missing
2010.iwslt-evaluation.11,E03-1076,0,\N,Missing
2010.iwslt-evaluation.11,N04-4015,0,\N,Missing
2010.iwslt-evaluation.11,E99-1010,0,\N,Missing
2010.iwslt-evaluation.11,W11-2124,1,\N,Missing
2010.iwslt-evaluation.11,P03-1054,0,\N,Missing
2010.iwslt-evaluation.11,W09-0435,1,\N,Missing
2010.iwslt-evaluation.11,W06-1607,0,\N,Missing
2010.iwslt-evaluation.11,P07-2045,0,\N,Missing
2010.iwslt-evaluation.11,N06-2013,0,\N,Missing
2010.iwslt-evaluation.11,P06-1096,0,\N,Missing
2010.iwslt-evaluation.11,J03-1002,0,\N,Missing
2010.iwslt-evaluation.11,2011.iwslt-evaluation.9,1,\N,Missing
2010.iwslt-evaluation.11,2005.iwslt-1.8,0,\N,Missing
2010.iwslt-evaluation.11,2011.iwslt-papers.7,0,\N,Missing
2010.iwslt-evaluation.11,W11-2145,1,\N,Missing
2010.iwslt-evaluation.11,W11-2123,0,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.iwslt-evaluation.9,2007.tmi-papers.21,0,0.269099,"from the training data may not help or may be even harmful when translating. Instead of using a translation model with phrase tables that are built on data containing punctuation, we tried to train the system for the speech translation task using the training corpus without punctuation marks. Therefore we mapped the alignment from the parallel corpus with punctuation to the corpus without source punctuation. Then we retrained the phrase table, the POS-based reordering model and the bilingual language model. 4. Word Reordering Model Our word reordering model relies on POS tags as introduced by [9]. The reordering is performed as preprocessing step. Rule extraction is based on two types of input: the Giza alignment of the parallel corpus and its corresponding POS tags generated by the TreeTagger for the source side. For each sequence of POS tags, where a reordering between source and target sentences is detected, a rule is generated. Its head consists of the sequence of source tags and its body is the permutation of POS tags in the head which matches the order of the corresponding aligned target words. After that, the rules are scored according to their occurrence and then pruned accord"
2011.iwslt-evaluation.9,W11-2124,1,0.64468,"nslation model, the adaptation of the language model is also achieved by a log-linear combination of different models. This also fits well into the global log-linear model used in the translation system. Therefore, we trained a separate language model using only the in-domain data for TED provided in the workshop. Then it was used as an additional language model during decoding and received optimal weights during tuning by the Minimum Error Rate training. 6. Bilingual Language Models To increase the context used during the translation process, we use a bilingual language model as described in [10]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. For training, we create a corpus of bilingual tokens from each of the parallel corpora (TED, UN, EPPS, NC and Giga) and then we train one SRI language model based on all the corpora of bilingual tokens. We use an n-gram length of four words. During decoding, this language model is then used to score the different translation hypotheses. 7. Cluster Language Models As m"
2011.iwslt-evaluation.9,E99-1010,0,0.385179,"language model, which is separately trained on the TED corpus and then combined (in a log-linear way) with the other models. Since the TED corpus is much smaller than the other corpora, the probabilities cannot be estimated as reliably. Furthermore, for the style of a document the word order may not be as important, but the sequence of used word classes may be sufficient to specify the style. To tackle both problems, we try to use a language model based on word classes in addition. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [11]. Then we replace the words in the TED corpus by their cluster IDs and train a n-gram language model on this corpus consisting of word classes (all cluster language models used in our systems are 5-gram). During decoding we use the cluster-based language model as an additional model in the log-linear combination. 8. Discriminative Word Lexica In [12] it was shown that the use of discriminative word lexica (DWL) can improve the translation quality quite significantly. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated se"
2011.iwslt-evaluation.9,D09-1022,0,0.0938278,"ses may be sufficient to specify the style. To tackle both problems, we try to use a language model based on word classes in addition. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [11]. Then we replace the words in the TED corpus by their cluster IDs and train a n-gram language model on this corpus consisting of word classes (all cluster language models used in our systems are 5-gram). During decoding we use the cluster-based language model as an additional model in the log-linear combination. 8. Discriminative Word Lexica In [12] it was shown that the use of discriminative word lexica (DWL) can improve the translation quality quite significantly. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not. As features for their classifier they used one feature per source word. One specialty of this task is that we have a lot of parallel data we can train our models on, but only a quite small portion of these data, the TED corpus, is very important to the translation quality. Since building the classifiers on the whole corpus is quite tim"
2011.iwslt-evaluation.9,J03-1002,0,0.0105903,"we used the provided monolingual data. Systems were tuned and tested against the provided Dev and Test sets. Before training any of our models, we perform the usual preprocessing, such as removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized; then the first word of every sentence is smart-cased. All the language models used are 4-gram language models with Kneser-Ney smoothing, trained with the SRILM toolkit [2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preproc"
2011.iwslt-evaluation.9,P07-2045,0,0.0204732,"he usual preprocessing, such as removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized; then the first word of every sentence is smart-cased. All the language models used are 4-gram language models with Kneser-Ney smoothing, trained with the SRILM toolkit [2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preprocessing The Giga corpus received a special preprocessing in a similar manner to [5]. An SVM classifier was used to filter out bad pairs from the Giga parallel"
2011.iwslt-evaluation.9,W05-0836,1,0.492103,"[2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preprocessing The Giga corpus received a special preprocessing in a similar manner to [5]. An SVM classifier was used to filter out bad pairs from the Giga parallel corpus. We used the same set of features. These are: IBM1 score in both directions, the number of unaligned source words, the difference in number of words between source and target, the maximum source word fertility, the number of unaligned target words, and the maximum target word fertility. The lexicons used were generated by Giza++ alignments train"
2011.iwslt-evaluation.9,W11-2145,1,\N,Missing
2011.iwslt-evaluation.9,2011.iwslt-evaluation.1,0,\N,Missing
2011.iwslt-papers.6,C88-1016,0,0.659552,"Missing"
2011.iwslt-papers.6,P11-2071,0,0.0190164,"erent translations for one word. In Section 5 we describe the generation of morphological operations that allow us to handle different morphological forms of the domain specific terms. In the end, we will evaluate the approach and close with a conclusion. 2. Related Work To adapt an SMT system towards a new domain, different problems have to be solved. One important question is to find translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolin"
2011.iwslt-papers.6,P11-1133,0,0.031428,"is to find translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation mode"
2011.iwslt-papers.6,C08-1125,0,0.0629483,"nd translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is ada"
2011.iwslt-papers.6,2009.mtsummit-posters.17,0,0.0175943,"rmed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language proces"
2011.iwslt-papers.6,W09-0432,0,0.0203857,"earch on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. Fo"
2011.iwslt-papers.6,W07-0717,0,0.0346657,"es using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been s"
2011.iwslt-papers.6,W07-0733,0,0.0237567,"ing only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several"
2011.iwslt-papers.6,D10-1044,0,0.017086,"a is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improve"
2011.iwslt-papers.6,D08-1090,0,0.0251582,"slating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introd"
2011.iwslt-papers.6,P07-2045,0,0.00483631,"edia into a translation system on the task of translating German computer science lectures into English. The baseline system is described in detail below. In addition to the improvements measured by automatic The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university l"
2011.iwslt-papers.6,2007.tmi-papers.21,0,0.520717,"tion system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university lectures from the computer science department as development and test data. Each set contains around 30K words. 6.2. Integration The results for the baseline system as well as the results for both methods (Lexicon and Cor"
2011.iwslt-papers.6,W09-0413,1,0.851372,"ent corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university lectures from the computer science department as development and test data. Each set contains around 30K words. 6.2. Integration The results for the baseline system as well as the results for both methods (Lexicon and Corpus) to integrate the data described in Section"
2011.iwslt-papers.6,2010.iwslt-evaluation.11,1,0.813051,"be combined with other adaptation techniques. Although it is hard to find parallel data that matches the domain of university lectures in computer science, it is possible to find data that at least matches the genre. In our case, we used the TED corpus consisting of the subtitles and translations of the talks published on the TED Website1 . We built additional translation systems, one which just uses the additional data from the TED corpus and one that is also adapted towards TED using a log-linear combination for the phrase table as well as for the language model as for example described in [25]. The results of these experiments are described in Table 5. First, we repeated the result for using the information from Wikipedia on the baseline system without and with the morphological operations. Afterwards, we performed the same series of experiments first with the system using in addition the TED corpus and secondly, using the system also adapted to the TED corpus. As it can be seen using the additional data from the same genre could improve the translation quality significantly on the devlopment as well as on the test set. But in all cases further improvements using also the Wikipedia"
2011.iwslt-papers.6,2009.mtsummit-posters.26,0,0.0311766,"n and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by lear"
2011.iwslt-papers.6,P08-1059,0,0.0205617,"tation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia contain"
2011.iwslt-papers.6,P07-1017,0,0.0315678,"y been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different langua"
2011.iwslt-papers.6,W11-2138,0,0.0168165,". proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different languages about the same topic. Using these links we can align the articles in source and target languag"
2011.iwslt-papers.6,P11-1140,0,0.0445901,"lingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different languages about the same topic. Using these links we can align the articles in source and target language. Although the articles"
2011.iwslt-papers.6,W08-0303,1,0.774517,"se pairs, since there is no phrase pair that exactly matches for f2 . 6. Results We evaluated the described approach to integrate data from Wikipedia into a translation system on the task of translating German computer science lectures into English. The baseline system is described in detail below. In addition to the improvements measured by automatic The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the op"
2011.iwslt-papers.6,D09-1074,0,\N,Missing
2012.amta-papers.19,2011.iwslt-evaluation.18,0,0.131199,"rase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their app"
2012.amta-papers.19,J93-2003,0,0.0170495,"echniques to adapt the phrase pair scoring and characterized them by using four key aspects. By analyzing the steps separately, we are able to combine the techniques from the approaches in a new way and improve the translation quality even further. The different strategies were evaluated on two different tasks of translating German to English: the translation of TED lectures1 and computer science university lectures. Introduction Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented in (Brown et al., 1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is matching in topic and genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system to the task at hand using in-domain training data. Since parallel in-domain data is available for our scenario, we will focus on the adaptation of the 2 Related work In recent years di"
2012.amta-papers.19,W07-0717,0,0.0673645,"on model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for all sentences in the parallel corpus. 3 Translation model When parallel in-domain data is available, we want to adapt the translation model to be able to encode the domain specifi"
2012.amta-papers.19,W06-1607,0,0.0639312,"n of histogram and beam pruning. We used at most 10 translations for every source phrase. We rank the phrase pairs in order to be able to select the top translations by scoring them using some initial weights that were determined heuristically. These weights are independent from the weights generated by MERT. In the baseline phrase table we use four different scores Φs ((f¯i , e¯i )) for every phrase pair (f¯i , e¯i ). The relative frequencies in both directions and the lexical probabilities in both directions. We use modified Kneser-Ney smoothing for the relative frequencies as described in (Foster et al., 2006). This leads to the following definition of the translation model when translating the source sentence f = f¯1I into the target sentence e = e¯I1 using the phrase pairs ((f¯1 , e¯1 ),(f¯2 , e¯2 ),..,(f¯I , e¯I )) log(p(e¯I1 |f¯1I )) = = I X log(p(e¯i |f¯i )) i=1 I X S X (1) λs log(Φs ((f¯i , e¯i ))) i=1 s=1 −log(Z) (2) The weights used for the four scores during the actual decoding are optimized using MER training on the development data. In our scenario there are three different phrase tables. One trained only on the in-domain data providing the candidate translations TIN (f¯i ) for a given s"
2012.amta-papers.19,W07-0733,0,0.0420753,"train a general system and to adapt the system to the task at hand using in-domain training data. Since parallel in-domain data is available for our scenario, we will focus on the adaptation of the 2 Related work In recent years different methods were proposed to adapt translation systems to a specific domain. Some adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage is that only monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain"
2012.amta-papers.19,P07-2045,0,0.00761247,"that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach desc"
2012.amta-papers.19,D09-1074,0,0.12043,"ds (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for all sentences in the parallel corpus. 3 Translation model When parallel in-domain data is available, we want to adapt the translation model to be able to encode the domain specific knowledge without losing the information learned from the much bigger parallel corpus. If we compare the different approaches of translation model adaptation, we see that two main aspects of the model can be adapted to better match the specific domain. The first aspect is the can"
2012.amta-papers.19,W09-0435,1,0.77875,"ata using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. We used TED talks as development and test data. In addition, we tested the systems on transcribed university lectures from the computer science(CS) department. Each test set contains at least 30K words. 6.2 Baseline In a first series of experiments we show the influence of the in-domain and out-of-domain data. We tested the systems on both tasks using three differe"
2012.amta-papers.19,W08-0303,1,0.796774,"l evaluate the influence of the candidate selection and aspects of the phrase scoring. We performed significance tests following (Zhang and Vogel, 2004). All results that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all"
2012.amta-papers.19,2010.eamt-1.29,1,0.757814,"monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were ava"
2012.amta-papers.19,2010.iwslt-evaluation.11,1,0.888005,"focus on the adaptation of the 2 Related work In recent years different methods were proposed to adapt translation systems to a specific domain. Some adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage is that only monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the num"
2012.amta-papers.19,W11-2124,1,0.725016,"Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-hou"
2012.amta-papers.19,2007.tmi-papers.21,0,0.610892,"uage model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. We used TED talks as development and test data. In addition, we tested the systems on transcribed university lectures from the computer science(CS) department. Each test set contains at least 30K words. 6.2 Baseline In a first series of experiments we show the influence of the in-domain and out-of-domain dat"
2012.amta-papers.19,2004.tmi-1.9,0,0.0416432,"view over the different aspects of the four approaches to phrase table adapation as mentioned in the related work is given in Table 1. 6 Results After analyzing the different approaches we will now evaluate their effects on translation quality. We perform experiments on two different German-toEnglish speech translation tasks. First, we describe the SMT system and then we run some baseline experiments to demonstrate the characteristics of the data. Afterwards, we will evaluate the influence of the candidate selection and aspects of the phrase scoring. We performed significance tests following (Zhang and Vogel, 2004). All results that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the M"
2012.iwslt-evaluation.3,2012.eamt-1.60,0,0.0164836,"n input by removing case information and punctuation except periods from the text translation model. 1. Introduction In the IWSLT 2012 Evaluation campaign [1], we participated in the tasks for text and speech translation for the EnglishFrench language pair. The TED tasks consist of automatic translation of both the manual transcripts and transcripts generated by automatic speech recognizers for talks held at the TED conferences 1 . The TED talks are given in English in a large number of different domains. Some of these talks are manually transcribed and translated by volunteers over the globe [2]. Given these manual transcripts and a large amount of outof-domain data (mainly news), our ambition is to perform optimal translation on the untranslated lectures which are more likely from different domains. Furthermore, we strive 1 http://www.ted.com for performing as well as possible on the automatically transcribed lectures. The contribution of this work is twofold: on the one hand, it demonstrates how the complementary manipulation of indomain and out-of-domain data is gainful in building more accurate translation models. It will be shown that while the large amount of out-of-domain data"
2012.iwslt-evaluation.3,2011.iwslt-evaluation.9,1,0.818995,"pact on the performance. 2 http://ngrams.googlelabs.com/datasets 38 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The ﬁrst letter of every sentence is smart-cased. Furthermore, an SVM classiﬁer was used to ﬁlter out the noisy sentences pairs in the Giga English-French corpus as described in [3]. The baseline system was trained on the EPPS, TED, and NC corpora. In addition to the French side of these corpora, we used the provided monolingual data and the French side of the parallel Giga corpus, for language model training. Systems were tuned and tested against the provided Dev 2010 and Test 2010 sets. All language models used are 4-gram language models with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit [4]. The word alignment of the parallel corpora was generated using the GIZA++ Toolkit [5] for both directions. Afterwards, the alignments were combined using the grow-d"
2012.iwslt-evaluation.3,P07-2045,0,0.00374563,"tion to the French side of these corpora, we used the provided monolingual data and the French side of the parallel Giga corpus, for language model training. Systems were tuned and tested against the provided Dev 2010 and Test 2010 sets. All language models used are 4-gram language models with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit [4]. The word alignment of the parallel corpora was generated using the GIZA++ Toolkit [5] for both directions. Afterwards, the alignments were combined using the grow-diag-ﬁnal-and heuristic. The phrases were extracted using the Moses toolkit [6] and then scored by our in-house parallel phrase scorer [7]. Phrase pair probabilities are computed using modiﬁed Kneser-Ney smoothing as in [8]. Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The POS tags for the reordering model are obtained using the TreeTagger [9]. Tuning is performed using Minimum Error Rate Training (MERT) against the BLEU score as described in [10]. All translations are generated using our in-house phrase-based decoder [11]. 3. Preprocessing for Speech Translation The system translating automatic transcripts ne"
2012.iwslt-evaluation.3,W06-1607,0,0.0311282,"del training. Systems were tuned and tested against the provided Dev 2010 and Test 2010 sets. All language models used are 4-gram language models with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit [4]. The word alignment of the parallel corpora was generated using the GIZA++ Toolkit [5] for both directions. Afterwards, the alignments were combined using the grow-diag-ﬁnal-and heuristic. The phrases were extracted using the Moses toolkit [6] and then scored by our in-house parallel phrase scorer [7]. Phrase pair probabilities are computed using modiﬁed Kneser-Ney smoothing as in [8]. Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The POS tags for the reordering model are obtained using the TreeTagger [9]. Tuning is performed using Minimum Error Rate Training (MERT) against the BLEU score as described in [10]. All translations are generated using our in-house phrase-based decoder [11]. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs some special preprocessing on the data, since generally there is no or not reliable case information and punctuation in the automatically ge"
2012.iwslt-evaluation.3,2007.tmi-papers.21,0,0.041135,"ways to optimize the system are possible. The ﬁrst one is to use the manual transcripts but it requires lower casing and removal of punctuation marks. The other one is to use the ASR single-best output released by the SLT task. The advantage of optimizing with the manual transcripts is that the system will be adjusted with higher quality sentences. On the other side, optimization using ASR output makes the system more consistent with the evaluation test data. We have tested both methods in our experiments. 4. Word Reordering Model Our word reordering model relies on POS tags as introduced by [12]. Rule extraction is based on two types of input: the Giza alignment of the parallel corpus and its corresponding POS tags generated by the TreeTagger for the source side. For each sequence of POS tags, where a reordering between source and target sentences is detected, a rule is generated. Its head consists of sequential source tags and its body is the permuted POS tags of the head which match the order of the corresponding aligned target words. After that, the rules are scored according to their occurrence and pruned according to a given threshold. In our system, the reordering is performed"
2012.iwslt-evaluation.3,E99-1010,0,0.0530703,"-linear model used in the translation system. Therefore, we train a separate language model using only the in-domain data from the TED corpus. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. 6. Cluster Language Model In addition to the word-based language model, we also use a cluster language model in the log-linear combination. The motivation is to make use of larger context information, since there is less data sparsity when we substitute words by word classes. First, we cluster the words in the corpus using the MKCLS algorithm [13] given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Because the TED corpus is small and important for this translation task and it exactly matches the target genre, we trained the cluster language model only on TED corpus in our experiments. The TED corpus is characterized by a huge variety of topics, but the style of the different talks of the corpus is quite similar. When translating a new talk from the same domain, we may not ﬁnd a good translation in the TED corpus"
2012.iwslt-evaluation.3,D09-1022,0,0.0224398,"tches the target genre, we trained the cluster language model only on TED corpus in our experiments. The TED corpus is characterized by a huge variety of topics, but the style of the different talks of the corpus is quite similar. When translating a new talk from the same domain, we may not ﬁnd a good translation in the TED corpus for many topic speciﬁc words. What TED corpus can help with, however, is to generate sentences in the same style. During decoding the cluster-based language model works as an additional model in the log-linear combination. 7. Discriminative Word Lexica Mauser et al. [14] have shown that the use of DWL can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. One specialty of this task is that we have a lot of parallel data we can train our models on, but only a quite small portion of these data, the TED corpus, is very important to the translation quality. Since building the classiﬁers on the whole corpus is quite time consuming, we try to train them on the TED corpus only. When applying DWL in our exp"
2012.iwslt-evaluation.3,N12-1005,0,0.0152204,"every training sentence. This vocabulary consists of all target side words of phrase pairs matching a source phrase in the source part of the training sentence. Then we use all sentence pairs where ej is in the target vocabulary but not in the target sentences as negative examples. This has shown to have a postive inﬂuence on the translation quality [3] and also reduces training time. 8. Continuous Space Language Model In recent years, different approaches to integrate a continuous space models have shown signiﬁcant improvements in the translation quality of machine translation systems, e.g. [15]. Since the long training time is the main disadvantage of this model, we only trained it on the small, but very domainrelevant TED corpus. 40 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 In contrast to most other approaches, we did not use a feed-forward neural network, but used a Restricted Bolzmann Machine (RBM). The main advantage of this approach is that we can calculate the free energy of the model, which is proportional to the language model probability, very fast. Therefore, we are able to use the RBM-based language model during decodi"
2012.iwslt-evaluation.3,2012.iwslt-papers.3,1,0.692244,"ly trained it on the small, but very domainrelevant TED corpus. 40 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 In contrast to most other approaches, we did not use a feed-forward neural network, but used a Restricted Bolzmann Machine (RBM). The main advantage of this approach is that we can calculate the free energy of the model, which is proportional to the language model probability, very fast. Therefore, we are able to use the RBM-based language model during decoding and not only in the rescoring phase. The model is described in detail in [16]. The RBM used for the language model consists of two layers, which are fully connected. In the input layer, for every word position there are as many nodes as words in the vocabulary. Since we used an 8-gram language model, there are 8 word positions in the input layer. These nodes are connected to the 32 hidden units in the hidden layer. During decoding, we calculate the free energy of the RBM for a given n-gram. The product of this values is then used as an additional feature in the log-linear model of the decoder. 9. Postprocessing for Agreement Correction The agreement in gender and numbe"
2012.iwslt-evaluation.3,W11-2124,1,0.838163,"language models trained on different data sets: the French side of the EPPS, TED, and NC corpora, the provided monolingual news data (Monolingual EPPS, NC and News Shufﬂed), and a smaller in-domain language model trained on TED data. The reordering in this system was handeled as a preprocessing step using POS-based rules as described in Section 4. The result of this setting was 28.5 BLEU points on Dev and 31.73 on Test. The performance could be improved by around 0.4 on Dev and 0.2 on Test by using a bilingual language model (details about bilingual language model computation can be found in [17]). An additional 0.2 on both Dev and test could be gained by using a cluster language model where the clusters were trained on the in-domain TED data. After that, changing the adaptation strategy by the union selection discussed in Section 5 shows slight improvement of 0.1 on both Dev and Test. The effect of the DWL trained on only the TED corpus was rather dissimilar on Dev and Test. While it slightly improved the score on Dev (0.1) it has a much greater effect on Test (0.5). Further small improvement could be observed by using a continuous space language model: around 0.09 on both Dev and Te"
2012.iwslt-evaluation.3,W09-0435,1,0.83614,"he English-Chinese translation. Further analysis and work need to be done on the reordering model. System Baseline(4gram LM) 8gram LM + 4gram UN LM + POS Reordering + 5gram google1980 on characters Dev Test (Cha.) (Cha.) 14.37 17.26 14.48 17.28 14.61 17.38 14.69 17.28 14.73 17.47 on words Test Test (Cha.) (Word) 16.69 9.92 17.08 10.03 16.80 9.99 17.32 10.23 16.82 9.84 Table 5: Translation results for English-Chinese The other models that we have tried, but have not given improvement to the system, include sentence-aligned extraction from the UN corpus and long-range reordering as described in [18]. 5 http://nlp.stanford.edu/software/segmenter. shtml 43 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 11. Conclusions 10.3.3. English-Arabic The parallel data provided for this direction was from TED and UN. As for the English-Chinese direction (presented in Section 10.3.2), greater effort was devoted to the data preprocessing. The preprocessing for the English side is identical to the one used in the English-French system of the MT Task. Some of these preprocessing operations, such as long pair removal, were also applied to the Arabic side. I"
2012.iwslt-evaluation.3,N06-2013,0,0.0180039,"ation Hong Kong, December 6th-7th, 2012 11. Conclusions 10.3.3. English-Arabic The parallel data provided for this direction was from TED and UN. As for the English-Chinese direction (presented in Section 10.3.2), greater effort was devoted to the data preprocessing. The preprocessing for the English side is identical to the one used in the English-French system of the MT Task. Some of these preprocessing operations, such as long pair removal, were also applied to the Arabic side. In addition to that, the Arabic side was further orthographically transliterated using Buckwalter transliteration [19]. Tokenization and POS tagging were performed by the AMIRA toolkit [20]. The resulting translation is converted back to Arabic scripting before evaluation. Table 6 presents some initial experiments for the EnglishArabic pair. The baseline system uses only TED data for translation and language modeling. This gave a score of 13.12 on Dev and 8.05 on Test. This system was remarkably enhanced by introducing the short range reordering rules. The scores were improved by about 0.3 on Dev and 0.2 on Test. Adding monolingual data from the UN corpus had a great impact on the score on Dev (improved by 0."
2012.iwslt-evaluation.3,W05-0836,0,0.0713406,"+ Toolkit [5] for both directions. Afterwards, the alignments were combined using the grow-diag-ﬁnal-and heuristic. The phrases were extracted using the Moses toolkit [6] and then scored by our in-house parallel phrase scorer [7]. Phrase pair probabilities are computed using modiﬁed Kneser-Ney smoothing as in [8]. Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The POS tags for the reordering model are obtained using the TreeTagger [9]. Tuning is performed using Minimum Error Rate Training (MERT) against the BLEU score as described in [10]. All translations are generated using our in-house phrase-based decoder [11]. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs some special preprocessing on the data, since generally there is no or not reliable case information and punctuation in the automatically generated transcripts. We have tried two ways to deal with the difference on casing and punctuation between a machine translation (MT) system and a SLT system. In addition, we also optimize the system with different development data: simulated ASR output and original automatic speech recogni"
2012.iwslt-papers.15,2011.iwslt-papers.7,0,0.599292,"ve proper segmentation before the translation to match the translation models in order to achieve better translation quality. Moreover, there are algorithmic constraints as well as user preferences, such as readability. When a sentence is excessively long, it either consumes a great deal of resources and time, or readability suffers. If the input is already augmented with punctuation in the source language, it is advantageous to the training procedure of MT. In this case, there is no need to retrain the translation system with modiﬁcation on the training data, in order to match the ASR output [1]. Nevertheless, most of the current ASR systems do not provide punctuation marks. It is one of the challenging tasks to restore segmentation and punctuation in the output of an ASR system, especially for speech translation. Sentence segmentation in the ASR system is often generated using prosodic features (pause duration, pitch, etc.) and lexical cues (e.g. language model probability). However, the performance of sentence segmentation degrades in spontaneous speech. This is because a large amount of the spontaneous utterance is less grammatical compared to written texts [2] and there are fewer"
2012.iwslt-papers.15,2005.eamt-1.37,0,0.0194998,"thors made an extensive analysis on how to predict punctuation using a machine translation system. In this work, it was assumed that the ASR output already has the proper segmentation, which is sentence-like units. They investigated three different approaches to restore punctuation marks; prediction in the source language, implicit prediction, and prediction in the target language. Using a translation system to translate from unpunctuated to punctuated text, they showed signiﬁcant improvements in the evaluation campaign of IWSLT 2011. Among different motivations for the sentence segmentation, [4] split long sentence pairs in the bilingual training corpora to make full use of training data and improved model estimation for statistical machine translation (SMT). For the splitting they used the lexicon information to ﬁnd splitting points. They showed that splitting sentences improved the performance for Chinese-English translation task. Similarly, to improve the performance of Example-based machine translation (EMBT) systems, [5] suggested a method to split sentences using sentence similarity based on editdistance. Combining prosodic and lexical information to detect sentence boundaries"
2012.iwslt-papers.15,C04-1017,0,0.0610404,"Missing"
2012.iwslt-papers.15,P07-2045,0,0.0447142,"anslation system is trained on 1.8 million sentences of German-English parallel data including the European Parliament data and News Commentary corpus. Before the training, the data is preprocessed and compound splitting for the German side is applied. Preprocessing consists of text normalization, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimizati"
2012.iwslt-papers.15,W11-2124,1,0.59087,"man side is applied. Preprocessing consists of text normalization, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase t"
2012.iwslt-papers.15,2007.tmi-papers.21,0,0.540685,"tion, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To"
2012.iwslt-papers.15,W09-0435,1,0.198885,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To investigate the impact of segmentation and punctuation marks on the translation quality, we conduct two experiments. In the ﬁrst experiment, we apply human-transcribed segments and punctuation marks to"
2012.iwslt-papers.15,W05-0836,1,0.940172,"odel is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To investigate the impact of segmentation and punctuation marks on the translation quality, we conduct two experiments. In the ﬁrst experiment, we apply human-transcribed segments and punctuation marks to the output of the speech recognition system. Thus, words are still from an ASR system, but the segments and punctuation marks are reused from a human-generated trans"
2012.iwslt-papers.15,2011.iwslt-papers.6,1,0.827411,"ﬁcantly improved translation performance. 3. System Description In this section we brieﬂy introduce the statistical MT system that we use in this experiment. As we work on translating speech in this experiment, we use the parallel TED1 data and manual transcripts of lecture data containing 63k sentences as indomain data and adapt our models at the domain. The lecture data is collected internally at our university, and the domain of each lecture differs from the others. To better cope with domain-speciﬁc terminologies in university lectures, Wikipedia2 title information is used as presented in [8]. For development and testing, we use the lecture data from different speakers. These are also collected internally from university classes and events. They consist of talks of 30 to 45 minutes and the topic varies from one speech to the other. For the development set we use manual transcripts of lectures, while for testing we use the transcripts generated by an ASR system. The development set consists of 14K parallel sentences, with 30K words on the source side and 33K words on the target side including punctuation marks. Detailed information on the source side of the test set, including the"
2012.iwslt-papers.15,2005.iwslt-1.19,0,\N,Missing
2012.iwslt-papers.15,P02-1040,0,\N,Missing
2012.iwslt-papers.3,C90-3038,0,0.788621,"stricted Boltzmann Machine (RBM) can be calculated very efﬁciently. This enables us to use the language models during the decoding of the source sentence and not only in a re-scoring step. The remaining paper is structured as follows: First we will review related work. Afterwards a brief overview of Restricted Boltzmann Machines will be given before we describe the RBM-based language model. In Section 5 we describe the results on different translation tasks. Afterwards, we will give a conclusion. 2. Related Work A ﬁrst approach to predict word categories using neural networks was presented in [1]. Later, [2] used neuronal networks for statistical language modelling. They described in detail an approach based on multi-layer perceptrons and could show that this reduces the perplexity on a test set compared to n-gram-based and class-based language models. In addition, they gave a short outlook to energy minimization networks. An approach using multi-layer perceptrons has successfully been applied to speech recognition by [3], [4] and [5]. One main problem of continuous space language models is the size of the output vocabulary in large vocabulary continuous speech recognition. A ﬁrst way"
2012.iwslt-papers.3,W08-0303,1,0.53004,"n We evaluated the RBM-based language model on different tasks. We will ﬁrst give a brief description of our SMT system. Then we will describe in detail our experiments on the German to English translation task. Afterwards, we will describe some more experiments on the English to French translation task. 5.1. System description The translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] an"
2012.iwslt-papers.3,P07-2045,0,0.00309671,"the English to French translation task. 5.1. System description The translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We"
2012.iwslt-papers.3,W11-2124,1,0.645436,"nd TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K w"
2012.iwslt-papers.3,D10-1076,0,0.0536508,"a short list. Recently, [6] presented a structured output layer neural network which is able to handle large output vocabularies by using automatic word classes to group the output vocabulary. 164 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzm"
2012.iwslt-papers.3,D07-1045,0,0.0957896,"rkshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzmann Machines (RBM). We will concentrate only on the points that are important for our RBM-based language model, which will be described in detail in the next section. RBMs are a generative model that have already been use"
2012.iwslt-papers.3,N12-1005,0,0.30541,"anguage Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzmann Machines (RBM). We will concentrate only on the points that are important for our RBM-based language model, which will be described in detail in the next section. RBMs are a generative model that have already been used successfully in m"
2012.iwslt-papers.3,2007.tmi-papers.21,0,0.559076,"in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown"
2012.iwslt-papers.3,W09-0435,1,0.782064,"lignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown in Table 1. The baseline system uses"
2012.iwslt-papers.3,W05-0836,1,0.90952,"kage [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown in Table 1. The baseline system uses a 4-gram language model trained on the target side of all parallel data. If we add a 4-gram RBM-based language model trained only on the TED data for 1 iteration using 32 hidden units we can improve the translation quality on t"
2012.iwslt-papers.3,E99-1010,0,0.422024,"erman TED lectures into English are shown in Table 1. The baseline system uses a 4-gram language model trained on the target side of all parallel data. If we add a 4-gram RBM-based language model trained only on the TED data for 1 iteration using 32 hidden units we can improve the translation quality on the test data by 0.8 BLEU points (RBMLM H32 1Iter). We can gain additional 0.6 BLEU points by carrying out 10 instead of only 1 iteration of contrastive divergence. If we use a factored language model trained on the surface word forms and the automatic clusters generated by the MKCLS algorithm [23] (FRBMLM H32 1Iter), we can get an improvement of 1.1 BLEU points already after the ﬁrst iteration. We grouped the words into 50 word classes by the MKCLS algorithm. If we add an n-gram-based language model trained only on the in-domain data (Baseline+NGRAM), we can improve by 1 BLEU point over the baseline system. So the factored RBM-based language model as well as the one trained for 10 iteration can outperform the second n-gram-based language model. We can get further improvements by combining the ngram-based in-domain language model and the RBM-based language model. In this case we use 3 d"
2013.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.863584,"al of 217 primary runs submitted. All runs were evaluated with objective metrics on a current test set and two progress test sets, in order to compare the progresses against systems of the previous years. In addition, submissions of one of the official machine translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been now running for a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9]. The 2013 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Machine translation (MT), i.e. the translation of a polished transcript into another language, • Spoken language translation (SLT), that addressed the conversion and translation"
2013.iwslt-evaluation.1,2012.eamt-1.60,1,0.897031,"on tracks, many other optional translation directions were also offered. Optional SLT directions were from English to Spanish, Portuguese (B), Italian, Chinese, Polish, Slovenian, Arabic, and Persian. Optional MT translation directions were: English from/to Arabic, Spanish, Portuguese (B), Italian, Chinese, Polish, Persian, Slovenian, Turkish, Dutch, Romanian, and Russian. For each official and optional translation direction, training and development data were supplied by the organizers through the workshop’s website. Major parallel collections made available to the participants were the WIT3 [10] corpus of TED talks, all data from the WMT 2013 workshop [11], the MULTIUN corpus [12], and the SETIMES parallel corpus [13]. A list of monolingual resources was provided too, that includes both freely available corpora and corpora available from the LDC. Test data were released at the begin of each test period, requiring participants to return one primary run and optional contrastive runs within one week. The schedule of the evaluation was organized as follows: June 8, release of training data; Sept 2-8, ASR test of period; Sept 9-15, SLT test period; Oct 7-13, MT test period; Oct 7-20, test"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.21,0,0.0238082,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.11,0,0.0818098,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.5,0,0.0447354,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.6,0,0.0540129,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.9,0,0.0348889,"Missing"
2013.iwslt-evaluation.1,2013.iwslt-evaluation.23,0,0.0935135,"Missing"
2013.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.125368,"y, the standard dev2010 and tst2010 development sets have been released as well. Tables 2 and 3 provide statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear inte"
2013.iwslt-evaluation.1,N04-4038,0,0.0999494,"tion purposes for the official directions. Reference results from baseline MT systems on the development set tst2010 are provided via the WIT3 repository. This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official languag"
2013.iwslt-evaluation.1,P07-2045,1,0.010973,"This helps participants and MT scientists to assess their experimental outcomes. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [38] was applied to all languages, with the exception of Chinese and Arabic languages, which were preprocessed by, respectively: the Stanford Chinese Segmenter [39]; either AMIRA [40], in the Arabic-to-English direction, or the QCRI-normalizer,3 in the English-to-Arabic direction. The baselines were developed with the Moses toolkit [41]. Translation and lexicalized reordering models were trained on the parallel training data; 5-gram LMs with improved Kneser-Ney smoothing were estimated on the target side of the training parallel data with the IRSTLM toolkit [42]. The weights of the log-linear interpolation model were 3 Specifically developed for IWSLT 2013 by P. Nakov and F. Al-Obaidli at Qatar Computing Research Institute. Table 3: Bilingual resources for official language pairs task MTEnF r MTDeEn MTEnDe data set train dev2010 tst2010 tst2011 tst2012 tst2013 train dev2010 tst2010 dev2012 tst2013 train dev2010 tst2010 tst20"
2013.iwslt-evaluation.1,2012.amta-papers.22,1,0.822673,"Missing"
2013.iwslt-evaluation.1,2006.amta-papers.25,0,0.207086,"Missing"
2013.iwslt-evaluation.1,J93-3001,0,0.516669,"Missing"
2013.iwslt-evaluation.1,W05-0908,0,0.0754408,"Missing"
2013.iwslt-evaluation.1,1993.eamt-1.1,0,0.4595,"Missing"
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.iwslt-evaluation.24,2012.eamt-1.60,0,0.0112029,"r text and speech translation for all the official language pairs: English→German, German→English and English→French as well as two optional directions. The TED tasks consist of automatic translation of both the manual transcripts (MT task) and transcripts generated by automatic speech recognizers (SLT task) for talks held at the TED conferences1 . For German→English, the test data was collected from the TEDx project2 . The TED talks are given in English in a large number of different domains. Some of these talks are manually transcribed and translated by global volunteers into many languages [2]. The TED translation tasks this year bring up interesting challenges: (1) the problem of adapting general models - mainly trained on news data - towards the diverse topics in TED talks, (2) the need of universal techniques for translating texts from and to various languages, and (3) the appropriate solution for inserting punctuation marks and case information on automatic speech recognition (ASR) outputs for the spoken language translation (SLT) task. To deal with those challenges, we provided several advanced adaptation methods both for translation and language models to leverage both the wi"
2013.iwslt-evaluation.24,E03-1076,0,0.0541217,", plus Giga for English→French. The monolingual data we used include the monolingual part of those parallel data, the News Shuffle corpus for all three directions and additionally the Gigaword corpus for English→French and German→English. A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use"
2013.iwslt-evaluation.24,2011.iwslt-evaluation.9,1,0.848162,"aword corpus for English→French and German→English. A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabi"
2013.iwslt-evaluation.24,W11-2123,0,0.0188113,"ength difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the"
2013.iwslt-evaluation.24,W08-0303,1,0.822978,"an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all"
2013.iwslt-evaluation.24,P07-2045,0,0.00484553,"nce pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like"
2013.iwslt-evaluation.24,W08-1006,0,0.0269265,"lly learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position of each word. Then the lattice is used as input to the decoder. During decoding the lexicalized reordering m"
2013.iwslt-evaluation.24,P03-1054,0,0.00975892,"lly learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position of each word. Then the lattice is used as input to the decoder. During decoding the lexicalized reordering m"
2013.iwslt-evaluation.24,W06-1607,0,0.0321156,"language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is"
2013.iwslt-evaluation.24,2012.amta-papers.19,1,0.820782,"provides the reordering probability for each phrase pair. At the phrase boundaries, the reordering orientation with respect to the original position of the words is checked. The probability for the respective orientation is included as an additional score in the log-linear model of the translation system. 5. Adaptation In order to achieve the best performance on the target domain, we perform adaptation for translation models as well as language models. We adapt the translation model (TM) by using the scores from the in-domain and out-of-domain phrase table as described in the backoff approach [22]. This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was"
2013.iwslt-evaluation.24,W11-2124,1,0.900081,"s. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of w"
2013.iwslt-evaluation.24,P10-2041,0,0.0251885,"he general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. T"
2013.iwslt-evaluation.24,E99-1010,0,0.0109,". In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of word classes. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs special preprocessing on the data, since generally there is no or no reliable case information and punctuation in the automatically generated transcripts. We have used a monolingual translation system as shown in [14] to deal with the difference in casing and punctuation between a machine translation (MT) and an SLT system. In contrast to the condition in their"
2013.iwslt-evaluation.24,D09-1022,0,0.0244263,"n, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. The target context features encode information about the surrounding target words. One specialty of the TED trans"
2013.iwslt-evaluation.24,2012.iwslt-papers.15,1,0.824592,"uage model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of word classes. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs special preprocessing on the data, since generally there is no or no reliable case information and punctuation in the automatically generated transcripts. We have used a monolingual translation system as shown in [14] to deal with the difference in casing and punctuation between a machine translation (MT) and an SLT system. In contrast to the condition in their work, in this evaluation campaign sentence boundaries are present in the test sets. Therefore, we use this monolingual translation system for predicting commas instead of all punctuation marks in the test set. In addition to predicting commas, we also predict casing of words using the monolingual translation system. This preprocessing will be denoted as Monolingual Comma and Case Insertion (MCCI). In order to build the monolingual system which trans"
2013.iwslt-evaluation.24,2005.iwslt-1.8,0,0.0415505,"tokens is used. Word reordering is ignored in these systems. In order to capture more context, we use a 9-gram language model trained on part-ofspeech (POS) tokens. Moreover, a 9-gram cluster language model is trained on 1,000 clusters, based on the MKCLS algorithm as described in the baseline system. For the speech translation tasks, the output of the monolingual translation system becomes the input to our regular translation system which is trained using data with punctuation marks. 4. Word Reordering Model Word reordering is modeled in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19"
2013.iwslt-evaluation.24,2007.tmi-papers.21,0,0.0281074,"ich is trained using data with punctuation marks. 4. Word Reordering Model Word reordering is modeled in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants"
2013.iwslt-evaluation.24,W13-2264,1,0.768923,"ese language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. The target context features encode information about the surrounding target words. One specialty of the TED translation task is that we have a lot of parallel data we can train our models on. However, only a quite small portion of these data, the TED corpus, is very important for the translation quality. Therefore, we achieve a better translation performance by training the models only on the TED data. The RBM used for the language model consists of two"
2013.iwslt-evaluation.24,N12-1005,0,0.0177851,"seline system also includes a cluster-based language model using the clusters automatically generated by the MKCLS toolkit. System Baseline + Tree-based Rules + Lexicalized Reordering + POSLM + DWL + Class-based 9-gram LMs + TargetContext + LM DataSelection Dev 23.58 23.61 23.74 23.81 24.44 24.19 24.24 Test 23.50 23.87 23.93 24.14 24.76 24.93 25.06 Table 1: Experiments for English→German (MT) 7. Continuous Space Language Model In recent years, different approaches to integrate continuous space models have shown significant improvements in the translation quality of machine translation systems [26]. Since the long training time is the main disadvantage of this model, we only train it on the small, but very domain-relevant TED corpus. In contrast to most other approaches, we did not use a feed-forward neural network, but used a Restricted Bolzmann Machine (RBM). The main advantage of this approach is that the free energy of the model, which is proportional to the language model probability, can be calculated very efficiently. Therefore, we are able to use the RBM-based language model during decoding and not only in the rescoring phase. By adding tree-based reordering rules and a lexicali"
2013.iwslt-evaluation.24,2012.iwslt-papers.3,1,0.814901,"our models on. However, only a quite small portion of these data, the TED corpus, is very important for the translation quality. Therefore, we achieve a better translation performance by training the models only on the TED data. The RBM used for the language model consists of two layers, which are fully connected. In the input layer, for every word position there are as many nodes as words in the vocabulary. Since we used a 4-gram language model, there are 4 word positions in the input layer. These nodes are connected to 32 hidden units in the hidden layer. The model is described in detail in [27]. 8. Results In this section, we present a summary of our experiments for all tasks we have carried out for the IWSLT 2013 evaluation. All the reported scores are case-sensitive BLEU scores calculated based on the provided development and test sets. 8.1. English→German We conducted several experiments for English→German translation using the available data. They are summarized in Table 1. The baseline system is a phrase-based translation system using POS-based reordering rules. Preprocessing of the source and target language of the training corpora is performed as described above. Adaptation o"
2013.iwslt-evaluation.24,W09-0435,1,0.841052,"in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reor"
2013.iwslt-evaluation.24,W13-0805,1,0.826891,"15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position o"
2013.iwslt-papers.11,2005.iwslt-1.8,0,0.0254136,"ents. We first present related work regarding reordering methods in machine translation and reference work on judging the quality of a given reordering. Then we mention work using oracles for the analysis of machine translation systems. Word reordering has been addressed by many approaches in statistical systems. In a state-of-the-art phrase-based machine translation system, the decoder processes the source sentence left to right, but allows changes in the order of source words while the translation hypothesis is generated. Many phrase-based systems also include a lexicalized reordering model [1] which provides additional reordering information for phrase pairs. It stores statistics on the orientation of adjacent phrase pairs on the lexical level. A very popular approach is to detach the reordering from the decoding procedure and to perform the reordering on the source sentence before translation. Such pre-reordering approaches use linguistic information about the source and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone transl"
2013.iwslt-papers.11,C04-1073,0,0.10438,"to detach the reordering from the decoding procedure and to perform the reordering on the source sentence before translation. Such pre-reordering approaches use linguistic information about the source and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], autom"
2013.iwslt-papers.11,P05-1066,0,0.145231,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,popovic-ney-2006-pos,0,0.0256285,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,2007.mtsummit-papers.29,0,0.0388472,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,D07-1077,0,0.027371,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,2007.tmi-papers.21,0,0.396563,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W07-0401,0,0.0810026,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W08-0307,0,0.0177742,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W09-0435,1,0.912576,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W06-1609,0,0.244986,"for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approache"
2013.iwslt-papers.11,2009.eamt-1.27,0,0.0201326,"y trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a"
2013.iwslt-papers.11,C10-1043,0,0.0158662,"s based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding r"
2013.iwslt-papers.11,D13-1049,0,0.13395,"ags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding reordering metrics and reordering quality includes the first descri"
2013.iwslt-papers.11,W13-0805,1,0.629678,"g rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding reordering metrics and reordering quality includes the first description of reorderings as permutations [16]. Later, the use of permutation distance metrics to measure reordering"
2013.iwslt-papers.11,D10-1091,0,0.0244358,"arding reordering metrics and reordering quality includes the first description of reorderings as permutations [16]. Later, the use of permutation distance metrics to measure reordering quality [17] leveraged research into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on sour"
2013.iwslt-papers.11,2012.iwslt-papers.15,1,0.840141,"utations [16]. Later, the use of permutation distance metrics to measure reordering quality [17] leveraged research into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VM"
2013.iwslt-papers.11,E12-1013,0,0.0190364,"search into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: Fi"
2013.iwslt-papers.11,W07-0414,0,0.0153014,"to permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: First, we investigate a reordering approach where reordering decisions are not deterministic. Inst"
2013.iwslt-papers.11,I11-1005,0,0.014754,"to permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: First, we investigate a reordering approach where reordering decisions are not deterministic. Inst"
2014.amta-researchers.17,W14-3313,1,0.878749,"Missing"
2014.amta-researchers.17,P07-2045,0,0.00783789,"25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate th"
2014.amta-researchers.17,D10-1076,1,0.734365,"the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in Le et al. (2010). In Niehues and Waibel (2012a), another RBM-based language model was introduced. This approach differs from the one intrdoduced in Mnih and Hinton (2007) by a simpler layout that allows us for a fast probability computation. This yields the integration of the model during the decoding step feasible. However, the training complexity heavily depends on the vocabulary size and this model can be trained on a limited amount of training data. In Dahl et al. (2012), a sampling method was presented to efficiently train restricted Boltzmann machines on word observations. This approach enables us to tr"
2014.amta-researchers.17,N12-1005,1,0.845844,"language models is the size of the output vocabulary in large vocabulary continuous speech recognition. A first way to overcome this is to use a short list. Recently, Le et al. (2011) presented a structured output layer neural network which is able to handle large output vocabularies by using a clustering tree to represent the output vocabulary. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In Schwenk et al. (2007) as well as in Le et al. (2012) the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in"
2014.amta-researchers.17,C90-3038,0,0.341794,"advantage of the RBM-based language model to use the language model during decoding. The remaining paper is structured as follows. First we review related work and then provide in section 3 a brief overview of RBM-based language models. Section 4 describes the tailored sampling strategies while we describe how the shared word representation is integrated into the RBM layout in section 5. Afterwards we describe and discuss experimental results measured in terms of translation quality in section 6. 2 Related Work A first approach to predict word categories using neural networks was presented in Nakamura et al. (1990). Later, Bengio et al. (2003) introduced neural networks for statistical language modeling. The authors described in detail an approach based on multi-layer neural networks and reported a significative perplexity reduction compared to conventional and class-based language models. In addition, they gave a short outlook to energy minimization networks. An approach using multi-layer neural networks has successfully been applied to speech recognition by Schwenk and Gauvain (2002), Schwenk (2007) and Mikolov et al. (2010). One main problem of continuous space language models is the size of the outp"
2014.amta-researchers.17,W11-2124,1,0.832421,"the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the"
2014.amta-researchers.17,W09-0435,1,0.867497,"and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we opti"
2014.amta-researchers.17,W08-0303,1,0.877148,"AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 226 Sampling size No RBMLM No Sampling 5 10 50 100 1000 BLEU Score 25.16 25.42 25.15 25.01 25.23 25.25 25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kols"
2014.amta-researchers.17,2012.iwslt-papers.3,1,0.361369,"ently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by techniques developed for other NN-based language model, we tackle in this work these two issues of RBM-based"
2014.amta-researchers.17,2007.tmi-papers.21,0,0.488897,"iminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used"
2014.amta-researchers.17,D07-1045,0,0.0703798,"Missing"
2014.amta-researchers.17,D13-1140,0,0.128933,"borhood can be modeled. In state of the art language models contexts of up to 10 words are used. Currently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by tech"
2014.amta-researchers.17,W05-0836,1,0.901514,"(Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we optimized and tested the systems on a set of computer science lectures collected at a university. The language models were tested on three different conditions. First, we used the baseline system, then we used a system, which has been adapted to the TED task by using an additional"
2014.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,0.792952,"pants were also asked to submit runs on the 2013 test set (progress test set), in order to measure the progress of systems with respect to the previous year. All runs were evaluated with objective metrics, and submissions for two of the official text translation tracks were also evaluated with human post-editing. 1. Introduction This paper overviews the results of the 2014 evaluation campaign organized by the International Workshop of Spoken Language Translation. The IWSLT evaluation has been running now for over a decade and has offered along these years a variety of speech translation tasks [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]. The 2014 IWSLT evaluation continued along the line set in 2010, by focusing on the translation of TED Talks, a collection of public speeches covering many different topics. As in the previous two years, the evaluation included tracks for all the core technologies involved in the spoken language translation task, namely: • Automatic speech recognition (ASR), i.e. the conversion of a speech signal into a transcript, • Spoken language translation (SLT), that addressed the conversion and translation of a speech signal into a transcript in another language, • Machine translation (MT), i.e. the tr"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.15,0,0.0679155,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.14,0,0.0614042,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.7,1,0.877515,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.13,0,0.0364543,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.5,1,0.82235,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.12,0,0.0879094,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.21,0,0.047922,"Missing"
2014.iwslt-evaluation.1,2014.iwslt-evaluation.10,0,0.0535615,"Missing"
2014.iwslt-evaluation.1,1993.eamt-1.1,0,0.523227,"Missing"
2014.iwslt-evaluation.1,2005.mtsummit-papers.11,0,0.0265085,"time that Italian is involved in ASR/SLT tracks, therefore no evaluation set is available for assessing progress. A single TEDx based development set was released for each pair, together with standard TED based development sets dev2010, tst2010, tst2011 and tst2012 sets. Tables 2 and 3 provides statistics on in-domain texts supplied for training, development and evaluation purposes for the official directions. MT baselines were trained from TED data only, i.e. no additional out-of-domain resources were used. The standard tokenization via the tokenizer script released with the Europarl corpus [33] was applied to all languages, with the exception of Chinese and Arabic languages, which were sent 179k 887 1,664 818 1,124 1,026 1,305 172k 887 1,565 1,433 1,700 993 1,305 1,165 1,363 1,414 182k 887 1,529 1,433 1,704 1,402 1,183 1,056 883 tokens talks En Fr 3.63M 3.88M 1415 20,1k 20,2k 8 32,0k 33,9k 11 14,5k 15,6k 8 21,5k 23,5k 11 21,7k 23,3k 16 24,8k 27,5k 15 En De 3.46M 3.24M 1361 20,1k 19,1k 8 32,0k 30,3k 11 26,9k 26,3k 16 30,7k 29,2k 15 20,9k 19,7k 16 24,8k 23,8k 15 21,6k 20,8k 7 23,3k 22,4k 9 28,1k 27,6k 10 En It 3.68M 3.44M 1434 20,1k 17,9k 8 31,0k 28,7k 10 26,9k 24,5k 16 30,7k 28,2k 15"
2014.iwslt-evaluation.1,2012.amta-papers.22,1,0.779032,"amely the MT English-German (EnDe) track and MT English-French (EnF r) track. Following the methodology introduced last year, human evaluation was based on PostEditing, and HTER (Human-mediated Translation Edit Rate) was adopted as the official evaluation metric to rank the systems. Post-Editing, i.e. the manual correction of machine translation output, has long been investigated by the translation industry as a form of machine assistance to reduce the costs of human translation. Nowadays, Computer-aided translation (CAT) tools incorporate post-editing functionalities, and a number of studies [35, 36] demonstrate the usefulness of MT to increase professional translators’ productivity. The MT TED task offered in IWSLT can be seen as an interesting application scenario to test the utility of MT systems in a real subtitling task. 5.4. Results Table 4: BLEU and TER scores of baseline SMT systems on all tst2014 sets. († ) TEDx test set. (⋆ ) Char-level scores. pair En Fr De It Ar Es Fa He Nl Pl Pt Ro Ru Sl Tr Zh direction BLEU 32.07 18.33 27.15 11.13 31.31 11.31 15.91 22.77 9.63 31.25 18.05 11.74 8.46 7.75 ⋆ 16.49 → TER 48.62 62.11 53.19 73.01 48.29 71.20 65.62 58.38 82.81 47.25 65.25 71.99 73."
2014.iwslt-evaluation.1,2006.amta-papers.25,0,0.397152,"paign, our goal was to adopt a human evaluation framework able to maximize the benefit to the research community, both in terms of information about MT systems and data and resources to be reused. With respect to other types of human assessment, such as judgments of translation quality (i.e. adequacy/fluency and ranking tasks), the post-editing task has the double advantage of producing (i) a set of edits pointing to specific translation errors, and (ii) a set of additional reference translations. Both these byproducts are very useful for MT system development and evaluation. Furthermore, HTER[37] - which consists of measuring the minimum edit distance between the machine translation and its manually post-edited version - has been shown to correlate quite well with human judgments of MT quality. First of all, for reference purposes Table 4 shows BLEU and TER scores on the tst2014 evaluation sets of the baseline systems we developed as described in Section 5.1. The results on the official test set for each participant are shown in Appendix A.1. For most languages, we show the case-sensitive and case-insensitive BLEU and TER scores. The human evaluation setup and the collection of posted"
2014.iwslt-evaluation.1,J93-3001,0,0.560866,"Missing"
2014.iwslt-evaluation.17,2011.iwslt-evaluation.9,1,0.868455,"English→French and German→English. The English→Chinese system setup is described in Section 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Sp"
2014.iwslt-evaluation.17,J03-1002,0,0.0076696,"tion 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Knese"
2014.iwslt-evaluation.17,E03-1076,0,0.139387,"For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle corpus. Additionally, we included the Gigaword corpus for English→French and German→English. The English→Chinese system setup is described in Section 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translation"
2014.iwslt-evaluation.17,P07-2045,0,0.00813363,"ference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we u"
2014.iwslt-evaluation.17,2012.amta-papers.19,1,0.864002,"lit up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [1"
2014.iwslt-evaluation.17,W11-2123,0,0.0404059,"stic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our syste"
2014.iwslt-evaluation.17,W11-2124,1,0.885113,"6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16,"
2014.iwslt-evaluation.17,E99-1010,0,0.400501,"8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as describ"
2014.iwslt-evaluation.17,W08-1006,0,0.0548998,"o increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic spee"
2014.iwslt-evaluation.17,P03-1054,0,0.0453561,"o increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic spee"
2014.iwslt-evaluation.17,2007.tmi-papers.21,0,0.338063,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,W09-0435,1,0.927039,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,W13-0805,1,0.88806,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,2005.iwslt-1.8,0,0.291731,"asses. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training dat"
2014.iwslt-evaluation.17,W13-2264,1,0.831034,"se two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects"
2014.iwslt-evaluation.17,W05-0836,1,0.81298,"based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects case information, so that there is less divergence between the MT training data and the SLT input data. A"
2014.iwslt-evaluation.17,2012.iwslt-papers.15,1,0.883589,"DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects case information, so that there is less divergence between the MT training data and the SLT input data. As sentence boundaries are already given in the test sets, we leave them as they are but predict other punctuation marks within the segment. This preprocessing will be denoted as Monolingual Comma and Case Insertion (MCCI). For building the systems, we took the preprocessed source side of the parallel training data. We remove all punctuation marks from the data and insert a final period at the end of each line. In addition to this, all word"
2014.iwslt-evaluation.17,2012.iwslt-papers.3,1,0.926316,"target side of the monolingual translation system, we keep the punctuation marks as well as case information, so that the “translation” of our MCCI system consists of inserting punctuation marks and correcting case information. We built an MCCI system for English and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source"
2014.iwslt-evaluation.17,2014.amta-researchers.17,1,0.856794,"serting punctuation marks and correcting case information. We built an MCCI system for English and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous 5 source words and its following 5 source words. We create this set of 8 target and 11 source words for every target"
2014.iwslt-evaluation.17,P14-1129,0,0.0721445,"and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous 5 source words and its following 5 source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel in-domain TED corpus. In rescoring, we then calculate the free energy o"
2014.iwslt-evaluation.17,P12-2059,0,0.0524998,"task [26]. 6. Arabic transliteration In most cases, untranslated words break the harmony of the translation into a language which uses a different scripting (e.g. English into Arabic.) Therefore, it is more conve120 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 NP CD ten Figure 1: Examples of trivial correspondences nient to transliterate those untranslated words, as they are unlikely to hurt the system performance further. Our transliteration is mostly similar to the character-based translation in its transliteration part [27]. It is consequently a statistical phrase-based translation based on unigram characters. The corresponding training data of this system is mainly a subset of the word pairs obtained from the aligned corpora (TED and UN). First, the Arabic word of each aligned pair is roughly transliterated into English, using only trivial correspondences (see Fig. 1 for an example). The Levenshtein distance ratio is then computed between the resulting rough transliteration and the English word. Finally, we retain only pairs with ratios higher than a certain threshold (our threshold was empirically set to 0.5)."
2014.iwslt-evaluation.17,2014.iwslt-papers.18,1,0.843414,"esponding training data of this system is mainly a subset of the word pairs obtained from the aligned corpora (TED and UN). First, the Arabic word of each aligned pair is roughly transliterated into English, using only trivial correspondences (see Fig. 1 for an example). The Levenshtein distance ratio is then computed between the resulting rough transliteration and the English word. Finally, we retain only pairs with ratios higher than a certain threshold (our threshold was empirically set to 0.5). NP JJ big For our English-Chinese translation we applied a novel rulebased preordering approach [28], which uses the tree information of multiple syntactic levels. This approach extends the tree-based reordering [17] from one level into multiple levels, which has the capability to process complex reordering cases. Reordering patterns are based on multiple levels of the syntax tree. Figure 2 illustrates how the reordering patterns are detected. The detection starts from the root node of the syntax tree, goes downwards multiple levels and uses the nodes in these levels to detect the reordering pattern. In this example, the nodes that are used for detecting the reordering pattern are colored gr"
2014.iwslt-evaluation.17,P10-2041,0,0.038163,"adapted by combining two phrase tables, one trained on all training data and one trained only on the TED in-domain corpus. Furthermore, the translation process is modeled using a bilingual language model trained on all parallel data and a discriminative word lexicon trained on the TED corpus. The DWL uses source context features. Finally, five language models are used. Three are word-based models, the first of which is trained on all available German data. The second one is trained only on the TED corpus. Finally, we use a word-based model trained on 5M sentences chosen through data selection [29]. In addition, a 9-gram POS-based language model and a 9-gram cluster language model using 1000 MKCLS classes are used. Afterwards, we rescored the system using the weights trained using the ListNet algorithm described in Section 4. The rescoring was trained on the test2010 and test2011 data and dev2010 was used as a cross-validation set. This results in an improvement of 0.3 BLEU points. Then we added an RBM-based language model and an RBM-based translation model. We could improve by using the RBM-based translation model by 0.4 BLEU points, reaching the best BLEU score on test2012 with 24.31"
2014.iwslt-evaluation.17,C08-1098,0,0.0173077,"hoe, December 4th and 5th, 2014 System Primary Stemmed Dev 39.03 39.22 Test 31.98 31.68 Table 4: Contrastive system for German→English (MT) the information encoded in inflections such as gender or case may be discarded. However, stemming the whole German corpus hurts translation since too much information is lost. We therefore experimented with only stemming adjectives, which in German can have five different suffixes depending on the gender and case. The stemming was performed on the preprocessed files before compound splitting. The files were tagged with the TreeTagger [12] and the RFTagger [30]. We based our decision when and how to stem on the fine-grained tags output by the RFTagger. We only stemmed words tagged as an attributive adjective, since they are inflected in German. If the word as tagged as a comparative or superlative, we manually removed the inflected suffix in order to maintain the comparative nature of the adjective. For all other adjectives, we used the stem output by the TreeTagger. After stemming, compound splitting was applied as described in Section 2. We then trained a new alignment and phrasetable on the stemmed corpora. Previous experiments had shown that usi"
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2014.iwslt-papers.10,N03-1017,0,0.0863198,"using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task. 1. Introduction Since the first attempt to statisical machine translation (SMT) [1], the approach has drawn much interest in the research community and huge improvements in translation quality have been achieved. Still, there are plenty of problems in SMT which should be addressed. One is that the translation decision depends on a quite small context. In standard phrase-based statistical machine translation (PBMT) [2], the two main components are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminati"
2014.iwslt-papers.10,D09-1022,0,0.282573,"nts are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminative Word Lexicon (DWL) [3][4] exploits the occurence of all the words in the whole source sentence to predict the presence of words in the target sentence. This wider context information is encoded as features and employed in a discriminative framework. Hence, they train a maximum entropy (MaxEnt) model for each target word. While this model can improve the translation quality in different conditions, MaxEnt models are linear classifiers. On the other hand, hierarchical non-linear classifiers can model dependencies between different source words better since they perform some abstraction over the input. Hence, introduc"
2014.iwslt-papers.10,W13-2264,1,0.936673,"are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminative Word Lexicon (DWL) [3][4] exploits the occurence of all the words in the whole source sentence to predict the presence of words in the target sentence. This wider context information is encoded as features and employed in a discriminative framework. Hence, they train a maximum entropy (MaxEnt) model for each target word. While this model can improve the translation quality in different conditions, MaxEnt models are linear classifiers. On the other hand, hierarchical non-linear classifiers can model dependencies between different source words better since they perform some abstraction over the input. Hence, introducing"
2014.iwslt-papers.10,W11-2124,1,0.93285,"e. In PBMT, the lexical joint models allow us to use local source and target contexts in the form of phrases. Lately, advanced joint models have been proposed to either enhance the joint probability model between source and target sides or engage more suitable contexts. The n-gram based approach [6] directly models the joint probability of source and target sentences from the conditional probability of a current n-gram pair givens sequences 223 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phras"
2014.iwslt-papers.10,N13-1090,0,0.0129941,"ceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentiall"
2014.iwslt-papers.10,N12-1005,0,0.0746463,"11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the jo"
2014.iwslt-papers.10,P14-1129,0,0.0504957,"n. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et"
2014.iwslt-papers.10,C12-2104,0,0.01501,"ently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest anothe"
2014.iwslt-papers.10,P07-1020,0,0.0138072,"n et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of ta"
2014.iwslt-papers.10,D08-1039,0,0.0155651,"onger source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of target words. They tre"
2014.iwslt-papers.10,D07-1007,0,0.0417135,"ds as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of target words. They treat the selection process as a Word Sense Disambiguation (WSD) task, where target words or phrases are WSD senses. They extract a rich feature set from the source sentences, including source words, and input them into a WSD classifier. Still, the problem persists since they use the shallow classifiers for that task. Considering the advantages of non-linear models mentioned before, we opt for using deep neural network architectures to learn the DWL. W"
2014.iwslt-papers.10,2012.eamt-1.60,0,0.0264452,"e experiments. Validation Sent. Tok. (avg.) Sent. Tok. (avg.) En-Fr 149991 3.1m 6153 125k En-Zh 140006 3.3m 8962 211k De-En 130654 2.5m 7430 142k Table 1: Statistics of the corpora used to train NNDWL 4.1. System description The system we use as our baseline is a state-of-the-art translation system for English to French without any DWL. To the baseline system, we add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target wo"
2014.iwslt-papers.10,P07-2045,0,0.00428231,"is a state-of-the-art translation system for English to French without any DWL. To the baseline system, we add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as d"
2014.iwslt-papers.10,2012.amta-papers.19,1,0.767356,"add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the siz"
2014.iwslt-papers.10,E99-1010,0,0.0823202,"he phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the sizes of the hidden layers |H1 |, |H2 |, |H3 |are 1000, 500, 1000, respectively. If we use the original source and target vocabularies, for the English→French direction trained on preprocessed TED 2013 data, Vs includes 47957 words and Vt includes 62660 words. Because of the non-linearity calculations through such a large network, the training is extremely time-consuming. In order to boost the efficiency, we limit the source an"
2014.iwslt-papers.10,2007.tmi-papers.21,0,0.380598,"he general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the sizes of the hidden layers |H1 |, |H2 |, |H3 |are 1000, 500, 1000, respectively. If we use the original source and target vocabularies, for the English→French direction trained on preprocessed TED 2013 data, Vs includes 47957 words and Vt includes 62660 words. Because of the non-linearity calculations through such a large network, the training is extremely time-consuming. In order to boost the efficiency, we limit the source and target vocabularies to the most frequent ones. All words outside the lists are t"
2014.iwslt-papers.4,P04-1005,0,0.0913588,"y removal. In order to explore the importance of domain in this task, we train disfluency removal models on in-domain and out-of-domain data and compare the results. Every experiment is conducted in two conditions whether turn information is available or not. Once the disfluencies of the meeting data are removed and punctuation marks are inserted, the data goes through our English to French MT system. For comparison, oracle experiments results and a baseline system are shown. 2. Related Work There has been extensive effort on disfluency removal on telephone speech, or Switchboard data [1]. In [2], Johnson et al. combined the noisy channel approach with a tree adjoining grammar for modeling speech disfluencies. In the noisy channel model, it is assumed that fluent text goes through a channel which adds disfluencies. Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the"
2014.iwslt-papers.4,P09-1084,0,0.0173964,"lts and a baseline system are shown. 2. Related Work There has been extensive effort on disfluency removal on telephone speech, or Switchboard data [1]. In [2], Johnson et al. combined the noisy channel approach with a tree adjoining grammar for modeling speech disfluencies. In the noisy channel model, it is assumed that fluent text goes through a channel which adds disfluencies. Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the scope of ASR. Baron et al. explored sentence boundary and disfluency detection in meetings using prosodic and lexical cues. For multi-party meeting data they used data collected as part of the ICSI Meeting Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore,"
2014.iwslt-papers.4,H01-1051,0,0.0196302,"Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the scope of ASR. Baron et al. explored sentence boundary and disfluency detection in meetings using prosodic and lexical cues. For multi-party meeting data they used data collected as part of the ICSI Meeting Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore, disfluency is viewed from a different perspective, as an interruption point, where once it occurs a new segment boundary is added. Baron et al. find that combining prosodic and word-based classifier information yields the best results for the given task. While the previous works have focused on enhancing the performance of speech recognition, Peitz et al. [6] compared the transla"
2014.iwslt-papers.4,2011.iwslt-papers.7,0,0.16,"ng Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore, disfluency is viewed from a different perspective, as an interruption point, where once it occurs a new segment boundary is added. Baron et al. find that combining prosodic and word-based classifier information yields the best results for the given task. While the previous works have focused on enhancing the performance of speech recognition, Peitz et al. [6] compared the translation performance using three different methods to punctuate TED talks. They compare methods depending on when and how the punctuation marks are inserted: prediction in the source language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translatio"
2014.iwslt-papers.4,2012.iwslt-papers.15,1,0.881532,"enhancing the performance of speech recognition, Peitz et al. [6] compared the translation performance using three different methods to punctuate TED talks. They compare methods depending on when and how the punctuation marks are inserted: prediction in the source language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzin"
2014.iwslt-papers.4,E14-4009,1,0.687316,"rce language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzing the importance of modeling contextual factors. They showed that word sense disambiguation using topic and domain knowledge yields a large improvement on MT performance. Recently Hassan et al. [10] investigated the impact of segmentation and disfluency removal"
2014.iwslt-papers.4,C10-1138,0,0.0193131,"ovements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzing the importance of modeling contextual factors. They showed that word sense disambiguation using topic and domain knowledge yields a large improvement on MT performance. Recently Hassan et al. [10] investigated the impact of segmentation and disfluency removal on translation of telephone speech. They use a CRF model to detect sentence units and a knowledge-based parser for complex disfluency removal. There are several notable differences between our and previous work. Contrary to many works in disfluency removal and punctuation insertion, our work is e"
2014.iwslt-papers.4,E09-1030,0,0.0855328,"ng involves 5 to 12 different speakers. All meetings are held in English. As in real meeting scenarios, the meeting participants consist of native and non-native English speakers. The eight meeting sessions are transcribed and then disfluencies are manually annotated. We use five of the meetings for training the disfluency removal model and the remaining three for testing. The test data is translated into French in order to evaluate the translation performance. 3.1.1. Speech disfluencies Disfluencies in the meeting data are annotated manually by human annotators. Previous work on disfluencies [2, 11, 12] categorized the disfluencies into three groups: filler, (rough)copy, and non-copy. filler contains filler words as well as discourse markers. Therefore, this class includes words such as uh, you know, and well in some cases. As the class name suggests, (rough)copy includes an exact or rough repetition of words or phrases. In spontaneous speech, speakers may repeat what has been already spoken, as stutter or correction. For example, a sentence There is, there was an advantage has (rough)copy in the phrase there is. non-copy includes the cases where the speaker aborts previously spoken segments"
2014.iwslt-papers.4,2013.iwslt-papers.12,1,0.752208,"ng involves 5 to 12 different speakers. All meetings are held in English. As in real meeting scenarios, the meeting participants consist of native and non-native English speakers. The eight meeting sessions are transcribed and then disfluencies are manually annotated. We use five of the meetings for training the disfluency removal model and the remaining three for testing. The test data is translated into French in order to evaluate the translation performance. 3.1.1. Speech disfluencies Disfluencies in the meeting data are annotated manually by human annotators. Previous work on disfluencies [2, 11, 12] categorized the disfluencies into three groups: filler, (rough)copy, and non-copy. filler contains filler words as well as discourse markers. Therefore, this class includes words such as uh, you know, and well in some cases. As the class name suggests, (rough)copy includes an exact or rough repetition of words or phrases. In spontaneous speech, speakers may repeat what has been already spoken, as stutter or correction. For example, a sentence There is, there was an advantage has (rough)copy in the phrase there is. non-copy includes the cases where the speaker aborts previously spoken segments"
2014.iwslt-papers.4,P05-1056,0,0.0287865,", we start with a sequence of words as input and need to mark parts of the sequence as disfluencies. This problem can intuitively be modeled as a sequence labeling task, where each word is either labeled by one of the disfluency classes (filler, (rough)copy, non-copy, and interruption), or by a label representing clean speech. Since sequence labeling is a common problem in NLP, it has been studied intensively. One succesful approach to model these problems is using CRF. As CRFs can represent long-range dependencies in the observations, they have shown good performance in sentence segmentation [14], parts of speech (POS) tagging [15] and shallow parsing [16]. In this work we use the CRF model implemented in the GRMM package [17] to mark the speech disfluencies. The CRF model was trained using L-BFGS, with the default parameters of the toolkit. 4.1. In-domain vs. out-of-domain data In the ideal case, disfluency annotated in-domain data is available for training the CRF model. However, the annotation of speech for different domains can be very timeconsuming. As disfluency annotated lecture data [13] is available, we use this data as our out-of-domain training data for the CRF model. As in"
2014.iwslt-papers.4,N03-1028,0,0.0305603,"parts of the sequence as disfluencies. This problem can intuitively be modeled as a sequence labeling task, where each word is either labeled by one of the disfluency classes (filler, (rough)copy, non-copy, and interruption), or by a label representing clean speech. Since sequence labeling is a common problem in NLP, it has been studied intensively. One succesful approach to model these problems is using CRF. As CRFs can represent long-range dependencies in the observations, they have shown good performance in sentence segmentation [14], parts of speech (POS) tagging [15] and shallow parsing [16]. In this work we use the CRF model implemented in the GRMM package [17] to mark the speech disfluencies. The CRF model was trained using L-BFGS, with the default parameters of the toolkit. 4.1. In-domain vs. out-of-domain data In the ideal case, disfluency annotated in-domain data is available for training the CRF model. However, the annotation of speech for different domains can be very timeconsuming. As disfluency annotated lecture data [13] is available, we use this data as our out-of-domain training data for the CRF model. As in-domain training data we use the inhouse English meeting data"
2014.iwslt-papers.4,P07-2045,0,0.00278013,"ctuation insertion. The results of disfluency removal are analyzed. Finally, the overview of our system is given in the end. 6.1. System description The translation system is trained on 2.3 million sentences of English-French parallel data including the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Le"
2014.iwslt-papers.4,W11-2124,1,0.887925,"translation system is trained on 2.3 million sentences of English-French parallel data including the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for"
2014.iwslt-papers.4,2007.tmi-papers.21,0,0.0240266,"uding the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Tab"
2014.iwslt-papers.4,W05-0836,1,0.818865,"the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Table 3: Oracle experiments System Baseline Oracle segmentation Oracle punctuation Oracle disfluency Oracle all No turns T"
2014.iwslt-papers.4,2005.iwslt-1.19,0,0.024144,"anguage model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Table 3: Oracle experiments System Baseline Oracle segmentation Oracle punctuation Oracle disfluency Oracle all No turns Turns 9.53 12.93 13.96 15.64 12.21 15.72 20.93 1 http://www.ted.com 180 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 In the first system, all disfluencies are kept and baseline segmentations are used. As the base"
2015.eamt-1.18,2012.eamt-1.60,0,0.0119108,"d used for translation. This allows us to retain our generalization won by using word clusters to estimate phrase probabilities, and still use all models trained on the surExperiments Since we expect stemming to have a larger impact in cases where training data is scarce, we evaluated the three presented strategies on two different scenarios: a low-resource condition and a state-ofthe-art large-scale system. In both scenarios we stemmed German adjectives and translated from German to English. In our low-resource condition, we trained an SMT system using only training data from the TED corpus (Cettolo et al., 2012). TED translations are currently available for 107 languages2 and are being continuously expanded. Therefore, there is a high chance that a small parallel corpus of translated TED talks will be available in the chosen language. In the second scenario, we used a large-scale state-of-the-art German→English translation system. This system was trained on significantly more data than available in the low-resource condition and incorporates several additional models. 5.1 System Description The low-resource system was trained only on the TED corpus provided by the IWSLT 2014 machine translation campa"
2015.eamt-1.18,P08-1115,0,0.0282724,"-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human t"
2015.eamt-1.18,W08-0509,0,0.0321613,"Missing"
2015.eamt-1.18,W10-1710,0,0.0163473,"Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Wel"
2015.eamt-1.18,W11-2123,0,0.0404294,"m/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced"
2015.eamt-1.18,W13-0805,1,0.859129,"performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is"
2015.eamt-1.18,D07-1091,0,0.0158193,"reviously unseen morphological variants of a word, thus leading to a better generalization of our models. To fully maximize the potential of our SMT system, we looked at three different integration strategies. We evaluated hard decision stemming, where all adjectives are replaced by their stem, as well as soft integration strategies, where we consider the words and their stemmed form as translation alternatives. 2 Related Work The specific challenges arising from the translation of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen wor"
2015.eamt-1.18,E03-1076,0,0.0705642,"ting before tagging and stemming. We only stemmed words tagged as attributive adjectives, since only they are inflected in German. Predicative adjectives are not inflected and therefore were left untouched. Since we want to retain the degree of comparison, we used the finegrained tags of the RFTagger to decide when and how to stem. Adjectives tagged as comparative or superlative were stemmed through the use of fixed rules. For all others, we used the lemma output by the TreeTagger, since it is the same as the stem and was already available in our system. Finally, our usual compound splitting (Koehn and Knight, 2003) was trained and performed on the stemmed corpus. 4 Integration After clustering the words into groups that can be translated in the same or at least in a similar way, there are different possibilities to use them in the translation system. A naive strategy is to replace each word by its cluster representative, called hard decision stemming. However, this carries the risk of discarding vital information. Therefore we investigated techniques to integrate both, the surface forms as well as the word stems, into the translation system. In the combined input, we add the stemmed adjectives as transl"
2015.eamt-1.18,2005.iwslt-1.8,0,0.0179766,"additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IW"
2015.eamt-1.18,P07-2045,0,0.00406102,"s. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale sy"
2015.eamt-1.18,P11-1140,0,0.0183423,"utions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and t"
2015.eamt-1.18,2011.iwslt-evaluation.9,1,0.838535,"nolingual training data we used the target side of the TED corpus. The large-scale system was trained on the European Parliament Proceedings, News Commentary, TED and Common Crawl corpora provided for the IWSLT 2014 machine translation campaign (Cettolo et al., 2014), encompassing 4.69M lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modifie"
2015.eamt-1.18,P10-2041,0,0.025409,"large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED"
2015.eamt-1.18,W09-0435,1,0.825095,"of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond p"
2015.eamt-1.18,2011.iwslt-papers.6,1,0.862723,"phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an"
2015.eamt-1.18,2012.amta-papers.19,1,0.788742,", the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting"
2015.eamt-1.18,W11-2124,1,0.932757,"dden combination strategy, stemming can easily be implemented into current state-of-the-art SMT systems without the need to change any of the advanced models beyond the phrase table. This makes our approach highly versatile and easy to implement for any number of system architectures and languages. 5 Figure 1: Workflow for unstemming the PT. 4.3 Hidden Combination While we are able to modify our phrase table to use both surface forms and stems in the last strategy, other models in our log-linear system suffer from the different types of source input. For example, the bilingual language model (Niehues et al., 2011) is based on tokens of target words and their aligned source words. In training, we can use either the stemmed corpus or the original one, but during decoding a mixture of stems and surface forms occurs. For the unknown word forms the scores will not be accurate and the performance of our model will suffer. Similar problems occur when using other translation models such as neural network based translation models. We therefore developed a novel strategy to integrate the word stems into the translation system. Instead of stemming the input to fit the stemmed phrase table, we modified the stemmed"
2015.eamt-1.18,E99-1010,0,0.150565,"le and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT te"
2015.eamt-1.18,2007.tmi-papers.21,0,0.0581703,"ining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context du"
2015.eamt-1.18,C08-1098,0,0.0280715,"m that does not exist in proper German. However, were we to apply the same stemming to the comparative case, we would lose the degree of comparison and still generate a valid German sentence (sch¨on wird es nicht [won’t be pretty]) with a different meaning than our original sentence. In order to differentiate between cases in which stemming is desirable and where we would lose information, a detailed morphological analysis of the source text prior to stemming is vital. 3.2 Implementation We used readily available part-of-speech (POS) taggers, namely the TreeTagger (Schmid, 1994) and RFTagger (Schmid and Laws, 2008), for morphological analysis and stemming. In order to achieve accurate results, we performed standard machine translation preprocessing on our corpora before tagging. We discarded exceedingly long sentences and sentence pairs with a large length difference from the training data. Special dates, numbers and symbols were normalized and we smart-cased the first letter of every sentence. Typically preprocessing for German also includes splitting up compounds into their separate parts. However, this would confuse the POS taggers, which have been trained on German text with proper compounds. Furthe"
2015.eamt-1.18,P06-1122,0,0.0153461,"ere we consider the words and their stemmed form as translation alternatives. 2 Related Work The specific challenges arising from the translation of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate th"
2015.eamt-1.18,W05-0836,1,0.751291,"lt on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IWSLT test 2011 as development data and IWSLT test 2012 as test data. All results are reported as case-sensitive BLEU scores calculated with one reference tran"
2015.eamt-1.18,P13-1058,0,0.0173103,"n of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morphologically reduced system due to better generalization ability. Their analysis showed the Russian system often produces an incorrect verb tense,"
2015.eamt-1.18,W12-3157,0,0.0169821,"rds. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the m"
2015.eamt-1.18,E06-1006,0,0.0340252,"ion of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al."
2015.eamt-1.18,W13-2213,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.19,0,\N,Missing
2015.iwslt-evaluation.1,2007.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2005.iwslt-1.1,0,\N,Missing
2015.iwslt-evaluation.1,2004.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,J93-3001,0,\N,Missing
2015.iwslt-evaluation.1,W05-0908,0,\N,Missing
2015.iwslt-evaluation.1,2005.mtsummit-papers.11,0,\N,Missing
2015.iwslt-evaluation.1,2015.iwslt-evaluation.16,0,\N,Missing
2015.iwslt-evaluation.1,2006.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2008.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2011.iwslt-evaluation.1,1,\N,Missing
2015.iwslt-evaluation.1,2009.iwslt-evaluation.1,0,\N,Missing
2015.iwslt-evaluation.1,2012.eamt-1.60,1,\N,Missing
2015.iwslt-evaluation.1,2010.iwslt-evaluation.1,1,\N,Missing
2020.amta-research.3,D19-1166,0,0.0198851,"nvestigated an approach to directly control the output length. Although their methods use similar techniques to ours, the model is trained in a supervised way. Motivated by recent success in unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018), a first approach to learn text compression in an unsupervised fashion was presented in Fevry and Phang (2018). Text compression in a supervised fashion for subtitles was investigated in Angerbauer et al. (2019). In contrast to text compression, the combination of readability and machine translation has been researched recently. (Agrawal and Carpuat, 2019) presented an approach to model the readability using source side annotation. In contrast to our work, they concentrated on the scenario where manually created training data is available. In Marchisio et al. (2019) the authors specified the desired readability difficulty either by a source token or through the architecture by different encoders. While they concentrate on a single task and have only a limited number of difficulty classes, the work presented here is able to handle a huge number of possible output classes (e.g. in text compression the number of words) and can be applied for diffe"
2020.amta-research.3,D18-1549,0,0.0251947,"syntactic translation (Cohn and Lapata, 2008) and phrase-based machine translation were investigated (Wubben et al., 2012). The success of encoder-decoder models in many areas of natural language processing (Sutskever et al., 2014; Bahdanau et al., 2014) motivated their successful application to sentence compression. (Kikuchi et al., 2016) and (Takase and Okazaki, 2019) investigated an approach to directly control the output length. Although their methods use similar techniques to ours, the model is trained in a supervised way. Motivated by recent success in unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018), a first approach to learn text compression in an unsupervised fashion was presented in Fevry and Phang (2018). Text compression in a supervised fashion for subtitles was investigated in Angerbauer et al. (2019). In contrast to text compression, the combination of readability and machine translation has been researched recently. (Agrawal and Carpuat, 2019) presented an approach to model the readability using source side annotation. In contrast to our work, they concentrated on the scenario where manually created training data is available. In Marchisio et al. (2019) the"
2020.amta-research.3,2015.iwslt-evaluation.1,1,0.890552,"Missing"
2020.amta-research.3,C08-1018,0,0.24041,"e tried to reduce the length of the sentence by a larger margin and thereby have the situation where the training and testing conditions differ more. Furthermore, the use of multi-lingual machine translation allows also the generation of compressed sentences in the same language. This work on length-controlled machine translation is strongly related to sentence compression, where the compression is performed in the monolingual case. First approach used rule-based approaches (Dorr et al., 2003) for extractive sentence compression. In abstractive compression methods using syntactic translation (Cohn and Lapata, 2008) and phrase-based machine translation were investigated (Wubben et al., 2012). The success of encoder-decoder models in many areas of natural language processing (Sutskever et al., 2014; Bahdanau et al., 2014) motivated their successful application to sentence compression. (Kikuchi et al., 2016) and (Takase and Okazaki, 2019) investigated an approach to directly control the output length. Although their methods use similar techniques to ours, the model is trained in a supervised way. Motivated by recent success in unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018), a"
2020.amta-research.3,W03-0501,0,0.184568,"Volume 1: MT Research Track Page 31 translation with the same length as the source sentence. Compared to these works, we tried to reduce the length of the sentence by a larger margin and thereby have the situation where the training and testing conditions differ more. Furthermore, the use of multi-lingual machine translation allows also the generation of compressed sentences in the same language. This work on length-controlled machine translation is strongly related to sentence compression, where the compression is performed in the monolingual case. First approach used rule-based approaches (Dorr et al., 2003) for extractive sentence compression. In abstractive compression methods using syntactic translation (Cohn and Lapata, 2008) and phrase-based machine translation were investigated (Wubben et al., 2012). The success of encoder-decoder models in many areas of natural language processing (Sutskever et al., 2014; Bahdanau et al., 2014) motivated their successful application to sentence compression. (Kikuchi et al., 2016) and (Takase and Okazaki, 2019) investigated an approach to directly control the output length. Although their methods use similar techniques to ours, the model is trained in a sup"
2020.amta-research.3,K18-1040,0,0.0821778,"ccess of encoder-decoder models in many areas of natural language processing (Sutskever et al., 2014; Bahdanau et al., 2014) motivated their successful application to sentence compression. (Kikuchi et al., 2016) and (Takase and Okazaki, 2019) investigated an approach to directly control the output length. Although their methods use similar techniques to ours, the model is trained in a supervised way. Motivated by recent success in unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018), a first approach to learn text compression in an unsupervised fashion was presented in Fevry and Phang (2018). Text compression in a supervised fashion for subtitles was investigated in Angerbauer et al. (2019). In contrast to text compression, the combination of readability and machine translation has been researched recently. (Agrawal and Carpuat, 2019) presented an approach to model the readability using source side annotation. In contrast to our work, they concentrated on the scenario where manually created training data is available. In Marchisio et al. (2019) the authors specified the desired readability difficulty either by a source token or through the architecture by different encoders. Whil"
2020.amta-research.3,Q17-1024,0,0.156789,"Missing"
2020.amta-research.3,D16-1140,0,0.561466,"ll possible target lengths are independent from each other. This poses a special challenge for long sentences which occur less frequently, e.g. there will be less sentence with length 63 than with length 9 and therefore the embedding of these lengths will not be learned as well as the frequent ones. Target embedding We address the first challenge by integrating the length constraint directly into the decoder. In this case we model locally at each decoding step by encoding the number of words remaining to be generated. This is motivated by similar approaches to supervised sentence compression (Kikuchi et al., 2016) and zero-shot machine translation (Ha et al., 2017). Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 24 We incorporate the information of the number of remaining target words at each target position. For one, this should ensure that the length information is not lost during the decoding process. Secondly, by embedding smaller numbers which occur more frequently in the corpus towards the end of the sentence, the problem of rare sentence lengths does not matter that much. Formally, at each decode"
2020.amta-research.3,kobus-etal-2017-domain,0,0.0683024,"ivation is thereby to ensure that the model uses the additional length information although it might not strictly necessary during training. Thereby, the training examples consist of a source sentence X = x1 , . . . , xI , a target sentence Y = y1 , . . . , yJ and the target length J. Source embedding A first method is to model the target length globally for the whole sentence. This can be achieved by including the target length into the source sentence as an additional token. This is motivated by successful approaches for multilingual machine translation (Ha et al., 2016), domain adaptation (Kobus et al., 2017) and formality levels (Sennrich et al., 2016a). We change the training procedure to not use X as the input to the encoder of the NMT system, but instead J, X. In this way, the encoder will learn an embedding for each target length seen during training. There are two challenges using this approach. First, the dependency between the described length J and the output Y is quite long within the model. Therefore, the model might ignore the information and just learn to generate the best translation for a given source sentence. Secondly, the representations for all possible target lengths are indepe"
2020.amta-research.3,W18-6450,0,0.0229236,"Missing"
2020.amta-research.3,W19-5302,0,0.0501401,"Missing"
2020.amta-research.3,W19-6619,0,0.0198349,"lation (Artetxe et al., 2018; Lample et al., 2018), a first approach to learn text compression in an unsupervised fashion was presented in Fevry and Phang (2018). Text compression in a supervised fashion for subtitles was investigated in Angerbauer et al. (2019). In contrast to text compression, the combination of readability and machine translation has been researched recently. (Agrawal and Carpuat, 2019) presented an approach to model the readability using source side annotation. In contrast to our work, they concentrated on the scenario where manually created training data is available. In Marchisio et al. (2019) the authors specified the desired readability difficulty either by a source token or through the architecture by different encoders. While they concentrate on a single task and have only a limited number of difficulty classes, the work presented here is able to handle a huge number of possible output classes (e.g. in text compression the number of words) and can be applied for different tasks. 6 Conclusion In this work, we investigated the challenge of generating translation with additional constraints. The main difficulty we addressed is the availability of training data. It is not only hard"
2020.amta-research.3,P02-1040,0,0.11272,"Missing"
2020.amta-research.3,N16-1005,0,0.331263,"l uses the additional length information although it might not strictly necessary during training. Thereby, the training examples consist of a source sentence X = x1 , . . . , xI , a target sentence Y = y1 , . . . , yJ and the target length J. Source embedding A first method is to model the target length globally for the whole sentence. This can be achieved by including the target length into the source sentence as an additional token. This is motivated by successful approaches for multilingual machine translation (Ha et al., 2016), domain adaptation (Kobus et al., 2017) and formality levels (Sennrich et al., 2016a). We change the training procedure to not use X as the input to the encoder of the NMT system, but instead J, X. In this way, the encoder will learn an embedding for each target length seen during training. There are two challenges using this approach. First, the dependency between the described length J and the output Y is quite long within the model. Therefore, the model might ignore the information and just learn to generate the best translation for a given source sentence. Secondly, the representations for all possible target lengths are independent from each other. This poses a special"
2020.amta-research.3,N19-1401,0,0.190903,"posed architecture allows the model to consider the number of remaining target words at each decoding step. While the baseline model will only cut the end of the sentences, the model is able to shorten already at the beginning of the sentence. Positional encoding Finally, we also address the challenge of representing sentence lengths that are less frequent. The transformer architecture introduced the positional encoding. This encodes the position within the sentence using a set of trigonometric functions. While their method encodes the position relative to the start of the sentence, we follow Takase and Okazaki (2019) to encode the position relative to the end of the sentence. Thereby, at each position we encode the number of remaining words of the sentence. Formally, we replace h0 = pos(emb(yj−1 ), j) by h∗0 = pos(emb(yj−1 ), J − j). 2.3 Additional constraints Besides constraining the number of words, other constraints can be implemented as easily using the same framework. In this work, we show this by limiting the number of complex and difficult words. One use case is the generation of paraphrases in simplified language. A metric to measure text difficulty, the Dale-Chall Readability metric (Chall and Da"
2020.amta-research.3,P16-1008,0,0.0633424,"Missing"
2020.amta-research.3,P12-1107,0,\N,Missing
2020.amta-research.3,W16-2301,0,\N,Missing
2020.amta-research.3,W18-6456,0,\N,Missing
2020.amta-research.3,P16-1162,0,\N,Missing
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.iwslt-1.30,2020.lrec-1.441,0,0.0455356,"e, language tokens are embedded together with the source and target sentences. At test time, the model is given the same source and target language token, which is a translation direction unseen in training. Since the multilingual training enables zero-shot translation, the model is able to reformulate the input in the same language. To achieve output compression, the length constraints introduced in Section 3.2 are applied in the decoder. 4 4.1 Experiment Setup Datasets Table 2 provides an overview of audio corpora we use. The baseline ASR model is trained on the German part of LibriVoxDeEn (Beilharz et al., 2020), a recently released corpus consisting of open-domain German audio books. Since the corpus creators did not suggest a train-dev-test partition, we split the dataset ourselves. The test set contains the following books: Jonathan Frock2 , Jolanthes Hochzeit and Kammmacher. For the spoken language compression adaptation corpus, we collect spoken utterances and subtitles from the German news program Tagesschau from 1 January to 15 August 2019. To control for recording condition and disfluency, we exclude interviews or press conferences and only keep utterances from the news anchors. The utterance"
2020.iwslt-1.30,N16-1012,0,0.0269946,"t with auditory input would create a distraction. To compress the subtitles, one straightforward approach is to post-process ASR transcriptions. The task of sentence compression has been wellstudied (Knight and Marcu, 2002; Clarke and Lapata, 2006; Rush et al., 2015). In extractive compression (Filippova et al., 2015; Angerbauer et al., 2019), only deletion operations are performed on the input. Despite the simplicity, this approach tends to produce outputs that are less grammatical (Knight and Marcu, 2002). On the other hand, abstractive compression (Cohn and Lapata, 2008; Rush et al., 2015; Chopra et al., 2016; Yu et al., 2018) involves more sophisticated input reformulation, such as word reordering and paraphrasing (Clarke and Lapata, 2006). For the task of compressing subtitles, however, the extent of rewriting must be controlled in order to retain consistency with spoken utterances. From a practical point of view, building a sentence compression system typically requires training corpora where the target sequences are summarized. For most languages and domains, there exists scarcely any resource suitable for supervised training. This low-resource condition is even more severe for audio inputs. T"
2020.iwslt-1.30,P06-1048,0,0.0393358,"audience need to multitask, i.e. simultaneously watch video contents, listen to speech utterances, and read subtitles. To avoid a visual overload, not every spoken word needs to be displayed. Meanwhile, the shortened subtitles must still retain the meaning of the spoken content. Moreover, large deviations from the original utterance are also undesirable, as the disagreement with auditory input would create a distraction. To compress the subtitles, one straightforward approach is to post-process ASR transcriptions. The task of sentence compression has been wellstudied (Knight and Marcu, 2002; Clarke and Lapata, 2006; Rush et al., 2015). In extractive compression (Filippova et al., 2015; Angerbauer et al., 2019), only deletion operations are performed on the input. Despite the simplicity, this approach tends to produce outputs that are less grammatical (Knight and Marcu, 2002). On the other hand, abstractive compression (Cohn and Lapata, 2008; Rush et al., 2015; Chopra et al., 2016; Yu et al., 2018) involves more sophisticated input reformulation, such as word reordering and paraphrasing (Clarke and Lapata, 2006). For the task of compressing subtitles, however, the extent of rewriting must be controlled i"
2020.iwslt-1.30,C08-1018,0,0.0554649,"e are also undesirable, as the disagreement with auditory input would create a distraction. To compress the subtitles, one straightforward approach is to post-process ASR transcriptions. The task of sentence compression has been wellstudied (Knight and Marcu, 2002; Clarke and Lapata, 2006; Rush et al., 2015). In extractive compression (Filippova et al., 2015; Angerbauer et al., 2019), only deletion operations are performed on the input. Despite the simplicity, this approach tends to produce outputs that are less grammatical (Knight and Marcu, 2002). On the other hand, abstractive compression (Cohn and Lapata, 2008; Rush et al., 2015; Chopra et al., 2016; Yu et al., 2018) involves more sophisticated input reformulation, such as word reordering and paraphrasing (Clarke and Lapata, 2006). For the task of compressing subtitles, however, the extent of rewriting must be controlled in order to retain consistency with spoken utterances. From a practical point of view, building a sentence compression system typically requires training corpora where the target sequences are summarized. For most languages and domains, there exists scarcely any resource suitable for supervised training. This low-resource condition"
2020.iwslt-1.30,D16-1140,0,0.0858838,"r requirement is that the subtitles should stay reasonably authentic to the spoken contents, only modifying them when necessary. Otherwise, the disagreement with audio contents could become distracting to the audience. Within the framework of common NLP tasks, the task of generating readable subtitles combines ASR and abstractive compression, while being subjected to the additional requirements as outlined above. ASR with Output Length Constraints We explore two ways to represent the length count-down. The first one utilizes length embeddings learned during training, motivated by the approach Kikuchi et al. (2016) proposed. Given a https://www.tagesschau.de/ 248 target sequence of length t, at decoding time step i, the input to decoder hidden state yi is based on a concatenation of the previous state yi−1 and an embedding of the remaining length yi−1 ⊕ emb(t − i), (1) where emb(t − i) is an embedding of the number of allowed tokens. To keep the same dimensionality as that of the original word embedding, the output from Equation 1 further undergoes a linear transformation followed by the ReLU activation. With the length embedding approach, the model learns representations of different length values duri"
2020.iwslt-1.30,W04-1013,0,0.0646781,"Missing"
2020.iwslt-1.30,W18-2706,0,0.013295,"g the outputs for long utterances, we found that the embedding model is more likely to abruptly stop, such as the example shown in Table 9. 6 6.1 Related Work Length-Controlled Text Generation Controlling output length of natural language generation systems has been studied for several tasks. For abstractive summarization, Kikuchi et al. (2016) proposed two methods to incorporate length constraints into LSTM-based encoder-decoder models. The first method uses length embedding at every decoding step, while the second adds the desired length in the first decoder state. For convolutional models, Fan et al. (2018) used special tokens to represent quantized length ranges, and provides the desired token to the decoder before output generation. Liu et al. (2018) adopted a more general approach, where the decoder directly ingests the desired length. More recently, Takase and Okazaki (2019) modified the positional encoding from the Transformer (Vaswani et al., 2017) to encode allowable lengths. Makino et al. (2019) proposed a loss function that encourages summaries within desired lengths. Saito et al. (2020) introduced a model that controls both output length and informativeness. For machine translation, La"
2020.iwslt-1.30,D18-1444,0,0.0182428,"lated Work Length-Controlled Text Generation Controlling output length of natural language generation systems has been studied for several tasks. For abstractive summarization, Kikuchi et al. (2016) proposed two methods to incorporate length constraints into LSTM-based encoder-decoder models. The first method uses length embedding at every decoding step, while the second adds the desired length in the first decoder state. For convolutional models, Fan et al. (2018) used special tokens to represent quantized length ranges, and provides the desired token to the decoder before output generation. Liu et al. (2018) adopted a more general approach, where the decoder directly ingests the desired length. More recently, Takase and Okazaki (2019) modified the positional encoding from the Transformer (Vaswani et al., 2017) to encode allowable lengths. Makino et al. (2019) proposed a loss function that encourages summaries within desired lengths. Saito et al. (2020) introduced a model that controls both output length and informativeness. For machine translation, Lakew et al. (2019) used both the length range token and reverse length encoding. Niehues (2020) used the length embedding, encoding, as well as a com"
2020.iwslt-1.30,K18-1040,0,0.016099,"lled ASR outputs is related to sentence compression, as the transcriptions can be compressed in post-processing. An early approach of supervised extractive sentence compression was by Filippova et al. (2015), who proposed to predict the delete-or-keep choice for each output symbol. Angerbauer et al. (2019) extended this approach by integrating the desired compression ratio as part of the prediction label. Yu et al. (2018) proposed to combine the merits of extractive and abstractive approaches by first deleting on non-essential words and then generating new words. For unsupervised compression, Fevry and Phang (2018) trained a denoising auto-encoder to reconstruct original sentences, and in this way circumvented the need for supervised corpora. 7 Conclusion In this work, we explored the task of compressing ASR outputs to enhance subtitle readability. This task has several unique properties. First, the compression is not solely deletion-based. Moreover, unnecessary paraphrasing must be limited to maintain a consistent user experience between hearing and reading. We first investigated cascading an ASR module with a sentence compression model. Due to the absence of supervised corpora, the compression model i"
2020.iwslt-1.30,P19-1099,0,0.0156223,"into LSTM-based encoder-decoder models. The first method uses length embedding at every decoding step, while the second adds the desired length in the first decoder state. For convolutional models, Fan et al. (2018) used special tokens to represent quantized length ranges, and provides the desired token to the decoder before output generation. Liu et al. (2018) adopted a more general approach, where the decoder directly ingests the desired length. More recently, Takase and Okazaki (2019) modified the positional encoding from the Transformer (Vaswani et al., 2017) to encode allowable lengths. Makino et al. (2019) proposed a loss function that encourages summaries within desired lengths. Saito et al. (2020) introduced a model that controls both output length and informativeness. For machine translation, Lakew et al. (2019) used both the length range token and reverse length encoding. Niehues (2020) used the length embedding, encoding, as well as a combination of the original positional encoding and length count-down. 6.2 Sentence Compression Our task of length-controlled ASR outputs is related to sentence compression, as the transcriptions can be compressed in post-processing. An early approach of supe"
2020.iwslt-1.30,D15-1042,0,0.0496597,"Missing"
2020.iwslt-1.30,2020.amta-research.3,1,0.916476,"embedding, the output from Equation 1 further undergoes a linear transformation followed by the ReLU activation. With the length embedding approach, the model learns representations of different length values during training. Therefore, learning to represent rarely-encountered lengths may be difficult. The second method modifies the trigonometric positional encoding from the Transformer (Vaswani et al., 2017) to represent the remaining length rather than the current position. This method has been applied in summarization (Takase and Okazaki, 2019) and machine translation (Lakew et al., 2019; Niehues, 2020) to limit output lengths. Motivated by these examples from related sequence generation tasks, we explore the “backward” positional encoding in ASR models. With the original positional encoding, for input dimension d ∈ {0, 1, . . . , D − 1}, the encoding at position i is defined as ( sin(i/10000d/D ), if d is even (2) (d−1)/D cos(i/10000 ), if d is odd. The backward positional encoding is the same as Equation (2), except that the current position i is replaced by the remaining length t − i. Given a target sequence of length t, the length encoding at decoding step i becomes ( sin((t − i)/10000d/"
2020.iwslt-1.30,P16-1162,0,0.031765,", we use 32 encoder and 12 decoder layers, and BPE of size 10,000. For the compression model, we use a Transformer with 8 encoder and decoder layers each.3 5 10 Ratio between output to desired length (# transcribed words / # words in reference) We use the Kaldi toolkit (Povey et al., 2011) to preprocess the raw audio utterances into 23dimensional filter banks. We choose not to apply any utterance-level normalization to allow for future work towards online processing. For text materials, i.e. audio transcriptions and the translation source and target sentences, we use byte-pair encoding (BPE) (Sennrich et al., 2016) to create subword-based dictionaries. 4.3 20 Table 3: Two examples of various levels of compression, where the first shortens from 17 to 6 words, and the second only removes one word. In Figure 2, we plot the distribution of the ratio between transcription lengths and target lengths over the test set. The first observation is that most of the transcriptions require shortening, as shown by the high frequencies of ratios over 1. Moreover, the compression ratio varies across different utterances. Table 3 shows two examples, where the first compressed from 17 to 6 words, while the second only del"
2020.iwslt-1.30,N19-1401,0,0.164577,"ed tokens. To keep the same dimensionality as that of the original word embedding, the output from Equation 1 further undergoes a linear transformation followed by the ReLU activation. With the length embedding approach, the model learns representations of different length values during training. Therefore, learning to represent rarely-encountered lengths may be difficult. The second method modifies the trigonometric positional encoding from the Transformer (Vaswani et al., 2017) to represent the remaining length rather than the current position. This method has been applied in summarization (Takase and Okazaki, 2019) and machine translation (Lakew et al., 2019; Niehues, 2020) to limit output lengths. Motivated by these examples from related sequence generation tasks, we explore the “backward” positional encoding in ASR models. With the original positional encoding, for input dimension d ∈ {0, 1, . . . , D − 1}, the encoding at position i is defined as ( sin(i/10000d/D ), if d is even (2) (d−1)/D cos(i/10000 ), if d is odd. The backward positional encoding is the same as Equation (2), except that the current position i is replaced by the remaining length t − i. Given a target sequence of length t, the leng"
2020.iwslt-1.30,C18-1091,0,0.0644656,"would create a distraction. To compress the subtitles, one straightforward approach is to post-process ASR transcriptions. The task of sentence compression has been wellstudied (Knight and Marcu, 2002; Clarke and Lapata, 2006; Rush et al., 2015). In extractive compression (Filippova et al., 2015; Angerbauer et al., 2019), only deletion operations are performed on the input. Despite the simplicity, this approach tends to produce outputs that are less grammatical (Knight and Marcu, 2002). On the other hand, abstractive compression (Cohn and Lapata, 2008; Rush et al., 2015; Chopra et al., 2016; Yu et al., 2018) involves more sophisticated input reformulation, such as word reordering and paraphrasing (Clarke and Lapata, 2006). For the task of compressing subtitles, however, the extent of rewriting must be controlled in order to retain consistency with spoken utterances. From a practical point of view, building a sentence compression system typically requires training corpora where the target sequences are summarized. For most languages and domains, there exists scarcely any resource suitable for supervised training. This low-resource condition is even more severe for audio inputs. To the best of our"
2021.acl-long.101,N19-1388,0,0.0460248,"ld still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Arivazhagan et al., 2019; Pham et al., 2019). In this work we took a different perspective: Instead of introducing additional objectives, we relax some of the pre-defined structure to"
2021.acl-long.101,P19-1121,0,0.096549,"me. From a modeling perspective, zero-shot translation calls for language-agnostic representations, which are likely more robust and can benefit low-resource translation directions. Despite the potential benefits, achieving highquality zero-shot translation is a challenging task. Prior works (Arivazhagan et al., 2019; Zhang et al., 2020a; Rios et al., 2020) have shown that standard systems tend to generate poor outputs, sometimes in an incorrect target language. It has been further shown that the encoder-decoder model captures spurious correlations between language pairs with supervised data (Gu et al., 2019). During training, the model only learns to encode the inputs in a form that facilitates translating the supervised directions. The decoder, when prompted for zero-shot translation to a different target language, has to handle inputs distributed differently from what was seen in training, which inevitably degrades performance. Ideally, the decoder could translate into any target language it was trained on given an encoded representation independent of input languages. In practice, however, achieving a language-agnostic encoder is not straightforward. 1259 Proceedings of the 59th Annual Meeting"
2021.acl-long.101,D19-1165,0,0.0185132,"zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivation for zero-shot translation. 7 Conclusion In"
2021.acl-long.101,2020.wmt-1.66,0,0.0264499,"age. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivation for zero-shot translation. 7 Conclusion In this work, we show that the positional correspondence to input tokens hinders zero-shot translation. Specifically, we demonstrate that: 1) the encoder outputs retain word orders of source languages; 2) this positional information reduces cross-lingual generalizability and therefore zero-shot translation quality; 3) the problems above can be easily al"
2021.acl-long.101,W18-6309,0,0.0219602,"likely more language-agnostic by seeing more languages, as we still present source sentences as a sequence of tokens, the residual connections, when present in all layers, would still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Ar"
2021.acl-long.101,W19-5202,1,0.853938,"Missing"
2021.acl-long.101,D18-1039,0,0.0163683,"uage-agnostic by seeing more languages, as we still present source sentences as a sequence of tokens, the residual connections, when present in all layers, would still enforce a one-to-one mapping to the input tokens. This condition allows our motivation and approach to remain applicable. 6 Related Work Initial works on multilingual translation systems already showed some zero-shot capability (Johnson et al., 2017; Ha et al., 2016). Since then, several works improved zero-shot translation performance by controlling or learning the level of parameter sharing between languages (Lu et al., 2018; Platanios et al., 2018). Recently, models with full parameter sharing have gained popularity, with massively multilingual systems showing encouraging results (Aharoni et al., 2019; Zhang et al., 2020a; Fan et al., 2020). Besides advantages such as compactness and ease of deployment, the tightly-coupled model components also open up new questions. One question is how to form language-agnostic representations at a suitable abstraction level. In this context, one approach is to introduce auxiliary training objectives to encourage similarity between the representations of different languages (Arivazhagan et al., 2019; P"
2021.acl-long.101,W18-6319,0,0.0117572,"guages, we use the IndicNLP library3 and SentencePiece (Kudo and Richardson, 2018) for tokenization and BPE respectively. We choose 40K merge operations and only use tokens with minimum frequency of 50 in the training set. For IWSLT, we use the official tst2017 set. For PMIndia, as the corpus does not come with dev and test sets, we partition the dataset ourselves by taking a multiway subset of all languages, resulting in 1,695 sentences in the dev and test set each. For Europarl, we use the test sets in the MMCR4NLP corpus (Dabre and Kurohashi, 2017). The outputs are evaluated by sacreBLEU4 (Post, 2018). 3.4 Adaptation Procedure To simulate the case of later adding a new language, we learn a new BPE model for the new language and keep the previous model unchanged. Due to the increased number of unique tokens, the vocabulary 3 https://github.com/anoopkunchukuttan/ indic_nlp_library 4 We use BLEU+case.mixed+numrefs.1+smooth .exp+tok.13a+version.1.4.12 by default. On PMIndia, we use the SPM tokenizer (tok.spm instead of tok.13a) for better tokenization of the Indic languages. At the time of publication, the argument tok.spm is only available as a pull request to sacreBLEU: https://github. com/m"
2021.acl-long.101,2020.wmt-1.64,0,0.0500768,"Missing"
2021.acl-long.101,P16-1162,0,0.0476895,"ranslating the unseen direction X → Y vs. using English as an intermediate step, as in X → English → Y. The pivoting is done by the baseline multilingual model, which we expect to have similar performance to separately trained bilingual models. For a fair comparison, in the Europarlfull case, pivoting is done by a baseline model trained till convergence with only supervised dev data rather than the early-stopped one. 3.3 Preprocessing and Evaluation For the languages with Latin script, we first apply the Moses tokenizer and truecaser, and then learn byte pair encoding (BPE) using subword-nmt (Sennrich et al., 2016). For the Indian languages, we use the IndicNLP library3 and SentencePiece (Kudo and Richardson, 2018) for tokenization and BPE respectively. We choose 40K merge operations and only use tokens with minimum frequency of 50 in the training set. For IWSLT, we use the official tst2017 set. For PMIndia, as the corpus does not come with dev and test sets, we partition the dataset ourselves by taking a multiway subset of all languages, resulting in 1,695 sentences in the dev and test set each. For Europarl, we use the test sets in the MMCR4NLP corpus (Dabre and Kurohashi, 2017). The outputs are evalu"
2021.acl-long.101,2020.acl-main.148,0,0.546152,"ion. Considering data collection, zero-shot translation does not require parallel data for a potentially quadratic number of language pairs, which is sometimes impractical to acquire especially between low-resource languages. Using less supervised data in turn reduces training time. From a modeling perspective, zero-shot translation calls for language-agnostic representations, which are likely more robust and can benefit low-resource translation directions. Despite the potential benefits, achieving highquality zero-shot translation is a challenging task. Prior works (Arivazhagan et al., 2019; Zhang et al., 2020a; Rios et al., 2020) have shown that standard systems tend to generate poor outputs, sometimes in an incorrect target language. It has been further shown that the encoder-decoder model captures spurious correlations between language pairs with supervised data (Gu et al., 2019). During training, the model only learns to encode the inputs in a form that facilitates translating the supervised directions. The decoder, when prompted for zero-shot translation to a different target language, has to handle inputs distributed differently from what was seen in training, which inevitably degrades perfor"
2021.acl-long.101,D19-1143,0,0.0281954,"t al., 2020a). With both approaches, the zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders and decoders is analogous to our motivatio"
2021.acl-long.101,D16-1163,0,0.0305819,"acktranslation (Gu et al., 2019; Zhang et al., 2020a). With both approaches, the zero-shot directions must be known upfront in order to train on the corresponding languages. In comparison, our adaptation procedure offers more flexibility, as the first training step remains unchanged regardless of which new language is later finetuned on. This could be suitable to the practical scenario of later acquiring data for the new language. Our work is also related to adaptation to new languages. While the existing literature mostly focused on adapting to one or multiple supervised training directions (Zoph et al., 2016; Neubig and Hu, 2018; Zhou et al., 2019; Murthy et al., 2019; Bapna and Firat, 2019), our focus in this work is to rapidly expand translation coverage via zero-shot translation. While our work concentrates on an Englishcentered data scenario, another promising direction to combat zero-shot conditions is to enrich available training data by mining parallel data between non-English languages (Fan et al., 2020; Freitag and Firat, 2020). On a broader scope of sequenceto-sequence tasks, Dalmia et al. (2019) enforced encoder-decoder modularity for speech recognition. The goal of modular encoders an"
2021.dravidianlangtech-1.7,D18-1549,0,0.0714085,"Missing"
2021.dravidianlangtech-1.7,2020.wmt-1.80,0,0.0795474,"languages are similar; 2) Monolingual data for both the languages belong to the same domain. In most realistic scenarios that call for unsupervised translation, these conditions are rarely fulfilled. Considering these challenges, one promising research direction to address these conditions is to combine multilingual NMT models with unsupervised translation (Liu et al., 2020; Li et al., 2020b; Garcia et al., 2020b; Guzm´an et al., 2019). Incorporating similar high resource languages could alleviate the data scarcity of low resource languages by exploiting the language similarity between them. Fraser (2020) describes this scenario as “unsupervised machine translation with multilingual transfer”. For example, Li et al. (2020a) investigate translating between German to Upper Sorbian with no parallel data between them but using parallel data between the high resource languages German and English. Among Dravidian languages, the comparatively better resourced ones are Tamil, Telugu, Malayalam and Kannada. In turn, among these four languages, Kannada is relatively low resource compared to the others (Reddy and Sharoff, 2011). Therefore, we consider a realistic scenario of improving translation quality"
2021.dravidianlangtech-1.7,2020.findings-emnlp.283,0,0.0733354,"resource languages. Despite this attractive property, existing literature (Marchisio et al., 2020; Kim et al., 2020) suggests that UNMT performs well under the following conditions: 1) Source and target languages are similar; 2) Monolingual data for both the languages belong to the same domain. In most realistic scenarios that call for unsupervised translation, these conditions are rarely fulfilled. Considering these challenges, one promising research direction to address these conditions is to combine multilingual NMT models with unsupervised translation (Liu et al., 2020; Li et al., 2020b; Garcia et al., 2020b; Guzm´an et al., 2019). Incorporating similar high resource languages could alleviate the data scarcity of low resource languages by exploiting the language similarity between them. Fraser (2020) describes this scenario as “unsupervised machine translation with multilingual transfer”. For example, Li et al. (2020a) investigate translating between German to Upper Sorbian with no parallel data between them but using parallel data between the high resource languages German and English. Among Dravidian languages, the comparatively better resourced ones are Tamil, Telugu, Malayalam and Kannada. I"
2021.dravidianlangtech-1.7,2018.gwc-1.10,0,0.04352,"Missing"
2021.dravidianlangtech-1.7,D19-1632,0,0.127997,"Missing"
2021.dravidianlangtech-1.7,W19-7101,0,0.0445385,"led to great improvements on high resource languages, a major challenge remains when there is little parallel data between the source and target languages. Compared to resource-rich languages like German and French, the amount of high-quality parallel data for Dravidian languages is minimal. As a result, the 55 Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 55–64 April 20, 2021 ©2021 Association for Computational Linguistics following the terminology in Li et al. (2020b). case that unsupervised translation could be helpful. Guzm´an et al. (2019) propose a hybrid approach that translates between Nepali and English without parallel data by utilizing parallel data between Hindi and English. Specifically, the model is trained jointly with supervised loss between English and Hindi and unsupervised loss between English and Nepali. In our work, we employ additional loss terms on top of this in order to more tightly connect the languages without parallel data. Moreover, we analyze ways to incorporate auxiliary parallel data and investigate the correlation to language similarity. The current state-of-the-art UNMT systems build upon multilingu"
2021.dravidianlangtech-1.7,2020.eamt-1.5,0,0.0952871,"Missing"
2021.dravidianlangtech-1.7,W19-6809,0,0.0682659,"ed to lead to better model generalization. In this work, we zoom in on Dravidian languages and study how we can best utilize the related languages within this particular family. We also study data scenarios with minimal auxiliary parallel data and use a training procedure that relies more on the monolingual data. The languages in our setup belong to the same family, but have unique scripts. Because of this, there is little vocabulary overlap. Chakravarthi et al. (2018, 2019a,b,c) compare the impact of transliteration in supervised multilingual NMT and shows that it was helpful. Different from Chakravarthi et al. (2019c), the focus of our work is unsupervised translation. We study the role of lexical overlap in a unsupervised training pipeline, including pretraining on monolingual data and supervised training on auxiliary language pairs. We also highlight the information loss caused by transliteration from code-mixing in data from certain domains. Figure 1: Illustration of the data conditions of the languages in our work. Connected nodes indicate availability of parallel data. We focus on translating between English and Kannada (blue nodes) without any parallel data. In this work, with a focus on Dravidian"
2021.dravidianlangtech-1.7,P07-2045,0,0.0191431,"for training is shown in Table 1. 5.2 Pre-Processing and Training Parameters We use the indic-transliteration1 toolkit for transliteration between Latin script and Indic script. We then use Byte Pair Encoding (Sennrich et al., 2016) 1 https://pypi.org/project/ indic-transliteration/ 59 Figure 3: Pictorial description of multiple architectures to include auxiliary parallel data in reference language R. Here R is chosen to be a language similar to Kannada. to learn sub-words with 10k merge operations. We tokenize with Indic NLP Library2 for Dravidian languages whereas for English we use Moses (Koehn et al., 2007). All the experiments have been performed using XLM 3 toolkit. For our multilingual models, we use the Transformer (Vaswani et al., 2017) architecture with 6 layers and 8 heads whereas for our bilingual supervised model baselines we have 5 layers and 2 heads following the same configuration as in Guzm´an et al. (2019). We use a smaller model for the supervised model to avoid overfitting the parallel data amount is minimal. The other parameters are set to default values as they are in XLM. The scores are reported in BLEU (Papineni et al., 2002) and characTER (Wang et al., 2016). When translatin"
2021.dravidianlangtech-1.7,W17-3204,0,0.0262887,"Missing"
2021.dravidianlangtech-1.7,P16-1162,0,0.135428,"Missing"
2021.dravidianlangtech-1.7,2020.wmt-1.22,0,0.0594593,"Missing"
2021.dravidianlangtech-1.7,W16-2342,0,0.0360856,"h we use Moses (Koehn et al., 2007). All the experiments have been performed using XLM 3 toolkit. For our multilingual models, we use the Transformer (Vaswani et al., 2017) architecture with 6 layers and 8 heads whereas for our bilingual supervised model baselines we have 5 layers and 2 heads following the same configuration as in Guzm´an et al. (2019). We use a smaller model for the supervised model to avoid overfitting the parallel data amount is minimal. The other parameters are set to default values as they are in XLM. The scores are reported in BLEU (Papineni et al., 2002) and characTER (Wang et al., 2016). When translating into Dravidian languages with transliteration, we report the scores after restoring the original scripts. We report detokenized BLEU scores using sacreBLEU4 (Post, 2018). 5.3 Languages Kannada (Kn) Tamil (Ta) Telugu (Te) Malayalam (Ml) English (En) Monolingual data 14M 21M 15M 11M 46M Parallel data to English 31k* 34k 34k 30k - Table 1: The number of sentences in the monolingual and parallel datasets for different languages. *Note that the English-Kannada parallel data is only used to train the supervised model (as performance upper bound). The training of all other models d"
2021.dravidianlangtech-1.7,2020.findings-emnlp.371,0,0.0893113,"Missing"
2021.dravidianlangtech-1.7,2020.tacl-1.47,0,0.0370702,"hich is especially difficult for low resource languages. Despite this attractive property, existing literature (Marchisio et al., 2020; Kim et al., 2020) suggests that UNMT performs well under the following conditions: 1) Source and target languages are similar; 2) Monolingual data for both the languages belong to the same domain. In most realistic scenarios that call for unsupervised translation, these conditions are rarely fulfilled. Considering these challenges, one promising research direction to address these conditions is to combine multilingual NMT models with unsupervised translation (Liu et al., 2020; Li et al., 2020b; Garcia et al., 2020b; Guzm´an et al., 2019). Incorporating similar high resource languages could alleviate the data scarcity of low resource languages by exploiting the language similarity between them. Fraser (2020) describes this scenario as “unsupervised machine translation with multilingual transfer”. For example, Li et al. (2020a) investigate translating between German to Upper Sorbian with no parallel data between them but using parallel data between the high resource languages German and English. Among Dravidian languages, the comparatively better resourced ones are"
2021.dravidianlangtech-1.7,2020.wmt-1.68,0,0.182354,"ledge Engineering, Maastricht University s.koneru@student.maastrichtuniversity.nl {danni.liu,jan.niehues}@maastrichtuniversity.nl Abstract current translation quality for Dravidian languages is largely lagging behind. Unsupervised neural machine translation (UNMT; Artetxe et al., 2018, Lample et al., 2018) learns to translate between languages without relying on parallel data. Instead, only monolingual corpora are required. This relieves the need to acquire large parallel datasets, which is especially difficult for low resource languages. Despite this attractive property, existing literature (Marchisio et al., 2020; Kim et al., 2020) suggests that UNMT performs well under the following conditions: 1) Source and target languages are similar; 2) Monolingual data for both the languages belong to the same domain. In most realistic scenarios that call for unsupervised translation, these conditions are rarely fulfilled. Considering these challenges, one promising research direction to address these conditions is to combine multilingual NMT models with unsupervised translation (Liu et al., 2020; Li et al., 2020b; Garcia et al., 2020b; Guzm´an et al., 2019). Incorporating similar high resource languages could a"
2021.dravidianlangtech-1.7,P02-1040,0,0.120636,"Dravidian languages whereas for English we use Moses (Koehn et al., 2007). All the experiments have been performed using XLM 3 toolkit. For our multilingual models, we use the Transformer (Vaswani et al., 2017) architecture with 6 layers and 8 heads whereas for our bilingual supervised model baselines we have 5 layers and 2 heads following the same configuration as in Guzm´an et al. (2019). We use a smaller model for the supervised model to avoid overfitting the parallel data amount is minimal. The other parameters are set to default values as they are in XLM. The scores are reported in BLEU (Papineni et al., 2002) and characTER (Wang et al., 2016). When translating into Dravidian languages with transliteration, we report the scores after restoring the original scripts. We report detokenized BLEU scores using sacreBLEU4 (Post, 2018). 5.3 Languages Kannada (Kn) Tamil (Ta) Telugu (Te) Malayalam (Ml) English (En) Monolingual data 14M 21M 15M 11M 46M Parallel data to English 31k* 34k 34k 30k - Table 1: The number of sentences in the monolingual and parallel datasets for different languages. *Note that the English-Kannada parallel data is only used to train the supervised model (as performance upper bound)."
2021.dravidianlangtech-1.7,W18-6319,0,0.0534957,"yers and 8 heads whereas for our bilingual supervised model baselines we have 5 layers and 2 heads following the same configuration as in Guzm´an et al. (2019). We use a smaller model for the supervised model to avoid overfitting the parallel data amount is minimal. The other parameters are set to default values as they are in XLM. The scores are reported in BLEU (Papineni et al., 2002) and characTER (Wang et al., 2016). When translating into Dravidian languages with transliteration, we report the scores after restoring the original scripts. We report detokenized BLEU scores using sacreBLEU4 (Post, 2018). 5.3 Languages Kannada (Kn) Tamil (Ta) Telugu (Te) Malayalam (Ml) English (En) Monolingual data 14M 21M 15M 11M 46M Parallel data to English 31k* 34k 34k 30k - Table 1: The number of sentences in the monolingual and parallel datasets for different languages. *Note that the English-Kannada parallel data is only used to train the supervised model (as performance upper bound). The training of all other models do not involve this set. language pair and Kannada and Telugu for similar pair. In Table 2, we report the performance before and after transliteration. First, for the supervised models, rom"
2021.eacl-main.70,D16-1162,0,0.0339193,"Missing"
2021.eacl-main.70,2012.eamt-1.60,0,0.0140822,"to be able to better inflect 834 these words, in a second approach we in addition split also all words within a dictionary phrase into characters. We refer to this technique as Mix+Ann. 4 Experiments We evaluate the approaches on three different data sizes and on two different language pairs (EnglishGerman and English-Czech). Since we are focusing on the generation of different morphological forms, we always use the morphologically rich language as the target language. 4.1 Data For English-to-German we created two datasets with different sizes. A first series of experiments is run on the TED (Cettolo et al., 2012) corpus. We split the corpus into training, validation and test sets as described in Section 2. In addition, we evaluate the system also on the official test sets tst2014, tst2015 and 2018 and report average metrics for these test sets. For the second system, we use the Europarl corpus (Koehn, 2005). This corpus is around 10 times bigger than the TED corpus as shown in Table 3. In addition to the target test set, we also tested the systems on the test2006 and test2007, which are the most recent official test sets from the same domain used for the WMT. Finally, we also tested the techniques on"
2021.eacl-main.70,W17-4716,0,0.0380519,"Missing"
2021.eacl-main.70,P19-1294,0,0.693029,"EU score calculated on all words is very limited. In order to have a valid evaluation approach, the evaluation should focus on phrases that cannot be learned from the parallel data. These are typically very rare phrases. Furthermore, we want to translate them in a real world situation. Therefore, the evaluation data should not be synthetic sentences. Finally, the approach should be using the standard parallel data without the need of collecting additional parallel data. A first attempt would be to use existing test data and select sentences where dictionary entries are needed as e.g. done in (Dinu et al., 2019). However, if we limit ourselves to phrases that do not occur in the parallel data or only a few times, the number of occurring words in the test sets are too low to draw any conclusions. Therefore, we evaluate our approach by proposing a new test-train split of existing parallel data. In a first step, we filter a large background dictionary for entries that help to translate phrases that only occur a few times in the existing parallel data. In a second step, we select some of the sentences with their matching dictionaries entries as the new test sets. An overview of the process is shown in Fi"
2021.eacl-main.70,2015.iwslt-evaluation.11,0,0.129109,"Missing"
2021.eacl-main.70,C16-1172,1,0.872634,"Missing"
2021.eacl-main.70,2011.iwslt-papers.6,1,0.702987,"o extracting phrases that occur more often (add. Annot). However, we did use the same split and also evaluated our approach only on the rare phrases. 3.2 Word representations A second challenge when building a machine translation system for the targeted scenario is the generation of the correct inflected word form. Since we have seen the new words only in the dictionary, we will often need to generate different inflected word forms that we have neither seen in the dictionary nor in the corpus. While there have been attempts to generate unknown inflected word forms for dictionary entries (e.g. Niehues and Waibel (2011)) prior to neural machine translation, the ability to represent parts of the words in neural machine translation offer a unique opportunity to model morphological inflection. Therefore, in this work, we concentrated on the word representation used in the NMT system. Thereby, we always use the same representation for the source and the target language. The most commonly used word representation used in stateof-the-art neural machine translation systems are byte-pair-encodings (BPE) (Sennrich et al., 2016b). A second successful approach to represent words in a neural machine translation system a"
2021.eacl-main.70,P02-1040,0,0.10913,"ped a targeted evaluation approach for the continuous learning of new translations (Section 2) • We showed that character-based representation is essential to inflect unknown words correctly. (Section 3) • We show that only the combination of word representation and one-shot learning enables the successful integration of bilingual dictionaries (Section 3) 2 Evaluation scenario The first important research question that needs to be addressed in the targeted continuous learning scenario is the evaluation approach. While the evaluation of machine translation is well-established (e.g. using BLEU (Papineni et al., 2002)), new learned words are typically rare words and therefore their influence on a BLEU score calculated on all words is very limited. In order to have a valid evaluation approach, the evaluation should focus on phrases that cannot be learned from the parallel data. These are typically very rare phrases. Furthermore, we want to translate them in a real world situation. Therefore, the evaluation data should not be synthetic sentences. Finally, the approach should be using the standard parallel data without the need of collecting additional parallel data. A first attempt would be to use existing t"
2021.eacl-main.70,N18-2081,0,0.0379559,"Missing"
2021.eacl-main.70,P17-1141,0,0.18933,"Missing"
2021.eacl-main.70,kobus-etal-2017-domain,0,0.0220519,"o learn the translation. Furthermore, it needs to be flexible, so new dictionary entries can be continuously added to the system and it is able to perform life-long learning by using the newly added entries. One large advantage of deep learning approaches is that they are able to easily incorporate additional information. By annotating the input with additional information, the model is able to learn automatically how to make use of this additional information. This has been successfully done, for example, for the translation of other MT systems (Niehues et al., 2016), for domain information (Kobus et al., 2017) or information about formality (Sennrich et al., 2016a). For the integration of additional knowledge about specific phrases, we follow similar approaches presented in Pham et al. (2018) and Dinu et al. (2019). The main idea is that we annotate each source phrase, for which a dictionary translation is available with this translation. This is done by appending the translation to the source phrase within the sentence as shown in Table 1. Since this is done during training and testing, the system is able to learn to copy and modify these suggestions. No further adaptation to the architecture of t"
2021.eacl-main.70,2005.mtsummit-papers.11,0,0.0550527,"nd English-Czech). Since we are focusing on the generation of different morphological forms, we always use the morphologically rich language as the target language. 4.1 Data For English-to-German we created two datasets with different sizes. A first series of experiments is run on the TED (Cettolo et al., 2012) corpus. We split the corpus into training, validation and test sets as described in Section 2. In addition, we evaluate the system also on the official test sets tst2014, tst2015 and 2018 and report average metrics for these test sets. For the second system, we use the Europarl corpus (Koehn, 2005). This corpus is around 10 times bigger than the TED corpus as shown in Table 3. In addition to the target test set, we also tested the systems on the test2006 and test2007, which are the most recent official test sets from the same domain used for the WMT. Finally, we also tested the techniques on a different language pair. For this we choose English to Czech and also use the Europarl corpus for these experiments. Since there is no official in-domain corpus available, we tested the systems also on the newstest2019 test set. As shown in Table 3, the parameters mentioned in Section 2 lead to a"
2021.eacl-main.70,2011.iwslt-evaluation.7,0,0.0841073,"Missing"
2021.eacl-main.70,W18-2712,1,0.891604,"newly added entries. One large advantage of deep learning approaches is that they are able to easily incorporate additional information. By annotating the input with additional information, the model is able to learn automatically how to make use of this additional information. This has been successfully done, for example, for the translation of other MT systems (Niehues et al., 2016), for domain information (Kobus et al., 2017) or information about formality (Sennrich et al., 2016a). For the integration of additional knowledge about specific phrases, we follow similar approaches presented in Pham et al. (2018) and Dinu et al. (2019). The main idea is that we annotate each source phrase, for which a dictionary translation is available with this translation. This is done by appending the translation to the source phrase within the sentence as shown in Table 1. Since this is done during training and testing, the system is able to learn to copy and modify these suggestions. No further adaptation to the architecture of the NMT system is necessary. The system will learn how to exploit these systems and can transfer this knowledge to new translations that have not been seen in training. Therefore, the tra"
2021.eacl-main.70,N18-1119,0,0.0303306,"Missing"
2021.eacl-main.70,2020.acl-demos.14,0,0.0280422,"tested the systems also on the newstest2019 test set. As shown in Table 3, the parameters mentioned in Section 2 lead to a reasonable test set size for all corpora. As mentioned in Section 3.1, we evaluate the system on Europarl with different amounts of training annotations. All data sets with their splits are available for further experiments 4 . Train - Annot - add. Annot Valid Test EN-DE TED Europarl 198K 1.9M 1.6K 1.2K 14.5K 1610 1196 3181 2140 EN-CS Europarl 636K 2.7K 24.3K 2000 5360 Table 3: Data size in number of sentences 4 4.2 System All data was processed using the Stanza toolkit (Qi et al., 2020) for tokenization and lemmatization. The lemmatization was only used for matching the dictionary entries, the translation systems were built on the inflected words. If BPE is applied, we used a BPE size of 20K. For the mixed representation, words occurring less than k = 50 times were represented as individual characters. We use the standard transformer architecture (Vaswani et al., 2017) and increase the number of layers to eight. The layer size is 512 and the inner size is 2048. Furthermore, we apply word dropout (Gal and Ghahramani, 2016) with p = 0.1. We use the same learning rate schedule"
2021.eacl-main.70,E17-2060,0,0.0218681,"l inflection. Therefore, in this work, we concentrated on the word representation used in the NMT system. Thereby, we always use the same representation for the source and the target language. The most commonly used word representation used in stateof-the-art neural machine translation systems are byte-pair-encodings (BPE) (Sennrich et al., 2016b). A second successful approach to represent words in a neural machine translation system are characterbased representations, where each word is split into its characters. While there have been several works on comparing these two representations(e.g. Sennrich (2017), they are mostly concentrating on generating the overall best translation performance. However, in this work, we will focus on the rare words. Since only for these words we need to learn how to generate different inflected forms. For the more frequent words, this is often not that important since all word forms occur several times in the corpus. Besides the generation of unknown inflected forms, the word representation is also important when learning to copy the annotations to the target. If we look at the example dictionary entry concentric → konzentrisch, the lemma konzentrisch got split in"
2021.eacl-main.70,N16-1005,0,0.116822,"flexible, so new dictionary entries can be continuously added to the system and it is able to perform life-long learning by using the newly added entries. One large advantage of deep learning approaches is that they are able to easily incorporate additional information. By annotating the input with additional information, the model is able to learn automatically how to make use of this additional information. This has been successfully done, for example, for the translation of other MT systems (Niehues et al., 2016), for domain information (Kobus et al., 2017) or information about formality (Sennrich et al., 2016a). For the integration of additional knowledge about specific phrases, we follow similar approaches presented in Pham et al. (2018) and Dinu et al. (2019). The main idea is that we annotate each source phrase, for which a dictionary translation is available with this translation. This is done by appending the translation to the source phrase within the sentence as shown in Table 1. Since this is done during training and testing, the system is able to learn to copy and modify these suggestions. No further adaptation to the architecture of the NMT system is necessary. The system will learn how"
2021.eacl-main.70,P16-1162,0,0.379659,"flexible, so new dictionary entries can be continuously added to the system and it is able to perform life-long learning by using the newly added entries. One large advantage of deep learning approaches is that they are able to easily incorporate additional information. By annotating the input with additional information, the model is able to learn automatically how to make use of this additional information. This has been successfully done, for example, for the translation of other MT systems (Niehues et al., 2016), for domain information (Kobus et al., 2017) or information about formality (Sennrich et al., 2016a). For the integration of additional knowledge about specific phrases, we follow similar approaches presented in Pham et al. (2018) and Dinu et al. (2019). The main idea is that we annotate each source phrase, for which a dictionary translation is available with this translation. This is done by appending the translation to the source phrase within the sentence as shown in Table 1. Since this is done during training and testing, the system is able to learn to copy and modify these suggestions. No further adaptation to the architecture of the NMT system is necessary. The system will learn how"
2021.eacl-main.70,W16-2342,0,0.0115098,"2048. Furthermore, we apply word dropout (Gal and Ghahramani, 2016) with p = 0.1. We use the same learning rate schedule as in the original work and the implementation presented in (Pham et al., 2019) 5 . All systems were always trained from scratch with random initialization. 4.3 TED A first series of experiments were performed on the TED task. We evaluated the one-shot learning approach by source sentence annotation as well as the three different word representations described in Section 3.2. In a first step, we evaluated the translation performance using BLEU (mteval-v14.pl) and characTER (Wang et al., 2016) on the continuous learning test set as well as on the official test set (Table 4). The baseline systems using no one-shot learning do not annotate the source at all and are trained on the standard parallel data. If we take a look at the official test set, we see systems using characterbased representation (Character and Mix) perform slightly better than the subword-based models. This might be due to the fact that the TED training data is rather small. Secondly, the one-shot learning approach has no influence on the translation performance of this test set. This is not surprising, since only 9"
2021.eacl-main.70,P17-1139,0,0.0351657,"Missing"
2021.eacl-tutorials.3,N19-1006,0,0.0287765,"from speech and MT needed for this interdisciplinary research. The topic has not been previously covered in *CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan.niehues@maastrichtuniversity.nl Website: https://dke.maastrichtuniversity. nl/jan.niehues/ Jan Niehues is an assistant professor at Maastricht University. He received his doctoral degree from Karlsruhe Institute of Technology in 2014 on the topic of “Domain Adaptation in Machine Translation.” He has conducted research at"
2021.eacl-tutorials.3,2020.acl-main.217,1,0.789214,"*CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan.niehues@maastrichtuniversity.nl Website: https://dke.maastrichtuniversity. nl/jan.niehues/ Jan Niehues is an assistant professor at Maastricht University. He received his doctoral degree from Karlsruhe Institute of Technology in 2014 on the topic of “Domain Adaptation in Machine Translation.” He has conducted research at Carnegie Mellon University and LIMSI/CNRS, Paris. His research has covered different aspects of Machine Translati"
2021.eacl-tutorials.3,2020.acl-main.619,1,0.833372,"Missing"
2021.eacl-tutorials.3,W16-2323,0,0.0802601,"Missing"
2021.eacl-tutorials.3,Q19-1020,1,0.901654,"he successful application of deep learning methods to speech and language processing has 1 For example, is use of pretrained models end-to-end? Is use of additional steps to create auxiliary target tasks like phoneme recognition? When do these distinctions matter? 10 Proceedings of EACL: Tutorials, pages 10–13 April 19 - 20, 2021. ©2020 Association for Computational Linguistics – – – – rently demonstrated through evaluation campaigns like IWSLT. A particular focus point of the tutorial will be the current data landscape, as well as techniques to exploit different resources (Kano et al., 2020; Sperber et al., 2019) to enable speech translation not just for the few high-resource languages for which multi-parallel speech, transcripts, and translations exist. After the survey of current state-of-the-art methods, we will present evaluation and analysis methods, and challenges when bringing these models from the lab to real-world environments. For example, one challenge of end-to-end models is their ‘opaqueness’; with one joint system, it is more difficult to isolate causes of particular model behaviors and perhaps intervene, to avoid situations where key terms are translated in unexpected ways. Further, mos"
2021.eacl-tutorials.3,2020.acl-main.661,0,0.0187525,"uaintance with basic knowledge of machine learning and sequence-tosequence models for machine translation, such as are covered in most introductory NLP courses. Any programming examples will be shown in Python. This tutorial will cover cutting-edge research in the emerging field of end-to-end speech translation, and the aspects from speech and MT needed for this interdisciplinary research. The topic has not been previously covered in *CL tutorials. 5 3 Current state (high level overview) Input representation Architecture modifications Output representation Outline Reading list • Survey paper (Sperber and Paulik, 2020) • Introduction (30 min) • The first papers on end-to-end ST (B´erard et al., 2016; Weiss et al., 2017) – Task definition – Challenges/differences in translating speech rather than text – Traditional cascade approach to ST • Data for end-to-end ST (Di Gangi et al., 2019b) • Integrating additional data (Bansal et al., 2019; Jia et al., 2019; Sperber et al., 2019) • End-to-End (45 min) 11 • Data representation (Salesky and Black, 2020) • Adapting the Transformer for ST (Di Gangi et al., 2019a) • Multilingual models (Inaguma et al., 2019) 6 Presenters Jan Niehues, Maastricht University Email: jan"
2021.eacl-tutorials.3,N19-1202,1,0.870523,"Missing"
2021.eacl-tutorials.3,1991.mtsummit-papers.18,0,0.432142,"endto-end speech translation for both high- and low-resource languages. In addition, we will discuss methods to evaluate analyze the proposed solutions, as well as the challenges faced when applying speech translation models for real-world applications. 1 Description Machine translation (MT) and automatic speech recognition (ASR) have been mainstays of the speech and natural language processing communities for decades. Speech translation (ST), the combination of both tasks to translate from speech in one language typically to text in another, has existed for nearly as long as either of these (Waibel et al., 1991), attracting interest from both academia and industry. Until very recently, however, research in this area involved a cascade of separately trained speech recognition and machine translation models, with main questions pertaining to intermediate representations and processing steps to best connect these models. The successful application of deep learning methods to speech and language processing has 1 For example, is use of pretrained models end-to-end? Is use of additional steps to create auxiliary target tasks like phoneme recognition? When do these distinctions matter? 10 Proceedings of EAC"
2021.iwslt-1.1,2020.lrec-1.520,0,0.0759372,"Missing"
2021.iwslt-1.1,2020.iwslt-1.3,0,0.0592828,"Missing"
2021.iwslt-1.1,2021.iwslt-1.5,0,0.0628415,"Missing"
2021.iwslt-1.1,N19-1202,1,0.897776,"Missing"
2021.iwslt-1.1,2021.acl-long.224,1,0.769761,"sh-German section of the MuST-C V2 corpus7 and include training, dev, and test (Test Common), in the same structure of the MuST-C V1 corpus (Cattoni et al., 2021) used last year. Since the 2021 test set was processed using the same pipeline applied to create MuST-C V2, the use of the new training resource was strongly recommended. The main differences with respect to MuST-C v1 are: gap with respect to the traditional cascade approach (integrating ASR and MT components in a pipelined architecture). In light of last year’s IWSLT results (Ansari et al., 2020) and of the findings of recent works (Bentivogli et al., 2021) attesting that the gap between the two paradigms has substantially closed, also this year a key element of the evaluation was to set up a shared framework for their comparison. For this reason, and to reliably measure progress with respect to the past rounds, the general evaluation setting was kept unchanged. This stability mainly concerns two aspects: the allowed architectures and the test set provision. On the architecture side, participation was allowed both with cascade and end-to-end (also known as direct) systems. In the latter case, valid submissions had to be obtained by models that:"
2021.iwslt-1.1,2021.iwslt-1.22,0,0.082468,"Missing"
2021.iwslt-1.1,2021.iwslt-1.27,1,0.721453,"to use the same training and development data as in the Offline Speech Translation track. More details are available in §3.2. For the English-Japanese text-to-text track, participants could use the parallel data and monolingual data available for the English-Japanese WMT20 news task (Barrault et al., 2020). For development, participants could use the IWSLT 2017 development sets,2 the IWSLT 2021 development set3 and the simultaneous interpretation transcripts for the IWSLT 2021 development set.4 The simultaneous interpretation was recorded as a part of NAIST Simultaneous Interpretation Corpus (Doi et al., 2021). Systems were evaluated with respect to quality and latency. Quality was evaluated with the standard BLEU metric (Papineni et al., 2002a). Latency was evaluated with metrics developed for simultaneous machine translation, including average proportion (AP), average lagging (AL) and differentiable average lagging (DAL, Cherry and Foster 2019), and later extended to the task of simultaneous speech translation (Ma et al., 2020b). The evaluation was run with the S IMUL E VAL toolkit (Ma et al., 2020a). For the latency measurement of speech input systems, we contrasted computation-aware and non com"
2021.iwslt-1.1,2012.eamt-1.60,1,0.698226,"rently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun,"
2021.iwslt-1.1,2020.iwslt-1.8,1,0.838546,"Missing"
2021.iwslt-1.1,2021.iwslt-1.12,0,0.0248123,"a et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two S"
2021.iwslt-1.1,L18-1001,0,0.0652124,"Missing"
2021.iwslt-1.1,2021.acl-long.68,1,0.787992,"Missing"
2021.iwslt-1.1,L18-1275,0,0.0286278,"ng two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guidelines.19 This makes them less literal compared to standard, unconstrained translations; • OpenSubtitles 2018 (Lison et al., 2018); • Augmented LibriSpeech et al., 2018)16 (Kocabiyikoglu • Mozilla Common Voice17 ; • Unconstrained translations. These references were created from scratch20 by adhering to the usual translation guidelines. They are hence exact (more literal) translations, without paraphrasing and with proper punctuation. • LibriSpeech ASR corpus (Panayotov et al., 2015). The list of allowed development data includes the dev set from IWSLT 2010, as well as the test sets used for the 2010, 2013, 2014, 2015 and 2018 IWSLT campaigns. Using other training/development resources was allowed but, in this case, parti"
2021.iwslt-1.1,2020.iwslt-1.9,0,0.0594925,"Missing"
2021.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0742394,"Missing"
2021.iwslt-1.1,2021.iwslt-1.4,0,0.134226,"essler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and acces"
2021.iwslt-1.1,N18-2074,0,0.0499123,"Missing"
2021.iwslt-1.1,2006.amta-papers.25,0,0.370434,"Missing"
2021.iwslt-1.1,W14-3354,0,0.0745687,"Missing"
2021.iwslt-1.1,W18-6319,0,0.0144137,"cipants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 this year only the BLEU metric (computed with SacreBLEU (Post, 2018) with default settings) has been considered. Instead of multiple metrics, the attention focused on considering two different types of target-language references, namely: • Speech-Translation TED corpus11 ; • How2 (Sanabria et al., 2018)12 ; • LibriVoxDeEn (Beilharz and Sun, 2019)13 ; • Europarl-ST (Iranzo-S´anchez et al., 2020); • TED LIUM v2 (Rousseau et al., 2014) and v3 (Hernandez et al., 2018); • WMT 201914 and 202015 ; • The original TED translations. Since these references come in the form of subtitles, they are subject to compression and omissions to adhere to the TED subtitling guideli"
2021.iwslt-1.1,2021.iwslt-1.19,0,0.0280723,", 2021) Fondazione Bruno Kessler, Italy (Papi et al., 2021) FAIR Speech Translation (Tang et al., 2021a) Huawei Noah’s Ark Lab, China (Zeng et al., 2021) Huawei Translation Services Center, China University of Stuttgart, Germany (Denisov et al., 2021) Karlsruhe Institute of Technology, Germany (Nguyen et al., 2021; Pham et al., 2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; comm"
2021.iwslt-1.1,2020.findings-emnlp.230,0,0.0691772,"Missing"
2021.iwslt-1.1,2021.iwslt-1.7,0,0.0794497,"Missing"
2021.iwslt-1.1,2021.iwslt-1.16,0,0.0332503,"nyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource speech translation, focusing on resource-scarce settings for translating two Swahili varieties (Congolese and Co"
2021.iwslt-1.1,2020.lrec-1.517,1,0.846095,"s were finally converted from two (stereo) to one (mono) channel and downsampled from 48 to 16 kHz, using FFmpeg.10 Upon inspection of the spectrograms of the same talks in the two versions of MuST-C, it clearly emerges that the upper limit band in the audios used in MuST-C V1 is 5 kHz, while it is at 8 kHz in the latest version, coherently with the 16 kHz sample rate. This difference does not guarantee the fully compatibility between V1 and V2 of MuST-C. Besides MuST-C V2, also this year the allowed training corpora include: • MuST-C V1 (Di Gangi et al., 2019); 3.2 Data and Metrics • CoVoST (Wang et al., 2020); Training and development data. Also this year, participants had the possibility to train their systems using several resources available for ST, ASR and MT. The major novelty on the data front 7 http://ict.fbk.eu/must-c/ http://www.youtube.com/c/TED/videos 9 http://youtube-dl.org/ 10 http://ffmpeg.org/ 8 6 • WIT3 (Cettolo et al., 2012) ; Metrics. Systems’ performance was evaluated with respect to their capability to produce translations similar to the target-language references. Differently from previous rounds, where such similarity was measured in terms of multiple automatic metrics,18 thi"
2021.iwslt-1.1,2021.iwslt-1.6,0,0.156379,"2021) Desheng Li Nara Institute of Science and Technology, Nara, Japan (Fukuda et al., 2021) NiuTrans Research, Shenyang, China (Xu et al., 2021b) ON-TRAC Consortium, France (Le et al., 2021) Beijing OPPO Telecommunications Co., Ltd., China University of Edinburgh, UK (Zhang and Sennrich, 2021; Sen et al., 2021) Maastricht University, The Netherlands (Liu and Niehues, 2021) Universitat Polit`ecnica de Catalunya, Spain (G´allego et al., 2021) USTC, iFlytek Research, China (Liu et al., 2021) University of Sydney, Peking University, JD Explore Academy (Ding et al., 2021) ByteDance AI Lab, China (Zhao et al., 2021) Voithru, Upstage, Seoul National University, South Korea (Jo et al., 2021) Zhejiang University (Zhang, 2021) Table 1: List of Participants on the use of multiple languages to improve supervised and zero-shot speech translation between four Romance languages and English; communication and access to multilingual multimedia content in real-time. The goal of this challenge, organized for the second consecutive year, is to examine systems that translate text or audio in a source language into text in a target language from the perspective of both translation quality and latency. • Low-resource spe"
2021.iwslt-1.15,D18-2012,0,0.0347316,"Missing"
2021.iwslt-1.15,2021.acl-long.68,0,0.0335671,"Speech translation (B´erard et al., 2016; Weiss et al., 2017) is the task of converting speech utterances to their translation in other languages. While “endto-end” modeling (Di Gangi et al., 2019; Sperber et al., 2019) of the speech translation pipeline has become the dominant approach, an open challenge remains in terms of data scarcity. As the amount of speech directly paired with translation is lower compared to speech transcription or text-to-text translation, it is especially crucial for models to be data-efficient. In this context, multilingual speech translation (Inaguma et al., 2019; Li et al., 2021) presents itself as a promising direction to alleviate data scarcity by leveraging commonalities across languages. In this multilingual translation track, we submit: 1) an end-to-end system (§5.2) that directly translates from speech and 2) a cascaded system (§5.1) that consists of a multilingual speech transcription module (§3) followed by a multilingual text translation module (§4). Our efforts to improve the speech translation system can be categorized as follows. When training, on the source side, we augment the speech data by speed perturbation. On the target side, we apply pseudo-labelin"
2021.iwslt-1.15,W19-5202,1,0.86534,"Missing"
2021.iwslt-1.15,P16-1009,0,0.0361817,"target side, we created pseudo-labels from ASR transcriptions. Furthermore, at test time we used different ensembling approaches to improve the performance of trained models. By experimenting under different data scenarios, we showed the benefit of multilingual training and the joint training speech transcription and translation. We note a few directions to further improve our systems: First, we expect that utterances augmented by SpecAugment (Park et al., 2019) could improve the quality of the ASR and ST systems. Second, our MT module can be improved by synthetic data from back-translation (Sennrich et al., 2016a), especially for the zero-shot directions. Regarding upcoming work, since the source languages all belong to the same family, an interesting next step is to investigate how to better utilize the relatedness between these languages. Acknowledgement We thank the anonymous reviewers for their helpful feedback. This work is supported by a Facebook Sponsored Research Agreement. References Results on Blind Test Set We submitted systems C3 and E5 for evaluation on the blind test set. The results are summarized in Table 6. In line with the results on the public test set in Table 5, the end-to-end sy"
2021.iwslt-1.15,P16-1162,0,0.0144232,"target side, we created pseudo-labels from ASR transcriptions. Furthermore, at test time we used different ensembling approaches to improve the performance of trained models. By experimenting under different data scenarios, we showed the benefit of multilingual training and the joint training speech transcription and translation. We note a few directions to further improve our systems: First, we expect that utterances augmented by SpecAugment (Park et al., 2019) could improve the quality of the ASR and ST systems. Second, our MT module can be improved by synthetic data from back-translation (Sennrich et al., 2016a), especially for the zero-shot directions. Regarding upcoming work, since the source languages all belong to the same family, an interesting next step is to investigate how to better utilize the relatedness between these languages. Acknowledgement We thank the anonymous reviewers for their helpful feedback. This work is supported by a Facebook Sponsored Research Agreement. References Results on Blind Test Set We submitted systems C3 and E5 for evaluation on the blind test set. The results are summarized in Table 6. In line with the results on the public test set in Table 5, the end-to-end sy"
2021.iwslt-1.15,Q19-1020,1,0.822638,"improve the performance of our systems. We also introduce an ensembling technique that consistently improves the quality of transcriptions and translations. The experiments show that the end-to-end system is competitive with its cascaded counterpart especially in zero-shot conditions. 1 Introduction In this paper, we describe our systems for the multilingual speech translation track of IWSLT 2021. Speech translation (B´erard et al., 2016; Weiss et al., 2017) is the task of converting speech utterances to their translation in other languages. While “endto-end” modeling (Di Gangi et al., 2019; Sperber et al., 2019) of the speech translation pipeline has become the dominant approach, an open challenge remains in terms of data scarcity. As the amount of speech directly paired with translation is lower compared to speech transcription or text-to-text translation, it is especially crucial for models to be data-efficient. In this context, multilingual speech translation (Inaguma et al., 2019; Li et al., 2021) presents itself as a promising direction to alleviate data scarcity by leveraging commonalities across languages. In this multilingual translation track, we submit: 1) an end-to-end system (§5.2) that d"
C16-1172,D16-1162,0,0.0249066,"s of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are ba"
C16-1172,J93-2003,0,0.0601026,"al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additio"
C16-1172,W07-0732,0,0.0330267,"er is structured as follows: In the next section we will review the related work. In Section 3, we will briefly review the phrase-based and neural approach to machine translation. Section 4 will introduce the approach presented in this paper to pre-translate the input using a PBMT system. In the following section, we will evaluate the approach and analyze the errors. Finally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound s"
C16-1172,W16-2314,1,0.88889,"Missing"
C16-1172,W13-0805,1,0.843516,"nalyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 http://www.statmt.org/wmt16/translation-task.html https://github.c"
C16-1172,W16-2378,0,0.0475053,"nally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discri"
C16-1172,E03-1076,0,0.0615016,"7). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016),"
C16-1172,N03-1017,0,0.0285031,"lation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additional features to better model the 1829 Figure 1: Pre-tr"
C16-1172,P15-1002,0,0.0552301,"und splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probabil"
C16-1172,W13-2264,1,0.925069,"veral attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) s"
C16-1172,W11-2124,1,0.311374,"ending on the frequency of the words and finally show some example translations. 5.1 System description For the pre-translation, we used a PBMT system. In order to analyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT"
C16-1172,J04-4002,0,0.063821,"es and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additional features to better model the 1829 Figure 1: Pre-translation methods (a"
C16-1172,E99-1010,0,0.053505,"e-translation, we used a PBMT system. In order to analyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 htt"
C16-1172,P03-1021,0,0.0300472,"proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 http://www.statmt.org/wmt16/translation-task.html https://github.com/rsennrich/nematus 1831 described in (Sennrich et al., 2016) with 40K operations. We run the NMT system for 420K iterations and stored a model every 30K iterations. We selected the model that performed best on the development data. For the ensemble syst"
C16-1172,2007.tmi-papers.21,0,0.0298329,"they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical"
C16-1172,P16-1162,0,0.803657,"translation for most of sentences. Neural machine translation systems provide the output with high fluency. A weakness of NMT systems, however, is that they sometimes lose the original meaning of the source words during translation. One example from the first conference on machine translation (WMT16) test set is the segment in Table 1. The English word goalie is not translated to the correct German word Torwart, but to the German word Gott, which means god. One problem could be that we need to limit the vocabulary size in order to train the model efficiently. We used Byte Pair Encoding (BPE) (Sennrich et al., 2016) to represent the text using a fixed size vocabulary. In our case the word goali is splitted into three parts go, al and ie. Then it is more difficult to transport the meaning to the translation. In contrast to this, in phrase-based machine translation (PBMT), we do not need to limit the vocabulary and are often able to translate words even if we have seen them only very rarely in the training. In the example mentioned before, for instance, the PBMT system had no problems translating the expression correctly. On the other hand, official evaluation campaigns (Bojar et al., 2016) have shown that"
C16-1172,N07-1064,0,0.0156756,"ollows: In the next section we will review the related work. In Section 3, we will briefly review the phrase-based and neural approach to machine translation. Section 4 will introduce the approach presented in this paper to pre-translate the input using a PBMT system. In the following section, we will evaluate the approach and analyze the errors. Finally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Ko"
C16-1172,P10-1049,0,0.0600704,"this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al"
C16-1172,W15-3001,0,\N,Missing
C16-1172,W16-2301,0,\N,Missing
C16-1292,W05-0909,0,0.0342364,"Q(true) := wi yi (1) i∈ALL Note that this definition does not aim to handle document-level discourse phenomena such as coherence, cohesion, and and consistency, but estimates the average sentence-level quality for the document in order to evaluate the overall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previou"
C16-1292,P13-2097,0,0.027685,"n found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding"
C16-1292,W14-3338,0,0.0219279,"information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold stan"
C16-1292,P07-1033,0,0.0452183,"Missing"
C16-1292,P15-1022,0,0.0463442,"Missing"
C16-1292,N15-1073,0,0.0557124,"Missing"
C16-1292,P15-1174,0,0.0243,"lly automatic baseline (§3.1). For comparison, we evaluate a mean-predictor baseline that always predicts the training mean, regardless of the input features. This baseline has been found surprisingly strong previously (Negri et al., 2014; Specia et al., 2015), which we confirm in Table 2. On segment-level, gains over the mean-predictor baseline are clearly visible only for the ASR setting. As expected, the out-of-domain tasks appear much more difficult than the in-domain setting. Note that even though the mean baseline sometimes achieves lower MAE, the XT regressor maintains the advantages 3 Graham (2015) argues that correlation is better for evaluating sentence-level QE, because MAE can be improved by transformation to match estimated global mean and variance. However, we find MAE more indicative for our purpose as it measures not only how well systems are compared against one another, but also how well overall quality is judged in absolute terms. Moreover, collecting global statistics for transformation seems problematic when flexibility for domain changes is required. 4 Tuning directly for MAE yielded similar results. 3108 MT.in-domain MT.out-of-domain ASR.in-domain ASR.out-of-domain ↓MAE m"
C16-1292,W04-3250,0,0.323366,"Missing"
C16-1292,C04-1072,0,0.326451,"verall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previous works. The 3104 second step is then to manually annotate the quality score for a certain number of segments. In our evaluation (§5), we experiment with typical amounts of tens to hundreds of annotated words. The final step is to aggregate manual"
C16-1292,C14-1171,0,0.0494527,"Missing"
C16-1292,P02-1040,0,0.103668,"chnology, enabling users and engineers to judge overall quality of the output, detect key problems, improve systems, and choose among competing systems. Although most users and engineers share these goals, the chosen evaluation approaches can differ strongly, with some people resorting to automatic, reference-based evaluation, while others rely on manual evaluation for their purposes. This is especially pronounced in the case of machine translation (MT), as pointed out by Harris et al. (2016). On one hand, much research effort has been devoted to devising reference-based methods such as BLEU (Papineni et al., 2002) that are well-correlated with human judgment. On the other hand, practitioners need to react to changing domains from customer to customer, and reflect multi-faceted quality requirements that are difficult to measure in a single, generic score, often leaving manual evaluation as the only choice. In recent years, automatic quality estimation (QE) has emerged as a method that could potentially address the lack of flexibility of reference-based evaluation to deal with changing requirements, and the high effort of manual evaluation. Automatic QE uses machine learning techniques that are trained o"
C16-1292,Q14-1025,0,0.0302706,"ia adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding to our fully manual baseline, the automatic estimation corresponding to our fully automatic baseline, and the workers being our systems. 7 Conclusion We proposed lightly supervised quality estimation at the document level, a framework that allows flexible quality estimation across changing domains and quality requirements, while requi"
C16-1292,2014.eamt-1.21,0,0.0283169,"ther or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). T"
C16-1292,W15-4916,0,0.0378992,"Missing"
C16-1292,P10-1063,0,0.019873,"ar observations for MT, and we expect these findings to hold for other datasets. We also confirmed that results are similar when using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-doma"
C16-1292,W12-3121,0,0.0227726,"n using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challengin"
C16-1292,P13-4014,0,0.0470002,"Missing"
C16-1292,P15-4020,0,0.102177,"tor who replicates these exactly. Our main evaluation measure is mean absolute error (MAE)3 between the predicted and true document-level TER/WER. We use 5 datasets as indicated in Table 1, with testing data varying over all datasets, but only the ones labeled “in-domain” used for regressor training. Moreover, for each evaluated document the QE regressor was retrained with training data excluding that which corresponded to the same system or document currently tested (for in-domain tests). This makes even our in-domain scenario more challenging than some of the previous works on automatic QE (Specia et al., 2015). We use scikit-learn (Pedregosa et al., 2011) for regressor training. We assign weights to training samples proportional to their segment length, because longer segments are weighted more strongly in our aggregation strategies (§3) and are thus more important to be accurately predicted. We perform random search with 20 iterations to optimize hyper-parameters (namely, the max-depth and min-samplessplit parameters of XTs) in terms of mean squared error.4 Tuning is conducted separately for every test document, using 10-fold cross validation on the respective training data. For regressor adaptati"
C16-1292,2015.eamt-1.17,0,\N,Missing
C18-2020,2008.iwslt-papers.5,1,0.787873,"Missing"
C18-2020,W18-2712,1,0.826226,"tial sentences and the translation of full sentences (Niehues et al., 2018). One-Shot Learning In addition to overall translation quality, we identify the importance of translating rare events which do not appear many times in the training data but are critical to individual lectures They can be difficult to translate using NMT, but it is crucial for the system to translate them consistently. In order to incorporate external translations into the system, we designed a framework that allows the model to dynamically interact with external knowledge bases via both data augmentation and modeling (Pham et al., 2018). During training, we pre-train phrase-tables with the parallel corpora, and use them to annotate possible translations for the rare-words that appear less than 3 times in the training data. We consider word-splitting methods such as BPE crucial efficiently represent words that do not appear in the training data, and therefore allow proper annotation. By using the COPY-NET the model is able to learn a bias towards the annotation, which might otherwise have be assigned very small probabilities by the NMT softmax function. Finally, we use reinforcement learning to guide the search operation to e"
D17-1145,P08-1115,0,0.298669,"ic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq"
D17-1145,P16-1078,0,0.0159692,"ords we replace (1) by the following:  → − → − h i = LatticeLSTM xi , { h k |k∈C(i)} (4) Similarly, we encode the lattice in backward direction and replace (2) accordingly. Figure 2 illustrates the result. The computational complexity of the encoder is O(|V |+ |E|), i.e. linear in the number of nodes plus number of edges in the graph. The complexity of the attention mechanism is O(|V |M ), where M is the output sequence 2 It is perhaps more common to think of each edge representing a word, but we will motivate why we instead assign word labels to nodes in §3.3. 3 This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model. 1382 Sequential LSTM recurrence ˜ i = hi−1 h forget gt.   ˜ i + bf f i = σ W f xi + Uf h TreeLSTM ˜i = P h k∈C(i) hk fik = σ(Wf xi + U f hk + bf ) Proposed LatticeLSTM Sh wb/f,k ˜i = P h k∈C(i) Zh,k hk (5)  fik = σ(Wf xi + Uf hk + (6)  ln wb/f,k Sf − Zf,k + bf ) update  ˜ i + bin ii = σ Win xi + Uin h   ˜ i + bo oi = σ Wo xi + Uo h   ˜ i + bu ui = tanh Wu xi + Uu h cell ci = ii ui + fi ci−1 ci = ii ui + P k∈C(i) fik ck as TreeLSTM hidden hi = oi tanh(ci ) as sequential as sequential input gt.  output gt. attention as se"
D17-1145,D13-1176,0,0.373947,"based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model. This is achieved by extending the encoder’s Gated Recurrent Units (GRUs) (Cho e"
D17-1145,D15-1166,0,0.0804604,"o hidden states are generated as  → − → − h i = LSTM Efwd (xi ), h i−1 (1) ← − ← −  h i = LSTM Ebwd (xi ), h i+1 , (2) where Efwd and Ebwd are source embedding lookup tables. We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015). We stack multiple LSTM layers and concatenate the final layer → − ← − into the final source hidden state hi = h i |h i , where layer indices are omitted for simplicity. 2.2 Attention We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a 1381 fixed-size representation. At each decoding time step j, a context vector cj is computed as a weighted PNaverage of the source hidden states: cj = i=1 αij hi . The normalized attentional weights αij measure the relative importance of the source words for the current decoding step and are computed as a softmax with normalization factor Z summing over i: αij =  1 exp s sj−1 , hi Z (3) s(·) is a feed-forward neural network with a single layer that estimates the importance of source hidden state hi for producing the next target symbol yj , conditione"
D17-1145,2013.iwslt-papers.14,0,0.531314,"r the baseline. According to our knowledge, this is the first attempt of integrating lattice scores already at the training stage of a machine translation model. • We exploit the fact that our lattice encoder is a strict generalization of a sequential encoder by pre-training on sequential data, obtaining faster and better training convergence on large corpora of parallel sequential data. 1 This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification. We conduct experiments on the Fisher and Callhome Spanish–English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.4 BLEU points on Fisher and 0.8 BLEU points on Callhome, compared to a strong baseline optimized for translating 1-best ASR outputs. We find that the proposed integration of lattice scores is crucial for achieving these improvements. 2 Background Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section. Given an input sequence x = (x1 , x2 , . . . , xN ), the goal is to generate an appropriate output sequence y = (y1 , y2 , . . . , yM ). T"
D17-1145,P15-1150,0,0.0479141,"Missing"
D17-1145,2005.iwslt-1.2,0,0.377035,"eech translation, where a down-stream translation system must consume the output of an up-stream automatic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017)"
D19-5535,J96-1002,0,0.370617,"dgement or echoing the user input. However, the triggered actions are not finished earlier with this approach, but in cases where long pauses cannot be avoided, even with incremental processing, such time buying strategies can be applied. The automatically generated backchannel described by R¨ude et al. (2017) gives feedback during the uttering of an utterance. However, only acoustic features are used and it does not reduce the latency of actions that can be triggered by the utterances. Studies have been conducted on incremental NLU. DeVault et al. (2009) used a maximum entropy classificator (Berger et al., 1996) to classify the partial utterances. They optimized the maximum entropy classificator for partial utterances by using an individual classificator for every utterance length. The problem of this classification approach is that it is not suitable for tasks with a lot of different parameter combinations; for such tasks, a slot filling (sequence labeling task) or word by word approach (sequence to sequence task) is more suitable. Such a more suitable approach is described by Niehues et al. (2018) for incrementally updating machine translations. The authors used an attention-based encoder decoder ("
D19-5535,W09-3902,0,0.0737373,"Missing"
D19-5535,E99-1023,0,0.0847023,"ic because of this separate treatment and therefore an arbitrary model that can process sequences can be used to process the partial and full utterances. The method is depicted in Figure 1 for the utterance Flights to Pittsburgh. 3.2 Data For our experiments, we used utterances from the Airline Travel Information System (ATIS) datasets. We used the utterances that are used by Hakkani-Tur et al. (2016) and are publicly available3 . These utterances were cleaned and every utterance is labeled with its intents and for every token, the corresponding slot is labeled with a tag (in the IOB2 format (Sang and Veenstra, 1999) that is depicted in Figure 2). We converted the data from the IOB2 format to a sequence to sequence format (Constantin et al., 2019). The source sequence is a user utterance Low-latency NLU component In this work, we present a model-agnostic method to build an incremental processing low-latency NLU component. The advantages of this modelagnostic method are that we can use state-ofthe-art neural network architectures and reuse the method for future state-of-the-art neural network architectures. Our used architecture is described 1 https://github.com/quanpn90/ NMTGMinor/tree/DbMajor 2 https://g"
E14-4009,2012.iwslt-papers.15,1,0.854925,"Missing"
E14-4009,2005.iwslt-1.19,0,0.343181,"Missing"
E14-4009,2013.iwslt-papers.12,1,0.876543,"Missing"
E14-4009,W09-0435,1,0.83048,"y the rough estimation in Equation 1, as disfluency removal does not depend on maximizing the translation quality itself. For example, we can consider the sentence Use what you build, build what you use. Due to its repetitive pattern in words and structure, the first clause is often detected as a disfluency using automatic means. To avoid this, we can change the scheme how the clean string is chosen as follows: eˆ = arg max(p(e|fc ) · p(fc |f )) e,fc CRF Model Training 3.3 Lattice Implementation We construct a word lattice which encodes longrange reordering variants (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). For translation we extend this so that potentially disfluent words can be skipped. A reordering lattice of the example sentence Das sind die Vorteile, die sie uh die sie haben. (En.gls: These are the advantages, that you uh that you have.) is shown in Figure 1, where words representing a disfluency are marked in bold letters. In this sentence, the part die sie uh was manually annotated as a disfluency, due to repetition and usage of a filler word. Table 1 shows the Pd obtained from the CRF model for each token. As expected, the words die sie uh obtain a high Pd from the CRF model. In order t"
E14-4009,W11-2124,1,0.893769,"System Description 5.1 The training data for our MT system consists of 1.76 million sentences of German-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experiment, the token is removed if the label output of the CRF model is a disfluency class. The fourth experiment uses the tight integration scheme, where new source paths which jump"
E14-4009,N09-1046,0,0.0244374,"cal, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different reordering variants. Lattices have also been used as a segmentation tactic for compound words (Dyer, 2009), where the segmentation is encoded as input in the lattice. One of the differences between our work and previous work is that we integrate the disfluency removal into an MT system. Our work is not limited to the preprocessing step of MT, instead we use the translation model to detect and remove disfluencies. Contrary to other systems where detection is limited on manual transcripts only, our sysWe train a CRF model to obtain a disfluency probability for each word. The SMT decoder will then skip the potentially disfluent word based on its disfluency probability. Using the suggested scheme, the"
E14-4009,P02-1040,0,0.0906842,"Missing"
E14-4009,E09-1030,0,0.381251,"spontaneous speech have been studied from various points of view. In the noisy channel model (Honal and Schultz, 2003), it is assumed that clean text without any disfluencies has passed through a noisy channel. The clean string is retrieved based on language model (LM) scores and five additional models. Another noisy channel approach involves a phrase-level statistical MT system, where noisy tokens are translated into clean tokens (Maskey et al., 2006). A tree adjoining grammar is combined with this noisy channel model in (Johnson and Charniak, 2004), using a syntactic parser to build an LM. Fitzgerald et al. (2009) present a method to detect speech disfluencies using a conditional random field (CRF) with lexical, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different"
E14-4009,2007.tmi-papers.21,0,0.0916531,"slation system is caused by the rough estimation in Equation 1, as disfluency removal does not depend on maximizing the translation quality itself. For example, we can consider the sentence Use what you build, build what you use. Due to its repetitive pattern in words and structure, the first clause is often detected as a disfluency using automatic means. To avoid this, we can change the scheme how the clean string is chosen as follows: eˆ = arg max(p(e|fc ) · p(fc |f )) e,fc CRF Model Training 3.3 Lattice Implementation We construct a word lattice which encodes longrange reordering variants (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). For translation we extend this so that potentially disfluent words can be skipped. A reordering lattice of the example sentence Das sind die Vorteile, die sie uh die sie haben. (En.gls: These are the advantages, that you uh that you have.) is shown in Figure 1, where words representing a disfluency are marked in bold letters. In this sentence, the part die sie uh was manually annotated as a disfluency, due to repetition and usage of a filler word. Table 1 shows the Pd obtained from the CRF model for each token. As expected, the words die sie uh obtain a high Pd from"
E14-4009,W13-0805,1,0.848869,"ntactic parser to build an LM. Fitzgerald et al. (2009) present a method to detect speech disfluencies using a conditional random field (CRF) with lexical, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different reordering variants. Lattices have also been used as a segmentation tactic for compound words (Dyer, 2009), where the segmentation is encoded as input in the lattice. One of the differences between our work and previous work is that we integrate the disfluency removal into an MT system. Our work is not limited to the preprocessing step of MT, instead we use the translation model to detect and remove disfluencies. Contrary to other systems where detection is limited on manual transcripts only, our sysWe train a CRF model to obtain a disfluency probability for e"
E14-4009,W05-0836,1,0.863742,"erman-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experiment, the token is removed if the label output of the CRF model is a disfluency class. The fourth experiment uses the tight integration scheme, where new source paths which jump over the potentially noisy words are inserted based on the disfluency probabilities assigned by the"
E14-4009,P07-2045,0,0.00544005,"to compare the effect of the tight integration with other disfluency removal strategies, we conduct different experiments on manual transcripts as well as on the ASR output. System Description 5.1 The training data for our MT system consists of 1.76 million sentences of German-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experime"
E14-4009,P04-1005,0,\N,Missing
federico-etal-2012-iwslt,niessen-etal-2000-evaluation,0,\N,Missing
federico-etal-2012-iwslt,N04-4038,0,\N,Missing
federico-etal-2012-iwslt,P02-1040,0,\N,Missing
federico-etal-2012-iwslt,W07-0734,0,\N,Missing
federico-etal-2012-iwslt,2005.mtsummit-papers.11,0,\N,Missing
federico-etal-2012-iwslt,O07-5005,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.10,0,\N,Missing
federico-etal-2012-iwslt,I05-3027,0,\N,Missing
federico-etal-2012-iwslt,2011.iwslt-evaluation.1,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.5,1,\N,Missing
federico-etal-2012-iwslt,2010.iwslt-evaluation.1,1,\N,Missing
herrmann-etal-2014-manual,J11-4002,0,\N,Missing
herrmann-etal-2014-manual,W12-3102,0,\N,Missing
herrmann-etal-2014-manual,P02-1040,0,\N,Missing
herrmann-etal-2014-manual,W09-0435,1,\N,Missing
herrmann-etal-2014-manual,P11-4010,0,\N,Missing
herrmann-etal-2014-manual,W13-0805,1,\N,Missing
herrmann-etal-2014-manual,stuker-etal-2012-kit,1,\N,Missing
herrmann-etal-2014-manual,federico-etal-2012-iwslt,1,\N,Missing
herrmann-etal-2014-manual,W13-2201,0,\N,Missing
herrmann-etal-2014-manual,vilar-etal-2006-error,0,\N,Missing
L18-1318,W17-4705,0,0.0255393,"y introduced modification process are targeting the same overall task, the performed inference on the data varies. The LAMBADA inference task targets the continuation of the current text passage at a known position in the text by predicting the last word within the paragraph. In contrast to that, our newly introduced substitution process replaces contextrelevant words within the text passage at arbitrary positions, increasing the complexity of the task. Sennrich (2016) introduces a dataset with automatically inserted errors focusing on advanced computational models for NMT tasks. The paper by Burlot and Yvon (2017) proposes an evaluation process for NMT models, assessing the morphological properties of a system. The process substitutes nouns, as well as other part-of-speech tokens with filtered and randomly chosen replacement words. 3. Task The task introduced in this paper is designed to evaluate the performance of computational models for out-of-context error detections. The fully automated modification process described in section 4. provides the ground truth for the task. The artificially inserted out-of-context tokens are uniformly distributed over the dataset, elevating the complexity of the task"
L18-1318,2012.eamt-1.60,0,0.0206891,"Missing"
L18-1318,W14-4012,0,0.012825,"Missing"
L18-1616,P15-2001,0,0.0536706,"Missing"
L18-1616,2012.eamt-1.60,0,0.0362581,"rce NMT (Ha et al., 2016), the words in each source sentence are coded with the language of that sentence before feeding to the training process of a standard neural machine translation system. For example, the source sentence in English: they have since abandoned that project would become en_they en_have en_since en_abandoned en_that en_project. anguage coding is conducted in the preprocessing phrase. Our multilingual embeddings are the derived product of this multi-source system. The figure 2 describes the process. 2.2. KIT-Multi Corpus Our corpus is induced from WIT3’s TED subtitle corpus (Cettolo et al., 2012) including bilingual corpora from French, German, Dutch, Italian and Romanian to English. TED is a much smaller multilingual data compared to Europarl and contains other languages than European languages. The multi-source NMT is trained using the NMT framework OpenNMT3 (Klein et al., 2016) to translate from aforementioned languages (including English) to the only target language English. The statistics of TED bilingual corpora and our multilingual embedding corpus are shown in Table 1 and Table 2, respectively. 3905 Language pairs German-English French-English Dutch-English Italian-English Rom"
L18-1616,W17-4715,0,0.0190103,"m (Bahdanau et al., 2014) consists of an encoder representing a source sentence and an attention-based decoder that produces the translated sentence. One of the most notable differences of NMT compared to the conventional statistical approach is that the source words can be represented in a continuous space (i.e. word embeddings) in which the semantic regularities are induced automatically. Being applied to multilingual settings, NMT systems have been proved to be benefited from additional information embedded in a common semantic space across languages (Johnson et al., 2016; Ha et al., 2016; Currey et al., 2017). An interesting and positive side effect of such a system is the simultaneous induction of multilingual embeddings from the source side. In a multi-source NMT systems where the sentences from several sources languages are translated to one target language, the source embeddings are tied to a common semantic space across languages. So the source embeddings has its inherent cross-lingual characteristics, which could be extremely helpful for the cross-lingual applications employing the embeddings. More specifically, in our previous work on multi-source NMT (Ha et al., 2016), the words in each so"
L18-1616,P14-1006,0,0.0199412,"t in other languages. @en@research Word Cosine Similarity @de@Forschung 0.727675 @fr@recherches 0.697122 @de@Forschungs 0.671166 @fr@recherche 0.643990 @de@geforscht 0.637604 @en@humanity Word Cosine Similarity @de@Menschlichkeit 0.691524 @fr@humanité 0.684639 @de@Menschheit 0.645123 @de@Menscheit 0.634902 @en@mankind 0.621472 4. Table 3: Top 5 closest words by Cosine similarity. 3. racy is used to judge the quality of the cross-lingual embeddings. The corpora chosen to be compared are the corpora induced by Skip - Bilingual Skip-gram (Luong et al., 2015), CVM - Bilingual Compositional Model (Hermann and Blunsom, 2014) and VCD - Bilingual Vectors from Comparable Data (Vulic and Moens, 2015), which are all trained on much bigger Europarl v7 parallel corpora4 (Koehn, 2005). To show the impact of the corpus size, we also train the Bilingual Skip-gram embeddings with the same corpora used to train our model, and name it Skip-TED. For the details of those methods, please refer to Upadhyay et al. (2016). In the intrinsic monolingual evaluation, we consider the word embeddings in one language at a time, i.e. the monolingual word embeddings, in order to conduct the word similarity. The Spearman’s rank correlation c"
L18-1616,2005.mtsummit-papers.11,0,0.00867475,"37604 @en@humanity Word Cosine Similarity @de@Menschlichkeit 0.691524 @fr@humanité 0.684639 @de@Menschheit 0.645123 @de@Menscheit 0.634902 @en@mankind 0.621472 4. Table 3: Top 5 closest words by Cosine similarity. 3. racy is used to judge the quality of the cross-lingual embeddings. The corpora chosen to be compared are the corpora induced by Skip - Bilingual Skip-gram (Luong et al., 2015), CVM - Bilingual Compositional Model (Hermann and Blunsom, 2014) and VCD - Bilingual Vectors from Comparable Data (Vulic and Moens, 2015), which are all trained on much bigger Europarl v7 parallel corpora4 (Koehn, 2005). To show the impact of the corpus size, we also train the Bilingual Skip-gram embeddings with the same corpora used to train our model, and name it Skip-TED. For the details of those methods, please refer to Upadhyay et al. (2016). In the intrinsic monolingual evaluation, we consider the word embeddings in one language at a time, i.e. the monolingual word embeddings, in order to conduct the word similarity. The Spearman’s rank correlation coefficient (Myers et al., 1995) between system similarity and human is the measure to judge the quality of the induced word embeddings. The English evaluat"
L18-1616,P16-1157,0,0.193085,"Multi and compares to other cross-lingual embedding corpora. It has been shown that our multilingual corpus achieves competitive performances in standard evaluations as well as it has better coverage while using much less data for the training process. The evaluations on other languages would be pulished in the final version of the paper. Figure 2: Multi-source Neural Machine Translation system and how to get multilingual word embeddings from it. 1 For a thoroughly review of the most popular and advantageous techniques of cross-lingual word embedding induction, please refer to Upadhyay et al. (2016). For even more detailed and broader survey, please refer to Ruder (2017). 2 The corpus is published and constantly updated athttp:// i13pc106.ira.uka.de/~tha/KIT-Multi/ 3904 médicaux médicale medical medi0inische médecine health Medi0in medicine 20 information Informationen 15 10 5 0 −5 −10 physical physischen physique Physik physics Spiele games DNA curiosity software Neugierde peace paix worktravaillent Barmher0igkeit Arbeit compassion data Daten données video TED œuvres Technik technology Technologie technologie education securityMicrosoft technologies computers digital Informatik experime"
N16-3017,W09-0435,1,0.690583,"cribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operation of the system itself is"
N16-3017,P14-2090,0,0.0233272,"pite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architecture. It is designed to be flexible and distributed. There are 3 types of components: A central server, called the “mediator”, “workers” for performing different tasks an"
N16-3017,P15-1020,0,0.0438838,"Missing"
N16-3017,2007.tmi-papers.21,0,0.0215344,". The audio is being transcribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operati"
N16-3017,2015.iwslt-papers.14,0,0.0823231,"Missing"
N16-3017,N13-1023,0,0.030049,"e very domain specific and formalized dialogues. Later, systems supported greater variety in language, but were still built for specific domains (St¨uker et al., 2007). Despite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architectu"
N16-3017,L16-1293,1,\N,Missing
Q19-1020,N16-1109,0,0.132041,"ntermediate representation for the speech translation task, corresponding to the second stage output. Toshniwal et al. (2017) explore a different way of lower-level supervision during training of an attentional speech recognizer by jointly training an auxiliary phoneme recognizer based on a lower layer in the acoustic encoder. Similarly to the discussed multi-task direct model, this approach discards many of the learned parameters when used on the main task and consequently may also suffer from data efficiency issues. Direct end-to-end speech translation models were first used by Duong et al. (2016), although the authors did not actually evaluate translation performance. Weiss et al. (2017) extended this model into a multi-task model and report excellent translation results. Our baselines do not match their results, despite considerable efforts. We note that other research groups have encountered similar replicability issues (Bansal et al., 2018), explanations include the lack of a large GPU cluster to perform ASGD training, as well as to explore an ideal number of training schedules and other hyper-parameter settings. B´erard et al. (2018) explored the translation of audio books with di"
Q19-1020,N18-1008,0,0.368969,"the speech recognizer passes an erroneous source text to the machine translation component, potentially leading to compounding follow-up errors. Another advantage is the ability to train all model parameters jointly. Despite these obvious advantages, two problems persist: (1) Reports on whether direct models outperform cascaded models (Fig. 1a,d) are inconclusive, with some work in favor of direct models (Weiss et al., 2017), some work in favor of cascaded models (Kano et al., 2017; B´erard et al., 2018), and one work in favor of direct models for two out of the three examined language pairs (Anastasopoulos and Chiang, 2018). (2) Cascaded and direct models have been compared under identical data situations, but this is an unrealistic assumption: In practice, cascaded models can be trained on much more abundant independent ASR and MT corpora, whereas end-to-end models require hard-to-acquire end-to-end corpora of speech utterances paired with textual translations. Our first contribution is a closer investigation of these two issues. Regarding the question of whether direct models or cascaded models are generally stronger, we hypothesize that direct models require more data to work well, due to the more complex map"
Q19-1020,N19-1006,0,0.2679,"which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additional auxiliary data. Figure 3: Direct multi-task model. 4 Auto-encoder (AE): Combines source te"
Q19-1020,L18-1001,0,0.109917,"h a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation"
Q19-1020,L16-1147,0,0.0282839,"This indicates that access to ASR labels in some form contributes to favorable data efficiency of speech translation models. Adding External Data Our approach for evaluating data efficiency so far has been to assume that end-to-end data are available for only a subset of the available auxiliary data. In practice, we can often train ASR and MT tasks on abundant external data. We therefore run experiments in which we use the full Fisher training data for all tasks as before, and add OpenSubtitle11 data for the auxiliary MT task. We clean and normalize the Spanish–English OpenSubtitle 2018 data (Lison and Tiedemann, 2016) to be consistent with the employed Fisher training data by lowercasing and removing punctuation. We apply a basic length filter and obtain 61 million sentences. During training, we include the same number of sentences from in-domain and out-of-domain MT tasks in each minibatch in order to prevent degradation due to domain mismatch. 11 Fisher Table 3: Adding auxiliary OpenSubtitles MT data to the training. The two-stage models benefit much more strongly than the direct model, with our proposed model yielding the strongest overall results. Figure 6: Data efficiency across model types. All model"
Q19-1020,cieri-etal-2004-fisher,0,0.135104,"er, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation model. We implement multi-task training by drawing several minibatches, one minibatch for each ta"
Q19-1020,N18-1031,0,0.0337135,"6.59 35.30 24.68 14.91 6.08 Table 1: BLEU scores (4 references) on the Fisher/ Test for various amounts of training data. The direct (multi-task) model performs best in the full data condition, but the cascaded model is best in all reduced conditions. attention MLP, 64 for target character embeddings, 256 for the encoder LSTMs in each direction, and 512 elsewhere. The model uses variational recurrent dropout with probability 0.3 and target character dropout with probability 0.1 (Gal and Ghahramani, 2016). We apply label smoothing (Szegedy et al., 2016) and fix the target embedding norm to 1 (Nguyen and Chiang, 2018). We use beam search with beam size 15 and polynomial length normalization with exponent 1.5.8 All BLEU scores are computed on Fisher/Test against 4 references. 5.1 Cascaded vs. Direct Models We first wish to shed light on the question of whether cascaded or direct models can be expected to perform better. This question has been investigated previously (Weiss et al., 2017; Kano et al., 2017; B´erard et al., 2018; Anastasopoulos and Chiang, 2018), but with contradictory findings. We hypothesize that the increased complexity of the direct mapping from speech to translation increases the data req"
Q19-1020,W17-4708,1,0.82047,"ilitate meaningful comparisons. The cascade consists of an ASR component and an MT component, which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additiona"
Q19-1020,2013.iwslt-papers.14,0,0.50882,"ons as outputs. Such a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters wit"
W07-0727,2005.mtsummit-papers.11,0,0.0174991,"3 2 have 1.0000 Members 0.6701 , 1.0000 4 we 1.0000 honourable 0.3299 6 5 a 1.0000 a 0.0825 8 7 … have 0.9175 Figure 2: Example for a source sentence lattice from the POS-based reordering scheme. sentence pairs unique sent. pairs sentence length words vocabulary English Spanish 1259914 1240151 25.3 26.3 31.84 M 33.16 M 266.9 K 346.3 K Table 1: Corpus statistics for the English/Spanish Europarl corpus. 3 3.1 Spanish ↔ English Europarl and News Commentary Task Data and Translation Tasks The systems for the English ↔ Spanish translation tasks were trained on the sentence-aligned Europarl corpus (Koehn, 2005). Detailed corpus statistics can be found in Table 1. The available parallel News Commentary training data of approximately 1 million running words for both languages was only used as additional language model training data, to adapt our in-domain (Europarl) system to the out-ofdomain (News Commentary) task. The development sets consist of 2000 Europarl sentences (dev-EU) and 1057 News Commentary sentences (dev-NC). The available developmenttest data consists of 2 x 2000 Europarl sentences (devtest-EU and test06-EU) and 1064 News Commentary sentences (test06-NC). All development and developmen"
W07-0727,P00-1056,0,0.0622652,"↔ Spanish Europarl and News Commentary tasks, along with corresponding performance numbers. Section 4 shows the data, final systems and results for the English ↔ German Europarl task. In Section 5, we present our experiments involving a combination of the syntax-augmented MT system with the phrase-based MT system and a combination of the Spanish → English and German → English phrase-based systems. 2 2.1 The ISL Phrase-Based MT System Word and Phrase Alignment Phrase-to-phrase translation pairs are extracted by training IBM Model-4 word alignments in both directions, using the GIZA++ toolkit (Och and Ney, 2000), and then extracting phrase pair candidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Knes"
W07-0727,2001.mtsummit-papers.46,0,0.169661,"201 PHRA 31.77 31.76 SYNT 32.48 32.15 COMB 32.77 32.27 Table 6: Results for combining the syntaxaugmented system (SYNT) with the phrase-based system (PHRA). augmented system was trained on the same normalized data as the phrase-based system. However, it was optimized on the in-domain development set only. More details on the syntax-augmented system can be found in (Zollmann et al., 2007). Table 6 lists the respective BLEU scores of both systems as well as the BLEU score achieved by combining and rescoring the individual 500-best lists. 5.3 Combining MT Systems with Different Source Languages (Och and Ney, 2001) describes methods for translating text given in multiple source languages into a single target language. The ultimate goal is to improve the translation quality when translating from one source language, for example English into multiple target languages, such as Spanish and German. This can be done by first translating the English document into German and then using the translation as an additional source, when translating to Spanish. Another scenario where a multi-source translation becomes desirable was described in (Paulik et al., 2005). The goal was to improve the quality of automatic sp"
W07-0727,W06-1607,0,0.0215739,"ndidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Kneser-Ney discounting as explained in (Foster et al., 2006). The resulting phrase translation tables are pruned by using the combined translation model score as determined by Minimum Error Rate (MER) optimization on the development set. 2.2 ⇒ PRP DT VB IN DT : 4 – 5 – 1 – 2 – 3 ⇒ PRP DT VB: 2 – 3 – 1 ⇒ PRP DT VB IN: 3 – 4 – 1 – 2 Figure 1: Rule extraction for the POS-based reordering scheme. Word Reordering We apply a part-of-speech (POS) based reordering scheme (J. M. Crego et al., 2006) to the POS-tagged source sentences before decoding. For this, we use the GIZA++ alignments and the POS-tagged source side of the training corpus to learn reordering"
W07-0727,W06-3125,0,0.0659682,"Missing"
W07-0727,E03-1076,0,0.0276122,"to the training data as for the English ↔ Spanish system. The main difference was that we did not replace numbers and that we removed all document references. In the translation process, the document references were treated as unknown words and therefore left unchanged. As above, we trained and optimized a first baseline system on the normalized source and reference sentences. However, we used only the Europarl task development set during optimization. To achieve further improvements on the German → English task, we applied a compound splitting technique. The compound splitting was based on (Koehn and Knight, 2003) and was applied on the lowercased source sentences. The words generated by the compound splitting were afterwards true-cased. Instead of replacing a compound by its separate parts, we added a parallel path into the source sentence lattices used for translation. The source sentence lattices were augmented with scores on their edges indicating whether each edge represents a word of the original text or if it was generated during compound splitting. Table 5 shows the case-sensitive BLEU scores for the final German ↔ English systems. In contrast to the English ↔ Spanish systems, we used only mono"
W07-0727,W06-3114,0,0.0553151,"d a combination of the Spanish → English and German → English phrase-based systems. 2 2.1 The ISL Phrase-Based MT System Word and Phrase Alignment Phrase-to-phrase translation pairs are extracted by training IBM Model-4 word alignments in both directions, using the GIZA++ toolkit (Och and Ney, 2000), and then extracting phrase pair candidates which are consistent with these alignments, starting from the intersection of both alignments. This is done with the help of phrase model training code provided by University of Edinburgh during the NAACL 2006 Workshop on Statistical Machine Translation (Koehn and Monz, 2006). The raw rel197 Proceedings of the Second Workshop on Statistical Machine Translation, pages 197–202, c Prague, June 2007. 2007 Association for Computational Linguistics ative frequency estimates found in the phrase translation tables are then smoothed by applying modified Kneser-Ney discounting as explained in (Foster et al., 2006). The resulting phrase translation tables are pruned by using the combined translation model score as determined by Minimum Error Rate (MER) optimization on the development set. 2.2 ⇒ PRP DT VB IN DT : 4 – 5 – 1 – 2 – 3 ⇒ PRP DT VB: 2 – 3 – 1 ⇒ PRP DT VB IN: 3 – 4"
W07-0727,H05-1096,0,0.0227669,"n-best list rescoring we used unique 500-best lists, which may have less than 500 entries for some sentences. In this evaluation, we used several features computed from different information sources such as features from the translation system, additional language models, IBM-1 word lexica and the n-best list itself. We calculated 4 features from the IBM-1 word lexica: the word probability sum as well as the maximum word probability in both language directions. From the n-best list itself, we calculated three different sets of scores. A position-dependent word agreement score as described in (Ueffing and Ney, 2005) with a position window instead of the Levenshtein alignment, the n-best list n-gram probability as described in (Zens and Ney, 2006) and a position-independent n-gram agreement, which is a variation on the first two. To tune the feature combination weights, we used MER optimization. Rescoring the n-best lists from our individual systems did not give significant improvements on the available unseen development-test data. For this reason, we did not apply n-best list rescoring to the individual systems. However, we investigated the feasibility of combining two different systems by rescoring the"
W07-0727,W07-0731,1,\N,Missing
W07-0727,W06-3110,0,\N,Missing
W07-0727,P03-1021,0,\N,Missing
W08-0303,atserias-etal-2006-freeling,0,0.00882796,"Missing"
W08-0303,P06-1009,0,0.580374,"d a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generated. Furthermore, this makes the model symmetric, so the"
W08-0303,J95-4004,0,0.174901,"weights constant. Initial results using a Gaussian prior showed no improvement. 4 Evaluation The word alignment quality of this approach was tested on three different language pairs. On the 22 Spanish-English task the hand-aligned data provided by the TALP Research Center (Lambert et al., 2005) was used. As proposed, 100 sentences were used as development data and 400 as test data. The so called “Final Text Edition of the European Parliament Proceedings” consisting of 1.4 million sentences and this hand-aligned data was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French"
W08-0303,N03-1017,0,0.00430919,"nly the first 200 sentences of the development data were used to speed up the training process. The FBIS-corpus was used as training corpus and all Chinese sentences were word segmented with the Stanford Segmenter (Tseng et al., 2005). The POS-tags for both sides were generated with the Stanford Parser (Klein and Manning, 2003). 4.1 Word alignment quality The GIZA++-toolkit was used to train a baseline system. The models and alignment information were then used as additional knowledge source for the discriminative word alignment. For the first two tasks, all heuristics of the Pharaoh-Toolkit (Koehn et al., 2003) as well as the refined heuristic (Och and Ney, 2003) to combine both IBM4-alignments were tested and the best ones are shown in the tables. For the Chinese task only the grow-diag-final heuristic was used. Table 1: AER-Results on EN-ES task Name IBM4 Source-Target IBM4 Target-Source IBM4 grow-diag DWA IBM1 + IBM4 + GIZA-fert. + Link feature + POS + Phrase feature Dev 15.26 14.23 13.28 12.26 9.21 8.84 Table 3: AER-Results on CH-EN task Test 21.49 19.23 16.48 20.82 18.67 18.02 15.97 15.36 14.77 Name IBM4 Source-target IBM4 Target-source IBM4 Grow-diag-final DWA IBM4 - similarity + Add. directio"
W08-0303,N06-1015,0,0.0622111,"given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generated. Furthermore, this makes th"
W08-0303,N06-1014,0,0.0353428,"d Cohn (2006). They also used CRFs, but they used two linear-chain CRFs, one for every directions. Consequently, they could find the optimal solution for each individual CRF, but they still needed the heuristics to combine both alignments. They reached an AER of 5.29 using the IBM4-alignment on the English-French task (compared to 4.30 of our approach). Lacoste-Julien et al. (2006) enriched the bipartite matching problem to model also larger fertilities and first-or der dependencies. They could reach an AER of 3.8 on the same task, but only if they also included the posteriors of the model of Liang et al. (2006). Using only the IBM4-alignment they generated an alignment with an AER of 4.5. But they did not use any POS-based features in their experiments. Finally, Moore et al. (2006) used a log-linear model for the features and performed a beam search. They could reach an AER as low as 3.7 with both types of alignment information. But they presented no results using only the IBM4-alignment features. 6 Conclusion In this paper a new discriminative word alignment model was presented. It uses a conditional random field to model directly the alignment matrix. Therefore, the algorithms used in the CRFs had"
W08-0303,W03-0301,0,0.0259111,"ed as development data and 400 as test data. The so called “Final Text Edition of the European Parliament Proceedings” consisting of 1.4 million sentences and this hand-aligned data was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French side, the TreeTagger (Schmid, 1994) was used. Finally, to test our alignment approach with languages that differ more in structure a ChineseEnglish task was selected. As hand-aligned data 3160 sentences aligned only with sure links were used (LDC2006E93). This was split up into 2000 sentences of test data and 1160 sentences of development data. In some"
W08-0303,P06-1065,0,0.120607,"ed. Therefore, for a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignment. The discriminative model presented in this paper uses a conditional random field (CRF) to model the alignment matrix. By modeling the matrix no restrictions to the alignment are required and even n:m alignments can be generate"
W08-0303,J03-1002,0,0.02279,"a was used as training corpus. The POS-tags were generated by the Brill-Tagger (Brill, 1995) and the FreeLing-Tagger (Asterias et al., 2006) for the English and the Spanish text respectively. To limit the number of different tags for Spanish we grouped them according to the first 2 characters in the tag names. A second group of experiments was done on an English-French text. The data from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003) was used. This data consists of 1.1 million sentences, a validation set of 37 sentences and a test set of 447 sentences, which have been hand-aligned (Och and Ney, 2003). For the English POS-tags again the Brill Tagger was used. For the French side, the TreeTagger (Schmid, 1994) was used. Finally, to test our alignment approach with languages that differ more in structure a ChineseEnglish task was selected. As hand-aligned data 3160 sentences aligned only with sure links were used (LDC2006E93). This was split up into 2000 sentences of test data and 1160 sentences of development data. In some experiments only the first 200 sentences of the development data were used to speed up the training process. The FBIS-corpus was used as training corpus and all Chinese s"
W08-0303,N03-1028,0,0.018501,"th the weights Θ. Then the probability of an assignment of the random variables, which corresponds to a word alignment, can be expressed as: pΘ (y|e, f ) = Y 1 Φc (Vc ) Z(e, f ) c∈V (2) FN with VF N the set of all factored nodes in the graph, and the normalization factor Z(e, f ) defined as: 2 Z(e, f ) = The Model X Y Φc (Vc ) (3) Y c∈VF N In the approach presented here the word alignment matrix is modeled by a conditional random field (CRF). A CRF is an unidirectional graphical model. It models the conditional distribution over random variables. In most applications like (Tseng et al., 2005; Sha and Pereira, 2003), a sequential model is used. But to model the alignment matrix the graphical structure of the model is more complex. The alignment matrix is described by a random variable yji for every source and target word pair (fj , ei ). These variables can have two values, 0 and 1, indicating whether these words are translations of each other or not. An example is shown in Figure 1. Gray circles represent variables with value 1, white circles stand for variables with value 0. Consequently, a word with zero fertility is indirectly modeled by setting all associated random variables to a value of 0. The st"
W08-0303,P06-1028,0,0.00658086,". So we developed a method to optimize the CRFs towards the alignment error rate (AER) or the F-score with sure and possible links as introduced in (Fraser and Marcu, 2007). The advantage of the F-score is, that there is an additional parameter α, which allows to bias the metric more towards precision or more towards recall. To be able to use a gradient descent method to optimize the weights, the derivation of the word alignment metric with respect to these weights must be computed. This cannot be done for the mentioned metrics since they are not smooth functions. We follow (Gao et al., 2006; Suzuki et al., 2006) and approximate the metrics using the sigmoid function. The sigmoid function uses the probabilities for every link calculated by the belief propagation algorithm. In our experiments we compared the maximum likelihood method and the optimization towards the AER. We also tested combinations of both. The best results were obtained when the weights were first trained using the ML method and the resulting factors were used as initial values for the AER optimization. Another problem is that the POS-based features and high frequency word features have a lot more parameters than all other features an"
W08-0303,C96-2141,1,0.496512,"nificantly. 1 Introduction In machine translation parallel corpora are one very important knowledge source. These corpora are often aligned at the sentence level, but to use them in the systems in most cases a word alignment is needed. Therefore, for a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++-Toolkit to generate the word alignment. This toolkit implements the IBM- and HMMmodels introduced in (Brown et al., 1993; Vogel et al., 1996). They have the advantage that they are trained unsupervised and are well suited for a noisychannel approach. But it is difficult to include additional features into these models. In recent years several authors (Moore et al., 2006; Lacoste-Julien et al., 2006; Blunsom and Cohn, 2006) proposed discriminative word alignment frameworks and showed that this leads to improved alignment quality. In contrast to generative models, these models need a small amount of handaligned data. But it is easy to add features to these models, so all available knowledge sources can be used to find the best alignm"
W08-0303,2004.tmi-1.9,1,0.731318,"Since the main application of the word alignment is statistical machine translation, the aim was not only to generate better alignments measured in AER, but also to generate better translations. Therefore, the word alignment was used to extract phrases and use them then in the translation system. In all translation experiments the beam decoder as described in (Vogel, 2003) was used together with a 3-gram language model and the results are reported in the BLUE metric. For test set translations the statistical significance of the results was tested using the bootstrap technique as described in (Zhang and Vogel, 2004). The baseline system used the phrases build with the Pharaoh-Toolkit. The new word alignment was tested on the English-Spanish translation task using the TC-Star 07 development and test data. The discriminative word alignment (DWA) used the configuration denoted by +POS system in Table 1. With this configuration it took around 4 hours to align 100K sentences. But, of course, generating the alignment can be parallelized to speed up the process. As shown in Table 4 the new word alignment could generate better translations as measured in BLEU scores. 24 Comparison to other work Several discrimin"
W08-0303,J93-2003,0,\N,Missing
W08-0303,J07-3002,0,\N,Missing
W08-0303,I05-3027,0,\N,Missing
W09-0413,W06-1607,0,0.0350992,"arned in a way similar to the other type of reordering rules described above, but contain a gap representing one or several arbitrary words. It is, for example, possible to have the following rule VAFIN * VVPP → VAFIN VVPP *, which puts both parts of the German verb next to each other. 4 4.2 The relative frequencies of the phrase pairs are a very important feature of the translation model, but they often overestimate rare phrase pairs. Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al. (2006). 4.3 Lattice Phrase Extraction For the test sentences the POS-based reordering allows us to change the word order in the source sentence, so that the sentence can be translated more easily. But this approach does not reorder the training sentences. This may cause problems for phrase extraction, especially for long-range reorderings. For example, if the English verb is aligned to both parts of the German verb, this phrase can not be extracted, since it is not continuous on the German side. In the case of German as source language, the phrase could be extracted if we also reorder the training c"
W09-0413,N03-1017,0,0.00774259,"vector of ones with length equal to the number of features in the other phrase table. The phrase pairs of the other phrase table were added with the features &lt; 1, θ &gt;. 5 Table 1: Translation results for English-German (BLEU Score) System Short-range + Smoothing + Adaptation + Discrim. WA + Long-range reordering 5.2 Test 14.99 15.38 15.44 15.61 15.70 German-English The German-English system was trained on the same data as the English-German except that we perform compound splitting as an additional preprocessing step. The compound splitting was done with the frequency-based method described in Koehn et al. (2003). For this language direction, the initial system already uses phrase table smoothing, adaptation and discriminative word alignment, in addition to the techniques of the English-German baseline system. The results are shown in Table 2. For this language pair, we could improve the translation quality, first, by adding the long-range reordering model. Further improvements could be achieved by using lattice phrase extraction as described before. Results We submitted system translations for the EnglishGerman, German-English, English-French and French-English task. Their performance is measured app"
W09-0413,W08-0303,1,0.791909,"ccount for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 80 of the submitted systems we used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the GIZA++ Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using the maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). In this framework, first, reordering rules are extracted from an aligned parallel corpus and POS"
W09-0413,W09-0435,1,0.815129,"t require the verb to be shifted nearly across the whole sentence. During this shift of the verb, the rest of the sentence remains mainly unchanged. It does not matter which words are in between, since they are moved as a whole. Furthermore, rules including an explicit sequence of POS-tags spanning the whole sentence would be too specific. A lot more rules would be needed to cover long-range reorderings with each rule being applicable only very sparsely. Therefore, we model long-range reordering by generalizing over the unaffected sequences and introduce rules with gaps. (For more details see Niehues and Kolss (2009)). These are learned in a way similar to the other type of reordering rules described above, but contain a gap representing one or several arbitrary words. It is, for example, possible to have the following rule VAFIN * VVPP → VAFIN VVPP *, which puts both parts of the German verb next to each other. 4 4.2 The relative frequencies of the phrase pairs are a very important feature of the translation model, but they often overestimate rare phrase pairs. Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting a"
W09-0413,2007.tmi-papers.21,0,0.359617,"mmentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 80 of the submitted systems we used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as w"
W09-0413,W05-0836,1,0.928162,"aseline architecture, followed by descriptions of the additional system components. Translation results for the different languages and system variants are presented in Section 5. 2.1 Training, Development and Test Data We submitted translations for the EnglishGerman, German-English, English-French and French-English tasks. All systems were trained on the Europarl and News Commentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical M"
W09-0413,P07-2045,0,\N,Missing
W09-0435,J93-2003,0,0.0332348,"03). Furthermore, techniques to use additional linguistic knowledge to improve the word order have been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et al. (2005). These models decide during decoding time for a given phrase, if Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented by Brown et al. (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000), (Vogel et al., 2003). Stateof-the-art SMT systems often use translation models based on phrases to describe translation correspondences and word reordering between two languages. The reordering of words is one of the main difficulties in machine translation. Phrase-based translation models by themselves have only limited capability to model different word orders in the source and target language, by capturing local reorderings within phrase pairs. In Proceedings of the"
W09-0435,2006.iwslt-papers.4,0,0.0214313,"the model has to allow some words to be shifted across the whole sentence. If this is not handled correctly, phrase-based systems sometimes generate translations that omit words, as will be shown in Section 7. This is especially problematic in the German-English case because the verb may be omitted, which carries the most important information of the sentence. the next phrase should be aligned to the left or to the right. In recent years several approaches using reordering rules on the source side have been applied successfully in different systems. These rules can be used in rescoring as in Chen et al. (2006) or can be used in a preprocessing step. The aim of this step is to monotonize the source and target sentence. In Collins et al. (2005) and Popovi´c and Ney (2006) hand-made rules were used to reorder the source side depending on information from a syntax tree or based on POS information. These rules had to be created manually, but only a few rules were needed and they were able to model long-range reorderings. Consequently, for every language pair these rules have to be created anew. In contrast, other authors propose data-driven methods. In Costa-juss`a and Fonollosa (2006) the source senten"
W09-0435,N04-1023,0,0.0110232,"0.8 BLEU points, compared to a system which already uses continuous POSbased rules to model short-range reorderings. 1 2 Introduction Related Work Several approaches have been proposed to address the problem of word reordering in SMT. Wu (1996) and Berger et al. (1996), for example, restrict the possible reorderings either during decoding time or during the alignment, but do not use any additional linguistic knowledge. A comparison of both methods can be found in Zens and Ney (2003). Furthermore, techniques to use additional linguistic knowledge to improve the word order have been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et al. (2005). These models decide during decoding time for a given phrase, if Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented by Brown et al. (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Kni"
W09-0435,P05-1066,0,0.120453,"Missing"
W09-0435,P05-1069,0,0.0257269,"lem of word reordering in SMT. Wu (1996) and Berger et al. (1996), for example, restrict the possible reorderings either during decoding time or during the alignment, but do not use any additional linguistic knowledge. A comparison of both methods can be found in Zens and Ney (2003). Furthermore, techniques to use additional linguistic knowledge to improve the word order have been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et al. (2005). These models decide during decoding time for a given phrase, if Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented by Brown et al. (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000), (Vogel et al., 2003). Stateof-the-art SMT systems often use translation models based on phrases to describe translation correspondences and word reordering between two languages. The reordering of wor"
W09-0435,W08-0307,0,0.351947,"ce side depending on information from a syntax tree or based on POS information. These rules had to be created manually, but only a few rules were needed and they were able to model long-range reorderings. Consequently, for every language pair these rules have to be created anew. In contrast, other authors propose data-driven methods. In Costa-juss`a and Fonollosa (2006) the source sentence is first translated into an auxiliary sentence, whose word order is similar to the one of the target sentences. Thereby statistical word classes were used. Rottmann and Vogel (2007),Zhang et al. (2007) and Crego and Habash (2008) used rules to reorder the source side and store different possible reorderings in a word lattice. They use POS tags and in the latter two cases also chunk tags to generalize the rules. The different reorderings are assigned weights depending on their relative frequencies (Rottmann and Vogel, 2007) or depending on a source side language model (Zhang et al., 2007). In the presented work we will use discontinuous rules in addition to the rules used in Rottmann and Vogel (2007). This enables us to model long-range reorderings although we only need POS information and no chunk tags. 3 4 POS-Based"
W09-0435,W06-1607,0,0.0909805,"trained on the European Parliament Proceedings (EPPS) and the News Commentary corpus. For the German-French task we used the intersection of the parallel corpora from the GermanEnglish and English-French task. The data was preprocessed and we applied compound splitting to the German corpus for the tasks translating from German. Afterwards, the word alignment was generated with the GIZA++-Toolkit and the alignments of the two directions were combined using the grow-diag-final-and heuristic. Then the phrase tables were created where we performed additional smoothing of the relative frequencies (Foster et al., 2006). Furthermore, the phrase table applied in the news task was adapted to this domain. In addition, a 4-gram language model was trained on both corpora. The rules were extracted using the POS tags generated by the TreeTagger (Schmid, 1994). In the end a beam-search decoder as described in Vogel (2003) was used to optimize the weights using the MER-training on the development sets provided for the different task by the workshop. The systems were tested 210 Figure 2: Most common long-range reordering rules of type Left Part NN ADV * VAFIN VAFIN ART * VVPP ˆ ADV * PPER $, ART * VVINF PTKZU PRELS AR"
W09-0435,P96-1021,0,0.155365,"entence in a preprocessing step to better match target sentences according to POS(Part-of-Speech)-based rules have been applied successfully. We enhance this approach to model long-range reorderings by introducing discontinuous rules. We tested this new approach on a GermanEnglish translation task and could significantly improve the translation quality, by up to 0.8 BLEU points, compared to a system which already uses continuous POSbased rules to model short-range reorderings. 1 2 Introduction Related Work Several approaches have been proposed to address the problem of word reordering in SMT. Wu (1996) and Berger et al. (1996), for example, restrict the possible reorderings either during decoding time or during the alignment, but do not use any additional linguistic knowledge. A comparison of both methods can be found in Zens and Ney (2003). Furthermore, techniques to use additional linguistic knowledge to improve the word order have been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et a"
W09-0435,P00-1056,0,0.0204404,"been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et al. (2005). These models decide during decoding time for a given phrase, if Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented by Brown et al. (1993) and has since been used in many translation systems (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000), (Vogel et al., 2003). Stateof-the-art SMT systems often use translation models based on phrases to describe translation correspondences and word reordering between two languages. The reordering of words is one of the main difficulties in machine translation. Phrase-based translation models by themselves have only limited capability to model different word orders in the source and target language, by capturing local reorderings within phrase pairs. In Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 206–214, c Athens, Greece, 30 March –"
W09-0435,P03-1019,0,0.0105072,". We tested this new approach on a GermanEnglish translation task and could significantly improve the translation quality, by up to 0.8 BLEU points, compared to a system which already uses continuous POSbased rules to model short-range reorderings. 1 2 Introduction Related Work Several approaches have been proposed to address the problem of word reordering in SMT. Wu (1996) and Berger et al. (1996), for example, restrict the possible reorderings either during decoding time or during the alignment, but do not use any additional linguistic knowledge. A comparison of both methods can be found in Zens and Ney (2003). Furthermore, techniques to use additional linguistic knowledge to improve the word order have been developed. Shen et al. (2004) and Och et al. (2004) presented approaches to re-rank the output of the decoder using syntactic information. Furthermore, lexical block-oriented reordering models have been developed in Tillmann and Zhang (2005) and Koehn et al. (2005). These models decide during decoding time for a given phrase, if Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented by Bro"
W09-0435,N04-1021,0,0.0678925,"Missing"
W09-0435,2004.tmi-1.9,0,0.0386168,"reorderings, we used two different thresholds. 7 Table 1: Evaluation of different Lattice sizes generated by changing the short-range threshold θshort and long-range threshold θlong θshort 0.2 0.1 0.2 0.2 0.2 0.1 0.1 θlong 1 1 0.2 0.1 0.05 0.1 0.05 #Edges 112K 203K 113K 121K 152K 212K 243K Dev 24.57 24.71 24.70 24.97 25.28 24.97 25.12 Test 27.25 27.48 27.51 27.56 27.80 27.49 27.81 on the test2007 set for the EPPS task and on the nc-test2007 testset for the news task. For test set translations the statistical significance of the results was tested using the bootstrap technique as described in Zhang and Vogel (2004). 7.1 Lattice Creation In a first group of experiments we analyzed the influence of the two thresholds that determine the minimal probability of a rule that is used to insert the reordering into the lattice. The experiments were performed on the news task and used only the long-range rules generated by the Part All rules. The results are shown in Table 1 where θshort is the threshold for the short-range reorderings and θlong for the long-range reorderings. Consequently, only paths were added that are generated by a short-range reordering rule that has a probability greater than θshort or paths"
W09-0435,popovic-ney-2006-pos,0,\N,Missing
W09-0435,J96-1002,0,\N,Missing
W09-0435,P01-1067,0,\N,Missing
W09-0435,W07-0401,0,\N,Missing
W09-0435,W06-1609,0,\N,Missing
W09-0435,2005.iwslt-1.8,0,\N,Missing
W10-1719,W05-0836,1,0.919745,"Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments. The difficult reordering between German and English was modeled using POS-based reordering rules. These rules were learned using a word-aligned parallel corpus. The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages. Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al. (2005). This paper describes our phrase-based Statistical Machine Translation (SMT) system for the WMT10 Translation Task. We submitted translations for the German to English and English to German translation tasks. Compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach. The word reordering was modeled using POS information and we extended the translation model with additional features. 1 Baseline System Introduction In this paper we describe the systems that we built for our participation in the Shared Translation Ta"
W10-1719,W06-1607,0,0.0512277,"ged from one spelling system to the other, for example replacing ’ß’ by ’ss’. If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words. The translation model was trained on the parallel corpus and the word alignment was generated by a discriminative word alignment model, which is described below. The phrase table was trained using the Moses training scripts, but for the German to English system we used a different phrase extraction method described in detail in Section 4.2. In addition, we applied phrase table smoothing as described in Foster et al. (2006). Furthermore, we extended the translation model by additional features for unaligned words and introduced bilingual language models. 4.1 As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus. Word Reordering Model Reordering was applied on the source side prior to decoding through the generation of lattices encoding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus an"
W10-1719,2009.mtsummit-papers.5,0,0.0856601,"rase pairs should be longer than the word-based ones. But this is not possible in many decoders or it leads to additional computation overhead. If we instead use a bilingual POS-based language model, the context length of the language model is independent from the other models. Consequently, a longer context can be considered for the POS-based language model than for the wordbased bilingual language model or the phrase pairs. Instead of using POS-based information, this approach can also be applied with other additional linguistic word-level information like word stems. Unaligned Word Feature Guzman et al. (2009) analyzed the role of the word alignment in the phrase extraction process. To better model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model. One feature counts the number of unaligned words on the source side and the other one does the same for the target side. Using these additional features they showed improvements on the Chinese to English translation task. In order to investigate the impact on closer related languages like English and German, we incorporated those two features into our systems. 4.4 Bilingual P"
W10-1719,E03-1076,0,0.125671,"d Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus. 3 Translation Model 4.2 Lattice Phrase Extraction In translations from German to English, we often have the case that the English verb is aligned to both parts of the German verb. Since this phrase pair is not continuous on the German side, it cannot be extracted. The phrase could be extracted, if we also reorder the training corpus. For the test sentences the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract the ph"
W10-1719,P07-2045,0,0.0103142,"Missing"
W10-1719,W09-0435,1,0.842648,"ding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007). To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extract the phrase pairs. We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of ha"
W10-1719,W08-0303,1,0.75518,"were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extract the phrase pairs. We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). When translating from German to English, we apply compound splitting as described in Koehn and Knig"
W10-1719,W09-0413,1,0.385327,"angen home Hause. As shown in the example, one problem with this approach is that unaligned source words are ignored in the model. One solution could be to have a second bilingual text ordered according to the source side. But since the target sentence and not the source sentence is generated from left to right during decoding, the integration of a source side language model is more complex. Therefore, as a first approach we only used a language model based on the target word order. Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al. (2009). Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths. So it would be possible to extract the example mentioned before, if both parts of the verb were put together by a reordering rule. To limit the number of extracted phrase pairs, we extract a source phrase only once per sentence even if it may be found on different paths. Furthermore, we do not use the weights in the lattice. If we used the same rules as for reordering the test sets, the lattice would be so big that the number of extracted phrase pairs would be still too high. As mentio"
W10-1719,2007.tmi-papers.21,0,0.171972,"es that are too long and empty lines to obtain the final training corpus. Word Reordering Model Reordering was applied on the source side prior to decoding through the generation of lattices encoding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007). To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extrac"
W11-2124,W10-1704,0,0.0698574,"Missing"
W11-2124,N03-2002,0,0.0604885,"been derived from the work of Casacuberta and Vidal (2004), which used finite state transducers for statistical machine translation. In this approach, units of source and target words are used as basic translation units. Then the translation model is implemented as an n-gram model over the tuples. As it is also done in phrase-based translations, the different translations are scored by a log-linear combination of the translation model and additional models. Crego and Yvon (2010) extended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words"
W11-2124,D07-1007,0,0.048499,"the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a similar way as the one used in the finite state transducer approach as an additional feature in the loglinear model. 3 Motivation To motivate the introduction of the bilingual lang"
W11-2124,P07-1005,0,0.0236585,"lingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a"
W11-2124,C10-1040,1,0.800514,"rate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used"
W11-2124,D08-1039,0,0.0110035,"xtended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding"
W11-2124,P07-2045,0,0.0121597,"model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the"
W11-2124,J06-4004,0,0.431121,"Missing"
W11-2124,W08-0303,1,0.31921,"k. On the other hand, we evaluated the approach on the Arabic-to-English direction on News and Web data. Additionally, we present the impact of the bilingual language model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Mod"
W11-2124,W09-0413,1,0.910746,"ined on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Commentary domain. The first consists of 2000 sentences with one reference. It will be referred to as Test 1. The second test set consists of 1000 sen"
W11-2124,2007.tmi-papers.21,1,0.445573,"as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Comment"
W11-2124,W10-1719,1,\N,Missing
W11-2124,J04-2004,0,\N,Missing
W11-2124,N03-1017,0,\N,Missing
W11-2124,W11-2145,1,\N,Missing
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2145,E03-1076,0,0.282695,"us to train bigger language models. For training a discriminative word alignment model, a small amount of hand-aligned data was used. 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first words of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus we use the hunspell1 lexicon to map words written according to old German spelling to new German spelling, to obtain a corpus with homogenous spelling. Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. 2.3 Special filtering of the Giga parallel Corpus The Giga corpus incorporates non-neglegible amounts of noise even after our usual preprocessing. This noise may be due to different causes. For instance: non-standard HTML characters, meaningless parts composed of only hypertext codes, sentences which are only partial translation of the source, or eventually not a correct translation at all. Such noisy pairs potentially degrade the translation model qu"
W11-2145,J05-4003,0,0.136591,"Missing"
W11-2145,W09-0435,1,0.795833,"t approach that relies on part-of-speech (POS) sequences. By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately. 2.4.1 POS-based Reordering Model To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder. 381 2.4.2 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract the phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test se"
W11-2145,W08-0303,1,0.785274,"ents including additional models that enhance translation quality by introducing alternative or additional information into the translation or language modelling process. 2.5.1 Discriminative Word Alignment In most of our systems we use the PGIZA++ Toolkit4 to generate alignments between words in the training corpora. The word alignments are generated in both directions and the grow-diag-final-and heuristic is used to combine them. The phrase extraction is then done based on this word alignment. In the English-German system we applied the Discriminative Word Alignment approach as described in Niehues and Vogel (2008) instead. This alignment model is trained on a small corpus of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ Toolkit and POS information. 2.5.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of context information at the phrase boundaries. Although more target side context is available to the language model, source 4 http://www.cs.cmu.edu/˜"
W11-2145,W11-2124,1,0.753565,"available to the language model, source 4 http://www.cs.cmu.edu/˜qing/ side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see (Niehues et al., 2011). 2.5.3 Parallel phrase scoring The process of phrase scoring is held in two runs. The objective of the first run is to compute the necessary counts and to estimate the scores, all based on the source phrases; while the second run is similarly held based on the target phrases. Thus, the extracted phrases have to be sorted twice: once by source phrase and once by target phrase. These two sorting operations are almost always done on an external storage device and hence consume most of the time spent in this step. The phrase scoring step was reimplemented in order to exploit the available computa"
W11-2145,P02-1040,0,0.0863525,"perplexity drops by half since the POS language model helps constructing sentences that have a better structure. System BLEU no POS LM POS LM 16.64 16.88 avg. ngram length Word POS 2.77 3.18 2.81 3.40 PPL POS 66.78 33.36 Table 3: Analysis of context length 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation. 3.1 German-English The German-to-English baseline system applies short-range reordering rules and uses a language model trained on the EPPS and News Commentary. By exchanging the baseline language model by one trained on the News Shuffle corpus we improve the translation quality considerably, by more than 3 BLEU points. When we expand the coverage of the reordering rules to enable long-range reordering we can improve even further by 0.4 and adding a second language model trained on the English Gigaword corpus we gain another 0.3 BLEU points. To ensure that the ph"
W11-2145,2007.tmi-papers.21,0,0.252178,"7 million pairs. Thus throwing around 6 million pairs. 2.4 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distortion model, we use a different approach that relies on part-of-speech (POS) sequences. By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately. 2.4.1 POS-based Reordering Model To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder. 381 2.4.2 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated"
W11-2145,C08-1098,0,0.0985813,"ge Models In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German. We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and 382 #pairs(G) 0.203 1.444 1.693 Moses ∗103 (s) 25.99 184.19 230.97 KIT ∗103 (s) 17.58 103.41 132.79 Table 2: Comparison of Moses and KIT phrase extraction systems therefore the more difficult target language generation. The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus. We tried n-gram lengths of 4 and 7. While no improvement in translation quality could be achieved using the POS language models based on the normal POS tags, the 4-gram POS language model based on fine-grained tags could improve the translation system by 0.2 BLEU points as shown in Table 3. Surprisingly, increasing the n-gram length to 7"
W11-2145,W05-0836,1,0.916992,"sed on a GIZA++ word alignment. The language model was trained on the monolingual parts of the same corpora by the SRILM Toolkit (Stolcke, 2002). It is a 4-gram SRI language model using Kneser-Ney smoothing. The problem of word reordering is addressed using the POS-based reordering model as described in Section 2.4. The part-of-speech tags for the reordering model are obtained using the TreeTagger (Schmid, 1994). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation and optimization with regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 20 translation options for every source phrase were considered. 2.1 Data We trained all systems using the parallel EPPS and News Commentary corpora. In addition, the UN corpus and the Giga corpus were used for training 379 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379–385, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics the French-English systems. Optimization was done for most languages using the news-test2008 data set and news-test2010 was used as test set. The only exception is GermanE"
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W12-3144,W11-2145,1,0.62336,"the target language. In this evaluation, the POS language model is applied for the English-German system. We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German. The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. We use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word classes. Here, we generated word classes in a different way. First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consis"
W12-3144,E03-1076,0,0.0436961,"ed on 500 hand-aligned sentences selected from the EPPS corpus. 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell1 lexicon to map words written according to old German spelling rules to new German spelling rules. In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system. The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011). The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets. Giving at the end around 16 million sentence pairs. 2.3 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different ap"
W12-3144,D09-1022,0,0.0220084,"ore target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see Niehues et al. (2011). 351 2.4.3 Discriminative Word Lexica Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not want to change the feature value. In order to keep track o"
W12-3144,2011.iwslt-evaluation.9,1,0.91403,"d of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell1 lexicon to map words written according to old German spelling rules to new German spelling rules. In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system. The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011). The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets. Giving at the end around 16 million sentence pairs. 2.3 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different approach that relies on POS sequences. By abstracting from surface words to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntacti"
W12-3144,W09-0435,1,0.802047,"to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntactic parse trees. 2.3.1 POS-based Reordering Model In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). 2.3.2 Tree-based Reordering Model Word order is quite different between German and English. And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence. Using discontinuous POS rules already improves the translation tremendously. In addition, we apply a tree-based reordering model for the German-English translation. Syntactic parse trees provide information about the words in a sentence that form constituents and should therefore be treated as inseparable units by the reordering model. For the tree-based reordering model, syntactic parse tr"
W12-3144,W08-0303,1,0.835417,"rase-based MT. In addition to the POS-based reordering model used in past years, for German-English we extended it to also use rules learned using syntax trees. The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier. For the French-English and English-French translation systems, we also used phrase table adaptation to avoid System Description For the French↔English systems the phrase table is based on a GIZA++ word alignment, while the systems for German↔English use a discriminative word alignment as described in Niehues and Vogel (2008). The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002). The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translati"
W12-3144,2011.iwslt-papers.6,1,0.852447,"., 2011) and also reduces training time. 2.4.4 Quasi-Morphological Operations for OOV words Since German is a highly inflected language, there will be always some word forms of a given Gerwe use the ending of the source and target word to determine which pair of operations should be used. Figure 1: Quasi-morphological operations man lemma that did not occur in the training data. In order to be able to also translate unseen word forms, we try to learn quasi-morphological operations that change the lexical entry of a known word form to the unknown word form. These have shown to be beneficial in Niehues and Waibel (2011) using Wikipedia2 titles. The idea is illustrated in Figure 1. If we look at the data, our system is able to translate a German word Kamin (engl. chimney), but not the dative plural form Kaminen. To address this problem, we try to automatically learn rules how words can be modified. If we look at the example, we would like the system to learn the following rule. If an “en” is appended to a German word, as it is done when creating the dative plural form of Kaminen, we need to add an “s” to the end of the English word in order to perform the same morphological word transformation. We use only ru"
W12-3144,2010.iwslt-evaluation.11,1,0.757697,"cing alternative or additional information into the translation modeling process. 2.4.1 Phrase table adaptation Since the Giga corpus is huge, but noisy, it is advantageous to also use the translation probabilities of the phrase pair extracted only from the more reliable EPPS and News commentary corpus. Therefore, we build two phrase tables for the French↔English system. One trained on all data and the other only trained on the EPPS and News commentary corpus. The two models are then combined using a log-linear combination to achieve the adaptation towards the cleaner corpora as described in (Niehues et al., 2010). The newly created translation model uses the four scores from the general model as well as the two smoothed relative frequencies of both directions from the smaller, but cleaner model. If a phrase pair does not occur in the indomain part, a default score is used instead of a relative frequency. In our case, we used the lowest probability. 2.4.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of"
W12-3144,W11-2124,1,0.872887,"of context information at the phrase boundaries. Although more target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see Niehues et al. (2011). 351 2.4.3 Discriminative Word Lexica Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not"
W12-3144,E99-1010,0,0.070697,"n the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word classes. Here, we generated word classes in a different way. First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Generally, all cluster language models used in our systems are 5-gram. 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni"
W12-3144,P02-1040,0,0.0865044,"h, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Generally, all cluster language models used in our systems are 5-gram. 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation. 3.1 BLEU score of 22.31 on the test data. For the last two systems, we did not perform new optimization runs. System Baseline + Lattice Phrase Extraction + Gigaward Language Model + Bilingual LM + Cluster LM + DWL + Tree-based Reordering + OOV 353 Test 21.32 21.36 21.73 21.91 22.09 22.19 22.26 22.31 Table 1: Translation results for German-English 3.2 English-German The English-German baseline system uses also POS-based reordering, discriminative word alignment and a language model based on EPPS, NC and News Shuffle. A small gain could be achieved by the POS-based"
W12-3144,W08-1006,0,0.0209559,"the French↔English systems the phrase table is based on a GIZA++ word alignment, while the systems for German↔English use a discriminative word alignment as described in Niehues and Vogel (2008). The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002). The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 10 translation options for every source phrase are considered. 2.1 Data Our translation models were trained on the EPPS and News Commentary (NC) corpora. Furthermore, the additional available d"
W12-3144,2007.tmi-papers.21,0,0.0608665,"by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different approach that relies on POS sequences. By abstracting from surface words to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntactic parse trees. 2.3.1 POS-based Reordering Model In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). 2.3.2 Tree-based Reordering Model Word order is quite different between German and English. And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence. Using discontinuous POS rules already improves the translation tremendously. In addition, we apply a tree-based reordering model for the German-English translation. Syntactic parse trees p"
W12-3144,C08-1098,0,0.0336822,"apply the POS and cluster language models in different systems. All language models are integrated into the translation system by a log-linear combination and received optimal weights during tuning by the MERT. 2.5.1 POS Language Models The POS language model is trained on the POS sequences of the target language. In this evaluation, the POS language model is applied for the English-German system. We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German. The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. We use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word cl"
W12-3144,W05-0836,1,0.887521,"nd tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 10 translation options for every source phrase are considered. 2.1 Data Our translation models were trained on the EPPS and News Commentary (NC) corpora. Furthermore, the additional available data for French and English (i.e. UN and Giga corpora) were exploited in the corresponding systems. The systems were tuned with the news-test2011 data, while news-test2011 was used for testing in all our systems. We trained language models for each language on the monolingual part of the training corpora as well as the News Shuffle and the Gigaword (version 4) corpora. The d"
W12-3144,W10-1719,1,\N,Missing
W13-0805,P05-1033,0,0.0351559,"rdering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al.,"
W13-0805,P05-1066,0,0.121228,"Missing"
W13-0805,W08-0307,0,0.138729,"OS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are"
W13-0805,D11-1018,0,0.0126381,"as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by the POS-based reordering models in Niehues and Kolss (2009) and Rottmann and Vogel (2007), we present a reordering model based on the syn"
W13-0805,C10-1043,0,0.0120148,"e more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by th"
W13-0805,2007.mtsummit-papers.29,0,0.0711089,"l MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and targ"
W13-0805,2010.amta-papers.11,0,0.0130572,"vements could be achieved by combining POS and tree-based reordering rules and applying a lexicalized reordering model in addition. Table 2 shows the results. Up to 0.7 BLEU points could be gained by adding tree rules and another 0.1 by lexicalized reordering. System Rule Type POS POS + Tree POS + Tree rec. POS + Tree rec.+ par. 45 noLexRM Dev Test 41.29 38.07 41.94 38.47 42.35 38.66 42.48 38.79 LexRM Dev Test 42.04 38.55 42.44 38.57 42.80 38.71 42.87 38.88 Table 2: German-French 6.5 Binarized Syntactic Trees Even though related work using syntactic parse trees in SMT for reordering purposes (Jiang et al., 2010) have reported an advantage of binarized parse trees over standard parse trees, our model did not benefit from binarized parse trees. It seems that the flat hierarchical structure of standard parse trees enables our reordering model to learn the order of the constituents most efficiently. 7 Example 3 shows another aspect of how the treebased rules work. With the help of the tree-based reordering rules, it is possible to relocate the separated prefix of German verbs and find the correct translation. The verb vorschlagen consist of the main verb (MV) schlagen (here conjugated as schl¨agt) and th"
W13-0805,2009.eamt-1.27,0,0.0657715,"icular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rule"
W13-0805,2005.iwslt-1.8,0,0.203704,"on hypotheses are generated. An additional reordering model might be included in the log-linear model of translation. However, these methods can cover reorderings only over a very limited distance. Recently, reordering as preprocessing has drawn much attention. The idea is to detach the reordering problem from the decoding process and 2 Related Work The problem of word reordering has been addressed by several approaches over the last years. In a phrase-based SMT system reordering can be achieved during decoding by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal,"
W13-0805,P07-2045,0,0.00413724,"ing by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed thes"
W13-0805,W09-0435,1,0.750859,"yntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by the POS-based reordering models in Niehues and Kolss (2009) and Rottmann and Vogel (2007), we present a reordering model based on the syntactic structure of the source sentence. We intend to cover both short-range and long-range re40 ordering more reliably by abstracting to constituents extracted from syntactic parse trees instead of working only with morphosyntactic information on the word level. Furthermore, we combine POS-based and tree-based models and additionally include a lexicalized reordering model. Altogether we apply word reordering on three different levels: lexicali"
W13-0805,P02-1040,0,0.106244,"rientations at the incoming and outgoing phrase boundaries: monotone, swap and discontinuous. In order to apply the lexicalized reordering model on lattices the original position of each word is stored in the lattice. While the translation hypothesis is generated, the reordering orientation with respect to the original position of the words is checked at each phrase boundary. The probability for the respective orientation is included as an additional score. 6 Results The tree-based models are applied for GermanEnglish and German-French translation. Results are measured in case-sensitive BLEU (Papineni et al., 2002). 6.1 General System Description First we describe the general system architecture which underlies all the systems used later on. We use a phrase-based decoder (Vogel, 2003) that takes word lattices as input. Optimization is performed using MERT with respect to BLEU. All POS-based or tree-based systems apply monotone translation only. Baseline systems without reordering rules use a distance-based reordering model. In addition, a lexicalized reordering model as described in (Koehn et al., 2005) is applied where indicated. POS tags and parse trees are generated using the Tree Tagger (Schmid, 199"
W13-0805,popovic-ney-2006-pos,0,0.335373,"Missing"
W13-0805,W08-1006,0,0.264869,"tion First we describe the general system architecture which underlies all the systems used later on. We use a phrase-based decoder (Vogel, 2003) that takes word lattices as input. Optimization is performed using MERT with respect to BLEU. All POS-based or tree-based systems apply monotone translation only. Baseline systems without reordering rules use a distance-based reordering model. In addition, a lexicalized reordering model as described in (Koehn et al., 2005) is applied where indicated. POS tags and parse trees are generated using the Tree Tagger (Schmid, 1994) and the Stanford Parser (Rafferty and Manning, 2008). 6.1.1 Data The German-English system is trained on the provided data of the WMT 2012. news-test2010 and news-test2011 are used for development and testing. The type of data used for training, development and testing the German-French system is similar to WMT data, except that 2 references are available. The training corpus for the reordering models consist of the word-aligned Europarl and News Commentary corpora where POS tags and parse trees are 44 generated for the source side. 6.2 German-English We built systems using POS-based and tree-based reordering and show the impact of the individu"
W13-0805,2007.tmi-papers.21,0,0.419866,"ed on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the pat"
W13-0805,N04-4026,0,0.00934785,"nerated. An additional reordering model might be included in the log-linear model of translation. However, these methods can cover reorderings only over a very limited distance. Recently, reordering as preprocessing has drawn much attention. The idea is to detach the reordering problem from the decoding process and 2 Related Work The problem of word reordering has been addressed by several approaches over the last years. In a phrase-based SMT system reordering can be achieved during decoding by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems"
W13-0805,D07-1077,0,0.0588143,"Missing"
W13-0805,C04-1073,0,0.048704,"source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering prob"
W13-0805,P01-1067,0,0.124032,"in a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft re"
W13-0805,W07-0401,0,\N,Missing
W13-0805,W06-1609,0,\N,Missing
W13-0805,W06-3119,0,\N,Missing
W13-2210,2010.iwslt-evaluation.11,1,0.829632,"Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The results are reported as casesensitive BLEU scores on one reference translation. For the French↔English systems, we built two phrase tables; one trained with all data and the other trained only with the EPPS and NC corpora. This is due to the fact that Giga corpus is big but noisy and EPPS and NC corpus are more reliable. The two models are combined log-linearly to achieve the adaptation towards the cleaner corpora as described in Niehues et al. (2010). 6 Cluster Language Models POS Language Models For the English→German system, we use the POS language model, which is trained on the POS sequence of the target language. The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German. The RFTagger generates finegrained tags which include person, gender, and case information. The language model is trained with up to 9-gram information, using the German side of the parallel EPPS and NC corpus, as well as the News Shuffle corpus. 7.2 English→German The English to German baseline system uses POSbased reordering and language model"
W13-2210,W11-2124,1,0.831087,"mann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German sour"
W13-2210,E99-1010,0,0.0718706,"tering to the features for higher order n-grams. Furthermore, we created the training examples differently in order to focus on addressing errors of the other models of the phrase-based translation 105 6.2 system. We first translated the whole corpus with a baseline system. Then we only used the words that occur in the N-Best List and not in the reference as negative examples instead of using all words that do not occur in the reference. 5.3 In order to use larger context information, we use a cluster language model for all our systems. The cluster language model is based on the idea shown in Och (1999). Using the MKCLS algorithm, we cluster the words in the corpus, given a number of classes. Then words in the corpus are replaced with their cluster IDs. Using these cluster IDs, we train n-gram language models as well as a phrase table with this additional factor of cluster ID. Our submitted systems have diversed range of the number of clusters as well as n-gram. Quasi-Morphological Operations Because of the inflected characteristic of the German language, we try to learn quasimorphological operations that change the lexical entry of a known word form to the out-ofvocabulary (OOV) word form a"
W13-2210,W13-0805,1,0.838059,"all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (2009) introduced the Discriminative Word Lexicon (DWL) into phrase-based machine translation. In this approach, a maximum entropy model is used to determine the probability of using a target word in the translation. In this evaluation, we used two extensions to this work as shown in (Niehues and Waibel, 2013)."
W13-2210,P02-1040,0,0.0915781,"he Karlsruhe Institute of Technology Translation Systems for the WMT 2013 Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues, Teresa Herrmann, Isabel Slawik and Alex Waibel Karlsruhe Institute of Technology Karlsruhe, Germany firstname.lastname@kit.edu Abstract Section 4. In addition to it, tree-based reordering model and lexicalized reordering were added for German↔English systems. An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. The translation was optimized using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) towards better BLEU (Papineni et al., 2002) scores. This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task. Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-ofspeech (POS) and automatic cluster language models and discriminative word lexica (DWL). In addition, we combined reordering models on different sentence abstraction levels. 1 2.1 The Europarl corpus (EPPS) and News Commentary (NC) corpus were used for training our translation models. W"
W13-2210,E03-1076,0,0.0487175,"der with lattice input. The paper is organized as follows: the next section gives a detailed description of our systems including all the models. The translation results for all directions are presented afterwards and we close with a conclusion. 2 Data 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentence pairs with length mismatch. Compound splitting is applied to the German part of the corpus of the German→English system as described in Koehn and Knight (2003). System Description The phrase table is based on a GIZA++ word alignment for the French↔English systems. For the German↔English systems we use a Discriminative Word Alignment (DWA) as described in Niehues and Vogel (2008). For every source phrase only the top 10 translation options are considered during decoding. The SRILM Toolkit (Stolcke, 2002) is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to"
W13-2210,W08-1006,0,0.0143303,"ion of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (20"
W13-2210,D09-1022,0,0.0267009,"Missing"
W13-2210,2007.tmi-papers.21,0,0.165327,"an↔English system, reordering rules learned from syntactic parse trees were used in addition. 4.1 Translation Models In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011)"
W13-2210,C08-1098,0,0.107428,"ord Reordering 5 Word reordering was modeled based on POS sequences. For the German↔English system, reordering rules learned from syntactic parse trees were used in addition. 4.1 Translation Models In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make b"
W13-2210,2011.iwslt-evaluation.9,1,0.878095,"is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to be noisy. Namely, the Giga EnglishFrench parallel corpus and the all the new webcrawled data . The operation was performed using 104 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104–108, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 4.3 an SVM classifier as in our past systems (Mediani et al., 2011). For each pair, the required lexica were extracted from Giza alignment of the corresponding EPPS and NC corpora. Furthermore, for the web-crawled data, higher precision classifiers were trained by providing a larger number of negative examples to the classifier. After filtering, we could still find English sentences in the other part of the corpus. Therefore, we performed a language identification (LID)based filtering afterwards (performed only on the French-English corpora, in this participation). 4 The lexicalized reordering model stores the reordering probabilities for each phrase pair. Po"
W13-2210,W09-0435,1,0.865694,"d experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tre"
W13-2210,W05-0836,1,0.872186,"Missing"
W13-2210,W08-0303,1,0.80366,"close with a conclusion. 2 Data 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentence pairs with length mismatch. Compound splitting is applied to the German part of the corpus of the German→English system as described in Koehn and Knight (2003). System Description The phrase table is based on a GIZA++ word alignment for the French↔English systems. For the German↔English systems we use a Discriminative Word Alignment (DWA) as described in Niehues and Vogel (2008). For every source phrase only the top 10 translation options are considered during decoding. The SRILM Toolkit (Stolcke, 2002) is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to be noisy. Namely, the Giga EnglishFrench parallel corpus and the all the new webcrawled data . The operation was performed using 104 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104–108, c"
W13-2210,2012.amta-papers.19,1,0.749834,"LS algorithm, we cluster the words in the corpus, given a number of classes. Then words in the corpus are replaced with their cluster IDs. Using these cluster IDs, we train n-gram language models as well as a phrase table with this additional factor of cluster ID. Our submitted systems have diversed range of the number of clusters as well as n-gram. Quasi-Morphological Operations Because of the inflected characteristic of the German language, we try to learn quasimorphological operations that change the lexical entry of a known word form to the out-ofvocabulary (OOV) word form as described in Niehues and Waibel (2012). 5.4 7 Phrase Table Adaptation 7.1 German→English The experiments for the German to English translation system are summarized in Table 1. The baseline system uses POS-based reordering, DWA with lattice phrase extraction and language models trained on the News Shuffle corpus and Giga corpus separately. Then we added a 5-gram cluster LM trained with 1,000 word classes. By adding a language model using the filtered crawled data we gained 0.3 BLEU on the test set. For this we combined all language models linearly. The filtered crawled data was also used to generate a phrase table, which brought a"
W13-2210,W13-2264,1,0.697889,"er (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (2009) introduced the Discriminative Word Lexicon (DWL) into phrase-based machine translation. In this approach, a maximum entropy model is used to determine the probability of using a target word in the translation. In this evaluation, we used two extensions to this work as shown in (Niehues and Waibel, 2013). First, we added additional features to model the order of the source words better. Instead of representing the source sentence as a bag-of-words, we used a bag-of-n-grams. We used n-grams up to the order of three and applied count filtering to the features for higher order n-grams. Furthermore, we created the training examples differently in order to focus on addressing errors of the other models of the phrase-based translation 105 6.2 system. We first translated the whole corpus with a baseline system. Then we only used the words that occur in the N-Best List and not in the reference as neg"
W13-2223,W13-0805,1,0.848446,"OS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its"
W13-2223,E03-1076,0,0.0855399,"n the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. 2.2 System Overview Karlsruhe Institute of Technology Single System 2.2.1 Preprocessing The training data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extra"
W13-2223,P07-2045,0,0.00527113,"ne Translation. The technique of Statistical Post-Editing (Dugast et al., 2007) is used to automatically edit the output of the rule-based system. A Statistical Post-Editing (SPE) module is generated from a bilingual corpus. It is basically a translation module by itself, however it is trained on rule-based • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained on 2M phrases from the news/europarl and CommonCrawl corpora, provided as training data for WMT 2013. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news development set. 5 http://geek.kyloo.net/software 6 The fifth edition of (LDC2011T07) was not used. the English Gigaword 188 0 5:that/1 7:this/3 1 3:is/3 8:was/1 2 0:*EPS*/3 4:it/1 0:*EPS*/3 2:in/1 3 4 0:*EPS*/3 6:the/1 5 0:*EPS*/1 1:future/3 6 Figure 1: Confusion network of four different hypotheses. 3 RWTH Aachen System Combination Table 1: Comparison of single systems tuned on newstest2009 and newstest2010. The results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different transla"
W13-2223,J04-2004,0,0.0419685,"d a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus mode"
W13-2223,W07-0734,0,0.0383886,"results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines. First, a word to word alignment for the given single system hypotheses is produced. In a second step a confusion network is constructed. Then, the hypothesis with the highest probability is extracted from this confusion network. For the alignment procedure, each of the given single systems generates one confusion network with its own as primary system. To this primary system all other hypotheses are aligned using the METEOR (Lavie and Agarwal, 2007) alignment and thus the primary system defines the word order. Once the alignment is given, the corresponding confusion network is constructed. An example is given in Figure 1. The final network for one source sentence is the union of all confusion networks generated from the different primary systems. That allows the system combination to select the word order from different system outputs. Before performing system combination, each translation output was normalized by tokenization and lowercasing. The output of the combination was then truecased based on the original truecased output. The mo"
W13-2223,W07-0732,0,0.0965924,"m = 10, and used k = 300. 2.3.4 translations and reference data. It applies corrections and adaptations learned from a phrase-based 5-gram language model. Using this two-step process will implicitly keep long distance relations and other constraints determined by the rule-based system while significantly improving phrasal fluency. It has the advantage that quality improvements can be achieved with very little but targeted bilingual data, thus significantly reducing training time and increasing translation performance. The basic setup of the SPE component is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the s"
W13-2223,N12-1005,0,0.0335346,"with standard n-gram translation models is that the elementary units are bilingual pairs, which means that the underlying vocabulary can be quite large, even for small translation tasks. Unfortunately, the parallel data available to train these models are typically order of magnitudes smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that LIMSI-CNRS Single System 2.3.1 System overview LIMSI’s system is built with n-code (Crego et al., 2011), an open source statistical machine translation system based on bilingual n-gram3 . In this approach, the translation model relies on a specific decomposition of the joint probability of a sentence pair using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual u"
W13-2223,2010.iwslt-papers.6,0,0.0342874,"Missing"
W13-2223,2012.iwslt-papers.7,1,0.838536,"an compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse tree"
W13-2223,W08-0310,0,0.0216057,"nal in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the same setup as last year6 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we also took advantage of our inhouse text processing tools for the tokenization and detokenization steps (Dchelotte et al., 2008) and our system is built in “true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar ElKahlout and Yvon, 2010)), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 • Named entities are re"
W13-2223,2011.iwslt-papers.5,1,0.849659,"processed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering"
W13-2223,J06-4004,0,0.0399278,"Missing"
W13-2223,D09-1022,1,0.905747,"Missing"
W13-2223,D08-1089,0,0.0206813,"Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185–192, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics additional feature are added to the log-linear combination. The model weights are optimized with standard Mert (Och, 2003a) on 200-best lists. The optimization criterion is B LEU. cleaner corpora, EPPS and NC. Assuming that this corpus is very noisy, we biased our classifier more towards precision than recall. This was realized by giving higher number of f"
W13-2223,2011.iwslt-evaluation.9,1,0.888245,"ing data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilin"
W13-2223,W09-0435,1,0.861514,"ith regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering"
W13-2223,W08-0303,1,0.903038,"Missing"
W13-2223,N04-4026,0,0.016377,"y so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overal"
W13-2223,W09-0413,1,0.842391,"The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the refe"
W13-2223,W05-0836,1,0.864882,"filtering task). 2.1.1 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short"
W13-2223,W11-2124,1,0.875547,"this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model r"
W13-2223,C12-3061,1,0.817205,"e of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2"
W13-2223,J03-1002,1,0.00903936,"translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings"
W13-2223,P03-1021,0,0.694457,"models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overall search is based on a beam-search strategy on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and Mario, 2006). 2.2.6 Language Models We build separate language models and combined them prior to decoding. As word-token based language models, one language model is built on EPPS, NC, and giga corpus, while another one is built u"
W13-2223,W08-1006,0,0.0614361,"and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its probability added as an additional score. 2.1.3 Language Model During decoding a 4-"
W13-2223,2007.tmi-papers.21,0,0.168794,") is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser"
W13-2223,W12-3140,1,\N,Missing
W13-2223,W11-2135,0,\N,Missing
W13-2223,W10-1704,0,\N,Missing
W13-2264,W09-0435,1,0.732129,". We only considered the examples for the classifier of target word e, where e occurs in the N -Best list entry E 0 . If the word does not occur in any N -Best list entry of a training sentence, but in the reference, we created an additional example (F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluat"
W13-2264,W08-0303,1,0.765241,"translation quality drops. Especially for System 1, we have a significant drop in the BLEU score of the test set by 0.6 BLEU points. One problem might be that most of the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k"
W13-2264,P07-1020,0,0.0606666,"ach is used. In this approach, instead of building the translation by translating word by word, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors i"
W13-2264,2012.amta-papers.19,1,0.706309,"potheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluated them also in a second system. In addition to the models used in the first system we performed a log-linear language model and phrase table adaptation as described in Niehues and Waibel (2012). To this system we refer as System 2 in the following experiments. (11) If we have seen the word only in the reference, we create an training example without target features. Therefore, we have again a training example which can not happen when using the DWL model. Therefore, we removed these examples in the last method (Restricted TF). 6 Experiments After presenting the different approaches to perform feature and example selection, we will now evaluate them. First, we will give a short overview of the MT system. Then we will give a detailed evaluation on the task of translating German lectur"
W13-2264,D07-1007,0,0.0450491,"kshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The focus of this work was to model complex morphology on the target language. 3 When we have the probability for every word ej given the source sentence F , we need to combine these probabilities into a probability of the whole target sentence E = e1 . . . eJ given F . Making an assumption of independence on the target side as well, the models can be combined to the probability"
W13-2264,W11-2124,1,0.819344,"d on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k 77k System 1 Dev Test 26.32 24.24 27.46 25.56 27.34 24.92 27.69 25.55 27.75 25.71 27.81 25.67 27.76 25.76 grams with count filtering in all experiments. In the first experiment, we used the original approach to create the training examples. In this case, all sentences where the word does not occur in the reference generate negative examples. In our setup, we nee"
W13-2264,2007.tmi-papers.21,0,0.387257,"ry training sentence N -Best list translation (F, E, E 0 ). We only considered the examples for the classifier of target word e, where e occurs in the N -Best list entry E 0 . If the word does not occur in any N -Best list entry of a training sentence, but in the reference, we created an additional example (F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order t"
W13-2264,2010.amta-papers.32,0,0.0406929,"eatures is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The"
W13-2264,2010.amta-papers.33,0,0.0194973,"ded by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The focus of this work was to model complex morphology on the target language. 3 When we have the probability for every word ej given the source sentence F , we need to combine these probabilities into a probability of the whole target sentence E = e1 . . . eJ given F . Making an assumption of independence on the target side as well, the models can be combined to the probability of E given F : p(E|F ) = ej ∈e The DWL is a maximum entropy model used to determine the probability of using a target word in the translation. Therefore, we train individual models for every target word. Each model i"
W13-2264,W05-0836,1,0.825363,"(F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluated them also in a second system. In addition to the models used in the first system we performed a log-linear language model and phrase table adaptation as described in Niehues and Waibel (2012). To this system we refer as System 2 in the following"
W13-2264,E03-1076,0,0.606126,"on bigrams instead of unigrams, the number of features increases by a factor of eight. Furthermore, in both cases the translation quality drops. Especially for System 1, we have a significant drop in the BLEU score of the test set by 0.6 BLEU points. One problem might be that most of the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features Syst"
W13-2264,N03-1017,0,0.0194217,"ource information, they ignore the structure of the source and target sentence. We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. Furthermore, as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors. By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL. 1 Introduction In many state-of-the-art SMT systems, the phrasebased (Koehn et al., 2003) approach is used. In this approach, instead of building the translation by translating word by word, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related"
W13-2264,P07-2045,0,0.0100206,"f the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k 77k System 1 Dev Test 26.32 24.24 27.46 25.56 27.34 24.92 27.69 25.55 27.75 25.71 27.81 25.67 27.76 25.76 grams with count filtering in all experiments. In the f"
W13-2264,D09-1022,0,0.105865,"called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related t"
W13-2264,2011.iwslt-evaluation.9,1,\N,Missing
W13-3204,W09-0435,1,0.874539,"Missing"
W13-3204,W08-0303,1,0.787059,"differently before. We also introduced an all caps feature which is turned on if the whole word was written in capital letters. We hope that this can help detect abbreviations which are usually written in all capital letters. For example EU will be represented as 5.2 Translation System Description The translation system for the German-to-English task was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was w1 = e u eu &lt;w&gt;e u&lt;/w&gt;&lt;ALLCAPS&gt; 5 Word Representation Evaluation We evaluated the RBM-based language model on different statistical machine translation (SMT) tasks. We will first analyze the letter-based word 1 34 http://www.ted.com Model WordIndex Letter 1-gram Letter 2-gram Letter 3-gram Letter 3-gram Letter 4-gram Letter 4-gram Caps No No No Yes No Yes VocSize 27,748 107 1,879 12,139 8,675 43,903 25,942 TotalVectors 27,748 21,216 27,671 27,720 27,710 27,737 27,728 1 Word 27,748 17,319 27,620 27,701"
W13-3204,2012.iwslt-papers.3,1,0.627669,"oser together and generalize better over unseen words. We hope that words containing similar letter n-grams will yield a good indicator for words that have the same function inside the sentence. Introducing a method for subword units also has the advantage that the input layer can be smaller, while still representing nearly the same vocabulary with unique feature vectors. By using a smaller input layer, less weights need to be trained and the training is faster. In this work we present the letter n-gram approach to represent words in an CSLM, and compare it to the word-based CSLM presented in Niehues and Waibel (2012). The rest of this paper is structured as follows: First we will give an overview of related work. After that we give a brief overview of restricted Boltzmann machines which are the basis of the letter-based CSLM presented in Section 4. Then we will present the results of the experiments and conclude our work. We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unkno"
W13-3204,2010.iwslt-evaluation.11,1,0.856402,"ter 4-gram Dev 27.45 27.70 27.45 27.52 27.60 System Baseline +Letter 3-gram +Letter 3-gram+caps Baseline+ngram +Letter 3-gram +Letter 3-gram+caps BL+ngram+adaptpt +Letter 3-gram +Letter 3-gram+caps Test 24.06 24.34 24.15 24.25 24.30 Table 3: Results of German-to-English TED translations using an additional in-domain language model. Dev 28.40 28.55 28.31 28.31 28.46 Test 23.02 23.84 23.85 24.06 24.25 24.47 24.57 24.71 24.66 Table 5: Difference between caps and non-caps letter n-gram models. A third experiment is presented in Table 4. Here we also applied phrase table adaptation as described in Niehues et al. (2010). In this experiment the word index model improves the system by 0.4 BLEU points. In this case all letter-based models perform very similar. They are again performing slightly worse than the word index-based system, but better than the baseline system. To summarize the results, we could always improve the performance of the system by adding the letter n-gram-based language model. Furthermore, in most cases, the bigram model performs worse than the higher order models. It seems to be important for this task to have more context information. The 3- and 4-gram-based models perform almost equal, b"
W13-3204,W05-0821,0,0.0350579,"wn in Niehues and Waibel (2012), that using a restricted Boltzmann machine with a different layout during decoding can yield an increase in BLEU score. There has also been a lot of research in the field of using subword units for language modeling. In Shaik et al. (2011) linguistically motivated sub-lexical units were proposed to improve open vocabulary speech recognition for German. Research on morphology-based and subword language models on a Turkish speech recognition task has been done by Sak et al. (2010). The idea of Factored Language models in machine translation has been introduced by Kirchhoff and Yang (2005). Similar approaches to develop joint language models for morphologically rich languages in machine translation have been presented by Sarikaya and Deng (2007). In Emami et al. (2008) a factored neural network language model for Arabic was built. They used different features such as segmentation, part-of-speech and diacritics to enrich the information for each word. Restricted Boltzmann Machine-based Language Model In this section we will briefly review the continuous space language models using restricted Boltzmann machines (RBM). We will focus on the parts that are important for the implemen"
W13-3204,W11-2124,1,0.895117,"Missing"
W13-3204,P07-2045,0,0.0034696,"Missing"
W13-3204,2007.tmi-papers.21,0,0.100036,"Missing"
W13-3204,N07-2037,0,0.0254383,"s also been a lot of research in the field of using subword units for language modeling. In Shaik et al. (2011) linguistically motivated sub-lexical units were proposed to improve open vocabulary speech recognition for German. Research on morphology-based and subword language models on a Turkish speech recognition task has been done by Sak et al. (2010). The idea of Factored Language models in machine translation has been introduced by Kirchhoff and Yang (2005). Similar approaches to develop joint language models for morphologically rich languages in machine translation have been presented by Sarikaya and Deng (2007). In Emami et al. (2008) a factored neural network language model for Arabic was built. They used different features such as segmentation, part-of-speech and diacritics to enrich the information for each word. Restricted Boltzmann Machine-based Language Model In this section we will briefly review the continuous space language models using restricted Boltzmann machines (RBM). We will focus on the parts that are important for the implementation of the input layers described in the next section. A restricted Boltzmann machine is a generative stochastic neural network which consists of a visible"
W13-3204,H05-1026,0,0.0323748,"l networks were used to predict word categories. Xu and Rudnicky (2000) proposed a language model that has an input consisting of one word and no hidden units. This network was limited to infer unigram and bigram statistics. There has been research on feed forward neural network language models where they 30 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 30–39, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics 3 achieved a decrease in perplexity compared to standard n-gram language models (Bengio et al., 2003). In Schwenk and Gauvain (2005) and later in Schwenk (2007) research was performed on training large scale neural network language models on millions of words resulting in a decrease of the word error rate for continuous speech recognition. In Schwenk et al. (2006) they use the CSLM framework to rescore n-best lists of a machine translation system during tuning and testing steps. Usually these networks use short lists to reduce the size of the output layer and to make calculation feasible. There have been approaches to optimize the output layer of such a network, so that vocabularies of arbitrary size can be used and there"
W13-3204,W05-0836,1,0.786372,"Missing"
W13-3204,C90-3038,0,\N,Missing
W13-3204,C94-1027,0,\N,Missing
W13-3204,W12-2702,0,\N,Missing
W13-3204,N12-1005,0,\N,Missing
W14-3307,N12-1047,0,0.166607,"or WMT 2014 ∗ Quoc Khanh Do, † Teresa Herrmann, ∗† Jan Niehues, Alexandre Allauzen, ∗ Franc¸ois Yvon and † Alex Waibel ∗ LIMSI-CNRS, Orsay, France † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ surname@limsi.fr † firstname.surname@kit.edu ∗ Abstract ples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approac"
W14-3307,W06-1607,0,0.0257787,"gopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper descri"
W14-3307,W13-0805,1,0.843382,"ection 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model"
W14-3307,P03-1054,0,0.00441034,"uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999)."
W14-3307,E03-1076,0,0.0233691,"is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on"
W14-3307,2005.iwslt-1.8,0,0.0327583,". 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source"
W14-3307,P07-2045,0,0.00834948,"erent language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield"
W14-3307,N12-1005,1,0.95525,"nts in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. The system participates in the German-to-English translation task. It consists of two main components. First, a k-best list is generated using a phrasebased machine translation system. This system will be described in Section 2. Afterwards, the kbest list is reranked using SOUL (Structured OUtput Layer) models. Thereby, a neural network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases fo"
W14-3307,P96-1041,0,0.212941,"target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Cr"
W14-3307,J06-4004,0,0.187497,"Missing"
W14-3307,W05-0836,1,0.776382,"second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006"
W14-3307,2011.iwslt-evaluation.9,1,0.848234,"llel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation"
W14-3307,P10-2041,0,0.0330092,". In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation"
W14-3307,W09-0435,1,0.847502,"splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. Th"
W14-3307,W11-2124,1,0.856532,"ition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target phrase t. Using the n-gram assumption, the joint probability of a"
W14-3307,J03-1002,0,0.0106255,"e corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration"
W14-3307,E99-1010,0,0.058547,"ning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target ph"
W14-3307,P03-1021,0,0.00840255,"and the target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ciated target hypothesis. The goal is to recover the information that is illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT ) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA ) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. model estimates the joint probability of a sentence pair using two sliding windows of length n, one for each language; however, the moves of these windows remain synchronized by the tuple"
W14-3307,W08-1006,0,0.0195332,"the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MK"
W14-3307,2007.tmi-papers.21,0,0.039184,"as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014"
W14-3307,W12-3141,1,\N,Missing
W14-3313,W06-1607,0,0.102882,"us Giga for English→French and French→English. The monolingual part 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics inal position of the words is included as an additional score in the log-linear model of the translation system. done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phra"
W14-3313,W11-2123,0,0.030482,"trained with 1,000 classes using EPPS, NC, and Common Crawl data. Dev 16.64 16.76 17.27 17.45 17.53 17.55 17.82 Test 18.60 18.66 19.66 19.75 19.85 19.92 20.21 Table 3: Experiments for English→German 4.4 German-English Table 4 shows the development steps of the German-English translation system. For the baseline system, the training data of the translation model consists of EPPS, NC and the filtered parallel crawled data. The phrase table is built using GIZA++ word alignment and lattice phrase extraction. All language models are trained with SRILM and scored in the decoding process with KenLM (Heafield, 2011). We use word lattices generated by short and long range reordering rules as input to the decoder. In addition, a bilingual language model and a target language model trained on word clusters with 1,000 classes are included in the system. Enhancing the word reordering with tree-based reordering rules and a lexicalized reordering 133 improve the system performance for GermanEnglish translation. In average we achieved an improvement of over 1.5 BLEU over the respective baselines for all our systems. model improved the system performance by 0.6 BLEU points. Adding a language model trained on sele"
W14-3313,W13-0805,1,0.87688,"model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used"
W14-3313,P03-1054,0,0.00683643,"ces and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase"
W14-3313,E03-1076,0,0.126476,"s well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions. 1 3 Before training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (200"
W14-3313,2005.iwslt-1.8,0,0.0779785,"POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase are extracted from the reordered word lattices instead of the original sentences. In addition, we use a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair. During decoding the lexicalized reordering model determines the reordering orientation of each phrase pair at the phrase boundaries. The probability for the respective orientation with respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language mod"
W14-3313,J03-1002,0,0.0206269,"quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. We participated in the English↔German and English↔French translation directions, usi"
W14-3313,D09-1022,0,0.020404,"or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and W"
W14-3313,E99-1010,0,0.237269,"model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 It is noteworthy that, for this direction, we chose to tune on a subset of 1,000 pairs from newstest2012, due to the long time the whole set takes to be decoded. In a preliminary set of experiments (not reported here), we found no significant differences between tuning on the small or the big development sets. The translation model of the baseline system is trained on the whole parallel data after filtering (EPPS, NC, Common Crawl, Giga). The same data was also used for language modeling. We also use POS-based reorde"
W14-3313,2011.iwslt-evaluation.9,1,0.935211,"re training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignme"
W14-3313,P02-1040,0,0.0896789,"information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 System Baseline + Big LMs + PT Adaptation + Bilingual + Cluster LM + Lexicalized Reordering + Source DWL Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the systems and report the translation qualities achieved during system development. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). 4.1 Dev 15.63 16.56 16.77 16.87 16.92 17.28 Test 27.61 29.02 29.32 29.64 30.17 30.19 Table 1: Experiments for English→French 4.2 French-English Several experiments were conducted for the French→English translation system. They are summarized in Table 2. The baseline system is essentially a phrasebased translation system with some preprocessEnglish-French The development of our English→French system is shown in Table 1. 132 ing steps on the source side and utilizing the short-range POS-based reordering on all parallel data and fine-grained monolingual corpora such as EPPS and NC. Adapting the"
W14-3313,W08-1006,0,0.0827719,"part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the tra"
W14-3313,2007.tmi-papers.21,0,0.507964,"indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the or"
W14-3313,P10-2041,0,0.0621761,"nally, using a discriminative word lexicon with source context has a very small positive effect on the test score, however more than 0.3 on dev. This final configuration was the basis of our submitted official translation. where the words have been replaced either by their corresponding POS tag or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani"
W14-3313,C08-1098,0,0.0570962,"ith respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 It is noteworthy that, for this direction, we chose to tune on a subset of 1,000 pairs from newstest2012, due to the long time the whole set takes to be decoded. In a preliminary set of experiments (not reported here), we found no significant differences between tuning on the small or the big development sets. The translatio"
W14-3313,W09-0435,1,0.901419,"model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice."
W14-3313,W08-0303,1,0.888132,"e models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. We participated in the English↔German and English↔French translation directions, using a phrase-based decoder with lattice input. The paper is organized as follows: the next section describes the data used for each translation direction. Section 3 gives a detailed description of our systems including all the mod"
W14-3313,2011.iwslt-papers.6,1,0.862993,"s inal position of the words is included as an additional score in the log-linear model of the translation system. done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Wai"
W14-3313,W05-0836,1,0.929843,"ehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop o"
W14-3313,2012.amta-papers.19,1,0.769296,"d Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Waibel (2012). This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we take the union of the candidate phrase pairs collected from both phrase tables A detailed description of the union method can be found in Mediani et al. (2012b). The language model is adapted by log-linearly combining the general language model and an indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Op"
W14-3313,W13-2264,1,0.762301,"et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and Waibel (2013). Instead of representing the source sentence as a bag-of-words, we model it as a bag-of-n-grams. This allows us to include information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 System Baseline + Big LMs + PT Adaptation + Bilingual + Cluster LM + Lexicalized Reordering + Source DWL Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the system"
W14-3313,W11-2124,1,0.804622,"ted from the reordered word lattices instead of the original sentences. In addition, we use a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair. During decoding the lexicalized reordering model determines the reordering orientation of each phrase pair at the phrase boundaries. The probability for the respective orientation with respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram lan"
W14-3313,P07-2045,0,\N,Missing
W14-3313,2010.iwslt-evaluation.11,1,\N,Missing
W14-3330,W08-0310,1,0.895121,"Missing"
W14-3330,W11-2123,0,0.0112757,"22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with distinctive features. As outof-domain data, we reuse the data proc"
W14-3330,D11-1125,0,0.0362648,"Missing"
W14-3330,P05-1032,0,0.0608395,"Missing"
W14-3330,J04-2004,0,0.042241,"al articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, the peculiarity of this approach is to rely on the n-gram assumption to decompos"
W14-3330,W04-3237,0,0.0452726,"for E MEA. Table 1 summarizes the data used along with some statistics after the cleaning and pre-processing steps. 3.2 3.3 Part-of-Speech Tagging Medical data exhibit many peculiarities, including different syntactic constructions and a specific vocabulary. As standard POS-taggers are known not to perform very well for this type of texts, we use a specific model trained on the Penn Treebank and on medical data from the MedPost project (Smith et al., 2004). We use Wapiti (Lavergne et al., 2010), a state-of-the-art CRF implementation, with a standard feature set. Adaptation is performed as in (Chelba and Acero, 2004) using the out-of-domain model as a prior when training the in-domain model on medical data. On a medical test set, this adaptation leads to a 8 point reduction of the error rate. A standard model is used for WMT’13 data. For the French side, due to the lack of annotaded data for the medical domain, corpora are tagged using the TreeTagger (Schmid, 1994). Language Models A medical-domain 4-gram language model is built by concatenating the target side of the paral1 Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combi"
W14-3330,P07-2045,0,0.00663677,"Missing"
W14-3330,P96-1041,0,0.209049,"pos-lm term dictionary short titles -3 26 22 6 4 -7 -5 -15 -1 21 2 -17 -22 -13 6 -7 27 16 -33 52 33 69 49 69 - concatenation khresmoi-summary see Section 3.4 from WMT’12 khresmoi-summary Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English and French tokens, respectively. Weights (λk ) from our best N CODE configuration are indicated for each sub-corpora’s bilingual word language model (wrd-lm) and POS factor language model (pos-lm). lel data and all the available monolingual data1 , with modified Kneser-Ney smoothing (Kneser and Ney, 1995; Chen and Goodman, 1996), using the S RI LM (Stolcke, 2002) and K EN LM (Heafield, 2011) toolkits. Although more similar to term-toterm dictionaries, UMLS and W IKIPEDIA proved better to be included in the language model. The large out-of-domain language model used for WMT’13 (Allauzen et al., 2013) is additionaly used (see Table 1). needed on a new corpus in order to adapt the parameters to the new domain. 3 3.1 Data and Systems Preparation Corpora We use all the available (constrained) medical data extracted using the scripts provided by the organizers. This resulted in 7 sub-corpora from the medical domain with di"
W14-3330,P13-1126,0,0.179519,"franc¸ais4 {firstname.lastname}@limsi.fr 2 Abstract 2.1 This paper describes LIMSI’s submission to the first medical translation task at WMT’14. We report results for EnglishFrench on the subtask of sentence translation from summaries of medical articles. Our main submission uses a combination of N CODE (n-gram-based) and M OSES (phrase-based) output and continuous-space language models used in a post-processing step for each system. Other characteristics of our submission include: the use of sampling for building M OSES’ phrase table; the implementation of the vector space model proposed by Chen et al. (2013); adaptation of the POStagger used by N CODE to the medical domain; and a report of error analysis based on the typology of Vilar et al. (2006). 1 System Overview N CODE N CODE implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Mari˜no et al., 2006; Crego and Mari˜no, 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, the translation is divided into two steps. To translate a source sentence f into a target sentence e, the source sentence is first reordered according to a set of rewriting rules so as to reproduc"
W14-3330,P10-1052,1,0.873159,"Missing"
W14-3330,N12-1047,0,0.0389371,"Missing"
W14-3330,2011.iwslt-evaluation.7,1,0.81625,"osal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, continuous models can also be easily and efficiently adapted as in (Lavergne et al., 2011). Starting from a previously trained S OUL model, only a few more training epochs are (2) where the f req(·) is the number of occurrences of the given phrase in the whole corpus, and the numerator p(¯ e|f¯) × f req(f¯) represents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model"
W14-3330,P11-2031,0,0.0126431,"ces from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum random sampling size of 1 000 examples and a maximum phrase length of 15. However, all sub-corpora but G IGA3 are used to compute the vectors for VSM features. Decoding is done with M OSES4 (Koehn et al., 2007). Experiments Tunin"
W14-3330,N12-1005,1,0.885486,"score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allows us to consider longer dependencies than the one used by conventional n-gram models (e.g. n = 10 instead of n = 4). Additionally, conti"
W14-3330,C08-1064,0,0.0173213,"sents the predicted joint count of f¯ and e¯. The other models in this system are the same as in the default configuration of M OSES. 2.3 countdev (f¯j , e¯k )wc (f¯j , e¯k ) (3) j=0 k=0 We develop an alternative approach implementing an on-the-fly estimation of the parameter of a standard phrase-based model as in (Le et al., 2012b), also adding an inverse translation model. Given an input source file, it is possible to compute only those statistics which are required to translate the phrases it contains. As in previous works on on-the-fly model estimation for SMT (CallisonBurch et al., 2005; Lopez, 2008), we first build a suffix array for the source corpus. Only a limited number of translation examples, selected by deterministic random sampling, are then used by traversing the suffix array appropriately. A coherent translation probability (Lopez, 2008) (which also takes into account examples where translation extraction failed) is then estimated. As we cannot compute exactly an inverse translation probability (because sampling is performed independently for each source phrase), we resort to the following approximation:   p(¯ e|f¯) × f req(f¯) ¯ p(f |¯ e) = min 1.0, f req(¯ e) J X K X Vector"
W14-3330,J06-4004,0,0.0612575,"Missing"
W14-3330,P03-1021,0,0.0609898,"Attempting include one language model per sub-corpora yielded a significant drop in performance. 248 3.4 Proxy Test Set System Combination As N CODE and OTF differ in many aspects and make different errors, we use system combination techniques to take advantage of their complementarity. This is done by reranking the concatenation of the 1 000-best lists of both systems. For each hypothesis within this list, we use two global features, corresponding either to the score computed by the corresponding system or 0 otherwise. We then learn reranking weights using Minimum Error Rate Training (MERT) (Och, 2003) on the development set for this combined list, using only these two features (SysComb-2). In an alternative configuration, we use the two systems without the S OUL rescoring, and add instead the five S OUL scores as features in the system combination reranking (SysComb-7). For this first edition of a Medical Translation Task, only a very small development set was made available (D EVEL in Table 1). This made both system design and tuning challenging. In fact, with such a small development set, conventional tuning methods are known to be very unstable and prone to overfitting, and it would be"
W14-3330,P02-1040,0,0.0986367,"it would be suboptimal to select a configuration based on results on the development set only.2 To circumvent this, we artificially created our own internal test set by randomly selecting 3 000 sentences out from the 30 000 sentences from PATTR -A BSTRACTS having the lowest perplexity according to 3-gram language models trained on both sides of the D EVEL set. This test set, denoted by L M T EST, is however highly biaised, especially because of the high redundancy in PATTR -A BSTRACTS, and should be used with great care when tuning or comparing systems. 3.5 Evaluation Metrics All BLEU scores (Papineni et al., 2002) are computed using cased multi-bleu with our internal tokenization. Reported results correspond to the average and standard deviation across 3 optimization runs to better account for the optimizer variance (Clark et al., 2011). Systems N CODE We use N CODE with default settings, 3gram bilingual translation models on words and 4gram bilingual translation factor models on POS, for each included corpora (see Table 1) and for the concatenation of them all. 4 4.1 OTF When using our OTF system, all indomain and out-of-domain data are concatenated, respectively. For both corpora, we use a maximum ra"
W14-3330,P06-2093,0,0.0274218,"ment data, respectively, and countdev (f¯j , e¯k ) is the joint count of phrase pairs (f¯j , e¯k ) found in the development set. The similarity score between each phrase pair’s vector and the development set vector is added into the phrase table as a VSM feature. We also replace the joint count with the marginal count of the source/target phrase to compute an alternative average representation for the development set, thus adding two VSM additional features. 2.4 S OUL Neural networks, working on top of conventional n-gram back-off language models, have been introduced in (Bengio et al., 2003; Schwenk et al., 2006) as a potential means to improve discrete language models. As for our submitted translation systems to WMT’12 and WMT’13 (Le et al., 2012b; Allauzen et al., 2013), we take advantage of the recent proposal of (Le et al., 2011). Using a specific neural network architecture, the Structured OUtput Layer (S OUL), it becomes possible to estimate n-gram models that use large vocabulary, thereby making the training of large neural network language models feasible both for target language models and translation models (Le et al., 2012a). Moreover, the peculiar parameterization of continuous models allo"
W14-3330,N04-4026,0,0.0199797,"(§2.3), and POS-tagging adaptation to the medical domain (§3.3). We also performed a small-scale error analysis of the outputs of some of our systems (§5). K X λk fk (f , e, a) (1) k=1 where K feature functions (fk ) are weighted by a set of coefficients (λk ) and a denotes the set of hidden variables corresponding to the reordering and segmentation of the source sentence. Along with the n-gram translation models and target ngram language models, 13 conventional features are combined: 4 lexicon models similar to the ones used in standard phrase-based systems; 6 lexicalized reordering models (Tillmann, 2004; Crego et al., 2011) aimed at predicting the orientation of the next translation unit; a “weak” distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. Features are estimated during the training phase. Training source sentences are first reordered so as to match 246 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246–253, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics (see Table 1). Each component wc (f¯, e¯) is a standard"
W14-3330,vilar-etal-2006-error,0,0.0370322,"Missing"
W15-3008,2005.iwslt-1.8,0,0.014132,"SVM classifier. Language models are built based on different tokens, such as word, partof-speech, and automacally generated word clusters. Final systems also include bilingual language models, part-of-speech and syntactic treebased reordering models as well as a lexicalized reordering model. For language modeling, a data selection strategy is also applied. A discriminative 92 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 92–97, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. original sentences. The lexicalized reordering (Koehn et al., 2005) encodes reordering probabilities for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the"
W15-3008,P07-2045,0,0.00889681,"2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use"
W15-3008,P06-1096,0,0.312539,"Missing"
W15-3008,D09-1022,0,0.0294609,"e source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase extraction (LPE) (Niehues et al., 2010) is applied on the training corpus, in order to get phrase pairs that match the reordered sentences. In this scheme, we use the reordered sentences to extract the phrases from, instead of the 3.3 Discriminative Word Lexicon First introduced by Mauser et al. (2009), a discriminative word lexicon (DWL) models the probability of a target word appearing in the translation 93 given the words of the source sentence. For every target word, a maximum entropy model is trained to determine whether this target word should be in the translated sentence or not using one feature per source word. 3.4 In order to facilitate more complex models like neural network translation models, we rescored the n-best lists. In our experiments we generated 300 best lists for the development and test data respectively. We used the same data to train the rescoring that we have used"
W15-3008,P14-1129,0,0.0788838,"Missing"
W15-3008,2011.iwslt-evaluation.9,1,0.889513,"ortugal, 17-18 September 2015. 2015 Association for Computational Linguistics. original sentences. The lexicalized reordering (Koehn et al., 2005) encodes reordering probabilities for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German di"
W15-3008,W11-2123,0,0.0358667,"ies for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different"
W15-3008,W13-0805,1,0.854708,"ls Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase"
W15-3008,P10-2041,0,0.0121769,"build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear combination. For the German→English system, the data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the English side of all data, including the filtered crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reo"
W15-3008,P02-1040,0,0.0947921,"94 described in Section 3.4 for the log-linear combination of features. By doing so, we improve the translation performance by another 0.8 BLEU points on the test set. This system was submitted to WMT 2015 and used for the translation of the official test set. sum of all free energies in the sentence is used as an additional feature for rescoring. 4 Results In this section, we present a summary of our experiments in the evaluation campaign. Individual components that lead to improvements in the translation performance are described step by step. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). 4.1 System Baseline + Non-word LMs + Tree + Lex. Reorderings + Source–context DWL + ListNet rescoring English-German Table 1 shows the results of our system for English→German translation task. The baseline system consists of a phrase table derived from DWA, the word-based language models built from different parts of the corpus and POS-based long-range reordering rules. Reordering rules, however, are extracted from the POStagged EPPS and NC only, and encoded as word lattices. The parallel data used to build the word alignments and the PT are EPPS, NC and the filtered Crawl data. Similarly,"
W15-3008,W09-0435,1,0.79966,"ted the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sent"
W15-3008,W08-0303,1,0.743399,"oth translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the En"
W15-3008,W08-1006,0,0.0296727,"They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase extraction (LPE) (Niehues et al., 2010) is applied on the training corpus, in order to get phrase pairs that match the r"
W15-3008,2012.iwslt-papers.3,1,0.896933,"l as source DWL. This model predicts the target word for a given source word as described in detail in (Herrmann, 2015). In a first step, we identify the 20 most frequent translations of each word. Then we build a multiclass classifier to predict the correct translation. For the classifier, we used a binary maximumentropy classifier1 trained using the one-againstall approach. 3.5 RBM Translation Model In rescoring, we used an restricted Boltzmann machine (RBM)-based translation model inspired by the work of Devlin et al. (2014). The model is based on the RBM-based language model introduced in Niehues and Waibel (2012). The RBM models the joint probability of eight target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous five source words and its following five source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel corpus and train the model using unigram sampling as described in"
W15-3008,2007.tmi-papers.21,0,0.0311498,"g the filtered crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to t"
W15-3008,W13-2264,1,0.785134,"e they are computed as the logarithm of relatively small probabilities. Therefore, we rescale all scores observed on the development data to the range of [−1, 1] prior to rescoring. Two simplifications of this model are used to improve the translation quality while maintaining the time efficiency as shown in Mediani et al. (2011). First, the score for every phrase pair is calculated before translation. Then we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, the DWL is further extended with n-gram source context features proposed by Niehues and Waibel (2013). In this paper, this model will be referred to as source-context DWL. The source sentence is represented as a bag-of-ngrams, instead of a bag-of-words. By doing so it is possible to include information about source word order in the model. We used one feature per ngram up to the order of three and applied count filtering for bigrams and trigrams. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL. This model predicts the target word for a given source word as described in detail in (Herrmann, 2015). In a first step, we"
W15-3008,C08-1098,0,0.0213769,"using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore alleviate the sparsity problem for surface words. In order to build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear com"
W15-3008,W11-2124,1,0.855568,"ations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore allevi"
W15-3008,W05-0836,1,0.776161,"bility for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words bey"
W15-3008,2014.amta-researchers.17,1,0.814055,". The RBM models the joint probability of eight target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous five source words and its following five source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel corpus and train the model using unigram sampling as described in Niehues et al. (2014). In rescoring, we then calculate the free energy of the RBM given the 8-gram and its source set as input. The As features for the classifier, we used the previous and following three words. Each word is represented by a continuous vector of 100 dimensions as described in (Mikolov et al., 2013). Using the predictions, we calculated four additional features. The first two features are the absolute and relative number of words, where the translation predicted by the classifier and the translation in the hypothesis is the same. The third feature is the sum of the word to word translation probabil"
W15-3008,W15-3030,1,0.840612,"rget word, a maximum entropy model is trained to determine whether this target word should be in the translated sentence or not using one feature per source word. 3.4 In order to facilitate more complex models like neural network translation models, we rescored the n-best lists. In our experiments we generated 300 best lists for the development and test data respectively. We used the same data to train the rescoring that we have used for optimizing the translation system. We trained the weights for the log-linear combination used during rescoring using the ListNet algorithm (Cao et al., 2007; Niehues et al., 2015). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then we use the cross entropy between both distributions as the loss function for our training. Using this loss function, we can compute the gradient and use stochastic gradient descent. We used batch updates with ten samples and tuned the learning rate on the development data. The range of the scores of the different models may greatly differ and many o"
W15-3008,J03-1002,0,0.012327,"l position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned so"
W15-3008,E99-1010,0,0.0262497,"e models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore alleviate the sparsity problem for surface words. In order to build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear combination. For the German→English system, the data selection language model is trained on data automatically selected using cross-entropy differ"
W15-3008,P03-1021,0,0.0103561,". The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between so"
W15-3008,W10-1719,1,\N,Missing
W15-3008,E03-1076,0,\N,Missing
W15-3008,P03-1054,0,\N,Missing
W15-3012,W13-0805,1,0.885747,"SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word p"
W15-3012,P03-1054,0,0.0197594,"he actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et"
W15-3012,N03-1017,0,0.0324773,". 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source"
W15-3012,2005.iwslt-1.8,0,0.122737,"g, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to"
W15-3012,P07-2045,0,0.0106185,"rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in ord"
W15-3012,J04-2004,0,0.0881722,"s Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of"
W15-3012,N12-1005,1,0.956768,"oint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translated candidates. Second, an n-gram translation model from LIMSI, named SOUL (Structured OUtput Layer), helps to rescore the k-best list by utilizing features extracted from translated tuples. In this year participation, we also use a version of the neural network translation models (Le et al., 2012) trained using NCE algorithm (Gutmann and Hyv¨arinen, 2010) as counterpart to SOUL models. A ListNet2.1 Data and Preprocessing The parallel data mainly used are the corpora extracted from Europarl Parliament (EPPS), News Commentary (NC) and the common part of webcrawled data (Common Crawl). The monolingual data are the monolingual part of those corpora. A preprocessing step is applied to the raw data before the actual training. It includes removing excessively long and length-mismatched sentences pairs. Special symbols and nummeric data are normalized, and smartcasing is applied. Sentence pair"
W15-3012,E99-1010,0,0.0623657,"onolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words. All of the language models are trained using the SRILM toolkit (Stolcke, 2002); The word-based 121 org : .... à recevoir le prix nobel de la paix s : .... s8: à s9: recevoir s10: le s11: nobel de la paix s12: prix .... t : .... t8: to t9: receive t10: the t11: nobel peace t12: prize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just abov"
W15-3012,P06-1096,0,0.312547,"Missing"
W15-3012,2007.tmi-papers.21,0,0.269383,"which contain textual elements in different 120 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and Germa"
W15-3012,J06-4004,0,0.194243,"Missing"
W15-3012,D07-1045,0,0.386972,"Missing"
W15-3012,D09-1022,0,0.247402,"Missing"
W15-3012,W05-0836,1,0.875755,"(Vogel, 2003) which finds the best combinations of features in a log-linear framework. The features consist of translation scores, distortion-based and lexicalized reordering scores as well as conventional and non-word language models. In addition, several reordering rules, including short-range, long-range and tree-based reorderings, are applied before decoding step as they are encoded as word lattices. The decoder then generates a list of the best candidates from the lattices. To optimize the factors of individual features on a development dataset, we use minimum error rate training (MERT) (Venugopal et al., 2005). We are going to describe those components in detail as follows. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points. 1 KIT Phrase-based Translation System Introduction In this paper, we present the English→German joint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translat"
W15-3012,W09-0435,1,0.878365,"eedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ to"
W15-3012,W08-0303,1,0.817804,"C. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy class"
W15-3012,W13-2264,1,0.876138,"ose phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to train those classifiers. 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et"
W15-3012,W11-2124,1,0.841127,"sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a Besides word-based n-gram language models trained on all preprocessed monolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words."
W15-3012,W15-3030,1,0.645912,"(Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs. Therefore, NCE models deliver a positive score, by applying the exponential function to the output layer activities, 122 4 Rescoring 5 After generating translation probabilities using the neural network translation models, we need to combine them with the baseline scores of the phrase-based system in order to select better translations from the k-best lists. As it is done in the baseline decoder, we used a log-linear combination of all features. We trained the model using the ListNet algorithm (Niehues et al., 2015; Cao et al., 2007). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. Therefore, a sentence-based translation quality metric is necessary. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then the model was trained by minimizing the cross entropy between both distributions on the development data. Using this loss function, we can compute the gradient with respect to the weight ωk as follows: System Baseline + ListNet rescoring + NCE + SOUL + NCE + SOUL"
W15-3012,J03-1002,0,0.0188001,"are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice o"
W15-3012,W11-2123,0,\N,Missing
W15-3012,P14-1129,0,\N,Missing
W15-3030,N12-1047,0,0.0900077,"es not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet presented in (Cao et al., 2007). Other methods based on more complex models have also been presented, for example (Liu et al., 2013), which uses an additive neural network instead of linear models. 3 exp(sj ) Ps (j) = Pn , k=1 exp(sk ) where sj is a score assigned to the j-th entry of ("
W15-3030,D08-1024,0,0.0298173,"fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community ("
W15-3030,P12-1031,0,0.0157894,"over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet"
W15-3030,D11-1125,0,0.115139,"number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motiva"
W15-3030,P07-2045,0,0.00929575,"n defines four different scores to evaluate a hypothesis. In such architecture, the size of the output vocabulary is a bottleneck when normalized distributions are needed. For efficient computation, these models rely on a tree-structured output layer called SOUL (Le et al., 2011). An effective alternative, which however only delivers unnormalized scores, is to train the network using the Noise 5.2 Other optimization techniques For comparison, experimental results include performance obtained with the most widely used algorithms: MERT, KB-MIRA (Cherry and Foster, 2012) as implemented in Moses (Koehn et al., 2007), along with the PRO algorithm. For the latter, we used the MegaM1 version (Daum´e III, 2004). All the results correspond to three random restarts and the weights are chosen according to the best performance on the development data. 5.3 WMT – English to German The results for the English to German news translation task are summarized in Table 1. The translations generated by the phrase-based decoder reach a BLEU score of 20.19. We compared the presented approach with MERT, KB-MIRA and PRO. KB-MIRA and MERT improve the performance by at most 0.3 BLEU points. In contrast, the PRO technique and t"
W15-3030,2014.iwslt-evaluation.1,1,0.736667,"ques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluation campaigns and when evaluated using the BLEU score they were all ranking within the top 3. Before discussing the results, we summarize the translation systems used for experiments along with the additionnal features that rely on continuous space translation models. 5.1 Systems The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system"
W15-3030,P06-1096,0,0.397769,"Missing"
W15-3030,P13-1078,0,0.0290246,"Missing"
W15-3030,2012.iwslt-papers.3,1,0.83718,"e of possible scores. 250 Contrastive Estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012) denoted by NCE in the rest of the paper. In this work, we used these both solutions as well as their combination. For the German to English translation task, we added a source side discriminative word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015"
W15-3030,P03-1021,0,0.0402058,"tion for Computational Linguistics. The aim is then to find a function fω that assigns a (i) score to every feature vector xj . This function is fully defined by its set of parameters ω. Using the (i) (i) vector of scores z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and th"
W15-3030,P02-1040,0,0.0926887,"entional scores calculated during decoding, as well as additional models such as neural network translation models. 4.1 Although both methods could be applied together, we did only use one of them, since both methods have similar effects. If not stated differently, we use the feature normalization method in our experiments. 4.2 To estimate the weights, we need to define a probability distribution Py associated to the reference ranking y following Euqation 1. In this work, we propose a distribution based on machine translation evaluation metrics. The most widely used evaluation metric is BLEU (Papineni et al., 2002), which only produces a score at the corpus level. As proposed by Hopkins and May (2011), we will use a smoothed sentence-wise BLEU score to generate the reference ranking. In this work, we use the BLEU+1 score introduced by Liang et al. (2006). When (i) using sj = BLEU(xj ) in Equation 1, whe get the follwing defintion of the probability distribution Py : Score normalization (i) The scores (xj )k are, for example, language model log-probabilities. Since the language model probabilities are calculated as the product of several n-gram probabilities, these values are typically very small. Theref"
W15-3030,W11-2119,0,0.0487127,"Missing"
W15-3030,2014.iwslt-evaluation.17,1,0.77366,"tive word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluati"
W15-3030,D07-1080,0,0.0294196,"z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the mac"
W15-3030,2010.iwslt-evaluation.11,1,\N,Missing
W15-3030,N12-1005,1,\N,Missing
W15-3030,W15-3008,1,\N,Missing
W15-4917,2012.eamt-1.60,0,0.012644,"d used for translation. This allows us to retain our generalization won by using word clusters to estimate phrase probabilities, and still use all models trained on the surExperiments Since we expect stemming to have a larger impact in cases where training data is scarce, we evaluated the three presented strategies on two different scenarios: a low-resource condition and a state-ofthe-art large-scale system. In both scenarios we stemmed German adjectives and translated from German to English. In our low-resource condition, we trained an SMT system using only training data from the TED corpus (Cettolo et al., 2012). TED translations are currently available for 107 languages2 and are being continuously expanded. Therefore, there is a high chance that a small parallel corpus of translated TED talks will be available in the chosen language. In the second scenario, we used a large-scale state-of-the-art German→English translation system. This system was trained on signiﬁcantly more data than available in the low-resource condition and incorporates several additional models. 5.1 System Description The low-resource system was trained only on the TED corpus provided by the IWSLT 2014 machine translation campai"
W15-4917,P08-1115,0,0.0298227,"ut-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human trans"
W15-4917,W08-0509,0,0.028417,"Missing"
W15-4917,W10-1710,0,0.0162401,"nd Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Welle"
W15-4917,W11-2123,0,0.0843226,"com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced"
W15-4917,W13-0805,1,0.866971,"is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is"
W15-4917,D07-1091,0,0.0514795,"previously unseen morphological variants of a word, thus leading to a better generalization of our models. To fully maximize the potential of our SMT system, we looked at three different integration strategies. We evaluated hard decision stemming, where all adjectives are replaced by their stem, as well as soft integration strategies, where we consider the words and their stemmed form as translation alternatives. 2 Related Work The speciﬁc challenges arising from the translation of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen wor"
W15-4917,E03-1076,0,0.239742,"plitting before tagging and stemming. We only stemmed words tagged as attributive adjectives, since only they are inﬂected in German. Predicative adjectives are not inﬂected and therefore were left untouched. Since we want to retain the degree of comparison, we used the ﬁnegrained tags of the RFTagger to decide when and how to stem. Adjectives tagged as comparative or superlative were stemmed through the use of ﬁxed rules. For all others, we used the lemma output by the TreeTagger, since it is the same as the stem and was already available in our system. Finally, our usual compound splitting (Koehn and Knight, 2003) was trained and performed on the stemmed corpus. 4 Integration After clustering the words into groups that can be translated in the same or at least in a similar way, there are different possibilities to use them in the translation system. A naive strategy is to replace each word by its cluster representative, called hard decision stemming. However, this carries the risk of discarding vital information. Therefore we investigated techniques to integrate both, the surface forms as well as the word stems, into the translation system. In the combined input, we add the stemmed adjectives as transl"
W15-4917,2005.iwslt-1.8,0,0.0605771,"additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IW"
W15-4917,P07-2045,0,0.00534336,"lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shufﬂe and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale syst"
W15-4917,P11-1140,0,0.0200421,"ibutions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and the"
W15-4917,2011.iwslt-evaluation.9,1,0.901491,"monolingual training data we used the target side of the TED corpus. The large-scale system was trained on the European Parliament Proceedings, News Commentary, TED and Common Crawl corpora provided for the IWSLT 2014 machine translation campaign (Cettolo et al., 2014), encompassing 4.69M lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shufﬂe and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed"
W15-4917,P10-2041,0,0.0286579,"large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED"
W15-4917,W09-0435,1,0.865559,"t-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond p"
W15-4917,2011.iwslt-papers.6,1,0.861961,"phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an al"
W15-4917,2012.amta-papers.19,1,0.791385,"ion, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting r"
W15-4917,W11-2124,1,0.93561,"idden combination strategy, stemming can easily be implemented into current state-of-the-art SMT systems without the need to change any of the advanced models beyond the phrase table. This makes our approach highly versatile and easy to implement for any number of system architectures and languages. 5 Figure 1: Workﬂow for unstemming the PT. 4.3 Hidden Combination While we are able to modify our phrase table to use both surface forms and stems in the last strategy, other models in our log-linear system suffer from the different types of source input. For example, the bilingual language model (Niehues et al., 2011) is based on tokens of target words and their aligned source words. In training, we can use either the stemmed corpus or the original one, but during decoding a mixture of stems and surface forms occurs. For the unknown word forms the scores will not be accurate and the performance of our model will suffer. Similar problems occur when using other translation models such as neural network based translation models. We therefore developed a novel strategy to integrate the word stems into the translation system. Instead of stemming the input to ﬁt the stemmed phrase table, we modiﬁed the stemmed p"
W15-4917,E99-1010,0,0.128458,"le and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT te"
W15-4917,2007.tmi-papers.21,0,0.467512,"mbining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context du"
W15-4917,C08-1098,0,0.163073,"m that does not exist in proper German. However, were we to apply the same stemming to the comparative case, we would lose the degree of comparison and still generate a valid German sentence (sch¨on wird es nicht [won’t be pretty]) with a different meaning than our original sentence. In order to differentiate between cases in which stemming is desirable and where we would lose information, a detailed morphological analysis of the source text prior to stemming is vital. 3.2 Implementation We used readily available part-of-speech (POS) taggers, namely the TreeTagger (Schmid, 1994) and RFTagger (Schmid and Laws, 2008), for morphological analysis and stemming. In order to achieve accurate results, we performed standard machine translation preprocessing on our corpora before tagging. We discarded exceedingly long sentences and sentence pairs with a large length difference from the training data. Special dates, numbers and symbols were normalized and we smart-cased the ﬁrst letter of every sentence. Typically preprocessing for German also includes splitting up compounds into their separate parts. However, this would confuse the POS taggers, which have been trained on German text with proper compounds. Further"
W15-4917,P06-1122,0,0.0149241,"where we consider the words and their stemmed form as translation alternatives. 2 Related Work The speciﬁc challenges arising from the translation of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the"
W15-4917,W05-0836,1,0.904528,"lt on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IWSLT test 2011 as development data and IWSLT test 2012 as test data. All results are reported as case-sensitive BLEU scores calculated with one reference tran"
W15-4917,P13-1058,0,0.0157141,"n of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morphologically reduced system due to better generalization ability. Their analysis showed the Russian system often produces an incorrect verb tense, whic"
W15-4917,W12-3157,0,0.0169282,"words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morph"
W15-4917,E06-1006,0,0.036887,"tion of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2"
W15-4917,W13-2213,0,\N,Missing
W16-2208,2014.iwslt-papers.10,1,0.871256,"Missing"
W16-2208,W16-2314,1,0.843692,"Missing"
W16-2208,W09-0435,1,0.856202,"anguage models based on each of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and"
W16-2208,2015.iwslt-papers.3,1,0.744609,"ures , we now can predict the target word given the previous target word and the aligned source word. In the example in Figure 2, we would insert (completed,VVD,87,ein,ART) to predict (a,DT,37). In this case the number of input factors and output factors are no longer the same. In the input, we have D + Ds input factors, while we have only D factors on the output of the network. 5 System Description EN-RO 2 1 2 0 22-23 EN-DE 3 0 1 1 20 DE-EN 3 0 2 1 22 In addition, we used discriminative word lexica (Niehues and Waibel, 2013) during decoding and source discriminative word lexica in rescoring (Herrman et al., 2015). A full system description can be found in (Ha et al., 2016). The German to English baseline system uses 20 features and the English to German systems uses 22 features. The English-Romanian system was optimized on the first part of news-dev2016 and the rescoring was optimized on this set and a subset of 2,000 Experiments We evaluated the factored RNNLM on three different language pairs of the WMT 2016 News Translation Task. In each language pair, we created an n-best list using our phrase-based MT system and used the factored RNNLM as an additional feature in rescoring. It is worth noting tha"
W16-2208,2012.iwslt-papers.3,1,0.916551,"gically rich languages which often have a large vocabulary size. Language models based on these factors are able to consider longer context and therefore improve the modelling of the overall structure. Furthermore, the POS information can be used to improve the modelling of word agreement, which is often a difficult task when handling morphologically rich languages. Until now, word factors have been used relatively limited in neural network models. Automatic word classes have been used to structure the output layer (Le et al., 2011) and as input in feed forward neural network language models (Niehues and Waibel, 2012). In this work, we propose a multi-factor recurrent neural network (RNN)-based language model that is able to facilitate all available information about the word in the input as well as in the output. We evaluated the technique using the surface form, POS-tag and automatic word clusters using different cluster sizes. Using this model, it is also possible to integrate source side information into the model. By using the model as a bilingual model, the probability of the translation can be modelled and not only the one of target sentence. As for the target side, we use a factored representation"
W16-2208,W13-0805,1,0.862292,"ach of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model a"
W16-2208,W13-2264,1,0.835485,"(fi+1,1 , . . . , fi+1,D , fa(i+1),1 , . . . , fa(i+1),D ) s wordLM POSLM clusterLM BiLM #features , we now can predict the target word given the previous target word and the aligned source word. In the example in Figure 2, we would insert (completed,VVD,87,ein,ART) to predict (a,DT,37). In this case the number of input factors and output factors are no longer the same. In the input, we have D + Ds input factors, while we have only D factors on the output of the network. 5 System Description EN-RO 2 1 2 0 22-23 EN-DE 3 0 1 1 20 DE-EN 3 0 2 1 22 In addition, we used discriminative word lexica (Niehues and Waibel, 2013) during decoding and source discriminative word lexica in rescoring (Herrman et al., 2015). A full system description can be found in (Ha et al., 2016). The German to English baseline system uses 20 features and the English to German systems uses 22 features. The English-Romanian system was optimized on the first part of news-dev2016 and the rescoring was optimized on this set and a subset of 2,000 Experiments We evaluated the factored RNNLM on three different language pairs of the WMT 2016 News Translation Task. In each language pair, we created an n-best list using our phrase-based MT system"
W16-2208,W11-2124,1,0.842249,"n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probab"
W16-2208,ion-etal-2012-rombac,0,0.030615,"ering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probability. When predicting the target word wi+1 with its factors fi+1,1 , . . . , fi+1,D , the input to the RNN is the previous target word wi = fi,1"
W16-2208,E99-1010,0,0.216665,"ed using stochastic gradient descent. The weights were updated using mini-batches with a batch size of 128. We used a maximum epoch size of 1 million examples and selected the model with the lowest perplexity on the development data. 4 Factored Language Model When using factored representation of words, words are no longer represented as indices in the neural network. Instead, they are represented a tuples of indices w = (f1 , . . . , fD ), where D is the number of different factors used to describe the word. These factors can be the word itself, as well as the POS, automatic learned classes (Och, 1999) or other information about the word. Furthermore, we can use different types of factors for the input and the output of the neural network. 1 75 http://torch.ch/ 4.2 Figure 1: Factored RNN Layout 4.1 Output Representation In addition to use different factors in the input of the neural network, we can also use different factors on the output. In phrase-based machine translation, n-gram language models based on POStags have been shown to be very successful for morphologically rich languages. Porting this idea to neural network language models, we can not only train a model to predict the origin"
W16-2208,D07-1091,0,0.0588701,"ge not only the target morphological information but also word factors from both source and target sides in our models. Furthermore, we could use as many types of word factors as we can provide. Thus, we are able to make the most of the information encoded in those factors for more accurate prediction. section, we will describe the experiments on the WMT 2016 data. Finally, we will end the paper with a conclusion of the work. 2 Related Work Additional information about words, encoded as word factors, e.g. the lemma of word, POS tags, etc., is employed in state-of-the-art phrasebased systems. (Koehn and Hoang, 2007) decomposes the translation of factored representations to smaller mapping steps, which are modelled by translation probabilities from input factor to output factor or by generating probabilities of additional output factors from existing output factors. Then those pre-computed probabilities are jointly combined in the decoding process as a standard translation feature scores. In addition, language models using these word factors have shown to be very helpful to improve the translation quality. In particular, the aligned-words, POS or word classes are used in the framework of modern language m"
W16-2208,P03-1021,0,0.0587758,"nces from the SETimes corpus. This part of the corpus was of course excluded for training the model. The system was tested on the second half of news-dev2016. The English-German and German-English systems were optimized on news-test2014 and also the re-scoring was optimized on this data. We tested the system on news-test2015. For English to Romanian and English to German we used an n-best List of 300 entries and for German to English we used an n-best list with 3,000 entries. For decoding, for all language directions, the weights of the system were optimized using minimum error rate training (Och, 2003). The weights in the rescoring were optimized using the ListNet algorithm (Cao et al., 2007) as described in (Niehues et al., 2015). The RNN-based language models for English to Romanian and German to English were trained on the target side of the parallel training data. For English to German, we trained the model and the Europarl corpus and the News commentary corpus. 5.2 Table 2: English - Romanian Single Score Input Word All factors All factors All factors All factors All factors Prediction Word Word POS 100 Cl. 1,000 Cl. All factors Single 27.88 28.46 28.48 28.23 28.49 28.54 If we predict"
W16-2208,P02-1040,0,0.0974026,"valuating the model as the only knowledge source, we also performed experiments using the model in combination with the other models. We evaluated the baseline and the best model in three different configuration in Table 3 using only the joint probability. The three baseline configuration differ in the models used during decoding. Thereby, we are able to generate different n-best lists and test the models on different conditions. English - Romanian In the first experiment on the English to Romanian task, we only used the scores of the RNN language models. The baseline system has a BLEU score (Papineni et al., 2002) of 29.67. Using only the language model instead of the 22 features, of course, leads to a lower performance, but we can see clear difference between the different language models. All systems use a word vocabulary of 5K words and we used four different factors. We used the word surface form, the POS tags and word clusters using 100 and 1,000 classes. The baseline model using words as input and words as output reaches a BLEU score of 27.88. If we instead represent the input words by factors, we select entries from the n-best list that generates a BLEU score of 28.46. As done with the n-gram la"
W16-2208,N12-1005,0,0.0205512,"013). Recently, neural network language models have been considered to perform better than standard n-gram language models (Schwenk, 2007; Le et al., 2011). Especially the neural language models constructed in recurrent architectures have shown a great performance by allowing them to take a longer context into account (Mikolov et al., 2010; Sundermeyer et al., 2013). In a different direction, there has been a great deal of research on bringing not only target words but also source words into the prediction process, instead of predicting the next target word based on the previous target words (Le et al., 2012; Devlin et al., 2014; Ha et al., 2014). However, to the best of our knowledge, word factors have been exploited in a relatively limited scope of neural network research. (Le et al., 2011; Le et al., 2012) use word classes to reduce the output layer’s complexity of such networks, both in language and translation models. In the work of (Niehues and Waibel, 2012), their Restricted Boltzmann Machines language models also encode word classes as an additional input feature in predicting the next target word. (Tran et al., 2014) use two separate feed forward networks to predict the target word and i"
W16-2208,2007.tmi-papers.21,0,0.0476998,"e system by n-gram-based language models based on each of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German"
W16-2208,C08-1098,0,0.231817,"Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probability. When predicting the target word wi+1 with its factors fi+1,1 , . . . , fi+1,D , the input to the RNN is the previous target word wi = fi,1 , . . . , fi,D . Using the alignment, we can find the source"
W16-2208,D14-1175,0,0.0264713,"ead of predicting the next target word based on the previous target words (Le et al., 2012; Devlin et al., 2014; Ha et al., 2014). However, to the best of our knowledge, word factors have been exploited in a relatively limited scope of neural network research. (Le et al., 2011; Le et al., 2012) use word classes to reduce the output layer’s complexity of such networks, both in language and translation models. In the work of (Niehues and Waibel, 2012), their Restricted Boltzmann Machines language models also encode word classes as an additional input feature in predicting the next target word. (Tran et al., 2014) use two separate feed forward networks to predict the target word and its corresponding suffixes with the source words and target stem as input features. Our work exhibits several essential differences 3 Recurrent Neural Network-based Language Models In contrast to feed forward neural network-based language models, recurrent neural network-based language models are able to store arbitrary long word sequences. Thereby, they are able to directly model P (w|h) and no approximations by limiting the history size are necessary. Recently, several authors showed that RNN-based language models could p"
W16-2208,D13-1138,0,0.0166919,"f factored representations to smaller mapping steps, which are modelled by translation probabilities from input factor to output factor or by generating probabilities of additional output factors from existing output factors. Then those pre-computed probabilities are jointly combined in the decoding process as a standard translation feature scores. In addition, language models using these word factors have shown to be very helpful to improve the translation quality. In particular, the aligned-words, POS or word classes are used in the framework of modern language models (Mediani et al., 2011; Wuebker et al., 2013). Recently, neural network language models have been considered to perform better than standard n-gram language models (Schwenk, 2007; Le et al., 2011). Especially the neural language models constructed in recurrent architectures have shown a great performance by allowing them to take a longer context into account (Mikolov et al., 2010; Sundermeyer et al., 2013). In a different direction, there has been a great deal of research on bringing not only target words but also source words into the prediction process, instead of predicting the next target word based on the previous target words (Le e"
W16-2208,N03-2002,0,\N,Missing
W16-2208,W15-3030,1,\N,Missing
W16-2208,P14-1129,0,\N,Missing
W16-2314,W16-2304,1,0.809831,"ed in this evaluation will be described. In this paper, we present the KIT translation systems as well as the KIT-LIMSI systems for the ACL 2016 First Conference on Machine Translation. We participated in the shared task Machine Translation of News and submitted translation systems for three different directions: English→German, German→English and English→Romanian. 2.1 For training our systems, we used all the data provided by the organizers. In all of our translation systems, the preprocessing step was conducted prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-ba"
W16-2314,J04-2004,0,0.034554,"ithub.com/lmthang/nmt.matlab Initialization is an important issue when optimizing neural networks. For CTMs, a solution consists in pre-training monolingual n-gram models. Their parameters are then used to initialize bilingual models. 2007) as a potential mean to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014). As in previous submissions, we investigated the integration of n-gram CTMs. Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), an n-gram translation model is constructed based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand. A sentence pair is decomposed into a sequence of bilingual units called tuples defining a joint segmentation. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a by-product of source reordering. During the inference step, the SMT dec"
W16-2314,2000.eamt-1.5,0,0.190582,"Missing"
W16-2314,P14-1129,0,0.101445,"Missing"
W16-2314,N12-1005,1,0.872007,"ides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we evaluated a combination of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based wor"
W16-2314,W15-3012,1,0.713624,"Missing"
W16-2314,P06-1096,0,0.260981,"Missing"
W16-2314,W11-2123,0,0.0251631,"Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation system and investigated several models to rescore the system. We used neural network language and translation models. Using these models, we could improve the translation performance in all language pairs we participated. 1 Introduction Following the research we have been conducted over previous years, in this paper, we describe our phase-based translation systems submitted to the First Conference on Machine Translation with the highlights on our new models. In this evaluation, we mainly focused on using neural models in rescoring of a phrase-bas"
W16-2314,D15-1166,0,0.031325,"n of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based word fac304 Figure 2: The recurrent encoder-decoder architecture for MT proposed by (Cho et al., 2014) Figure 1: Factored RNN Layout of attention mechanism allow us to train the networks to be capable of remembering longer contexts and putting decent word alignments between two sentences (Bahdanau et al., 2015; Luong et al., 2015). Instead of using the architecture in an end-toend fashion, which often called Neural MT (Bahdanau et al., 2015), in order to leverage other translation models that the phrase-based system produces, we opted to use it in our rescoring scheme (see 3.4). We adapted the Neural MT framework1 from (Luong et al., 2015) to be able to compute the conditional probability p(f, ei ) in which f is the source sentence and ei is the ith translation candidate of f produced by our phrase-based decoder. Due to the limited time, this recurrent encoderdecoder-based (ReEnDe) feature was only employed in the dire"
W16-2314,W13-0805,1,0.83729,"nslation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored wor"
W16-2314,J06-4004,0,0.0970116,"Missing"
W16-2314,2015.iwslt-papers.3,1,0.710126,"2009), is a lexical translation model which calculates the probability of a target word given the words of the source sentence. (Niehues and Waibel, 2013) proposed an extension of DWL where they use n consecutive source words as one feature, thus they could incorporate better the order information of the source sentences into classification. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL (SDWL). This model predicts the target word for a given source word using numbers of context features as described in details in (Herrmann et al., 2015). To deal with the differences in word order between source and target languages, our systems employed various reordering strategies, which are described in the next section. 2.2 3 N -best list rescoring In order to easily integrate more complex models, we used n-best list rescoring in our submission. We evaluated a neural network language model using a factored representation of the words. Using this framework, we were also able to easily extend the model to a bilingual model. Furthermore, we investigated the use of an encoder-decoder model in rescoring. Finally, in cooperation with LIMSI, we"
W16-2314,D09-1022,0,0.0653791,"Missing"
W16-2314,D07-1091,0,0.0617416,"to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we evaluated a combination of both. We used RNN-based language models that use a factored representation. We hoped to improve the modeling of rare words by richer word representations. In the experiments we used up to four different word factors: the word surface form, the POS tags as well as two cluster based word fac304 Figure 2: The recurrent encoder-decoder architecture for MT proposed by (Cho et al., 2014) Figure 1: Factored RNN L"
W16-2314,P03-1054,0,0.0137533,"odels, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). In this evaluation, we eval"
W16-2314,2011.iwslt-evaluation.9,1,0.890671,"prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from"
W16-2314,E03-1076,0,0.154227,"erent directions: English→German, German→English and English→Romanian. 2.1 For training our systems, we used all the data provided by the organizers. In all of our translation systems, the preprocessing step was conducted prior to training. For English→Romanian, we used the preprocessing described in (Allauzen et al., 2016). For the systems involving German and English, it includes removing very long sentences and the sentence pairs which are length-mismatched, normalizing special symbols and smart-casing the first word of each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the"
W16-2314,W15-4917,1,0.842151,"Missing"
W16-2314,W09-0435,1,0.830188,"uous space translation models in rescoring. We used the ListNet approach as described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of p"
W16-2314,W13-2264,1,0.886114,"nslation systems. They included bilingual LMs, cluster LMs and the LMs based on POS sequences. For cluster and POS-based LMs, we used an n-gram size of nine tokens. During decoding, these language models were used as additional models in the log-linear combination. A family of lexical translation models, which we called discriminative word lexicon (DWL), were also utilized in our translation systems. A discriminative word lexicon, first introduced by (Mauser et al., 2009), is a lexical translation model which calculates the probability of a target word given the words of the source sentence. (Niehues and Waibel, 2013) proposed an extension of DWL where they use n consecutive source words as one feature, thus they could incorporate better the order information of the source sentences into classification. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL (SDWL). This model predicts the target word for a given source word using numbers of context features as described in details in (Herrmann et al., 2015). To deal with the differences in word order between source and target languages, our systems employed various reordering strategies"
W16-2314,W05-0836,0,0.102374,"each sentence. In the direction of German→English, compound splitting (Koehn and Knight, 2003) was applied on the German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation sys"
W16-2314,J03-1002,0,0.0288076,"he German side of the corpus. To improve the quality of the Common Crawl corpus being used in training, we filtered out noisy sentence pairs using an SVM classifier as described in (Mediani et al., 2011). All of our translation systems are basically phrase-based. An in-house phrase-based decoder (Vogel, 2003) was used to generate all translation candidates from the word lattice and then the weights for the models were optimized following the Minimum Error Rate Training (MERT) method (Venugopal et al., 2005). The word alignments were produced from the parallel corpora using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments were combined using the growdiag-final-and heuristic to form the phrase table. It was done by running the phrase extraction scripts from Moses toolkit (Koehn et al., 2007). Unless stated otherwise, we used 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). In the decoding phase, the LMs were scored by KenLM toolkit (Heafield, 2011). In We used a phrase-based machine translation system and investigated several models to rescore the system. We used neural network language and translation"
W16-2314,P02-1040,0,0.101538,"any of these values are negative numbers with high absolute value since they are computed as the logarithm of relatively small probabilities. Therefore, we rescaled all scores observed on the development data to the range of [−1, 1] prior to rescoring. 306 4 Results recurrent encoder-decoder scores. It was submitted as the joint KIT-LIMSI submission system. In this section, we present a summary of our experiments in the evaluation campaign. Individual components that lead to improvements in the translation performance are described step by step. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). In the rescoring scheme of our systems, the BLEU scores on the development set are normally smaller than those in the decoding phase because they are tuned by different optimization algorithms (ListNet and MERT). The rescoring configurations are mentioned in the tables in italic texts. The test scores from which we choose to be the submitted systems are mentioned in the tables in bold numbers. 4.1 System Baseline + DWL + Lex. Reorderings + ReEnDe + SDWL + ReEnDe + Factored + ReEnDe + CTMs Dev 21.81 22.44 20.76 20.79 20.78 Test 22.91 23.34 24.08 24.21 24.24 Table 1: Experiments for English→Ge"
W16-2314,W08-1006,0,0.0257277,"r to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improvements (Le et al., 2012; Ha et al., 2015). In addition, phrasebased machine translation can profit from factored word representations (Hoang, 2007). Using POS-tags or automatic word classes often helps to model long-range dependencies (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). I"
W16-2314,2007.tmi-papers.21,0,0.529618,"oring. Finally, in cooperation with LIMSI, we used the continuous space translation models in rescoring. We used the ListNet approach as described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models"
W16-2314,C08-1098,0,0.0960257,"described in Section 3.4 to estimate the weights of different models in our systems. Reordering Models In all translation directions, the reordering models based on POS tags were applied to change the word positions of the source sentence according to the target word order. In order to train such reordering models, probabilistic rules were extracted automatically from the POS-tagged training corpus and the alignments. The rules cover short-range reorderings (Rottmann and Vogel, 2007) as well as long-range reorderings (Niehues and Kolss, 2009). The POS tags were generated using the TreeTagger (Schmid and Laws, 2008). Besides the POS-based reordering models, a tree-based reordering model, as described in (Herrmann et al., 2013), was also applied to better address the differences in sentence structure between German and English in our systems. We used the Stanford Parser (Rafferty and Manning, 2008; Klein and Manning, 2003) to generate syntactic parse trees for the source sentences in the training data. Then the tree-based reordering rules 3.1 Factored Neural Network Models Recently, the use of neural network models in rescoring of phrase-based machine translation has shown to lead to significant improveme"
W16-2314,D07-1045,0,0.0833493,"Missing"
W16-2314,P07-2045,0,\N,Missing
W16-2314,W15-3030,1,\N,Missing
W16-2314,2005.iwslt-1.8,0,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W17-3202,P11-2031,0,0.0388621,"on data we used the newstest13 set from IWSLT evaluation campaign. Therefore, this data is from TED talks. Test is applied on two domains. First domain is TED talks, same as the optimization set. We use newstest14 for this testing. Another domain is telephone conversation and we used MSLT (Christian Federmann, 2016) for testing. Since no exact genre-matching development data is published for the evaluation campaign (Cettolo et al., 2015), we used the TED-optimized system for the MSLT testing. For each experiment, we also offer oracle BLEU scores on the n-best lists, calculated using multeval (Clark et al., 2011). 4 We tried different system configurations to generate and rescore the n-best lists. By using 40K operations of BPE we had SmallVoc configuration, and with 80K BigVoc configuration. In SmallVoc.rev, target sentence are generated in the reversed order. In SmallVoc.mix, target side corpus is joined with the source side corpus to form a mixed input as described in Cho et al. (2016). We build an NMT system which takes pre-translation 4.1 System Description Our German↔English NMT systems are built using an encoder-decoder framework with attention mechanism, nematus.1 Byte pair encoding (BPE) is u"
W17-3202,N07-2035,0,0.0605851,"Missing"
W17-3202,P02-1040,0,0.099716,"he impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the restriction of the Introduction Recent advances in NMT systems (Bahdanau et al., 2014; Cho et al., 2014) have shown impressive results in improving machine translation tasks. Not only it performed greatly in recent machine translation campaigns (Cettolo et al., 2015; Bojar et al., 2016) measured in BLEU (Papineni et al., 2002), it is considered to be able to generate sentences with better fluency. Despite the successful results in translation performance, however, the optimality of the search algorithm in NMT has been left under-explored. In this work, we analyze the influence of search and modeling of an NMT system by evaluating them 11 Proceedings of the First Workshop on Neural Machine Translation, pages 11–17, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics search space (search error) from the errors due to models not good enough to cover the best translation (model error). A"
W17-3202,D13-1111,0,0.0154739,"slation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the combined system. Costa-Juss`a et al. (2007) analyze the impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the restriction of the Introduction Recent advances in NMT systems (Bahdanau et al., 2014; Cho et al., 2014) have shown impressive results in improving mach"
W17-3202,W09-0408,0,0.0232936,"ntly. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine th"
W17-3202,2008.amta-srw.3,0,0.0234655,"ain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the combined system. Costa-Juss`a et al. (2007) analyze the impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the"
W17-3202,D07-1105,0,0.0260245,"erently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the comb"
W17-3202,E06-1005,0,0.0101683,"n NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-"
W17-3202,W15-5003,0,0.0254054,"the search algorithm in NMT has been left under-explored. In this work, we analyze the influence of search and modeling of an NMT system by evaluating them 11 Proceedings of the First Workshop on Neural Machine Translation, pages 11–17, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics search space (search error) from the errors due to models not good enough to cover the best translation (model error). Although this work is the closest to our work in terms of analysis methods, our work differs from theirs by addressing the issue focused on the NMT systems. In Neubig et al. (2015), the size of the n-best list produced by a phrase-based SMT and rescored by an NMT is taken into account for an error investigation. The work also shows which types of errors from the phrase-based system can be corrected or improved after NMT rescoring. To the best of our knowledge, our work is the first to examine the impact of search and model performance in pure NMT systems. probabilities to be selected based on the context vector from the attention layer, the previous recurrent state and the embedding of the previously chosen word. The whole network is then trained in an end-toend fashion"
W17-4708,N12-4000,0,0.219318,"enforced by different model architectures. The main contributions of this paper are (1) that we show multi-task learning is possible within attention-based sequence-to-sequence models, which are state-of-the-art in machine translation and (2) that we analyze the influence of three main design decisions. 2 Related Work Motivated by the success of using features learned from linguistic resources in various NLP tasks, there have been several approaches including external information into neural network-based systems. The POS-based information has been integrated for language models in Wu et al. (2012); Niehues et al. (2016). In the neural machine translation, using additional word factors like POS-tags has shown to be beneficial (Sennrich and Haddow, 2016). The initial approach for multi-task learning for neural networks was presented in Collobert et al. (2011). The authors used convolutional and feed forward networks for several tasks such as semantic parsing and POS tagging. This idea was extended to sequence to sequence models in Luong et al. (2015). A special case of multi-task learning for attention based models has been explored. In multilingual machine translation, for example, the"
W17-4708,D16-1026,0,0.0679144,"). The initial approach for multi-task learning for neural networks was presented in Collobert et al. (2011). The authors used convolutional and feed forward networks for several tasks such as semantic parsing and POS tagging. This idea was extended to sequence to sequence models in Luong et al. (2015). A special case of multi-task learning for attention based models has been explored. In multilingual machine translation, for example, the tasks are still machine translation tasks but they need to consider different language pairs. In this case, a system with an individual encoder and decoder (Firat et al., 2016b) as well as a system with a shared encoder-decoder (Ha et al., 2016; Johnson et al., 2016) has been proposed. 2.1 3 Multi-task Learning In a traditional NLP pipeline, a named entity recognition or machine translation system employ POS information by using the POS tags as additional features. For example, the system will learn that the probability of a word being a named entity is higher if the word is marked as a noun. First, a POS tagger is used to annotate the input data. Combining the statistical models used for POS tagging and named entity recognition might not be straightforward. Recent"
W17-4708,D15-1025,0,0.0868377,"Missing"
W17-4708,2011.iwslt-evaluation.7,0,0.0228126,"ncoder EALL is used for all tasks, but sepa82 Figure 1: Overview on the different architectures used for multi-task learning length of the labels is not exactly the length of the input to the model itself. Our model uses inputs with subwords units generated by byte-pair encoding (Sennrich et al., 2016). This challenge is strongly related with the problem of domain adaptation in machine translation, where a large out-of-domain data is available but only a small amount of in-domain data. For this scenario, first training on all data and then finetuning on the in-domain data was very successful (Lavergne et al., 2011; Cho et al., 2016). Therefore, we adapt this approach to the multi-task scenario. In this case, we first trained the model on all tasks and then continued training only on the main task. We will refer to this training schedule as adapted. 3.3 4 Experimental Setup We conduct experiments using the multi-task approach on three different tasks: machine translation from German to English, German finegrained POS tagging and German NE tagging. As briefly mentioned in Section 1, multi-task approach can be helpful when data is sparse. In order to simulate this, we deploy only German to English TED dat"
W17-4708,2012.eamt-1.60,0,0.0074748,"cases. During training this does not matter as the target sequence is given. For testing the system, however, this issue is crucial to address. In our initial experiment, it was shown that the POS tagger was able to learn the correct target length in most of the cases. For some sentences, however, the estimated target length was not correct. Therefore, the prior knowledge of sequence length is used during decoding so that label sequences are generated with the correct target length. It is worth to mention that the desired 4.1 Data For the translation task, we used 4M tokens of the WIT corpus (Cettolo et al., 2012) for German to English as training data. We used dev2010 for validation and tst2013 and tst2014 for testing, provided by the IWSLT. We only used training examples shorter than 60 words per sentence. The POS tagger was trained on 720K tokens the Tiger Corpus (Brants et al., 2004). This corpus contains German newspaper text. Consequently, it is out-of-domain data for the machine translation task. The development and the test data are also from this corpus. The POS tag set consists of 54 tags and the fine-grained POS tags with morphological annotations has 774 labels. Finally, we trained the Germ"
W17-4708,W16-2208,1,0.842383,"d by different model architectures. The main contributions of this paper are (1) that we show multi-task learning is possible within attention-based sequence-to-sequence models, which are state-of-the-art in machine translation and (2) that we analyze the influence of three main design decisions. 2 Related Work Motivated by the success of using features learned from linguistic resources in various NLP tasks, there have been several approaches including external information into neural network-based systems. The POS-based information has been integrated for language models in Wu et al. (2012); Niehues et al. (2016). In the neural machine translation, using additional word factors like POS-tags has shown to be beneficial (Sennrich and Haddow, 2016). The initial approach for multi-task learning for neural networks was presented in Collobert et al. (2011). The authors used convolutional and feed forward networks for several tasks such as semantic parsing and POS tagging. This idea was extended to sequence to sequence models in Luong et al. (2015). A special case of multi-task learning for attention based models has been explored. In multilingual machine translation, for example, the tasks are still machine"
W17-4708,W09-0435,1,0.782969,"models (Bahdanau et al., 2014) showed significant gains over traditional approaches, they are often trained only on the parallel data in an end-to-end fashion. In most cases, therefore, they do not facilitate other knowledge sources. When parallel data is sparse, exploiting other knowledge sources can be crucial for performance. Two techniques to integrate the additional resources are well studied. In one technique, we train a tool on the additional resources (e.g. POS tagger) and then annotate the parallel data using this tool. This technique has been applied extensively in SMT systems (e.g. Niehues and Kolss (2009)) as well as in some NMT systems (e.g. Sennrich and Haddow (2016)). The second technique would be to use the annotated data directly to train the model. The goal of this work is to integrate the additional linguistic resources directly into neural models, in order to achieve better performance. To do so, we build a multi-task model and train several NLP tasks jointly. We use an attention-based sequence-tosequence model for all tasks. Experiments show that we are able to improve the performance on the German to English machine translation task measured in BLEU, BEER and CharacTER. Furthermore,"
W17-4708,P15-1166,0,0.0568827,"Missing"
W17-4708,P02-1040,0,0.110578,"Missing"
W17-4708,W16-2209,0,0.208186,"aditional approaches, they are often trained only on the parallel data in an end-to-end fashion. In most cases, therefore, they do not facilitate other knowledge sources. When parallel data is sparse, exploiting other knowledge sources can be crucial for performance. Two techniques to integrate the additional resources are well studied. In one technique, we train a tool on the additional resources (e.g. POS tagger) and then annotate the parallel data using this tool. This technique has been applied extensively in SMT systems (e.g. Niehues and Kolss (2009)) as well as in some NMT systems (e.g. Sennrich and Haddow (2016)). The second technique would be to use the annotated data directly to train the model. The goal of this work is to integrate the additional linguistic resources directly into neural models, in order to achieve better performance. To do so, we build a multi-task model and train several NLP tasks jointly. We use an attention-based sequence-tosequence model for all tasks. Experiments show that we are able to improve the performance on the German to English machine translation task measured in BLEU, BEER and CharacTER. Furthermore, we analyze three important decisions when designing multi-task mo"
W17-4708,P16-1162,0,0.289176,"task. Shared encoder (shrd Enc) One promising way is to share components that handle the same type of data. Since all our tasks share English as input here is the encoder. In this architecture, we therefore use one encoder for all tasks. This is the minimal degree of sharing we consider in our experiments. A common encoder EALL is used for all tasks, but sepa82 Figure 1: Overview on the different architectures used for multi-task learning length of the labels is not exactly the length of the input to the model itself. Our model uses inputs with subwords units generated by byte-pair encoding (Sennrich et al., 2016). This challenge is strongly related with the problem of domain adaptation in machine translation, where a large out-of-domain data is available but only a small amount of in-domain data. For this scenario, first training on all data and then finetuning on the in-domain data was very successful (Lavergne et al., 2011; Cho et al., 2016). Therefore, we adapt this approach to the multi-task scenario. In this case, we first trained the model on all tasks and then continued training only on the main task. We will refer to this training schedule as adapted. 3.3 4 Experimental Setup We conduct experi"
W17-4708,D14-1025,0,0.0391737,"Missing"
W17-4708,W16-2342,0,0.0161289,"ed with Adam, where we restarted the algorithm twice and early stopping is applied using log-likelihood of the concatenated validation sets from the considered tasks. For the adapted schedule, Adam is started once again when training only on the target task. The model is implemented in lamtram (Neubig, 2015)1 . 4.3 5.2 Following the initial experiment, we address the following three design questions: Evaluation • What kind of influence does the secondary task have? The machine translation output is evaluated with BLEU (Papineni et al., 2002), BEER (Stanojevic and Sima’an, 2014) and CharacTER (Wang et al., 2016). For the POS tags, we report error rates on the small label set as well as on the large label set. 5 • How do the different architectures perform? • Do we need to adapt the training schedule? In order to clarify the impact of the three hyperparameters (the architectures, the tasks and the training) we performed experiments based on possible combinations. We used two most promising architectures, shrd Enc and shrd Att as discussed in Section 5.1. We use three task combinations, POS+MT, NE+MT and NE+POS+MT. Two training strategies are applied with and without adaptation as described in Section"
W17-4708,C12-1173,0,0.0217428,"er sharing enforced by different model architectures. The main contributions of this paper are (1) that we show multi-task learning is possible within attention-based sequence-to-sequence models, which are state-of-the-art in machine translation and (2) that we analyze the influence of three main design decisions. 2 Related Work Motivated by the success of using features learned from linguistic resources in various NLP tasks, there have been several approaches including external information into neural network-based systems. The POS-based information has been integrated for language models in Wu et al. (2012); Niehues et al. (2016). In the neural machine translation, using additional word factors like POS-tags has shown to be beneficial (Sennrich and Haddow, 2016). The initial approach for multi-task learning for neural networks was presented in Collobert et al. (2011). The authors used convolutional and feed forward networks for several tasks such as semantic parsing and POS tagging. This idea was extended to sequence to sequence models in Luong et al. (2015). A special case of multi-task learning for attention based models has been explored. In multilingual machine translation, for example, the"
W17-4708,N16-1101,0,\N,Missing
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W17-4736,W17-3202,1,0.871075,"Missing"
W17-4736,N16-1139,0,0.0611081,"Missing"
W17-4736,W16-2314,1,0.406544,"Missing"
W17-4736,2015.iwslt-papers.9,0,0.381074,"verage vector for maintaining attentional information during translation. Several techniques to integrated additional information into the source text have be investigated: Pretranslation with statistical systems, mono-lingual data and phrase-table entries. Finally, we combined different models using n-best lists reranking. 2 2.1.1 We experimented with using domain adaptation techniques to select monolingual data for backtranslation. In particular, we concatenated all news-test data sets up until 2013 to form our indomain corpus, and used news-shuffle as background data. We used the method by Axelrod et al. (2015), a class-based extension of the widely used cross-entropy difference based data selection method by Moore and Lewis (2010). For word clustering, we used Clustercat (Dehdari et al., 2016) with 20 classes. We selected an amount of data equal to the available bilingual training data. Backtranslation was done as in (Sennrich et al., Data This section describes the preprocessing steps for the parallel and monolingual corpora for the language pairs involved in the systems as well as the data selection methods investigated. 2.1 Monolingual data selection German↔English As parallel data for our Germa"
W17-4736,P17-4012,0,0.021322,"arameters following previous works (Sennrich et al., 2017): minibatch size of 80, maximum sentence length of 50, word embedding size of 650, a one layer GRU with size 1,024 in the encoder and a conditional GRU decoder with hidden layer size 1,024. The gradients are scaled with norm of 1.0 and the gradient update method being used is Adam (Kingma and Ba, 2014) with learning rate 0.0001. Models are trained until the BLEU score on the validation set stops increasing. Checkpoints are saved every 20K iterations. 3.2 OpenNMT We also employed the Torch-based (Collobert et al., 2011) toolkit OpenNMT (Klein et al., 2017) 3 . All models trained with this toolkit have two LSTM layers of 1,024 units each, and we also use the input-feeding method as described in (Luong et al., 2015). For optimization, the gradients are scaled at 5, and we experimentally use Adam with a high learning rate of 0.001 and then reduce it to 0.0005 when the perplexity of the model does not decrease anymore. Checkpoints are saved every epoch (all of the sentences are seen). We also enhanced the toolkits with different features, namely the Context Gate for attentional model (Tu et al., 2016a) and using coverage information during learning"
W17-4736,P06-1096,0,0.142634,"Missing"
W17-4736,N16-1046,0,0.129837,"s data. We use newstest2013 as validation data. Using this data, we train our initial system with the Nematus toolkit and a byte pair encoding size of 40K operations (Nematus 40K). The translation for all Nematus based systems are generated with ensembled system of different checkpoints. Although we also attempted to select the data for backtranslation as described in Section 2.1, initial experiments did not show improvements on the translation quality. Therefore, we use the randomly selected data for the remaining experiments. In addition, we build a system with a reverse target order (R2L) (Liu et al., 2016) and the pretranslation. The pre-translation was generated by the PBMT system used in WMT 2016 (Ha et al., 2016a). Both performed slightly better than the baseline system. When increasing the size of BPE operation to 80K, we observe the improvements on the translation quality, by 1.4 BLEU points. In addition to Nematus, we also used the OpenNMT framework to build a network. For this language pair, we used the context gate, but not the coverage model. In contrast to the Nematus based systems, we did not ensemble different checkpoints. When using OpenNMT this technique did not yield an improveme"
W17-4736,D15-1166,0,0.562406,"size 1,024 in the encoder and a conditional GRU decoder with hidden layer size 1,024. The gradients are scaled with norm of 1.0 and the gradient update method being used is Adam (Kingma and Ba, 2014) with learning rate 0.0001. Models are trained until the BLEU score on the validation set stops increasing. Checkpoints are saved every 20K iterations. 3.2 OpenNMT We also employed the Torch-based (Collobert et al., 2011) toolkit OpenNMT (Klein et al., 2017) 3 . All models trained with this toolkit have two LSTM layers of 1,024 units each, and we also use the input-feeding method as described in (Luong et al., 2015). For optimization, the gradients are scaled at 5, and we experimentally use Adam with a high learning rate of 0.001 and then reduce it to 0.0005 when the perplexity of the model does not decrease anymore. Checkpoints are saved every epoch (all of the sentences are seen). We also enhanced the toolkits with different features, namely the Context Gate for attentional model (Tu et al., 2016a) and using coverage information during learning to translate (Tu et al., 2016b; Sankaran et al., 2016). English→Latvian The parallel corpus English-Latvian contains 2.9 million sentences which are proprocesse"
W17-4736,P16-5005,0,0.0867149,"ed (Collobert et al., 2011) toolkit OpenNMT (Klein et al., 2017) 3 . All models trained with this toolkit have two LSTM layers of 1,024 units each, and we also use the input-feeding method as described in (Luong et al., 2015). For optimization, the gradients are scaled at 5, and we experimentally use Adam with a high learning rate of 0.001 and then reduce it to 0.0005 when the perplexity of the model does not decrease anymore. Checkpoints are saved every epoch (all of the sentences are seen). We also enhanced the toolkits with different features, namely the Context Gate for attentional model (Tu et al., 2016a) and using coverage information during learning to translate (Tu et al., 2016b; Sankaran et al., 2016). English→Latvian The parallel corpus English-Latvian contains 2.9 million sentences which are proprocessed by TILDE1 with language specific tokenizers. The Latvian text is only true-cased on the first letter of the sentence. We also further clean the data by using the language detection library Shuyo (2010) and remove the lines that the target sentences cannot be recognized as Latvian by the tool, resulting in about 25K sentences removed. Aside from the main data provided by the organizer,"
W17-4736,P16-1008,0,0.284526,"ed (Collobert et al., 2011) toolkit OpenNMT (Klein et al., 2017) 3 . All models trained with this toolkit have two LSTM layers of 1,024 units each, and we also use the input-feeding method as described in (Luong et al., 2015). For optimization, the gradients are scaled at 5, and we experimentally use Adam with a high learning rate of 0.001 and then reduce it to 0.0005 when the perplexity of the model does not decrease anymore. Checkpoints are saved every epoch (all of the sentences are seen). We also enhanced the toolkits with different features, namely the Context Gate for attentional model (Tu et al., 2016a) and using coverage information during learning to translate (Tu et al., 2016b; Sankaran et al., 2016). English→Latvian The parallel corpus English-Latvian contains 2.9 million sentences which are proprocessed by TILDE1 with language specific tokenizers. The Latvian text is only true-cased on the first letter of the sentence. We also further clean the data by using the language detection library Shuyo (2010) and remove the lines that the target sentences cannot be recognized as Latvian by the tool, resulting in about 25K sentences removed. Aside from the main data provided by the organizer,"
W17-4736,2011.iwslt-evaluation.9,1,0.776932,"Missing"
W17-4736,D16-1096,0,0.0203109,"so shared across languages to help the decoder selects better German words in the target side. The system implemented this idea is referred as a mix-source system. For this evaluation, we apply the idea of that multilingual NMT approach in the EnglishGerman direction in order to make use of the German monolingual corpus and gain additional imCoverage mechanism for attention model Various works have pointed out that the attention neural machine translation model can be benefit by constraining the attentional process to adequately cover the source words (Sankaran et al., 2016; Tu et al., 2016b; Mi et al., 2016; Luong et al., 2015). Different proposals share similar ideas which is to incorporate alignment information from the previous time steps into the attentional neural network. Our experiment inherits the neural fertility model from (Tu et al., 2016b) which uses an explicit vector to keep track of the alignment information. At every time step, the network makes an attentional decision with the help of the coverage vector, which is in turn updated using the alignment vector and the source context with a simple Gated Recurrent Unit (GRU). 4 Monolingual Data Integration of Additional Resources In t"
W17-4736,P10-2041,0,0.0191922,"rmation into the source text have be investigated: Pretranslation with statistical systems, mono-lingual data and phrase-table entries. Finally, we combined different models using n-best lists reranking. 2 2.1.1 We experimented with using domain adaptation techniques to select monolingual data for backtranslation. In particular, we concatenated all news-test data sets up until 2013 to form our indomain corpus, and used news-shuffle as background data. We used the method by Axelrod et al. (2015), a class-based extension of the widely used cross-entropy difference based data selection method by Moore and Lewis (2010). For word clustering, we used Clustercat (Dehdari et al., 2016) with 20 classes. We selected an amount of data equal to the available bilingual training data. Backtranslation was done as in (Sennrich et al., Data This section describes the preprocessing steps for the parallel and monolingual corpora for the language pairs involved in the systems as well as the data selection methods investigated. 2.1 Monolingual data selection German↔English As parallel data for our German↔English systems, we used Europarl v7 (EPPS), News Commentary v12 (NC), Rapid corpus of EU press releases, Common Crawl co"
W17-4736,C16-1172,1,0.892925,"Missing"
W18-2606,J81-4005,0,0.607392,"Missing"
W18-2606,P17-1055,0,0.015887,"k (Hermann et al., 2015), we show the adaptability of the rsDNC and achieve passable results without task-specific adaption. 2 Figure 1: System overview of the DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexible and universal design. The different parts of the DNC can be exchanged or adjusted independently which allows simpler handling of new tasks. The versatility is shown in the original paper with"
W18-2606,P17-1168,0,0.0138981,"DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexible and universal design. The different parts of the DNC can be exchanged or adjusted independently which allows simpler handling of new tasks. The versatility is shown in the original paper with three different tasks (Graves et al., 2016). Related Work This section considers the related work regarding the two used datasets. Related to bAbI task Rae et al."
W18-2606,P16-1086,0,0.0313559,"t random initializations. Additionally, with training-data augmentation on one task, our model solves all tasks and provides the bestrecorded results to the best of our knowledge. On the CNN RC task (Hermann et al., 2015), we show the adaptability of the rsDNC and achieve passable results without task-specific adaption. 2 Figure 1: System overview of the DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexib"
W18-2712,W17-3204,0,0.10496,"t directly approximate the conditional probability of a target sequence Y = y1 , y2 , · · · , yT given a source sequence X = x1 , x2 , · · · , xM . The model is normally trained to maximize the log-likelihood of each target token given the previous words as well as the source sequence with respect to model parameters θ as in Equation 1: log P (Y |X; θ) = ΣTt=1 (log P (yt |X, y1 , y2 , · · · , yt − 1)) The challenges of NMT These models are often attacked over their inability to learn to translate rare events, which are often named entities and rare morphological variants (Arthur et al., 2016; Koehn and Knowles, 2017; Nguyen and Chiang, 2017). Learning from rare events is difficult due to the fact that the model parameters are not adequately updated. For example, the embeddings of the rare words are only updated a few times during training, and similarly for the patterns learned by the recurrent structures in the encoders / decoders and attention models. (1) The advantages of NMT compared to phrasedbased machine translation come from the neural architecture components: 3 Expert framework description Human translators can benefit from external knowledge such as dictionaries, particularly in specific domain"
W18-2712,D16-1162,0,0.0272404,"ani et al., 2017) that directly approximate the conditional probability of a target sequence Y = y1 , y2 , · · · , yT given a source sequence X = x1 , x2 , · · · , xM . The model is normally trained to maximize the log-likelihood of each target token given the previous words as well as the source sequence with respect to model parameters θ as in Equation 1: log P (Y |X; θ) = ΣTt=1 (log P (yt |X, y1 , y2 , · · · , yt − 1)) The challenges of NMT These models are often attacked over their inability to learn to translate rare events, which are often named entities and rare morphological variants (Arthur et al., 2016; Koehn and Knowles, 2017; Nguyen and Chiang, 2017). Learning from rare events is difficult due to the fact that the model parameters are not adequately updated. For example, the embeddings of the rare words are only updated a few times during training, and similarly for the patterns learned by the recurrent structures in the encoders / decoders and attention models. (1) The advantages of NMT compared to phrasedbased machine translation come from the neural architecture components: 3 Expert framework description Human translators can benefit from external knowledge such as dictionaries, partic"
W18-2712,N03-1017,0,0.0718052,"oving more than 1.0 BLEU point in both translation directions English→Spanish and German→English. 1 Introduction Sequence to sequence models have recently become the state-of-the-art approach for machine translation (Luong et al., 2015; Vaswani et al., 2017). This model architecture can directly approximate the conditional probability of the target sequence given a source sequence using neural networks (Kalchbrenner and Blunsom, 2013). As a result, not only do they model a smoother probability distribution (Bengio et al., 2003) than the sparse phrase tables in statistical machine translation (Koehn et al., 2003), but they can also jointly learn translation models, language models and even alignments in a single model (Bahdanau et al., 2014). One of the main weaknesses of neural machine 100 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 100–109 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics which is capable of learning to translate lexically with one or few examples, such as dictionaries, or phrase-tables, or even human annotators. We realize our proposed framework with experiments on English→Spanish and German→English transla"
W18-2712,2015.iwslt-evaluation.11,0,0.0882722,"017) or fall into underor over-translation (Tu et al., 2016). Examples of these situations include named entities, dates, and rare morphological forms. Improper handling of rare events can be harmful to industrial systems (Wu et al., 2016), where translation mistakes can have serious ramifications. Similarly, translating in specific domains such as information technology or biology, a slight change in vocabulary can drastically alter meaning. It is important, then, to address translation of rare words. While domain-specific parallel corpora can be used to adapt translation models efficiently (Luong and Manning, 2015), parallel corpora for many domains can be difficult to collect, and this requires continued training. Translation lexicons, however, are much more commonly available. In this work, we introduce a strategy to incorporate external lexical knowledge, dubbed “Expert annotation,” into neural machine translation models. First, we annotate the lexical translations directly into the source side of the parallel data, so that the information is exposed during both training and inference. Second, inspired by CopyNet (Gu et al., 2016), we utilize a pointer network (Vinyals et al., 2015) to introduce a co"
W18-2712,D15-1166,0,0.743937,"eraction with a pointer network and reinforcement learning. Our experiments using phrase-based models to simulate Experts to complement neural machine translation models show that the model can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof-domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English→Spanish and German→English. 1 Introduction Sequence to sequence models have recently become the state-of-the-art approach for machine translation (Luong et al., 2015; Vaswani et al., 2017). This model architecture can directly approximate the conditional probability of the target sequence given a source sequence using neural networks (Kalchbrenner and Blunsom, 2013). As a result, not only do they model a smoother probability distribution (Bengio et al., 2003) than the sparse phrase tables in statistical machine translation (Koehn et al., 2003), but they can also jointly learn translation models, language models and even alignments in a single model (Bahdanau et al., 2014). One of the main weaknesses of neural machine 100 Proceedings of the 2nd Workshop on"
W18-2712,2012.eamt-1.60,0,0.016575,"o et al., 2015) in which we have the model sample and provide a learning signal by rewarding the model if it copies the annotation into the target, as seen in the loss function in Equation 3: L(θ) = −EW ∼pθ (r(W, REF )) Notably, there is no gradient flowing in the baseline subgraph since the argmax operators used in the greedy search are not differentiable. 4 In the experiments, we realise the generic framework described in Section 3 with the tasks of translating from English→Spanish and German→English. For both language pairs, we used data from Europarl (version 7) (Koehn, 2005) and IWSLT17 (Cettolo et al., 2012) to train our neural networks. For validation, we use the IWSLT validation set (dev2010) to select the best models based on perplexity (for cross-entropy loss) and BLEU score (for reinforcement learning). For evaluation, we use IWSLT tst2010 as the indomain test set. We also evaluate our models on out-of-domain corpora. For English→Spanish an additional Business dataset is used. The corpus statistics can be seen on Table 1. The out-ofdomain experiments for the German→English are carried out on the medical domain, in which we use the UFAL Medical Corpus v1.0 corpus (2.2 million sentences) to tr"
W18-2712,W17-3203,0,0.0255068,"Missing"
W18-2712,P16-1154,0,0.279851,"l corpora can be used to adapt translation models efficiently (Luong and Manning, 2015), parallel corpora for many domains can be difficult to collect, and this requires continued training. Translation lexicons, however, are much more commonly available. In this work, we introduce a strategy to incorporate external lexical knowledge, dubbed “Expert annotation,” into neural machine translation models. First, we annotate the lexical translations directly into the source side of the parallel data, so that the information is exposed during both training and inference. Second, inspired by CopyNet (Gu et al., 2016), we utilize a pointer network (Vinyals et al., 2015) to introduce a copy distribution over the source sentence, to increase the generation probability of rare words. Given that the expert annotation can differ from the reference, in order to encourage the model to copy the annotation we use reinforcement learning to guide the search, giving rewards when the annotation is used. Our work is motivated to be able to achieve One-Shot learning, which can help the model to accurately translate the events that are annotated during inference. Such ability can be transferred from an Expert Neural machi"
W18-2712,D17-1153,0,0.0485725,"Missing"
W18-2712,P16-1014,0,0.153188,"r situations. For example, we might use it to integrate a model that can do metric conversion or handling of links to web addresses, which can be useful for certain applications. Then NMT model then learns to translate to the target sentence using the annotated source. 3.1 Copy-Generator Hypothetically, the model could learn to simply ignore the annotation during optimization because it contains strange symbols (the target language) in source language sentences. If this were the case, adding annotations would not help translate rare events. Therefore, inspired by the CopyNet (Gu et al., 2016; Gulcehre et al., 2016), which originates from pointer networks (Vinyals et al., 2015) that learn to pick the tokens that appeared in the memory of the models, we incorporate the copymechanism into the neural translation model so that the annotations can be simply pasted into the translation. Explicitly, the conditional probability is now presented as a mixture of two distributions: copy and generated. Annotation The aforementioned idea of Experts in our work is inspired by the fact that human translators can benefit from domain experts when translating domainspecific content. Accordingly, we design the annotation a"
W18-2712,C16-1172,1,0.90779,"Missing"
W18-2712,P02-1040,0,0.101902,"opying the annotation into the target, but still maintain a reasonable translation quality. For suggestion utilization, we denote HIT as the score function that gives rewards for every overlap of the output and the suggestion. If all annotated words are used then HIT (W, REF ) = 1.0, otherwise the percentage of the copied words. For the translation score, we use the GLEU function (Wu et al., 2016) - the minimum of recall and precision of the n-grams up to 4-gram between the sample and the reference, which has been reported to correspond well with corpus-level translation metrics such as BLEU (Papineni et al., 2002). The reward function is defined as in Equation 4: r(W, REF ) = αHIT (W, REF )+ (1 − α)GLEU (W, REF ) Experiment setup (4) Variance reduction The use of reinforcement learning with translation models has been explored in various works (Ranzato et al., 2015; Bahdanau et al., 2016; Rennie et al., 2016; Nguyen et al., 2017), in which the models are difficult to train due to the high variance of the gradients (Schulman et al., 2017). To tackle this problem, we follow the Self-Critical model proposed by (Rennie et al., 2016) for variance reduction: 4.1 Implementation details Our base neural machine"
W18-2712,D13-1176,0,0.110095,"can be trained to copy the annotations into the output consistently. We demonstrate the benefit of our proposed framework in outof-domain translation scenarios with only lexical resources, improving more than 1.0 BLEU point in both translation directions English→Spanish and German→English. 1 Introduction Sequence to sequence models have recently become the state-of-the-art approach for machine translation (Luong et al., 2015; Vaswani et al., 2017). This model architecture can directly approximate the conditional probability of the target sequence given a source sequence using neural networks (Kalchbrenner and Blunsom, 2013). As a result, not only do they model a smoother probability distribution (Bengio et al., 2003) than the sparse phrase tables in statistical machine translation (Koehn et al., 2003), but they can also jointly learn translation models, language models and even alignments in a single model (Bahdanau et al., 2014). One of the main weaknesses of neural machine 100 Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 100–109 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics which is capable of learning to translate lexically with on"
W18-2712,2005.mtsummit-papers.11,0,0.0426624,"ment learning task (Ranzato et al., 2015) in which we have the model sample and provide a learning signal by rewarding the model if it copies the annotation into the target, as seen in the loss function in Equation 3: L(θ) = −EW ∼pθ (r(W, REF )) Notably, there is no gradient flowing in the baseline subgraph since the argmax operators used in the greedy search are not differentiable. 4 In the experiments, we realise the generic framework described in Section 3 with the tasks of translating from English→Spanish and German→English. For both language pairs, we used data from Europarl (version 7) (Koehn, 2005) and IWSLT17 (Cettolo et al., 2012) to train our neural networks. For validation, we use the IWSLT validation set (dev2010) to select the best models based on perplexity (for cross-entropy loss) and BLEU score (for reinforcement learning). For evaluation, we use IWSLT tst2010 as the indomain test set. We also evaluate our models on out-of-domain corpora. For English→Spanish an additional Business dataset is used. The corpus statistics can be seen on Table 1. The out-ofdomain experiments for the German→English are carried out on the medical domain, in which we use the UFAL Medical Corpus v1.0 c"
W18-2712,P17-1139,0,0.0373144,"we identified 106 Figure 2: Top: Examples of name annotations with our framework from tst2010. The name Kean is originally split by BPE into ‘K’ and ‘ean’. This is incorrectly translated without annotation (in blue) and corrected with the annotation (in red). Bottom: An example of phrase copying, in which the German word is translated into a long English phrase. Figure 3: An attention heat map of an English-Spanish sentence pair (source on X-axis, target on Y-axis) with annotated sections in red rectangles. Annotations and their source are bounded by # characters. naries and phrase-tables in (Zhang et al., 2017). They also rely on sample-based techniques, (Shen et al., 2015), to train their networks, but their computation is more expensive than the self-critical network in our work. We focus here on rare events, with the possibility to construct interactive models for fast updating without retraining. We also use the ideas of using REINFORCE to train sequence generators for arbitrary rewards (Ranzato et al., 2015; Nguyen et al., 2017; Bahdanau et al., 2016). While this method remains difficult to train, it is promising to use to achieve non-probabilistic features for neural models: for example enforc"
W18-2712,P16-1162,0,0.471894,"rks the authors had to build dynamic vocabulary for each sample due to the vocabulary mismatch between the source and target (Gu et al., 2016). Since we tied the embeddings of source and target languages, it becomes trivial to combine the two distributions. The use of byte-pair encodings also helps to eliminate unknown words on both sides, alleviating the task of excluding copying unknown tokens. • Train a neural machine translation model using these annotated sentences. • During inference, we annotate the source sentence in the same fashion as in training. Byte-Pair encoding We consider BPE (Sennrich et al., 2016) one of the crucial factors for annotation in order to efficiently represent words that do not appear in the training data. The rare words (and their translation suggestions, which can be rare as well) are split into smaller segments, alleviating the problem of dealing with U N K tokens (Luong et al., 2014). Embedding sharing Our annotation method includes target language tokens directly in the source sentence. In order to make the model perceive these words the same way in the source and the target, we create a joint vocabulary of the source and target language and simply tie the embedding pr"
W18-2712,P16-1008,0,0.0428865,"Missing"
W18-2712,1983.tc-1.13,0,0.685173,"Missing"
W18-6422,P16-1162,0,0.0416552,"LEU points. 1 Data 2.1 English↔German As parallel data for our German↔English systems, we used Europarl v7 (EPPS), News Commentary v12 (NC), Rapid corpus of EU press releases, Common Crawl corpus, the ParaCrawl corpus and simulated data. The preprocessing includes tokenization, removing very long sentences and the sentence pairs which are lengthmismatched, normalizing special symbols and different writing rules and smart-casing the first word of each sentence. Those tools are provided in the Moses Toolkit 1 . We integrated the monolingual news data by generating synthetic data as motivated by Sennrich et al. (2016a). We used the translated data provided by University of Edinburgh. Once the data is preprocessed, we applied bytepair encoding (BPE) (Sennrich et al., 2016b) on the corpus. In this work, we deploy an operation size of 40K (shared between English and German languages) and applied vocabulary filtering in a way that every token occurs at least 50 times. Introduction This manuscript provides the technical details regarding our submission in the WMT18 shared task on English→German news translation. Our submission has two major research contributions: Firstly, the development of a deep, efficient"
W18-6422,W17-4739,0,0.0189197,"evious work (Britz et al., 2017) explores different depths during training NMT models with similar architectures to our baseline discovering that it is not trivial to improve Recurrent NMT models just by increasing depth even with residual connections. It is notable that recent work (Chen et al., 2018) empirically proved that RNN models with hyper parameter tuning and layer normalization strategy can perform on par with the Transformer. 4.4 As illustrated above, the Transformer models produced strong results which can outperform the best system of last year which is an ensemble of RNN models (Sennrich et al., 2017). We proceed to improve the system further by providing additional training data. Table 3 shows that a naive addiction of the paraCrawl data yields only a boost of 0.3 BLEU points, while our filtering method impressively improves the result by 0.8. Model comparison In a first series of experiment we compared different architectures (RNNs and Transformers) and the influence of the deeps of the network. The transformer-based models are implemented using PyTorch (Paszke et al., 2017) and the source codes are open sourced. 3 . We provided our starting point as a reference to our participation to t"
W18-6422,1983.tc-1.13,0,0.4589,"Missing"
W18-6422,2015.iwslt-evaluation.11,0,0.105525,"T given a source sequence X = x1 , x2 , · · · , xM . The basic concept of the model is to encode the source sequence with a neural network to capture the neural representation of the source sentence, which is then referred multiple times during a decoding process, in which another neural network auto-regressively generates tokens in the target language. The architectural choice is important in building neural machine translation systems. While Recurrent Neural Networks (RNN) have become the defacto model to represent sequences and were applied very successfully in NMT (Sutskever et al., 2014; Luong and Manning, 2015), self-attention networks (or Transformer) arose as a potentially better alternative (Vaswani et al., 2017). 3.2 Transformer overview The transformer architecture was previously introduced with the following novel features: Deep Transformer • Long range dependency is modeled using the self-attention mechanism instead of recurrent connections used in recurrent networks, like the Long-Short Term Memories. The mechanism allows direct connection between two different two arbitrary positions in the sequences, which in turns alleviates the gradient flow problem existing in recurrent networks. The re"
W18-6422,D15-1166,0,0.0579106,"ffers created during training can be discarded, resulting in smaller memory requirement and bigger batch size. 4.2 Training hyper parameters For RNN models, we use 4-layer-models with Long-Short Term Memory (Hochreiter and Schmidhuber, 1997). The bi-directional LSTM is used in the Encoder for all 4 layers. We use batch 2 469 http://opennmt.net size of 128 sentences (notably, the measurement of batch size in Transformer is denoted by the number of tokens, not sentences) and simply trained with Stochastic Gradient Descent with learning rate decay when the validation perplexity does not improve (Luong et al., 2015). For Transformer models, we set the base layer size to 512, while the hidden layer in each Position Wise Feed Forward network has 2048 neurons, which matches the Base model in (Vaswani et al., 2017). The learning method is Adam (Kingma and Ba, 2014) with the learning rate schedule similar to the original paper, with a minor difference that we increase the number of warm up steps to 8192 and double the base learning rate. If Dropout is applied, we use dropout at each Position Wise Feed Forward hidden layer and the attention weights. 4.3 the RNN baseline by 2.3 BLEU points just by increasing th"
W18-6422,W17-4736,1,0.702879,"., 2016) prevents network state values from exploding; label smoothing regularizes the cross entropy loss function to improves the models’ generalization; 3.3 Training 4 Results 4.1 Baseline System Our baseline system uses the openNMT-py Toolkit2 and uses an RNN based translation model with 4 layers in both decoders and encoders (bidirectional RNN on the encoder side). The model is equipped with dropout= 0.2 following the work of (Zaremba et al., 2014) for better regularization and label smoothing improving the cross-entropy loss. The training details and hyper-parameters are replicated from (Pham, 2017). In all of our experiments, we use the concatenation of test sets from 2013 to 2016 as our development set for model/checkpoint selection. While we use perplexity for model selection, the BLEU score on newstest2017 calculated by mteval-v13a.pl is used to report the models’ performance. Efficient memory usage NMT models in general are very memory consuming due to the fact that they need to apply transformation on a sequence of states instead of single states in feed-forward neural networks. For other architectures, like feed-forward neural networks, convolution neural networks and recurrent ne"
W18-6422,P16-1009,0,0.0438286,"LEU points. 1 Data 2.1 English↔German As parallel data for our German↔English systems, we used Europarl v7 (EPPS), News Commentary v12 (NC), Rapid corpus of EU press releases, Common Crawl corpus, the ParaCrawl corpus and simulated data. The preprocessing includes tokenization, removing very long sentences and the sentence pairs which are lengthmismatched, normalizing special symbols and different writing rules and smart-casing the first word of each sentence. Those tools are provided in the Moses Toolkit 1 . We integrated the monolingual news data by generating synthetic data as motivated by Sennrich et al. (2016a). We used the translated data provided by University of Edinburgh. Once the data is preprocessed, we applied bytepair encoding (BPE) (Sennrich et al., 2016b) on the corpus. In this work, we deploy an operation size of 40K (shared between English and German languages) and applied vocabulary filtering in a way that every token occurs at least 50 times. Introduction This manuscript provides the technical details regarding our submission in the WMT18 shared task on English→German news translation. Our submission has two major research contributions: Firstly, the development of a deep, efficient"
W19-5202,N19-1121,0,0.046519,"Missing"
W19-5202,2004.iwslt-evaluation.1,0,0.21075,"Missing"
W19-5202,P17-1176,0,0.0444999,"Missing"
W19-5202,D17-1158,0,0.0191546,"for the encoder is possible. In this section, we navigate the target NMT architecture back to the popular variable-length sequential encoder in which no such compromise was made. Starting from the above motivation, the key idea is to force a source language-independent representation in the decoder using an additional loss function. We achieve this by operating the encoder-decoder flow not only from the source sentence to the target, but also from the source to itself to recreate the source sentence. While this resembles an auto-encoder which can be combined with translation (He et al., 2016; Domhan and Hieber, 2017), it is not necessary to minimize the auto-encoder likelihood as in the multitask approach (Niehues and Cho, 2017), but only the decoder-level similarity between the true target sentence and the auto-encoded source sentence. Due to the lack of true parallel data, this method serves as a bridge between the different languages. An important feature of the NMT attention mechanism is that it extracts relevant information in encoded memory (the keys and queries, in this case they are the source sentence hidden states) and compresses them into one single state. More importantly, in the decoder opera"
W19-5202,N16-1101,0,0.0244141,"nguage pairs compared to the zero-shot performance of a state-of-the-art multilingual system. Additionally, we carry out further experiments in which the effect is confirmed even for language pairs with multiple intermediate pivots. 1 Introduction Neural machine translation (NMT) exploits neural networks to directly learn to transform sentences from a source language to a target language (Sutskever et al., 2014; Bahdanau et al., 2014). Universal multilingual NMT discovered that a neural translation system can be trained on datasets containing source and target sentences in multiple languages (Firat et al., 2016; Johnson et al., 2016). Successfully trained models using this approach can be used to translate arbitrar13 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 13–23 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics sentence X: First, we attempt to investigate the relationship of encoder representation and ZS performance. By modifying the Transformer architecture of Vaswani et al. (2017) to afford a fixed-size representation for the encoder output, we found that we can significantly improve zero-shot performance"
W19-5202,N18-1032,0,0.104034,"Missing"
W19-5202,P17-2089,0,0.0282361,"endent encoder representations. Therefore, we propose different architectures with fixed-size encoder representations. This also allows us to directly compare encoder representations of different languages, and to enforce such similarity through an additional loss function. This modification comes with the price of an information bottleneck due to the process of removing the length variability. On the other hand, it adds additional regularization which would naturally prioritize the features shared between languages. Motivated by the literature in sentence embeddings (Schwenk and Douze, 2017; Wang et al., 2017), we take the average over time of the encoder states. Specifically, assume that X is the set of source embeddings input to the encoder: Background: Multilingual Neural Machine Translation Given an input sequence X and its translation Y , neural machine translation (NMT) uses sequenceto-sequence models (Sutskever et al., 2014) to directly model the posterior probability of generating Y from X. Universal multilingual NMT expands the original bilingual setting by combining parallel corpora from multiple language pairs into one single corpus. By directly training the NMT model on this combined co"
W19-5202,Q17-1024,0,0.0614481,"Missing"
W19-5202,D16-1163,0,0.0306712,"zero-shot translation (ZS), or translation between languages included in multilingual data for which no directly parallel training data exists. Application-wise, ZS offers a faster and more direct path between languages compared to pivot translation, which requires translation to one or many intermediate languages. This can result in large latency and error propagation, common issues in non-end-to-end pipelines.From a representation learning point of view, there is evidence of NMT’s ability to capture language-independent features, which have proved useful for crosslingual transfer learning (Zoph et al., 2016; Kim et al., 2019) and provide motivation for ZS translation. However it is still unclear if minimizing the difference in representations between languages is beneficial for zero-shot learning. On the other hand, the current neural architecture and learning mechanisms of multilingual NMT is not geared towards having a common representation. Different languages are likely to convey the same semantic content with sentences of different lengths (Kalchbrenner et al., 2016), which makes the desiderata difficult to achieve. Moreover, the loss function of the neural translation model does not favour"
W19-5202,D13-1176,0,0.0609237,"y opens more possibilities for language-independent representation to occur, because every sentence is compressed into a consistent number of states. Second, we can observe the balance between language-independent and language-dependent information in the encoder; if zero-shot performance is minimally affected, then the encoder is in general able to capture language-independent information, and this restricted encoder retains this information. However, this model naturally has a disadvantage due to the introduced information bottleneck, similar to non-attention models (Sutskever et al., 2014; Kalchbrenner and Blunsom, 2013). We alleviate this problem by expanding the number of hidden states of the encoder output. As a result we investigate two variations of pooling as follows: Language-Independent Objective With constant length encoder output, we can now design an objective function using this advantage for language-independent representation. Hypothetically, for true multi-parallel data in which sentences from different languages are aligned, we can force the encoder outputs to be the same for the aligned sentences (newly enabled by the fixed state size). In other multilingual frameworks in which each data samp"
W19-5202,P19-1120,0,0.0195554,"eared towards zero-shot translation. Lu et al. (2018) proposed to explicitly define a recurrent layer with a fixed number of states as “Interlingua” which resembles our attention-pooling models. However, they compromise the model compactness by having separate encoder-decoder per language, which linearly increases the model size across languages. On the other hand, Platanios et al. (2018) shares all parameters, but utilized a parameter generator to generate specific parameters for the LSTMs in each language pair using language embeddings. The closest to our work is probably Arivazhagan et al. (2019). The authors aimed to regularize 20 Pair/Model Transformer en-de de-en en-ro ro-en en-it it-en en-nl nl-en de-nl nl-de it-ro ro-it de-it it-de nl-ro ro-nl nl-it it-nl de-ro ro-de avg ∆ 27.51 30.73 27.45 33.65 31.84 35.84 32.15 34.81 19.04 20.46 18.45 19.84 16.59 17.55 16.89 18.12 18.11 18.71 15.33 17.92 18.08 +Pivot 21.59 22.14 20.68 22.32 19.08 20.68 19.25 21.38 21.7 22.67 17.69 20.84 20.83 +2.64 +MSE -Attn 27.44 30.6 27.32 33.24 31.61 35.76 31.85 34.3 20.93 21.99 20.25 21.44 18.12 19.09 18.41 20.16 20.04 21.41 16.77 19.89 19.88 +1.80 +MSEdec 27.21 30.37 27.1 33.62 31.84 35.93 31.38 34.52 21"
W19-5202,W18-6309,0,0.0293561,"compared to the baseline. On the other hand, the Nl⇐⇒It language pairs were most difficult to improve. This is also the setting in which pivot suffered the heaviest loss. To summarize, these multi-steps experiments showed the drawbacks of pivot while at the same Related Work Zero-shot translation is of considerable concern among the multilingual translation community. By sharing network parameters across languages, ZS was proven feasible for universal multilingual MT (Ha et al., 2016; Johnson et al., 2016). There are many variations of multilingual models geared towards zero-shot translation. Lu et al. (2018) proposed to explicitly define a recurrent layer with a fixed number of states as “Interlingua” which resembles our attention-pooling models. However, they compromise the model compactness by having separate encoder-decoder per language, which linearly increases the model size across languages. On the other hand, Platanios et al. (2018) shares all parameters, but utilized a parameter generator to generate specific parameters for the LSTMs in each language pair using language embeddings. The closest to our work is probably Arivazhagan et al. (2019). The authors aimed to regularize 20 Pair/Model"
W19-5202,D15-1166,0,0.060334,"oftmax layer), because this state summarizes the information gathered in the decoder at each timestep. This approach will be referred as MSE-Decoder. Similar to Equation 4, we denote the decoder state of the conditional probability at time t as Dec(Yt |X, Y1..t−1 ). R(X, Y ) = T −1 X (Dec(Yt |X, Y1..t−1 ) (5) t=0 2 − Dec(Yt |Y, Y1..t−1 )) Softmax Forcing Similar to our second variation, the restriction is now put at the final layer. By running the decoder twice for translation and auto-encoder, we can force the output distribution of each step P (Yt |X, Y1..t−1 ) to be equal using KL 1 As in (Luong et al., 2015) the context vector denotes the weighted sum of the encoder after the attention operation 2 For multi-head attention we take the output after concatenating the heads. 16 Figure 2: Three different constraints for language-independent decoders. The model is run twice as translation (left) and auto-encoder (right). The KL-Softmax is applied at the very top, while the MSE-Decoder minimizes difference between the layer-normalized states at the end of the decoder. The MSE-attention operates on nonnormalized attention outputs. divergence minimization. The purpose of this step is to enable the decoder"
W19-5202,W17-4708,1,0.852066,"ength sequential encoder in which no such compromise was made. Starting from the above motivation, the key idea is to force a source language-independent representation in the decoder using an additional loss function. We achieve this by operating the encoder-decoder flow not only from the source sentence to the target, but also from the source to itself to recreate the source sentence. While this resembles an auto-encoder which can be combined with translation (He et al., 2016; Domhan and Hieber, 2017), it is not necessary to minimize the auto-encoder likelihood as in the multitask approach (Niehues and Cho, 2017), but only the decoder-level similarity between the true target sentence and the auto-encoded source sentence. Due to the lack of true parallel data, this method serves as a bridge between the different languages. An important feature of the NMT attention mechanism is that it extracts relevant information in encoded memory (the keys and queries, in this case they are the source sentence hidden states) and compresses them into one single state. More importantly, in the decoder operation this operator dynamically repeats every timestep. By using the encoder to encode both (source and target) Att"
W19-5202,D18-1039,0,0.182207,"g models suffered from information bottleneck and lost 1 − 2 BLEU for each language pair compared to the base Transformer model, the Mean-Pooling model is surprisingly better than the baseline at zero-shot tests. The Attention-Pooling model outperformed the Mean-Pooling at non-zero-shot tests, yet is worse at zero-shot conditions. Compared to other published works on this dataset (which are trained on all 20 directions), our supervised directions set the state-of-the-art for these directions while the zero-shot results approach the best supervised models in the literature (Dabre et al., 2017; Platanios et al., 2018)). Furthermore, by training these two models with a loss function including MSE-loss for encoder similarity, we found noticeable gains on zero-shot performance. More importantly, the zero-shot performance of our Mean-Pooling model with MSE-encoder not only outperforms the baseline, but also rivals the Training Details For all three variations of the decoder, the most important factor is the coefficient α of the second loss term (as in Equation 3 which decides the importance of this term during the training process. In the beginning of training, it is more important to focus on the main transla"
W19-5202,W17-2619,0,0.0300794,"l to learn language-independent encoder representations. Therefore, we propose different architectures with fixed-size encoder representations. This also allows us to directly compare encoder representations of different languages, and to enforce such similarity through an additional loss function. This modification comes with the price of an information bottleneck due to the process of removing the length variability. On the other hand, it adds additional regularization which would naturally prioritize the features shared between languages. Motivated by the literature in sentence embeddings (Schwenk and Douze, 2017; Wang et al., 2017), we take the average over time of the encoder states. Specifically, assume that X is the set of source embeddings input to the encoder: Background: Multilingual Neural Machine Translation Given an input sequence X and its translation Y , neural machine translation (NMT) uses sequenceto-sequence models (Sutskever et al., 2014) to directly model the posterior probability of generating Y from X. Universal multilingual NMT expands the original bilingual setting by combining parallel corpora from multiple language pairs into one single corpus. By directly training the NMT model"
W19-8671,W14-3354,0,0.0282249,"Missing"
W19-8671,J07-1003,0,0.0557021,"ence towards data points that are similar to their training data. Motivated by this, our first contribution is an autoencoder network that is applied as an extension to the sequence-to-sequence models to measure the training-testing discrepancy. In contrast to methods that directly compare the test and training data to generate confidence scores, we do not need to store the whole training data, thereby enabling our method to scale to larger datasets and tasks. Motivated by the successful application of posterior probabilities for confidence estimation in statistical machine translation (SMT) (Ueffing and Ney, 2007) and traditional ASR systems (Siu and Gish, 1999), our second contribution is a combination our approach with this prior approach. Traditionally, confidence estimation has been defined as a task of assessing the quality of the whole sequence of words in the target sentence. Especially when evaluating translations, there are also several cases when it can be very beneficial to estimate how well the source words are translated beyond coverage. For example, a person only speaking the source language might be able to reformulate the source sentence, if he knows that the system has difficulties wit"
W19-8671,2012.eamt-1.60,0,0.0105277,"sequence models in our work are based on the state-of-the-art Transformer architecture (Vaswani et al., 2017). We followed the model configuration with the learning rate schedule from the Base configuration in the original work. The number of layers is adapted for each task for the best performance possible and will be reported respectively. The autoencoders are implemented on top of the Transformer (with PyTorch (Paszke et al., 2017)) using one hidden layer with different sizes and sigmoid activation function. 3 The MT model is a 12-layer Transformer trained on the German-English TED corpus (Cettolo et al., 2012) with the development set and test set from the IWSLT 2017 evaluation campaign. The data is preprocessed with Moses tokenization, true-casing and segmented with byte-pair encoding (Sennrich et al., 2016) with 40K codes. The model achieves a BLEU score of 28.82 on the development set and 30.63 on the test data. We conducted further ASR experiments on the Switchboard-1 Release 2 (LDC97S62) corpus, which contains over 300 hours of speech. The Hub5’00 evaluation data (LDC2002S09) was used as our test set. On this set, we are especially inter2 If a word in the reference was deleted, we marked the p"
W19-8671,W03-0413,0,0.214765,"Missing"
W19-8671,J03-1002,0,0.107745,"coder states D instead of the encoder states E, we can calculate S d accordingly and use it to estimate the sequence or target token confidence. 4 While the previously presented models are all able to generate confidence measures for each target token, only the distance-based similarity measures are able to also generate scores for the source tokens. In order to generate source token confidence qualitatively, a straightforward approach is to use word alignment to map the confidence score from the target side to the source. Our baseline for these experiments uses the IBM4 GIZA alignment model (Och and Ney, 2003) to map the posterior probabilities and the combined approach’s confidence estimations from target to source tokens. If several target tokens align to the same source token, we again use the minimal confidence. Motivated by our autoencoding approach to measure similarity between training and test data, we investigate similar approaches to model the alignment between source and target tokens. In this case, we used a model to predict a target hidden state dj given a source state ei . If a source word aligns to a target word, it should be possible to predict this target word primarily based on th"
W19-8671,P16-1162,0,0.0289301,"tion in the original work. The number of layers is adapted for each task for the best performance possible and will be reported respectively. The autoencoders are implemented on top of the Transformer (with PyTorch (Paszke et al., 2017)) using one hidden layer with different sizes and sigmoid activation function. 3 The MT model is a 12-layer Transformer trained on the German-English TED corpus (Cettolo et al., 2012) with the development set and test set from the IWSLT 2017 evaluation campaign. The data is preprocessed with Moses tokenization, true-casing and segmented with byte-pair encoding (Sennrich et al., 2016) with 40K codes. The model achieves a BLEU score of 28.82 on the development set and 30.63 on the test data. We conducted further ASR experiments on the Switchboard-1 Release 2 (LDC97S62) corpus, which contains over 300 hours of speech. The Hub5’00 evaluation data (LDC2002S09) was used as our test set. On this set, we are especially inter2 If a word in the reference was deleted, we marked the previous and next word also as an erroneous word. 3 579 https://github.com/isl-mt/NMTGMinor aged to get the best performance in all measures. While we see a drop in performance due to the approximation, e"
W19-8671,W18-6451,0,0.172443,"Missing"
