2020.acl-main.253,W19-5206,0,0.126322,"nerations. Finally, we recommend to improve automatic evaluation by complementing BLEU with a language model score which can better assess fluency in the target language while avoiding the artifacts of translationese references. 2 Related Work Back-translation has been originally introduced for phrase-based machine translation (Bojar and Tamchyna, 2011). For back-translation with neural machine translation, there is a large body of literature building upon the seminal work of Sennrich et al. (2016a), from large-scale extensions with sampling (Edunov et al., 2018; Ott et al., 2018) or tagging (Caswell et al., 2019) to its use for unsupervised machine translation (Lample et al., 2018) as well as analysis (Poncelas et al., 2018b) and iterative versions (Hoang et al., 2018). More similar to our work, Toral et al. (2018) analyzed performance of trained state-of-the-art NMT systems in direct and reverse mode. They observe that translationese is simpler to translate and claimed that gains for such systems mostly come from improvements in the reverse direction. Concurrent to our work, Graham et al. (2019) find that automatic evaluation with BLEU does not align with the hypothesis that reverse sentences are eas"
2020.acl-main.253,D18-1045,1,0.939413,"ces because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency. 1 Introduction Back-translation (BT; Bojar and Tamchyna 2011; Sennrich et al. 2016a; Poncelas et al. 2018a) is a data augmentation method that is a key ingredient for improving translation quality of neural machine translation systems (NMT; Sutskever et al. 2014; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017). NMT systems using largescale BT have been ranked top at recent WMT evaluation campaigns (Bojar et al., 2018; Edunov et al., 2018; Ng et al., 2019). The idea is to train a target-to-source model to generate additional synthetic parallel data from monolingual target data. The resulting sentence pairs have synthetic sources and natural targets which are then added to the original bitext in order to train the desired sourceto-target model. BT improves generalization and Michael Auli can be used to adapt models to the test domain by adding appropriate monolingual data. Parallel corpora are usually comprised of two types of sentence-pairs: sentences which originate in the source language and have been translated by humans in"
2020.acl-main.253,W18-2703,0,0.0227386,"language while avoiding the artifacts of translationese references. 2 Related Work Back-translation has been originally introduced for phrase-based machine translation (Bojar and Tamchyna, 2011). For back-translation with neural machine translation, there is a large body of literature building upon the seminal work of Sennrich et al. (2016a), from large-scale extensions with sampling (Edunov et al., 2018; Ott et al., 2018) or tagging (Caswell et al., 2019) to its use for unsupervised machine translation (Lample et al., 2018) as well as analysis (Poncelas et al., 2018b) and iterative versions (Hoang et al., 2018). More similar to our work, Toral et al. (2018) analyzed performance of trained state-of-the-art NMT systems in direct and reverse mode. They observe that translationese is simpler to translate and claimed that gains for such systems mostly come from improvements in the reverse direction. Concurrent to our work, Graham et al. (2019) find that automatic evaluation with BLEU does not align with the hypothesis that reverse sentences are easier to translate instead. Unfortunately, their findings are not very conclusive because they do not control for the change of actual content, as sentences in o"
2020.acl-main.253,W04-3250,0,0.287751,"and human preference judgements on four language directions with a bitext-only model as well as a back-translation model (BT). BLEU shows no strong preference when the source is natural text (X) but professional human translators prefer BT regardless of whether the source is X or translationese (X ∗ ). Backtranslation also does not overproportionally benefit from inputting translationese since both OP and BT show similar improvements when switching from X to X ∗∗ inputs. BT human scores are statistically significantly better at p=0.05 than the respective OP as per paired bootstrap resampling (Koehn, 2004). sentences; X ∗ → Y ) and the forward direction (source original sentences; X → Y ∗ ). Table 1 shows that BT does not improve over OP on direct sentences (X → Y ∗ ) in aggregate. However, on the reverse portion BT does improve, and it does so by very large margins of between 5.7-10.1 BLEU. Appendix C shows that TER (Snover et al., 2006), BEER (Stanojevic and Sima’an, 2014), METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019) also do not distinguish very strongly between OP and BT for direct sentences. A possible explanation for this result is that BT can better translate targ"
2020.acl-main.253,P11-1132,0,0.0988759,"BLEU is substantially higher when the input is translationese (X ∗∗ ) compared to natural language (X), however, both BT and OP obtain comparable improvements. Therefore, the BLEU discrepancy between BT and OP in direct vs. reverse cannot be explained by BT gaining an advantage over OP through translationese inputs. 4.3 4.2 Translationese Benefits Both BT & OP Translationese is known to be a different dialect with lower complexity than naturally occurring text (Toral et al., 2018). This is corroborated by the fact that this data is straightforward to identify by simple automatic classifiers (Koppel and Ordan, 2011). One possible explanation for why back-translation could be more effective for target original sentences is that the input to the system is translated language. This may give the BT system two advantages: i) the input is simpler than Human Evaluation Contradicts BLEU The aforementioned experiments were evaluated in terms of BLEU, an automatic metric. To get a more complete picture, we ask professional human translators to judge translations using source-based direct assessment (unless otherwise specified, this is our default type of human evaluation; see §3.4). Table 3 (first two sets of rows"
2020.acl-main.253,2009.mtsummit-papers.9,0,0.0377443,"extracted from documents which are just harder to translate. In this work we correct for this effect by comparing translations of source original sentences with their double translations. Graham et al. (2019) also observe that BLEU does not reliably correlate with human judgements. While they consider a large variety of systems trained in various ways, we instead focus on the comparison between the same NMT system trained with and without back-translated data. Earlier work on statistical machine translation models argued in favor of using source original data only to train translation models (Kurokawa et al., 2009), language models for translation (Lembersky et al., 2011), and to tune translation models (Stymne, 2017). All these studies base most of their conclusions on automatic evaluation with BLEU, which is problematic since BLEU is not reliable and this procedure may overly optimize towards translationese references. Freitag et al. (2019) proposed a post-editing method to turn translationese system outputs into more natural text. As part of their evaluation, they also observed that human assessments poorly correlate with BLEU. While we confirm some of these observations, our goal is an in-depth anal"
2020.acl-main.253,D11-1034,0,0.0281937,"te. In this work we correct for this effect by comparing translations of source original sentences with their double translations. Graham et al. (2019) also observe that BLEU does not reliably correlate with human judgements. While they consider a large variety of systems trained in various ways, we instead focus on the comparison between the same NMT system trained with and without back-translated data. Earlier work on statistical machine translation models argued in favor of using source original data only to train translation models (Kurokawa et al., 2009), language models for translation (Lembersky et al., 2011), and to tune translation models (Stymne, 2017). All these studies base most of their conclusions on automatic evaluation with BLEU, which is problematic since BLEU is not reliable and this procedure may overly optimize towards translationese references. Freitag et al. (2019) proposed a post-editing method to turn translationese system outputs into more natural text. As part of their evaluation, they also observed that human assessments poorly correlate with BLEU. While we confirm some of these observations, our goal is an in-depth analysis of the evaluation of NMT systems trained with backtra"
2020.acl-main.253,P12-3005,0,0.0327114,"50 words as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. For back-translation, we use the same setup as the WMT’18 winning entry for this language pair which entails sampled back-translation of 226M German newscrawl sentences (Edunov et al., 2018).1 For De-En, En-Ru, Ru-En we use all parallel data provided by the WMT’19 news translation task, including Paracrawl. We remove sentences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5 and sentences which are not in the correct language (Lui and Baldwin, 2012). This resulted in 27.7M sentence-pairs for En-De and 26M for EnRu. For the back-translation models we use the top ranked Facebook-FAIR systems of the WMT’19 news shared translation task.2 The parallel data and pre-processing of those systems is identical to our baselines which are trained only on parallel data (Ng et al., 2019). As monolingual data, the WMT’19 newscrawl data was filtered by langid, resulting in 424M English and 76M Russian monolingual sentences. For En-De and De-En models use a joined byte-pair encoding (BPE; Sennrich et al. 2016b) with 32K split operations, and for En-Ru and"
2020.acl-main.253,W19-5302,0,0.0195617,"r match with the back-translated training sentences in this case. Second, it may further reduce 2836 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2836–2846 c July 5 - 10, 2020. 2020 Association for Computational Linguistics our confidence in automatic evaluation, if human judges disagree with BLEU for systems trained with back-translation. Indeed, human evaluations of top performing systems at WMT’18 (Bojar et al., 2018) and WMT’19 (Bojar et al., 2019) did not agree with BLEU to the extent that correlation is even negative for the top entries (Ma et al., 2019). In this paper, we shed light on the following questions. First, do BT systems only work better in the reverse direction? Second, does BLEU reflect human assessment for BT models? And if that is not the case, why not and how can we alleviate the weaknesses of BLEU? Our contribution is an extensive empirical evaluation of top-performing NMT systems to validate or disproof some of the above conjectures. First, we show that translationese sources are indeed easier to translate, but this is true for both NMT systems trained with and without back-translated data. Second, we confirm that human asse"
2020.acl-main.253,W19-5333,1,0.868416,"s are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency. 1 Introduction Back-translation (BT; Bojar and Tamchyna 2011; Sennrich et al. 2016a; Poncelas et al. 2018a) is a data augmentation method that is a key ingredient for improving translation quality of neural machine translation systems (NMT; Sutskever et al. 2014; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017). NMT systems using largescale BT have been ranked top at recent WMT evaluation campaigns (Bojar et al., 2018; Edunov et al., 2018; Ng et al., 2019). The idea is to train a target-to-source model to generate additional synthetic parallel data from monolingual target data. The resulting sentence pairs have synthetic sources and natural targets which are then added to the original bitext in order to train the desired sourceto-target model. BT improves generalization and Michael Auli can be used to adapt models to the test domain by adding appropriate monolingual data. Parallel corpora are usually comprised of two types of sentence-pairs: sentences which originate in the source language and have been translated by humans into the target lang"
2020.acl-main.253,N19-4009,1,0.856288,"on of (X, Y ∗ ) pairs (direct mode) and (X ∗ , Y ) (reverse mode). According to BLEU, a system trained with BT improves only in reverse mode. As part of this study we have collected double translations, which are useful to assess whether translationese inputs are easier to translate (by comparing performance when the input is X ∗∗ versus X and the reference is Y ∗ ) and easier to predict (by comparing performance when the reference is Y ∗∗ versus Y and the input is X ∗ ). 3.2 Sequence to Sequence Models We train models using the big Transformer implementation of fairseq (Vaswani et al., 2017; Ott et al., 2019). All our models are trained on 128 Volta GPUs, following the setup described in Ott et al. (2018). For En-De we used single Transformer Big models without checkpoint averaging. For De-En and En-Ru we increased model capacity by using larger FFN size (8192) and we also used an ensemble of models trained with three different seeds. In the remainder of this paper, we will refer to baseline NMT models trained only on parallel data as OP, and to models trained on both parallel data and back-translated data as BT. 3.3 Test sets and Reference Collection In order to assess differences in model perfor"
2020.acl-main.253,W18-6301,1,0.93845,"human preference towards BT generations. Finally, we recommend to improve automatic evaluation by complementing BLEU with a language model score which can better assess fluency in the target language while avoiding the artifacts of translationese references. 2 Related Work Back-translation has been originally introduced for phrase-based machine translation (Bojar and Tamchyna, 2011). For back-translation with neural machine translation, there is a large body of literature building upon the seminal work of Sennrich et al. (2016a), from large-scale extensions with sampling (Edunov et al., 2018; Ott et al., 2018) or tagging (Caswell et al., 2019) to its use for unsupervised machine translation (Lample et al., 2018) as well as analysis (Poncelas et al., 2018b) and iterative versions (Hoang et al., 2018). More similar to our work, Toral et al. (2018) analyzed performance of trained state-of-the-art NMT systems in direct and reverse mode. They observe that translationese is simpler to translate and claimed that gains for such systems mostly come from improvements in the reverse direction. Concurrent to our work, Graham et al. (2019) find that automatic evaluation with BLEU does not align with the hypothe"
2020.acl-main.253,P02-1040,0,0.109277,"mpared to naturally occurring text (Baker, 1993; Zhang and Toral, 2019; Toury, 2012). Several recent studies found that such reverse test sentences are easier to translate than direct sentences (Toral et al., 2018; Graham et al., 2019), and human judges consistently assign higher ratings to translations of target original sentences than to source original sentences. These studies therefore recommend to restrict test sets to source original sentences, a methodology which has been adopted by the 2019 edition of the WMT news translation shared task. Unfortunately, automatic evaluation with BLEU (Papineni et al., 2002) only weakly correlates with human judgements (Graham et al., 2019). Furthermore, recent WMT submissions relying heavily on back-translation mostly improved BLEU on the reverse direction with little gains on the direct portion (Toral et al. 2018; Barry Haddow’s personal communication and see also Appendix A, Table 7; Freitag et al. 2019). This finding is concerning for two reasons. First, back-translation may not be effective after all since gains are limited to the reverse portion. Improvements on reverse sentences may only be due to a better match with the back-translated training sentences"
2020.acl-main.253,W18-6319,0,0.034746,"than 30 points. Evaluation was blind and randomized: human raters did not know the identity of the systems and all outputs were shuffled to ensure that each rater provides a similar number of judgements for each system. Following the WMT shared task evaluation (Bojar et al., 2018), we normalize the scores of each rater by the mean and standard deviation of all ratings provided by the rater. Next, we average the normalized ratings for each sentence and average all per-sentence scores to produce an aggregate per-system z-score. As automatic metric, we report case-sensitive BLEU using SacreBLEU (Post, 2018).3 We also consider other metrics in Appendix C, but conclusions remain the same. 4 4.1 Results Evaluating BT with Automatic Metrics We first reproduce the known discrepancy between BT and OP in the reverse direction (target original 3 SacreBLEU signature: BLEU+case.mixed+numrefs.1+ smooth.exp+tok.13a+version.1.3.1 2839 src ref sys en-de BLEU human de-en BLEU human en-ru BLEU human ru-en BLEU human X Y∗ OP BT 33.7 32.3 -0.18 -0.05 40.3 38.6 -0.07 0.03 31.3 31.9 -0.66 -0.35 43.8 41.2 -0.37 -0.12 X∗ Y OP BT 31.3 38.9 -0.01 0.10 43.0 48.7 0.06 0.13 40.5 50.6 0.06 0.16 31.8 40.3 -0.02 0.07 X ∗∗ Y∗"
2020.acl-main.253,P16-1009,0,0.764457,"at this conjecture is not empirically supported and that backtranslation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency. 1 Introduction Back-translation (BT; Bojar and Tamchyna 2011; Sennrich et al. 2016a; Poncelas et al. 2018a) is a data augmentation method that is a key ingredient for improving translation quality of neural machine translation systems (NMT; Sutskever et al. 2014; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017). NMT systems using largescale BT have been ranked top at recent WMT evaluation campaigns (Bojar et al., 2018; Edunov et al., 2018; Ng et al., 2019). The idea is to train a target-to-source model to generate additional synthetic parallel data from monolingual target data. The resulting sentence pairs have synthetic sources and natural targets which are"
2020.acl-main.253,P16-1162,0,0.859892,"at this conjecture is not empirically supported and that backtranslation improves translation quality of both naturally occurring text as well as translationese according to professional human translators. We provide empirical evidence to support the view that back-translation is preferred by humans because it produces more fluent outputs. BLEU cannot capture human preferences because references are translationese when source sentences are natural text. We recommend complementing BLEU with a language model score to measure fluency. 1 Introduction Back-translation (BT; Bojar and Tamchyna 2011; Sennrich et al. 2016a; Poncelas et al. 2018a) is a data augmentation method that is a key ingredient for improving translation quality of neural machine translation systems (NMT; Sutskever et al. 2014; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017). NMT systems using largescale BT have been ranked top at recent WMT evaluation campaigns (Bojar et al., 2018; Edunov et al., 2018; Ng et al., 2019). The idea is to train a target-to-source model to generate additional synthetic parallel data from monolingual target data. The resulting sentence pairs have synthetic sources and natural targets which are"
2020.acl-main.253,2006.amta-papers.25,0,0.258224,"ot overproportionally benefit from inputting translationese since both OP and BT show similar improvements when switching from X to X ∗∗ inputs. BT human scores are statistically significantly better at p=0.05 than the respective OP as per paired bootstrap resampling (Koehn, 2004). sentences; X ∗ → Y ) and the forward direction (source original sentences; X → Y ∗ ). Table 1 shows that BT does not improve over OP on direct sentences (X → Y ∗ ) in aggregate. However, on the reverse portion BT does improve, and it does so by very large margins of between 5.7-10.1 BLEU. Appendix C shows that TER (Snover et al., 2006), BEER (Stanojevic and Sima’an, 2014), METEOR (Banerjee and Lavie, 2005) and BERTScore (Zhang et al., 2019) also do not distinguish very strongly between OP and BT for direct sentences. A possible explanation for this result is that BT can better translate target-original test sentences because those sentences mimic the training data of BT. The BT training data (§3) consists largely of target original sentences-pairs with back-translated sources which could explain the discrepancy between performance of the BT system on the direct and reverse portions. naturally occurring text and ii) this set"
2020.acl-main.253,W14-3354,0,0.0496051,"Missing"
2020.acl-main.253,W17-0230,0,0.0133725,"translations of source original sentences with their double translations. Graham et al. (2019) also observe that BLEU does not reliably correlate with human judgements. While they consider a large variety of systems trained in various ways, we instead focus on the comparison between the same NMT system trained with and without back-translated data. Earlier work on statistical machine translation models argued in favor of using source original data only to train translation models (Kurokawa et al., 2009), language models for translation (Lembersky et al., 2011), and to tune translation models (Stymne, 2017). All these studies base most of their conclusions on automatic evaluation with BLEU, which is problematic since BLEU is not reliable and this procedure may overly optimize towards translationese references. Freitag et al. (2019) proposed a post-editing method to turn translationese system outputs into more natural text. As part of their evaluation, they also observed that human assessments poorly correlate with BLEU. While we confirm some of these observations, our goal is an in-depth analysis of the evaluation of NMT systems trained with backtranslated data. We provide empirical evidence cor"
2020.acl-main.253,W18-6312,0,0.375446,"into the target language, or sentences which originate from the target language and have been translated into the source language. We refer to the former as the direct portion and the latter as the reverse portion. The setup we are ultimately interested in is models that translate direct sentences. Translations produced by human translators, or translationese tend to be simpler and more standardized compared to naturally occurring text (Baker, 1993; Zhang and Toral, 2019; Toury, 2012). Several recent studies found that such reverse test sentences are easier to translate than direct sentences (Toral et al., 2018; Graham et al., 2019), and human judges consistently assign higher ratings to translations of target original sentences than to source original sentences. These studies therefore recommend to restrict test sets to source original sentences, a methodology which has been adopted by the 2019 edition of the WMT news translation shared task. Unfortunately, automatic evaluation with BLEU (Papineni et al., 2002) only weakly correlates with human judgements (Graham et al., 2019). Furthermore, recent WMT submissions relying heavily on back-translation mostly improved BLEU on the reverse direction with"
2020.acl-main.253,W19-5208,0,0.0454028,"corpora are usually comprised of two types of sentence-pairs: sentences which originate in the source language and have been translated by humans into the target language, or sentences which originate from the target language and have been translated into the source language. We refer to the former as the direct portion and the latter as the reverse portion. The setup we are ultimately interested in is models that translate direct sentences. Translations produced by human translators, or translationese tend to be simpler and more standardized compared to naturally occurring text (Baker, 1993; Zhang and Toral, 2019; Toury, 2012). Several recent studies found that such reverse test sentences are easier to translate than direct sentences (Toral et al., 2018; Graham et al., 2019), and human judges consistently assign higher ratings to translations of target original sentences than to source original sentences. These studies therefore recommend to restrict test sets to source original sentences, a methodology which has been adopted by the 2019 edition of the WMT news translation shared task. Unfortunately, automatic evaluation with BLEU (Papineni et al., 2002) only weakly correlates with human judgements (G"
2020.wmt-1.69,W11-2138,0,0.0328909,"anslation. Unfortunately, na¨ıve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique"
2020.wmt-1.69,J93-2003,0,0.10393,"ults on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the arrival of neural sequence to sequence models (Brown et al., 1993; Koehn et al., 2003). Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival (Yu et al., 2017; Yee et al., 2019), it has been an important part in the winning entries of several high resource language pairs at WMT 2019 (Ng et al., 2019), improving over strong ensembles that used 500M back-translated sentences. At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry (Chen et al., 2019). Noisy channel modeling turns text generation on the head: ins"
2020.wmt-1.69,P19-4007,0,0.0422076,"Missing"
2020.wmt-1.69,N19-1423,0,0.0221116,"cient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the"
2020.wmt-1.69,N19-1409,1,0.816928,"d data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the arrival of neural sequence to sequence models (Brown et al., 1993; Koehn et al., 2003). Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival (Yu et al., 2017; Yee et al., 2019), it has been an important part in the winning entries"
2020.wmt-1.69,D18-1045,1,0.9197,"eling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation liter"
2020.wmt-1.69,P07-2045,0,0.0134379,"that both models score the exact same units during beam search. For Romanian-English (Ro-En), we train on WMT’16 training data, comprising 612K sentence pairs, validate on newsdev2016 and test on newstest2016. We learn a joint BPE vocabulary of 18K types on the bitext training data which is used for both the source and target. Different to GermanEnglish, we learn a joint BPE vocabulary to enable sharing the source and target embeddings which we found to perform better for Romanian-English in early experiments. 4.2 as the language model training data are preprocessed with the Moses tokenizer (Koehn et al., 2007). We normalize punctuation and remove nonprinting characters. Romanian-English data is preprocessed following Sennrich et al. (2016a) by applying Moses tokenization and special normalization for Romanian text.1 Language Models For German-English, we train a sentence-level English Transformer language model with 16 layers and Transformer-Big architecture (Vaswani et al., 2017; Radford et al., 2018). The model is trained on de-duplicated English Newscrawl data from 2007-2018 comprising 186 million sentences or 4.5B words after normalization and tokenization. We use a BPE vocabulary of 24K types"
2020.wmt-1.69,N03-1017,0,0.0512003,"labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the arrival of neural sequence to sequence models (Brown et al., 1993; Koehn et al., 2003). Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival (Yu et al., 2017; Yee et al., 2019), it has been an important part in the winning entries of several high resource language pairs at WMT 2019 (Ng et al., 2019), improving over strong ensembles that used 500M back-translated sentences. At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry (Chen et al., 2019). Noisy channel modeling turns text generation on the head: instead of modeling an o"
2020.wmt-1.69,2020.tacl-1.47,1,0.91466,"tions to noisy channel decoding scale much better in terms of speed as the beam size increases. Figure 3 compares the accuracy of fast noisy channel decoding at larger beam sizes with that of na¨ıve noisy channel decoding. Using the big and big 1 1 channel models gives the best performance across all beam sizes for na¨ıve noisy channel decoding. With fast noisy channel decoding, we see an average drop of 0.3 BLEU and 0.2 BLEU for big and big 1 1 respectively. On the other hand, for smaller channel models, the difference between na¨ıve and fast noisy Generation Time (secs) (log scale) mBART02 (Liu et al., 2020), the previous stateof-the-art result on Romanian-English with backtranslation, we achieve a 0.5 BLEU improvement. We use a similar number of total model parameters, but much less monolingual English data. Our English language model is trained on 4.5B tokens, while mBART02 uses 66B tokens of English and Romanian monolingual data. 3000 1000 500 Finally, Table 5 shows that fast approximations and smaller channel models achieve similar performance but much higher speed compared to na¨ıve noisy channel decoding on WMT RomanianEnglish with back-translation. Fast noisy channel decoding with base 1 1"
2020.wmt-1.69,P12-3005,0,0.0410357,"ootprint of small channel models and enables the use of much larger batch sizes which leads to faster inference as we will see in § 5. 3.3 Reducing the Number of Candidates We also study the effect of reducing the number of next token candidates k2 scored for each beam at each step of beam search. This reduces the computation as well as memory overhead of channel model scoring. 586 4 4.1 Experimental Setup Datasets We consider two datasets for our experiments: For German-English (De-En), we train on WMT’19 training data. Following (Ng et al., 2019), we apply language identification filtering (Lui and Baldwin, 2012) and remove sentences longer than 250 tokens as well as sentence pairs with a source/target length ratio exceeding 1.5. This results in 26.8M sentence pairs. We validate on newstest2016 and test on newstest2014, newstest2015, newstest2017, and newstest2018. For all models, the source vocabulary is a 24K byte pair encoding (BPE; Sennrich et al., 2016) learned on the source portion of the bitext. For the target side, we use the vocabulary of the language model (§4.2) so that both models score the exact same units during beam search. For Romanian-English (Ro-En), we train on WMT’16 training data,"
2020.wmt-1.69,P16-2021,0,0.0240159,"x blocks each in the encoder and the decoder, except for models ending in ” 1 1” which have only a single block in the encoder and the decoder. channel models have an embedding dimension of just 32. To address this issue, we make use of the fact that we know exactly which input tokens need to be scored (since the input sequence is given) instead of computing probabilities for the entire vocabulary. This is similar to vocabulary reduction techniques used for early neural sequence to sequence models, and it is particularly convenient since we know exactly which tokens are in the input sequence (Mi et al., 2016; L’Hostis et al., 2016). Similar to prior work on vocabulary reduction, we found it useful to not just score the input words but also a subset of the most frequent words in the vocabulary. Specifically, for each batch, we enumerate all input word types, add the 500 most frequent types and then compute output probabilities for this subset with the channel model. The number of output probabilities calculated is typically at least one order of magnitude smaller than the full vocabulary, as shown in § 5.4.1. This approach substantially reduces the memory footprint of small channel models and enab"
2020.wmt-1.69,W19-5333,1,0.782764,"r text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the arrival of neural sequence to sequence models (Brown et al., 1993; Koehn et al., 2003). Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival (Yu et al., 2017; Yee et al., 2019), it has been an important part in the winning entries of several high resource language pairs at WMT 2019 (Ng et al., 2019), improving over strong ensembles that used 500M back-translated sentences. At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry (Chen et al., 2019). Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes’ rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model. This enables the effective use of strong language models trained on lar"
2020.wmt-1.69,N19-4009,1,0.893688,"Missing"
2020.wmt-1.69,W18-6301,1,0.926089,"velopment set (Yee et al., 2019; Ng et al., 2019). Hyperparameters are sampled within the interval [0, 2], For direct models (dir), we sample ten random weights for the length penalty. For direct models combined with language models (dir + lm), we evaluate 100 randomly sampled configurations for the length penalty and the language model weight (λ2 ). For direct models combined with language models and channel models (dir + lm + ch), we evaluate 1000 configurations of the length penalty, the language model weight (λ2 ) and the channel model weight (λ1 ). We use 16-bit floating point precision (Ott et al., 2018, 2019) for decoding with the online noisy channel setup. Accuracy is measured via sacreBLEU (Post, 2018) for WMT German-English. We report the average BLEU of the newstest2014-2015 and newstest2017-2018 test sets, averaged over 3 random seeds for model weight initialization. Speed 587 1 https://github.com/rsennrich/ wmt16-scripts/tree/master/preprocess Ensembles dir 2 dir 3 dir Ensembles + LMs dir + lm 2 dir + lm 3 dir + lm Total Params (M) BLEU Time (s) 283 565 848 38.8 39.3 39.5 20 40 59 539 822 1104 39.7 40.2 40.3 44 65 84 Noisy Channel Modeling (Yee et al., 2019) dir + lm + big 822 40.5 5"
2020.wmt-1.69,W18-6319,0,0.0362683,"direct models (dir), we sample ten random weights for the length penalty. For direct models combined with language models (dir + lm), we evaluate 100 randomly sampled configurations for the length penalty and the language model weight (λ2 ). For direct models combined with language models and channel models (dir + lm + ch), we evaluate 1000 configurations of the length penalty, the language model weight (λ2 ) and the channel model weight (λ1 ). We use 16-bit floating point precision (Ott et al., 2018, 2019) for decoding with the online noisy channel setup. Accuracy is measured via sacreBLEU (Post, 2018) for WMT German-English. We report the average BLEU of the newstest2014-2015 and newstest2017-2018 test sets, averaged over 3 random seeds for model weight initialization. Speed 587 1 https://github.com/rsennrich/ wmt16-scripts/tree/master/preprocess Ensembles dir 2 dir 3 dir Ensembles + LMs dir + lm 2 dir + lm 3 dir + lm Total Params (M) BLEU Time (s) 283 565 848 38.8 39.3 39.5 20 40 59 539 822 1104 39.7 40.2 40.3 44 65 84 Noisy Channel Modeling (Yee et al., 2019) dir + lm + big 822 40.5 550 Fast Noisy Channel Modeling (This work) dir + lm + 16th 1 1 542 40.2 dir + lm + base 1 1 574 40.5 2 di"
2020.wmt-1.69,W16-2323,0,0.348964,"na¨ıve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical ma"
2020.wmt-1.69,P16-1009,0,0.383811,"na¨ıve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical ma"
2020.wmt-1.69,P16-1162,0,0.741819,"na¨ıve noisy channel modeling with modern sequence to sequence models is up to an order of magnitude slower than alternatives. We address this issue by introducing efficient approximations to make inference with the noisy channel approach as fast as strong ensembles while increasing accuracy. We also show that the noisy channel approach can outperform strong pretraining results by achieving a new state of the art on WMT Romanian-English translation. 1 Introduction Unlabeled data has been leveraged in many ways in natural language processing including backtranslation (Bojar and Tamchyna, 2011; Sennrich et al., 2016b; Edunov et al., 2018), self-training (He et al., 2020), or language model pre-training which led to improvements in many natural language tasks (Devlin et al., 2019). While pre-training has achieved impressive results on tasks where labeled data is limited, improvements in settings with abundant labeled data are modest (Raffel et al., 2020) with controlled studies showing a clear trend of diminishing returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical ma"
2020.wmt-1.69,W18-6321,0,0.0140296,"modeling an output sequence given an input, Bayes’ rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior probability of the output, typically a language model. This enables the effective use of strong language models trained on large amounts of unlabeled data. The role of the backward model, or the channel model, is to validate outputs preferred by the language model with respect to the input. A straightforward way to use language models is to pair them with standard sequence to sequence models (G¨ulc¸ehre et al., 2015; Stahlberg et al., 2018). However, this does not address explaining away effects under which modern neural sequence models still suffer (Klein and Manning, 2001; Li et al., 2019). As a consequence, models are susceptible to producing fluent outputs that are unrelated to the input (Li et al., 2019). The noisy channel approach explicitly addresses this via the channel model. However, a major obstacle to efficient noisy channel modeling is that generating outputs is much slower than decoding from a standard sequence to sequence model. We address this issue by introducing several simple yet highly ef584 Proceedings of th"
2020.wmt-1.69,2020.lrec-1.494,0,0.0806851,"Missing"
2020.wmt-1.69,D19-1571,1,0.277552,"returns as the amount of training data increases (Edunov et al., 2019). In this paper, we focus on noisy channel modeling for text generation tasks, a classical technique from the statistical machine translation literature which had been the workhorse of text gen† Work done while at Facebook during a Facebook AI Residency. eration tasks for decades before the arrival of neural sequence to sequence models (Brown et al., 1993; Koehn et al., 2003). Unlike pre-training approaches, this approach is very effective irrespective of the amount of labeled data: since a recent revival (Yu et al., 2017; Yee et al., 2019), it has been an important part in the winning entries of several high resource language pairs at WMT 2019 (Ng et al., 2019), improving over strong ensembles that used 500M back-translated sentences. At the low resource WAT 2019 machine translation competition, noisy channel modeling was also a key factor for the winning entry (Chen et al., 2019). Noisy channel modeling turns text generation on the head: instead of modeling an output sequence given an input, Bayes’ rule is applied to model the input given the output, via a backward sequence to sequence model which is combined with the prior pr"
2020.wmt-1.69,2020.tacl-1.23,0,0.0133058,"needs to be substantially reduced. This leads to slower inference on GPUs since less computation can be parallelized. 585 3 Fast Noisy Channel Modeling Na¨ıve online noisy channel modeling is significantly slower than standard direct models. In this section, we present approximations to make noisy channel modeling substantially faster. 3.1 Reducing Channel Model Size Prior work on neural noisy channel used channel models which were of the same size as the direct model (Yu et al., 2017; Yee et al., 2019). The most recent work uses standard Transformer models (Yee et al., 2019; Ng et al., 2019; Yu et al., 2020). In this study, we hypothesize that the primary role of the channel model is to avoid explaining away effects by the language model. This primarily entails assigning low scores to unrelated outputs, which may not require a very powerful model. In this case, we may be able to substantially decrease the size of the channel model at only a small loss in accuracy. Recent work demonstrates that direct models with shallow decoders can give comparable accuracy, while being faster at inference time, compared to models with deep decoders (Wu et al., 2019; Elbayad et al., 2020; Kasai et al., 2020; Fan"
2021.acl-long.331,2020.emnlp-main.19,0,0.0423297,"that modern NLP models are able to function with different numbers of layers for different examples (Elbayad et al., 2019; Fan et al., 2019; He et al., 2021); that different layers specialize for different purposes (Zhang et al., 2019); that layers can be compressed (Li et al., 2020; Zhu et al., 2019; Shen et al., 2020; Sun et al., 2020); and, that layers can be reordered (Press et al., 2019). There is a growing body of work in efficient self-attention networks (Tay et al., 2020b), such as linear attention (Wang et al., 2020), on how to process long context information (Beltagy et al., 2020; Ainslie et al., 2020) and on approximations to make transformers more scalable (Kitaev et al., 2020; Katharopoulos et al., 2020). BigBIRD (Zaheer et al., 2020) provides random keys as additional inputs to its attention mechanism. Locality sensitive hashing (LSH) as employed e.g. in Reformer (Kitaev et al., 2020) utilizes a fixed random projection. Random Feature Attention (Peng et al., 2021) uses random fea4301 ture methods to approximate the softmax function. Performer (Choromanski et al., 2020) computes the transformer’s multi-head attention weights as a fixed orthogonal random projection. Closely related to thi"
2021.acl-long.331,P19-1375,0,0.0295658,"spersed with regular transformer layers, and show improvements in wall-clock compute time until convergence, as well as overall performance, on various machine translation and (masked) language modelling tasks. 1 Introduction Transformers (Vaswani et al., 2017) have dominated natural language processing (NLP) in recent years, from large scale machine translation (Ott et al., 2018) to pre-trained (masked) language modeling (Devlin et al., 2018; Radford et al., 2018), and are becoming more popular in other fields as well, from reinforcement learning (Vinyals et al., 2019) to speech recognition (Baevski et al., 2019) and computer vision (Carion et al., 2020). Their success is enabled in part by ever increasing computational demands, which has naturally led to an increased interest in improving their efficiency. Scalability gains in transformers could facilitate bigger, deeper networks with longer contexts (Kitaev et al., 2020; Wang et al., 2020; Beltagy et al., 2020; Kaplan et al., 2020; Tay et al., 2020b). Conversely, improved efficiency could reduce environmental costs (Strubell et al., 2019) and hopefully help democratize the technology. In this work, we explore a simple question: if some layers of the"
2021.acl-long.331,D13-1170,0,0.0128778,"mance on SST-2 (left) and MultiNLI-matched (right). Model Max BLEU AUCC Train time Transformer 34.59 ± 0.11 114.57 ± 0.08 142.28 ± 1.87 T Reservoir 34.80 ± 0.07 115.26 ± 0.26 134.49 ± 1.70 Backskip Reservoir 34.75 ± 0.05 115.99 ± 0.23 119.54 ± 1.78 Table 3: Validation max BLEU, AUCC at 4h and wallclock time per epoch (averaged over multiple runs, in seconds) on IWSLT comparing backskipping with regular and reservoir transformers. and similar AUCC perplexity (see Appendix D). We then examine the performance of these models when fine-tuned on downstream tasks, specifically the well known SST-2 (Socher et al., 2013) and MultiNLI-matched (Williams et al., 2017) tasks. When fine-tuning the reservoir models, we keep the reservoir layers fixed (also fine-tuning them did not work very well, see Appendix D). Figure 2 shows the results of fine-tuning. We observe that the reservoir transformer outperforms normal RoBERTa at all depths in both tasks. At lower depth, the improvements are substantial. As a sanity check, we also experiment with freezing some of the layers in a regular pre-trained RoBERTa model during fine-tuning only (Transformer “frozen finetuned” in the Figure) and show that this helps a little but"
2021.acl-long.563,P05-1022,0,0.342637,"improve its generalization using pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output. 1 Introduction Reranking models take a number of different output hypotheses generated by a baseline model and select one hypothesis based on more powerful features. Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al., 2004; Shen et al., 2004). Traditional statistical models (SMT) based on n-gram counts made very strong independence assumptions where features would only capture very local context information to avoid sparsity and poor generalization. A large n-best list produced by these models would then be passed to a discriminatively trained reranker which leverages features engineered to capture more global context (Och et al., 2004) yielding significant improvements to the quality of the translations. On the other hand, modern neu"
2021.acl-long.563,J05-1003,0,0.200704,"sing pre-training and data augmentation techniques. Experiments on four WMT directions show that our discriminative reranking approach is effective and complementary to existing generative reranking approaches, yielding improvements of up to 4 BLEU over the beam search output. 1 Introduction Reranking models take a number of different output hypotheses generated by a baseline model and select one hypothesis based on more powerful features. Before the recent re-emergence of neural networks, these models have been well studied for several NLP tasks including parsing (Charniak and Johnson, 2005; Collins and Koo, 2005) and statistical machine translation (Och et al., 2004; Shen et al., 2004). Traditional statistical models (SMT) based on n-gram counts made very strong independence assumptions where features would only capture very local context information to avoid sparsity and poor generalization. A large n-best list produced by these models would then be passed to a discriminatively trained reranker which leverages features engineered to capture more global context (Och et al., 2004) yielding significant improvements to the quality of the translations. On the other hand, modern neural models (NMT) make mu"
2021.acl-long.563,P19-4007,0,0.0566599,"Missing"
2021.acl-long.563,kobus-etal-2017-domain,0,0.0205656,"BPE units for Ru-En separately, using the sentencepiece toolkit (Kudo and Richardson, 2018). All systems are evaluated using S ACRE BLEU (Post, 2018). 5.2 Baselines We use the Transformer (Vaswani et al., 2017) architecture and train MT models using bitext data only. These are the models that generate the n-best list, and which serve also as a lower bound for the performance of DrNMT. BT data is generated from beam decoding with beam size equal to 5. Since the bitext data of En-Ta originates from seven different sources, we prepend dataset tags to each source sentence to indicate the origin (Kobus et al., 2017). We do not prepend any tags on the validation and test sets when decoding, as this choice worked best during cross-validation. In general and for each language pair, we tune the model architecture and De-En En-De En-Ta Ru-En 326K 5.2K 11K 17M 326K 5.2K 11K 37M 621K 2K 1K 27M 28.9M 5.8K 8K 17M Table 1: Number of sentences in each dataset used in the experiments after pre-processing. all hyper-parameters on the validation set. In addition to beam decoding, we consider two reranking baselines. First, we consider the method recently introduced by Salazar et al. (2019). In its simplest formulation"
2021.acl-long.563,P10-2041,0,0.0605566,"itext and monolingual data shared by the WMT’20 news translation task for training, and the officially released development and test sets for validation and testing purposes. For Ru-En, we use all the parallel data from WMT’19 (Barrault et al., 2019) and NewsCrawl2018 as the monolingual dataset for training, validate on newstest2015 and 2016, and test on newstest 2017, 2018 and 2019. We follow the steps in Ng et al. (2019) for data preprocessing, including sentence deduplication, language identification filtering on all bitext and monolingual data (Joulin et al., 2017) and indomain filtering (Moore and Lewis, 2010) on Tamil CommonCrawl data. Table 1 shows the resulting size of each dataset. For the base NMT models, we learn 30K byte-pair encoding (BPE) units for De-En and En-De, 20K BPE units for En-Ta and 24K BPE units for Ru-En separately, using the sentencepiece toolkit (Kudo and Richardson, 2018). All systems are evaluated using S ACRE BLEU (Post, 2018). 5.2 Baselines We use the Transformer (Vaswani et al., 2017) architecture and train MT models using bitext data only. These are the models that generate the n-best list, and which serve also as a lower bound for the performance of DrNMT. BT data is g"
2021.acl-long.563,W18-6319,0,0.0193205,"stest 2017, 2018 and 2019. We follow the steps in Ng et al. (2019) for data preprocessing, including sentence deduplication, language identification filtering on all bitext and monolingual data (Joulin et al., 2017) and indomain filtering (Moore and Lewis, 2010) on Tamil CommonCrawl data. Table 1 shows the resulting size of each dataset. For the base NMT models, we learn 30K byte-pair encoding (BPE) units for De-En and En-De, 20K BPE units for En-Ta and 24K BPE units for Ru-En separately, using the sentencepiece toolkit (Kudo and Richardson, 2018). All systems are evaluated using S ACRE BLEU (Post, 2018). 5.2 Baselines We use the Transformer (Vaswani et al., 2017) architecture and train MT models using bitext data only. These are the models that generate the n-best list, and which serve also as a lower bound for the performance of DrNMT. BT data is generated from beam decoding with beam size equal to 5. Since the bitext data of En-Ta originates from seven different sources, we prepend dataset tags to each source sentence to indicate the origin (Kobus et al., 2017). We do not prepend any tags on the validation and test sets when decoding, as this choice worked best during cross-validation. In"
2021.acl-long.563,P16-1009,0,0.102075,"Language Processing, pages 7250–7264 August 1–6, 2021. ©2021 Association for Computational Linguistics reranker has hundreds of millions of parameters yet it receives only one gradient and weight update per source/target sentence pair as opposed to one per token as for standard NMT models. In our work, we mitigate overfitting in two ways. First, we leverage the success of pre-training by finetuning masked language models (MLM; Devlin et al. 2019) which initializes the model with features trained on much more training data. Second, we augment the original dataset with back-translated data (BT; Sennrich et al. 2016). Experiments show that DrNMT can match the performance of a strong NCD baseline and that their combination leads to further improvements as measured by BLEU, TER and also human evaluation. 2 Related Work Our method is inspired by the seminal work of Shen et al. (2004) and Och et al. (2004) who introduced and popularized discriminative reranking to SMT. Besides using a weaker MT system to generate the n-best list, these works relied on a linear discriminator trained on human-designed features as opposed to a transformer taking the raw source sentence and hypothesis. Most work using NMT has foc"
2021.acl-long.563,W19-5333,1,0.921925,"on model outputs. Second, beam search with autoregressive models uses the chain rule to sum individual token-level probabilities to obtain a target sequence probability. However, individual probabilities are based on a limited amount of target context, while a reranking model can condition on the entire target context. Indeed, recent generative reranking approaches applied to NMT, such as Noisy-Channel Decoding (NCD, Yee et al. 2019) which leverages a pre-trained language model and a backward model, show strong improvements over beam search outputs, as demonstrated in recent WMT evaluations (Ng et al., 2019). In this paper, we explore whether training large transformer models using the reranking objective can further improve performance. Our model, dubbed DrNMT, takes as input the entire source sentence and an n-best list of output hypotheses to predict a distribution of sentence-level evaluation scores, such as BLEU.1 This setup is similar to earlier work with SMT, except that the baseline model is an NMT model and the reranker is a big transformer architecture as opposed to a log-linear model on top of discrete or human engineered features. Unfortunately, optimizing for the task of interest doe"
2021.acl-long.563,N04-1023,0,0.11739,"odels. In our work, we mitigate overfitting in two ways. First, we leverage the success of pre-training by finetuning masked language models (MLM; Devlin et al. 2019) which initializes the model with features trained on much more training data. Second, we augment the original dataset with back-translated data (BT; Sennrich et al. 2016). Experiments show that DrNMT can match the performance of a strong NCD baseline and that their combination leads to further improvements as measured by BLEU, TER and also human evaluation. 2 Related Work Our method is inspired by the seminal work of Shen et al. (2004) and Och et al. (2004) who introduced and popularized discriminative reranking to SMT. Besides using a weaker MT system to generate the n-best list, these works relied on a linear discriminator trained on human-designed features as opposed to a transformer taking the raw source sentence and hypothesis. Most work using NMT has focused on generative reranking methods (Liu et al., 2018; Imamura and Sumita, 2017; Wang et al., 2017), where the reranker’s parameters are optimized using a criterion which is different from the metric of interest. For instance, Yu et al. (2017); Yee et al. (2019) perfo"
2021.acl-long.563,P03-1021,0,0.273305,"r projection block to map each d dimensional vector to a single score as before, yielding n scores for reranking. This design enables the model to have set-level information during reranking, and thus the scoring has to be performed on the full set at once. Table 7 shows that these two model variants perform the same, suggesting that set level representations may need to be captured at a lower layer of the transformer. We leave this avenue of exploration for future work. 7 Conclusions Reranking is effective for both SMT and NMT. Inspired by work done almost two decades ago (Shen et al., 2004; Och, 2003), we studied discriminative reranking for NMT and found that it performs at least as well as the strongest generative reranking method we are aware of, namely noisy channel decoding (NCD) (Yee et al., 2019) - as long as care is taken to alleviate overfitting. There is a subtle trade-off between improvements stemming from optimizing the end metric and addressing exposure bias on the one hand, and poor generalization and sample inefficiency of discriminative training on the other hand. In this study we regularize the reranker by using dropout, by pre-training on large corpora and by performing d"
2021.acl-long.563,2006.amta-papers.25,0,0.0478775,"6.2 Table 3: Average validation and test BLEU and TER on WMT‘19 De-En with beam size 50 from rerankers trained with different metrics (B: BLEU, T: TER). guage pair, in which the baseline NMT is weak and none of the reranking approaches work nearly as well as in the other language directions. The difference between validation and test BLEU scores suggests also a certain degree of overfitting to the validation set. Despite this, our reranker still yields the largest improvement over beam. Appendix B shows similar trends when test performance is measured in terms of translation error rate (TER) (Snover et al., 2006), showing that DrNMT is not particularly overfitting to the training metric. Human evaluation: We randomly sample 750 sentences from the De-En test sets and collect human ratings. We perform A/B testing, where a rater can see the source sentence together with translated sentences from two systems. We conduct two rounds of human evaluation by comparing the proposed ”DrNMT + NCD” vs. ”beam”, and ”DrNMT + NCD” vs. ”NCD”. For each sentence, we collect three ratings (between 0 to 100) and average the scores, treating sentences with a score difference less than 5 as equally good. Out of the 750 sent"
2021.acl-long.68,N18-1008,0,0.0215287,"e adapted to the speech encoder output. Combined with LayerNorm parameter is the proposed LNA-Minimalist finetuning. In addition, we also investigate the role of self attention (SA) parameters in facilitating crosslingual transfer ability. 2.4 Methods Pretrained Modules 3 3.1 Length Adaptor We add a lightweight adaptor module in between encoder and decoder to better align the two modJoint Speech-text Finetuning Multi-task learning has been shown as an effective approach to improve the performance of the speech translation task using other related tasks, such as MT and ASR (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Tang et al., 2021a,b). We jointly train MT and ST tasks in the finetuning with pretrained models. The speech transcripts are used as input for the MT task and the corresponding speech data is used as input for the ST task. As a result, we can leverage abundant parallel text data to further improve the performance. Our model leverages a pretrained wav2vec 2.0 (Baevski et al., 2020) as encoder for acoustic modeling, a pretrained multilingual BART (mBART) (Liu et al., 2020) as decoder for language modeling. Both models are pretrained on unlabelled data via self-supervised le"
2021.acl-long.68,N19-1006,0,0.198698,"cept English to and from “Ca” and “Cy”. We fine-tune all parameters in this experiments due to the large mismatch of the pretrained model (mBART encoder as part of the speech encoder) and more available training data. 829 3.3 Baselines From scratch: The first baseline trains a sequenceto-sequence model with Transformer architecture without any pretraining.For CoVoST 2 experiments, we use the same model configuration as is provided by (Wang et al., 2020b). ASRPT+Multi: Pretraining encoder on ASR task was shown to be an effective method to improve speech translation and accelerates convergence (Bansal et al., 2019). We compare our results to a strong baseline provided by (Wang et al., 2020b), consisting of a multilingual Transformer model trained on CoVoST 2 with multilingual ASR pretraining (ST). For the Europarl ST many-to-many baseline, we use Transformer architecture with 12layer encoder, 6-layer decoder, and trained on all 30 directions. To provide the strongest baseline, encoder was pre-trained on LibriSpeech English ASR). XMEF-BL: Multilingual models for En-X (oneto-many) usually face more challenges from interference as they were found to underperform the bilingual counterparts (Arivazhagan et a"
2021.acl-long.68,P19-4007,0,0.0397334,"Missing"
2021.acl-long.68,N19-1423,0,0.21309,"xt translation via transfer learning from efficient finetuning of single-modality pretrained models. The proposed LNA finetuning is applied to each layer. Yang et al., 2019; Tang et al., 2020). In this paper, we advance the one-model-for-all paradigm further by adapting audio and multilingual text pretraining and finetuning to improve multilingual speech-totext translation. Our contributions are as follows: Introduction Recent advances in pretraining over unlabeled data and then finetuning on labeled data leads to significant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018). Lately, such text pretraining and finetuning paradigms have been extended to other modalities: audio (Schneider et al., 2019; Baevski et al., 2020), images (Su et al., 2019; Lu et al., 2019), and video (Sun et al., 2019). At the same time, pretraining and finetuning techniques have improved multitasking applications significantly, such as multilingual translation, cross-lingual representations, question-answering and so on (Raffel et al., 2020; • We propose a simple and effective approach to combine pretrained single-modality modules to"
2021.acl-long.68,2020.iwslt-1.8,0,0.573886,"Missing"
2021.acl-long.68,E17-2076,0,0.028709,"performance is sensitive to downsampling ratio in the adaptor module. We conduct the experiments on CoVoST 2 many-to-one experiments, and report perplexity on dev set of three directions with diverse input languages: German-English (De-En), Chinese-English (Zh-En) and Estonian-English (Et-En). Table 7 shows our approach is not sensitive to common downsampling ratios (4 or 8) while extreme downsampling (27) hurts performance. 6 Related Work Speech Translation. Sequence-to-sequence based speech translation has shown very good potential over the traditional cascaded system (Berard et al., 2016; Goldwater et al., 2017; Weiss et al., 2017) with end-to-end approaches surpassing cascaded system for the first time at IWSLT (Ansari et al., 2020) in a shared task setting. However, previous work also indicates that its success heavily relies on large amounts of labelled training data, which is difficult to acquire. In order 833 Target Source De De En Es Fr It Pt 13.1/22.5* 9.2/12.1 9.8/13.6 10.1/11.9 9.0/11.4 En Es Fr It Pt 12.8/20.6 10.2/13.8 23.1/32.3* 11.6/14.9 22.1/30.0* 19.0/21.8 6.6/8.6 14.9/21.5 13.3/15.4 13.8/15.2 10.4/13.0 20.7/28.4 20.0/21.9 19.7/21.4 19.8/19.2 18.9/26.0 19.8/27.9* 19.8/25.6 19.0/24.1 1"
2021.acl-long.68,2020.findings-emnlp.106,0,0.046981,"Missing"
2021.acl-long.68,W18-6319,0,0.0279488,"Missing"
2021.acl-long.68,D19-1445,0,0.0166133,"efficiently adapt large pretrained models has gained growing interest. (Houlsby et al., 2019) and (Pfeiffer et al., 2020) represent the stream of work which adds additional “adaptor modules” to achieve fast adaptation to downstream tasks. Another category of solutions focus selective finetuning (only subset of parameters) suitable for downstream tasks. Our work belongs to the second category of efficient finetuning without adding extra parameters (e.g. adaptor modules). Empirical studies shows that finetuning the final layers of BERT account for most of the quality gains on downstream tasks (Kovaleva et al., 2019; Lee et al., 2019). Finetuning LayerNorm parameters was also found effective for adapting pretrained BART or mBART for machine translation (Stickland et al., 2020). A general approach is to automatically learn which layers/parameters from a large-pretrained model to finetune and freeze (Guo et al., 2019), which we found is an exciting direction for future work. 7 Conclusion We proposed a simple and effective approach to leverage pretrained single-modality models (such as wav2vec 2.0, mBART) to perform speech-totext translation. On two large-scale multilingual speech translation benchmarks, ou"
2021.acl-long.68,2020.tacl-1.47,1,0.944137,"ansfer learning from efficient finetuning of single-modality pretrained models. The proposed LNA finetuning is applied to each layer. Yang et al., 2019; Tang et al., 2020). In this paper, we advance the one-model-for-all paradigm further by adapting audio and multilingual text pretraining and finetuning to improve multilingual speech-totext translation. Our contributions are as follows: Introduction Recent advances in pretraining over unlabeled data and then finetuning on labeled data leads to significant performance improvement in text understanding and generation tasks (Devlin et al., 2019; Liu et al., 2020; Conneau et al., 2019; Radford, 2018). Lately, such text pretraining and finetuning paradigms have been extended to other modalities: audio (Schneider et al., 2019; Baevski et al., 2020), images (Su et al., 2019; Lu et al., 2019), and video (Sun et al., 2019). At the same time, pretraining and finetuning techniques have improved multitasking applications significantly, such as multilingual translation, cross-lingual representations, question-answering and so on (Raffel et al., 2020; • We propose a simple and effective approach to combine pretrained single-modality modules to perform speech-to"
2021.acl-long.68,2021.acl-long.328,1,0.895718,"LayerNorm parameter is the proposed LNA-Minimalist finetuning. In addition, we also investigate the role of self attention (SA) parameters in facilitating crosslingual transfer ability. 2.4 Methods Pretrained Modules 3 3.1 Length Adaptor We add a lightweight adaptor module in between encoder and decoder to better align the two modJoint Speech-text Finetuning Multi-task learning has been shown as an effective approach to improve the performance of the speech translation task using other related tasks, such as MT and ASR (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Bahar et al., 2019; Tang et al., 2021a,b). We jointly train MT and ST tasks in the finetuning with pretrained models. The speech transcripts are used as input for the MT task and the corresponding speech data is used as input for the ST task. As a result, we can leverage abundant parallel text data to further improve the performance. Our model leverages a pretrained wav2vec 2.0 (Baevski et al., 2020) as encoder for acoustic modeling, a pretrained multilingual BART (mBART) (Liu et al., 2020) as decoder for language modeling. Both models are pretrained on unlabelled data via self-supervised learning. We provide an overview of the p"
2021.acl-long.68,N19-4009,1,0.87234,"Missing"
2021.acl-long.68,2020.emnlp-main.617,0,0.0384494,"Missing"
2021.acl-long.68,2020.aacl-demo.6,1,0.952234,"efficiency and crosslingual transfer ability of LNA finetuning in the bilingual setting without the additional effect from multilingual training. Drawing learnings on that, we then evaluate applying LNA finetuning to encoder only (LNA-E), decoder only (LNA-D), and both (LNA-E,D) respectively. For multilingual finetuning on CoVoST 2, we use all X-En training data (except zero-shot crosslingual transfer experiments) for evaluating X-En perfor70 60 50 40 30 20 10 Valid. PPL 3.2 Validation Perpelxity Statistics of the datasets and implementation details are reported in the A.2 and A.3. CoVoST 2 (Wang et al., 2020b) is a multilingual speech-to-text translation corpus with English into 15 languages (En-X) and 21 languages into English (X-En). It provides a comprehensive test bed for low-resource scenarios, with 4 X-En directions between 10 hours and 20 hours training data, and 11 X-En directions less than 4 hours training data. Europarl ST (Iranzo-S´anchez et al., 2020) has both English-centric as well as non-English directions, which allow us to evaluate the proposed method’s effectiveness of multilingual translation between any pair, especially zero-shot performance. We experiment on all 6 languages ("
2021.acl-long.68,1983.tc-1.13,0,0.159182,"Missing"
2021.acl-long.68,2020.acl-main.344,0,0.0378462,"efficiency and crosslingual transfer ability of LNA finetuning in the bilingual setting without the additional effect from multilingual training. Drawing learnings on that, we then evaluate applying LNA finetuning to encoder only (LNA-E), decoder only (LNA-D), and both (LNA-E,D) respectively. For multilingual finetuning on CoVoST 2, we use all X-En training data (except zero-shot crosslingual transfer experiments) for evaluating X-En perfor70 60 50 40 30 20 10 Valid. PPL 3.2 Validation Perpelxity Statistics of the datasets and implementation details are reported in the A.2 and A.3. CoVoST 2 (Wang et al., 2020b) is a multilingual speech-to-text translation corpus with English into 15 languages (En-X) and 21 languages into English (X-En). It provides a comprehensive test bed for low-resource scenarios, with 4 X-En directions between 10 hours and 20 hours training data, and 11 X-En directions less than 4 hours training data. Europarl ST (Iranzo-S´anchez et al., 2020) has both English-centric as well as non-English directions, which allow us to evaluate the proposed method’s effectiveness of multilingual translation between any pair, especially zero-shot performance. We experiment on all 6 languages ("
2021.acl-long.68,D19-1077,0,0.0497743,"Missing"
2021.eacl-main.130,W19-5206,0,0.231746,"is contributing to the observed mismatch, it is important to be aware of its existance and effect on MT algorithms. The source-target domain mismatch (STDM) can be understood as an instance of multi-domain MT (see Fig. 1 for an illustration and §3 for a formal definition), whereby part of the parallel dataset and the source monolingual dataset are “in-domain” because they originate from the source domain, and the remaining part of the parallel dataset as well as the target monolingual data are “out-of-domain”. There already exist several techniques for domain adaptation, like domain tagging (Caswell et al., 2019) and dataset weighting (Edunov et al., 2018; Wang et al., 2017; van der Wees et al., 2017), which are applicable and which we also employ in this work. However, these may not be enough to improve generalization on low resource languages because STDM effectively decreases the already scarce amount of useful (in-domain) parallel data, hindering good generalization. It is therefore important to quantify STDM (§4) and consider how STDM affects methods that leverage monolingual data (§5). For instance, STDM may negatively impact the effectiveness of back-translation because, even if the backward mo"
2021.eacl-main.130,W18-6315,0,0.0279524,"riking when considering low resource language pairs, for which differences in local context and cultures are often more pronounced. Recent studies (Søgaard et al., 2018; Neubig and Hu, 2018) have warned that removing the assumption on comparable corpora strongly deteriorates performance of lexicon induction techniques which are at the foundation of MT. Back-translation (Sennrich et al., 2015) has been the workhorse of modern neural MT, enabling very effective use of target side monolingual data. Backtranslation is beneficial because it helps regularizing the model and adapting to new domains (Burlot and Yvon, 2018). However, the typical setting of current MT benchmarks as popularized by re1520 Latent Space DSdistribution over topics distribution over words Source Language Target Language <latexit sha1_base64=&quot;+4g7QrHz8quNx3pLpZZQAime8gE=&quot;&gt;AAAGTXichVRbaxNBFN7WpK3x1uqjLwdLIYtryEZBQQptTcEHK5VeIZsss5NJM83e2JktWbbzB30RfPNf+OKDIuLspXGTbONA4GTO9835zjdnx/Jtyniz+W1p+U6lurK6drd27/6Dh4/WNx6fMi8MMDnBnu0F5xZixKYuOeGU2+TcDwhyLJucWaN3Sf7sigSMeu4xj3zSddCFSwcUIy63zI0K3jIcxIcY2XFbwDYYMdTHWqSaFAxhxnRb1xoN7aOo/cMdiB7LkOMeMy9T3GWGOzDZDJJnyKjHzVGKHN0geRHZFr3YcCxvHKNwLG6EhNrVjJBDSfLr0fVYrW3J6mAw6sCUssKhuxOh9VSpBoaFgjgS5qV6u2h"
2021.eacl-main.130,D18-1045,1,0.848637,"is important to be aware of its existance and effect on MT algorithms. The source-target domain mismatch (STDM) can be understood as an instance of multi-domain MT (see Fig. 1 for an illustration and §3 for a formal definition), whereby part of the parallel dataset and the source monolingual dataset are “in-domain” because they originate from the source domain, and the remaining part of the parallel dataset as well as the target monolingual data are “out-of-domain”. There already exist several techniques for domain adaptation, like domain tagging (Caswell et al., 2019) and dataset weighting (Edunov et al., 2018; Wang et al., 2017; van der Wees et al., 2017), which are applicable and which we also employ in this work. However, these may not be enough to improve generalization on low resource languages because STDM effectively decreases the already scarce amount of useful (in-domain) parallel data, hindering good generalization. It is therefore important to quantify STDM (§4) and consider how STDM affects methods that leverage monolingual data (§5). For instance, STDM may negatively impact the effectiveness of back-translation because, even if the backward model was perfect, the backtranslated data is"
2021.eacl-main.130,P95-1026,0,0.538985,"efore important to quantify STDM (§4) and consider how STDM affects methods that leverage monolingual data (§5). For instance, STDM may negatively impact the effectiveness of back-translation because, even if the backward model was perfect, the backtranslated data is out-of-domain relative to the source domain from which we aim to translate. Empirically we found that this is the case both in a controlled setting (§6.1) as well as in realistic datasets (§6.2). However, this issue can be compensated by adding more target-side monolingual data and by combining back-translation with selftraining (Yarowski, 1995). 2 Related Work The observation that topic distributions and various kinds of lexical variabilities depend on the local context has been known and studied for a long time (Firth, 1935). For instance, Firth (1935) says “Most of the give-and-take of conversation in our everyday life is stereotyped and very narrowly conditioned by our particular type of culture”. In her seminal work, Johnstone (2010) analyzed the role of place in language, focusing on lexical variations within the same language, a subject further explored by Britain (2013). Some of these works were the basis for later studies th"
2021.eacl-main.130,W19-5208,0,0.0623793,"ao and Paul, 2018; Yang et al., 2019) how to compare the distribution of {src(z s )} against {tgt(z t )}, since these are two possibly incomparable corpora in different languages. In this work, we therefore leverage the existence of a parallel corpus and compare the distribution of AT = {tgt(z t )}z t ∼DT with AS = {hs→t (src(z s ))}z s ∼DS . The underlying assumptions are a) we know the originating language of each training example, b) the effect of the change of the word distribution is negligible compared to the shift in topic distribution, and c) the effect of translationese (Baker, 1993; Zhang and Toral, 2019; Toury, 2012) is negligible compared to the actual STDM, and therefore, we can ignore changes to the distribution brought by the mapping hs→t (we validate this assumption in §C). Under these assumptions, we define the score as a measure of the topic discrepancy between AS and AT . Let A = AS ∪ AT be the concatenation of the corpus originating in the source and target language. We first extract topics using LSA. Let S T A ∈ R(n +n )×k be the TF-IDF matrix derived from A where the first nS rows are representations taken from AS , the bottom nT rows are representations of AT , and k is the numbe"
2021.naacl-main.426,S13-1004,0,0.080572,"Missing"
2021.naacl-main.426,S12-1051,0,0.0206892,", Italian and Indonesian - language pairs that were shown to provide good paraphrastic sentence embeddings (Wieting et al., 2019a). We pretrain the model with a multilingual masked language modeling objective (Devlin et al., 2018; Conneau and Lample, 2019) in these 4 languages, with a sentence piece segmentation trained on a corpus with 3/4 of English data to give more importance to English, and the rest in other languages. We use a triplet loss to learn cosine sentence embedding similarity where the negative is selected to be the hardest in the batch. We evaluate our model on STS benchmarks (Agirre et al., 2012) and report results in Section 5 where we show our model outperforms previous approaches. We found that due to pretraining and being trained on longer sentences, our model is also more adapted to raw and long sentences from CommonCrawl. We also consider word2vec embeddings (Mikolov et al., 2013) and the uSIF approach (Ethayarajh, 2018b; Arora et al., 2017) as baselines in our experimental results. Fine-tuning the student model. We use fairseq (Ott et al., 2019) and the open-source RoBERTa-Large model (Liu et al., 2019) as our pretrained Transformer baseline and perform finetuning on each downs"
2021.naacl-main.426,Q19-1038,0,0.0120781,"contribution. In this paper, we propose a data augmentation 5408 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5408–5418 June 6–11, 2021. ©2021 Association for Computational Linguistics method, SentAugment, to build datasets of “indomain” data for a given task from data crawled on the web. Web data covers many domains, and is available in large quantities. We use a large bank of web documents and construct sentence embeddings (Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Artetxe and Schwenk, 2019; Cer et al., 2018; Arora et al., 2017) that allow us to retrieve domain-specific unannotated sentences, which are similar to the existing training set of the downstream tasks. Our sentence embedding model is optimized for similarity search, trained with a triplet loss on ground-truth paraphrases, parallel sentences as well as as hard negatives (Wieting et al., 2016; Wieting and Gimpel, 2017). We train a teacher model using the labeled task data and then further use it to synthetically label the retrieved sentences, and train the final model based on this synthetic dataset. Experiments show th"
2021.naacl-main.426,W11-2138,0,0.0692322,"Missing"
2021.naacl-main.426,S17-2001,0,0.0361409,"Missing"
2021.naacl-main.426,D18-2029,0,0.0118691,"r, we propose a data augmentation 5408 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5408–5418 June 6–11, 2021. ©2021 Association for Computational Linguistics method, SentAugment, to build datasets of “indomain” data for a given task from data crawled on the web. Web data covers many domains, and is available in large quantities. We use a large bank of web documents and construct sentence embeddings (Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Artetxe and Schwenk, 2019; Cer et al., 2018; Arora et al., 2017) that allow us to retrieve domain-specific unannotated sentences, which are similar to the existing training set of the downstream tasks. Our sentence embedding model is optimized for similarity search, trained with a triplet loss on ground-truth paraphrases, parallel sentences as well as as hard negatives (Wieting et al., 2016; Wieting and Gimpel, 2017). We train a teacher model using the labeled task data and then further use it to synthetically label the retrieved sentences, and train the final model based on this synthetic dataset. Experiments show that SentAugment is"
2021.naacl-main.426,D17-1070,1,0.778985,"006). domains? ∗ Equal contribution. In this paper, we propose a data augmentation 5408 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5408–5418 June 6–11, 2021. ©2021 Association for Computational Linguistics method, SentAugment, to build datasets of “indomain” data for a given task from data crawled on the web. Web data covers many domains, and is available in large quantities. We use a large bank of web documents and construct sentence embeddings (Kiros et al., 2015; Wieting et al., 2016; Conneau et al., 2017; Artetxe and Schwenk, 2019; Cer et al., 2018; Arora et al., 2017) that allow us to retrieve domain-specific unannotated sentences, which are similar to the existing training set of the downstream tasks. Our sentence embedding model is optimized for similarity search, trained with a triplet loss on ground-truth paraphrases, parallel sentences as well as as hard negatives (Wieting et al., 2016; Wieting and Gimpel, 2017). We train a teacher model using the labeled task data and then further use it to synthetically label the retrieved sentences, and train the final model based on this synthetic d"
2021.naacl-main.426,L18-1218,0,0.0198673,"Missing"
2021.naacl-main.426,I05-5002,0,0.0736545,"ication (CR) from (Hu and Liu, 2004), hate-speech comment classification3 (IMP), question classification (TREC) from (Voorhees and Tice, 2000) and named entity recognition (CoNLL 2002) from (Sang and De Meulder, 2003). We provide details of each task including task, domain, size and number of classes in Table 1. 2 3 www.github.com/facebookresearch/cc net www.kaggle.com/c/detecting-insults-in-social-commentary/overview Training details Our sentence embeddings. We train our own SentAugment Sentence Encoder (SASE) by leveraging paraphrases from NLI entailment pairs (Williams et al., 2017), MRPC (Dolan and Brockett, 2005), Quora Question Pairs (QQP), round-trip translation (Wieting and Gimpel, 2017) and web paraphrases (Creutz et al., 2018), together with OpenSubtitles (Lison et al., 2019) and Europarl (Koehn, 2005) parallel data from English to French, Italian and Indonesian - language pairs that were shown to provide good paraphrastic sentence embeddings (Wieting et al., 2019a). We pretrain the model with a multilingual masked language modeling objective (Devlin et al., 2018; Conneau and Lample, 2019) in these 4 languages, with a sentence piece segmentation trained on a corpus with 3/4 of English data to giv"
2021.naacl-main.426,D18-1045,1,0.853622,"Missing"
2021.naacl-main.426,W18-3012,0,0.23088,"cting similar datasets on the fly from the large bank of unannotated text. In what follows, we describe our data retrieval strategy for augmentation. Large-scale sentence bank. Our approach relies on a large-scale corpus of unsupervised sentences, derived from data crawled on the web (Wenzek et al., 2019). Because of its scale and diversity, our sentence bank contains data from various domains and with different styles, allowing to retrieve relevant data for many downstream tasks. We embed each sentence using a universal paraphrastic sentence encoder (Wieting et al., 2016; Arora et al., 2017; Ethayarajh, 2018a), a model which was trained to output similar representations for sentences of similar meaning. This sentence embedding space does not depend on the downstream tasks, and will be used to retrieve subsets of the sentence bank which are relevant to particular tasks. For sentence encoders, we consider word2vec embeddings (Mikolov et al., 2013, 2018) and uSIF (Ethayarajh, 2018b). We also train our own English sentence encoder, a Transformer pretrained with masked language modeling and finetuned to maximize cosine similarity between similar sentences. Specifically, we use a triplet loss L(x, y) ="
2021.naacl-main.426,P84-1044,0,0.193374,"Missing"
2021.naacl-main.426,P18-1031,0,0.06459,"Missing"
2021.naacl-main.426,D19-1410,0,0.0251675,"Missing"
2021.naacl-main.426,W03-0419,0,0.697907,"Missing"
2021.naacl-main.426,2005.mtsummit-papers.11,0,0.0779451,"2003). We provide details of each task including task, domain, size and number of classes in Table 1. 2 3 www.github.com/facebookresearch/cc net www.kaggle.com/c/detecting-insults-in-social-commentary/overview Training details Our sentence embeddings. We train our own SentAugment Sentence Encoder (SASE) by leveraging paraphrases from NLI entailment pairs (Williams et al., 2017), MRPC (Dolan and Brockett, 2005), Quora Question Pairs (QQP), round-trip translation (Wieting and Gimpel, 2017) and web paraphrases (Creutz et al., 2018), together with OpenSubtitles (Lison et al., 2019) and Europarl (Koehn, 2005) parallel data from English to French, Italian and Indonesian - language pairs that were shown to provide good paraphrastic sentence embeddings (Wieting et al., 2019a). We pretrain the model with a multilingual masked language modeling objective (Devlin et al., 2018; Conneau and Lample, 2019) in these 4 languages, with a sentence piece segmentation trained on a corpus with 3/4 of English data to give more importance to English, and the rest in other languages. We use a triplet loss to learn cosine sentence embedding similarity where the negative is selected to be the hardest in the batch. We e"
2021.naacl-main.426,2021.ccl-1.108,0,0.14017,"Missing"
2021.naacl-main.426,N06-1020,0,0.162691,"cher. Self-training has been successfully ap- data in the same domain as the downstream task. plied to a variety of tasks, including image recog- This assumption limits the broad application of nition (Yalniz et al., 2019; Xie et al., 2020; Zoph such semi-supervised methods, in particular in the et al., 2020), automatic speech recognition (Syn- case of low-resource downstream tasks. A secnaeve et al., 2019; Kahn et al., 2020; Park et al., ond important question is thus: how can we obtain 2020), sequence generation (He et al., 2019), and large amounts of unannotated data from specific parsing (McClosky et al., 2006). domains? ∗ Equal contribution. In this paper, we propose a data augmentation 5408 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5408–5418 June 6–11, 2021. ©2021 Association for Computational Linguistics method, SentAugment, to build datasets of “indomain” data for a given task from data crawled on the web. Web data covers many domains, and is available in large quantities. We use a large bank of web documents and construct sentence embeddings (Kiros et al., 2015; Wieting et al., 2016; Conn"
2021.naacl-main.426,L18-1008,1,0.855656,"Missing"
2021.naacl-main.426,N19-4009,1,0.833581,"ntence embedding similarity where the negative is selected to be the hardest in the batch. We evaluate our model on STS benchmarks (Agirre et al., 2012) and report results in Section 5 where we show our model outperforms previous approaches. We found that due to pretraining and being trained on longer sentences, our model is also more adapted to raw and long sentences from CommonCrawl. We also consider word2vec embeddings (Mikolov et al., 2013) and the uSIF approach (Ethayarajh, 2018b; Arora et al., 2017) as baselines in our experimental results. Fine-tuning the student model. We use fairseq (Ott et al., 2019) and the open-source RoBERTa-Large model (Liu et al., 2019) as our pretrained Transformer baseline and perform finetuning on each downstream task. We use Adam, with learning-rate schedule 1e-5. We use batch-sizes of 16 and dropout rate 0.1. We fine-tune on synthetically annotated data using 5411 Model RoBERTaLarge RoBERTaLarge + ICP RoBERTaLarge + ST SST-2 SST-5 CR IMP TREC NER Avg 96.5 93.9 96.7 57.8 55.1 60.4 94.8 93.7 95.7 84.6 84.4 87.7 97.8 97.8 97.8 92.7 92.1 93.3 87.4 86.2 88.6 Table 2: Results of self-training on natural language understanding benchmarks. We report a strong RoBERTaLarg"
2021.naacl-main.426,P16-1009,0,0.151676,"Missing"
2021.naacl-main.426,D13-1170,0,0.00598847,"documents. We use three corpora, CC-100M with one hundred million sentences (2B words), CC-1B with one billion sentences (20B words) and CC-5B with five billion sentences (100B words), the first two being random subsets of the biggest one. When retrieving sentences, we remove those that overlap with sentences from the test set of the downstream task. CommonCrawl data contains a wide variety of domains and text styles which makes it a good general-purpose corpus. We release pointers to obtain a similar corpus. 3.2 Evaluation datasets We evaluate our approach on the Stanford Sentiment Treebank (Socher et al., 2013) binary and fine-grained sentiment analysis datasets (SST-2 and SST-5), on product classification (CR) from (Hu and Liu, 2004), hate-speech comment classification3 (IMP), question classification (TREC) from (Voorhees and Tice, 2000) and named entity recognition (CoNLL 2002) from (Sang and De Meulder, 2003). We provide details of each task including task, domain, size and number of classes in Table 1. 2 3 www.github.com/facebookresearch/cc net www.kaggle.com/c/detecting-insults-in-social-commentary/overview Training details Our sentence embeddings. We train our own SentAugment Sentence Encoder"
2021.naacl-main.426,P19-1453,0,0.0802041,"embedding space does not depend on the downstream tasks, and will be used to retrieve subsets of the sentence bank which are relevant to particular tasks. For sentence encoders, we consider word2vec embeddings (Mikolov et al., 2013, 2018) and uSIF (Ethayarajh, 2018b). We also train our own English sentence encoder, a Transformer pretrained with masked language modeling and finetuned to maximize cosine similarity between similar sentences. Specifically, we use a triplet loss L(x, y) = max(0, α − cos(x, y) + cos(x, yc )) where positive pairs (x, y) are either paraphrases or parallel sentences (Wieting et al., 2019a) and yc are in-batch hard negatives (Wieting et al., 2016). Downstream task embeddings. For each downstream task, we build embeddings that are representative of the task, using the same paraphrastic • We release code and models for researchers model. Then, we use these task embeddings as to build on top of our work.1 queries for retrieving similar sentences from the sentence bank, using cosine similarity in the embed1 https://github.com/facebookresearch/SentAugment ding space. Specifically, we consider three ways 5409 Step 2: Retrieval-based augmentation of in-domain unannotated data from a"
2021.naacl-main.426,P95-1026,0,0.674257,"without self-training. This is in conshow strong gains on knowledge-distillation trast to natural language understanding for which and few-shot learning. language modeling pre-training is a very strong baseline that leads to large improvements for all 1 Introduction the tasks we consider. Self-training is a semi-supervised method which An important ingredient for self-training, and uses a teacher model, trained using labeled data, semi-supervised learning in general, is the unanto create synthetic labels for unlabeled exam- notated data and the fact that it comes from the ples (Scudder, 1965; Yarowsky, 1995). These syn- same domain as the downstream task. Existthetic labels are then used to train a student model. ing work, such as UDA (Xie et al., 2019), selfThis approach is called self-training when the stu- training (He et al., 2019; Xie et al., 2020) and dent model has a similar or higher capacity than the back-translation for machine translation (Bojar and teacher, and knowledge distillation (Hinton et al., Tamchyna, 2011; Sennrich et al., 2015; Edunov 2015) when the student model is smaller than the et al., 2018), assumes the existence of unannotated teacher. Self-training has been successfu"
D11-1031,P11-1048,1,0.312904,"Missing"
D11-1031,J99-2004,0,0.0767254,"point when both the head and the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+ . We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most s"
D11-1031,C04-1041,0,0.346788,"ly, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can be parsed with very tight beams. Reverse adaptive supertagging is a much less aggressive method that seeks only to make sentences parsable when they otherwise would not be due to an impractically large search space. Reverse AST starts with a wide beam, narrowing"
D11-1031,J07-4004,0,0.881739,"used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective using F-measure as a loss (§3). We experiment with this and several other metrics, including precision, recall, and decomposable approximations thereof. Our ability to optimise towards exact metrics enables us to verify the effectiveness of more efficient approximations. We test the training procedures on the state-of-the-art Combinatory Categorial Grammar (CCG; Steedman 2000) parser of Clark and Curran (2007), obtaining substantial improvements under a variety of conditions. We then embed this model into a more accurate model that incorporates additional supertagging features via loopy belief propagation. The improvements are additive, obtaining the best reported results on this task (§4). 2 Softmax-Margin Training The softmax-margin objective modifies the standard likelihood objective for CRF training by reweighting 333 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333–343, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computatio"
D11-1031,P02-1042,0,0.182477,"tic head of coordinations. The coordination rule (Φ) does not yet establish the dependency “and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T (y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+ (·) and d+ (·). X DecP (y) = d+ (t) − n+ (t) (8) t∈T (y) To compute our approximation to recall we require the number of gold dependencies, c"
D11-1031,W02-2203,0,0.429695,"nd the dependent are in the same span, violating the assumption used to compute c+ (see again Figure 2). Exceptions like this can cause mismatches between n+ and c+ . We set c+ = n+ whenever c+ &lt; n+ to account for these occasional discrepancies. Finally, we obtain a decomposable approximation to F-measure. DecF 1(y) = DecP (y) + DecR(y) 4 (10) Experiments Parsing Strategy. CCG parsers use a pipeline strategy: we first multitag each word of the sentence with a small subset of its possible lexical categories using a supertagger, a sequence model over these categories (Bangalore and Joshi, 1999; Clark, 2002). Then we parse the sentence under the requirement that the lexical categories are fixed to those preferred by the supertagger. In our experiments we used two variants on this strategy. First is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical categories only if the parser fails to find an analysis. The process either succeeds and returns a parse after some iteration or gives up after a predefined number of iterations. As Clark and Curran (2004) show, most sentences can b"
D11-1031,D09-1011,0,0.0319889,"Missing"
D11-1031,P08-1109,0,0.0891358,"Missing"
D11-1031,P10-1035,0,0.230107,"-accuracy (SA) as loss in the supertagger. LF 85.53 85.79 86.45 86.73 86.51 CLL Petrov I-5 BP +DecF1 +SA LP 85.73 86.09 86.75 87.07 86.86 section 00 (dev) LR UF 85.33 91.99 85.50 92.44 86.17 92.60 86.39 92.79 86.16 92.60 UP 92.20 92.76 92.92 93.16 92.98 UR 91.77 92.13 92.29 92.43 92.23 LF 85.74 86.01 86.84 87.08 87.20 LP 85.90 86.29 87.08 87.37 87.50 section 23 (test) LR UF 85.58 91.92 85.73 92.34 86.61 92.57 86.78 92.68 86.90 92.76 UP 92.09 92.64 92.82 93.00 93.08 UR 91.75 92.04 92.32 92.37 92.44 Table 7: Results on automatically assigned POS tags. Petrov I-5 is based on the parser output of Fowler and Penn (2010); evaluation is based on sentences for which all parsers returned an analysis. anonymous reviewers for helpful comments. We also acknowledge funding from EPSRC grant EP/P504171/1 (Auli); and the resources provided by the Edinburgh Compute and Data Facility. A Computing F-Measure-Augmented Expectations at the Corpus Level To compute exact corpus-level expectations for softmaxmargin using F-measure, we add an additional transition before reaching the GOAL item in our original program. To reach it, we must parse every sentence in the corpus, associating statistics of aggregate hn, di pairs for th"
D11-1031,N10-1112,0,0.152716,"sults for this task. 1 Introduction Parsing models based on Conditional Random Fields (CRFs; Lafferty et al., 2001) have been very successful (Clark and Curran, 2007; Finkel et al., 2008). In practice, they are usually trained by maximising the conditional log-likelihood (CLL) of the training data. However, it is widely appreciated that optimizing for task-specific metrics often leads to better performance on those tasks (Goodman, 1996; Och, 2003). An especially attractive means of accomplishing this for CRFs is the softmax-margin (SMM) objective (Sha and Saul, 2006; Povey and Woodland, 2008; Gimpel and Smith, 2010a) (§2). In addition to retaining a probabilistic interpretation and optimizing towards a loss function, it is also convex, making it straightforward to optimise. Gimpel and Smith (2010a) show that it can be easily implemented with a simple change to standard likelihood-based training, provided that the loss function decomposes over the predicted structure. Unfortunately, the widely-used F-measure metric does not decompose over parses. To solve this, we introduce a novel dynamic programming algorithm that enables us to compute the exact quantities needed under the softmax-margin objective usin"
D11-1031,P96-1024,0,0.693397,"Missing"
D11-1031,J07-3004,0,0.236323,"Curran (2007), which contains features over both normalform derivations and CCG dependencies. The parser relies solely on the supertagger for pruning, using exact CKY for search over the pruned space. Training requires calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 2) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS trainer. Evaluation. We evaluated on CCGbank (Hockenmaier and Steedman, 2007), a right-most normalform CCG version of the Penn Treebank. We use sections 02-21 (39603 sentences) for training, section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. We evaluate on labelled and unlabelled predicate argument structure recovery and supertag accuracy. 4.1 Training with Maximum F-measure Parses So far we discussed how to optimise towards taskspecific metrics via changing the training objective. In our first experiment we change the data on which we optimise CLL. This is a kind of simpl"
D11-1031,P08-1067,0,0.00904733,"Importantly, both n and d decompose over parses. The key idea will be to treat F1 as a non-local feature of the parse, dependent on values n and d.2 To compute expectations we split each span in an otherwise usual inside-outside computation by all pairs hn, di incident at that span. Formally, our goal will be to compute expectations over the sentence a1 ...aL . In order to abstract away from the particulars of CCG we present the algorithm in relatively familiar terms as a variant of 1 2 For numerator and denominator. This is essentially the same trick used in the oracle F-measure algorithm of Huang (2008), and indeed our algorithm is a sumproduct variant of that max-product algorithm. min θ min θ ∂ = ∂λk m X i=1  m X i=1 m X i=1   −θT f (x(i) , y (i) ) + log −hk (x(i) , y (i) ) + y∈Y(x(i) ) y∈Y(x(i) ) X −θT f (x(i) , y (i) ) + log X X y∈Y(x(i) ) exp{θT f (x(i) , y)} y 0 ∈Y(x(i) ) (2)  exp{θT f (x(i) , y) + `(y (i) , y)} exp{θT f (x(i) , y) P  + `(y (i) , y)} exp{θT f (x(i) , y 0 ) + `(y (i) , y 0 )} (3)  hk (x(i) , y) (4) Figure 1: Conditional log-likelihood (Eq. 2), Softmax-margin objective (Eq. 3) and gradient (Eq. 4). the classic inside-outside algorithm (Baker, 1979). We use th"
D11-1031,P03-1021,0,0.0374292,"Missing"
D11-1031,D08-1016,0,0.0419993,"Missing"
D11-1031,W04-3201,0,0.0359019,"and - pears” (dotted line); it is the backward application (&lt;) in the larger span, “apples and pears”, that establishes it, together with “and - pears”. CCG also deals with unbounded dependencies which potentially lead to more dependencies than words (Steedman, 2000); in this example a unification mechanism creates the dependencies “likes - apples” and “likes - pears” in the forward application (&gt;). For further examples and a more detailed explanation of the mechanism as used in the C&C parser refer to Clark et al. (2002). available within the current local structure, similar to those used by Taskar et al. (2004) for tracking constituent errors in a context-free parser. We design three simple losses to approximate precision, recall and F-measure on CCG dependency structures. Let T (y) be the set of parsing actions required to build parse y. Our decomposable approximation to precision simply counts the number of incorrect dependencies using the local dependency counts, n+ (·) and d+ (·). X DecP (y) = d+ (t) − n+ (t) (8) t∈T (y) To compute our approximation to recall we require the number of gold dependencies, c+ (·), which should have been introduced by a particular parsing action. A gold dependency is"
D11-1031,P08-1000,0,\N,Missing
D13-1106,W12-2703,0,0.286096,"average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as conventional n-gram models, both of which are limited to fixed-length contexts. Building on the success of recurrent architectures, we base our joint language and translation model on an extension of the recurrent neural network language model (Mikolov and Zweig, 2012) that introduces a layer of additional inputs (§2). Most previous work on"
D13-1106,J92-4003,0,0.206592,"s from 2010-2011 containing between 2034-3003 sentences. Log-linear weights are estimated on the 2009 data set comprising 2525 sentences. We rescore the lattices produced by the baseline systems with an aggressive but effective context beam of k = 1 that did not harm accuracy in preliminary experiments (§3). Neural Network Language Model. The vocabularies of the language models are comprised of the words in the training set after removing singletons. We obtain word-classes using a version of Brown-Clustering with an additional regularization term to optimize the runtime of the language model (Brown et al., 1992; Zweig and Makarychev, 2013). 1048 Direct connections use maximum entropy features over unigrams, bigrams and trigrams (Mikolov et al., 2011a). We use the standard settings for the model with the default learning rate α = 0.1 that decays exponentially if the validation set entropy does not increase after each epoch. Back propagation through time computes error gradients over the past twenty time steps. Training is stopped after 20 epochs or when the validation entropy does not decrease over two epochs. We experiment with varying training data sizes and randomly draw the data from the same cor"
D13-1106,D11-1103,0,0.0212121,"e representing an unbounded history of both source and target words, rather than a feedforward style network. Feed-forward networks and n-gram models have a finite history which makes predictions independent of anything but a small history of words. Furthermore, we only model the target-side which is different to previous work modeling both sides. We introduced a new algorithm to tackle lattice rescoring with an unbounded model. The automatic speech recognition community has previously addressed this issue by either approximating longspan language models via simpler but more tractable models (Deoras et al., 2011b), or by identifying confusable subsets of the lattice from which n-best lists are constructed and rescored (Deoras et al., 2011a). We extend their work by directly mapping a recurrent neural network model onto the structure of the lattice, rescoring all states instead of focusing only on subsets. 7 Conclusion and Future Work Joint language and translation modeling with recurrent neural networks leads to substantial gains over the 1-best decoder output, raising accuracy by up to 1.5 BLEU and by 1.1 BLEU on average across several test sets. The joint approach also improves over the gains of th"
D13-1106,N03-1017,0,0.0236734,"result in similar recurrent histories, which in turn reduces the effect of aggressive pruning. 4 Language Model Experiments Recurrent neural network language models have previously only been used in n-best rescoring settings and on small-scale tasks with baseline language models trained on only 17.5m words (Mikolov, 2012). We extend this work by experimenting on lattices using strong baselines with ngram models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel"
D13-1106,P07-2045,0,0.0219354,"ynamic programming point of view. Fortunately, we do not need to maintain entire translations as context in the states: the recurrent model compactly encodes the entire history of previous words in the hidden layer configuration hi . It is therefore sufficient to add hi as context, instead of the entire translation. The language model can then simply score any new words based on hi from the previous state when a new state is created. A much larger problem is that items, that were previously equivalent from a dynamic programming perspective, may now be different. Standard phrasebased decoders (Koehn et al., 2007) recombine decoder states with the same context into a single state because they are equivalent to the model features; usually recombination retains only the highest scoring candidate.3 However, if the context is large, then the amount of recombination will decrease significantly, leading to less variety in the decoder beam. This was confirmed in preliminary experiments where we simulated context sizes of up to 100 words but found that accuracy dropped by between 0.5-1.0 BLEU. Integrating a long-span language model na¨ıvely requires to keep context equivalent to the entire left prefix of the t"
D13-1106,W04-3250,0,0.111211,"d any significant improvements. Even this representation of sentences is composed of a large number of instances, and so we resorted to feature hashing by computing feature ids as the least significant 20 bits of each feature name. Our best transform achieved a cosine similarity of 0.816 on the training data, 0.757 on the validation data, and 0.749 on news2011. The results (Table 8) show that the transform improves over the recurrent neural network language model on all test sets and by 0.2 BLEU on average. We verified significance over the target-only model using paired bootstrap resampling (Koehn, 2004) over all test sets (7526 sentences) at the p < 0.001 level. Overall, we improve accuracy by up to 1.5 1052 BLEU and by 1.1 BLEU on average across all test sets over the decoder 1-best with our joint language and translation model. 6 Related Work Our approach of combining language and translation modeling is very much in line with recent work on n-gram-based translation models (Crego and Yvon, 2010), and more recently continuous space-based translation models (Le et al., 2012a; Gao et al., 2013). The joint model presented in this paper differs in a number of key aspects: we use a recurrent arc"
D13-1106,N12-1005,0,0.864704,"l builds on a well known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward net"
D13-1106,D08-1076,0,0.0229444,"Missing"
D13-1106,P03-1021,0,0.174336,"models trained on over one billion words and by evaluating on a number of language pairs. 4.1 Experimental Setup Baseline. We experiment with an in-house phrasebased system similar to Moses (Koehn et al., 2003), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrasepenalties, a linear distortion feature and a lexicalized reordering feature. Log-linear weights are estimated with minimum error rate training (Och, 2003). Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English, German-English and EnglishGerman. Translation models are estimated on 102m words of parallel data for French-English, 91m words for German-English and English-German; between 3.5-5m words are newswire, depending on the language pair, and the remainder are parliamentary proceedings. The baseline systems use two 5-gram modified Kneser-Ney language models; the first is estimated on the target-side of the parallel data, while the second is based on a large newswire corpus released as part o"
D13-1106,W12-2702,0,0.0348831,"known recurrent neural network language model (Mikolov, 2012) augmented by a layer of additional inputs from the source language. We show competitive accuracy compared to the traditional channel model features. Our best results improve the output of a system trained on WMT 2012 French-English data by up to 1.5 BLEU, and by 1.1 BLEU on average across several test sets. 1 Introduction Recently, several feed-forward neural networkbased language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012). In this paper we focus on recurrent neural network architectures, which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012), outperforming multi-layer feed-forward based networks in both perplexity and word error rate in speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). The major attraction of recurrent architectures is their potential to capture long-span dependencies since predictions are based on an unbounded history of previous words. This is in contrast to feed-forward networks as well as convent"
D13-1106,N13-1090,1,\N,Missing
D13-1106,W11-2135,0,\N,Missing
D14-1132,J92-4003,0,0.0612884,"+j , we have: model. We experiment with these variants and extensions: ∂γθT h(f, e) ∂hk (e, f ) = γλk ∂sφ (o, pp) ∂sφ (o, pp) • SparseHRMLocal: This feature set is exclu= γλk N (o, pp, e, f ) sively based on the local phrase-pair and By using the following definition: 1254 consists of features over the first and last word of both the source and target phrase.5 We use four different word representations: The word identity itself, but only for the 80 most common source and target language words. The three other word representations are based on Brown clustering with either 20, 50 or 80 classes (Brown et al., 1992). There is one feature for every orientation type. • SparseHRM: The main feature set of Cherry (2013). This is an extension of SparseHRMLocal adding features based on the first and last word of both the source and the target of the hierarchical block at the top of the stack. There are also features based on the source words in-between the current phrase and the hierarchical block at the top of the stack. • SparseHRM+UncommonWords: This set is identical to SparseHRM, except that wordidentity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regard"
D14-1132,N13-1003,0,0.0676183,"red interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to"
D14-1132,N09-1025,0,0.0205233,"extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with"
D14-1132,J07-2003,0,0.116682,"ering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in p"
D14-1132,P13-4034,0,0.0438564,"Missing"
D14-1132,D08-1089,1,0.946153,"ingle phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum like"
D14-1132,N04-1035,1,0.812734,"train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work"
D14-1132,P06-1121,1,0.750834,"discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer fea"
D14-1132,N13-1048,1,0.922125,"dels to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c Octobe"
D14-1132,P14-1066,1,0.937605,"mensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doh"
D14-1132,W14-3360,0,0.0343077,"t attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doha, Qatar. 2014 Associ"
D14-1132,P12-1031,0,0.116388,"parse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1"
D14-1132,D11-1125,0,0.0430797,"of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baselin"
D14-1132,N03-1017,0,0.233513,"features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering mode"
D14-1132,P07-2045,0,0.0295763,"ient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯ e1 , . . . , e¯n } currently hypothesized by the decoder, a phrase alignment a = {a1 , . . . , an } that defines a foreign phrase f¯ai for each English phrase e¯i , and an orientation oi which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from wordaligned training corpora. Most models estimate a probability d"
D14-1132,P06-1096,0,0.0617731,"Missing"
D14-1132,C12-1121,0,0.0133419,"to be useful. To prevent overfitting, we experimented with `2 regularization, but found that it did not improve test accuracy. We also tuned the probability scaling parameter γ (Eq. 6) but found γ = 1 to be very good among other settings. We evaluate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling `2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley & Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1"
D14-1132,2009.mtsummit-papers.10,0,0.125252,"Missing"
D14-1132,J04-4002,0,0.749151,"s of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maxi"
D14-1132,P03-1021,0,0.208745,"ights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to E(f ), the set of possible translations of f , used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1, . . . θm+j = 1 and optimize φ with respect to the loss function on the training data using stochastic gradient descent.2 4. Finally, we fix φ and re-optimize θ in the presence of the discriminative reordering model using Minimum Error Rate Training (MERT; Och 2003; §7). l(φ) = − xBLEU(φ) X pθ,φ (e|f ) sBLEU(e, e(i) ) =− e∈E(f ) pθ,φ (e|f ) = P (6) ∂l(φ) X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a"
D14-1132,P02-1040,0,0.0906386,"llow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with lit"
D14-1132,W10-1748,0,0.0531366,"eters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natur"
D14-1132,W11-2119,0,0.0180257,"ould like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processi"
D14-1132,N03-2036,0,0.0831567,"ering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering mode"
D14-1132,D08-1065,0,0.0274346,") X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a tuned scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in sφ (o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Using the observation that the loss does not explicitly depend on φ, we get: We found that re-optimizing θ after a few iterations of stochastic gradient descent in step 3 did not improve accuracy. 5 (5) We tuned θm+1 , . . . θm+j on the development set but found that setting them uniformly to one resulted in faster training and equal accuracy. where δo,pp is the error term for orientation o of phrase p"
D14-1132,J97-3002,0,0.339127,"ows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation,"
D14-1132,P06-1066,0,0.283179,"t is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Mann"
D14-1132,D13-1112,0,0.029053,"ill achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline features (§7) as well as the 4.4K indicator features corresponding to the sparse reordering model. For expected BLEU t"
D14-1132,N04-4026,0,\N,Missing
D14-1132,W12-3102,0,\N,Missing
D14-1132,2005.iwslt-1.8,0,\N,Missing
D16-1128,D10-1049,0,0.0552504,"t determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues"
D16-1128,H05-1042,0,0.596791,"ional Linguistics sopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences. Data-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011)"
D16-1128,N06-1046,0,0.0122865,"e sentences. Data-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach"
D16-1128,E06-1040,0,0.0699903,"Missing"
D16-1128,P14-1129,0,0.0118713,"g, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design. 1204 Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist,"
D16-1128,W02-2112,0,0.177316,"6 Association for Computational Linguistics sopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences. Data-driven approaches have been proposed to automatically learn the individual modules. One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007;"
D16-1128,W07-2322,0,0.120459,"Missing"
D16-1128,W06-1417,0,0.056578,"ining grammar (Gyawali and Gardent, 2014). Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality to understand highlevel themes such as that the biography is about a scientist or an artist, while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox. We analyze the effectiveness of each and demonstrate their complementarity. 2 Related Work Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androut1203 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203–1213, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics sopoulos, 2007; Turner et al., 2010). Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences. Data-driv"
D16-1128,P14-1040,0,0.0213678,"model operating on them (Bengio et al., 2003). This factorization allows us to scale to a larger number of words and fields than Liang et al. (2009), or Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields. Moreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014). Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality to understand highlevel themes such as that the biography is about a scientist or an artist, while as local conditioning describes the previously generated tokens in terms of the their relationship to the infobox. We analyze the effectiveness of each and demonstrate their complementarity. 2 Related Work Traditionally, generation systems relied on rules and hand-crafted specifications (Dale et al., 2003; Reiter et al., 2005; Green, 2006; Galanis and Androut1203 Proc"
D16-1128,P13-2121,0,0.020273,"Missing"
D16-1128,C10-2062,0,0.411713,"on model conditioned on a Wikipedia infobox. We focus on the generation of the first sentence of a biography which requires the model to select among a large number of possible fields to generate an adequate output. Such diversity makes it difficult for classical count-based models to estimate probabilities of rare events due to data sparsity. We address this issue by parameterizing words and fields as embeddings, along with a neural language model operating on them (Bengio et al., 2003). This factorization allows us to scale to a larger number of words and fields than Liang et al. (2009), or Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields. Moreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014). Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a personality to understand high"
D16-1128,P98-1116,0,0.0406254,"problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their archit"
D16-1128,E14-1051,1,0.748012,"Missing"
D16-1128,P09-1011,0,0.505706,"e a statistical generation model conditioned on a Wikipedia infobox. We focus on the generation of the first sentence of a biography which requires the model to select among a large number of possible fields to generate an adequate output. Such diversity makes it difficult for classical count-based models to estimate probabilities of rare events due to data sparsity. We address this issue by parameterizing words and fields as embeddings, along with a neural language model operating on them (Bengio et al., 2003). This factorization allows us to scale to a larger number of words and fields than Liang et al. (2009), or Kim and Mooney (2010) where the number of parameters grows as the product of the number of words and fields. Moreover, our approach does not restrict the relations between the field contents and the generated text. This contrasts with less flexible strategies that assume the generation to follow either a hybrid alignment tree (Kim and Mooney, 2010), a probabilistic context-free grammar (Konstas and Lapata, 2013), or a tree adjoining grammar (Gyawali and Gardent, 2014). Our model exploits structured data both globally and locally. Global conditioning summarizes all information about a pers"
D16-1128,D11-1149,0,0.00961662,"nd Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin e"
D16-1128,P15-1002,0,0.109457,"nd surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design. 1204 Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist, plant pathologist, computer scientist, math"
D16-1128,J11-3002,0,0.0252181,"o a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanis"
D16-1128,P14-5010,0,0.0045893,"Missing"
D16-1128,N16-1086,0,0.414298,"d statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design. 1204 Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.”. 3 Language Modeling for Constrained Sentence generation Conditional language models are a popular choice to ge"
D16-1128,W00-0306,0,0.18179,"nd defines an output domain W ∪Q. Q defines all tokens in the table, which might include out of vocabulary words (∈ / W). For instance Park-Rhodes in Figure 1 is not in W. However, Park-Rhodes will be included in Q as name 2 (since it is the second token of the name field) which allows our model to generate it. This mechanism is inspired by recent work on attention based word copying for neural machine translation (Luong et al., 2015) as well as delexicalization for neural dialog systems (Wen et al., 2015). It also builds upon older work such as class-based language models for dialog systems (Oh and Rudnicky, 2000). 4 A Neural Language Model Approach A feed-forward neural language model (NLM) estimates P (wt |ct ) with a parametric function φθ 1206 (Equation 1), where θ refers to all learnable parameters of the network. This function is a composition of simple differentiable functions or layers. 4.1 Mathematical notations and layers We denote matrices as bold upper case letters (X, Y, Z), and vectors as bold lower-case letters (a, b, c). Ai represents the ith row of matrix A. When A is a 3-d matrix, then Ai,j represents the vector of the ith first dimension and j th second dimension. Unless otherwise st"
D16-1128,P15-1152,0,0.0250921,"Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design. 1204 Figure 1: Wikipedia infobox of Frederick Parker-Rhodes. The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist, plant pathologist, computer scientist, mathematician, mystic, and mycologist.”. 3 Language Modeling for C"
D16-1128,P06-1139,0,0.156702,"h partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015). Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the W EATHERGOV and ROBOCUP tasks. Their architecture relies on LSTM uni"
D16-1128,D15-1199,0,0.111366,"Missing"
D16-1128,N07-1022,0,0.0174007,"uboue and McKeown, 2002; Barzilay and Lapata, 2005). Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009). Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006). End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011). More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013). At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011). Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015),"
D16-1128,W09-0607,0,\N,Missing
D16-1128,P02-1040,0,\N,Missing
D16-1128,C98-1112,0,\N,Missing
D18-1045,W09-0432,0,0.0785806,"Missing"
D18-1045,W11-2138,0,0.31148,"gual data in the target language are available. Back-translation first trains an intermediate system on the parallel data which is used to translate the target monolingual data into the source language. The result is a parallel corpus where the source side is synthetic machine translation output while the target is genuine text written by humans. The synthetic parallel corpus is then simply added to the real bitext in order to train a final system that will translate from the source to the target language. Although simple, this method has been shown to be helpful for phrase-based translation (Bojar and Tamchyna, 2011), NMT (Sennrich et al., 2016a; Poncelas et al., 2018) as well as unsupervised MT (Lample et al., 2018a). In this paper, we investigate back-translation for neural machine translation at a large scale by adding hundreds of millions of back-translated sentences to the bitext. Our experiments are based on strong baseline models trained on the public bitext of the WMT competition. We extend previous analysis (Sennrich et al., 2016a; Poncelas et al., 2018) of back-translation in several ways. We provide a comprehensive analysis of different methods to generate synthetic source sentences and we show"
D18-1045,D07-1090,0,0.00824643,"l. (2016a); Xia et al. (2017) show how monolingual text from both languages can be leveraged by extending back-translation to dual learning: when training both source-to-target and target-to-source models jointly, one can use back-translation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that learn distributions ov"
D18-1045,J90-2002,0,0.518363,"ously trained with new synthetic data. There has also been work using source-side monolingual data (Zhang and Zong, 2016). Furthermore, Cheng et al. (2016); He et al. (2016a); Xia et al. (2017) show how monolingual text from both languages can be leveraged by extending back-translation to dual learning: when training both source-to-target and target-to-source models jointly, one can use back-translation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generativ"
D18-1045,P16-1185,0,0.285934,"sentences in both the source and target language. However, bitext is limited and there is a much larger amount of monolingual data available. Monolingual data has been traditionally used to train language models which improved the fluency of statistical machine translation (Koehn, 2010). In the context of neural machine translation (NMT; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017), there has been extensive work to improve models with monolingual data, including language model fusion (Gulcehre et al., 2015, 2017), back-translation (Sennrich et al., 2016a) and dual learning (Cheng et al., 2016; He et al., 2016a). These methods have different advantages and can be combined to reach high accuracy (Hassan et al., 2018). *Work done while at Facebook AI Research. 489 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics A similar strategy can be applied to NMT (He et al., 2016b). Besides improving accuracy during decoding, neural LM and NMT can benefit from deeper integration, e.g. by combining the hidden states of both models (Gulcehre et al"
D18-1045,W17-4715,0,0.0589374,"er et al., 2017) and transformer networks (Vaswani et al., 2017). Recent work relies on attention mechanisms where the encoder produces a sequence of vectors and, for each target token, the decoder attends to the most relevant part of the source through a contextdependent weighted-sum of the encoder vectors (Bahdanau et al., 2015; Luong et al., 2015). Attention has been refined with multi-hop attention (Gehring et al., 2017), self-attention (Vaswani et al., 2017; Paulus et al., 2018) and multi-head attention (Vaswani et al., 2017). We use a transformer architecture (Vaswani et al., 2017). 2.2 Currey et al. (2017) show that low resource language pairs can also be improved with synthetic data where the source is simply a copy of the monolingual target data. Concurrently to our work, Imamura et al. (2018) show that sampling synthetic sources is more effective than beam search. Specifically, they sample multiple sources for each target whereas we draw only a single sample, opting to train on a larger number of target sentences instead. Hoang et al. (2018) and Cotterell and Kreutzer (2018) suggest an iterative procedure which continuously improves the quality of the back-translation and final systems. Niu"
D18-1045,D17-1158,0,0.0175897,"al., 2018). *Work done while at Facebook AI Research. 489 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489–500 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics A similar strategy can be applied to NMT (He et al., 2016b). Besides improving accuracy during decoding, neural LM and NMT can benefit from deeper integration, e.g. by combining the hidden states of both models (Gulcehre et al., 2017). Neural architecture also allows multi-task learning and parameter sharing between MT and target-side LM (Domhan and Hieber, 2017). ing only on public WMT bitext as well as 226M monolingual sentences. This outperforms the system of DeepL by 1.7 BLEU who train on large amounts of high quality non-benchmark data. On WMT’14 English-French we achieve 45.6 BLEU. 2 Related work This section describes prior work in machine translation with neural networks as well as semisupervised machine translation. 2.1 Back-translation (BT) is an alternative to leverage monolingual data. BT is simple and easy to apply as it does not require modification to the MT training algorithms. It requires training a targetto-source system in order to"
D18-1045,P18-1082,0,0.0238398,"mple et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated probability given an input. Beam is generally successful in finding high probability outputs (Ott et al., 2018a). However, MAP prediction can lead to less rich translations (Ott et al., 2018a) since it always favors the most likely alternative in case of ambiguity. This is particularly problematic in tasks where there is a high level of uncertainty such as dialog (Serban et al., 2016) and story generation (Fan et al., 2018). We argue that this is also problematic for a data augmentation scheme such as backtranslation. Beam and greedy focus on the head of the model distribution which results in very regular synthetic source sentences that do not properly cover the true data distribution. As alternative, we consider sampling from the model distribution as well as adding noise to beam search outputs. First, we explore unrestricted sampling which generates outputs that are very diverse but sometimes highly unlikely. Second, we investigate sampling restricted to the most likely words (Graves, 2013; Ott et al., 2018a;"
D18-1045,N16-1101,0,0.0111034,"slation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that learn distributions over image transformations (Hauberg et al., 2016). cial for the autoencoder setups of (Lample et al., 2018a; Hill et al., 2016) which is inspired by denoising autoencoders (Vincent et al., 2008). In particular, we transform source sent"
D18-1045,P13-2121,0,0.0614298,"Missing"
D18-1045,N16-1162,0,0.0148879,"pproaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that learn distributions over image transformations (Hauberg et al., 2016). cial for the autoencoder setups of (Lample et al., 2018a; Hill et al., 2016) which is inspired by denoising autoencoders (Vincent et al., 2008). In particular, we transform source sentences with three types of noise: deleting words with probability 0.1, replacing words by a filler token with probability 0.1, and swapping words which is implemented as a random permutation over the tokens, drawn from the uniform distribution but restricted to swapping words no further than three positions apart. 3 The majority of our experiments are based on data from the WMT’18 English-German news translation task. We train on all available bitext excluding the ParaCrawl corpus and rem"
D18-1045,D16-1026,0,0.0161244,"slation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that learn distributions over image transformations (Hauberg et al., 2016). cial for the autoencoder setups of (Lample et al., 2018a; Hill et al., 2016) which is inspired by denoising autoencoders (Vincent et al., 2008). In particular, we transform source sent"
D18-1045,W18-2703,0,0.0818873,"swani et al., 2017; Paulus et al., 2018) and multi-head attention (Vaswani et al., 2017). We use a transformer architecture (Vaswani et al., 2017). 2.2 Currey et al. (2017) show that low resource language pairs can also be improved with synthetic data where the source is simply a copy of the monolingual target data. Concurrently to our work, Imamura et al. (2018) show that sampling synthetic sources is more effective than beam search. Specifically, they sample multiple sources for each target whereas we draw only a single sample, opting to train on a larger number of target sentences instead. Hoang et al. (2018) and Cotterell and Kreutzer (2018) suggest an iterative procedure which continuously improves the quality of the back-translation and final systems. Niu et al. (2018) experiment with a multilingual model that does both the forward and backward translation which is continuously trained with new synthetic data. There has also been work using source-side monolingual data (Zhang and Zong, 2016). Furthermore, Cheng et al. (2016); He et al. (2016a); Xia et al. (2017) show how monolingual text from both languages can be leveraged by extending back-translation to dual learning: when training both sour"
D18-1045,W18-2707,0,0.0748514,"coder attends to the most relevant part of the source through a contextdependent weighted-sum of the encoder vectors (Bahdanau et al., 2015; Luong et al., 2015). Attention has been refined with multi-hop attention (Gehring et al., 2017), self-attention (Vaswani et al., 2017; Paulus et al., 2018) and multi-head attention (Vaswani et al., 2017). We use a transformer architecture (Vaswani et al., 2017). 2.2 Currey et al. (2017) show that low resource language pairs can also be improved with synthetic data where the source is simply a copy of the monolingual target data. Concurrently to our work, Imamura et al. (2018) show that sampling synthetic sources is more effective than beam search. Specifically, they sample multiple sources for each target whereas we draw only a single sample, opting to train on a larger number of target sentences instead. Hoang et al. (2018) and Cotterell and Kreutzer (2018) suggest an iterative procedure which continuously improves the quality of the back-translation and final systems. Niu et al. (2018) experiment with a multilingual model that does both the forward and backward translation which is continuously trained with new synthetic data. There has also been work using sour"
D18-1045,N18-1032,0,0.0146132,"ultiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that learn distributions over image transformations (Hauberg et al., 2016). cial for the autoencoder setups of (Lample et al., 2018a; Hill et al., 2016) which is inspired by denoising autoencoders (Vincent et al., 2008). In particular, we transform source sentences with three types of noise: delet"
D18-1045,Q17-1024,0,0.0583787,"Missing"
D18-1045,W18-6301,1,0.897331,"jority of results in this paper are in terms of case-sensitive tokenized BLEU (Papineni et al., 2002) but we also report test accuracy with detokenized BLEU using sacreBLEU (Post, 2018). 4 4.1 Generating synthetic sources Back-translation typically uses beam search (Sennrich et al., 2016a) or just greedy search (Lample et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated probability given an input. Beam is generally successful in finding high probability outputs (Ott et al., 2018a). However, MAP prediction can lead to less rich translations (Ott et al., 2018a) since it always favors the most likely alternative in case of ambiguity. This is particularly problematic in tasks where there is a high level of uncertainty such as dialog (Serban et al., 2016) and story generation (Fan et al., 2018). We argue that this is also problematic for a data augmentation scheme such as backtranslation. Beam and greedy focus on the head of the model distribution which results in very regular synthetic source sentences that do not properly cover the true data distribution. As alternative"
D18-1045,P02-1040,0,0.102557,"ditionally we consider a held-out set from the training data of 52K sentence-pairs. We also experiment on the larger WMT’14 English-French task which we filter in the same way as WMT’18 English-German. This results in 35.7M sentence-pairs for training and we learn a joint BPE vocabulary of 44K types. As monolingual data we use newscrawl2010-2014, comprising 31M sentences after language identification (Lui and Baldwin, 2012). We use newstest2012 as development set and report final results on newstest2013-2015. The majority of results in this paper are in terms of case-sensitive tokenized BLEU (Papineni et al., 2002) but we also report test accuracy with detokenized BLEU using sacreBLEU (Post, 2018). 4 4.1 Generating synthetic sources Back-translation typically uses beam search (Sennrich et al., 2016a) or just greedy search (Lample et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated probability given an input. Beam is generally successful in finding high probability outputs (Ott et al., 2018a). However, MAP prediction can lead to less rich translations (Ott et al., 2018a) si"
D18-1045,J10-4005,0,0.0157889,"compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT’14 English-German test set. 1 Introduction Machine translation relies on the statistics of large parallel corpora, i.e. datasets of paired sentences in both the source and target language. However, bitext is limited and there is a much larger amount of monolingual data available. Monolingual data has been traditionally used to train language models which improved the fluency of statistical machine translation (Koehn, 2010). In the context of neural machine translation (NMT; Bahdanau et al. 2015; Gehring et al. 2017; Vaswani et al. 2017), there has been extensive work to improve models with monolingual data, including language model fusion (Gulcehre et al., 2015, 2017), back-translation (Sennrich et al., 2016a) and dual learning (Cheng et al., 2016; He et al., 2016a). These methods have different advantages and can be combined to reach high accuracy (Hassan et al., 2018). *Work done while at Facebook AI Research. 489 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 48"
D18-1045,P07-2045,0,0.0115511,"restricted to swapping words no further than three positions apart. 3 The majority of our experiments are based on data from the WMT’18 English-German news translation task. We train on all available bitext excluding the ParaCrawl corpus and remove sentences longer than 250 words as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. For the backtranslation experiments we use the German monolingual newscrawl data distributed with WMT’18 comprising 226M sentences after removing duplicates. We tokenize all data with the Moses tokenizer (Koehn et al., 2007) and learn a joint source and target Byte-Pair-Encoding (BPE; Sennrich et al., 2016) with 35K types. We develop on newstest2012 and report final results on newstest20132017; additionally we consider a held-out set from the training data of 52K sentence-pairs. We also experiment on the larger WMT’14 English-French task which we filter in the same way as WMT’18 English-German. This results in 35.7M sentence-pairs for training and we learn a joint BPE vocabulary of 44K types. As monolingual data we use newscrawl2010-2014, comprising 31M sentences after language identification (Lui and Baldwin, 20"
D18-1045,N03-1017,0,0.0634733,"al. (2016); He et al. (2016a); Xia et al. (2017) show how monolingual text from both languages can be leveraged by extending back-translation to dual learning: when training both source-to-target and target-to-source models jointly, one can use back-translation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent outputs during decoding (Koehn et al., 2003; Brants et al., 2007). 490 lar idea is applied in unsupervised NMT (Lample et al., 2018a,b). Besides monolingual data, various approaches have been introduced to benefit from parallel data in other language pairs (Johnson et al., 2017; Firat et al., 2016a,b; Ha et al., 2016; Gu et al., 2018). Data augmentation is an established technique in computer vision where a labeled dataset is supplemented with cropped or rotated input images. Recently, generative adversarial networks (GANs) have been successfully used to the same end (Antoniou et al., 2017; Perez and Wang, 2017) as well as models that"
D18-1045,W11-2132,0,0.0283452,". 2.1 Back-translation (BT) is an alternative to leverage monolingual data. BT is simple and easy to apply as it does not require modification to the MT training algorithms. It requires training a targetto-source system in order to generate additional synthetic parallel data from the monolingual target data. This data complements human bitext to train the desired source-to-target system. BT has been applied earlier to phrase-base systems (Bojar and Tamchyna, 2011). For these systems, BT has also been successful in leveraging monolingual data for domain adaptation (Bertoldi and Federico, 2009; Lambert et al., 2011). Recently, BT has been shown beneficial for NMT (Sennrich et al., 2016a; Poncelas et al., 2018). It has been found to be particularly useful when parallel data is scarce (Karakanta et al., 2017). Neural machine translation We build upon recent work on neural machine translation which is typically a neural network with an encoder/decoder architecture. The encoder infers a continuous space representation of the source sentence, while the decoder is a neural language model conditioned on the encoder output. The parameters of both models are learned jointly to maximize the likelihood of the targe"
D18-1045,W18-6319,0,0.0668607,"eriment on the larger WMT’14 English-French task which we filter in the same way as WMT’18 English-German. This results in 35.7M sentence-pairs for training and we learn a joint BPE vocabulary of 44K types. As monolingual data we use newscrawl2010-2014, comprising 31M sentences after language identification (Lui and Baldwin, 2012). We use newstest2012 as development set and report final results on newstest2013-2015. The majority of results in this paper are in terms of case-sensitive tokenized BLEU (Papineni et al., 2002) but we also report test accuracy with detokenized BLEU using sacreBLEU (Post, 2018). 4 4.1 Generating synthetic sources Back-translation typically uses beam search (Sennrich et al., 2016a) or just greedy search (Lample et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated probability given an input. Beam is generally successful in finding high probability outputs (Ott et al., 2018a). However, MAP prediction can lead to less rich translations (Ott et al., 2018a) since it always favors the most likely alternative in case of ambiguity. This is parti"
D18-1045,J82-2005,0,0.694397,"Missing"
D18-1045,P16-1162,0,0.844173,"are available. Back-translation first trains an intermediate system on the parallel data which is used to translate the target monolingual data into the source language. The result is a parallel corpus where the source side is synthetic machine translation output while the target is genuine text written by humans. The synthetic parallel corpus is then simply added to the real bitext in order to train a final system that will translate from the source to the target language. Although simple, this method has been shown to be helpful for phrase-based translation (Bojar and Tamchyna, 2011), NMT (Sennrich et al., 2016a; Poncelas et al., 2018) as well as unsupervised MT (Lample et al., 2018a). In this paper, we investigate back-translation for neural machine translation at a large scale by adding hundreds of millions of back-translated sentences to the bitext. Our experiments are based on strong baseline models trained on the public bitext of the WMT competition. We extend previous analysis (Sennrich et al., 2016a; Poncelas et al., 2018) of back-translation in several ways. We provide a comprehensive analysis of different methods to generate synthetic source sentences and we show that this choice matters: s"
D18-1045,P12-3005,0,0.0288967,"Koehn et al., 2007) and learn a joint source and target Byte-Pair-Encoding (BPE; Sennrich et al., 2016) with 35K types. We develop on newstest2012 and report final results on newstest20132017; additionally we consider a held-out set from the training data of 52K sentence-pairs. We also experiment on the larger WMT’14 English-French task which we filter in the same way as WMT’18 English-German. This results in 35.7M sentence-pairs for training and we learn a joint BPE vocabulary of 44K types. As monolingual data we use newscrawl2010-2014, comprising 31M sentences after language identification (Lui and Baldwin, 2012). We use newstest2012 as development set and report final results on newstest2013-2015. The majority of results in this paper are in terms of case-sensitive tokenized BLEU (Papineni et al., 2002) but we also report test accuracy with detokenized BLEU using sacreBLEU (Post, 2018). 4 4.1 Generating synthetic sources Back-translation typically uses beam search (Sennrich et al., 2016a) or just greedy search (Lample et al., 2018a,b) to generate synthetic source sentences. Both are approximate algorithms to identify the maximum a-posteriori (MAP) output, i.e. the sentence with the largest estimated"
D18-1045,D15-1166,0,0.0518442,"us space representation of the source sentence, while the decoder is a neural language model conditioned on the encoder output. The parameters of both models are learned jointly to maximize the likelihood of the target sentences given the corresponding source sentences from a parallel corpus (Sutskever et al., 2014; Cho et al., 2014). At inference, a target sentence is generated by left-to-right decoding. Different neural architectures have been proposed with the goal of improving efficiency and/or effectiveness. This includes recurrent networks (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), convolutional networks (Kalchbrenner et al., 2016; Gehring et al., 2017; Kaiser et al., 2017) and transformer networks (Vaswani et al., 2017). Recent work relies on attention mechanisms where the encoder produces a sequence of vectors and, for each target token, the decoder attends to the most relevant part of the source through a contextdependent weighted-sum of the encoder vectors (Bahdanau et al., 2015; Luong et al., 2015). Attention has been refined with multi-hop attention (Gehring et al., 2017), self-attention (Vaswani et al., 2017; Paulus et al., 2018) and multi-head attention (Vaswan"
D18-1045,N18-2074,0,0.0175579,"n all 226M available monolingual training sentences and perform 250K updates in 22.5 hours on 128 GPUs. We upsample the bitext with a rate of 16 so that we observe every bitext sentence 2 3 sacreBLEU signatures: BLEU+case.mixed+lang.enfr+numrefs.1+smooth.exp+test.SET+tok.13a+version.1.2.7 with SET ∈ {wmt13, wmt14/full, wmt15} sacreBLEU signatures: BLEU+case.mixed+lang.enLANG+numrefs.1+smooth.exp+test.wmt14/full+ tok.13a+version.1.2.7 with LANG ∈ {de,fr} 496 news13 bitext +sampling 36.97 37.85 news14 news15 42.90 45.60 a. Gehring et al. (2017) b. Vaswani et al. (2017) c. Ahmed et al. (2017) d. Shaw et al. (2018) 39.92 43.95 Table 4: Tokenized BLEU on various test sets for WMT English-French translation. bitext +sampling news13 news14 news15 35.30 36.13 41.03 43.84 38.31 40.91 DeepL Our result detok. sacreBLEU3 33.3 35.0 33.8 45.9 45.6 43.8 BLEU on newstest2014 for WMT English-German (En–De) and English-French (En–Fr). The first four results use only WMT bitext (WMT’14, except for b, c, d in En–De which train on WMT’16). DeepL uses proprietary high-quality bitext and our result relies on back-translation with 226M newscrawl sentences for En–De and 31M for En–Fr. We also show detokenized BLEU (SacreBLE"
D18-1045,W18-2710,0,0.0787135,"Missing"
D18-1045,D16-1160,0,0.0797918,"tic sources is more effective than beam search. Specifically, they sample multiple sources for each target whereas we draw only a single sample, opting to train on a larger number of target sentences instead. Hoang et al. (2018) and Cotterell and Kreutzer (2018) suggest an iterative procedure which continuously improves the quality of the back-translation and final systems. Niu et al. (2018) experiment with a multilingual model that does both the forward and backward translation which is continuously trained with new synthetic data. There has also been work using source-side monolingual data (Zhang and Zong, 2016). Furthermore, Cheng et al. (2016); He et al. (2016a); Xia et al. (2017) show how monolingual text from both languages can be leveraged by extending back-translation to dual learning: when training both source-to-target and target-to-source models jointly, one can use back-translation in both directions and perform multiple rounds of BT. A simiSemi-supervised NMT Monolingual target data has been used to improve the fluency of machine translations since the early IBM models (Brown et al., 1990). In phrase-based systems, language models (LM) in the target language increase the score of fluent ou"
D19-1539,I05-5002,0,0.0272817,"Missing"
D19-1539,W07-1401,0,0.115964,"Missing"
D19-1539,L18-1550,0,0.0519103,"Missing"
D19-1539,P18-1249,0,0.0230217,"79.3 80.4 Table 5: Different loss functions on the development sets of GLUE (cf. Table 2). Results are based on the CNN base model (Table 1) CRF with 1E-03 and pretrained language model with 1E-05 gave us the best result. Table 3 shows the results, with comparison to previous published ELMoBASE results (Peters et al., 2018) and the BERT models. Both of our stacking methods outperform the previous state of the art, but fine tuning gives the biggest gain. Constituency Parsing We also report parseval F1 for Penn Treebank constituency parsing. We adopted the current state-ofthe-art architecture (Kitaev and Klein, 2018). We again used grid search for learning rates and number of layers in parsing encoder, and used 8E-04 for language model finetuning, 8E-03 for the parsing model parameters, and two layers for encoder. Table 4 shows the results. Here, fine tuning is required to achieve gains over the previous state of the art, which used ELMo embeddings. 6.3 Average GLUE score Avg. GLUE score 6.2.2 81.5 Objective functions for pretraining The two-tower model is trained to predict the current token given representations of the entire left and right context (cloze). Next we compare this choice to two alternative"
D19-1539,K16-1006,0,0.0373645,"combined to predict center words. BERT is also a transformer encoder that has access to the entire input but this choice requires a special training regime. In particular, they multi-task between predicting a subset of masked input tokens, similar to a denoising autoencoder, and a next sentence prediction task. In comparison, we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens. We use all tokens as training targets and therefore extract learning signal from every single token in the sentence and not just a subset. Melamud et al. (2016) follow a similar approach to ours by predicting the center word but their architecture is based on LSTMs and we include the center word when we actually fine-tune on downstream tasks. BERT tailors pretraining to capture dependencies between sentences via a next sentence prediction task as well as by constructing training examples of sentence-pairs with input markers that distinguish between tokens of the two sentences. Our model is trained similarly to a classical language model since we do not adapt the training examples to resemble the end task data and we do not solve a denoising task duri"
D19-1539,N19-4009,1,0.87296,"Missing"
D19-1539,W18-6301,1,0.836844,"erated gradient method (Sutskever et al., 2013) with a momentum of 0.99 and we renormalize gradients if their norm exceeds 0.1 (Pascanu et al., 2013). The learning rate is linearly warmed up from 10−7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 (Loshchilov and Hutter, 2016). We run experiments on DGX-1 machines with 8 NVIDIA V100 GPUs and machines are interconnected by Infiniband. We also use the NCCL2 library and the torch.distributed package for interGPU communication. We train models with 16bit floating point precision, following Ott et al. (2018). The BPE model trains much faster than the character CNN models (Table 1). 6 6.1 Results GLUE First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in Wang et al. (2018). There are two singlesentence classification tasks: First, the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is a binary task to judge sentence grammaticality; evaluation is in terms of the Matthews correlation coefficient (mcc). Second, the Stanford Sentiment Treebank (SST-2"
D19-1539,N18-1202,1,0.936093,"s ablated and must be predicted given the rest of the text. Experiments demonstrate large performance gains on GLUE and new state of the art results on NER as well as constituency parsing benchmarks, consistent with BERT. We also present a detailed analysis of a number of factors that contribute to effective pretraining, including data domain and size, model capacity, and variations on the cloze objective. 1 comb Introduction Language model pretraining has recently been shown to provide significant performance gains for a range of challenging language understanding problems (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018). However, existing work has either used unidirectional (left-to-right) language models (LMs) (Radford et al., 2018) or bi-directional (both left-to-right and right-to-left) LMs (BiLMs) where each direction is trained with an independent loss function (Peters et al., 2018). In this paper, we show that even larger performance gains are possible by jointly pretraining both directions of a large language-model-inspired self-attention cloze model. Our bi-directional transformer architecture predicts every token in the training data (Figure 1). We achieve this by introducing"
D19-1539,E17-2025,0,0.022973,"96 4096 12 32 32 Query formation (final layer) Sum Concat Concat Train time (days) 6 10 4.5 Table 1: Hyper-parameters for our models. Parameter count excludes the (adaptive) softmax layer. Train time as measured on 128 Volta GPUs for the CNN models and 64 Volta GPUs for the BPE model. 1024, followed by a 160K band with dimensionality 256. The remaining types have dimensionality 64; there are 480K types for the small model and 780K for the large model. The BPE model uses a vocabulary of 55K types and we share input and output embeddings in a flat softmax with dimension 1024 (Inan et al., 2016; Press and Wolf, 2017). The BPE vocabulary was constructed by applying 30K merge operations over the training data, then applying the BPE code to the training data and retaining all types occurring at least three times. Every setup uses model dimensionaltiy d = 1024 with H = 16 attention heads for all but the final attention layer. Model based on character inputs use character embedding size 128 and we apply six filters of size 1x128, 2x256, 3x384, 4x512, 5x512, 6x512 followed by a single highway layer. The models are trained with model and attention dropout rate of 0.1 and ReLU dropout rate of 0.05. Different to V"
D19-1539,P16-1162,0,0.0572388,"pus + Wikipedia. This is similar to the training data used by BERT which comprises the BooksCorpus (Zhu et al., 2015) of about 800M words plus English Wikipedia data of 2.5B words. 5.2 Pretraining hyper-parameters We adapt the transformer implementation available in the fairseq toolkit to our two tower architecture (Ott et al., 2019). For hyper-parameter and optimization choices we mostly follow Baevski and Auli (2018). Our experiments consider three model sizes shown in Table 1: There are two CNN input models in a base and large configuration as well as a Byte-Pair-Encoding based model (BPE; Sennrich et al., 2016). The CNN models have unconstrained input vocabulary, and an output vocabulary limited to 1M most common types for the large model, and 700K most common types for the base model. CNN models use an adaptive softmax in the output: the head band contains the 60K most frequent types with dimensionality 5363 Model CNN Base CNN Large BPE Large Parameters Updates Blocks FFN Dim Attn Heads (final layer) 177M 330M 370M 600K 1M 1M 6 12 12 4096 4096 4096 12 32 32 Query formation (final layer) Sum Concat Concat Train time (days) 6 10 4.5 Table 1: Hyper-parameters for our models. Parameter count excludes t"
D19-1539,D13-1170,0,0.00600016,"The BPE model trains much faster than the character CNN models (Table 1). 6 6.1 Results GLUE First, we conduct experiments on the general language understanding evaluation benchmark (GLUE; Wang et al., 2018) and present a short overview of the tasks. More information can be found in Wang et al. (2018). There are two singlesentence classification tasks: First, the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018) is a binary task to judge sentence grammaticality; evaluation is in terms of the Matthews correlation coefficient (mcc). Second, the Stanford Sentiment Treebank (SST-2; Socher et al., 2013) requires to judge if movie reviews have positive or negative sentiment; evaluation is in terms of accuracy (acc). There are three tasks assessing sentence similarity: The Microsoft Research Paragraph Corpus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we evaluate in terms of F1. The Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefficient (scc). Finally, there are four natural language inference tasks: the Multi-Genre Natural Langu"
D19-1539,W18-5446,0,0.0427983,"Missing"
D19-1539,N18-1101,0,0.0438876,"movie reviews have positive or negative sentiment; evaluation is in terms of accuracy (acc). There are three tasks assessing sentence similarity: The Microsoft Research Paragraph Corpus (MRPC; Dolan and Brockett, 2015) and the Quora Question Pairs benchmark (QQP); we evaluate in terms of F1. The Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) requires predicting a similarity score between 1 and 5 for a sentence pair; we report the Spearman correlation coefficient (scc). Finally, there are four natural language inference tasks: the Multi-Genre Natural Language Inference (MNLI; Williams et al., 2018), the Stanford Question Answering Dataset (QNLI; Rajpurkar et al., 2016), the Recognizing Textual Entailment (RTE; Dagan et al., 2006, Bar Haim et al., 2006, Ciampiccolo et al., 2007 Bentivogli et al., 2009). We exclude the Winograd NLI task from our results similar to Radford et al. (2018); Devlin et al. (2018) and report accuracy. For MNLI we report both matched (m) and mismatched (mm) accuracy on test. We also report an average over the GLUE metrics. This figure is not comparable to the average on the official GLUE leaderboard since we exclude Winograd and do not report MRPC accuracy 5364 C"
D19-1539,W18-5448,0,0.0209368,"lly (3) our novel cloze-driven training regime is more effective than predicting left and right tokens separately. 2 Related work There has been much recent work on learning sentence-specific representations for language understanding tasks. McCann et al. (2017) learn contextualized word representations from a sequence to sequence translation task and uses the representations from the encoder network to improve a variety of language understanding tasks. Subsequent work focused on language modeling pretraining which has been shown to be more effective and which does not require bilingual data (Zhang and Bowman, 2018). Our work was inspired by ELMo (Peters et al., 2018) and the generative pretraining (GPT) approach of Radford et al. (2018). ELMo introduces language models to pretrain word representations for downstream tasks including a novel mechanism to learn a combination of different layers in the language model that is most beneficial to the current task. GPT relies on a left to right language model and an added projection layer for each downstream task without a task-specific model. Our approach mostly follows GPT, though we show that our model also works well with an ELMo module on NER and constitue"
D19-1539,D16-1264,0,\N,Missing
D19-1539,S17-2001,0,\N,Missing
D19-1539,W18-6401,0,\N,Missing
D19-1539,N19-1423,0,\N,Missing
D19-1571,J93-2003,0,0.117255,"decision. Introduction Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an incomplete target sentence. This bases decoding decisions on scoring the entire source sequence and it is very simple and effective (§2). We a"
D19-1571,W18-6408,0,0.0213561,"e decoding due to the limited conditioning context provided by partial target prefixes. 3.5 Re-ranking Next, we switch to n-best re-ranking where we have the full target sentence available compared to online decoding. Noisy channel model reranking has been used by the top ranked entries of the WMT 2019 news translation shared task for English-German, German-English, EnglshRussian and Russian-English (Ng et al., 2019). We compare to various baselines including right-toleft sequence to sequence models which are a popular choice for re-ranking and regularly feature in successful WMT submissions (Deng et al., 2018; 5699 DIR DIR ENS DIR + LM DIR + RL DIR + RL + LM CH + DIR CH + DIR + LM 5 10 50 100 39.1 40.1 39.2 40.2 39.3 40.3 39.2 40.3 40.0 39.7 40.4 39.7 40.8 40.2 40.1 40.9 40.0 41.5 40.6 40.8 41.6 40.5 42.8 40.7 40.8 41.8 40.5 43.2 Table 2: Re-ranking BLEU with different n-best list sizes on news2016 of WMT De-En. We compare to decoding with a direct model only (DIR) and decoding with an ensemble of direct models (DIR ENS). Table 5 in the appendix shows standard deviations. DIR DIR ENS DIR + LM DIR + RL DIR + RL + LM CH + DIR CH + DIR + LM WMT De-En WMT En-De WMT Zh-En IWSLT De-En 34.5 35.5 28.4 29."
D19-1571,D18-1045,1,0.938094,"anguages where word order differs significantly, we may need to take the entire source sentence into account to make a decision. Introduction Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an incomplete target sentenc"
D19-1571,N18-1033,1,0.929482,"anguages where word order differs significantly, we may need to take the entire source sentence into account to make a decision. Introduction Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an incomplete target sentenc"
D19-1571,W18-6415,0,0.0205476,"6 of WMT De-En. We compare to decoding with a direct model only (DIR) and decoding with an ensemble of direct models (DIR ENS). Table 5 in the appendix shows standard deviations. DIR DIR ENS DIR + LM DIR + RL DIR + RL + LM CH + DIR CH + DIR + LM WMT De-En WMT En-De WMT Zh-En IWSLT De-En 34.5 35.5 28.4 29.0 24.4 25.2 33.3 34.5 36.0 35.7 36.8 35.1 37.7 29.4 29.3 30.0 28.3 30.5 24.9 25.3 25.4 24.8 25.6 34.2 34.4 34.9 34.0 35.5 Table 3: Re-ranking accuracy with k1 = 50 on four language directions on the respective test sets. See Table 6 in the appendix for standard deviations. Koehn et al., 2018; Junczys-Dowmunt, 2018). Table 2 shows that the noisy channel model outperforms the baseline (DIR) by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU (DIR ENS) and the best right-to-left configuration by 1.4 BLEU (DIR + RL + LM). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from k1 = 5 to k1 = 100. Other methods improve a lot less with larger beams, e.g., DIR + RL + LM has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Adding a language model bene"
D19-1571,W17-4742,0,0.0201372,"M) but the channel approach benefits most (CH + DIR vs CH + DIR + LM ). The direct model with a language model (DIR + LM) performs better than for online decoding, likely because the constrained reranking setup mitigates explaining away effects (cf. Table 1). Interestingly, both CH + DIR or DIR + LM give only modest improvements compared to CH + DIR + LM . Although previous work demonstrated that reranking with CH + DIR can improve over DIR, we show that the channel model is important to properly leverage the language model without suffering from explaining away effects (Xu and Carpuat, 2018; Wang et al., 2017). Test results on all language directions confirm that CH + DIR + LM performs best (Table 3). 4 Conclusion Previous work relied on incremental channel models which do not make use of the entire source even though it is available and, as we demonstrate, beneficial. Standard sequence to sequence models are a simple parameterization for the channel probability that naturally exploits the entire source. This parameterization outperforms strong baselines such as ensembles of direct models and right-to-left models. Channel models are particularly effective with large context sizes and an interesting"
D19-1571,W18-6431,0,0.02323,"RL + LM, CH + DIR + LM) but the channel approach benefits most (CH + DIR vs CH + DIR + LM ). The direct model with a language model (DIR + LM) performs better than for online decoding, likely because the constrained reranking setup mitigates explaining away effects (cf. Table 1). Interestingly, both CH + DIR or DIR + LM give only modest improvements compared to CH + DIR + LM . Although previous work demonstrated that reranking with CH + DIR can improve over DIR, we show that the channel model is important to properly leverage the language model without suffering from explaining away effects (Xu and Carpuat, 2018; Wang et al., 2017). Test results on all language directions confirm that CH + DIR + LM performs best (Table 3). 4 Conclusion Previous work relied on incremental channel models which do not make use of the entire source even though it is available and, as we demonstrate, beneficial. Standard sequence to sequence models are a simple parameterization for the channel probability that naturally exploits the entire source. This parameterization outperforms strong baselines such as ensembles of direct models and right-to-left models. Channel models are particularly effective with large context size"
D19-1571,W18-6417,0,0.0186361,"ist sizes on news2016 of WMT De-En. We compare to decoding with a direct model only (DIR) and decoding with an ensemble of direct models (DIR ENS). Table 5 in the appendix shows standard deviations. DIR DIR ENS DIR + LM DIR + RL DIR + RL + LM CH + DIR CH + DIR + LM WMT De-En WMT En-De WMT Zh-En IWSLT De-En 34.5 35.5 28.4 29.0 24.4 25.2 33.3 34.5 36.0 35.7 36.8 35.1 37.7 29.4 29.3 30.0 28.3 30.5 24.9 25.3 25.4 24.8 25.6 34.2 34.4 34.9 34.0 35.5 Table 3: Re-ranking accuracy with k1 = 50 on four language directions on the respective test sets. See Table 6 in the appendix for standard deviations. Koehn et al., 2018; Junczys-Dowmunt, 2018). Table 2 shows that the noisy channel model outperforms the baseline (DIR) by up to 4.0 BLEU for very large beams, the ensemble by up to 2.9 BLEU (DIR ENS) and the best right-to-left configuration by 1.4 BLEU (DIR + RL + LM). The channel approach improves more than other methods with larger n-best lists by adding 2.4 BLEU from k1 = 5 to k1 = 100. Other methods improve a lot less with larger beams, e.g., DIR + RL + LM has the next largest improvement of 1.4 BLEU when increasing the beam size but this is still significantly lower than for the noisy channel approach. Addi"
D19-1571,N03-1017,0,0.125063,"ion Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an incomplete target sentence. This bases decoding decisions on scoring the entire source sequence and it is very simple and effective (§2). We analyze this approach"
D19-1571,W19-5333,1,0.837739,"ent shows the importance of large context sizes for the channel approach to work well. It indicates that the channel approach may not be able to effectively exploit the large search space in online decoding due to the limited conditioning context provided by partial target prefixes. 3.5 Re-ranking Next, we switch to n-best re-ranking where we have the full target sentence available compared to online decoding. Noisy channel model reranking has been used by the top ranked entries of the WMT 2019 news translation shared task for English-German, German-English, EnglshRussian and Russian-English (Ng et al., 2019). We compare to various baselines including right-toleft sequence to sequence models which are a popular choice for re-ranking and regularly feature in successful WMT submissions (Deng et al., 2018; 5699 DIR DIR ENS DIR + LM DIR + RL DIR + RL + LM CH + DIR CH + DIR + LM 5 10 50 100 39.1 40.1 39.2 40.2 39.3 40.3 39.2 40.3 40.0 39.7 40.4 39.7 40.8 40.2 40.1 40.9 40.0 41.5 40.6 40.8 41.6 40.5 42.8 40.7 40.8 41.8 40.5 43.2 Table 2: Re-ranking BLEU with different n-best list sizes on news2016 of WMT De-En. We compare to decoding with a direct model only (DIR) and decoding with an ensemble of direct"
D19-1571,N19-4009,1,0.858643,"ost, 2018). Language Models. We train two big Transformer language models with 12 blocks (Baevski and Auli, 2018): one on the German newscrawl data distributed by WMT’18 comprising 260M sentences and another one on the English newscrawl data comprising 193M sentences. Both use a BPE vocabulary of 32K types. We train on 32 Nvidia V100 GPUs with 16-bit floating point operations (Ott et al., 2018) and training took four days. Sequence to Sequence Model training. For EnDe, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer (Vaswani et al., 2017) as implemented in fairseq (Ott et al., 2019). For online decoding experiments, we do not share encoder and decoder embeddings since the source and target vocabularies were learned separately. We report average accuracy of three random initializations of a each configuration. We generally use k1 = 5 and k2 = 10. We tune 1 , and a length penalty on the validation set. Simple Channel Model We first motivate a standard sequence to sequence model to parameterize p(x|y) as opposed to a model that is trained to operate over prefixes. We train Transformer models to translate from the target to the source (En-De) and compare two variants: i) a s"
D19-1571,W18-6301,1,0.866243,"(2018), we develop on dev2017 and test on news2017. For IWSLT’14 De-En we follow the setup of Edunov et al. (2018b) and measure case-sensitive tokenized BLEU. For WMT DeEn, En-De and Zh-En we measure detokenized BLEU (Post, 2018). Language Models. We train two big Transformer language models with 12 blocks (Baevski and Auli, 2018): one on the German newscrawl data distributed by WMT’18 comprising 260M sentences and another one on the English newscrawl data comprising 193M sentences. Both use a BPE vocabulary of 32K types. We train on 32 Nvidia V100 GPUs with 16-bit floating point operations (Ott et al., 2018) and training took four days. Sequence to Sequence Model training. For EnDe, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer (Vaswani et al., 2017) as implemented in fairseq (Ott et al., 2019). For online decoding experiments, we do not share encoder and decoder embeddings since the source and target vocabularies were learned separately. We report average accuracy of three random initializations of a each configuration. We generally use k1 = 5 and k2 = 10. We tune 1 , and a length penalty on the validation set. Simple Channel Model We first motivate a standard seque"
D19-1571,W18-6319,0,0.0435397,"sible target prefixes with the full source (prefix-model). We measure accuracy of predicting the full source with increasing target prefixes for both models. Results are on news2016. byte pair encoding on the source portion of the bitext; we find using LM and bitext vocabularies give similar accuracy. For Chinese-English (ZhEn), we pre-process WMT’17 data following Hassan et al. (2018), we develop on dev2017 and test on news2017. For IWSLT’14 De-En we follow the setup of Edunov et al. (2018b) and measure case-sensitive tokenized BLEU. For WMT DeEn, En-De and Zh-En we measure detokenized BLEU (Post, 2018). Language Models. We train two big Transformer language models with 12 blocks (Baevski and Auli, 2018): one on the German newscrawl data distributed by WMT’18 comprising 260M sentences and another one on the English newscrawl data comprising 193M sentences. Both use a BPE vocabulary of 32K types. We train on 32 Nvidia V100 GPUs with 16-bit floating point operations (Ott et al., 2018) and training took four days. Sequence to Sequence Model training. For EnDe, De-En, Zh-En we use big Transformers and for IWSLT De-En a base Transformer (Vaswani et al., 2017) as implemented in fairseq (Ott et al."
D19-1571,P16-1009,0,0.116859,"source prefix, but for languages where word order differs significantly, we may need to take the entire source sentence into account to make a decision. Introduction Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an inc"
D19-1571,P16-1162,0,0.53503,"source prefix, but for languages where word order differs significantly, we may need to take the entire source sentence into account to make a decision. Introduction Sequence to sequence models directly estimate the posterior probability of a target sequence y given a source sequence x (Sutskever et al., 2014; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) and can be trained with pairs of source and target sequences. Unpaired sequences can be leveraged by data augmentation schemes such as back-translation, but direct models cannot naturally take advantage of unpaired data (Sennrich et al., 2016a; Edunov et al., 2018a). The noisy channel approach is an alternative which is used in statistical machine translation (Brown et al., 1993; Koehn et al., 2003). It entails a channel model probability p(x|y) that operates in the reverse direction as well as a language † Work done while at Facebook AI Research. We release code and pre-trained models https://github.com/pytorch/fairseq 1 at In this paper, we show that a standard sequence to sequence model is an effective parameterization of the channel probability. We train the model on full sentences and apply it to score the source given an inc"
E14-1003,D13-1176,0,0.0658436,"eural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging. We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and"
E14-1003,W06-3114,0,0.0109025,"rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The parallel corpus includes about 35M words of parliamentary proceedings for training, a development set and two test sets with 2000 sentences each. Second, we experiment with a French to English system based on 102M words of training data from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for about 5m words which are newswire; all MTU models are trained on the newswire subset since we found similar accuracy to using all data in initial experiments. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 s"
E14-1003,W12-2703,0,0.131776,"context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models. We therefore focus in this paper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which mak"
E14-1003,N03-1017,0,0.0394319,"Missing"
E14-1003,P07-2045,0,0.00595061,"ed to a convolutional output layer yt by weights summarized in the sparse 4 24 In initial experiments we found this model to be over twenty times slower than the atomic MTU RNN model with estimated training times of over 6 weeks. This was despite using a vastly smaller vocabulary and by computing the word layer on a, by current standards, high-end GPU (NVIDIA Tesla K20c) using sparse matrix optimizations (cuSPARSE) for the convolutional layer. 5 mt 1 src MTU D 0 1 ht V 1 0 mt+1 0 W ht-1 5.1 T ct Experimental Setup Baselines. We experiment with an in-house phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder"
E14-1003,D13-1106,1,0.695842,"aper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging. We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a s"
E14-1003,W05-0823,0,0.0608087,"Missing"
E14-1003,N12-1005,0,0.579306,"ersity of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-"
E14-1003,J92-4003,0,0.17638,"Missing"
E14-1003,P13-2121,0,0.0223918,"Missing"
E14-1003,P03-1021,0,0.123998,"phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The paral"
E14-1003,N06-1002,0,0.0800215,"ity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harde"
E14-1003,W12-2707,0,0.0646677,"Missing"
E14-1003,D07-1045,0,0.150337,"Missing"
E14-1003,W12-2702,0,0.018086,"um Translation Modeling with Recurrent Neural Networks Yuening Hu Department of Computer Science University of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited c"
E14-1003,D13-1140,0,0.0985043,"g with Recurrent Neural Networks Yuening Hu Department of Computer Science University of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, si"
E14-1003,N13-1002,1,0.723742,"and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram mod"
E14-1003,P13-2071,0,\N,Missing
E14-1003,N13-1090,0,\N,Missing
E14-1003,N13-1001,0,\N,Missing
E14-1003,W11-2135,0,\N,Missing
N15-1020,D13-1106,1,0.323212,"bels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses c"
N15-1020,W05-0909,0,0.0736125,"uced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using m"
N15-1020,D14-1179,0,0.0151747,"Missing"
N15-1020,P14-1129,0,0.0340037,"when generating long responses. 4.3 Dynamic-Context Generative Model II Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r. Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right). Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014). The forward equations for the context encoder are: 1 > 1 k1 = [b> c Wf , bm Wf ], > ` k` = σ(k`−1 Wf ) for ` = 2, · · · , L (8) where [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7. 5 Experimental Setting 5.1 Dataset Construction For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence. Hence, our dataset is composed of “triples” τ ≡ (cτ , mτ , rτ ) consisting of three sent"
N15-1020,P14-1066,1,0.560619,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D14-1002,1,0.788594,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D13-1176,0,0.046151,"tates. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probabilit"
N15-1020,P07-2045,0,0.0499714,"3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011). We also included MT decoder features specifical"
N15-1020,J04-4002,0,0.0179799,"a set of candidate triples {˜ τ }, human evaluators are asked to rate the quality of the response within the new triples {(cτ , mτ , rτ˜ )}. After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very lar"
N15-1020,P03-1021,0,0.041731,"r generalization. The last layer embeds the context vector into the hidden space of the decoder RLM. 5.5 Rescoring Setup We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system. In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues. The different n-best lists will provide a comprehensive testbed for our experiments. First, we augment the n-best list of the tuning set with the scores of the model of interest. Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we rescore the test n-best list with the new weights. 6 Results 6.1 Lower and Upper Bounds Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline. The RAN DOM system comprises responses randomly extracted from the triples corpus. HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems"
N15-1020,P02-1040,0,0.121444,"an 3 times in the corpus. This produced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by o"
N15-1020,D11-1054,0,0.803346,"on-context-sensitive Machine Translation and Information Retrieval baselines. 1 message response yeah i’m on my way now ok good luck ! Figure 1: Example of three consecutive utterances occurring between two Twitter users A and B. Introduction Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive. However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally. The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response. However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of *The entirety of this work was conducted while at Microsoft Research. † Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com). generating responses that are sensitive to the context of the conv"
N15-1176,N06-1003,0,0.146091,"Missing"
N15-1176,P05-1033,0,0.320995,"Missing"
N15-1176,D08-1089,0,0.0324104,"dimensional space. We therefore use RBV in the following experiments. 5.3 Evaluation of Rule Generation Next, we evaluate the quality of the generated translation rules for Arabic-English translation (Table 3) using either SLP, the global linear projection (GLP), or the local linear projection (LLP). Our baseline system is an in-house phrase-based system similar to Moses with a 4-gram language model. The underlying log-linear model comprises of 13 features: two maximum likelihood translation probabilities, two lexicalized translation probabilities, five hierarchical reordering model features (Galley and Manning, 2008), one language model, word penalty, phrase length, and distortion penalty), and is tuned with minimum error rate training (MERT; Och 2003). Translation quality is measured with BLEU (Papineni et al., 2002). For comparison, we reimplemented the graphbased method in Saluja et al. (2014). This method calculates the pairwise mutual information (PMI) between phrases, and employs all the techniques mentioned in Saluja et al. (2014) to speedup the 1533 computations. Our reimplementation achieves similar performance to Saluja et al. (2014) (with a negligible ∼ 0.06 drop in BLEU). We parallelized the a"
N15-1176,P14-1066,0,0.0278822,"horse cow dog pig cerdo (pig) gato (cat) cat Figure 1: Illustration of word representations in Spanish and English (Figure from Mikolov et al. (2013a)). The plots are based on a two-dimensional projection of the original vectors with principal component analysis. A logical next step is to learn representations for larger linguistic units, a topic which has received a lot of interest (Mitchell and Lapata, 2010; Socher et al., 2011; Le and Mikolov, 2014). For machine translation there have been efforts to learn representations for entire bilingual phrases (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). In this work, we only require representations for monolingual phrases that are relatively short.1 We therefore decided to use off-the-shelf word representations to build phrase vectors. In particular, we chose the continuous bag-of-words model (Mikolov et al., 2013b) which is very fast to train and scales very well to large monolingual corpora. The resulting word vectors are then used to build phrase vectors via simple element-wise addition which has been found to perform very competitively in comparison to alternative approaches (Mitchell and Lapata, 2010). Note that all the algorithms desc"
N15-1176,D12-1098,0,0.0522594,"Missing"
N15-1176,P08-1088,0,0.222695,"ethods and 0.5 BLEU more accurate. 1 Introduction Statistical translation models (Koehn et al. 2003, Chiang et al. 2005) are trained with bilingual data and a simple solution to improve accuracy is to train on more data. However, for many language pairs we only have a very limited amount of bilingual data and even when dealing with resource-rich languages, we still often perform poorly when dealing with rare words or phrases. On the other hand, there is plenty of monolingual data and previous work has investigated its use in learning translation models (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Saluja et *The entirety of this work was conducted while at Microsoft Research. Michael Auli∗ Facebook AI Research Menlo Park, CA 94025, USA michaelauli@fb.com al., 2014). However, most methods rely on statistics that are computationally expensive. As a concrete example, the graph propagation algorithm of Saluja et al. (2014) relies on pair-wise mutual information statistics between any pair of phrases in the monolingual corpus that is very expensive to compute, even for moderately sized corpora. In this paper, we study the use of standard continuous representations for words to generate tra"
N15-1176,N03-1017,0,0.074822,"sed on phrases with similar continuous representations for which a translation is known. To speed up the retrieval of similar phrases, we investigate approximated nearest neighbor search with redundant bit vectors which we find to be three times faster and significantly more accurate than locality sensitive hashing. Our approach of learning new translation rules improves a phrase-based baseline by up to 1.6 BLEU on Arabic-English translation, it is three-orders of magnitudes faster than existing semi-supervised methods and 0.5 BLEU more accurate. 1 Introduction Statistical translation models (Koehn et al. 2003, Chiang et al. 2005) are trained with bilingual data and a simple solution to improve accuracy is to train on more data. However, for many language pairs we only have a very limited amount of bilingual data and even when dealing with resource-rich languages, we still often perform poorly when dealing with rare words or phrases. On the other hand, there is plenty of monolingual data and previous work has investigated its use in learning translation models (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Saluja et *The entirety of this work was conducted while at Microsoft Resear"
N15-1176,P12-1032,0,0.0581677,"ind the labeled neighbors for each unlabeled source phrase. However, this extra computation takes only a negligible amount of time, since the number of labeled phrases on the source side is significantly smaller than the number of phrases on the target side. Our approach of learning many different mappings is similar to the locality preserving projections method of He and Niyogi (2004), which also construct a locally precise projection in order to map to another space. 3.3 4 Structured Label Propagation with Continuous Representation Saluja et al. (2014) use Structured Label Propagation (SLP; Liu et al. 2012) to propagate candidate translations from frequent source phrases that are labeled to unlabeled neighbors that are infrequent. The algorithm works as follows: for a known translation rule (f 0 , e0 ), SLP propagates the target side phrases e ∈ N (e0 ), that are similar to e0 , to the unlabeled source phrases f ∈ N (f 0 ), that are similar to f 0 , as new translation rules. This propagation runs for several iterations. At each iteration, the translation probability between known translations is fixed. More formally, for iteration t + 1 we have Pt+1 (e|f ) = X T (f 0 |f ) f 0 ∈N (f ) X T (e|e0 )"
N15-1176,N13-1090,0,0.66107,"l information statistics between any pair of phrases in the monolingual corpus that is very expensive to compute, even for moderately sized corpora. In this paper, we study the use of standard continuous representations for words to generate translation rules for infrequent phrases (§2). We explore linear projections that map continuous representations of rare foreign phrases to English phrases. In particular, we propose to learn many local projections that are specific to a given foreign phrase. We find this to be much more accurate than a single globally learned mapping such as proposed by (Mikolov et al. 2013; §3). Our method relies on the fast retrieval of similar phrases in continuous space. We explore both Locality Sensitive Hashing (LSH; Indyk and Motwani, 2008) as well as the lesser known Redundant Bit Vector method (RBV; Goldstein et al. 2005) for fast k-nearest neighbor (k-NN) search. RBV outperforms the popular LSH algorithm by a large margin, both in speed as well as accuracy (§4). Our results show that the local linear projection method is not only three orders of magnitudes faster than the algorithm of Saluja et al. (2014) but also by 0.5 BLEU more accurate. We achieve a 1.6 BLEU improv"
N15-1176,P03-1021,0,0.272558,"nslation rules for Arabic-English translation (Table 3) using either SLP, the global linear projection (GLP), or the local linear projection (LLP). Our baseline system is an in-house phrase-based system similar to Moses with a 4-gram language model. The underlying log-linear model comprises of 13 features: two maximum likelihood translation probabilities, two lexicalized translation probabilities, five hierarchical reordering model features (Galley and Manning, 2008), one language model, word penalty, phrase length, and distortion penalty), and is tuned with minimum error rate training (MERT; Och 2003). Translation quality is measured with BLEU (Papineni et al., 2002). For comparison, we reimplemented the graphbased method in Saluja et al. (2014). This method calculates the pairwise mutual information (PMI) between phrases, and employs all the techniques mentioned in Saluja et al. (2014) to speedup the 1533 computations. Our reimplementation achieves similar performance to Saluja et al. (2014) (with a negligible ∼ 0.06 drop in BLEU). We parallelized the algorithm on a cluster since a single core implementation would run for ∼ 10k hours.5 Our continuous phrase based version of SLP is orders"
N15-1176,P02-1040,0,0.102978,") using either SLP, the global linear projection (GLP), or the local linear projection (LLP). Our baseline system is an in-house phrase-based system similar to Moses with a 4-gram language model. The underlying log-linear model comprises of 13 features: two maximum likelihood translation probabilities, two lexicalized translation probabilities, five hierarchical reordering model features (Galley and Manning, 2008), one language model, word penalty, phrase length, and distortion penalty), and is tuned with minimum error rate training (MERT; Och 2003). Translation quality is measured with BLEU (Papineni et al., 2002). For comparison, we reimplemented the graphbased method in Saluja et al. (2014). This method calculates the pairwise mutual information (PMI) between phrases, and employs all the techniques mentioned in Saluja et al. (2014) to speedup the 1533 computations. Our reimplementation achieves similar performance to Saluja et al. (2014) (with a negligible ∼ 0.06 drop in BLEU). We parallelized the algorithm on a cluster since a single core implementation would run for ∼ 10k hours.5 Our continuous phrase based version of SLP is orders of magnitudes faster than the SLP variant of Saluja et al. (2014) b"
N15-1176,N10-1021,0,0.0108191,"Missing"
N15-1176,P95-1050,0,0.298918,"s faster than existing semi-supervised methods and 0.5 BLEU more accurate. 1 Introduction Statistical translation models (Koehn et al. 2003, Chiang et al. 2005) are trained with bilingual data and a simple solution to improve accuracy is to train on more data. However, for many language pairs we only have a very limited amount of bilingual data and even when dealing with resource-rich languages, we still often perform poorly when dealing with rare words or phrases. On the other hand, there is plenty of monolingual data and previous work has investigated its use in learning translation models (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Saluja et *The entirety of this work was conducted while at Microsoft Research. Michael Auli∗ Facebook AI Research Menlo Park, CA 94025, USA michaelauli@fb.com al., 2014). However, most methods rely on statistics that are computationally expensive. As a concrete example, the graph propagation algorithm of Saluja et al. (2014) relies on pair-wise mutual information statistics between any pair of phrases in the monolingual corpus that is very expensive to compute, even for moderately sized corpora. In this paper, we study the use of standard c"
N15-1176,P05-1077,0,0.0521322,"em are not efficient enough in high dimensional space, such as our setting. We therefore investigate approximated k-NN query methods which sacrifice some accuracy for a large gain in speed. Specifically, we look into locality sensitive hashing (LSH; §4.1), a popular method, as well as redundant bit vectors (RBV; §4.2), which to our knowledge has not been previously used for natural language processing tasks. 4.1 Locality Sensitive Hashing One popular approximated method is Locality Sensitive Hashing (LSH; Indyk and Motwani, 1998), which has been used in many NLP tasks such as noun clustering (Ravichandran et al., 2005), topic detection (Petrovi´c et al., 2010), and fast k-NN query for similar words (Goyal et al., 2012). For our particular task, assume each phrase is represented by a d-dimensional vector p of real values. The core of LSH is a set of hash functions. We choose p-stable distribution based functions (Datar et al., 2004) of the following form: hi (p) = b xi · p + bi c, 1 ≤ i ≤ s. w This function can be viewed as a quantized random projection, where each element in xi is selected randomly from a Gaussian distribution N (0, 1), w is the width of the bin, bi is a linear bias selected from a uniform"
N15-1176,P14-1064,1,0.278234,"ith resource-rich languages, we still often perform poorly when dealing with rare words or phrases. On the other hand, there is plenty of monolingual data and previous work has investigated its use in learning translation models (Rapp, 1995; CallisonBurch et al., 2006; Haghighi et al., 2008; Saluja et *The entirety of this work was conducted while at Microsoft Research. Michael Auli∗ Facebook AI Research Menlo Park, CA 94025, USA michaelauli@fb.com al., 2014). However, most methods rely on statistics that are computationally expensive. As a concrete example, the graph propagation algorithm of Saluja et al. (2014) relies on pair-wise mutual information statistics between any pair of phrases in the monolingual corpus that is very expensive to compute, even for moderately sized corpora. In this paper, we study the use of standard continuous representations for words to generate translation rules for infrequent phrases (§2). We explore linear projections that map continuous representations of rare foreign phrases to English phrases. In particular, we propose to learn many local projections that are specific to a given foreign phrase. We find this to be much more accurate than a single globally learned map"
N15-1176,P14-1011,0,0.0233061,"le Generation target horse cow dog pig cerdo (pig) gato (cat) cat Figure 1: Illustration of word representations in Spanish and English (Figure from Mikolov et al. (2013a)). The plots are based on a two-dimensional projection of the original vectors with principal component analysis. A logical next step is to learn representations for larger linguistic units, a topic which has received a lot of interest (Mitchell and Lapata, 2010; Socher et al., 2011; Le and Mikolov, 2014). For machine translation there have been efforts to learn representations for entire bilingual phrases (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). In this work, we only require representations for monolingual phrases that are relatively short.1 We therefore decided to use off-the-shelf word representations to build phrase vectors. In particular, we chose the continuous bag-of-words model (Mikolov et al., 2013b) which is very fast to train and scales very well to large monolingual corpora. The resulting word vectors are then used to build phrase vectors via simple element-wise addition which has been found to perform very competitively in comparison to alternative approaches (Mitchell and Lapata, 2010). Note that all"
N15-1176,D13-1141,0,0.0377696,") 3 Translation Rule Generation target horse cow dog pig cerdo (pig) gato (cat) cat Figure 1: Illustration of word representations in Spanish and English (Figure from Mikolov et al. (2013a)). The plots are based on a two-dimensional projection of the original vectors with principal component analysis. A logical next step is to learn representations for larger linguistic units, a topic which has received a lot of interest (Mitchell and Lapata, 2010; Socher et al., 2011; Le and Mikolov, 2014). For machine translation there have been efforts to learn representations for entire bilingual phrases (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). In this work, we only require representations for monolingual phrases that are relatively short.1 We therefore decided to use off-the-shelf word representations to build phrase vectors. In particular, we chose the continuous bag-of-words model (Mikolov et al., 2013b) which is very fast to train and scales very well to large monolingual corpora. The resulting word vectors are then used to build phrase vectors via simple element-wise addition which has been found to perform very competitively in comparison to alternative approaches (Mitchell and Lapata, 2"
N15-1176,P10-1040,0,\N,Missing
N16-1012,P00-1041,0,0.682766,"ata set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization incl"
N16-1012,C08-1018,0,0.0286084,"ere is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance o"
N16-1012,D13-1155,0,0.547969,"cularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Ban"
N16-1012,N10-1131,0,0.0426305,"task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks. Our model can be seen as an extension of their model. 3 Attentive Recurrent Architecture Let x denote the input sentence consisting of a sequence of M words x = [x1 , . . . , xM ], where each word xi is part of vocabulary V, of size |V |= V . Our task is to generate a target sequence y = [y1 , . . . , yN ], of N words, where N < M , such that the meaning of x is preserved:"
N16-1012,A00-1043,0,0.112081,"e state-of-the-art systems of Rush et al. (2015) on multiple data sets. Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heav"
N16-1012,W04-1013,0,0.382019,"nization and sentence separation while discarding other annotations such as tags and parses. We pair the first sentence of each article with its headline to form sentence-summary pairs. The data is pre-processed in the same way as Rush et al. (2015) and we use the same splits for training, validation, and testing. For Gigaword we report results on the same randomly held-out test set of 2000 sentence-summary pairs as (Rush et al., 2015).1 We also evaluate our models on the DUC-2004 evaluation data set comprising 500 pairs (Over et al., 2007). Our evaluation is based on three variants of ROUGE (Lin, 2004), namely, ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring). 4.2 Architectural Choices We implemented our models in the Torch library (http://torch.ch/)2 . To optimize our loss (Equation 5) we used stochastic gradient descent with mini-batches of size 32. During training we measure the perplexity of the summaries in the validation set and adjust our hyper-parameters, such as the learning rate, based on this number. 1 We remove pairs with empty titles resulting in slightly different accuracy compared to Rush et al. (2015) for their systems. 2 Our code can found at ww"
N16-1012,D15-1166,0,0.367153,"Missing"
N16-1012,E06-1038,0,0.0119889,"t al. (2015) on multiple data sets. Particularly notable is the fact that even with a simple generation module, which does not use any extractive feature tuning, our model manages to significantly outperform their ABS+ system on the Gigaword data set and is comparable on the DUC-2004 task. 93 Proceedings of NAACL-HLT 2016, pages 93–98, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics 2 Previous Work vidual conditional probabilities: While there is a large body of work for generating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the s"
N16-1012,K16-1028,0,0.805068,"Missing"
N16-1012,W11-1610,0,0.0643834,"Missing"
N16-1012,W12-3018,0,0.478482,"Missing"
N16-1012,P03-1021,0,0.0330929,"on the held-out set and use it to compute the F1-score of ROUGE-1, ROUGE-2, and ROUGE-L on the test sets, all of which we report. For the DUC corpus however, inline with the standard, we report the recall-only ROUGE. As baseline we use the state-of-the-art attention-based system (ABS) of Rush et al. (2015) which relies on a feed-forward network decoder. Additionally, we compare to an enhanced version of their system (ABS+), which relies on a range of separate extractive summarization features that are added as log-linear features in a secondary learning step with minimum error rate training (Och, 2003). Table 1 shows that both our RAS-Elman and RAS-LSTM models achieve lower perplexity than 96 ROUGE recall, while as we use the more balanced F-measure. RG-1 RG-2 RG-L ABS ABS+ RAS-Elman (k = 1) RAS-Elman (k = 10) RAS-LSTM (k = 1) RAS-LSTM (k = 10) 26.55 28.18 29.13 28.97 26.90 27.41 7.06 8.49 7.62 8.26 6.57 7.69 22.05 23.81 23.92 24.06 22.12 23.06 Luong-NMT 28.55 8.79 24.43 Table 3: ROUGE results (recall-only) on the DUC-2004 test sets. ABS and ABS+ are the systems of Rush et al. 2015. k refers to the size of the beam for generation; k = 1 implies greedy generation. RG refers to ROUGE. ABS as"
N16-1012,D15-1044,1,0.730926,"del. In addition, at every time step the decoder also takes a conditioning input which is the output of an encoder module. Depending on the current state of the RNN, the encoder computes scores over the words in the input sentence. These scores can be interpreted as a soft alignment over the input text, informing the decoder which part of the input sentence it should focus on to generate the next word. Both the decoder and encoder are jointly trained on a data set consisting of sentence-summary pairs. Our model can be seen as an extension of the recently proposed model for the same problem by Rush et al. (2015). While they use a feed-forward neural language model for generation, we use a recurrent neural network. Furthermore, our encoder is more sophisticated, in that it explicitly encodes the position information of the input words. Lastly, our encoder uses a convolutional network to encode input words. These extensions result in improved performance. Abstractive Sentence Summarization generates a shorter version of a given sentence while attempting to preserve its meaning. We introduce a conditional recurrent neural network (RNN) which generates a summary of an input sentence. The conditioning is"
N16-1012,D10-1050,0,0.0107957,"enerating extractive summaries of sentences (Jing, 2000; Knight and Marcu, 2002; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Altun, 2013; Filippova et al., 2015), there has been much less research on abstractive summarization. A count-based noisy-channel machine translation model was proposed for the problem in Banko et al. (2000). The task of abstractive sentence summarization was later formalized around the DUC-2003 and DUC-2004 competitions (Over et al., 2007), where the T OP IARY system (Zajic et al., 2004) was the state-ofthe-art. More recently Cohn and Lapata (2008) and later Woodsend et al. (2010) proposed systems which made heavy use of the syntactic features of the sentence-summary pairs. Later, along the lines of Banko et al. (2000), M OSES was used directly as a method for text simplification by Wubben et al. (2012). Other works which have recently been proposed for the problem of sentence summarization include (Galanis and Androutsopoulos, 2010; Napoles et al., 2011; Cohn and Lapata, 2013). Very recently Rush et al. (2015) proposed a neural attention model for this problem using a new data set for training and showing state-of-the-art performance on the DUC tasks. Our model can be"
N16-1012,P12-1107,0,0.0118831,"Missing"
N16-1012,D15-1042,0,\N,Missing
N16-1025,D11-1031,1,0.961006,"f the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline"
N16-1025,D14-1132,1,0.719403,"11) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing with RNNs. A line of work is devoted to parsing with RNN models, including using RNNs (Miikkulainen, 1996; Mayberry and Miikkulainen, 1999; Legrand and Collobert, 2015; Watanabe and Sumita, 2015) and LSTM (Hochreiter and Schmidhu"
N16-1025,D15-1041,0,0.0238985,"e scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing with RNNs. A line of work is devoted to parsing with RNN models, including using RNNs (Miikkulainen, 1996; Mayberry and Miikkulainen, 1999; Legrand and Collobert, 2015; Watanabe and Sumita, 2015) and LSTM (Hochreiter and Schmidhuber, 1997) RNNs (Vinyals et al., 2015; Ballesteros et al., 2015; Dyer et al., 2015; Kiperwasser and Goldberg, 2016). Legrand and Collobert (2015) used RNNs to learn conditional distributions over syntactic rules; Vinyals et al. (2015) explored sequenceto-sequence learning (Sutskever et al., 2014) for parsing; Ballesteros et al. (2015) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based obje"
N16-1025,J99-2004,0,0.0559903,"op element and replaces it with a unary subtree rooted in x. The Transition System The transition system we use in this work is based on the CCG transition system of Zhang and Clark (2011). We denote parse items as (j, δ, β, ∆)3 , where δ is the stack (with top element δ|s0 ), β is the queue (with top element xwj |β), j is the positional index of the word at the front of the queue, and ∆ is the set of CCG dependencies realized for the input consumed so far (needed to calculate the expected F-score). We also assume a set of lexical categories has been assigned to each word using a supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004). The transition system is specified using three action types: • S HIFT (sh) removes one of the lexical categories xwj of the front word wj in the queue, and pushes it onto the stack; and removes wj from the queue. 3 We partly adopt standard notations from dependency parsing (Nivre, 2008). 214 The deduction system (Fig. 2) of our shift-reduce parser follows from the transition system.4 Each parse item is associated with a step indicator ω, which denotes the number of actions used to build it. Given a sentence of length n, a full derivation requires 2n − 1 + µ steps to"
N16-1025,P05-1022,0,0.0475868,", directed dependencies, denoted as ∆, associated with its parse item. (We assume F1 over labeled, directed dependencies is also the parser evaluation metric.) 3. We compute the negative expected F1 objective (-xF1, defined below) for xn using the scores obtained in the above step and minimize this objective using SGD (maximizing the expected F1 for xn ). These three steps repeat for other sentences in the training data, updating θ after processing each sentence, and training iterates in epochs until convergence. We note that the above process is different from parse reranking (Collins, 2000; Charniak and Johnson, 2005), in which Λ(xn ) would stay the same for each xn in the training data across all epochs, and a reranker is trained on all fixed Λ(xn ); whereas the xF1 training procedure is on-line learning with parameters updated after processing each sentence and each Λ(xn ) is generated with a new θ. More formally, we define the loss J(θ), which incorporates all action scores in each action sequence, and all action sequences in Λ(xn ), for each xn as J(θ) = −xF1(θ) X =− p(yi |θ)F1(∆yi , ∆G xn ), (1) yi ∈Λ(xn ) where F1(∆yi , ∆G xn ) is the sentence level F1 of the parse derived by yi , with respect to the"
N16-1025,D14-1082,0,0.461101,"ased. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In t"
N16-1025,C04-1041,1,0.796621,"Missing"
N16-1025,J07-4004,1,0.53161,"lication. Given CCGBank (Hockenmaier and Steedman, 2007), there are two approaches to extract a grammar from this data. The first is to treat all CCG derivations as phrase-structure trees, and a binary, context-free “cover” grammar, consisting of all CCG rule instances in the treebank, is extracted from local trees in all the derivations (Fowler and Penn, 2010; Zhang and Clark, 2011). In contrast, one can extract the lexicon from the treebank and define only the rule schemas, without explicitly enumerating any rule instances (Hockenmaier, 2003). This is the approach taken in the C & C parser (Clark and Curran, 2007) and the one we use here. Moreover, following Zhang and Clark (2011), our CCG parsing model is also a normal-form model, which models action sequences of normal-form derivations in CCGBank. 3.2 ω : (j, δ, xwj |β, ∆) ω + 1 : (j + 1, δ|xwj , β, ∆) (sh; 0 ≤ j &lt; n) ω : (j, δ|s1 |s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆ ∪ hxi)) (re; s1 s0 → x) ω : (j, δ|s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆) (un; s0 → x) Figure 2: The shift-reduce deduction system. • R EDUCE (re) combines the top two subtrees s0 and s1 on the stack using a CCG rule (s1 s0 → x) and replaces them with a subtree rooted in x. It also appends the se"
N16-1025,P04-1015,0,0.13841,"15) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy"
N16-1025,W02-1001,0,0.0353133,"eros et al. (2015) utilized characterlevel representations and Kiperwasser and Goldberg (2016) built an easy-first dependency parser using tree-structured compositional LSTMs. However, all these parsers use greedy search and are trained using the maximum likelihood criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we woul"
N16-1025,P14-1129,0,0.00697777,"Missing"
N16-1025,P15-1033,0,0.179087,"combining distributed representations and neural network models (Chen and Manning, 2014), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models. In practice, the most common objective for optimizing neural network shift-reduce parsing models is maximum likelihood. In the greedy search setting, the log-likelihood of each target action is maximized during training, and the most likely action is committed to at each step of the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin ("
N16-1025,P10-1035,0,0.13279,"cat NP /N N /N N /N N /N N NP &gt;B N axiom: 0 : (0, , β, φ) goal: 2n − 1 + µ : (n, δ, , ∆) &gt; &gt; Figure 1: An example CCG derivation. forward rules; for example, N /N N /N → N /N is an instance of forward composition and N /N N → N is an instance of forward application. Given CCGBank (Hockenmaier and Steedman, 2007), there are two approaches to extract a grammar from this data. The first is to treat all CCG derivations as phrase-structure trees, and a binary, context-free “cover” grammar, consisting of all CCG rule instances in the treebank, is extracted from local trees in all the derivations (Fowler and Penn, 2010; Zhang and Clark, 2011). In contrast, one can extract the lexicon from the treebank and define only the rule schemas, without explicitly enumerating any rule instances (Hockenmaier, 2003). This is the approach taken in the C & C parser (Clark and Curran, 2007) and the one we use here. Moreover, following Zhang and Clark (2011), our CCG parsing model is also a normal-form model, which models action sequences of normal-form derivations in CCGBank. 3.2 ω : (j, δ, xwj |β, ∆) ω + 1 : (j + 1, δ|xwj , β, ∆) (sh; 0 ≤ j &lt; n) ω : (j, δ|s1 |s0 , β, ∆) ω + 1 : (j, δ|x, β, ∆ ∪ hxi)) (re; s1 s0 → x) ω : (j"
N16-1025,N13-1048,0,0.0235347,"computation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from http://svn.ask.it.usyd.edu.au/trac/ candc/wiki/Download. 10 formance improvements. Auli and Lopez (2011) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are optimized using backpropagation and SGD. Parsing"
N16-1025,P14-1066,0,0.0261525,"Missing"
N16-1025,N10-1112,0,0.439849,". In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the po"
N16-1025,P13-2111,0,0.0405929,"Missing"
N16-1025,P96-1024,0,0.569337,"4) for reference only, since it uses a more sophisticated dependency, rather than normal-form derivation, model. At test time, we also used the precomputation trick of Devlin et al. (2014) to speed up the RNN models by caching the top 20K word embeddings and all POS embeddings,9 and this made the greedy RNN parser more than 3 times faster than the C & C parser (all speed experiments were measured on a workstation with an Intel Core i7 4.0GHz CPU).10 6 Related Work Optimizing for Task-specific Metrics. Our training objective is largely inspired by task-specific optimization for parsing and MT. Goodman (1996) proposed algorithms for optimizing a parser for various constituent matching criteria, and it was one of the earliest work that we are aware of on optimizing a parser for evaluation metrics. Smith and Eisner (2006) proposed a framework for minimizing expected loss for log-linear models and applied it to dependency parsing by optimizing for labeled attachment scores, although they obtained little per9 8 The C & C parser fails to produce spanning analyses for a very small number of sentences (Clark and Curran, 2007) on both dev and test sets, which is not the case for any of the shiftreduce par"
N16-1025,P12-1031,0,0.0256443,"Missing"
N16-1025,J07-3004,0,0.292569,"icit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away fr"
N16-1025,Q16-1032,0,0.0266013,"Missing"
N16-1025,Q14-1032,0,0.0779368,"nd the latter gives schemas which dictate whether two categories can be combined. Given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules. More generally, both lexical and non-lexical CCG categories can be either atomic or complex: atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward (/) and backward slashes () as two binary operators. As such, all categories can be represented as follows (Vijay-Shanker and Weir, 1993; Kuhlmann and Satta, 2014): x := α|1 z1 |2 z2 . . . |m zm , where m ≥ 0, α is an atomic category, |1 , . . . , |m ∈ {, /} and zi are meta-variables for categories. CCG rules have the following two schematic forms, each a generalized version of functional composition (Vijay-Shanker and Weir, 1993): x/y y|1 z1 . . . |m zm → x|1 z1 . . . |m zm , y|1 z1 . . . |m zm xy → x|1 z1 . . . |m zm . The first schematic form above instantiates into a forward application rule (&gt;) for m = 0, and forward composition rules (&gt;B ) for m &gt; 0. Similarly, the second schematic form, which is symmetric to the first, instantiates into backwar"
N16-1025,C04-1010,0,0.15768,"Missing"
N16-1025,J08-4003,0,0.00498973,"e positional index of the word at the front of the queue, and ∆ is the set of CCG dependencies realized for the input consumed so far (needed to calculate the expected F-score). We also assume a set of lexical categories has been assigned to each word using a supertagger (Bangalore and Joshi, 1999; Clark and Curran, 2004). The transition system is specified using three action types: • S HIFT (sh) removes one of the lexical categories xwj of the front word wj in the queue, and pushes it onto the stack; and removes wj from the queue. 3 We partly adopt standard notations from dependency parsing (Nivre, 2008). 214 The deduction system (Fig. 2) of our shift-reduce parser follows from the transition system.4 Each parse item is associated with a step indicator ω, which denotes the number of actions used to build it. Given a sentence of length n, a full derivation requires 2n − 1 + µ steps to terminate, where µ is the total number of un actions applied. In Zhang and Clark (2011), a finish action is used to indicate termination, which we do not use in our parser: an item finishes when no further action can be taken. Another difference between the transition systems is that Zhang and Clark (2011) omit t"
N16-1025,P03-1021,0,0.125757,"e results. 217 We used b = 8 to do the precomputation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from http://svn.ask.it.usyd.edu.au/trac/ candc/wiki/Download. 10 formance improvements. Auli and Lopez (2011) optimized the C & C parser for F-measure. However, they used the softmax-margin (Gimpel and Smith, 2010) objective, which required decomposing precision and recall statistics over parse forests. Instead, we directly optimize for an F-measure loss. In MT, task-specific optimization has also received much attention (e.g., see Och (2003)). Closely related to our work, Gao and He (2013) proposed training a Markov random field translation model as an additional component in a log-linear phrase-based translation system using a k-best list based expected BLEU objective; using the same objective, Auli et al. (2014) and Auli and Gao (2014) trained a large scale phrase-based reordering model and a RNN language model respectively, all as additional components within a log-linear translation model. In contrast, our RNN parsing model is trained in an end-toend fashion with an expected F-measure loss and all parameters of the model are"
N16-1025,W10-1748,0,0.0680122,"Missing"
N16-1025,P06-2089,0,0.0663224,"Missing"
N16-1025,P06-2101,0,0.191119,"NN models by caching the top 20K word embeddings and all POS embeddings,9 and this made the greedy RNN parser more than 3 times faster than the C & C parser (all speed experiments were measured on a workstation with an Intel Core i7 4.0GHz CPU).10 6 Related Work Optimizing for Task-specific Metrics. Our training objective is largely inspired by task-specific optimization for parsing and MT. Goodman (1996) proposed algorithms for optimizing a parser for various constituent matching criteria, and it was one of the earliest work that we are aware of on optimizing a parser for evaluation metrics. Smith and Eisner (2006) proposed a framework for minimizing expected loss for log-linear models and applied it to dependency parsing by optimizing for labeled attachment scores, although they obtained little per9 8 The C & C parser fails to produce spanning analyses for a very small number of sentences (Clark and Curran, 2007) on both dev and test sets, which is not the case for any of the shiftreduce parsers; and for brevity, we omit C & C coverage results. 217 We used b = 8 to do the precomputation. The speed results for the C & C parser were obtained using the per-compiled C & C binary for Linux available from ht"
N16-1025,P10-1040,0,0.0103662,"agger by all RNN parsing models for both training and testing. F-score over directed, labeled CCG predicate-argument dependencies was used as the parser evaluation metric, obtained using the script from C & C. Hyperparameters. For the BRNN supertagging model, we used identical hyperparameter settings as in Xu et al. (2015). For all RNN parsing models, the weights were uniformly initialized using the interval [−2.0, 2.0], and scaled by their fanin (Bengio, 2012); the hidden layer size was 220, and 50-dimensional embeddings were used for all feature types and scaled Turian embeddings were used (Turian et al., 2010) for word embeddings. We also pretrained CCG lexcial category and POS embeddings by using the GENSIM word2vec implementation.7 The data used for this was obtained by parsing a Wikipedia dump using the C & C parser and concatenating the output with CCGBank Sections 02-21. Embeddings for unknown words and CCG categories outside of the lexical category set were uniformly initialized ([−2.0, 2.0]) without scaling. 6 Training: Sections 02-21; development: Section 00; test Section 23. 7 https://radimrehurek.com/gensim/ Supertagger C & C (gold POS ) C & C (auto POS ) RNN BRNN Dev 92.60 91.50 93.07 93"
N16-1025,Q16-1014,0,0.0402546,"d criterion (except Kiperwasser and Goldberg (2016), who used a margin-based objective). For learning global models, Watanabe and Sumita (2015) used a marginbased objective, which was not optimized for the evaluation metric; although not using RNNs, Weiss et al. (2015) proposed a method using the averaged perceptron with beam search (Collins, 2002; Collins and Roark, 2004; Zhang and Clark, 2008), which required fixing the neural network representations, and 218 thus their model parameters were not learned using end-to-end backpropagation. Finally, a number of recent work (Bengio et al., 2015; Vaswani and Sagae, 2016) explored training neural network models for parsing and other tasks such that the network learns from the oracle as well as its own predictions, and are hence more robust to search errors during inference. In principle, these techniques are largely orthogonal to both global learning and task-based optimization, and we would expect further accuracy gains are possible by combining these techniques in a single model. 7 Conclusion Neural network shift-reduce parsers are often trained by maximizing likelihood, which does not optimize towards the final evaluation metric. In this paper, we addressed"
N16-1025,J93-4002,0,0.105107,"epresenting syntactic types, and the latter gives schemas which dictate whether two categories can be combined. Given the lexicon and the rules, the syntactic types of complete constituents can be obtained by recursive combination of categories using the rules. More generally, both lexical and non-lexical CCG categories can be either atomic or complex: atomic categories are categories without any slashes, and complex categories are constructed recursively from atomic ones using forward (/) and backward slashes () as two binary operators. As such, all categories can be represented as follows (Vijay-Shanker and Weir, 1993; Kuhlmann and Satta, 2014): x := α|1 z1 |2 z2 . . . |m zm , where m ≥ 0, α is an atomic category, |1 , . . . , |m ∈ {, /} and zi are meta-variables for categories. CCG rules have the following two schematic forms, each a generalized version of functional composition (Vijay-Shanker and Weir, 1993): x/y y|1 z1 . . . |m zm → x|1 z1 . . . |m zm , y|1 z1 . . . |m zm xy → x|1 z1 . . . |m zm . The first schematic form above instantiates into a forward application rule (&gt;) for m = 0, and forward composition rules (&gt;B ) for m &gt; 0. Similarly, the second schematic form, which is symmetric to the first"
N16-1025,P15-1113,0,0.210249,"k-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics"
N16-1025,P15-1032,0,0.262661,"pected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an expected F-measure objective, derivable from only a set of shift-reduce action sequences and sentence-level F-scores. More generally, our method can be seen as an alternative approach for training a neural beam search parsing model (Watanabe and Sumita, 2015; Weiss et al., 2015; Zhou et al., 2015), combining the benefits of global learning and taskspecific optimization. We also introduce a simple recurrent neural network (RNN) model to shift-reduce parsing on which the greedy baseline and the global model is based. Compared with feed-forward networks, RNNs have the potential to capture and use an unbounded history, and they have been used to learn explicit representations for parser states as well as actions 210 Proceedings of NAACL-HLT 2016, pages 210–220, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics performed on the sta"
N16-1025,P14-1021,1,0.744248,"lowing Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are maintained on a stack, and a queue stores remaining words from the input string; the initial parse item has a"
N16-1025,P15-2041,1,0.871297,"s performed on the stack and queue in shift-reduce parsers (Dyer et al., 2015; Watanabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are"
N16-1025,D08-1059,1,0.740405,"easure loss, which gives further significant accuracy improvements (§5). 2.4 Expected F1 Training The RNN we use to train the global model has the same Elman architecture as the greedy model. Given the greedy model, we summarize its weights as θ = {U, V, W} and initialize the weights of the global model to θ, and training proceeds as follows: 1. We use a beam-search decoder to parse a sentence xn in the training data and let the decoder generate a k-best list2 of output parses using the current θ, denoted as Λ(xn ). Similar to other structured training approaches that use inexact beam search (Zhang and Clark, 2008; Weiss et al., 2015; Watanabe and Sumita, 2015; Zhou et al., 2015), Λ(xn ) is as an approximation to the set of all possible parses of an input sentence. 2. Let yi be the shift-reduce action sequence of a parse in the k-best list Λ(xn ), and let |yi |be its total number of actions and yij be the j th action in yi , for 1 ≤ j ≤ |yi |. We compute the log-linear action sequence score of yi , ρ(yi ), as a sum of individual action scores in that 2 We do not put a limit on k, and whenever an item is finished, it is appended to the k-best list. We found the size of the k-best lists were on average t"
N16-1025,P11-1069,1,0.364226,"nabe and Sumita, 2015), following Miikkulainen (1996) and Mayberry and Miikkulainen (1999). In comparison, our model is a natural extension of the feed-forward architecture in Chen and Manning (2014) using Elman RNNs (Elman, 1990). We apply our models to CCG, and evaluate the resulting parsers on standard CCGBank data (Hockenmaier and Steedman, 2007). More specifically, by combining the global RNN parsing model with a bidirectional RNN CCG supertagger that we have developed (§4) — building on the supertagger of Xu et al. (2015), we obtain accuracies higher than the shift-reduce CCG parsers of Zhang and Clark (2011) and Xu et al. (2014). Finally, although we choose to focus on shift-reduce parsing for CCG, we expect the methods to generalize to other shift-reduce parsers. 2 RNN Models In this section, we start by describing the baseline model, which is also taken as the pretrained model to train the global model (§2.4). We abstract away from the details of CCG and present the models in a canonical shift-reduce parsing framework (Aho and Ullman, 1972), which is henceforth assumed: partially constructed derivations are maintained on a stack, and a queue stores remaining words from the input string; the ini"
N16-1025,P15-1117,0,0.232056,"l network models (Chen and Manning, 2014), accurate and efficient shift-reduce parsing models can be obtained with little feature engineering, largely alleviating the feature sparsity problem of linear models. In practice, the most common objective for optimizing neural network shift-reduce parsing models is maximum likelihood. In the greedy search setting, the log-likelihood of each target action is maximized during training, and the most likely action is committed to at each step of the parsing process during inference (Chen and Manning, 2014; Dyer et al., 2015). In the beam search setting, Zhou et al. (2015) show that sentence-level likelihood, together with contrastive learning (Hinton, 2002), can be used to derive a global model which incorporates beam In this paper, we present a global neural network parsing model, optimized for a task-specific loss based on expected F-measure. The model naturally incorporates beam search during training, and is globally optimized, to learn shift-reduce action sequences that lead to parses with high expected Fscores. In contrast to Auli and Lopez (2011), who optimize a CCG parser for F-measure via softmaxmargin (Gimpel and Smith, 2010), we directly optimize an"
N16-1025,P13-1043,0,0.0191998,"Missing"
N16-1025,P14-2023,1,\N,Missing
N18-1025,D15-1042,0,0.034701,"vement was to simplify the insertion, deletion, and copy of text compared to typewriters. Computers then enabled the emergence of computerized language assistance tools such as spelling correctors (Brill and Moore, 2000) or next word suggestions (Bickel et al., 2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005; Mallinson et al., 2017), compressing sentences (Rush et al., 2015) or simplifying sentences (Nisioi et al., 2017). This type of work expands the possibilities for interactive text generation tools, like our work. Related to our work, Filippova et al. (2015) considers the task of predicting which tokens can be removed from a sentence without modifying its meaning relying on a recurrent neural network. Our work pursues a different goal since our model does not predict which token to remove, as the user provides this information. Our generation is more involved as our model rephrases the sentences, which includes introducing new words, reordering text, inflecting nouns and verbs, etc. Guu et al. (2017) considers generating text with latent edits. Their goal is not to enable users to control which words need to be changed in an initial sentence but"
N18-1025,P05-1074,0,0.0991389,"ring et al. (2017) since this model offers a good trade-off between high accuracy and fast decoding. 2.2 2.3 Computer-Assisted Text Editing Computer assisted text editing has been introduced with interactive computer terminals (Irons and Djorup, 1972). Its first achievement was to simplify the insertion, deletion, and copy of text compared to typewriters. Computers then enabled the emergence of computerized language assistance tools such as spelling correctors (Brill and Moore, 2000) or next word suggestions (Bickel et al., 2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005; Mallinson et al., 2017), compressing sentences (Rush et al., 2015) or simplifying sentences (Nisioi et al., 2017). This type of work expands the possibilities for interactive text generation tools, like our work. Related to our work, Filippova et al. (2015) considers the task of predicting which tokens can be removed from a sentence without modifying its meaning relying on a recurrent neural network. Our work pursues a different goal since our model does not predict which token to remove, as the user provides this information. Our generation is more involved as our model rephrases the senten"
N18-1025,J09-1002,0,0.0743767,"ng Post-editing leverages a machine translation system and enable human translators to edit its output with different levels of computer assistance. This enables improving machine translation outputs with lesser effort than purely manual translation. Green et al. (2014) implement such a system relying on a phrase-based translation system. The system presents an initial translation to the user who can accept a prefix and select among the most likely postfix iteratively. Similar ideas relying on decoding with prefix constrains are common in post-translation (Langlais et al., 2000; Koehn, 2009b; Barrachina et al., 2009). Recently, these approaches based on left-to-right decoding have been extended to neural machine translation (Peris et al., 2017). Closer to our work, Marie and Max (2015) propose light-weight interactions based on accepting/rejecting spans from the output of a statistical machine translation system. The user labels each span that should appear in the final translation. Unmarked spans are assumed to be undesirable and the system removes any entries that could generate those spans from the phrase table. The phrase table is modified such that only positively marked target spans are allowed to e"
N18-1025,D14-1130,0,0.0621708,"text with latent edits. Their goal is not to enable users to control which words need to be changed in an initial sentence but to enable sampling valid English sentences with high lexical overlap around a starting sentence. Contrary to paraphrasing, such samples might introduce negations and other changes impacting meaning. Translation Post-Editing Post-editing leverages a machine translation system and enable human translators to edit its output with different levels of computer assistance. This enables improving machine translation outputs with lesser effort than purely manual translation. Green et al. (2014) implement such a system relying on a phrase-based translation system. The system presents an initial translation to the user who can accept a prefix and select among the most likely postfix iteratively. Similar ideas relying on decoding with prefix constrains are common in post-translation (Langlais et al., 2000; Koehn, 2009b; Barrachina et al., 2009). Recently, these approaches based on left-to-right decoding have been extended to neural machine translation (Peris et al., 2017). Closer to our work, Marie and Max (2015) propose light-weight interactions based on accepting/rejecting spans from"
N18-1025,H05-1025,0,0.192359,"nce-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation postediting through simulated post-edits. We also evaluate our model for paraphrasing through a user study. 1 Introduction Computers can help humans edit text more efficiently. In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes (Brill and Moore, 2000) or suggest likely completions of a sentence (Bickel et al., 2005). In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided. Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing (Quirk et al., 2004), or a translation, i.e. translation postediting (Koehn, 2009b). Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it. Our proposal relies on very simple interactions: a human editor modifies a sent"
N18-1025,P09-5002,0,0.0267618,"raphrasing through a user study. 1 Introduction Computers can help humans edit text more efficiently. In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes (Brill and Moore, 2000) or suggest likely completions of a sentence (Bickel et al., 2005). In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided. Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing (Quirk et al., 2004), or a translation, i.e. translation postediting (Koehn, 2009b). Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it. Our proposal relies on very simple interactions: a human editor modifies a sentence by selecting tokens they would like the system to replace and no other feedback. Our system then generates a new sentence which reformulates the initial sen2 Related Work Our work builds upon previous research on neural machine translation, machine translation postediting, and computer-assisted editing. 2."
N18-1025,P09-4005,0,0.151513,"raphrasing through a user study. 1 Introduction Computers can help humans edit text more efficiently. In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes (Brill and Moore, 2000) or suggest likely completions of a sentence (Bickel et al., 2005). In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided. Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing (Quirk et al., 2004), or a translation, i.e. translation postediting (Koehn, 2009b). Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it. Our proposal relies on very simple interactions: a human editor modifies a sentence by selecting tokens they would like the system to replace and no other feedback. Our system then generates a new sentence which reformulates the initial sen2 Related Work Our work builds upon previous research on neural machine translation, machine translation postediting, and computer-assisted editing. 2."
N18-1025,P00-1037,0,0.814352,"ence by avoiding marked words. The approach builds upon neural sequence-to-sequence modeling and introduces a neural network which takes as input a sentence along with change markers. Our model is trained on translation bitext by simulating post-edits. We demonstrate the advantage of our approach for translation postediting through simulated post-edits. We also evaluate our model for paraphrasing through a user study. 1 Introduction Computers can help humans edit text more efficiently. In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes (Brill and Moore, 2000) or suggest likely completions of a sentence (Bickel et al., 2005). In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided. Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing (Quirk et al., 2004), or a translation, i.e. translation postediting (Koehn, 2009b). Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it. Our proposal"
N18-1025,N09-2055,0,0.188124,"Missing"
N18-1025,2014.iwslt-evaluation.1,0,0.206117,"Missing"
N18-1025,W00-0507,0,0.15993,"cting meaning. Translation Post-Editing Post-editing leverages a machine translation system and enable human translators to edit its output with different levels of computer assistance. This enables improving machine translation outputs with lesser effort than purely manual translation. Green et al. (2014) implement such a system relying on a phrase-based translation system. The system presents an initial translation to the user who can accept a prefix and select among the most likely postfix iteratively. Similar ideas relying on decoding with prefix constrains are common in post-translation (Langlais et al., 2000; Koehn, 2009b; Barrachina et al., 2009). Recently, these approaches based on left-to-right decoding have been extended to neural machine translation (Peris et al., 2017). Closer to our work, Marie and Max (2015) propose light-weight interactions based on accepting/rejecting spans from the output of a statistical machine translation system. The user labels each span that should appear in the final translation. Unmarked spans are assumed to be undesirable and the system removes any entries that could generate those spans from the phrase table. The phrase table is modified such that only positiv"
N18-1025,P15-2026,0,0.0418245,"Missing"
N18-1025,P17-2031,0,0.0292538,"Missing"
N18-1025,D15-1166,0,0.0850883,"Missing"
N18-1025,E17-1083,0,0.36081,"l offers a good trade-off between high accuracy and fast decoding. 2.2 2.3 Computer-Assisted Text Editing Computer assisted text editing has been introduced with interactive computer terminals (Irons and Djorup, 1972). Its first achievement was to simplify the insertion, deletion, and copy of text compared to typewriters. Computers then enabled the emergence of computerized language assistance tools such as spelling correctors (Brill and Moore, 2000) or next word suggestions (Bickel et al., 2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005; Mallinson et al., 2017), compressing sentences (Rush et al., 2015) or simplifying sentences (Nisioi et al., 2017). This type of work expands the possibilities for interactive text generation tools, like our work. Related to our work, Filippova et al. (2015) considers the task of predicting which tokens can be removed from a sentence without modifying its meaning relying on a recurrent neural network. Our work pursues a different goal since our model does not predict which token to remove, as the user provides this information. Our generation is more involved as our model rephrases the sentences, which includes intro"
N18-1025,D15-1120,0,0.139047,"ine translation outputs with lesser effort than purely manual translation. Green et al. (2014) implement such a system relying on a phrase-based translation system. The system presents an initial translation to the user who can accept a prefix and select among the most likely postfix iteratively. Similar ideas relying on decoding with prefix constrains are common in post-translation (Langlais et al., 2000; Koehn, 2009b; Barrachina et al., 2009). Recently, these approaches based on left-to-right decoding have been extended to neural machine translation (Peris et al., 2017). Closer to our work, Marie and Max (2015) propose light-weight interactions based on accepting/rejecting spans from the output of a statistical machine translation system. The user labels each span that should appear in the final translation. Unmarked spans are assumed to be undesirable and the system removes any entries that could generate those spans from the phrase table. The phrase table is modified such that only positively marked target spans are allowed to explain the cor273 Figure 1: QuickEdit architecture for translation post-editing. The decoder attends to both encodings, one for the source and one for the initial translati"
N18-1025,P17-2014,0,0.0258786,"Text Editing Computer assisted text editing has been introduced with interactive computer terminals (Irons and Djorup, 1972). Its first achievement was to simplify the insertion, deletion, and copy of text compared to typewriters. Computers then enabled the emergence of computerized language assistance tools such as spelling correctors (Brill and Moore, 2000) or next word suggestions (Bickel et al., 2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005; Mallinson et al., 2017), compressing sentences (Rush et al., 2015) or simplifying sentences (Nisioi et al., 2017). This type of work expands the possibilities for interactive text generation tools, like our work. Related to our work, Filippova et al. (2015) considers the task of predicting which tokens can be removed from a sentence without modifying its meaning relying on a recurrent neural network. Our work pursues a different goal since our model does not predict which token to remove, as the user provides this information. Our generation is more involved as our model rephrases the sentences, which includes introducing new words, reordering text, inflecting nouns and verbs, etc. Guu et al. (2017) cons"
N18-1025,W04-3219,0,0.0715755,"iting through simulated post-edits. We also evaluate our model for paraphrasing through a user study. 1 Introduction Computers can help humans edit text more efficiently. In particular, statistical models are used for that purpose, for instance to help correct spelling mistakes (Brill and Moore, 2000) or suggest likely completions of a sentence (Bickel et al., 2005). In this work, we rely on statistical learning to enable a computer to rephrase a sentence by only pointing at words that should be avoided. Specifically, we consider the task of reformulating either a sentence, i.e. paraphrasing (Quirk et al., 2004), or a translation, i.e. translation postediting (Koehn, 2009b). Paraphrasing reformulates a sentence with different words preserving its meaning, while translation post-editing takes a candidate translation along with the corresponding source sentence and improves it. Our proposal relies on very simple interactions: a human editor modifies a sentence by selecting tokens they would like the system to replace and no other feedback. Our system then generates a new sentence which reformulates the initial sen2 Related Work Our work builds upon previous research on neural machine translation, machi"
N18-1025,D15-1044,0,0.049803,"and fast decoding. 2.2 2.3 Computer-Assisted Text Editing Computer assisted text editing has been introduced with interactive computer terminals (Irons and Djorup, 1972). Its first achievement was to simplify the insertion, deletion, and copy of text compared to typewriters. Computers then enabled the emergence of computerized language assistance tools such as spelling correctors (Brill and Moore, 2000) or next word suggestions (Bickel et al., 2005). More recently, research has focused on generating paraphrases (Bannard and Callison-Burch, 2005; Mallinson et al., 2017), compressing sentences (Rush et al., 2015) or simplifying sentences (Nisioi et al., 2017). This type of work expands the possibilities for interactive text generation tools, like our work. Related to our work, Filippova et al. (2015) considers the task of predicting which tokens can be removed from a sentence without modifying its meaning relying on a recurrent neural network. Our work pursues a different goal since our model does not predict which token to remove, as the user provides this information. Our generation is more involved as our model rephrases the sentences, which includes introducing new words, reordering text, inflecti"
N18-1025,P16-1162,0,0.0446112,"airs held-out from the original training corpus. We test on the concatenation of tst2010, tst2011, tst2012, tst2013, dev2010 and dev2012 comprising 6,750 sentence pairs. The vocabulary for this dataset is 24k for English and 36k for German. For WMT’14 English to German and German to English, we use the same setup as Luong et al. (2015) which comprises 4.5M sentence pairs for training and we test on newstest2014.2 We took 45k sentences out of the training set for validation purpose. As vocabulary, we learn a joint source and target byte-pair encoding (BPE) with 44k types from the training set (Sennrich et al., 2016b,a). Note that even when using BPE, we solely rely on full word markers, i.e. all the BPE tokens of a given word carry the same binary indication (to be changed/no preference). For WMT’14 English to French and French to English (Bojar et al., 2014), we also rely on BPE with 44k types. This dataset is larger with 35.4M sentences for training and 26k sentences for validation. We rely on newstest2014 for testing3 . The model architecture settings are borrowed from (Gehring et al., 2017). For IWSLT’14 deen and IWSLT’14 en-de, we rely on 4-layer encoders and 3-layer decoders, both with 256 hidden"
N18-1033,N10-1112,0,0.177526,"Prediction Losses for Sequence to Sequence Learning Sergey Edunov∗, Myle Ott∗, Michael Auli, David Grangier, Marc’Aurelio Ranzato Facebook AI Research Menlo Park, CA and New York, NY Abstract sequence level. For example, direct loss optimization has been popularized in machine translation with the Minimum Error Rate Training algorithm (MERT; Och 2003) and expected risk minimization has an extensive history in NLP (Smith and Eisner, 2006; Rosti et al., 2010; Green et al., 2014). This paper revisits several objective functions that have been commonly used for structured prediction tasks in NLP (Gimpel and Smith, 2010) and apply them to a neural sequence to sequence model (Gehring et al., 2017b) (§2). Specifically, we consider likelihood training at the sequencelevel, a margin loss as well as expected risk training. We also investigate several combinations of global losses with token-level likelihood. This is, to our knowledge, the most comprehensive comparison of structured losses in the context of neural sequence to sequence models (§3). We experiment on the IWSLT’14 GermanEnglish translation task (Cettolo et al., 2014) as well as the Gigaword abstractive summarization task (Rush et al., 2015). We achieve"
N18-1033,W14-3360,0,0.0333072,"Missing"
N18-1033,D17-1222,0,0.109845,"If we train Risk only on the news-commentary portion of the training data, then we achieve state of the art accuracy on this dataset of 41.5 BLEU (Xia et al., 2017). Table 6: Accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for token-level label smoothing, and Risk optimization of all three ROUGE F1 metrics. [T] indicates a token-level objective and [S] indicates a sequence level objectives. ABS+ refers to Rush et al. (2015), RNN MLE/MRT (Ayana et al., 2016), WFE (Suzuki and Nagata, 2017), SEASS (Zhou et al., 2017), DRGD (Li et al., 2017). with Adam (Kingma and Ba, 2014) for another 10 epochs with learning rate 0.00003 and 16 candidate sequences per training example. We conduct experiments with Risk since it performed best in trial experiments. Different from other sequence-level experiments (§5), we rescale the BLEU scores in each candidate set by the difference between the maximum and minimum scores of each sentence. This avoids short sentences dominating the sequence updates, since candidate sets for short sentences have a wider range of BLEU scores compared to longer sentences; a similar rescaling was used by Bahdanau et a"
N18-1033,W04-1013,0,0.0527679,"ce, U(x), generated by the model. We discuss approaches for generating this subset in §4. u∗ (x) = arg max BLEU(t, u) u∈U (x) As is common practice when computing BLEU at the sentence-level, we smooth all initial counts to one (except for unigram counts) so that the geometric mean is not dominated by zero-valued ngram match counts (Lin and Och, 2004). Expected Risk Minimization (Risk) This objective minimizes the expected value of a given cost function over the space of candidate sequences (Risk, Equation 4). In this work we use task-specific cost functions designed to maximize BLEU or ROUGE (Lin, 2004), e.g., cost(t, u) = Sequence Negative Log Likelihood (SeqNLL) 2 Another option is to use the gold reference target, t, but in practice this can lead to degenerate solutions in which the model assigns low probabilities to nearly all outputs. This is discussed further in §4. Similar to TokNLL, we can minimize the negative log likelihood of an entire sequence rather than individual tokens (SeqNLL, Equation 3). The log357 1 − BLEU(t, u), for a given a candidate sequence u and target t. Different to SeqNLL (§3.2), this loss may increase the score of several candidates that have low cost, instead o"
N18-1033,C04-1072,0,0.119622,"such as BLEU or ROUGE. Unfortunately, these objectives are also typically defined over the entire space of possible output sequences, which is intractable to enumerate or score with our models. Instead, we compute our sequence losses over a subset of the output space, U(x), generated by the model. We discuss approaches for generating this subset in §4. u∗ (x) = arg max BLEU(t, u) u∈U (x) As is common practice when computing BLEU at the sentence-level, we smooth all initial counts to one (except for unigram counts) so that the geometric mean is not dominated by zero-valued ngram match counts (Lin and Och, 2004). Expected Risk Minimization (Risk) This objective minimizes the expected value of a given cost function over the space of candidate sequences (Risk, Equation 4). In this work we use task-specific cost functions designed to maximize BLEU or ROUGE (Lin, 2004), e.g., cost(t, u) = Sequence Negative Log Likelihood (SeqNLL) 2 Another option is to use the gold reference target, t, but in practice this can lead to degenerate solutions in which the model assigns low probabilities to nearly all outputs. This is discussed further in §4. Similar to TokNLL, we can minimize the negative log likelihood of a"
N18-1033,D15-1166,0,0.0656388,"mpling for various candidate set sizes during sequence-level training in terms of validation accuracy. Token-level label smoothing (TokLS) is the baseline. Table 4: Generating candidates online or offline. 6.4 16 Candidate set size 6.6 Comparison to Beam-Search Optimization Next, we compare classical sequence-level training to the recently proposed Beam Search Optimization (Wiseman and Rush, 2016). To enable a fair comparison, we re-implement their baseline, a single layer LSTM encoder/decoder model with 256-dimensional hidden layers and word embeddings as well as attention and input feeding (Luong et al., 2015). This baseline is trained with Adagrad (Duchi et al., 2011) using a learning rate of 0.05 for five epochs, with batches of 64 sequences. For sequence-level training we initialize weights with the baseline parameters and train Beam Search vs. Sampling and Candidate Set Size So far we generated candidates with beam search, however, we may also sample to obtain a more diverse set of candidates (Shen et al., 2016). Fig361 RG-1 RG-2 RG-L ABS+ [T] RNN MLE [T] RNN MRT [S] WFE [T] SEASS [T] DRGD [T] 29.78 32.67 36.54 36.30 36.15 36.27 11.89 15.23 16.59 17.31 17.54 17.57 26.97 30.56 33.44 33.88 33.63"
N18-1033,D10-1059,0,0.0238452,"aseline reward for REINFORCE with a separate linear regressor over the model’s current hidden state. 358 score with our models. We therefore use a subset of K candidate sequences U(x) = {u1 , . . . , uK }, which we generate with our models. We consider two search strategies for generating the set of candidate sequences. The first is beam search, a greedy breadth-first search that maintains a “beam” of the top-K scoring candidates at each generation step. Beam search is the de facto decoding strategy for achieving state-ofthe-art results in machine translation. The second strategy is sampling (Chatterjee and Cancedda, 2010), which produces K independent output sequences by sampling from the model’s conditional distribution. Whereas beam search focuses on high probability candidates, sampling introduces more diverse candidates (see comparison in §6.5). We also consider both online and offline candidate generation settings in §6.4. In the online setting, we regenerate the candidate set every time we encounter an input sentence x during training. In the offline setting, candidates are generated before training and are never regenerated. Offline generation is also embarrassingly parallel because all samples use the"
N18-1033,P03-1021,0,0.363949,"Missing"
N18-1033,D16-1137,0,0.112592,"as expected risk training. We also investigate several combinations of global losses with token-level likelihood. This is, to our knowledge, the most comprehensive comparison of structured losses in the context of neural sequence to sequence models (§3). We experiment on the IWSLT’14 GermanEnglish translation task (Cettolo et al., 2014) as well as the Gigaword abstractive summarization task (Rush et al., 2015). We achieve the best reported accuracy to date on both tasks. We find that the sequence level losses we survey perform similarly to one another and outperform beam search optimization (Wiseman and Rush, 2016) on a comparable setup. On WMT’14 English-French, we also illustrate the effectiveness of risk minimization on a larger translation task. Classical losses for structured prediction are still very competitive and effective for neural models (§5, §6). There has been much recent work on training neural attention models at the sequencelevel using either reinforcement learning-style methods or by optimizing the beam. In this paper, we survey a range of classical objective functions that have been widely used to train linear models for structured prediction and apply them to neural sequence to seque"
N18-1033,D15-1044,0,0.694112,"n NLP (Gimpel and Smith, 2010) and apply them to a neural sequence to sequence model (Gehring et al., 2017b) (§2). Specifically, we consider likelihood training at the sequencelevel, a margin loss as well as expected risk training. We also investigate several combinations of global losses with token-level likelihood. This is, to our knowledge, the most comprehensive comparison of structured losses in the context of neural sequence to sequence models (§3). We experiment on the IWSLT’14 GermanEnglish translation task (Cettolo et al., 2014) as well as the Gigaword abstractive summarization task (Rush et al., 2015). We achieve the best reported accuracy to date on both tasks. We find that the sequence level losses we survey perform similarly to one another and outperform beam search optimization (Wiseman and Rush, 2016) on a comparable setup. On WMT’14 English-French, we also illustrate the effectiveness of risk minimization on a larger translation task. Classical losses for structured prediction are still very competitive and effective for neural models (§5, §6). There has been much recent work on training neural attention models at the sequencelevel using either reinforcement learning-style methods or"
N18-1033,P16-1162,0,0.146639,"= U(x) ∪ {t}, we find this can destabilize training since the model learns to assign low probabilities nearly everywhere, ruining the candidates generated by the model, while still assigning a slightly higher score to the reference (cf. Shen et al. (2016)). Accordingly we do not add the reference translation to our candidate sets. 5 5.1 domly sampled and held-out from the train data. We test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010 which is of similar size to the validation set. All data is lowercased and tokenized with a byte-pair encoding (BPE) of 14,000 types (Sennrich et al., 2016) and we evaluate with case-insensitive BLEU. We also experiment on the much larger WMT’14 English-French task. We remove sentences longer than 175 words as well as pairs with a source/target length ratio exceeding 1.5 resulting in 35.5M sentence-pairs for training. The source and target vocabulary is based on 40K BPE types. Results are reported on both newstest2014 and a validation set held-out from the training data comprising 26,658 sentence pairs. We modify the fairseq-py toolkit to implement the objectives described in §3.6 Our translation models have four convolutional encoder layers and"
N18-1033,P17-1101,0,0.107093,"gain of +0.2 BLEU by Risk. If we train Risk only on the news-commentary portion of the training data, then we achieve state of the art accuracy on this dataset of 41.5 BLEU (Xia et al., 2017). Table 6: Accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for token-level label smoothing, and Risk optimization of all three ROUGE F1 metrics. [T] indicates a token-level objective and [S] indicates a sequence level objectives. ABS+ refers to Rush et al. (2015), RNN MLE/MRT (Ayana et al., 2016), WFE (Suzuki and Nagata, 2017), SEASS (Zhou et al., 2017), DRGD (Li et al., 2017). with Adam (Kingma and Ba, 2014) for another 10 epochs with learning rate 0.00003 and 16 candidate sequences per training example. We conduct experiments with Risk since it performed best in trial experiments. Different from other sequence-level experiments (§5), we rescale the BLEU scores in each candidate set by the difference between the maximum and minimum scores of each sentence. This avoids short sentences dominating the sequence updates, since candidate sets for short sentences have a wider range of BLEU scores compared to longer sentences; a similar rescaling w"
N18-1033,P16-1159,0,0.3967,"gly parallel because all samples use the same model. The disadvantage is that the candidates become stale. Our model may perfectly be able to discriminate between them after only a single update, hindering the ability of the loss to correct eventual search errors.4 Finally, while some past work has added the reference target to the candidate set, i.e., U 0 (x) = U(x) ∪ {t}, we find this can destabilize training since the model learns to assign low probabilities nearly everywhere, ruining the candidates generated by the model, while still assigning a slightly higher score to the reference (cf. Shen et al. (2016)). Accordingly we do not add the reference translation to our candidate sets. 5 5.1 domly sampled and held-out from the train data. We test on the concatenation of tst2010, tst2011, tst2012, tst2013 and dev2010 which is of similar size to the validation set. All data is lowercased and tokenized with a byte-pair encoding (BPE) of 14,000 types (Sennrich et al., 2016) and we evaluate with case-insensitive BLEU. We also experiment on the much larger WMT’14 English-French task. We remove sentences longer than 175 words as well as pairs with a source/target length ratio exceeding 1.5 resulting in 35"
N18-1033,P06-2101,0,0.376133,"Missing"
N18-1033,E17-2047,0,0.0464229,"fatt) which results in a smaller gain of +0.2 BLEU by Risk. If we train Risk only on the news-commentary portion of the training data, then we achieve state of the art accuracy on this dataset of 41.5 BLEU (Xia et al., 2017). Table 6: Accuracy on Gigaword abstractive summarization in terms of F-measure Rouge-1 (RG-1), Rouge-2 (RG-2), and Rouge-L (RG-L) for token-level label smoothing, and Risk optimization of all three ROUGE F1 metrics. [T] indicates a token-level objective and [S] indicates a sequence level objectives. ABS+ refers to Rush et al. (2015), RNN MLE/MRT (Ayana et al., 2016), WFE (Suzuki and Nagata, 2017), SEASS (Zhou et al., 2017), DRGD (Li et al., 2017). with Adam (Kingma and Ba, 2014) for another 10 epochs with learning rate 0.00003 and 16 candidate sequences per training example. We conduct experiments with Risk since it performed best in trial experiments. Different from other sequence-level experiments (§5), we rescale the BLEU scores in each candidate set by the difference between the maximum and minimum scores of each sentence. This avoids short sentences dominating the sequence updates, since candidate sets for short sentences have a wider range of BLEU scores compared to longer sente"
N19-1409,P07-2045,0,0.0124085,"Missing"
N19-1409,W04-1013,0,0.0641703,"ocument summarization task comprising over 280K news articles paired with multi-sentence summaries. CNN-DailyMail is a widely used dataset for abstractive text summarization. Following (See et al., 2017), we report results on the non-anonymized version of CNNDailyMail rather than the entity-anonymized version (Hermann et al., 2015; Nallapati et al., 2016) because the language model was trained on full text. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types (Fan et al., 2017). We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).3 3.2 Pre-training. We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT’18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary. Machine translation. We consider two benchmarks: Most experiments are run on the WMT’18 English-German (en-de) news translation task and we validate our fi"
N19-1409,K16-1028,0,0.0366239,"arget vocabulary we learn a BPE code with 32K merge operations on the Turkish side of the bitext. Both datasets are evaluated in terms of case-sensitive de-tokenized BLEU (Papineni et al., 2002; Post, 2018).2 Summarization. We consider the CNNDailyMail abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries. CNN-DailyMail is a widely used dataset for abstractive text summarization. Following (See et al., 2017), we report results on the non-anonymized version of CNNDailyMail rather than the entity-anonymized version (Hermann et al., 2015; Nallapati et al., 2016) because the language model was trained on full text. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types (Fan et al., 2017). We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).3 3.2 Pre-training. We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT’18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of"
N19-1409,N19-4009,1,0.852947,"evski and Auli (2018). The learning rate is linearly warmed up from 10−7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 (Loshchilov and Hutter, 2016). We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16bit floating point operations (Ott et al., 2018) and it took six days for the bi-directional model and four days for the uni-directional model. 3.3 Sequence to sequence model We use the transformer implementation of the fairseq toolkit (Ott et al., 2019). The WMT en-de and en-tr experiments are based on the Big Transformer sequence to sequence architecture with 6 blocks in the encoder and decoder. For abstractive summarization we use a base transformer model (Vaswani et al., 2017). We tune dropout values of between 0.1 and 0.4 on the validation set. Models are optimized with Adam (Kingma and Ba, 2015) using β1 = 0.9, β2 = 0.98, and  = 1e − 8 and we use the same learning rate schedule as Vaswani et al. (2017); we perform 10K-200K depending on bitext size. All models use label smoothing with a uniform prior distribution over the vocabulary  ="
N19-1409,W18-6301,1,0.867417,"el contains 353M parameters and the unidirectional model 190M parameters. Both models were trained for 1M steps using Nesterov’s accelerated gradient (Sutskever et al., 2013) with momentum 0.99 following Baevski and Auli (2018). The learning rate is linearly warmed up from 10−7 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 (Loshchilov and Hutter, 2016). We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16bit floating point operations (Ott et al., 2018) and it took six days for the bi-directional model and four days for the uni-directional model. 3.3 Sequence to sequence model We use the transformer implementation of the fairseq toolkit (Ott et al., 2019). The WMT en-de and en-tr experiments are based on the Big Transformer sequence to sequence architecture with 6 blocks in the encoder and decoder. For abstractive summarization we use a base transformer model (Vaswani et al., 2017). We tune dropout values of between 0.1 and 0.4 on the validation set. Models are optimized with Adam (Kingma and Ba, 2015) using β1 = 0.9, β2 = 0.98, and  = 1e −"
N19-1409,P02-1040,0,0.103664,"the Moses tokenizer (Koehn et al., 2007) and apply the BPE vocabulary learned on the monolingual corpora. For WMT’18 English-Turkish, we use all of the available bitext comprising 208K sentence-pairs without any filtering. We develop on newstest2017 and test on newstest2018. For en-tr we only experiment with adding representations to the encoder and therefore apply the language model vocabulary to the source side. For the target vocabulary we learn a BPE code with 32K merge operations on the Turkish side of the bitext. Both datasets are evaluated in terms of case-sensitive de-tokenized BLEU (Papineni et al., 2002; Post, 2018).2 Summarization. We consider the CNNDailyMail abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries. CNN-DailyMail is a widely used dataset for abstractive text summarization. Following (See et al., 2017), we report results on the non-anonymized version of CNNDailyMail rather than the entity-anonymized version (Hermann et al., 2015; Nallapati et al., 2016) because the language model was trained on full text. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types (Fan et al., 2017)."
N19-1409,N18-1202,0,0.225906,"Missing"
N19-1409,W18-6319,0,0.0255812,"oehn et al., 2007) and apply the BPE vocabulary learned on the monolingual corpora. For WMT’18 English-Turkish, we use all of the available bitext comprising 208K sentence-pairs without any filtering. We develop on newstest2017 and test on newstest2018. For en-tr we only experiment with adding representations to the encoder and therefore apply the language model vocabulary to the source side. For the target vocabulary we learn a BPE code with 32K merge operations on the Turkish side of the bitext. Both datasets are evaluated in terms of case-sensitive de-tokenized BLEU (Papineni et al., 2002; Post, 2018).2 Summarization. We consider the CNNDailyMail abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries. CNN-DailyMail is a widely used dataset for abstractive text summarization. Following (See et al., 2017), we report results on the non-anonymized version of CNNDailyMail rather than the entity-anonymized version (Hermann et al., 2015; Nallapati et al., 2016) because the language model was trained on full text. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types (Fan et al., 2017). We evaluate"
N19-1409,E17-2025,0,0.0680031,"Missing"
N19-1409,P16-1162,0,0.0398722,"et al., 2015; Nallapati et al., 2016) because the language model was trained on full text. Articles are truncated to 400 tokens (See et al., 2017) and we use a BPE vocabulary of 32K types (Fan et al., 2017). We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L (Lin, 2004).3 3.2 Pre-training. We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT’18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary. Machine translation. We consider two benchmarks: Most experiments are run on the WMT’18 English-German (en-de) news translation task and we validate our findings on the WMT’18 EnglishTurkish (en-tr) news task. For WMT’18 EnglishGerman, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we Language model pre-training We consider two types of architectures: a bidirectional language model to augment the sequence to sequence encoder and a uni-directi"
N19-1409,D16-1163,0,0.0303423,"nts for a range of language understanding tasks (Peters et al., 2018; Radford et al., 2018; Phang et al., 2018; Devlin et al., 2018). The key idea is to train a large generative model on vast corpora and use the resulting representations on tasks for which only limited amounts of labeled data is available. Pre-training of sequence to sequence models has been previously investigated for text classification (Dai and Le, 2015) but not for text generation. In neural machine translation, there has been work on transferring representations from high-resource language pairs to low-resource settings (Zoph et al., 2016). In this paper, we apply pre-trained representations from language models to language genera∗ Equal contribution. Code and pre-trained models are available at https://github.com/pytorch/fairseq/tree/ bi_trans_lm/examples/pretraining 1 2 Strategies to add representations We consider augmenting a standard sequence to sequence model with pre-trained representations following an ELMo-style regime (§2.1) as well as by fine-tuning the language model (§2.2). 2.1 ELMo augmentation The ELMo approach of Peters et al. (2018) forms contextualized word embeddings based on language model representations wi"
N19-4009,W18-1820,0,0.0207348,"sily extensible sequence modeling toolkit. There are several toolkits with similar basic functionality, but they differ in focus area and intended audiences. For example, OpenNMT (Klein et al., 2017) is a community-built toolkit written in multiple languages with an emphasis on extensibility. MarianNMT (Junczys-Dowmunt et al., 2018) focuses on performance and the backend is written in C++ for fast automatic differentiation. OpenSeq2Seq (Kuchaiev et al., 2018) provides reference implementations for fast distributed and mixed precision training. Tensor2tensor (Vaswani et al., 2018) and Sockeye (Hieber et al., 2018) focus on production-readiness. In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production. FAIRSEQ features: (i) a common interface across models and tasks that can be extended ∗ † Sam Gross4 2 Design Extensibility. FAIRSEQ can be extended through five types of user-supplied plug-ins, which enable experimenting with new ideas while reusing existing components as much as possible. Models define the neural network architecture and encapsulate all learnable parameters. Models extend the BaseFairseqModel"
N19-4009,D18-1035,0,0.0210075,"on FAIRSEQ is implemented in PyTorch and it provides efficient batching, mixed precision training, multi-GPU as well as multi-machine training. Batching. There are multiple strategies to batch input and output sequence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a b"
N19-4009,P18-4020,0,0.0459403,"Missing"
N19-4009,D18-1045,1,0.918465,"is isolates model implementation from the generation algorithm. Criterions compute the loss given the model and a batch of data, roughly: loss = criterion(model, batch). This formulation makes criterions very expressive, since they have complete access to the model. For example, a criterion may perform on-the-fly generaequal contribution Work done while at Facebook AI Research. 48 Proceedings of NAACL-HLT 2019: Demonstrations, pages 48–53 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Sync after backward tion to support sequence-level training (Edunov et al., 2018b) or online backtranslation (Edunov et al., 2018a; Lample et al., 2018). Alternatively, in a mixture-of-experts model, a criterion may implement EM-style training and backpropagate only through the expert that produces the lowest loss (Shen et al., 2019). gpu1 a) gpu4 Overlap sync with backward gpu1 b) gpu4 + sync after 2 backwards gpu1 Tasks store dictionaries, provide helpers for loading and batching data and define the training loop. They are intended to be immutable and primarily interface between the various components. We provide tasks for translation, language modeling, and classificat"
N19-4009,P17-4012,0,0.0466622,"mixed-precision training and inference on modern GPUs. A demo video can be found here: https://www.youtube. com/watch?v=OtgDdWtHvto. 1 Introduction Neural sequence-to-sequence models have been successful on a variety of text generation tasks, including machine translation, abstractive document summarization, and language modeling. Accordingly, both researchers and industry professionals can benefit from a fast and easily extensible sequence modeling toolkit. There are several toolkits with similar basic functionality, but they differ in focus area and intended audiences. For example, OpenNMT (Klein et al., 2017) is a community-built toolkit written in multiple languages with an emphasis on extensibility. MarianNMT (Junczys-Dowmunt et al., 2018) focuses on performance and the backend is written in C++ for fast automatic differentiation. OpenSeq2Seq (Kuchaiev et al., 2018) provides reference implementations for fast distributed and mixed precision training. Tensor2tensor (Vaswani et al., 2018) and Sockeye (Hieber et al., 2018) focus on production-readiness. In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and produc"
N19-4009,N18-1033,1,0.93022,"is isolates model implementation from the generation algorithm. Criterions compute the loss given the model and a batch of data, roughly: loss = criterion(model, batch). This formulation makes criterions very expressive, since they have complete access to the model. For example, a criterion may perform on-the-fly generaequal contribution Work done while at Facebook AI Research. 48 Proceedings of NAACL-HLT 2019: Demonstrations, pages 48–53 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Sync after backward tion to support sequence-level training (Edunov et al., 2018b) or online backtranslation (Edunov et al., 2018a; Lample et al., 2018). Alternatively, in a mixture-of-experts model, a criterion may implement EM-style training and backpropagate only through the expert that produces the lowest loss (Shen et al., 2019). gpu1 a) gpu4 Overlap sync with backward gpu1 b) gpu4 + sync after 2 backwards gpu1 Tasks store dictionaries, provide helpers for loading and batching data and define the training loop. They are intended to be immutable and primarily interface between the various components. We provide tasks for translation, language modeling, and classificat"
N19-4009,W18-2507,0,0.0260205,"translation, abstractive document summarization, and language modeling. Accordingly, both researchers and industry professionals can benefit from a fast and easily extensible sequence modeling toolkit. There are several toolkits with similar basic functionality, but they differ in focus area and intended audiences. For example, OpenNMT (Klein et al., 2017) is a community-built toolkit written in multiple languages with an emphasis on extensibility. MarianNMT (Junczys-Dowmunt et al., 2018) focuses on performance and the backend is written in C++ for fast automatic differentiation. OpenSeq2Seq (Kuchaiev et al., 2018) provides reference implementations for fast distributed and mixed precision training. Tensor2tensor (Vaswani et al., 2018) and Sockeye (Hieber et al., 2018) focus on production-readiness. In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production. FAIRSEQ features: (i) a common interface across models and tasks that can be extended ∗ † Sam Gross4 2 Design Extensibility. FAIRSEQ can be extended through five types of user-supplied plug-ins, which enable experimenting with new ideas while reusing existin"
N19-4009,W18-2706,1,0.907574,"tegies to batch input and output sequence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a big Transformer model. effective batch size but we found that models can still be trained effectively (Ott et al., 2018b). 4.1 Mixed precision. Recent GPUs enable efficient half"
N19-4009,P18-1082,1,0.938124,"tegies to batch input and output sequence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a big Transformer model. effective batch size but we found that models can still be trained effectively (Ott et al., 2018b). 4.1 Mixed precision. Recent GPUs enable efficient half"
N19-4009,P19-1254,1,0.877236,"Missing"
N19-4009,W04-1013,0,0.0422983,"2017) Gehrmann et al. (2018) 39.5 41.2 17.3 18.7 36.4 38.3 FAIRSEQ 40.1 41.6 17.6 18.9 36.8 38.5 + pre-trained LM Perplexity Grave et al. (2016) Dauphin et al. (2017) Merity et al. (2018) Rae et al. (2018) FAIRSEQ Adaptive inputs Table 5: Abstractive summarization results on the fulltext version of CNN-DailyMail dataset. 40.8 37.2 33.0 29.2 the full-text version with no entity anonymization (See et al., 2017); we truncate articles to 400 tokens (See et al., 2017). We use BPE with 30K operations to form our vocabulary following Fan et al. (2018a). To evaluate, we use the standard ROUGE metric (Lin, 2004) and report ROUGE -1, ROUGE -2, and ROUGE - L . To generate summaries, we follow standard practice in tuning the minimum output length and disallow repeating the same trigram (Paulus et al., 2017). Table 5 shows results of FAIRSEQ. We also consider a configuration where we input pre-trained language model representations to the encoder network and this language model was trained on newscrawl and CNN-Dailymail, totalling 193M sentences. 18.7 Table 3: Test perplexity on WikiText-103 (cf. Table 4). et al., 2016), adaptive softmax (Grave et al., 2017), and adaptive inputs (Baevski and Auli, 2019)."
N19-4009,D18-1443,0,0.0626867,"Missing"
N19-4009,D18-1444,0,0.0225715,"ut and output sequence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a big Transformer model. effective batch size but we found that models can still be trained effectively (Ott et al., 2018b). 4.1 Mixed precision. Recent GPUs enable efficient half precision floating"
N19-4009,D15-1166,0,0.143016,"Missing"
N19-4009,D17-2014,0,0.0316737,"hroughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a big Transformer model. effective batch size but we found that models can still be trained effectively (Ott et al., 2018b). 4.1 Mixed precision. Recent GPUs enable efficient half precision floating point (FP16) computation. FAIRSEQ provides support for both full precision (FP32) and FP16 at training and inference. We perform all forward-backward computations as well as the all-reduce for gradient synchr"
N19-4009,C18-1259,0,0.0248643,"Missing"
N19-4009,W17-3208,0,0.0240619,"nts contain the full state of the model, optimizer and dataloader, so that results are reproducible if training is interrupted and resumed. FAIRSEQ also provides forward compatibility, i.e., models trained using old versions of the toolkit will continue to run on the latest version through automatic checkpoint upgrading. 3 Gradient sync. Forward Backward Idle Implementation FAIRSEQ is implemented in PyTorch and it provides efficient batching, mixed precision training, multi-GPU as well as multi-machine training. Batching. There are multiple strategies to batch input and output sequence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story genera"
N19-4009,W18-1819,0,0.041356,"nals can benefit from a fast and easily extensible sequence modeling toolkit. There are several toolkits with similar basic functionality, but they differ in focus area and intended audiences. For example, OpenNMT (Klein et al., 2017) is a community-built toolkit written in multiple languages with an emphasis on extensibility. MarianNMT (Junczys-Dowmunt et al., 2018) focuses on performance and the backend is written in C++ for fast automatic differentiation. OpenSeq2Seq (Kuchaiev et al., 2018) provides reference implementations for fast distributed and mixed precision training. Tensor2tensor (Vaswani et al., 2018) and Sockeye (Hieber et al., 2018) focus on production-readiness. In this paper, we present FAIRSEQ, a sequence modeling toolkit written in PyTorch that is fast, extensible, and useful for both research and production. FAIRSEQ features: (i) a common interface across models and tasks that can be extended ∗ † Sam Gross4 2 Design Extensibility. FAIRSEQ can be extended through five types of user-supplied plug-ins, which enable experimenting with new ideas while reusing existing components as much as possible. Models define the neural network architecture and encapsulate all learnable parameters. M"
N19-4009,D18-1206,0,0.0181039,"ence pairs (Morishita et al., 2017). FAIRSEQ minimizes padding within a minibatch by grouping source and target sequences of similar length. The content of each mini-batch stays the same throughout training, however minibatches themselves are shuffled randomly every epoch. When training on more than one GPU or machine, then the mini-batches for each worker 49 Edunov et al., 2018b,a; Chen et al., 2018; Ott et al., 2018a; Song et al., 2018; Wu et al., 2019), language modeling (Dauphin et al., 2017; Baevski and Auli, 2019), abstractive document summarization (Fan et al., 2018a; Liu et al., 2018; Narayan et al., 2018), story generation (Fan et al., 2018b, 2019), error correction (Chollampatt and Ng, 2018), multilingual sentence embeddings (Artetxe and Schwenk, 2018), and dialogue (Miller et al., 2017; Dinan et al., 2019). Sentences/sec FP32 FAIRSEQ FP16 FAIRSEQ 88.1 136.0 Table 1: Translation speed measured on a V100 GPU on the test set of the standard WMT’14 EnglishGerman benchmark using a big Transformer model. effective batch size but we found that models can still be trained effectively (Ott et al., 2018b). 4.1 Mixed precision. Recent GPUs enable efficient half precision floating point (FP16) computati"
N19-4009,W18-6301,1,0.942718,"ch GPU has a copy of the model to process a sub-batch of data after which gradients are synchronized between GPUs; all sub-batches constitute a minibatch. Even though sub-batches contain a similar number of tokens, we still observe a high variance in processing times. In multi-GPU or multimachine setups, this results in idle time for most GPUs while slower workers are finishing their work (Figure 1 (a)). FAIRSEQ mitigates the effect of stragglers by overlapping gradient synchronization between workers with the backward pass and by accumulating gradients over multiple minibatches for each GPU (Ott et al., 2018b). Overlapping gradient synchronization starts to synchronize gradients of parts of the network when they are computed. In particular, when the gradient computation for a layer finishes, FAIRSEQ adds the result to a buffer. When the size of the buffer reaches a predefined threshold, the gradients are synchronized in a background thread while back-propagation continues as usual (Figure 1 (b)). Next, we accumulate gradients for multiple sub-batches on each GPU which reduces the variance in processing time between workers since there is no need to wait for stragglers after each sub-batch (Figure"
N19-4009,W18-6319,0,0.243209,"Missing"
N19-4009,P17-1099,0,0.162809,"Missing"
N19-4009,P16-1162,0,0.893413,"Missing"
N19-4009,N18-2074,0,0.0261463,"nguage modeling supports language modeling with gated convolutional models (Dauphin et al., 2017) and Transformer models (Vaswani et al., 2017). Models can be trained using a variety of input and output representations, such as standard token embeddings, convolutional character embeddings (Kim FAIRSEQ Applications FAIRSEQ has been used in many applications, such as machine translation (Gehring et al., 2017; 1 SacreBLEU hash: BLEU+case.mixed+lang.en-{de,fr}+ numrefs.1+smooth.exp+test.wmt14/full+tok.13a+version.1.2.9 50 a. Gehring et al. (2017) b. Vaswani et al. (2017) c. Ahmed et al. (2017) d. Shaw et al. (2018) Transformer base FAIRSEQ Transformer big detok. SacreBLEU 8 GPU training time 128 GPU training time FAIRSEQ En–De En–Fr 25.2 28.4 28.9 29.2 40.5 41.0 41.4 41.5 28.1 29.3 28.6 ∼12 h ∼1.3 h 41.1 43.2 41.4 ∼73 h ∼7.2 h Perplexity Dauphin et al. (2017) J´ozefowicz et al. (2016) Shazeer et al. (2017) FAIRSEQ 31.9 30.0 28.0 Adaptive inputs 23.0 Table 4: Test perplexity on the One Billion Word benchmark. Adaptive inputs share parameters with an adaptive softmax. ROUGE 1 2 L Table 2: BLEU on news2014 for WMT EnglishGerman (En–De) and English-French (En–Fr). All results are based on WMT’14 training da"
P11-1048,C04-1041,0,0.147319,"> As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004a). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical cat"
P11-1048,P04-1014,0,0.0132581,"> As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004a). It is based on a step function over supertagger beam widths, relaxing the pruning threshold for lexical cat"
P11-1048,J07-4004,0,0.684144,"ective is an upper bound that we want to make as tight as possible by solving for minu L(u). We optimize the values of the u(i, t) variables using the same algorithm as Rush et al. (2010) for their tagging and parsing problem (essentially a perceptron update).4 An advantages of DD is that, on convergence, it recovers exact solutions to the combined problem. However, if it does not converge or we stop early, an approximation must be returned: following Rush et al. (2010) we used the highest scoring output of the parsing submodel over all iterations. 5 Experiments Parser. We use the C&C parser (Clark and Curran, 2007) and its supertagger (Clark, 2002). Our baseline is the hybrid model of Clark and Curran (2007); our integrated model simply adds the supertagger features to this model. The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space. Training requires repeated calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 3) than in previous work (Clark and Curran, 2007). Models were t"
P11-1048,W02-2203,0,0.580056,"he first derivation below, (SN P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. I like tea I NP (S NP)/NP NP NP S NP S < > like tea (S NP)/NP NP >T S /(S NP) S /NP S >B > As can be inferred from even this small example, a key difficulty in parsing CCG is that the number of categories quickly becomes extremely large, and there are typically many ways to analyze every span of a sentence. Supertagging (Bangalore and Joshi, 1999; Clark, 2002) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Because they do this with high accuracy, they are often exploited to prune the parser’s search space: the parser only considers lexical categories with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004a). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possibl"
P11-1048,J85-1006,0,0.804591,"arch problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and mildly context-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar"
P11-1048,D09-1011,0,0.0713145,"d as the product of all incoming messages. The only difference from Equation 6 is the addition of the outside message. (6) Our parsing model is also a distribution over variables Ti , along with an additional quadratic number of span(i, j) variables. Though difficult to represent pictorially, a distribution over parses is captured by an extension to graphical models called case-factor diagrams (McAllester et al., 2008). We add this complex distribution to our model as a single factor (Figure 3). This is a natural extension to the use of complex factors described by Smith and Eisner (2008) and Dreyer and Eisner (2009). When a factor graph is a tree as in Figure 2, BP converges in a single iteration to the exact marginals. However, when the model contains cycles, as in Figure 3, we can iterate message passing. Under certain assumptions this loopy BP it will converge to approximate marginals that are bounded under an interpretation from statistical physics (Yedidia et al., 2001; Sutton and McCallum, 2010). The TREE factor exchanges inside ni and outside oi messages with the tag and span variables, taking into account beliefs from the sequence model. 474 p(Ti ) = 1 fi (Ti )bi (Ti )ei (Ti )oi (Ti ) Z (8) The a"
P11-1048,N10-1128,0,0.0359153,"Missing"
P11-1048,P08-1115,0,0.0278088,"Missing"
P11-1048,N09-1037,0,0.0473688,"Missing"
P11-1048,W06-1673,0,0.068112,"Missing"
P11-1048,P08-1109,0,0.051969,"bout the effect on speed for our model. We measured the runtime of the algorithms under 477 Training the Integrated Model In the experiments reported so far, the parsing and supertagging models were trained separately, and only combined at test time. Although the outcome of these experiments was successful, we wondered if we could obtain further improvements by training the model parameters together. Since the gradients produced by (loopy) BP are approximate, for these experiments we used a stochastic gradient descent (SGD) trainer (Bottou, 2003). We found that the SGD parameters described by Finkel et al. (2008) worked equally well for our models, and, on the baseline, produced similar results to L-BFGS. Curiously, however, we found that the combined model does not perform as well when Baseline BPk=1 BPk=5 BPk=25 DDk=1 DDk=5 DDk=25 AST sent/sec LF 65.8 87.38 60.8 87.70 46.7 87.70 35.3 87.70 64.6 87.40 41.9 87.65 32.5 87.71 Reverse sent/sec LF 5.9 87.36 5.8 88.35 4.7 88.34 3.5 88.33 5.9 87.38 3.1 88.09 1.9 88.29 Table 6: Parsing time in seconds per sentence (vs. Fmeasure) on section 00. Baseline BP inf BP train LF 86.7 86.8 86.3 AST UF 92.7 92.8 92.5 ST 94.0 94.1 93.8 LF 86.7 87.2 85.6 Reverse UF 92.7"
P11-1048,P10-1035,0,0.436075,"pertagger. Variations on this approach drive the widely-used, broad coverage C&C parser (Clark and Curran, 2004a; Clark and Curran, 2007; Kummerfeld et al., 2010). However, it fails when the supertagger makes errors. We show experimentally that this pipeline significantly lowers the upper bound on parsing accuracy (§3). Introduction Accurate and efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity. Even for practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained over 1500. The The same experiment shows that the supertagger prunes many bad parses. So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical catego"
P11-1048,P02-1043,0,0.0192316,"significantly lowers the upper bound on parsing accuracy (§3). Introduction Accurate and efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics, due to the complexities associated its mild context sensitivity. Even for practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing is much harder than with Penn Treebank-style contextfree grammars, with vast numbers of nonterminal categories leading to increased grammar constants. Where a typical Penn Treebank grammar may have fewer than 100 nonterminals (Hockenmaier and Steedman, 2002), we found that a CCG grammar derived from CCGbank contained over 1500. The The same experiment shows that the supertagger prunes many bad parses. So, while we want to avoid the error propagation inherent to a pipeline, ideally we still want to benefit from the key insight of supertagging: that a sequence model over lexical categories can be quite accurate. Our solution is to combine the features of both the supertagger and the parser into a single, less aggressively pruned model. The challenge with this model is its prohibitive complexity, which we address with approximate methods: dual decom"
P11-1048,J07-3004,0,0.0453391,"and Curran (2007); our integrated model simply adds the supertagger features to this model. The parser relies solely on the supertagger for pruning, using CKY for search over the pruned space. Training requires repeated calculation of feature expectations over packed charts of derivations. For training, we limited the number of items in this chart to 0.3 million, and for testing, 1 million. We also used a more permissive training supertagger beam (Table 3) than in previous work (Clark and Curran, 2007). Models were trained with the parser’s L-BFGS trainer. Evaluation. We evaluated on CCGbank (Hockenmaier and Steedman, 2007), a right-most normalform CCG version of the Penn Treebank. We use sections 02-21 (39603 sentences) for training, 4 The u terms can be interpreted as the messages from factors to variables (Sontag et al., 2010) and the resulting message passing algorithms are similar to the max-product algorithm, a sister algorithm to BP. 475 section 00 (1913 sentences) for development and section 23 (2407 sentences) for testing. We supply gold-standard part-of-speech tags to the parsers. Evaluation is based on labelled and unlabelled predicate argument structure recovery and supertag accuracy. We only evaluat"
P11-1048,P08-1067,0,0.0532508,"Missing"
P11-1048,P08-1102,0,0.0198951,"Missing"
P11-1048,D10-1125,0,0.0442242,"ntext-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar with two special cases of belief propagation: the forward-backward and inside-outside algorithms, used for com"
P11-1048,P10-1036,0,0.040232,"Missing"
P11-1048,D10-1004,0,0.0667418,"Missing"
P11-1048,P08-1023,0,0.0600893,"Missing"
P11-1048,P06-1055,0,0.0220359,"Missing"
P11-1048,D10-1001,0,0.334442,"bservation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both uses in §5. Many researchers in NLP are familiar with two special cases of belief propagation: the forward-backward and inside-outside algorithms, used for computing expectations in sequence models and context"
P11-1048,D08-1016,0,0.735978,"of only a subset of these sequences, then the search problem is equivalent to finding the optimal derivation in the weighted intersection of a regular and mildly context-sensitive language. Even allowing for the observation of Fowler and Penn (2010) that our practical CCG is context-free, this problem still reduces to the construction of Bar-Hillel et al. (1964), making search very expensive. Therefore we need approximations. Fortunately, recent literature has introduced two relevant approximations to the NLP community: loopy belief propagation (Pearl, 1988), applied to dependency parsing by Smith and Eisner (2008); and dual decomposition (Dantzig and Wolfe, 1960; Komodakis et al., 2007; Sontag et al., 2010, inter alia), applied to dependency parsing by Koo et al. (2010) and lexicalized CFG parsing by Rush et al. (2010). We apply both techniques to our integrated supertagging and parsing model. 4.1 Belief propagation (BP) is an algorithm for computing marginals (i.e. expectations) on structured models. These marginals can be used for decoding (parsing) in a minimum-risk framework (Smith and Eisner, 2008); or for training using a variety of algorithms (Sutton and McCallum, 2010). We experiment with both"
P11-1048,J99-2004,0,\N,Missing
P11-1048,D08-1022,0,\N,Missing
P11-1048,W05-0636,0,\N,Missing
P11-1048,P08-1000,0,\N,Missing
P11-1158,J99-2004,0,0.270147,"s (SN P2 )/N P1 , specifying the first argument as an NP to the right and the second as an NP to the left. In parsing, adjacent spans are combined using a small number of binary combinatory rules like forward application or composition on the spanning categories (Steedman, 2000; Fowler and Penn, 2010). In the first derivation below, (SN P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. 1578 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a"
P11-1158,C04-1041,0,0.0656093,"N P )/N P and N P combine to form the spanning category SN P , which only requires an NP to its left to form a complete sentence-spanning S. The second derivation uses type-raising to change the category type of I. 1578 2.1 Adaptive Supertagging Supertagging (Bangalore and Joshi, 1999) treats the assignment of lexical categories (or supertags) as a sequence tagging problem. Once the supertagger has been run, lexical categories that apply to each word in the input sentence are pruned to contain only those with high posterior probability (or other figure of merit) under the supertagging model (Clark and Curran, 2004). The posterior probabilities are then discarded; it is the extensive pruning of lexical categories that leads to substantially faster parsing times. Pruning the categories in advance this way has a specific failure mode: sometimes it is not possible to produce a sentence-spanning derivation from the tag sequences preferred by the supertagger, since it does not enforce grammaticality. A workaround for this problem is the adaptive supertagging (AST) approach of Clark and Curran (2004). It is based on a step function over supertagger beam ratios, relaxing the pruning threshold for lexical catego"
P11-1158,J07-4004,0,0.692551,"er underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model of the parser. The PCFG model simply generates a tree top down and uses very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Su"
P11-1158,W02-2203,0,0.0304568,"mal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4 . 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported beyond the speedups (Clark, 2002) We ran experiments to understand the time/accuracy tradeoff of adaptive supertagging, and to serve as baselines. Adaptive supertagging is parametrized by a beam size β and a dictionary cutoff k that bounds the number of lexical categories considered for each word (Clark and Curran, 2007). Table 3 shows both the standard beam levels (AST) used for the C&C parser and looser beam levels: AST-covA, a simple extension of AST with increased coverage and AST-covB, also increasing coverage but with better performance for the HWDep model. Parsing results for the AST settings (Tables 4 and 5) confirm t"
P11-1158,W07-2206,0,0.0304452,"Missing"
P11-1158,P10-1035,0,0.178199,"ighest probability derivation is pruned, the parser will not find that derivation (§2). Since the supertagger enforces no grammaticality constraints it may even prefer a sequence of lexical categories that cannot be combined into any derivation (Figure 1). Empirically, we show that supertagging improves efficiency by an order of magnitude, but the tradeoff is a significant loss in accuracy (§3). Introduction Efficient parsing of Combinatorial Categorial Grammar (CCG; Steedman, 2000) is a longstanding problem in computational linguistics. Even with practical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2"
P11-1158,P02-1043,0,0.0845529,"ontrol loop. Instead, they are processed from a priority queue, which orders them by the product of their inside probability and a heuristic estimate of their outside probability. Provided that the heuristic never underestimates the true outside probability (i.e. it is admissible) the solution is guaranteed to be exact. Heuristics are model specific and we consider several variants in our experiments based on the CFG heuristics developed by Klein and Manning (2003) and Pauls and Klein (2009a). 3 Adaptive Supertagging Experiments Parser. For our experiments we used the generative CCG parser of Hockenmaier and Steedman (2002). Generative parsers have the property that all edge weights are non-negative, which is required for A* techniques.1 Although not quite as accurate as the discriminative parser of Clark and Curran (2007) in our preliminary experiments, this parser is still quite competitive. It is written in Java and implements the CKY algorithm with a global pruning threshold of 10−4 for the models we use. We focus on two parsing models: PCFG, the baseline of Hockenmaier and Steedman (2002) which treats the grammar as a PCFG (Table 1); and HWDep, a headword dependency model which is the best performing model"
P11-1158,J07-3004,0,0.0274272,"s very simple structural probabilities while the HWDep model conditions node expansions on headwords and their lexical categories. Supertagger. For supertagging we used Dennis Mehay’s implementation, which follows Clark 1 Indeed, all of the past work on A* parsing that we are aware of uses generative parsers (Pauls and Klein, 2009b, inter alia). 1579 (2002).2 Due to differences in smoothing of the supertagging and parsing models, we occasionally drop supertags returned by the supertagger because they do not appear in the parsing model 3 . Evaluation. All experiments were conducted on CCGBank (Hockenmaier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank. Models were trained on sections 2-21, tuned on section 00, and tested on section 23. Parsing accuracy is measured using labelled and unlabelled predicate argument structure recovery (Clark and Hockenmaier, 2002); we evaluate on all sentences and thus penalise lower coverage. All timing experiments reported in the paper were run on a 2.5 GHz Xeon machine with 32 GB memory and are averaged over ten runs4 . 3.1 Results Supertagging has been shown to improve the speed of a generative parser, although little analysis has been reported bey"
P11-1158,W01-1812,0,0.108966,"Missing"
P11-1158,N03-1016,0,0.70744,"ractical CCG that are strongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their"
P11-1158,P10-1036,0,0.127158,"Missing"
P11-1158,N09-1063,0,0.318789,"ongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their complexity, a key diffi"
P11-1158,P09-1108,0,0.610846,"ongly context-free (Fowler and Penn, 2010), parsing can be much harder than with Penn Treebank-style context-free grammars, since the number of nonterminal categories is generally much larger, leading to increased grammar constants. Where a typical Penn Treebank grammar Can we improve on this tradeoff? The line of investigation we pursue in this paper is to consider more efficient exact algorithms. In particular, we test different variants of the classical A* algorithm (Hart et al., 1968), which has met with success in Penn Treebank parsing with context-free grammars (Klein and Manning, 2003; Pauls and Klein, 2009a; Pauls and Klein, 2009b). We can substitute A* for standard CKY on either the unpruned set of lexical categories, or the pruned set resulting from su1577 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1577–1585, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Attainable parses Valid supertag-sequences I like tea I like tea NP (S NP)/NP NP NP (S NP)/NP NP Desirable parses S NP S High scoring supertags &gt; &gt;T S /(S NP) S /NP S &gt;B &gt; Because of the number of lexical categories and their complexity, a key diffi"
P11-1158,N09-1073,0,0.0556641,"Missing"
P11-1158,C10-2168,0,0.107645,"Missing"
P14-2023,P07-2045,0,0.00409186,"th language pairs. We tuned the learning rate µ of our mini-batch SGD trainer as well as the probability scaling parameter γ (3) on a held-out set and found simple settings of µ = 0.1 and γ = 1 to be good choices. To prevent over-fitting, we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated. Experiments Baseline. We use a phrase-based system similar to Moses (Koehn et al., 2007) based on a set of common features including maximum likelihood estimates pM L (e|f ) and pM L (f |e), lexically weighted estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified Kneser-Ney language model trained on the target-side of the parallel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Translation models are estimated on 102M words of parallel data for F"
P14-2023,N12-1005,0,0.350927,"er performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. Howeve"
P14-2023,W12-2703,0,0.0503028,"-specific metric often leads to better performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar t"
P14-2023,D11-1031,1,0.919574,"Missing"
P14-2023,D13-1106,1,0.82945,"odman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. However, for recurrent netw"
P14-2023,P06-1096,0,0.149188,"Missing"
P14-2023,D08-1089,0,0.0559234,"we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated. Experiments Baseline. We use a phrase-based system similar to Moses (Koehn et al., 2007) based on a set of common features including maximum likelihood estimates pM L (e|f ) and pM L (f |e), lexically weighted estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified Kneser-Ney language model trained on the target-side of the parallel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Translation models are estimated on 102M words of parallel data for French-English, and 99M words for German-English; about 6.5M words for each language pair are newswire, the remainder are parliamentary proceedings. We evaluate on six newswire domain test sets from 2008 to 2013 containing between 2034 to 3003 s"
P14-2023,N13-1048,1,0.795058,"Missing"
P14-2023,C12-1121,0,0.0345476,"Missing"
P14-2023,P14-1066,1,0.417006,"r Integration and Expected BLEU Training for Recurrent Neural Network Language Models Michael Auli Microsoft Research Redmond, WA, USA michael.auli@microsoft.com Abstract et al., 2012) or more recently, noise-contrastive estimation (Vaswani et al., 2013). However, it is widely appreciated that directly optimizing for a task-specific metric often leads to better performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more acc"
P14-2023,P03-1021,0,0.521333,"for Computational Linguistics wt time algorithm, which unrolls the network and then computes error gradients over multiple time steps (Rumelhart et al., 1986); we use the expected BLEU loss (§3) to obtain the error with respect to the output activations. After training, the output layer represents scores s(wt+1 |w1 . . . wt , ht ) for the next word given the previous t input words and the current hidden layer configuration ht . yt 0 ht 0 1 0 U V 0 0 ht-1 W 3 We integrate the recurrent neural network language model as an additional feature into the standard log-linear framework of translation (Och, 2003). Formally, our phrase-based model is parameterized by M parameters Λ where each λm ∈ Λ, m = 1 . . . M is the weight of an associated feature hm (f, e). Function h(f, e) maps foreign sentences f and English sentences e to the vector h1 (f, e) . . . (f, e), and the model chooses translations according to the following decision rule: Figure 1: Structure of the recurrent neural network language model. 2 Expected BLEU Training Recurrent Neural Network LMs Our model has a similar structure to the recurrent neural network language model of Mikolov et al. (2010) which is factored into an input layer,"
P14-2023,P96-1024,0,0.102983,"Missing"
P14-2023,W10-1748,0,0.0572215,"Missing"
P14-2023,P12-1031,0,0.0907025,"Expected BLEU Objective Formally, we define our loss function l(θ) as the negative expected BLEU score, denoted as xBLEU(θ) for a given foreign sentence f : l(θ) = − xBLEU(θ) X pΛ,θ (e|f )sBLEU(e, e(i) ) = Next, we apply the quotient rule of differentiation: (2) ∂xBLEU(θ) ∂(G(θ)/Z(θ)) = ∂sθ (wt ) ∂sθ (wt )   1 ∂G(θ) ∂Z(θ) = − xBLEU(θ) Z(θ) ∂sθ (wt ) ∂sθ (wt ) e∈E(f ) δwt = where sBLEU(e, e(i) ) is a smoothed sentencelevel BLEU score with respect to the reference translation e(i) , and E(f ) is the generation set given by an n-best list.2 We use a sentence-level BLEU approximation similar to He and Deng (2012).3 The normalized probability pΛ,θ (e|f ) of a particular translation e given f is defined as: pΛ,θ (e|f ) = P exp{γΛT h(f, e)} e0 ∈E(f ) exp{γΛ T h(f, e0 )} Using the observation that θ is only relevant to the recurrent neural network hM +1 (e) (1) we have ∂γΛT h(f, e) ∂hM +1 (e) γλM +1 = γλM +1 = ∂sθ (wt ) ∂sθ (wt ) sθ (wt ) (3) which together with the chain rule, (3) and (4) allows us to rewrite δwt as follows: where ΛT h(f, e) includes the recurrent neural network hM +1 (e), and γ ∈ [0, inf) is a scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et"
P14-2023,W11-2119,0,0.0284544,"Missing"
P14-2023,E14-1003,1,0.582922,"A jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a crossentropy objective (Mikolov et al., 2010; Schwenk 136 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136–142, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics wt time algorithm, which unrolls t"
P14-2023,D13-1176,0,0.060824,"crosoft Research Redmond, WA, USA jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a crossentropy objective (Mikolov et al., 2010; Schwenk 136 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136–142, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics wt time algorith"
P14-2023,W12-2702,0,0.1388,"current network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a crossentropy trained model by up to 0.6 BLEU in a single reference setup. 1 Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usual"
P14-2023,D08-1065,0,0.0131301,"Deng (2012).3 The normalized probability pΛ,θ (e|f ) of a particular translation e given f is defined as: pΛ,θ (e|f ) = P exp{γΛT h(f, e)} e0 ∈E(f ) exp{γΛ T h(f, e0 )} Using the observation that θ is only relevant to the recurrent neural network hM +1 (e) (1) we have ∂γΛT h(f, e) ∂hM +1 (e) γλM +1 = γλM +1 = ∂sθ (wt ) ∂sθ (wt ) sθ (wt ) (3) which together with the chain rule, (3) and (4) allows us to rewrite δwt as follows: where ΛT h(f, e) includes the recurrent neural network hM +1 (e), and γ ∈ [0, inf) is a scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(θ) using the observation that the loss does not explicitly depend on θ: δwt = X  = pΛ,θ (e|f )U (θ, e)λM +1 e∈E(f ), s.t.wt ∈e |e| XX e t=1 −δwt γ sθ (wt )  where U (θ, e) = sBLEU(e, ei ) − xBLEU(θ). t=1 =  X  ∂ exp{γΛT h(f, e)} 1 U (θ, e) Z(θ) ∂sθ (wt ) e∈E(f ), s.t.wt ∈e ∂l(θ) X X ∂l(θ) ∂sθ (wt ) = ∂θ ∂sθ (wt ) ∂θ e |e| Derivation of the Error Term δwt ∂sθ (wt ) ∂θ 4 Decoder Integration Directly integrating our recurrent neural network language model into first-pass decoding enables us to search a much larger space than"
P14-2023,D13-1140,0,0.314434,"on (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. However, for recurrent networks we have to deal with the unbounded history, which breaks the usual dynamic programming assumptions for efficient search. We show how a sim"
P14-2023,D13-1112,0,0.0152677,"n the n-best list of the development set, augmented by scores corresponding to the neural network models. At test time we rescore n-best lists with the new weights. Neural Network Training. All neural network models are trained on the news portion of the parallel data, corresponding to 136K sentences, which we found to be most useful in initial experiments. As training data we use unique 100-best lists generated by the baseline system. We use the same data both for training the phrase-based system as well as the language model but find that the resulting bias did not hurt end-to-end accuracy (Yu et al., 2013). The vocabulary consists of words that occur in at least two different sentences, which is 31K words for both language pairs. We tuned the learning rate µ of our mini-batch SGD trainer as well as the probability scaling parameter γ (3) on a held-out set and found simple settings of µ = 0.1 and γ = 1 to be good choices. To prevent over-fitting, we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer use"
P14-2023,W11-2135,0,\N,Missing
P15-2041,Q14-1026,0,0.565729,"he size of the output layer being equal to the size of the lexical category set. The parameterization of the network consists of three matrices which are learned during supervised training. Matrix U contains weights between the input and hidden layers, V contains weights between the hidden and output layers, and W contains weights between the previous hidden state and the current hidden state. The following recurrence2 is used to compute the activations of the hidden state at word position t: ht = f (xt U + ht−1 W), ewt = bj Lw ∈ R1×n , where j is the look-up index for wt . In addition, as in Lewis and Steedman (2014), for every word we also include its 2-character suffix and capitalization as features. Two more lookup tables are used for these features. Ls ∈ R|s|×m is the look-up table for suffix embeddings, where |s |is the suffix vocabulary size. Lc ∈ R2×m is the look-up table for the capitalization embeddings. Lc contains only two embeddings, representing whether or not a given word is capitalized. We extract features from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, bk/2c words either side of the target word are included. For a"
P15-2041,D11-1031,1,0.94047,"Missing"
P15-2041,J99-2004,0,0.735192,"pedia and biomedical text. 1 Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Ass"
P15-2041,C04-1041,1,0.83064,"Stephen Clark University of Cambridge Computer Laboratory sc609@cam.ac.uk Introduction Combinatory Categorial Grammar (CCG; Steedman, 2000) is a highly lexicalized formalism; the standard parsing model of Clark and Curran (2007) uses over 400 lexical categories (or supertags), compared to about 50 POS tags for typical CFG parsers. This makes accurate disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computationa"
P15-2041,J07-4004,1,0.644157,"Missing"
P15-2041,D08-1050,1,0.934316,"Missing"
P15-2041,P06-1088,1,0.843558,"te disambiguation of lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tions (§2.2). Our model is highly accurate, and by integrating it with the C & C parser as its adaptive supertagger, we obtain substantial accura"
P15-2041,P07-2009,1,0.651356,"Missing"
P15-2041,J07-3004,0,0.433218,"learning and better generalization of the trained model with dropout. Similar to other forms of droput (Srivastava et al., 2014), we randomly drop units and their connections to other units at training time. Concretely, we apply a binary dropout mask to xt , with a dropout rate of 0.25, and at test time no mask is applied, but the input to the network, xt , at each word position is scaled by 0.75. We experimented during development with different dropout rates, but found the above choice to be optimal in our setting. Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C & C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C & C parser. We use the same 425 supertag set used in both C & C and NN. Hyperparameters and Training. For Lw"
P15-2041,P10-1040,0,0.0323139,"s from a context window surrounding the current word to make a tagging decision. Concretely, with a context window of size k, bk/2c words either side of the target word are included. For a word wt , its continuous feature representation is: fwt = [ewt ; swt ; cwt ], (2) xt = [fwt−bk/2c ; . . . fwt ; . . . ; fwt+bk/2c ], where g is the softmax activation function g(zi ) = z Pe i zj that squeezes raw output activations into a e (5) where xt ∈ R1×k(n+2m) and the right-hand side is the concatenation of all feature representations in a size k context window. We use pre-trained word embeddings from Turian et al. (2010) to initialize lookup table Lw , and we apply a set of word pre-processing techniques at both training and test time to reduce sparsity. All words are first lower-cased, and all numbers are collapsed into a single digit ‘0’. If a lower-cased hyphenated j probability distribution. 2.2 (4) where ewt ∈ R1×n , swt ∈ R1×m and cwt ∈ R1×m are the output vectors from the three different look-up tables, and [ewt ; swt ; cwt ] denotes the concatenation of three vectors and hence fwt ∈ R1×(n+2m) . At word position t, the input layer of the network xt is: (1) where f is a non-linear activation function; h"
P15-2041,W09-3306,0,0.0176849,"other units at training time. Concretely, we apply a binary dropout mask to xt , with a dropout rate of 0.25, and at test time no mask is applied, but the input to the network, xt , at each word position is scaled by 0.75. We experimented during development with different dropout rates, but found the above choice to be optimal in our setting. Experiments Datasets and Baseline. We follow the standard splits of CCGBank (Hockenmaier and Steedman, 2007) for all experiments using sections 2-21 for training, section 00 for development and section 23 as in-domain test set. The Wikipedia corpus from Honnibal et al. (2009) and the Bioinfer corpus (Pyysalo et al., 2007) are used as two outof-domain test sets. We compare supertagging accuracy with the MaxEnt C & C supertagger and the neural network tagger of Lewis and Steedman (2014) (henceforth NN), and we also evaluate parsing accuracy using these three supertaggers as a front-end to the C & C parser. We use the same 425 supertag set used in both C & C and NN. Hyperparameters and Training. For Lw , we use the scaled 50-dimensional Turian embeddings (n = 50 for Lw ) as initialization. We have experimented during development with using 100dimensional embeddings a"
P15-2041,P14-1021,1,0.848389,"Missing"
P15-2041,P11-1069,1,0.927753,"Missing"
P15-2041,P10-1036,1,0.88165,"lexical types much more challenging. However, the assignment of lexical categories can still be solved reasonably well by treating it as a sequence tagging problem, often referred to as supertagging (Bangalore and Joshi, 1999). Clark and Curran (2004) show that high tagging accuracy can be achieved by leaving some ambiguity to the parser to resolve, but with enough of a reduction in the number of tags assigned to each word so that parsing efficiency is greatly increased. In addition to improving parsing efficiency, supertagging also has a large impact on parsing accuracy (Curran et al., 2006; Kummerfeld et al., 2010), since the derivation space of the parser is determined by the supertagger, at both train*All work was completed before the author joined Facebook. 250 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 250–255, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics tions (§2.2). Our model is highly accurate, and by integrating it with the C & C parser as its adaptive supertagger, we obtain substantial accuracy improvements, outperfor"
P15-2073,W05-0909,0,0.0517039,"irable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the"
P15-2073,E06-1032,0,0.0476642,"on, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propos"
P15-2073,2003.mtsummit-papers.9,0,0.031569,"run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Ve"
P15-2073,N12-1017,0,0.0244703,"ri,j )  PP i g ∈ n-grams(hi ) maxj wi,j ·#g (hi ) n 1 e(1−ρ/η) if η &gt; ρ otherwise Discriminative B LEU (2) where ρ and η are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as:  P P i g ∈ n-grams(hi ) maxj #g (hi , ri,j ) P P pn = i g ∈ n-grams(hi ) #g (hi ) where #g (·) is the number of occurrences of n-gram g in a given  sentence, and #g (u, v) is a shorthand for min #g (u), #g (v) . It has been demonstrated that metrics such as B LEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1 Unless mentioned otherwise, B LEU refers to the original IBM B LEU as first described in (Papineni et al., 2002). 2 In the case of multiple references, B LEU selects the reference whose length is closest to that of the hypothesis. In a nuts"
P15-2073,D14-1020,0,0.0691491,"evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), emp"
P15-2073,N15-1124,0,0.0162952,"w model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU"
P15-2073,W07-0734,0,0.0243517,"ool”, “well then! Why were the biscuits needed?”); others are a little more plausible, but irrelevant or possibly topic changing (“ohh I love that song”). Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though 447 3 For this work, we sought 2 additional annotations of the seed responses for consistency with the mined responses. As a result, scores for some seed responses slipped below our initial threshold of 4. Nonetheless, these responses were retained. test set.4 While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwi"
P15-2073,C12-1121,0,0.0718467,"corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: B LEU, ∆B LEU, and sentence-level B LEU (sB LEU). The last computes sentence-level B LEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of B LEU use n-gram order up to 2 (B LEU-2), as this achieves better correlation for all metrics on this data. extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples. 4.2 Human Evaluation of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems,"
P15-2073,P03-1021,0,0.0436342,"rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While ∆B LEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the"
P15-2073,P02-1040,0,0.116946,"f outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result"
P15-2073,D11-1054,0,0.592121,"f a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for C"
P15-2073,N15-1020,1,0.452044,"to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic"
P15-2073,P12-2008,0,0.0788347,"llison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU in which n-grams are weighted by tf ·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU"
P16-1186,N15-1027,0,0.129721,"w, x) = pˆ(w|x)+kp (data) noise (w|x) pˆ(y = 0|w, x) = 1 − pˆ(y = 1|w, x) (noise). This formulation still involves a softmax over the vocabulary to compute pˆ(w|x). However, Mnih and Teh (2012) suggest to forego normalization and replace pˆ(w|x) with unnormalized exponentiated scores. This makes the training complexity independent of the vocabulary size. At test time, softmax normalization is reintroduced to get a proper distribution. We also follow Mnih and Teh (2012) recommendations for pnoise and rely on a unigram distribution of the training set. 1977 2.6 Devlin et al. (2014), followed by Andreas and Klein (2015), proposed to relax score normalization. Their strategy (here referred to as WeaknormSQ) associates unnormalized likelihood maximization with a penalty term that favors normalized predictions. This yields the following loss over the training set T X X (log Z(x))2 s(w|x) + α L(2) α =− (w,x)∈T (w,x)∈T where s(w|x) refers to the unnormalized score of P word w given context x and Z(x) = w exp(s(w|x)) refers to the partition function for context x. This strategy therefore pushes the log partition towards zero. For efficient training, the second term can be down-sampled X α X (log Z(x))2 L(2) s(w|x)"
P16-1186,W12-2703,0,0.0101182,"ural strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 1 Introduction Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014). Similar models are also developed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015). Language models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010; Jozefowicz et al., 2016), or"
P16-1186,J92-4003,0,0.553708,"Missing"
P16-1186,P14-1129,0,0.460267,"to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 1 Introduction Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014). Similar models are also developed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015). Language models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010; Jozefowicz et al., 2016), or log-bilinear models (Mnih and Hinton, 2010). This paper does not focus on architecture or context representation but rather on how to efficiently deal with large output vocabularies, a problem commo"
P16-1186,N12-1005,0,0.0121268,"lf normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 1 Introduction Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014). Similar models are also developed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015). Language models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010; Jozefowicz et al., 2016), or log-bilinear models (Mnih and Hinton, 2010). This paper does not focus on architecture or context representation but rather on how to efficiently deal with large output vocabula"
P16-1186,E14-1051,0,0.0643209,"contains an equal share of the total unigram probability. We compare this strategy to random class assignment and to clustering based on word 1976 W k+1 dA |A| speeds up both training and inference. This is in contrast to hierarchical softmax which is fast during training but requires even more effort than softmax for computing the most likely next word. hk dB |B| dC |C| dA dB dC 2.4 Figure 1: Output weight matrix W k+1 and hidden layer hk for differentiated softmax for vocabulary partitions A, B, C with embedding dimensions dA , dB , dC ; non-shaded areas are zero. contexts, relying on PCA (Lebret and Collobert, 2014). A full comparison of context-based clustering is beyond the scope of this work (Brown et al., 1992; Mikolov et al., 2013). 2.3 Differentiated Softmax This section introduces a novel variation of softmax that assigns a variable number of parameters to each word in the output layer. The weight matrix of the final layer W k+1 ∈ Rdk ×V stores output embeddings of size dk for the V words the language model may predict: W1k+1 ; . . . ; WVk+1 . Differentiated softmax (D-Softmax) varies the dimension of the output embeddings dk across words depending on how much model capacity, or parameters, are de"
P16-1186,J93-2004,0,0.0615191,"n function only for a small fraction of the training data. We extend this strategy to the case where the log partition term is not squared (Weaknorm), i.e., X α X L(1) = − s(w|x) + log Z(x) α,γ γ (w,x)∈T (w,x)∈Tγ For α = 1, this loss is an unbiased estimator of the (2) negative log-likelihood of the training data L1 = P − (w,x)∈T s(w|x) + log Z(x). 3 Dataset PTB gigaword billionW Infrequent Normalization Experimental Setup Datasets We run experiments over three news datasets of different sizes: Penn Treebank (PTB), WMT11-lm (billionW) and English Gigaword, version 5 (gigaword). Penn Treebank (Marcus et al., 1993) is the smallest corpus with 1M tokens and we use a vocabulary size of 10k (Mikolov et al., 2011a). The billion word benchmark (Chelba et al., 2013) comprises almost one billion tokens and a vocabulary of about 800k words1 . Gigaword (Parker et al., 2011) is even larger with 5 billion tokens and was previously used for language modeling (Heafield, 2011) but there is no standard train/test split or vocabulary for this set. We split according to time: training covers 1994–2009 and test covers 2010. The vocabulary comprises the 100k most frequent words in train. Table 1 summarizes the data statis"
P16-1186,W11-2123,0,0.146785,"Missing"
P16-1186,D14-1162,0,0.0875346,", 512 hidden units yield the best validation performance. D-softmax shows that it is possible to selectively increase capacity, i.e., to allocate more hidden units to the most frequent words at the expense of rarer words. This captures most of the benefit of a larger softmax model while staying within a reasonable training budget. 5.2 Effect of Initialization We consider initializing both the input word embeddings and the output matrix from Hellinger PCA embeddings. Several alternative techniques for pre-training embeddings have been proposed (Mikolov et al., 2013; Lebret and Collobert, 2014; Pennington et al., 2014). Our experiment highlights the advantage of initialization and do not aim to compare embedding techniques. Figure 8 shows that PCA is better than random for initializing both input and output word representations; initializing both from PCA is even better. We see that even after long training sessions, the initial conditions still impact the validation perplexity. We observed this trend also with 160 Perplexity 5.1 140 120 100 80 0 50 100 150 200 250 300 Training tokens (millions) Figure 7: Validation perplexity per iteration on billionW for softmax and D-softmax. Softmax uses the same number"
P16-1186,W12-2702,0,0.0204101,"including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 1 Introduction Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014). Similar models are also developed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015). Language models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010; Jozefowicz et al., 2016), or log-bilinear models (Mnih and Hinton, 2010). This paper do"
P16-1186,N15-1020,1,0.470863,"Missing"
P16-1186,D13-1140,0,0.0178582,"erarchical softmax, target sampling, noise contrastive estimation and self normalization. We extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney. 1 Introduction Neural network language models (Bengio et al., 2003; Mikolov et al., 2010) have gained popularity for tasks such as automatic speech recognition (Arisoy et al., 2012) and statistical machine translation (Schwenk et al., 2012; Vaswani et al., 2013; Baltescu and Blunsom, 2014). Similar models are also developed for translation (Le et al., 2012; Devlin et al., 2014; Bahdanau et al., 2015), summarization (Chopra et al., 2015) and language generation (Sordoni et al., 2015). Language models assign a probability to a word given a context of preceding, and possibly subsequent, words. The model architecture determines how the context is represented and there are several choices including recurrent neural networks (Mikolov et al., 2010; Jozefowicz et al., 2016), or log-bilinear models (Mnih and Hinton, 2010). This paper does not focus on archit"
P16-1186,P15-1001,0,\N,Missing
P16-1186,D11-1125,0,\N,Missing
P17-1012,D13-1176,0,0.0955095,"newstest2016. WMT’15 English-German. We use all available parallel training data, namely Europarl v7, Comaij CNN-c(e)j . j=1 In practice, we found that two different CNNs resulted in better perplexity as well as BLEU compared to using a single one (§5.3). We also found this to perform better than directly summing the ei without transformation as for the pooling model. 3.3 Experimental Setup Related Work There are several past attempts to use convolutional encoders for neural machine translation, however, to our knowledge none of them were able to match the performance of recurrent encoders. (Kalchbrenner and Blunsom, 2013) introduce a convolutional sentence encoder in which a multi-layer CNN generates a fixed sized embedding for a source sentence, or an n-gram representation followed by transposed convolutions for directly generating a per-token decoder input. The latter requires the length of the translation prior to generation and both models were evaluated by rescoring the output of an existing translation system. (Cho et al., 2014a) propose a gated recursive CNN which is repeatedly applied until a fixed-size representation is ob3 Different to the other datasets, we lowercase the training data and evaluate w"
P17-1012,P07-2045,0,0.0046479,"formation because this information can be naturally extracted through the sequential computation. When tuning model settings, we generally observe good correlation between perplexity and BLEU. However, for convolutional encoders perplexity gains translate to smaller BLEU improvements compared to recurrent counterparts (Table 1). We observe a similar trend on larger datasets. Results Recurrent vs. Non-recurrent Encoders We first compare recurrent and non-recurrent encoders in terms of perplexity and BLEU on IWSLT’14 with and without position embeddings (§3.1) and include a phrase-based system (Koehn et al., 2007). Table 1 shows that a single-layer convolutional model with position embeddings (Convolutional) can outperform both a uni-directional LSTM encoder (LSTM) as well as a bi-directional LSTM encoder (BiLSTM). Next, we increase the depth of the convolutional encoder. We choose a 5.2 Evaluation on WMT Corpora Next, we evaluate the BiLSTM encoder and the convolutional encoder architecture on three larger tasks and compare against previously published results. On WMT’16 English-Romanian translation we compare to (Sennrich et al., 2016a), the winning single system entry for this language pair. Their m"
P17-1012,W14-4012,0,0.146078,"Missing"
P17-1012,D14-1179,0,0.0164991,"Missing"
P17-1012,P16-1160,0,0.0357086,"Missing"
P17-1012,D15-1166,0,0.382535,"y deep LSTM setup on WMT’14 English-French translation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bidirectional LSTM.1 1 Introduction Neural machine translation (NMT) is an end-to-end approach to machine translation (Sutskever et al., 2014). The most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a; Bradbury and Socher, 2016; Sennrich et al., 2016a). Recurrent networks are typically parameterized as long short term memory networks (LSTM; Hochreiter et al. 1997) or gated recurrent units (GRU; Cho et al. 2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (§2). There have been several attempts to use convolutional encoder models for neural machine trans2 For width k and sequence length n we require  kernel l m max 1, n−1 forwards on a succession of stacked convok−1 lutional layers compared to n forwards with an RNN. 1"
P17-1012,D16-1123,0,0.00905263,"016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-ofthe-art accuracy. (Lamb and Xie, 2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU. Meng et al. (2015) and (Tu et al., 2015) applied convolutional models to score phrase-pairs of traditional phrasebased and dependency-based translation models. Convolutional architectures have also been successful in language modeling but so far failed to outperform LSTMs (Pham et al., 2016). be addressed by stacking several layers of convolutions followed by non-linearities: additional layers increase the total context size while non-linearities can modulate the effective size of the context as needed. For instance, stacking 5 convolutions with kernel width k = 3 results in an input field of 11 words, i.e., each output depends on 11 input words, and the non-linearities allow the encoder to exploit the full input field, or to concentrate on fewer words as needed. To ease learning for deep encoders, we add residual connections from the input of each convolution to the output and t"
P17-1012,W15-3014,0,0.228853,"valuation. Translations are generated by a beam search and we normalize log-likelihood scores by sentence length. On IWSLT’14 we use a beam width of 10 and for WMT models we tune beam width and word penalty on a separate test set, that is newsdev2016 for WMT’16 English-Romanian, newstest2014 for WMT’15 English-German and ntst1213 for WMT’14 English-French.5 The word penalty adds a constant factor to log-likelihoods, except for the end-of-sentence token. Prior to scoring the generated translations against the respective references, we perform unknown word replacement based on attention scores (Jean et al., 2015). Unknown words are replaced by looking up the source word with the maximum attention score in a pre-computed dictionary. If the dictionary contains no translation, then we simply copy the source word. Dictionaries were extracted from the aligned training data that was aligned with fast align (Dyer et al., 2013). Each source word is mapped to the target word it is most frequently aligned to. For convolutional encoders with stacked CNN-c layers we noticed for some models that the attention maxima were consistently shifted by one word. We determine this per-model offset on the abovementioned dev"
P17-1012,P16-1162,0,0.688281,"lation. We speed up CPU decoding by more than two times at the same or higher accuracy as a strong bidirectional LSTM.1 1 Introduction Neural machine translation (NMT) is an end-to-end approach to machine translation (Sutskever et al., 2014). The most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a; Bradbury and Socher, 2016; Sennrich et al., 2016a). Recurrent networks are typically parameterized as long short term memory networks (LSTM; Hochreiter et al. 1997) or gated recurrent units (GRU; Cho et al. 2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (§2). There have been several attempts to use convolutional encoder models for neural machine trans2 For width k and sequence length n we require  kernel l m max 1, n−1 forwards on a succession of stacked convok−1 lutional layers compared to n forwards with an RNN. 1 The source code will be availabe at https://github"
P17-1012,P15-2088,0,0.0162783,"ecurrent encoder achieves higher accuracy. In follow-up work, the authors improved the model via a soft-attention mechanism but did not reconsider convolutional encoder models (Bahdanau et al., 2015). Concurrently to our work, (Kalchbrenner et al., 2016) have introduced convolutional translation models without an explicit attention mechanism but their approach does not yet result in state-ofthe-art accuracy. (Lamb and Xie, 2016) also proposed a multi-layer CNN to generate a fixed-size encoder representation but their work lacks quantitative evaluation in terms of BLEU. Meng et al. (2015) and (Tu et al., 2015) applied convolutional models to score phrase-pairs of traditional phrasebased and dependency-based translation models. Convolutional architectures have also been successful in language modeling but so far failed to outperform LSTMs (Pham et al., 2016). be addressed by stacking several layers of convolutions followed by non-linearities: additional layers increase the total context size while non-linearities can modulate the effective size of the context as needed. For instance, stacking 5 convolutions with kernel width k = 3 results in an input field of 11 words, i.e., each output depends on 1"
P17-1012,Q16-1027,0,0.152629,"e most successful approach to date encodes the source sentence with a bi-directional recurrent neural network (RNN) into a variable length representation and then generates the translation left-to-right with another RNN where both components interface via a soft-attention mechanism (Bahdanau et al., 2015; Luong et al., 2015a; Bradbury and Socher, 2016; Sennrich et al., 2016a). Recurrent networks are typically parameterized as long short term memory networks (LSTM; Hochreiter et al. 1997) or gated recurrent units (GRU; Cho et al. 2014), often with residual or skip connections (Wu et al., 2016; Zhou et al., 2016) to enable stacking of several layers (§2). There have been several attempts to use convolutional encoder models for neural machine trans2 For width k and sequence length n we require  kernel l m max 1, n−1 forwards on a succession of stacked convok−1 lutional layers compared to n forwards with an RNN. 1 The source code will be availabe at https://github. com/facebookresearch/fairseq 123 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 123–135 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://do"
P19-1346,Q17-1010,0,0.013519,"raw upon when generating an answer. Wikipedia has been found effective for factoid-oriented questions (Joshi et al., 2017; Chen et al., 2017). However, early experiments in our setting showed it to be insufficient to cover the wide range of topics present in ELI5 and to address the open-ended nature of the questions. Instead, we use web data provided by Common Crawl.4 Specifically, we consider each of the individual pages in the July 2018 archive (roughly one per URL) as a single document. The data is tokenized with Spacy5 and we select English documents with FastText language identification (Bojanowski et al., 2017). Finally, we index the data with Apache Lucene.6 Creating support documents. We query the index for the 272K questions and gather the 100 most relevant web sources for each question, excluding Reddit. Each web source is the extracted text of one page in Common Crawl. This leads to supporting text for each question of a few hundred thousand words. There is a good chance that the supporting text contains the necessary information to answer the question, but the sheer amount of data is far beyond the scope of what many modern models can handle. We therefore filter the 100 web sources by selectin"
P19-1346,P17-1171,1,0.901368,"Missing"
P19-1346,W18-2706,1,0.891356,"Missing"
P19-1346,P17-1147,0,0.164702,"al overlap methods (Weissenborn et al., 2017), ELI5 poses a significant challenge in siphoning out important information, as no single sentence or phrase contains the full answer. While there are some datasets that do require multi-sentence supporting knowl3558 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Figure 2: ELI5 questions by starting word, where box size represents frequency. Questions are open ended and diverse. edge such as TriviaQA (Joshi et al., 2017), their answers are still short. We benchmark the performance of several extractive, retrieval, and generative models. Evaluation of our task, and of multi-sentence text generation in general, is challenging. We draw upon several evaluation metrics that quantify performance on intermediary fill-in tasks that lead up to the full answer generation. The overall answer generation quality is measured with ROUGE (Lin, 2004) and various human evaluation studies. We develop a strong abstractive baseline by training a Seq2Seq model on multiple tasks over the same data: language modeling, masked word pr"
P19-1346,Q18-1023,0,0.103701,"et al., 2017), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) constrain the answer to a word or short phrase from the input and evaluate using exact match or F1 with the ground truth span. HotpotQA (Yang et al., 2018) extends this approach by building questions which challenge models to conduct multi-hop reasoning across multiple paragraphs, but the answer is still a short span. Further, the answer must be straightforward, as it needs to be copied from the supporting evidence — precluding most “how” or “why” type questions. Abstractive QA Abstractive datasets include NarrativeQA (Kocisky et al., 2018), a dataset of movie and book summaries and CoQA (Reddy et al., 2018), a multi-domain dialogue dataset. Both collect responses with crowdworkers and find that written answers are mostly extractive and short. MS MARCO (Nguyen et al., 2016), a dataset of crowdsourced responses to Bing queries, has written answers around 1 sentence long with short input passages. TriviaQA (Joshi et al., 2017) contains longer multi-document web input, collected using Bing and Wikipedia. As the dataset is built from trivia, most questions can be answered with a short extractive span. Multi-document summarization Th"
P19-1346,W04-1013,0,0.0827838,"Missing"
P19-1346,W18-6301,1,0.842759,"Seq2Seq models. We train several models based on the Transformer architecture (Vaswani et al., 2017), both in its language model and sequence-to-sequence (Seq2Seq) con3562 Model PPL to generate 40K codes which are applied to all datasets. We model a vocabulary of 52,863 tokens for answer generation. We use the Transformer implementation of fairseq-py (Gehring et al., 2017) and train with the big architecture following the details in (Vaswani et al., 2017). Given our data length, we train with a large batch size by delaying gradient updates until a sufficient number of examples have been seen (Ott et al., 2018). ROUGE 2 L 2.3 10.2 2.3 12.5 Support Document Nearest Neighbor - 1 16.8 16.7 Extractive (TFIDF) Extractive (BidAF) Oracle support doc Oracle web sources - 20.6 23.5 27.4 54.8 2.9 3.1 2.8 8.6 17.0 17.5 19.9 40.3 LM Q + A LM Q + D + A Seq2Seq Q to A Seq2Seq Q + D to A Seq2Seq Multi-task 42.2 33.9 52.9 55.1 32.7 27.8 26.4 28.3 28.3 28.9 4.7 4.0 5.1 5.1 5.4 23.1 20.5 22.7 22.8 23.1 Table 3: Comparison of oracles, baselines, retrieval, extractive, and abstractive models on the full proposed answers. Model LM Q + A LM Q + D + A S2S Q to A S2S Q + D to A S2S Multi-task FILL-1 acc. N V A 31.0 29.6 20"
P19-1346,P18-2124,0,0.0638254,"Missing"
P19-1346,D16-1264,0,0.0741944,"s and generating paragraph-length explanations in response to complex, diverse questions (see illustrations in Figures 1 and 2). The first challenge of ELI5 is the length and diversity of answers that span multiple sentences: ⇤ Equal contribution ‡ Work done while at Facebook AI Research 1 Dataset, Pretrained Models, and Additional Information is available: https://facebookresearch. github.io/ELI5, https://github.com/ facebookresearch/ELI5 questions are complex and cannot be easily addressed by a short response (Nguyen et al., 2016) or by extracting a word or phrase from an evidence document (Rajpurkar et al., 2016). Answers also represent one of several valid ways of addressing the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection (Radford et al., 2018; Devlin et al., 2018). However, their success does not directly carry over to our setting. The second challenge is the length and diversity of the content from knowledge sources required to answer our questions. We leverage evidence queried from the web for each question. In contrast to previous datasets where the human written answer could be found with lexical overlap method"
P19-1346,P16-1162,0,0.0249473,"Missing"
P19-1346,W17-2623,0,0.064004,"Missing"
P19-1346,K17-1028,0,0.0124095,"nswers also represent one of several valid ways of addressing the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection (Radford et al., 2018; Devlin et al., 2018). However, their success does not directly carry over to our setting. The second challenge is the length and diversity of the content from knowledge sources required to answer our questions. We leverage evidence queried from the web for each question. In contrast to previous datasets where the human written answer could be found with lexical overlap methods (Weissenborn et al., 2017), ELI5 poses a significant challenge in siphoning out important information, as no single sentence or phrase contains the full answer. While there are some datasets that do require multi-sentence supporting knowl3558 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Figure 2: ELI5 questions by starting word, where box size represents frequency. Questions are open ended and diverse. edge such as TriviaQA (Joshi et al., 2017), their answers are still"
P19-1346,D18-1259,0,0.0321957,"t and generate long outputs to form a comprehensive answer, leaving this challenge for future research. 2 Related Work Various QA datasets have been proposed in roughly two categories: extractive answers and short abstractive answers (see Table 1). Extractive QA Extractive question answering datasets such as TREC (Voorhees, 2003), SQuAD (Rajpurkar et al., 2016, 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) constrain the answer to a word or short phrase from the input and evaluate using exact match or F1 with the ground truth span. HotpotQA (Yang et al., 2018) extends this approach by building questions which challenge models to conduct multi-hop reasoning across multiple paragraphs, but the answer is still a short span. Further, the answer must be straightforward, as it needs to be copied from the supporting evidence — precluding most “how” or “why” type questions. Abstractive QA Abstractive datasets include NarrativeQA (Kocisky et al., 2018), a dataset of movie and book summaries and CoQA (Reddy et al., 2018), a multi-domain dialogue dataset. Both collect responses with crowdworkers and find that written answers are mostly extractive and short. M"
W09-0437,C08-1144,0,0.171475,"e contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between postal services Hierarchical: postal services Deviation: ( [0-4: @S -> @Xˆ1 |@Xˆ1 ] ( [0-4: @X -> concurrence @Xˆ1 postaux |postal @Xˆ1 ] ( [1-3: @X ) -> des services | services ] postal services ) ) Figure 5: Derivation of a hierarchical translation which cannot be generated by the phrase-based system, in the format"
W09-0437,P06-1002,0,0.031231,"Missing"
W09-0437,D08-1023,0,0.0286128,"Missing"
W09-0437,J07-2003,0,0.0953474,"tion for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based translation focuses quite intently on its structural differences from phrase-based translation. • A hierarchical model can tra"
W09-0437,W07-0414,0,0.210248,"Missing"
W09-0437,P07-1094,0,0.0347971,"t criterion such as WER and measure the amount of deviation from the reference. We could also maximize BLEU with respect to the reference as in Dreyer et al. (2007), but it is less interpretable. 7 Conclusion and Future Work Sparse distributions are common in natural language processing, and machine translation is no exception. We showed that utilizing more of the entire distribution can dramatically improve the coverage of translation models, and possibly their accuracy. Accounting for sparsity explicitly has achieved significant improvements in other areas such as in part of speech tagging (Goldwater and Griffiths, 2007). Considering the entire tail is challenging, since the search space grows exponentially with the number of translation options. A first step might be to use features that facilitate more variety in the top 20 translation options. A more elaborate aim is to look into alternatives to maximum likelihood hood estimation such as in Blunsom and Osborne (2008). Additionally, our expressiveness analysis shows Acknowledgements This research was supported by the Euromatrix Project funded by the European Commission (6th Framework Programme). The experiments were conducted using the resources provided by"
W09-0437,N03-1017,1,0.0368304,"otone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are respon"
W09-0437,P07-2045,1,0.0272604,"coders to generate the reference via disallowing reference-incompatible hypothesis or chart entries. This leaves only some search restrictions such as the distortion limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was imp"
W09-0437,2005.mtsummit-papers.11,1,0.0556356,"reorder phrases arbitrarily within the distortion limit, while the hierarchical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is"
W09-0437,C08-1064,1,0.811433,"spaces of phrase-based and hierarchical phrase-based models are highly overlapping despite the well known structural differences. 1 • Second, we find that the high-probability regions in the search spaces of phrase-based and hierarchical systems are nearly identical (§4). This means that reported differences between the models are due to their rankings of competing hypotheses, rather than structural differences of the derivations they produce. 2 Introduction Models, Search Spaces, and Errors A translation model consists of two distinct elements: an unweighted ruleset, and a parameterization (Lopez, 2008a; 2009). A ruleset licenses the steps by which a source string f1 ...fI may be rewritten as a target string e1 ...eJ . A parameterization defines a weight function over every sequence of rule applications. In a phrase-based model, the ruleset is simply the unweighted phrase table, where each phrase pair fi ...fi0 /ej ...ej 0 states that phrase fi ...fi0 in the source can be rewritten as ej ...ej 0 in the target. The model operates by iteratively applying rewrites to the source sentence until each source word has been consumed by exactly one rule. There are two additional heuristic rules: The"
W09-0437,E09-1061,1,0.884152,"Missing"
W09-0437,J03-1002,0,0.00694681,"chical model requires some lexical evidence for movement, resorting to monotone translation otherwise. • While both models can indirectly model word deletion in the context of phrases, the hierarchical model can delete words using non-local context due to its use of discontiguous phrases. 5.2 Experimental Systems Experimental Data We conducted experiments in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of th"
W09-0437,P03-1021,0,0.063762,"nts in French-English translation, attempting to make the experimental conditions for both systems as equal as possible. Each system was trained on French-English Europarl (Koehn, 2005), version 3 (40M words). The corpus was aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the grow-diag-finaland heuristic (Koehn et al., 2003). A trigram language model with modified Kneser-Ney discounting and interpolation was used as produced by the SRILM toolkit (Stolcke, 2002). Systems were optimized on the WMT08 French-English development data (2000 sentences) using minimum error rate training (Och, 2003) and tested on the WMT08 test data (2000 sentences). Rules based on unaligned words at the edges of foreign and source spans were not allowed unless otherwise stated, this is denoted as the tightness conThe underlying assumption in most discussions of these models is that these differences in their generative stories are responsible for differences in performance. We believe that this assumption should be investigated empirically. In an interesting analysis of phrase-based and hierarchical translation, Zollmann et al. (2008) forced a phrase-based system to produce the translations generated by"
W09-0437,P02-1040,0,0.0796532,"e are two additional heuristic rules: The distortion limit dl constrains distances over which phrases can be reordered, and the translation option limit tol constrains the number of target phrases that may be considered for any given source phrase. Together, these rules completely determine the finite set of all possible target sentences for a given source sentence. We call this set of target sentences the model search space. The parameterization of the model includes all information needed to score any particular seMost empirical work in translation analyzes models and algorithms using BLEU (Papineni et al., 2002) and related metrics. Though such metrics are useful as sanity checks in iterative system development, they are less useful as analytical tools. The performance of a translation system depends on the complex interaction of several different components. Since metrics assess only output, they fail to inform us about the consequences of these interactions, and thus provide no insight into the errors made by a system, or into the design tradeoffs of competing systems. In this work, we show that it is possible to obtain such insights by analyzing translation system components in isolation. We focus"
W09-0437,2008.amta-srw.6,0,0.0121521,"n limit for the phrase-based system for which we controlled, or the maximum number of source words involved in a rule application for the hierarchical system. Table 2: Ruleset size expressed as percentage of available rules when varying the limit of translation options tol per English span and percentage of French spans with up to tol translations. port this hypothesis experimentally in §5.4. 5.1 4 Our phrase-based system is Moses (Koehn et al., 2007). We set its stack size to 105 , disabled the beam threshold, and varied the translation option limit tol. Forced translation was implemented by Schwartz (2008) who ensures that hypothesis are a prefix of the reference to be generated. Our hierarchical system is Hiero (Chiang, 2007), modified to construct rules from a small sample of occurrences of each source phrase in training as described by Lopez (2008b). The search parameters restricting the number of rules or chart entries as well as the minimum threshold were set to very high values (1050 ) to prevent pruning. Forced translation was implemented by discarding rules and chart entries which do not match the reference. How Similar are Model Search Spaces? Most work on hierarchical phrase-based tra"
W09-0437,P06-1123,0,0.0248631,"ation involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Reference: competition between"
W09-0437,C08-1136,0,0.0120129,": The second rule application involves both the reordering of the translation of postaux and the omittance of a translation for concurrence. This translation could be easily captured by a phrase-pair, however, it requires that the training data contains exactly such an example which was not the case. The closest rule the phrase-based rulestore contains is des services postaux / postal services which fails since it does not cover all of the input. This is an example for when the generalisation of the hierarchical model is superior to the phrase-based approach. 6 Related Work and Open Questions Zhang et al. (2008) and Wellington et al. (2006) answer the question: what is the minimal grammar that can be induced to completely describe a training set? We look at the related question of what a heuristically induced ruleset can translate in an unseen test set, considering both phrase- and grammar-based models. We also extend the work of Zollmann et al. (2008) on Chinese-English, performing the analysis in both directions and providing a detailed qualitative explanation. Our focus has been on the induction error of models, a previously unstudied cause of transla230 Source: concurrence des services postaux Re"
W16-2207,bojar-prokopova-2006-czech,0,0.0542835,"Missing"
W16-2207,P14-1138,0,0.287134,"Missing"
W16-2207,C96-2141,0,0.659753,"ummarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. 1 Introduction Word alignment is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other. Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007). Recent neural approaches include Yang et al. (2013) who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model. This treats potentially erroneous alignments as supervision. Tamura et al. (2014) sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data. They optimize a global loss that requires an expensive beam search to approximate the sum over all alignments. 2 Aggregation Model In the following,"
W16-2207,N13-1073,0,0.300881,"Missing"
W16-2207,P13-1017,0,0.157415,"target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. 1 Introduction Word alignment is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other. Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007). Recent neural approaches include Yang et al. (2013) who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model. This treats potentially erroneous alignments as supervision. Tamura et al. (2014) sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data. They optimize a global loss that requires an expensive beam search to approximate the sum over all alignments. 2 Aggregation Model In the following, we consider a target-source sentence pair (e, f ), with e = (e1 , . . . , e|e |) and f = (f1 , . . . , f|f |). Words are represented by fj and ei"
W16-2207,N03-1017,0,0.0479833,"Missing"
W16-2207,P07-2045,0,0.0134393,"models are evaluated in terms of precision, recall, F-measure and Alignment Error Rate (AER). We train models in each language direction and then symmetrize the resulting alignments using either the intersection or the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). We validated the choice of symmetrization heuristic on each language pair and chose the best one for each model considering the two aforementioned types as well as grow-diag-final and growdiag. Additionally, we train phrase-based machine translation models with our alignments using the popular Moses toolkit (Koehn et al., 2007). For English-French, we train on the news commentary corpus v10, for English-Czech we used news commentary corpus v11, and for Romanian-English we used the Europarl corpus v8. We tuned our models on the WMT2015 test set for EnglishCzech as well as for Romanian-English; for English-French we tuned on the WMT2014 test set. Final results are reported on the WMT2016 test set for English-Czech as well as RomanianEnglish, and for English-French we report results on the WMT2015 test set (as there is no track for this language-pair in 2016). We compare our model to Fast Align, a popular log-linear re"
W16-2207,E14-1051,1,0.820557,"are mapped to a unique UNK token. The word embedding sizes deemb and dfemb , as well as the char-n-gram embedding size is 128. For LSE, we set r = 1 in (4). Table 1 shows that the LogSumExp (LSE) aggregator performs best on all datasets for every direction as well as in the symmetrized setting using the grow-diag-final heuristic. All results are based on a single model trained with the ’distance to the diagonal’ feature detailed above.4 We therefore use LSE for the remaining experiments. We initialize the word embeddings with a simple PCA computed over the matrix of word cooccurrence counts (Lebret and Collobert, 2014). The co-occurrence counts were computed over the common crawl corpus provided by WMT16. For part of speech tagging we used the Stanford parser on English-French data, and MarMoT (Mueller et al., 2013) for Romanian-English as well as English-Czech. En-Fr Fr-En symmetrized Ro-En En-Ro symmetrized En-Cz Cz-En symmetrized We trained 4 systems for the ensembles, each using a different random seed to vary the weight initialization as well as the shuffling of the training set. We averaged the alignment scores predicted by each system before decoding. The alignment threshold variables µ− (ei ) and σ"
W16-2207,W05-0809,0,0.261088,"Missing"
W16-2207,W95-0115,0,0.0463518,"nd a source word fj : i j diag(i, j) = − , |e ||f | This feature allows the model to learn that aligned sentence pairs use roughly the same word order and that alignment links remain close to the diagonal. We use this feature only for the source network because it encodes relative position information which only needs to be encoded once. If we would use absolute position instead, then we would need to encode this information both on the source and the target side. Part-of-speech Words pairs that are good translations of each other are likely to carry the same part of speech in both languages (Melamed, 1995). We therefore add the part-of-speech information to the model. Char n-gram. We consider unigram character position features. Let K be the maximum size for a word in a dictionary. We denote the dictionary of characters as C. Every character is represented by its index c (with 1 < c < |C|). We associate every character c at position k with a vector at position ((k − 1) ∗ |C|) + c in a lookup-table. For a given word, we extract all unigram character position embeddings, and average them to obtain a character embedding for a given word. 4.3 Setup The kernel sizes of the target network nete (·) ar"
W16-2207,W03-0301,0,0.230786,") = M e,1  .   k1e LTW e ([e]i+a ) 2 This can be seen by observing that the gradients for all source words are the same. 3 This may result in a source word being aligned to multiple target words. 68 ke e e 4 e where a = b 22 c, M e,1 ∈ Rdhu ×(demb k1 ) is a matrix of parameters, and dehu is the number of hidden units (hu). The outputs of the first layer cnne are concatenated to form a matrix of size k2e dehu which is fed to the second layer: nete (xei ) = M e,2 tanh(cnne (xei )) demb ×(k2e 4.1 Datasets We use the English-French Hansards corpus as distributed by the NAACL 2003 shared task (Mihalcea and Pedersen, 2003). This dataset contains 1.1M sentence pairs and the test and validation sets contain 447 and 37 examples respectively. We also evaluate on the Romanian-English dataset of the ACL 2005 shared task (Martin et al., 2005) comprising 48K sentence pairs for training, 248 for testing and 17 for validation. For EnglishCzech experiments, we use the WMT news commentary corpus for training (150K sentence pairs) and a set of 515 sentences for testing (Bojar and Prokopov´a, 2006). (6) dehu ) where M e,2 ∈ R is a matrix of parameters, and the tanh(·) operation is applied element wise. The parameters W e , M"
W16-2207,D13-1032,0,0.0425505,"Missing"
W16-2207,J03-1002,0,0.129016,"on operation that summarizes the alignment scores for a given target word. A soft-margin objective increases scores for true target words while decreasing scores for target words that are not present. Compared to the popular Fast Align model, our approach improves alignment accuracy by 7 AER on EnglishCzech, by 6 AER on Romanian-English and by 1.7 AER on English-French alignment. 1 Introduction Word alignment is the task of finding the correspondence between source and target words in a pair of sentences that are translations of each other. Generative models for this task (Brown et al., 1990; Och and Ney, 2003; Vogel et al., 1996) still form the basis for many machine translation systems (Koehn et al., 2003; Chiang, 2007). Recent neural approaches include Yang et al. (2013) who introduce a feed-forward networkbased model trained on alignments that were generated by a traditional generative model. This treats potentially erroneous alignments as supervision. Tamura et al. (2014) sidesteps this issue by negative sampling to train a recurrent-neural network on unlabeled data. They optimize a global loss that requires an expensive beam search to approximate the sum over all alignments. 2 Aggregation Mod"
W16-2207,P03-1021,0,0.0077114,"7 34.8 30.2 31.3 71.3 78.1 78.4 60.8 61.7 63.2 65.6 69.0 70.0 34.4 31.1 30.0 69.5 74.1 73.0 66.5 71.8 74.5 68.0 73.0 73.7 32.0 27.0 26.0 Table 4: Romanian-English results (cf. Table 3). 4.4.4 BLEU evaluation Table 6 presents the BLEU evaluation of our alignments. For each language-pair, we select the best alignment model reported in Tables 3, 4 and 5, and align the training data. We use the alignments to run the standard phrase-based training pipeline using those alignments. Our BLEU results show the average BLEU score and standard deviation for five runs of minimum error rate training (MERT; Och 2003). Our alignments achieve slightly better results for Romanian-English as well as English-Czech while performing on par with Fast Align on English-French translation. affects accuracy, both for the baseline and NNSA. On Romanian-English (Table 4) our model outperforms the baseline in both directions as well. Adding ensembles further improves accuracy and leads to a significant improvement of 6 AER over the best symmetrized baseline result (from 32 to 26). On English-Czech (Table 5) our model outperforms the baseline in both directions as well. We added the character feature to better deal with"
W16-2207,J90-2002,0,\N,Missing
W16-2207,J07-2003,0,\N,Missing
W18-2706,kobus-etal-2017-domain,0,0.0141827,"ample et al., 2017). Text generation work focuses on controlling tense or sentiment with variational auto-encoders (Hu et al., 2017). Shen et al. (2017) relies on adversarial training for manipulating sentence sentiment and Sennrich et al. (2016a) propose using side constraints for polite neural machine translation models. Takeno et al. (2017) extend the side constraints to control further aspects of translation output, such as length. Others have worked on style, for example Ficler and Goldberg (2017) propose using a conditional language model to generate text with stylistic requirements and Kobus et al. (2017) propose using tokens and additional features to translate text in different domains. Filippova (2017) proposes controlling length for generating answers in a question answering task. Kikuchi et al. (2016) explores length control for sentence compression using decoding-time restrictions and training-time length token embeddings. Motivated by simplicity, our work relies on conditional language modeling and does not require 1 48 github.com/facebookresearch/fairseq Model with beam size 5. To avoid repetition, we prevent the decoder from generating the same trigram more than once, following Paulus"
W18-2706,K16-1002,0,0.123005,"Missing"
W18-2706,N16-1012,1,0.919466,"Missing"
W18-2706,W04-1013,0,0.131198,"ength control for sentence compression using decoding-time restrictions and training-time length token embeddings. Motivated by simplicity, our work relies on conditional language modeling and does not require 1 48 github.com/facebookresearch/fairseq Model with beam size 5. To avoid repetition, we prevent the decoder from generating the same trigram more than once, following Paulus et al. (2017). fairseq + trigram decoding + intra-attention + BPE + tuning min/max len Evaluation: On the CNN-Dailymail benchmark, our automatic evaluation reports F1-ROUGE scores for ROUGE-1, ROUGE-2, and ROUGE-L (Lin, 2004). We compare to existing abstractive baselines (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017). We also compare with Lead-3 which selects the first three sentences of the article as the summary. Note that, although simple, this baseline is not outperformed by all models. 1 ROUGE 2 L 33.32 36.18 36.69 37.48 37.73 12.64 14.10 14.28 15.12 15.03 30.57 33.18 33.47 34.16 34.49 Table 1: Baseline without control variables. Each row add a feature on top of the previous row features. Model For human evaluation, we conduct a human evaluation study using Amazon Mechanical Turk and the test"
W18-2706,W17-4912,0,0.0962568,"rch field. Research in computer vision includes style transfer (Gatys et al., 2015) or controllable image generation (Lample et al., 2017). Text generation work focuses on controlling tense or sentiment with variational auto-encoders (Hu et al., 2017). Shen et al. (2017) relies on adversarial training for manipulating sentence sentiment and Sennrich et al. (2016a) propose using side constraints for polite neural machine translation models. Takeno et al. (2017) extend the side constraints to control further aspects of translation output, such as length. Others have worked on style, for example Ficler and Goldberg (2017) propose using a conditional language model to generate text with stylistic requirements and Kobus et al. (2017) propose using tokens and additional features to translate text in different domains. Filippova (2017) proposes controlling length for generating answers in a question answering task. Kikuchi et al. (2016) explores length control for sentence compression using decoding-time restrictions and training-time length token embeddings. Motivated by simplicity, our work relies on conditional language modeling and does not require 1 48 github.com/facebookresearch/fairseq Model with beam size"
W18-2706,D15-1166,0,0.0835683,"esulting from changing source-style conditioning. 3 Related Work 3.1 Sequence-to-Sequence for Summarization Automatic summarization has been an active research field for 60 years (Luhn, 1958). Extractive and abstractive methods have benefited from advances in natural language processing, pattern recognition, and machine learning (Nenkova et al., 2011). Recently, sequenceto-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b). Neural abstractive summarization has built upon advances from machine translation and related fields: attention (Bahdanau et al., 2015) enables generation to focus on parts of the source document while pointers (Vinyals et al., 2015a) help abstractive summarization to copy entities from the input (See et al., 2017; Paulus et al., 2017; Nallapati et al., 2016). However, summarization also has distinct challenges. The generation of multi-sentence summaries differs from single sentence translation: left-to-right decod"
W18-2706,K16-1028,0,0.128394,"a decoder network to generate a summary by attending to the source representation (Bahdanau et al., 2015). We introduce a straightforward and extensible controllable summarization model to enable personalized generation and fully leverage that automatic summaries are generated at the reader’s request. We show that (1) our generated summaries follow the specified preferences and (2) these control variables guide the learning process and improve generation even when they are set automatically during inference. Our comparison with state-of-the-art models on the standard CNN/DailyMail benchmark (Nallapati et al., 2016), a multi-sentence summarization news corpus, highlights the advantage of our approach. On both the entity-anonymized (+0.76 F1-ROUGE1) and full text versions (+0.85 F1-ROUGE1) of the dataset, we outperform previous pointer-based models trained with maximum likelihood despite the relative simplicity of our model. Further, we demonstrate in a blind human evaluation study that our model generates summaries preferred by human readers. Current models for document summarization disregard user preferences such as the desired length, style, the entities that the user might be interested in, or how mu"
W18-2706,D16-1140,0,0.515817,"ir-encoding (BPE) for tokenization, a proven strategy that has been shown to improve the generation of proper nouns in translation (Sennrich et al., 2016b). We share the representation of the tokens in the encoder and decoder embeddings and in the last decoder layer. text by prepending a particular length marker token. Our experiments (§5.2) provide quantitative and qualitative evidence that the model effectively uses this variable: output length is easily controlled by changing the length marker and supplying ground truth markers drastically improves summary quality. We compare our method to Kikuchi et al. (2016) and demonstrate that our straightforward length control strategy is more effective. 2.3 Entity-Centric Summarization The reader might be interested in a document to learn about specific entities, such as people or locations. For example, a sports fan reading about a recent game might want to focus the summary on the performance of their favorite player. To enable entity-centric summaries, we first anonymize entities by replacing all occurrences of a given entity in a document by the same token. For training, we also anonymize the corresponding reference summary. For a (document, summary) pair"
W18-2706,D15-1044,0,0.158433,"ggested by (See et al., 2017). We evaluate on two versions of the data: the entity anonymized version (Hermann et al., 2015; Nallapati et al., 2016; Paulus et al., 2017) and the full text version (See et al., 2017). We use BPE with 30K types (Sennrich et al., 2016b) for most experiments. For non-BPE models, input and output vocabularies have resp. 47k and 21k word types, corresponding to types with more than 20 train occurrences. Further, we compare length control with (Kikuchi et al., 2016) on DUC-2004 single-sentence summarization task. We train on English Gigaword following the protocol of Rush et al. (2015). The data consist of 3.6 million pairs (first sentence, headline of news articles). Following (Kikuchi et al., 2016), we evaluate on the 500 documents in the DUC2004 task-1. We use a source and target vocabulary of 30k words. Architecture, Training, and Generation: We implement models with the fairseq library1. For CNN-Dailymail, our model has 8 layers in the encoder and decoder, each with kernel width 3. We use 512 hidden units for each layer, embeddings of size 340, and dropout 0.2. For DUC, we have 6 layers in the encoder and decoder with 256 hidden units. Similar to Gehring et al. (2017),"
W18-2706,P17-1099,0,0.404629,"ies that are closer to the reference summary. We additionally provide examples of distinct summaries resulting from changing source-style conditioning. 3 Related Work 3.1 Sequence-to-Sequence for Summarization Automatic summarization has been an active research field for 60 years (Luhn, 1958). Extractive and abstractive methods have benefited from advances in natural language processing, pattern recognition, and machine learning (Nenkova et al., 2011). Recently, sequenceto-sequence neural networks (Sutskever et al., 2014) have been applied to abstractive summarization (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) following their success in translation (Bahdanau et al., 2015; Luong et al., 2015b), parsing (Luong et al., 2015a) and image captioning (Vinyals et al., 2015b). Neural abstractive summarization has built upon advances from machine translation and related fields: attention (Bahdanau et al., 2015) enables generation to focus on parts of the source document while pointers (Vinyals et al., 2015a) help abstractive summarization to copy entities from the input (See et al., 2017; Paulus et al., 2017; Nallapati et al., 2016). However, summarization also has distinct challenges."
W18-2706,N16-1005,0,0.295319,"ni et al., 2017; Paulus et al., 2017). To combine encoder and decoder attention, we alternate between each type of attention at every layer. Much prior work on the CNN-Dailymail benchmark employed pointer networks to copy rare entities from the input (Nallapati et al., 2016), which introduces additional complexity to the model. Instead, we rely on sub-word tokenization and weight sharing. We show this simple approach is very effective. Specifically, we use byte-pair-encoding (BPE) for tokenization, a proven strategy that has been shown to improve the generation of proper nouns in translation (Sennrich et al., 2016b). We share the representation of the tokens in the encoder and decoder embeddings and in the last decoder layer. text by prepending a particular length marker token. Our experiments (§5.2) provide quantitative and qualitative evidence that the model effectively uses this variable: output length is easily controlled by changing the length marker and supplying ground truth markers drastically improves summary quality. We compare our method to Kikuchi et al. (2016) and demonstrate that our straightforward length control strategy is more effective. 2.3 Entity-Centric Summarization The reader mig"
W18-2706,P16-1162,0,0.442752,"ni et al., 2017; Paulus et al., 2017). To combine encoder and decoder attention, we alternate between each type of attention at every layer. Much prior work on the CNN-Dailymail benchmark employed pointer networks to copy rare entities from the input (Nallapati et al., 2016), which introduces additional complexity to the model. Instead, we rely on sub-word tokenization and weight sharing. We show this simple approach is very effective. Specifically, we use byte-pair-encoding (BPE) for tokenization, a proven strategy that has been shown to improve the generation of proper nouns in translation (Sennrich et al., 2016b). We share the representation of the tokens in the encoder and decoder embeddings and in the last decoder layer. text by prepending a particular length marker token. Our experiments (§5.2) provide quantitative and qualitative evidence that the model effectively uses this variable: output length is easily controlled by changing the length marker and supplying ground truth markers drastically improves summary quality. We compare our method to Kikuchi et al. (2016) and demonstrate that our straightforward length control strategy is more effective. 2.3 Entity-Centric Summarization The reader mig"
W18-2706,E17-2047,0,0.0195359,"e source document while pointers (Vinyals et al., 2015a) help abstractive summarization to copy entities from the input (See et al., 2017; Paulus et al., 2017; Nallapati et al., 2016). However, summarization also has distinct challenges. The generation of multi-sentence summaries differs from single sentence translation: left-to-right decoders need to be aware of their previous generation at a larger time scale, otherwise models tend to produce repeated text. To address this impediment, (See et al., 2017) introduce coverage modeling, (Paulus et al., 2017) propose intra-decoder attention, and (Suzuki and Nagata, 2017) equip the decoder with an estimator of unigram frequency. Previous work has also explored learning objectives: (Paulus et al., 2017) investigates replacing maximum likelihood training with Reinforcement Learning (RL) to optimize ROUGE, the most common automatic metric to assess summarization. Combining both strategies is found to perform best in human evaluations, as training with RL alone often produces non-grammatical text. Our work builds upon prior research: like (Gehring et al., 2017), we rely on convolutional networks, which enable faster training. This contrasts with prior 2.5 Remainde"
W18-2706,W17-5702,0,0.0259431,"al., 2016; Yu et al., 2017; Zhao et al., 2017; Rajeswar et al., 2017). Building upon unconditioned generation, controllable generation is an emerging research field. Research in computer vision includes style transfer (Gatys et al., 2015) or controllable image generation (Lample et al., 2017). Text generation work focuses on controlling tense or sentiment with variational auto-encoders (Hu et al., 2017). Shen et al. (2017) relies on adversarial training for manipulating sentence sentiment and Sennrich et al. (2016a) propose using side constraints for polite neural machine translation models. Takeno et al. (2017) extend the side constraints to control further aspects of translation output, such as length. Others have worked on style, for example Ficler and Goldberg (2017) propose using a conditional language model to generate text with stylistic requirements and Kobus et al. (2017) propose using tokens and additional features to translate text in different domains. Filippova (2017) proposes controlling length for generating answers in a question answering task. Kikuchi et al. (2016) explores length control for sentence compression using decoding-time restrictions and training-time length token embeddi"
W18-6301,P18-1008,0,0.0364524,"model layers on different workers (Coates et al., 2013) and (ii) data parallel keeps a copy of the model on each worker but distributes different batches to different machines (Dean et al., 2012). We rely on the second scheme and follow synchronous SGD, which has recently been deemed more efficient than asynchronous SGD (Chen et al., 2016). Synchronous SGD distributes the computation of gradients over multiple machines and then performs a synchronized update of the model weights. Large neural machine translation systems have been recently trained with this algorithm with success (Dean, 2017; Chen et al., 2018). Recent work by Puri et al. (2018) considers large-scale distributed training of language models (LM) achieving 109x scaling with 128 GPUs. Compared to NMT training, however, LM training does not face the same challenges of variable batch sizes. Moreover, we find that large batch training requires warming up the learning rate, whereas their work begins training with a large learning rate. There has also been recent work 3 3.1 Experimental Setup Datasets and Evaluation We run experiments on two language pairs, English to German (En–De) and English to French (En–Fr). For En–De we replicate the"
W18-6301,W18-6319,0,0.0924322,"s. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE factorization. We also experiment with scaling training beyond 36M sentence pairs by using data from the Paracrawl corpus (ParaCrawl, 2018). This dataset is extremely large with more than 4.5B pairs for En–De and more than 4.2B pairs for 2 En–Fr. We rely on the BPE vocabulary built on WMT data for each language pair and explore filtering this noisy dataset in Section 4.5. We measure case-sensitive tokenized BLEU with multi-bleu.pl2 and de-tokenized BLEU with SacreBLEU3 (Post, 2018). All results use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. 2017. Checkpoint averaging is not used, except where specified otherwise. 3.2 Pereyra et al., 2017). All experiments are run on DGX-1 nodes c with 8 NVIDIA V100 GPUs interconnected by Infiniband. We use the NCCL2 library and torch.distributed for inter-GPU communication. 4 Experiments and Results In this section we present results for improving training efficiency via reduced precision floating point (Section 4.1), training with larger batches (Section 4.2), and training with multiple nodes"
W18-6301,P16-2008,0,0.059617,"Missing"
W18-6301,N18-3014,0,0.0232821,"tional Linguistics https://doi.org/10.18653/v1/W18-64001 2.5 2.11 2.0 10k 30k opt. steps valid loss (NLL) valid loss (NLL) bsz=25k bsz=100k bsz=200k bsz=400k 50k bsz=25k bsz=100k bsz=200k bsz=400k 2.5 2.11 2.0 10 20 epochs 30 Figure 1: Validation loss for Transformer model trained with varying batch sizes (bsz) as a function of optimization steps (left) and epochs (right). Training with large batches is less data-efficient, but can be parallelized. Batch sizes given in number of target tokens excluding padding. WMT En-De, newstest13. 2 Related Work on using lower precision for inference only (Quinn and Ballesteros, 2018). Another line of work explores strategies for improving communication efficiency in distributed synchronous training setting by abandoning “stragglers,” in particular by introducing redundancy in how the data is distributed across workers (Tandon et al., 2017; Ye and Abbe, 2018). The idea rests on coding schemes that introduce this redundancy and enable for some workers to simply not return an answer. In contrast, we do not discard any computation done by workers. Previous research considered training and inference with reduced numerical precision for neural networks (Simard and Graf, 1993; C"
W18-6301,P17-1099,0,0.0260869,"U after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT’14 EnglishFrench task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs. 1 Introduction Neural Machine Translation (NMT) has seen impressive progress in the recent years with the introduction of ever more efficient architectures (Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). Similar sequence-to-sequence models are also applied to other natural language processing tasks, such as abstractive summarization (See et al., 2017; Paulus et al., 2018) and dialog (Sordoni et al., 2015; Serban et al., 2017; Dusek and Jurc´ıcek, 2016). Currently, training state-of-the-art models on large datasets is computationally intensive and can require several days on a machine with 8 highend graphics processing units (GPUs). Scaling training to multiple machines enables faster experimental turn-around but also introduces new challenges: How do we maintain efficiency in a distributed setup when some batches process faster *Work done while at Facebook AI Research. Our implementation is available at: https://www.github.com/pytorch/fai"
W18-6301,P16-1162,0,0.477631,"es. Moreover, we find that large batch training requires warming up the learning rate, whereas their work begins training with a large learning rate. There has also been recent work 3 3.1 Experimental Setup Datasets and Evaluation We run experiments on two language pairs, English to German (En–De) and English to French (En–Fr). For En–De we replicate the setup of Vaswani et al. (2017) which relies on the WMT’16 training data with 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (BPE; Sennrich et al. 2016). For En–Fr, we train on WMT’14 and borrow the setup of Gehring et al. (2017) with 36M training sentence pairs. We use newstest12+13 for validation and newstest14 for test. The 40K vocabulary is based on a joint source and target BPE factorization. We also experiment with scaling training beyond 36M sentence pairs by using data from the Paracrawl corpus (ParaCrawl, 2018). This dataset is extremely large with more than 4.5B pairs for En–De and more than 4.2B pairs for 2 En–Fr. We rely on the BPE vocabulary built on WMT data for each language pair and explore filtering this noisy dataset in Sect"
W18-6301,N18-2074,0,0.0467512,"th. To illustrate how these characteristics impact training Analysis of Stragglers In a distributed training setup with synchronized SGD, workers may take different amounts of time to compute gradients. Slower workers, or stragglers, cause other workers to wait. There are sev6 vestigated lower precision computation, very large batch sizes (up to 400k tokens), and larger learning rates. Our careful implementation speeds up the training of a big transformer model (Vaswani et al., 2017) by nearly 5x on one machine with 8 GPUs. We improve the state-of-the-art for WMT’14 En-Fr to 43.2 vs. 41.5 for Shaw et al. (2018), training in less than 9 hours on 128 GPUs. On WMT’14 En-De test set, we report 29.3 BLEU vs. 29.2 for Shaw et al. (2018) on the same setup, training our model in 85 minutes on 128 GPUs. BLEU is further improved to 29.8 by scaling the training set with Paracrawl data. Overall, our work shows that future hardware will enable training times for large NMT systems that are comparable to phrase-based systems (Koehn et al., 2007). We note that multi-node parallelization still incurs a significant overhead: 16-node training is only ∼10x faster than 1-node training. Future work may consider better ba"
W18-6301,P07-2045,0,0.0109454,"he training of a big transformer model (Vaswani et al., 2017) by nearly 5x on one machine with 8 GPUs. We improve the state-of-the-art for WMT’14 En-Fr to 43.2 vs. 41.5 for Shaw et al. (2018), training in less than 9 hours on 128 GPUs. On WMT’14 En-De test set, we report 29.3 BLEU vs. 29.2 for Shaw et al. (2018) on the same setup, training our model in 85 minutes on 128 GPUs. BLEU is further improved to 29.8 by scaling the training set with Paracrawl data. Overall, our work shows that future hardware will enable training times for large NMT systems that are comparable to phrase-based systems (Koehn et al., 2007). We note that multi-node parallelization still incurs a significant overhead: 16-node training is only ∼10x faster than 1-node training. Future work may consider better batching and communication strategies. speed, Figure 6 shows the amount of time required to process the 44K sub-batches in the En-De training data. There is large variability in the amount time to process sub-batches with different characteristics: the mean time to process a sub-batch is 0.11 seconds, the slowest sub-batch takes 0.228 seconds and the fastest 0.049 seconds. Notably, there is much less variability if we only con"
W18-6301,N15-1020,1,0.882733,"Missing"
W18-6301,D17-1319,0,0.0249311,"2.0 10k 30k 50k opt. steps 70k Figure 5: Validation loss when training on Paracrawl+WMT with varying sampling ratios. 1:4 means sampling 4 Paracrawl sentences for every WMT sentence. WMT En-De, newstest13. Results with WMT & Paracrawl Training Percentage of sub-batches Fast parallel training lets us additionally explore training over larger datasets. In this section we consider Paracrawl (ParaCrawl, 2018), a recent dataset of more than 4B parallel sentences for each language pair (En-De and En-Fr). Previous work on Paracrawl considered training only on filtered subsets of less than 30M pairs (Xu and Koehn, 2017). We also filter Paracrawl by removing sentence-pairs with a source/target length ratio exceeding 1.5 and sentences with more than 250 words. We also remove pairs for which the source and target are copies (Ott et al., 2018). On En–De, this brings the set from 4.6B to 700M. We then train a En–De model on a clean dataset (WMT’14 news commentary) to score the remaining 700M sentence pairs, and retain the 140M pairs with best average token log-likelihood. To train an En–Fr model, we filter the data to 129M pairs using the same procedure. Next, we explored different ways to weight the WMT and Para"
W19-5333,W18-6301,1,0.876911,"Overview 3.1 Base System Our base system is based on the big Transformer architecture (Vaswani et al., 2017) as implemented in FAIRSEQ. We experiment with increasing network capacity by increasing embed dimension, FFN size, number of heads, and number of layers. We find that using a larger FFN size (8192) gives a reasonable improvement in performance while maintaining a manageable network size. All subsequent models, including ensembles, use this larger FFN Transformer architecture. We trained all our models using FAIRSEQ (Ott et al., 2019) on 128 Volta GPUs, following the setup described in Ott et al. (2018) Data Filtering Bitext Large datasets crawled from the internet are naturally very noisy and can potentially decrease the performance of a system if they are used in their raw form. Cleaning these datasets is an important step to achieving good performance on any downstream tasks. We apply language identification filtering (langid; Lui et al., 2012), keeping only sentence pairs with correct languages on both sides. Although not the most accurate method of language identification (Joulin et al., 2016), one side effect of using langid is the removal of very noisy sentences consisting of mostly g"
W19-5333,W18-6319,0,0.071078,"y scores for the language models on the bolded target language of each translation direction are shown in table 4. With a smaller amount of monolingual Russian data available, we observe that our Russian language model performs worse than the German and English language models. Postprocessing For En→De and En→Ru, we also change the standard English quotation marks (“ ... ”) to Germanstyle quotation marks ( ... “). ” 4 Results Results and ablations for En→De are shown in Table 5, De→En in Table 6, En→Ru in Table 7 and Ru→En in Table 8. We report case-sensitive SacreBLEU scores using SacreBLEU (Post, 2018)1 , using international tokenization for En→Ru. In the final row of each table we also report the casesensitive BLEU score of our submitted system on this year’s test set. All single models and individual models within ensembles are averages of the last 10 checkpoints of training. Our baseline systems are big Transformers as described in (Vaswani et al., 2017). The baselines were trained with minimally filtered data, removing only those sentences longer than 250 words and exceeding a source/target length ratio of 1.5 This setup gave us a reasonable baseline to evaluate data filtering. To selec"
W19-5333,P16-1162,0,0.71702,"s can be attributed to differences in dataset quality, but we believe most of the improvement comes from larger models, larger scale back-translation, and noisy channel model reranking with strong channel and language models. Introduction We participate in the WMT19 shared news translation task in two language pairs and four language directions, English→German (En→De), German→English (De→En), English→Russian (En→Ru), and Russian→English (Ru→En). Our methods are based on techniques and approaches used in our submission from last year (Edunov et al., 2018), including the use of subword models, (Sennrich et al., 2016), large-scale backtranslation, and model ensembling. We train all models using the FAIRSEQ sequence modeling toolkit (Ott et al., 2019). Although document level context for En→De is now available, all our systems are pure sentence level systems. In the future, we expect better results from leveraging this additional context information. Compared to our WMT18 submission, we also decide to compete in the En↔Ru and De→En translation directions. Although all four directions are considered high resource settings where lar2 Data For the En↔De language pair we use all available bitext data including"
W19-5333,D18-1045,1,0.938333,"rve substantial improvements of 4.5 BLEU. Some of these gains can be attributed to differences in dataset quality, but we believe most of the improvement comes from larger models, larger scale back-translation, and noisy channel model reranking with strong channel and language models. Introduction We participate in the WMT19 shared news translation task in two language pairs and four language directions, English→German (En→De), German→English (De→En), English→Russian (En→Ru), and Russian→English (Ru→En). Our methods are based on techniques and approaches used in our submission from last year (Edunov et al., 2018), including the use of subword models, (Sennrich et al., 2016), large-scale backtranslation, and model ensembling. We train all models using the FAIRSEQ sequence modeling toolkit (Ott et al., 2019). Although document level context for En→De is now available, all our systems are pure sentence level systems. In the future, we expect better results from leveraging this additional context information. Compared to our WMT18 submission, we also decide to compete in the En↔Ru and De→En translation directions. Although all four directions are considered high resource settings where lar2 Data For the E"
W19-5333,W11-2123,0,0.209175,"Missing"
W19-5333,P07-2045,0,0.0164531,"l as well as a filtered portion of Russian Commoncrawl. We choose to use Russian Commoncrawl to augment our monolingual data due to the relatively small size of Russian Newscrawl compared to English and German. 314 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 314–319 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics 2.1 Data Preprocessing No filter + length filter + langid filter Similar to last year’s submission for En→De, we normalize punctuation and tokenize all data with the Moses tokenizer (Koehn et al., 2007). For En↔De we use joint byte pair encodings (BPE) with 32K split operations for subword segmentation (Sennrich et al., 2016). For En↔Ru, we learn separate BPE encodings with 24K split operations for each language. Systems trained with this separate BPE encoding performed significantly better than those trained with joint BPE. 2.2 2.2.1 En-Ru 38.8M 35.7M 27.7M 38.5M 33.4M 26.0M Table 1: Number of sentences in bitext datasets for different filtering schemes 3 System Overview 3.1 Base System Our base system is based on the big Transformer architecture (Vaswani et al., 2017) as implemented in FAI"
W19-5333,P12-3005,0,0.248476,"Missing"
W19-5333,P10-2041,0,0.20935,"Missing"
W19-5333,N19-4009,1,0.922625,"anslation, and noisy channel model reranking with strong channel and language models. Introduction We participate in the WMT19 shared news translation task in two language pairs and four language directions, English→German (En→De), German→English (De→En), English→Russian (En→Ru), and Russian→English (Ru→En). Our methods are based on techniques and approaches used in our submission from last year (Edunov et al., 2018), including the use of subword models, (Sennrich et al., 2016), large-scale backtranslation, and model ensembling. We train all models using the FAIRSEQ sequence modeling toolkit (Ott et al., 2019). Although document level context for En→De is now available, all our systems are pure sentence level systems. In the future, we expect better results from leveraging this additional context information. Compared to our WMT18 submission, we also decide to compete in the En↔Ru and De→En translation directions. Although all four directions are considered high resource settings where lar2 Data For the En↔De language pair we use all available bitext data including the bicleaner version of Paracrawl. For our monolingual data we use English and German Newscrawl. Although our language models were tra"
W19-5333,W19-5301,0,\N,Missing
