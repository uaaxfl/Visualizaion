2014.iwslt-papers.2,E09-1081,0,0.0188895,"with a tempo-elastic speech synthesizer. 1. Introduction Today’s speech-to-speech translation solutions are a long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly"
2014.iwslt-papers.2,N09-1043,1,0.713289,"re a long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on"
2014.iwslt-papers.2,W11-2014,0,0.0216987,"long way from transparent and ubiquitous universal translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on inton"
2014.iwslt-papers.2,W10-4301,0,0.0233288,"al translators as envisioned in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for"
2014.iwslt-papers.2,P12-3018,1,0.847891,"ed in science fiction literature (e. g. [1]), for a multitude of reasons. One of the shortcomings is translation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for translation [9], as phras"
2014.iwslt-papers.2,N12-1048,1,0.791569,"ranslation latency, which in speech can be described as the latency between when a concept can be grasped from listening to the source utterance and producing it as part of the target utterance. For swift and seamless communication across language barriers, low translation latency is key. Incremental processing [2] is a technical means to implement interactive speech processing systems for online speech recognition [3], [4], [5], language understanding and generation [6], for speech synthesis [7]. Incremental processing has also been successfully applied to speech-to-speech translation (e. g. [8]), where it helps to bring down processing latency in an integrated system. An important aspect of incremental processing (and hence, incremental translation) is the granularity at which material is being added. A fine granularity of processing is a precondition to low latency, as smaller units can more quickly be passed on to a next module. Previous work on incremental translation has focused on phrasing (based on intonation and somewhat related to meaning units) for translation [9], as phrases can easily be passed on to speech synthesis as one unit. Recently, incremental speech synthesis is"
2014.iwslt-papers.2,2013.iwslt-papers.3,0,0.021799,"we propose that automatic simultaneous interpreting modules, just like human experts, must have recovery capabilities, which enable them to cope with situations in which already-delivered parts of a translation should be revoked and replaced by a different translation. Human experts use and combine various strategies to cope with the problem [15]. We experiment here with the simplest possible solution of dealing with changes: we ignore all changes to words that have already or are currently being spoken. This causes the translation performance to deteriorate, given a fixed delay (similarly to [16]), which will also be analyzed in Section 4. Finally, one intuitively important strategy of human experts is to vary the latency between input and output by varying speech delivery tempo. We report on our initial progress in determining overall latency and reducing it in Section 5. 3. Corpus and Experiment Setup We use the IWSLT 2011 test set of the TED talks corpus as provided by the Web Inventory of Transcribed and Translated Talks [17]. As translation quality and stability may depend to a large extent on languages, we include analyses for three language pairs: en → de, en → es, and de → en."
2014.iwslt-papers.2,2012.eamt-1.60,0,0.0144957,"l changes to words that have already or are currently being spoken. This causes the translation performance to deteriorate, given a fixed delay (similarly to [16]), which will also be analyzed in Section 4. Finally, one intuitively important strategy of human experts is to vary the latency between input and output by varying speech delivery tempo. We report on our initial progress in determining overall latency and reducing it in Section 5. 3. Corpus and Experiment Setup We use the IWSLT 2011 test set of the TED talks corpus as provided by the Web Inventory of Transcribed and Translated Talks [17]. As translation quality and stability may depend to a large extent on languages, we include analyses for three language pairs: en → de, en → es, and de → en.3 We tokenize the respective source material with WASTE [18], using the included models for German and English. We then feed each of the utterances to standard, per-se non-incremental translation systems in a restart-incremental fashion: first translating just the first token, then the first two, then the first three, and so on, ending with the full utterance. This results in a large processing overhead and may confuse the translation sys"
2014.iwslt-papers.2,W09-3943,1,0.848194,"is difference in the following experiments. 4. Evaluation of Basic Measures For a time-aligned source sentence and its corresponding time-aligned incremental translation output that represents the final target language sentence, we find the minimum necessary delay at which the target sentence can be delivered such that the partial translation hypotheses always match the final target language sentence (i. e., the synthesis would never be triggered to start saying a word that is later replaced by a different word during incremental translation). Using the incremental evaluation toolbox intelida [21], we compute the delay that is necessary in order to have all finally chosen target language words available before their scheduled 4 http://translate.google.com/ with the help of some PHPbased automation code. 5 Of course, we could have extracted more precise source language timing information from TED videos, but results would likely be similar and only be available for English as source language. 6 We thank Marcela Charfuelan for making a Spanish voice and linguistic resources available. delivery starts, and without intermittent interruptions from synthesis running out of words to speak. De"
2014.iwslt-papers.2,P06-1140,0,0.0184614,"asily be passed on to speech synthesis as one unit. Recently, incremental speech synthesis is progressing well at a wordby-word granularity, if some additional boundary and finality information is provided [10], [11]. In building language processing systems, joint analysis and optimization across module boundaries often greatly improves performance. The combination of speech recognition with understanding (e. g. [12]) or translation (e. g. [13]) is quite common, but this is less often done for the output side. (One notable exception is joint optimization of natural language generation and TTS [14], however not in an incremental setting.) In this paper, we analyze the timing properties of source and target speech in an incremental machine translation setting in order to evaluate the improvements possible when combining word-by-word incremental machine translation with speech synthesis, particularly with respect to delivery latency. We do not yet actually employ fully incremental synthesis but focus our analysis on the advantages of such a synthesis technique in this contribution. The remainder of this paper is structured as follows: in Section 2, we describe the interplay of incremental"
2020.alw-1.2,S19-2007,0,0.0278286,"uire from these experiments and apply them to classifiers built on our own data once we have accumulated a sufficient amount. We also plan to check the cross-performance between models trained on our own and historical corpora as quality assurance. The data which we have accumulated so far gives us a good idea of which historical corpora are most similar to our own. We explored several corpora, including (Waseem and Hovy, 2016) and (Founta et al., 2018), but focused on Task 5 of SemEval 2019, “Multilingual detection of hate speech against immigrants and women in Twitter (HatEval)” in English (Basile et al., 2019), as it is most recent and they are all of similar genre. Both Task 5 subtasks used the same dataset (cicl2018/HateEvalTeam, 2019) but with different labels. Subtask A was a binary classification task to assign a label of “hate” or “non-hate” to each tweet. Subtask B was a multi-class classification task to assign two additional label pairs to each tweet in addition to “hate” or “non-hate”: “individual” or “group” and “aggressive” or “non-aggressive”. The split across train and development datasets was 9000 to 1000 tweets; these have been open-sourced by the organizing team. The true labels fo"
2020.alw-1.2,S19-2077,0,0.0383704,"Missing"
2020.alw-1.2,D18-2029,0,0.0122786,"ult nature, linking to malicious websites, phishing attempts and other kinds 6.1.1 Annotation Platform After data upload and preprocessing, the annotation platform is deployed and sent to the study participant. Participants annotate each tweet sent 1 The code for all of our tweet filtering heuristics and thread retrieval methods can be accessed at the following GitHub repository: https://github.com/ ishaan007/woah_emnlp_2020 11 of unwanted information, usually executed repeatedly. SMOTE to over-sample the “hate” class as a preprocessing step, followed by the use of Universal Sentence Encoder (Cer et al., 2018) to generate a vector representation of the tweet, and SVM (RBF kernel) to classify the tweet. We also implemented a transformer-based approach for this subtask, based on (MacAvaney et al., 2019), which uses pre-trained BERT for sequence classification, fine-tuned for 10 epochs. This approach in fact outperforms the aforementioned winning approach. For sub-task B, the multi-classification task, we replicated the winning approach (Bauwelinck et al., 2019) by training three separate classifiers to classify three label pairs individually; these classifiers used a linear SVM on handcrafted syntact"
2020.alw-1.2,S19-2009,0,0.0172156,"eEvalTeam, 2019) but with different labels. Subtask A was a binary classification task to assign a label of “hate” or “non-hate” to each tweet. Subtask B was a multi-class classification task to assign two additional label pairs to each tweet in addition to “hate” or “non-hate”: “individual” or “group” and “aggressive” or “non-aggressive”. The split across train and development datasets was 9000 to 1000 tweets; these have been open-sourced by the organizing team. The true labels for the test set have not, however, so we evaluate only on the development set. We replicated the winning approach (Indurthi et al., 2019) for sub-task A in English, which used 8 Results and Discussion Although testing of our platform is still in the pilot phase, early users have shared positive feedback regarding its usability, and have also been able to perform the annotation task with good efficiency, on the order of ∼300 tweets per hour. Given the size of previously-collected datasets in this space, our methodology is efficient enough to generate sufficient training data in less than 40 hours, making it both a cost-effective and robust approach. Given the high fidelity of our labels and the nearperfect ecological validity of"
2020.alw-1.2,W18-5104,1,0.851248,"-specific approach to studying hate speech. (Salem et al., 2016) used content from self-identified hate communities, instead of keywords from hand-coded speech or manually coded hate speech terms, as training data for their work on hate speech detection with some success. In (Nobata et al., 2016), researchers studied abusive language in online user comments on news and finance forums using linguistic, syntactic, and distributed semantic features as well as lexicon-based features. Their dataset has been used to benchmark performance in hate speech detection, as has (Waseem and Hovy, 2016). In (Kshirsagar et al., 2018), researchers developed deep learning models for hate speech detection on Twitter, using transformed word embeddings to classify hate speech on three public datasets. 8 Researchers in journalism have also used more qualitative methods to study abusive and hateful speech towards journalists. For example, UT Austin’s School of Journalism published results from in-depth interviews with 75 female journalists describing how rampant online sexual harassment disrupts their ability to do their jobs (Chen et al., 2018). The Committee to Protect Journalists reported similar findings in 2019 (Westcott an"
2020.alw-1.2,N19-1221,0,0.0145972,"tion, our pilot interviews suggest that including a fill-in “other” label may be useful for generating more nuanced classifiers, especially as there has historically been 12 a lack of annotator agreement on what constitutes hateful speech, which tends to vary in severity and lexical nature depending on the situation (Waseem et al., 2017). 9 of the rich metadata and parent tweet text embeddings present in tweet threads, and have the potential to achieve significantly boosted classification performance compared to that of models built on text embeddings of the potentially harassing tweet alone (Mishra et al., 2019). For the purpose of building the eventual tool to aid journalists in the field, we could alternatively address the relatively small size of our manuallylabelled datasets for training deep learning classifiers, by augmenting them against the large, popular corpora already in existence. We could investigate whether this addition would boost performance compared to classifiers trained only on those large, crowd-sourced corpora, as a measure of effectiveness of our methodology. Finally, we note that while certain semantic features of the classifiers developed using our methodology will differ dep"
2020.alw-1.2,W16-5618,0,0.0172895,"journalism-related Twitter accounts to generate a list of target accounts across five languages. Using the Twitter API to conduct keyword-based searches, they then manually annotated hate vs. non-hate tweets. This yielded highly imbalanced corpora, with more “hate” than “non-hate” tweets for each language. Deep learning models trained on each language corpus achieved best macro-F1 scores over .80 for English, French and Greek but somewhat lower for Spanish and German. Other work has addressed the more general problem of automatic identification of hate speech and abusive language online. In (Waseem, 2016), researchers found that crowd-sourced annotations performed poorly. This indicates the importance of expert annotators, which (Blackwell et al., 2017) situates specifically in terms of classifying harassment. In (Warner and Hirschberg, 2012), researchers using data from Yahoo and the American Jewish Congress found that anti-Semitic hate speech differed linguistically from speech that targeted other religious or ethnic groups, highlighting the need for a community-specific approach to studying hate speech. (Salem et al., 2016) used content from self-identified hate communities, instead of keyw"
2020.alw-1.2,W17-3012,0,0.0130364,". From early feedback, we have also identified additional labels that participants found relevant, such as “campaign” or “brigade”, used to indicate a lexically generic Tweet that is still part of a harassment campaign, as in 2019’s “Learn to code” campaign (Molloy, 2019). In addition, our pilot interviews suggest that including a fill-in “other” label may be useful for generating more nuanced classifiers, especially as there has historically been 12 a lack of annotator agreement on what constitutes hateful speech, which tends to vary in severity and lexical nature depending on the situation (Waseem et al., 2017). 9 of the rich metadata and parent tweet text embeddings present in tweet threads, and have the potential to achieve significantly boosted classification performance compared to that of models built on text embeddings of the potentially harassing tweet alone (Mishra et al., 2019). For the purpose of building the eventual tool to aid journalists in the field, we could alternatively address the relatively small size of our manuallylabelled datasets for training deep learning classifiers, by augmenting them against the large, popular corpora already in existence. We could investigate whether thi"
2020.alw-1.2,N16-2013,0,0.16219,"ing the need for a community-specific approach to studying hate speech. (Salem et al., 2016) used content from self-identified hate communities, instead of keywords from hand-coded speech or manually coded hate speech terms, as training data for their work on hate speech detection with some success. In (Nobata et al., 2016), researchers studied abusive language in online user comments on news and finance forums using linguistic, syntactic, and distributed semantic features as well as lexicon-based features. Their dataset has been used to benchmark performance in hate speech detection, as has (Waseem and Hovy, 2016). In (Kshirsagar et al., 2018), researchers developed deep learning models for hate speech detection on Twitter, using transformed word embeddings to classify hate speech on three public datasets. 8 Researchers in journalism have also used more qualitative methods to study abusive and hateful speech towards journalists. For example, UT Austin’s School of Journalism published results from in-depth interviews with 75 female journalists describing how rampant online sexual harassment disrupts their ability to do their jobs (Chen et al., 2018). The Committee to Protect Journalists reported similar"
2020.alw-1.2,W06-2209,0,0.0613185,"cipants, it would also provide an ongoing source of test data with which we could refine and improve our classifiers in much closer to real-time. By leveraging the methods presented in (Wulczyn et al., 2017), moreover, we also believe we could augment and improve the classifiers built from our hand-labeled data using a combination of machine learning and crowdsourcing. We are in general investigating ways to overcome the inherent shortcomings of manual expert annotation, while retaining its significant benefits; for example, augmenting our data annotation tool with active learning annotation (Vlachos, 2006), so that participants only need to annotate the most unclear instances of hateful/harassing/neutral speech. In regards to model-building, we are exploring ways we can take advantage of the contextual thread annotation scheme present in our annotation platform. Specifically, we have investigated methods using LSTMs (Huang et al., 2016), and are presently investigating graph attention networks (Veliˇckovi´c et al., 2017); these architectures and others like them could allow us to take advantage 11 Conclusion This work has focused on outlining a novel and generalizable methodology for generating"
2020.alw-1.2,W12-2103,1,0.572024,"ly imbalanced corpora, with more “hate” than “non-hate” tweets for each language. Deep learning models trained on each language corpus achieved best macro-F1 scores over .80 for English, French and Greek but somewhat lower for Spanish and German. Other work has addressed the more general problem of automatic identification of hate speech and abusive language online. In (Waseem, 2016), researchers found that crowd-sourced annotations performed poorly. This indicates the importance of expert annotators, which (Blackwell et al., 2017) situates specifically in terms of classifying harassment. In (Warner and Hirschberg, 2012), researchers using data from Yahoo and the American Jewish Congress found that anti-Semitic hate speech differed linguistically from speech that targeted other religious or ethnic groups, highlighting the need for a community-specific approach to studying hate speech. (Salem et al., 2016) used content from self-identified hate communities, instead of keywords from hand-coded speech or manually coded hate speech terms, as training data for their work on hate speech detection with some success. In (Nobata et al., 2016), researchers studied abusive language in online user comments on news and fi"
2020.louhi-1.16,C18-1126,0,0.14695,"Missing"
2020.louhi-1.16,W15-1201,0,0.444588,"cly available datasets for identifying linguistic markers of mental illness. To date, unique linguistic markers and patterns have been identified for several psychiatric conditions, such as major depressive disorder (MDD)(De Choudhury et al., 2013; Vedula and Parthasarathy, 2017), general anxiety disorder (GAD) (Shen and Rudzicz, 2017), bipolar disorder (BD) (Huang et al., 2017; Sekuli´c et al., 2018), eating disorders (ED) (Mohammadi et al., 2019; Naderi et al., 2019), schizophrenia (SZ) (Mitchell et al., 2015; Birnbaum et al., 2017; Zomick et al., 2019), obsessive compulsive disorder (OCD) (Coppersmith et al., 2015a), posttraumatic stress disorder (PTSD) (Coppersmith et al., 2014), as well as others (Coppersmith et al., 2015a). Linguistic findings have spanned various domains of language, including the use of pronouns, emotion words, tentative language, tangentiality, punctuation, and content analysis. The majority of these models have been developed to successfully predict if a given user has self-disclosed receiving a diagnosis for a psychiatric condition and is currently suffering with mental illness. However, much of this previous research on social media and mental health has focused on comparing u"
2020.louhi-1.16,W15-1204,0,0.0504889,"Missing"
2020.louhi-1.16,N19-1419,0,0.0193072,"the model parameters. We propose to use pre-trained BERT model from Hugging Face (Wolf et al., 2019) to encode every post; 151 Figure 3: LIWC analysis across 8 diagnostic groups and control group for anxiety (ANX, in blue) and singular personal pronoun usage (I, in orange). Only significant results are displayed in this table. we then averaged representation of all positions as a pooling result to build an attention-based classifier (Bahdanau et al., 2014; Sutskever et al., 2014) over all the post-level representations for a single user3 . This resembles the settings of many “probing tasks” (Hewitt and Manning, 2019) used to investigate whether BERT embeddings encode useful linguistic information about a user’s mental health condition. 5.2 tion x that can be utilized for relevant document selection, so we now use a trainable attention head to calculate the retrieval probability p(z). Thus the probability of a user having condition y can be factorized as: X p(y) = p(y|z)p(z) (1) z∈Z where REALM-like Models Guu et al. (2020) propose Retrieval-Augmented Language Model pretraining to augment a pretrained LM as a textual knowledge retriever. To tackle the scalability issue of retrieving over large corpora, a r"
2020.louhi-1.16,W15-1202,0,0.177479,"istics, researchers have been able to leverage the widespread use of social media to analyze large, publicly available datasets for identifying linguistic markers of mental illness. To date, unique linguistic markers and patterns have been identified for several psychiatric conditions, such as major depressive disorder (MDD)(De Choudhury et al., 2013; Vedula and Parthasarathy, 2017), general anxiety disorder (GAD) (Shen and Rudzicz, 2017), bipolar disorder (BD) (Huang et al., 2017; Sekuli´c et al., 2018), eating disorders (ED) (Mohammadi et al., 2019; Naderi et al., 2019), schizophrenia (SZ) (Mitchell et al., 2015; Birnbaum et al., 2017; Zomick et al., 2019), obsessive compulsive disorder (OCD) (Coppersmith et al., 2015a), posttraumatic stress disorder (PTSD) (Coppersmith et al., 2014), as well as others (Coppersmith et al., 2015a). Linguistic findings have spanned various domains of language, including the use of pronouns, emotion words, tentative language, tangentiality, punctuation, and content analysis. The majority of these models have been developed to successfully predict if a given user has self-disclosed receiving a diagnosis for a psychiatric condition and is currently suffering with mental i"
2020.louhi-1.16,D19-5542,0,0.100653,"(F(8, 24442) = 438.738, p&lt;.0001). Figure 150 5 Methods for Classification Experiments Having identified significant differences in linguistic features between the disorders, and between the control users, we next explore several classification methods for automatically identifying different mental health conditions. Previous efforts to identify such conditions in Reddit have primarily employed simple logistic regression or SVMs using a bag-of-words representation or LIWC features, and some have explored RNN/CNN based text encoder models (Coppersmith et al., 2014, 2015a,b; Cohan et al., 2018b; Sekulic and Strube, 2019). However, recent advances in contextual representations like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which have enabled substantial performance increases across many NLP task, have not been well integrated into mental health identification tasks. This is due to model size and scalability issues of the large number of posts generated by a user. In this work we focus on methods utilizing contextual representations for mental health identification and compare their effectiveness to a logistic regression baseline trained on LIWC features. We present an attention-based model usi"
2020.louhi-1.16,W17-3107,0,0.169551,"Missing"
2020.louhi-1.16,N18-1202,0,0.0492123,"identified significant differences in linguistic features between the disorders, and between the control users, we next explore several classification methods for automatically identifying different mental health conditions. Previous efforts to identify such conditions in Reddit have primarily employed simple logistic regression or SVMs using a bag-of-words representation or LIWC features, and some have explored RNN/CNN based text encoder models (Coppersmith et al., 2014, 2015a,b; Cohan et al., 2018b; Sekulic and Strube, 2019). However, recent advances in contextual representations like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which have enabled substantial performance increases across many NLP task, have not been well integrated into mental health identification tasks. This is due to model size and scalability issues of the large number of posts generated by a user. In this work we focus on methods utilizing contextual representations for mental health identification and compare their effectiveness to a logistic regression baseline trained on LIWC features. We present an attention-based model using BERT representations as input features, as well as a REALMlike model (Guu et al., 202"
2020.louhi-1.16,W18-6211,0,0.0242342,"Missing"
2020.louhi-1.16,W19-3009,1,0.946092,"the widespread use of social media to analyze large, publicly available datasets for identifying linguistic markers of mental illness. To date, unique linguistic markers and patterns have been identified for several psychiatric conditions, such as major depressive disorder (MDD)(De Choudhury et al., 2013; Vedula and Parthasarathy, 2017), general anxiety disorder (GAD) (Shen and Rudzicz, 2017), bipolar disorder (BD) (Huang et al., 2017; Sekuli´c et al., 2018), eating disorders (ED) (Mohammadi et al., 2019; Naderi et al., 2019), schizophrenia (SZ) (Mitchell et al., 2015; Birnbaum et al., 2017; Zomick et al., 2019), obsessive compulsive disorder (OCD) (Coppersmith et al., 2015a), posttraumatic stress disorder (PTSD) (Coppersmith et al., 2014), as well as others (Coppersmith et al., 2015a). Linguistic findings have spanned various domains of language, including the use of pronouns, emotion words, tentative language, tangentiality, punctuation, and content analysis. The majority of these models have been developed to successfully predict if a given user has self-disclosed receiving a diagnosis for a psychiatric condition and is currently suffering with mental illness. However, much of this previous resear"
2020.tacl-1.14,C08-1006,0,0.125339,"Missing"
2020.tacl-1.14,P13-1025,0,0.0637052,"Missing"
2020.tacl-1.14,P12-2034,0,0.072165,"Missing"
2020.tacl-1.14,N18-1176,1,0.659629,"ng the number of neighbors within a certain cosine distance in the TD-IDF space. This result is robust against various threshold of cosine distance (0.1–0.9 with 0.1 as the step size). The difference was not due to response length, as we found no correlation between creativity and response length (spearman, ρ = 0.007, p > 0.05). However, we did find that judgments were not influenced by whether the response was creative or not. Perhaps when people lie they try to tell a compelling story, which results in a more creative response regardless of length. Uncertainty Prosody Our previous research (Levitan et al., 2018b) has found that linguistic markers of certainty and uncertainty are significant indicators of deception. So we measured certainty and uncertainty in two ways: words from LIWC’s ‘‘certainty’’ category as linguistic markers of certainty (e.g., always, never) and hedge words and phrases (e.g., possible, sort of) (Ulinski et al., 2018) as indicators of uncertainty. As shown in Table 5, there was a match between rater trust and true responses for hedge words and phrases. In the CXD corpus, lies included hedge phrases more often than true responses did, and we found that listeners did mistrust res"
2020.tacl-1.14,P11-1032,0,0.117118,"Missing"
2020.tacl-1.14,D15-1281,0,0.0389819,"Missing"
2020.tacl-1.14,S17-1013,1,0.845065,"to ask follow-up questions to help determine whether the interviewee was lying or telling the truth about each question. Participants were financially compensated for both successful deception and successful deception detection. Table 1 provides sample responses to one of the questions. The recorded interviews were orthographically transcribed using Amazon Mechanical Turk (AMT) crowd-sourcing and the transcripts were force-aligned with the audio recordings using the Kaldi Speech Recognition Toolkit (Povey et al., 2011). The interviews were segmented using a question identification classifier (Maredia et al., 2017). All interviewee turns were automatically identified using the question identification system and subsequently hand-corrected. The corpus was segmented into: 1) question responses: The single interviewee turn directly following the question; 2) question chunks: All interviewee turns in (1) plus answers to subsequent follow-up questions. We used the single turn question response segmentation for our deception perception study, so as not to influence raters’ responses with interviewers’ follow-up questions. 4 LieCatcher Using the data described in Section 3, we created a lie detection game call"
2020.tacl-1.14,D15-1133,0,0.0387208,"Missing"
2020.tacl-1.14,P09-2078,0,0.0941715,"Missing"
2020.tacl-1.14,W18-1301,1,0.838063,"ever, we did find that judgments were not influenced by whether the response was creative or not. Perhaps when people lie they try to tell a compelling story, which results in a more creative response regardless of length. Uncertainty Prosody Our previous research (Levitan et al., 2018b) has found that linguistic markers of certainty and uncertainty are significant indicators of deception. So we measured certainty and uncertainty in two ways: words from LIWC’s ‘‘certainty’’ category as linguistic markers of certainty (e.g., always, never) and hedge words and phrases (e.g., possible, sort of) (Ulinski et al., 2018) as indicators of uncertainty. As shown in Table 5, there was a match between rater trust and true responses for hedge words and phrases. In the CXD corpus, lies included hedge phrases more often than true responses did, and we found that listeners did mistrust responses containing hedge phrases. However, although linguistic markers of certainty in the corpus (e.g., ‘‘always,’’ ‘‘never’’—which are the opposite of hedge words) were indicators of truth, raters failed to perceive this. Previous studies have shown that pitch maximum and intensity maximum are significant indicators of deception (Le"
2021.clpsych-1.14,W17-1612,0,0.0271889,"ospitalized. In order to train classification models, we first need to select negative samples as a control group for our experiments. We describe our methods of pairing negative samples in subsection 4.1. We experiment with three set of features: unigram, bigram2 and LIWC (Pennebaker et al., 2007, 2015) features. We perform hyper-parameter grid search to optimize performance. For all features we use the Naive-Bayes classifier, as it has been found to perform well on small datasets (NG and Jordan, 2002). We pre-process the text by lower-casing all input posts and, following the guidelines of (Benton et al., 2017), we de-identify posts by anonymizing URLs and replacing usernames with randomly generated strings. 4.1 Pairing Negative Samples To form a challenging prediction task, we compile negative samples for classification by selecting control users from the same candidate pool that the target hospitalization group was selected from. The control users are those who do not have associated hospitalization time spans, but did have similar media blackout periods (described in subsection 3.3). Classification Table 2 shows mean F-1 scores from crossvalidation on both user-level and post-level tasks. In all"
2021.clpsych-1.14,C18-1126,0,0.0469176,"Missing"
2021.clpsych-1.14,W18-2501,0,0.0363645,"Missing"
2021.clpsych-1.14,W17-3107,0,0.0260171,"onal linguistics techniques applied to social media data for predicting and detecting mental illness across a broad range of psychiatric conditions (Guntuku et al., 2017; Wongkoblap et al., 2017). To date, linguistic indicators of psychopathology have been identified 3.1 Candidate Collection for a wide range of psychiatric conditions (Zomick We begin data collection by identifying candidate et al., 2019; Coppersmith et al., 2015; Birnbaum Reddit users who may be at risk for a psychiatric et al., 2017; Huang et al., 2017; De Choudhury hospitalization. We focus on two user groups: et al., 2013; Shen and Rudzicz, 2017). Recent those that self-identify with a psychiatric disorder, work has also looked at detecting and predicting suicidality using linguistic features from social me- and those that self-identify with suicidal ideation or dia posts (Du et al., 2018; Coppersmith et al., 2018; attempted suicide. To identify such users, we leverage subreddits, or forums on Reddit dedicated to Zirikly et al., 2019). While the majority of past research has com- specific topics. Following Shing et al. (2018), we collect posts from the r/SuicideWatch (SW) subpared specific psychiatric conditions with healthy control g"
2021.clpsych-1.14,2020.louhi-1.16,1,0.901927,"pted suicide. To identify such users, we leverage subreddits, or forums on Reddit dedicated to Zirikly et al., 2019). While the majority of past research has com- specific topics. Following Shing et al. (2018), we collect posts from the r/SuicideWatch (SW) subpared specific psychiatric conditions with healthy control groups, more recent work has begun ana- reddit, and following (Cohan et al., 2018b; Jiang lyzing and identifying unique differences and dis- et al., 2020) we collect posts from subreddits related to 8 different mental health conditions: obcriminators among psychiatric conditions (Jiang et al., 2020; Cohan et al., 2018a; Coppersmith et al., sessive compulsive disorder (OCD), schizophrenia (SZ), borderline personality disorder (BPD), post2015). As this area progresses, we have begun to traumatic stress disorder (PTSD), eating disorder investigate whether this technology can be used (ED), major depression dis-order (MDD), general beyond detection of mental illness for detecting 1 severity of symptomatology and prediction of acute This study received IRB approval and all human subjects psychiatric episodes that result in hospitalization. protection guidelines were followed. 117 anxiety diso"
2021.clpsych-1.14,W18-0618,0,0.0473074,"Missing"
2021.clpsych-1.14,W18-0603,0,0.0363843,"et al., 2017; Huang et al., 2017; De Choudhury hospitalization. We focus on two user groups: et al., 2013; Shen and Rudzicz, 2017). Recent those that self-identify with a psychiatric disorder, work has also looked at detecting and predicting suicidality using linguistic features from social me- and those that self-identify with suicidal ideation or dia posts (Du et al., 2018; Coppersmith et al., 2018; attempted suicide. To identify such users, we leverage subreddits, or forums on Reddit dedicated to Zirikly et al., 2019). While the majority of past research has com- specific topics. Following Shing et al. (2018), we collect posts from the r/SuicideWatch (SW) subpared specific psychiatric conditions with healthy control groups, more recent work has begun ana- reddit, and following (Cohan et al., 2018b; Jiang lyzing and identifying unique differences and dis- et al., 2020) we collect posts from subreddits related to 8 different mental health conditions: obcriminators among psychiatric conditions (Jiang et al., 2020; Cohan et al., 2018a; Coppersmith et al., sessive compulsive disorder (OCD), schizophrenia (SZ), borderline personality disorder (BPD), post2015). As this area progresses, we have begun to t"
2021.clpsych-1.14,W19-3009,1,0.865915,"Missing"
2021.eacl-main.23,W09-0612,0,0.0447917,"rall language use here. Importantly, there are correlations between lexical entrainment and interesting aspects of the conversation. These include task success for both speaker pairs (Reitter and Moore, 2007; Nenkova et al., 2008) and groups (Gonzales et al., 2010; Friedberg et al., 2012), conversation flow and perceived naturalness (Nenkova et al., 2008), as well as power differences between the speakers (Danescu-Niculescu-Mizil et al., 2011). This suggests practical applications and has led to the development of entraining natural language generators in Dutch (De Jong et al., 2008), German (Buschmeier et al., 2009), and American English and European Portuguese (Lopes et al., 2015), among others. To the best of our knowledge, there has not been any systematic research on lexical entrainment in Hebrew or any other Semitic Language. Previous studies analyzing lexical choice in Semitic Languages focus on borrowing and code-switching, for instance between Arabic and English (Abu-Melhim et al., 2016) and Arabic and Hebrew (Hawker, 2018). Given the important social role of entrainment and its potential applications, our study provides an important contribution by presenting the first analysis of lexical entrai"
2021.eacl-main.23,P08-2043,1,0.783409,"s been studied for several types of lexical choices from specific sets of words – such as referring expressions (Brennan and Clark, 1996), high-frequency words and taskrelated words (Rahimi et al., 2017), as well as hedge and cue phrases (Levitan et al., 2018) – to the wider linguistic style (Niederhoffer and Pennebaker, 2002). This motivates us to consider both specific word sets and overall language use here. Importantly, there are correlations between lexical entrainment and interesting aspects of the conversation. These include task success for both speaker pairs (Reitter and Moore, 2007; Nenkova et al., 2008) and groups (Gonzales et al., 2010; Friedberg et al., 2012), conversation flow and perceived naturalness (Nenkova et al., 2008), as well as power differences between the speakers (Danescu-Niculescu-Mizil et al., 2011). This suggests practical applications and has led to the development of entraining natural language generators in Dutch (De Jong et al., 2008), German (Buschmeier et al., 2009), and American English and European Portuguese (Lopes et al., 2015), among others. To the best of our knowledge, there has not been any systematic research on lexical entrainment in Hebrew or any other Semi"
2021.eacl-main.23,P07-1102,0,0.0858351,"). Lexical entrainment has been studied for several types of lexical choices from specific sets of words – such as referring expressions (Brennan and Clark, 1996), high-frequency words and taskrelated words (Rahimi et al., 2017), as well as hedge and cue phrases (Levitan et al., 2018) – to the wider linguistic style (Niederhoffer and Pennebaker, 2002). This motivates us to consider both specific word sets and overall language use here. Importantly, there are correlations between lexical entrainment and interesting aspects of the conversation. These include task success for both speaker pairs (Reitter and Moore, 2007; Nenkova et al., 2008) and groups (Gonzales et al., 2010; Friedberg et al., 2012), conversation flow and perceived naturalness (Nenkova et al., 2008), as well as power differences between the speakers (Danescu-Niculescu-Mizil et al., 2011). This suggests practical applications and has led to the development of entraining natural language generators in Dutch (De Jong et al., 2008), German (Buschmeier et al., 2009), and American English and European Portuguese (Lopes et al., 2015), among others. To the best of our knowledge, there has not been any systematic research on lexical entrainment in H"
2021.eacl-main.23,N18-2048,1,0.758536,"for S1 to predict all utterances of S2 , computed with SRILM. Low perplexity indicates that the model for S1 is a good representation of the utterances of S2 . In this case, the phrases used by S2 are essentially a subset of those used by S1 . We interpret this as entrainment of S1 towards S2 as it signals that S1 incorporated S2 ’s phrases into their own. Conversely, high perplexity indicates a lack of entrainment. This is why we use negated perplexity for sim2 . Note that this measure is asymmetric. For a symmetric version, we simply add the asymmetric values for both directions, following Weise and Levitan (2018). To determine whether significant entrainment is present, we follow Levitan et al. (2012). Each similarity value simi (S1 , S2 ) for a speaker S1 with their partner S2 is compared with the weighted average similarity of S1 with non-partners, using paired Student’s t-tests. Non-partners must have the same gender as S2 and their partners must have the same gender as S1 . For similarity per task, nonpartners must also be talking about the same map and have the same role as S2 . Non-partners are weighted by how closely their language model’s entropy, computed using SRILM, matches that of the actu"
2021.emnlp-main.364,P06-4018,0,0.00747834,"issell, 2009) to humor –– for COVID-related posts, this opposite and the Vader sentiment tool (Hutto and Gilbert, turns out to be almost exclusively sad posts about 2014); we also analyzed the complexity of posts, people’s deaths and illness. Though sad posts are and the use of emojis as a social media specific feacertainly non-humorous, they don’t represent the ture. All word-level features were normalized by full scope of non-humorous expression. Thus, we the total number of words after using the Twitterneed a new technique to retrieve a broader range of aware tokenizer of the NLTK Toolkit (Bird, 2006). non-humorous posts, which should include neutral We calculated Pearson’s correlation between the posts, sad posts, as well as other emotional posts features and the HS of posts, and all reported rethat do not evoke a humorous reaction. sults are significant with a p < 0.05. We instead define our Non-Humor Score (NS) as LIWC The top categories that positively correposts whose reaction distributions have the lowest late with HS include singular first-person pronouns, divergence from the standard Facebook post dis- total pronouns, anger words, negative emotional tribution. Given the fact that t"
2021.emnlp-main.364,P19-1394,0,0.0151208,"halcea and Pul- chose COVID-19 as the topic for our dataset. There has been extensive discussion on the pandemic with man (2007) extended the work to longer humorous articles and news articles. Yang et al. (2015) identi- a wide range of audiences, so this topic prevents us from biasing our posts and labels toward a specific fied the semantic structures of humor by studying demographic group. the differences between puns and news. Chen and Soo (2018) built deep learning humor detection 3.1 Data Collection and Cleaning models on four datasets with jokes as humor data and news as non-humor data. Blinov et al. (2019) We collected our Facebook posts from CrowdTangle by searching COVID-related keywords (“covidcollected jokes in Russian, combining with forum posts that have low similarity to the jokes as non- 19, coronavirus, corona, covid 19, sars-cov-2, humorous samples. More recently, Annamoradne- covid, sars cov 2”), and downloading posts from January 20th, 2020 until March 18th, 2021. We set jad and Zoghi (2020) combined Reddit jokes with the language as English and post type as Status on news headlines and used a BERT-based model to CrowdTangle, in order to ensure that we retrieve classify these two se"
2021.emnlp-main.364,2020.aacl-main.31,0,0.0419434,"jokes and news, chose to focus on COVID-19 because of its univerwhich both have natural labels on humor and can sality as a phenomenon that affects all Facebook be scraped automatically. This large stylistic dif- users. CHoRaL, however, can be easily adapted to ference makes detecting humor easier –– but it is other topics, making it the most extendable humor 4429 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4429–4435 c November 7–11, 2021. 2021 Association for Computational Linguistics data collection framework yet. 2 amined in internet memes (Chauhan et al., 2020; Sharma et al., 2020). Related Work 3 CHoRaL Framework and Dataset Most corpora for textual humor detection use onIn this section, we introduce our Facebook post line joke compilations as humor data and more serious sources, like news or proverbs, as non- collection process, as well as our algorithm to assign humor and non-humor scores to the posts. Alhumor data. Mihalcea and Strapparava (2005) built a model to distinguish one-liners from short sen- though CHoRaL can be applied to any topic, we tences such as news titles, and Mihalcea and Pul- chose COVID-19 as the topic for our dataset. Ther"
2021.emnlp-main.364,N18-2018,0,0.0157781,"halcea and Strapparava (2005) built a model to distinguish one-liners from short sen- though CHoRaL can be applied to any topic, we tences such as news titles, and Mihalcea and Pul- chose COVID-19 as the topic for our dataset. There has been extensive discussion on the pandemic with man (2007) extended the work to longer humorous articles and news articles. Yang et al. (2015) identi- a wide range of audiences, so this topic prevents us from biasing our posts and labels toward a specific fied the semantic structures of humor by studying demographic group. the differences between puns and news. Chen and Soo (2018) built deep learning humor detection 3.1 Data Collection and Cleaning models on four datasets with jokes as humor data and news as non-humor data. Blinov et al. (2019) We collected our Facebook posts from CrowdTangle by searching COVID-related keywords (“covidcollected jokes in Russian, combining with forum posts that have low similarity to the jokes as non- 19, coronavirus, corona, covid 19, sars-cov-2, humorous samples. More recently, Annamoradne- covid, sars cov 2”), and downloading posts from January 20th, 2020 until March 18th, 2021. We set jad and Zoghi (2020) combined Reddit jokes with"
2021.emnlp-main.364,N19-1423,0,0.0276236,"Missing"
2021.emnlp-main.364,D19-1211,0,0.0349916,"Missing"
2021.emnlp-main.364,N19-1012,0,0.0155605,"ap- fields and some remaining non-English posts. We tion contest, and, similarly, Potash et al. (2017) also removed posts with rendered links to minimize the influence of non-text elements on the viewers’ obtained humorous tweets from the official website of a TV show. Chen and Lee (2017); Hasan et al. perception of humor. For posts with non-rendered links, we replaced the links with a special token. (2019) generated humor labels using the audience laughter marker in the transcripts of TED talks. This replacement allowed more posts to pass our final filter, which was to cap post length at 500 Hossain et al. (2019, 2020) asked annotators to edit characters to suit the max token length of BERTnews headlines to make them funny. There are also based models. About 785K posts remained in our some hand-annotated humor datasets (Chiruzzo corpus after this local filtering round. et al., 2020; Zhang and Liu, 2014). However, these methods either need extensive human annotation 3.2 Defining the Humor Score (HS) or suffer from low label accuracy. The line of work most relevant to our paper is We used Facebook’s built-in reactions feature to the rJokes dataset (Weller and Seppi, 2019, 2020), determine how funny a p"
2021.emnlp-main.364,2020.semeval-1.98,0,0.0702786,"Missing"
2021.emnlp-main.364,2021.ccl-1.108,0,0.0349658,"Missing"
2021.emnlp-main.364,S17-2004,0,0.0175408,"model to CrowdTangle, in order to ensure that we retrieve classify these two sets of data. For other forms of naturally labeled humor- text-only posts without images or videos attached. ous texts, Reyes et al. (2012) obtained humor- This initial retrieval surfaced 2 million posts. We further cleaned these 2 million downloaded ous tweets with the hashtag “humor” and nonhumorous tweets from other hashtags. Radev et al. posts locally. We removed posts with duplicate text (2016) obtained humor scores from a cartoon cap- fields and some remaining non-English posts. We tion contest, and, similarly, Potash et al. (2017) also removed posts with rendered links to minimize the influence of non-text elements on the viewers’ obtained humorous tweets from the official website of a TV show. Chen and Lee (2017); Hasan et al. perception of humor. For posts with non-rendered links, we replaced the links with a special token. (2019) generated humor labels using the audience laughter marker in the transcripts of TED talks. This replacement allowed more posts to pass our final filter, which was to cap post length at 500 Hossain et al. (2019, 2020) asked annotators to edit characters to suit the max token length of BERTne"
2021.emnlp-main.364,W06-1625,0,0.0948917,"Missing"
2021.emnlp-main.364,L16-1076,0,0.105961,"Missing"
2021.emnlp-main.364,H05-1067,0,0.0743479,"021 Conference on Empirical Methods in Natural Language Processing, pages 4429–4435 c November 7–11, 2021. 2021 Association for Computational Linguistics data collection framework yet. 2 amined in internet memes (Chauhan et al., 2020; Sharma et al., 2020). Related Work 3 CHoRaL Framework and Dataset Most corpora for textual humor detection use onIn this section, we introduce our Facebook post line joke compilations as humor data and more serious sources, like news or proverbs, as non- collection process, as well as our algorithm to assign humor and non-humor scores to the posts. Alhumor data. Mihalcea and Strapparava (2005) built a model to distinguish one-liners from short sen- though CHoRaL can be applied to any topic, we tences such as news titles, and Mihalcea and Pul- chose COVID-19 as the topic for our dataset. There has been extensive discussion on the pandemic with man (2007) extended the work to longer humorous articles and news articles. Yang et al. (2015) identi- a wide range of audiences, so this topic prevents us from biasing our posts and labels toward a specific fied the semantic structures of humor by studying demographic group. the differences between puns and news. Chen and Soo (2018) built dee"
2021.emnlp-main.364,2020.semeval-1.99,0,0.0902251,"Missing"
2021.emnlp-main.364,2020.emnlp-demos.2,0,0.0481343,"Missing"
2021.emnlp-main.364,2020.eamt-1.7,0,0.0309905,"Missing"
2021.emnlp-main.364,D19-1372,0,0.0759707,"aL enables the development of large-scale humor detection models on any topic and opens a new path to the study of humor on social media. Figure 1: User reactions to a humorous Facebook post (top) and a non-humorous post (bottom). far from most real-world scenarios where humorous and non-humorous texts come from the same domain. Another technique collects social media posts by humor- and non-humor-related hashtags, but this method suffers from large data noise and low labeling accuracy (Zhang and Liu, 2014). Finally, there have been studies to use the number of Reddit upvotes as humor labels (Weller and Seppi, 2019, 2020). Though this technique sources data from the same domain, that domain is too limited in scope: all the data comes from one single subreddit. This specificity means that the data represents only the humor perception of a particular group of Reddit users, dedicated to producing witty jokes. To address these problems of specificity and 1 Introduction domain discrepancy in humorous data collection, we propose CHoRaL, a framework for Collecting Humor is ubiquitous –– it forms a crucial part of Humor Reaction Labels. CHoRaL generates perpeople’s lives both online and off. Automatically ceive"
2021.emnlp-main.364,2020.lrec-1.753,0,0.0930605,"Missing"
2021.emnlp-main.364,D15-1284,0,0.0165066,"n, we introduce our Facebook post line joke compilations as humor data and more serious sources, like news or proverbs, as non- collection process, as well as our algorithm to assign humor and non-humor scores to the posts. Alhumor data. Mihalcea and Strapparava (2005) built a model to distinguish one-liners from short sen- though CHoRaL can be applied to any topic, we tences such as news titles, and Mihalcea and Pul- chose COVID-19 as the topic for our dataset. There has been extensive discussion on the pandemic with man (2007) extended the work to longer humorous articles and news articles. Yang et al. (2015) identi- a wide range of audiences, so this topic prevents us from biasing our posts and labels toward a specific fied the semantic structures of humor by studying demographic group. the differences between puns and news. Chen and Soo (2018) built deep learning humor detection 3.1 Data Collection and Cleaning models on four datasets with jokes as humor data and news as non-humor data. Blinov et al. (2019) We collected our Facebook posts from CrowdTangle by searching COVID-related keywords (“covidcollected jokes in Russian, combining with forum posts that have low similarity to the jokes as non"
A00-2029,P98-1122,0,0.19038,"e morning&quot;). The current paper looks at prosody as one possible predictor of ASR performance. ASR performance is known to vary based upon speaking style (Weintraub et al., 1996), speaker gender and age, native versus non-native speaker status, and, in general, the deviation of new speech from the training data. Some of this variation is linked to prosody, as prosodic differences have been found to characterize differences in speaking style (Blaauw, 1992) and idiosyncratic differences (Kraayeveld, 1997). Several other studies (Wade et al., 1992; Oviatt et al., 1996; Swerts and Ostendorf, 1997; Levow, 1998; Bell and Gustafson, 1999) report that hyperarticulated speech, characterized by careful enunciation, slowed speaking rate, and increase in pitch and loudness, often occurs when users in human-machine interactions try to correct system errors. Still others have shown that such speech also decreases recognition performance (Soltau and Waibel, 1998). Prosodic features have also been shown to be effective in ranking recognition hypotheses, as a post-processing filter to score ASR hypotheses (Hirschberg, 1991; Veilleux, 1994; Hirose, 1997). In this paper we present results of empirical studies te"
A00-2029,P99-1024,0,0.0840224,"Missing"
A00-2029,P99-1040,1,0.858751,"Missing"
A00-2029,C98-1117,0,\N,Missing
C16-1043,J09-3007,0,0.0783328,"Missing"
C16-1043,W06-2920,0,0.0431974,"with nonlexical models. We discuss the annotation experiments and results in Section 6. We conclude in Section 7. 2 Related Work There have been a number of investigations into multilingual dependency parsing. For example, Nivre et al. (2007b) presents detailed results for 11 languages using the arc-eager deterministic parsing algorithm included in MaltParser. However, results are reported only for the parser trained on the full training set and would not generalize to situations where training data is limited. Likewise, the 2006 and 2007 CoNLL shared tasks of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) relied on the existence of ample training data for the languages being investigated. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource lang"
C16-1043,D15-1040,0,0.0258905,"esents detailed results for 11 languages using the arc-eager deterministic parsing algorithm included in MaltParser. However, results are reported only for the parser trained on the full training set and would not generalize to situations where training data is limited. Likewise, the 2006 and 2007 CoNLL shared tasks of multilingual dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007a) relied on the existence of ample training data for the languages being investigated. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages"
C16-1043,N10-1115,0,0.0139292,"arser will reduce their workload by letting them correct errors in a dependency structure rather than starting from scratch. This method of syntactic documentation does not limit the field linguist to a particular syntactic theory. We chose to use the universal labels and analyses in our corpus, but WELT users will have complete control over assignment of heads and choice of dependency labels. The only requirement is that they are consistent across sentences. In future work, we will experiment with other parsers, such as TurboParser (Martins et al., 2010), Mate (Bohnet, 2010), and Easy-First (Goldberg and Elhadad, 2010). Furthermore, we will continue to investigate methods of re-using existing parsers and dependency annotations with new languages (see Section 5); specifically, we will investigate more effective methods of adapting existing parsers to other languages. For example, we will investigate how to combine a non-lexical model with a lexical model obtained from a small number of target language sentences. We will also investigate ways for linguists to directly specify syntactic properties that can be used by the parser, similar to the way FLEx converts morphological properties specified by users into"
C16-1043,P15-1119,0,0.0142221,"ted. Our work differs in that we are interested in the performance of a dependency parser trained on very little data. Duong et al. (2015) approach dependency parsing for a low-resource language as a domain adaptation task; a treebank in a high-resource language is considered out-of-domain, and a much smaller treebank in a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages. They focus on lexical features, which are not directly transferable among languages, and propose the use of distributed feature representations instead of discrete lexical features. Lacroix et al. (2016) describe a method for transferring dependency parsers across languages by projecting annotations across word alignments and learning from the partially annotated data. However, both of these methods rely on large amounts of (unannotated) data in the target language in orde"
C16-1043,N16-1121,0,0.0535688,"a low-resource language is considered in-domain. They jointly train a neural network dependency parser to model the syntax of both the high-resource and the low-resource language. In this paper, we focus on the alternate approach of training directly on small amounts of data. Guo et al. (2015) also investigate inducing dependency parsers for low-resource languages using training data from high-resource languages. They focus on lexical features, which are not directly transferable among languages, and propose the use of distributed feature representations instead of discrete lexical features. Lacroix et al. (2016) describe a method for transferring dependency parsers across languages by projecting annotations across word alignments and learning from the partially annotated data. However, both of these methods rely on large amounts of (unannotated) data in the target language in order to learn the word embeddings and alignments. It is unclear how well these approaches would work in the context of an endangered language where large amounts of unannotated text will not be available. Our work also differs from the above because our goal is to incorporate a parser into tools for field linguists studying end"
C16-1043,D10-1004,0,0.0179462,"tax into language documentation. The incrementally trained parser will reduce their workload by letting them correct errors in a dependency structure rather than starting from scratch. This method of syntactic documentation does not limit the field linguist to a particular syntactic theory. We chose to use the universal labels and analyses in our corpus, but WELT users will have complete control over assignment of heads and choice of dependency labels. The only requirement is that they are consistent across sentences. In future work, we will experiment with other parsers, such as TurboParser (Martins et al., 2010), Mate (Bohnet, 2010), and Easy-First (Goldberg and Elhadad, 2010). Furthermore, we will continue to investigate methods of re-using existing parsers and dependency annotations with new languages (see Section 5); specifically, we will investigate more effective methods of adapting existing parsers to other languages. For example, we will investigate how to combine a non-lexical model with a lexical model obtained from a small number of target language sentences. We will also investigate ways for linguists to directly specify syntactic properties that can be used by the parser, similar to the w"
C16-1043,H05-1066,0,0.241287,"Missing"
C16-1043,W06-2932,0,0.330188,"per makes three contributions. First, we introduce a new corpus of English, Spanish, German, and Egyptian Arabic descriptions of spatial relations and motion events, which we have annotated with dependency structures and other linguistic information. We focused on spatial relations and motion because one of the other functions of WELT will be to assist field linguists with elicitation of spatial language and documentation of spatial and motion semantics. We are making the corpus available to the public. Second, we compare the performance of two existing dependency parsing packages, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006), using incrementally increasing amounts of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 440 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 440–449, Osaka, Japan, December 11-17 2016. this training data. Third, we show that using a parser trained on small amounts of data can assist with dependency annotation. In Section 2, we discuss related work. In Section 3, we describe the new publicly"
C16-1043,nivre-etal-2006-maltparser,0,0.342647,"e introduce a new corpus of English, Spanish, German, and Egyptian Arabic descriptions of spatial relations and motion events, which we have annotated with dependency structures and other linguistic information. We focused on spatial relations and motion because one of the other functions of WELT will be to assist field linguists with elicitation of spatial language and documentation of spatial and motion semantics. We are making the corpus available to the public. Second, we compare the performance of two existing dependency parsing packages, MSTParser (McDonald et al., 2006) and MaltParser (Nivre et al., 2006), using incrementally increasing amounts of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 440 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 440–449, Osaka, Japan, December 11-17 2016. this training data. Third, we show that using a parser trained on small amounts of data can assist with dependency annotation. In Section 2, we discuss related work. In Section 3, we describe the new publicly available corpus. In Section 4, we"
C16-1043,C08-1085,0,0.0520103,"Missing"
C16-1043,W14-2202,1,0.900449,"Missing"
C16-1043,P14-5009,1,0.904061,"Missing"
C16-1043,L16-1262,0,\N,Missing
C90-2044,P87-1023,1,0.94398,"ue phrases produced by a single speaker in part of a recorded, transcribed lecture. In Section 2 we review our own and other work on cue phrases, in Section 3 we describe our current empirical studies, in Section 4 we present the results of our analysis, and in Section 5 we discuss theoretical and practical applications of our findings. Cue phrases are linguistic expressions such as &apos;now&apos; and &apos;welg t h a t m a y explicitly mark the structure of a discourse. For example, while the cue phrase &apos;inczdcntally&apos; m a y be used SENTENTIALLY as an adverbial, the DISCOUaSE use initiates a digression. In [8], we noted the ambiguity of cue phrases with respect to discourse and sentential usage and proposed an intonational model for their disambiguation. In this paper, we extend our previous characterization of cue phrases aald generalize its domain of coverage, based on a larger and more comprehensive empirical study: an examination of all cue phrases produced by a single ,~peaker in recorded natural speech. We also associate this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational"
C90-2044,P84-1085,0,0.0275878,"also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, C L U E W O R D S , D I S C O U R S E MAI:tKERS~ a r i d DISCOURSE PARTICLES [3, 4, 14, 17, 19]. Some exarnpies are &apos;now&apos;, which marks the introduction of a new subtopic or return to a previous one, &apos;incidentally&apos; and &apos;by the way&apos;, which indicate the beginning of a digression, and &apos;anyway&apos; and &apos;in any case&apos;, which indicate return from a digression. In a previous study[8], we noted that such terms are potentially ambiguous between DISCOURSE and SENTENTIAL uses[18]. So, &apos;now&apos; may be used as a temporal adverbial as well as a discourse marker, &apos;incidentally&apos; may also function as an adverbial, and other cue phrases similarly have one or more senses in addition to their function as markers of"
C90-2044,A88-1019,0,0.0920564,"Missing"
C90-2044,P84-1055,0,0.114207,"scourse and practical application to the generation of synthetic speech. 1 2 Studies The i m p o r t a n t role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature. For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora[5, 4, 17] and in the identification of rhetorical relations [10, 12, 17]. Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.ASES, C L U E W O R D S , D I S C O U R S E MAI:tKERS~ a r i d DISCOURSE PARTICLES [3, 4, 14, 17, 19]. Some exarnpies are &apos;now&apos;, which marks the introduction of a new subtopic or return to a"
C90-2044,J86-3001,0,0.0975938,"te this prosodic model with orthographic and part-of-speech analyses of cue phrases in text. Such a dual model provides both theoretical justification for current computational models of discourse and practical application to the generation of synthetic speech. 1 2 Studies The i m p o r t a n t role that cue phrases play in understanding and generating discourse has been well documented in the computational linguistics literature. For example, by indicating the presence of a structural boundary or a relationship between parts of a discourse, cue phrases caa assist in the resolution of anaphora[5, 4, 17] and in the identification of rhetorical relations [10, 12, 17]. Cue phrases have also been used to reduce the complexity of discourse processing and to increase textual coherence[3, 11, 21]. In Example (1) 1, interpretation of the anaphor &apos;it&apos; as (correctly) co-indexed with THE SYSTEM is facilitated by the presence of the cue phrases &apos;say&apos; and &apos;then&apos;, marking potential antecedents in &apos;... as an E X P E R T D A T A B A S E for AN E X P E R T SYSTEM ...&apos; a s structurally unavailable. 2 Introduction Words and phrases that may directly mark the structure of a discourse have been termed CUE PttR.A"
C90-2044,P89-1015,0,0.027293,"Missing"
D07-1043,W04-0817,0,0.0233241,"re V-measure to a number of popular cluster evaluation measures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering. 1 Introduction Clustering techniques have been used successfully for many natural language processing tasks, such as document clustering (Willett, 1988; Zamir and Etzioni, 1998; Cutting et al., 1992; Vempala and Wang, 2005), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006). They are particularly appealing for tasks in which there is an abundance of language data available, but manual annotation of this data is very resource-intensive. Unsupervised clustering can eliminate the need for (full) manual annotation of the data into desired classes, but often at the cost of making evaluation of success more difficult. External evaluation measures for clustering can be applied when class labels for each data point in some evaluation set can be determined a priori. The clustering task is then to assign these data points to"
D07-1043,N06-1029,0,0.0145055,"ures and demonstrate that it satisfies several desirable properties of clustering solutions, using simulated clustering results. Finally, we use V-measure to evaluate two clustering tasks: document clustering and pitch accent type clustering. 1 Introduction Clustering techniques have been used successfully for many natural language processing tasks, such as document clustering (Willett, 1988; Zamir and Etzioni, 1998; Cutting et al., 1992; Vempala and Wang, 2005), word sense disambiguation (Shin and Choi, 2004), semantic role labeling (Baldewein et al., 2004), pitch accent type disambiguation (Levow, 2006). They are particularly appealing for tasks in which there is an abundance of language data available, but manual annotation of this data is very resource-intensive. Unsupervised clustering can eliminate the need for (full) manual annotation of the data into desired classes, but often at the cost of making evaluation of success more difficult. External evaluation measures for clustering can be applied when class labels for each data point in some evaluation set can be determined a priori. The clustering task is then to assign these data points to any number of clusters such that each cluster c"
H89-2004,H89-2039,0,0.0648724,"Missing"
H89-2004,P83-1013,0,0.0727744,"Missing"
H91-1074,J90-3003,0,\N,Missing
H91-1074,H89-2048,0,\N,Missing
H91-1074,H90-1006,0,\N,Missing
H91-1074,A88-1019,0,\N,Missing
H91-1074,P89-1015,0,\N,Missing
H91-1074,P90-1002,0,\N,Missing
H92-1089,J86-3001,1,0.563695,"nt. INTRODUCTION The hypothesis that discourse structure is signalled by variation in intonational features such as pitch range, timing, and amplitude has been examined in studies such as [1, 2, 3, 4, 5, 6, 7]. However, as Brown and her colleagues note [2, p. 27]: ""... until an independent theory of topic-structure is formulated, much of our argument in this area is in danger of circularity."" In this paper we examine the relationship between discourse structure and variation in intonational features using just such an independent model of discourse structure, that proposed by Grosz and Sidner [8] (G&S). We present results of an empirical study comparing intonational features of read text with elements of both the local and global structure of discourse. Our study has immediate application to the generation of appropriate intonational features for synthetic speech, and future applicability to the recognition of discourse structure in speech recognition tasks. Our corpus consisted of AP news stories recorded by a professional speaker. The intonational features we considered included pitch range, contour, timing, and amplitude. The discourse structural elements we examined at the local l"
H92-1089,P89-1025,0,0.0177674,"cluded pitch range, contour, timing, and amplitude. The discourse structural elements we examined at the local level included parentheticals, quotations, tags, and indirect reported speech; at the global level, we studied discourse segmentation - - the division of a discourse into constituents that provide the basis for determining discourse meaning. The discourses were labeled by two groups: one group labeled from text; the other group 441 3. SCOPE OF THE STUDY Although computational theories of discourse make different claims about the basis of discourse structure - e.g. coherence relations [9, 10, 11, 12], syntactic features [13], intentions [8] - - all agree that utterances in a discourse group together into segments and that the determination of discourse meaning depends crucially on identifying the ways segments fit together. However, discourse segment boundaries do not always align with paragraph boundaries or other orthographic markers in text. And there have been no systematic studies of human labeling of discourse segmentation. As a result, attempts to apply theories of discourse structure have sometimes been frustrated by apparent ambiguities in the structure of a single discourse. Thu"
H92-1089,P83-1007,1,0.767277,"Missing"
H93-1066,H90-1021,0,0.0839818,"Missing"
H93-1066,H92-1003,0,0.0579026,"Missing"
H93-1066,P83-1019,0,0.209901,", to permit early pruning of the hypothesis space, rather than carrying along competing hypotheses, as in ""text-first"" approaches. Fourth, utterances containing overlapping repairs such as (4) (noted in [2, p. 123]) cannot be handled by simple surface structure manipulations. (4) I think that it you get- it's more strict in Catholic schools. Finally, on a cognitive level, there is recent psycholinguistic evidence that humans detect repairs in the vicinity of the interruption point, well before the end of the repair utterance [10, 11, 12]. An exception to ""text-first"" approaches is Hindle 1983 [2]. Hindle decouples repair detection from repair correction. His correction strategies rely upon an inventory of three repair types that are defined in relation to independently formulated linguistic principles. Importantly, Hindle allows non-surfacebased transformations as correction strategies. A related property is that the correction of a single repair may be achieved by sequential application of several correction rules. Hindle classifies repairs as 1) full sentence restarts, in which an entire utterance is re-initiated; 2) constituent repairs, in which one syntactic constituent is replace"
H93-1066,H92-1085,0,0.0263652,"Missing"
H93-1066,J83-3001,0,0.0720481,"Missing"
H93-1066,J83-3003,0,0.0534179,"Missing"
H93-1066,J86-1002,0,0.0422102,"Missing"
H93-1066,P92-1008,0,0.0428918,"Missing"
H93-1066,J80-2003,0,\N,Missing
H93-1066,J90-3003,0,\N,Missing
J06-3004,P98-1122,0,0.360877,"t. In particular, dialogue confirmation strategies may hinder users’ ability to correct system error. For instance, if a system wrongly presents information as being correct, as when it verifies information implicitly, users become confused about how to respond (Krahmer et al. 2001). Other studies have shown that speakers tend to switch to a prosodically “marked” speaking style after communication errors, comparing repetition corrections with the speech being repeated (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; 418 Litman, Hirschberg, and Swerts Corrections in Spoken Dialogue Systems Levow 1998; Bell and Gustafson 1999). Although this speaking style may be effective in problematic human–human communicative settings, there is evidence that suggests it leads to further errors in human–machine interactions (Levow 1998; Soltau and Waibel 2000). That corrections are difficult for ASR systems is generally explained by the fact that they tend to be hyperarticulated—higher, louder, longer—than other turns (Wade, Shriberg, and Price 1992; Oviatt et al. 1996; Levow 1998; Bell and Gustafson 1999; Shimojima, et al. 2001; Soltau and Waibel 1998, 2000; Soltau, Metze, and Waibel 2002), where ASR m"
J06-3004,P99-1040,1,0.609359,"Missing"
J06-3004,A00-2028,1,0.694232,"Corrections in Spoken Dialogue Systems 4.1.4 Dialogue Position and History Features. We also showed that the further a correction is from the original error, the less likely it is to be recognized correctly, and the stronger the correlation with prosodic deviation from the mean values over a speaker’s turns (e.g., more distant corrections are higher in pitch than closer corrections). As a first approximation of this distance feature, we included the feature diadist—distance of the current turn from the beginning of the dialogue. In addition, previous research (Litman, Walker, and Kearns 1999; Walker et al. 2000) has shown that features of the dialogue as a whole and features of more local context can be helpful in predicting “problematic” dialogues. So we looked at a set of features summarizing aspects of the prior dialogue for both the absolute number of times prior turns exhibited certain characteristics (e.g., contained a key word like cancel—priorcancnum) and the percentage of the prior dialogue containing one of these features (e.g., priorcancpct). We also examined means for all our continuous-valued features over the entire dialogue preceding the turn to be predicted (pmn ), such as pmnsyls, th"
J06-3004,C98-1117,0,\N,Missing
J12-1001,2007.sigdial-1.49,0,0.115771,"Missing"
J12-1001,N01-1016,0,0.029684,"and after w. Number and proportion of turns in w’s task before and after w. Number of words uttered by the other speaker during w’s turn. Number of words in the following turn by the other speaker. Number of ACWs in w’s turn other than w. Our lexical features consist of the lexical identity and the part-of-speech (POS) tag of the target word (w), the word immediately preceding w, and the word immediately following w (see Table 4). POS tags were labeled automatically for the whole corpus using Ratnaparkhi, Brill, and Church’s (1996) maxent tagger trained on a subset of the Switchboard corpus (Charniak and Johnson 2001) in lower-case with all punctuation removed, to simulate spoken language transcripts. Each word had an associated POS tag from the full Penn Treebank tag set (Marcus, Marcinkiewicz, and Santorini 1993), and one of the following simpliﬁed tags: noun, verb, adjective, adverb, contraction, or other. For our discourse features, listed in Table 4, we deﬁne an inter-pausal unit (IPU) as a maximal sequence of words surrounded by silence longer than 50 msec. A turn is a maximal sequence of IPUs from one speaker, such that between any two adjacent IPUs there is no speech from the interlocutor.3 , 4 Bou"
J12-1001,P84-1055,0,0.732589,"iscourse, and listeners and readers in processing it. In previous literature, these constructions have also been termed discourse markers, pragmatic connectives, discourse operators, and clue words. Examples of cue phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence, as a matter of fact, in fact, actually, okay, alright, for example, and incidentally. The ability to correctly determine the function of cue phrases is critical for important natural language processing tasks, including anaphora resolution (Grosz and Sidner 1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986; Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995). ∗ Departamento de Computación, FCEyN, Universidad de Buenos Aires, Pabellón I, Ciudad Universitaria, (C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar. ∗∗ E-mail: julia@cs.columbia.edu. † E-mail: sbenus@ukf.sk. Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for publication: 13 March 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 1 Furthermore, correctly determinin"
J12-1001,P07-1101,1,0.817906,"f ACWs have been studied in very small corpora, with some proposals about their prosodic and functional variations. For example, Hockey (1993) examines the prosodic variation of two ACWs, okay and uh-huh (66 and 77 data points, respectively) produced as full intonational phrases in two spontaneous task-oriented dialogues. She groups the F0 contours visually and auditorily, and shows that instances of okay produced with a high-rise contour are signiﬁcantly more likely to be followed by speech from the other speaker than from the same speaker. The results of a perception experiment conducted by Gravano et al. (2007) suggest that, in task-oriented American English dialogue, contextual information (e.g., duration of surrounding silence, number of surrounding words) as well as word-ﬁnal intonation ﬁgure as the most salient cues to disambiguation of the function of the word okay by human listeners. Also, in a study of the function of intonation in Scottish English task-oriented dialogue, Kowtko (1996) examines a corpus of 273 instances of single-word utterances, including afﬁrmative cue words such as mmhm, okay, right, uh-huh, and yes. Kowtko ﬁnds a signiﬁcant correlation between discourse function and inton"
J12-1001,W09-3936,1,0.798604,"Japanese and American English, a region of low pitch lasting at least 110 msec which may function as a prosodic cue inviting the realization of a backchannel response from the interlocutor. In a corpus study of Japanese dialogues, Koiso et al. (1998) ﬁnd that both syntax and prosody play a central role in predicting the occurrence of backchannels. Cathcart, Carletta, and Klein (2003) propose a method for automatically predicting the placement of backchannels in Scottish English conversation, based on pause durations and part-of-speech tags, that outperforms a random baseline model. Recently, Gravano and Hirschberg (2009a, 2009b, 2011) describe six distinct prosodic, acoustic, and lexical events in American English speech that tend to precede the occurrence of a backchannel by the interlocutor. Despite their high frequency in spontaneous conversation, the set of ACWs we examine here have seldom, if ever, been an object of study in themselves, as a separate subclass of cue phrases or dialogue acts. Some have attempted to model other types of cue phrases (e.g., well, like) or cue phrases in general; others discuss discourse/ pragmatic functions that may be conveyed through ACWs, but which may also be conveyed t"
J12-1001,J86-3001,0,0.700024,"hey aid speakers and writers in organizing the discourse, and listeners and readers in processing it. In previous literature, these constructions have also been termed discourse markers, pragmatic connectives, discourse operators, and clue words. Examples of cue phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence, as a matter of fact, in fact, actually, okay, alright, for example, and incidentally. The ability to correctly determine the function of cue phrases is critical for important natural language processing tasks, including anaphora resolution (Grosz and Sidner 1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986; Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995). ∗ Departamento de Computación, FCEyN, Universidad de Buenos Aires, Pabellón I, Ciudad Universitaria, (C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar. ∗∗ E-mail: julia@cs.columbia.edu. † E-mail: sbenus@ukf.sk. Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for publication: 13 March 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Numbe"
J12-1001,P87-1023,1,0.498626,"rance in the discourse—that is, they indicate the discourse segment to which an utterance belongs. However, she suggests that cue phrases only display discourse structure relations; they do not create them. In a critique of Schiffrin’s work, Redeker (1991) proposes deﬁning cue phrases as phrases “uttered with the primary function of bringing to the listener’s attention a particular kind of linkage of the upcoming utterance with the immediate discourse context” (page 1169). Prior work on the automatic classiﬁcation of cue phrases includes a series of studies performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and Hirschberg 1990), which focus on differentiating between the discourse and sentential senses of single-word cue phrases such as now, well, okay, say, and so in American English. When used in a discourse sense, a cue phrase explicitly conveys information about the discourse structure; when used in a sentential sense, a cue phrase instead conveys semantic information. Hirschberg and Litman present two manually developed classiﬁcation models, one based on prosodic features, and one based on textual features. This line of research is further pursued by Litman (1994, 1996), who i"
J12-1001,J93-3003,1,0.632114,"Missing"
J12-1001,P96-1038,1,0.326009,"ons were calculated using z-scores: z = (x − μ )/σ, where x is a raw measurement to be normalized (e.g., the duration of a particular word), and μ and σ are the mean and standard deviation of a certain population (e.g., all instances of the same word by the same speaker in the whole conversation). For our phonetic features (listed in Table 8), we trained an automatic phone recognizer based on the Hidden Markov Model Toolkit (HTK) (Young et al. 2006), using three corpora as training data: the TIMIT Acoustic-Phonetic Continuous Speech Corpus (Garofolo et al. 1993), the Boston Directions Corpus (Hirschberg and Nakatani 1996), and the Columbia Games Corpus. With this recognizer, we obtained automatic timealigned phonetic transcriptions of each instance of alright, mm-hm, okay, right, uh-huh, and yeah in the corpus. To improve accuracy, we restricted the recognizer’s grammar to accept only the most frequent variations of each word, as shown in Table 9. We extracted our phonetic features, such as phone and syllable durations, from the resulting timealigned phonetic transcriptions. The remaining ﬁve ACWs in our corpus (gotcha, huh, yep, yes, and yup) had too low counts to contain meaningful phonetic variation; thus,"
J12-1001,W98-0319,0,0.417651,"uh-huh), which, according to Walker, “add no new propositional content to the common ground” (Walker 1993a, page 32). Walker adopts the term “continuer” from the Conversational Analysis school to further describe these prompts (Walker 1993a). Walker describes some intonational contours which are used to realize IRUs in generation in Walker (1993a) and in Walker (1993b), examining 63 IRU tokens and ﬁnding ﬁve different types of contour used among them. As part of a larger project on automatically detecting discourse structure for speech recognition and understanding tasks in American English, Jurafsky et al. (1998) present a study of four particular discourse/pragmatic functions, or dialog acts (Bunt 1989; Core and Allen 1997), closely related to ACWs: backchannel, agreement, incipient speakership (indicating an intention to take the ﬂoor), and yes-answer (afﬁrmative answer to a yes–no question). The authors examine 1,155 conversations from the Switchboard database (Godfrey, Holliman, and McDaniel 1992), and report that the vast majority of these four dialogue acts are realized with words like yeah, okay, or uh-huh. They ﬁnd that the lexical realization of the dialogue act is the strongest cue to its id"
J12-1001,C90-2044,1,0.555417,"ey indicate the discourse segment to which an utterance belongs. However, she suggests that cue phrases only display discourse structure relations; they do not create them. In a critique of Schiffrin’s work, Redeker (1991) proposes deﬁning cue phrases as phrases “uttered with the primary function of bringing to the listener’s attention a particular kind of linkage of the upcoming utterance with the immediate discourse context” (page 1169). Prior work on the automatic classiﬁcation of cue phrases includes a series of studies performed by Hirschberg and Litman (Hirschberg and Litman 1987, 1993; Litman and Hirschberg 1990), which focus on differentiating between the discourse and sentential senses of single-word cue phrases such as now, well, okay, say, and so in American English. When used in a discourse sense, a cue phrase explicitly conveys information about the discourse structure; when used in a sentential sense, a cue phrase instead conveys semantic information. Hirschberg and Litman present two manually developed classiﬁcation models, one based on prosodic features, and one based on textual features. This line of research is further pursued by Litman (1994, 1996), who incorporates machine learning techni"
J12-1001,P95-1015,0,0.129548,"o been termed discourse markers, pragmatic connectives, discourse operators, and clue words. Examples of cue phrases include now, well, so, and, but, then, after all, furthermore, however, in consequence, as a matter of fact, in fact, actually, okay, alright, for example, and incidentally. The ability to correctly determine the function of cue phrases is critical for important natural language processing tasks, including anaphora resolution (Grosz and Sidner 1986), argument understanding (Cohen 1984), plan recognition (Grosz and Sidner 1986; Litman and Allen 1987), and discourse segmentation (Litman and Passonneau 1995). ∗ Departamento de Computación, FCEyN, Universidad de Buenos Aires, Pabellón I, Ciudad Universitaria, (C1428EGA) Buenos Aires, ARGENTINA. E-mail: gravano@dc.uba.ar. ∗∗ E-mail: julia@cs.columbia.edu. † E-mail: sbenus@ukf.sk. Submission received: 9 December 2009; revised submission received: 2 February 2011; accepted for publication: 13 March 2011. © 2012 Association for Computational Linguistics Computational Linguistics Volume 38, Number 1 Furthermore, correctly determining the function of cue phrases using features of the surrounding text can be used to improve the naturalness of synthetic s"
J12-1001,J93-2004,0,0.0419623,"Missing"
J12-1001,P94-1014,0,0.144193,"understanding, and attitudinal reactions. These correspond respectively 3 Computational Linguistics Volume 38, Number 1 to whether the interlocutor is willing and able to continue the interaction, perceive the message, understand the message, and react and respond to the message. Allwood, Nivre, and Ahlsen posit that “simple feedback words, like yes, [...] involve a high degree of context dependence” (page 5), and suggest that their basic communicative function strongly depends on the type of speech act, factual polarity, and information status of the immediately preceding communicative act. Novick and Sutton (1994) propose an alternative categorization of linguistic feedback in task-oriented dialogue, which is based on the structural context of exchanges rather than on the characteristics of the preceding utterance. The three main classes in Novick and Sutton’s catalogue are: (i) other → ackn, where an acknowledgment immediately follows a contribution by other speaker; (ii) self → other → ackn, where self initiates an exchange, other eventually completes it, and self utters an acknowledgment; and (iii) self + ackn, where self includes an acknowledgment in an utterance independently of other’s previous c"
J12-1001,W96-0213,0,0.102185,"Missing"
J12-1001,N09-2021,1,0.822555,"Missing"
J12-1001,N10-1109,0,0.0406936,"Missing"
J12-1001,J00-3003,0,0.254603,"ond task, the detection of a discourse segment boundary function, should help in discourse segmentation and meeting processing tasks (Litman and Passonneau 1995). Our SVM models based on lexical, discourse, timing, and acoustic features approach the error rate of trained human labelers in all tasks, while our automatically computed phonetic features offer no improvement. Previous studies indicate that the pragmatic function of ambiguous expressions may be effectively predicted by models that combine information extracted from various sources, including lexical and prosodic (e.g., Litman 1996; Stolcke et al. 2000). Our results support this, and extend the list of useful information sources to include discourse and timing features that may be easily extracted from the time-aligned transcripts. Additionally, our machine learning study includes experiments with several combinations of feature sets, in an attempt to simulate the conditions of different applications. Models that are trained using features extracted only from the speech signal up to the IPU containing the target word simulate on-line applications such as spoken dialogue systems with access to acoustic/prosodic features. Although such models"
J12-1001,C92-1054,0,0.325625,"vidence supporting different degrees of grounding (Roque and Traum 2009). Our ACWs often occur in the process of establishing such common ground. Prosodic characteristics of the responses involved in grounding have been studied in the Australian English Map Task corpus by Mushin et al. (2003), who ﬁnd that these utterances often consist of acknowledgment contributions such as okay or yeh produced with a “non-ﬁnal” intonational contour, and followed by speech by the same speaker which appears to continue the intonational phrase. Studies by Walker of informationally redundant utterances (IRUs) (Walker 1992, 1996), utterances which express “a proposition already entailed, presupposed or implicated by a previous utterance in the same discourse situation” (Walker 1993a, page 12), also include some of our ACWs, such as IRU prompts (e.g., uh-huh), which, according to Walker, “add no new propositional content to the common ground” (Walker 1993a, page 32). Walker adopts the term “continuer” from the Conversational Analysis school to further describe these prompts (Walker 1993a). Walker describes some intonational contours which are used to realize IRUs in generation in Walker (1993a) and in Walker (19"
J12-1001,W04-2313,0,0.173101,"stead conveys semantic information. Hirschberg and Litman present two manually developed classiﬁcation models, one based on prosodic features, and one based on textual features. This line of research is further pursued by Litman (1994, 1996), who incorporates machine learning techniques to derive classiﬁcation models automatically. Litman uses different combinations of prosodic and text-based features to train decision-tree and rule learners, and shows that machine learning constitutes a powerful tool for developing automatic classiﬁers of cue phrases into their sentential and discourse uses. Zufferey and Popescu-Belis (2004) present a similar study on the automatic classiﬁcation of like and well into discourse and sentential senses, achieving a performance close to that of human annotators. Besides the binary division of cue phrases into discourse vs. sentential meanings, the Conversational Analysis (CA) literature describes items it terms linguistic feedback or acknowledgments. These include not only the computational linguists’ cue phrases but also expressions such as I see or oh wow, which CA research describes in terms of attention, understanding, and acceptance by the speaker of a proposition uttered by anot"
J12-1001,U06-1007,0,\N,Missing
J93-3003,A88-1019,0,0.0146172,"Missing"
J93-3003,P84-1055,0,0.797345,"-oriented dialogs, plan-based knowledge could be used to assist in the recognition of discourse structure (Grosz 1977). However, such analysis is often beyond the capabilities of current natural language processing systems. Many domains are also not task-oriented. Additionally, cue phrases are widely used in the identification of rhetorical relations among portions of a text or discourse (Hobbs 1979; Mann and Thompson 1983; Reichman 1985), and have been claimed in general to reduce the complexity of discourse processing and to increase textual coherence in natural language processing systems (Cohen 1984; Litman and Allen 1987; Zuckerman and Pearl 1986). Previous attempts to characterize the set of cue phrases in the linguistic and in the computational literature have typically been extensional, with each cue phrase or set of phrases associated with one or more discourse or conversational functions. In the linguistic literature, cue phrases have been the subject of a number of theoretical and descriptive corpus-based studies that emphasize the diversity of meanings associated with cue phrases as a class, within an overarching framework of function such as discourse c o h e s i v e n e s s or"
J93-3003,J86-3001,0,0.985314,"ture. These include items such as now, which marks the introduction of a new subtopic or return to a previous one; well, which indicates a response to previous material or an explanatory comment; incidentally, by the way, and that reminds me, which indicate the beginning of a digression; and anyway and in any case, which indicate a return from a digression. The recognition and appropriate generation of cue phrases is of particular interest to research in discourse structure. The structural information conveyed by these phrases is crucial to many tasks, such as anaphora resolution (Grosz 1977; Grosz and Sidner 1986; Reichman 1985), the inference of speaker intention and the recognition of speaker plans (Grosz and Sidner 1986; Sidner 1985; Litman and Allen 1987), and the generation of explanations and other text (Zuckerman and Pearl 1986). Despite the crucial role that cue phrases play in theories of discourse and their implementation, however, many questions about how cue phrases are identified and defined remain to be examined. In particular, the question of cue phrase polysemy has yet to receive a satisfactory solution. Each lexical item that has one or more discourse * 600 MountainAvenue,MurrayHill,"
J93-3003,P87-1023,1,0.920857,"n Example 6 is in fact reduced, as Halliday and Hassan (1976) propose, that in Example 10, while interpreted as a discourse use, is nonetheless clearly intonationally prominent. Furthermore, both of the n o w s in Example 5 are also prominent. So it would seem that intonational prominence alone is insufficient to disambiguate between sentential and discourse uses. In this paper we present a more complex model of intonational features and textbased features that can serve to disambiguate between sentential and discourse instances of cue phrases. Our model is based on several empirical studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pier"
J93-3003,P86-1021,1,0.458406,"cal studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pierrehumbert and Hirschberg 1990; Silverman 1987). However, just which intonational features are important and how they communicate discourse information is not well understood. Prerequisite, however, to addressing these issues is the adoption of a framework of intonational description to identify which intonational features will be examined and how they will be characterized. For the studies discussed below, we have adopted Pierrehumbert's (1980) theory of English intonation, which we will describe briefly below. In Pierrehumbert's phonological description of English, intonational contours"
J93-3003,C90-2044,1,0.721327,"ed, as Halliday and Hassan (1976) propose, that in Example 10, while interpreted as a discourse use, is nonetheless clearly intonationally prominent. Furthermore, both of the n o w s in Example 5 are also prominent. So it would seem that intonational prominence alone is insufficient to disambiguate between sentential and discourse uses. In this paper we present a more complex model of intonational features and textbased features that can serve to disambiguate between sentential and discourse instances of cue phrases. Our model is based on several empirical studies (Hirschberg and Litman 1987; Litman and Hirschberg 1990): two studies of individual cue phrases in which we develop our model, and a more comprehensive study of cue phrases as a class, in which we confirm and expand our model. Before describing these studies and their results, we must first describe the intonational features examined in our analyses. 3. Phrasing and Accent in English The importance of intonational information to the communication of discourse structure has been recognized in a variety of studies (Butterworth 1975; Schegloff 1979; Brazil, Coulthard, and Johns 1980; Hirschberg and Pierrehumbert 1986; Pierrehumbert and Hirschberg 1990"
J93-3003,H92-1088,0,0.0134156,"Missing"
L18-1107,P13-2037,0,0.0191872,"and their code-switching rate – i.e., how frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional"
L18-1107,W16-5813,0,0.0256447,"2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is the focus of this paper, C¸etinoglu (2016) obtained a corpus of German-Turkish tweets by automatically computing dictionaries of pure German and Turkish from a mill"
L18-1107,W16-5812,1,0.545608,"nguage Identification in code-switched (CS) Data. We compare them in terms of the amount of bilingualism they contain and their code-switching rate – i.e., how frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Ma"
L18-1107,W14-3902,0,0.0566331,"Missing"
L18-1107,L16-1667,0,0.0293037,"Missing"
L18-1107,W14-3911,0,0.0266792,"2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Share"
L18-1107,goldhahn-etal-2012-building,0,0.0796682,"Missing"
L18-1107,W16-6212,1,0.855114,"o and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is the focus of this paper, C¸etinoglu (2016) obtained a corpus of German-Turkish tweets by automatically computing dictionaries of pure German and Turkish from a million Turkish, German and English 671 tweets. They"
L18-1107,W16-5807,1,0.850811,"o and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is the focus of this paper, C¸etinoglu (2016) obtained a corpus of German-Turkish tweets by automatically computing dictionaries of pure German and Turkish from a million Turkish, German and English 671 tweets. They"
L18-1107,R15-1033,0,0.0378356,"Missing"
L18-1107,W15-3116,0,0.0199625,"rkshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a"
L18-1107,C12-1102,0,0.023121,"alism they contain and their code-switching rate – i.e., how frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labelin"
L18-1107,D14-1098,0,0.0262883,"w frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2"
L18-1107,P12-3005,0,0.105564,"Missing"
L18-1107,W16-2609,1,0.587243,"the task using weak anchors as defined above. At the expense of 0.01 absolute precision points, recall is improved by almost 0.35 points. The bottom subtable of Table 1 shows results using weak anchors and Language Id. Although with this method the recall drops 0.03 points with respect to the weak anchors, we achieve the advantage of being able to reduce the number of queries we need for the collection, and make the search less restrictive. In the next section of the paper we use weak anchors with the Language ID restriction to collect code-switched tweets. 4. Data Collection We used Babler1 (Mendels et al., 2016) to collect codeswitched data from Twitter. Babler is a tool designed for harvesting web-data for NLP and machine learning tasks. Babler’s pipeline is launched by querying a seed word s ∈ S using Twitter’s API. The tweets retrieved by the query are later processed and passed through a set of filtering rules R which are predefined for the task. Following the definition of “weak anchor plus Language Id” given in section 3. we used the “weak” anchors to 1 Babler is publicly available from https://github. com/gidim/Babler seed the Twitter API and the filtering rules R to enforce the LID restrictio"
L18-1107,W16-5805,0,0.361141,"; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is th"
L18-1107,W16-5815,0,0.0173004,"code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is the focus of this paper, C¸etinoglu (2016) obtained a corpus of German-Turkish tweets by automatically computing dictionaries of pure German and Turkish from a million Turkish, German and English 671 tweets. They subsequently used those dictionaries to automatically tag ten million Turkish tweets from which they obtained 8,000 potentially code-switched tweets which they manually filtered down to 680. Samih (2016) obtained a corpus of forum posts written in MSA and the Darija Dialect"
L18-1107,W16-5806,0,0.0193482,"gnition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data from the web, which is the focus of this paper, C¸etinoglu (2016) obtained a corpus of German-Turkish tweets by automatically computing dictionaries of pure German and Turkish from a million Turkish, German and English 671 tweets. They subsequently used those dictionaries to automatically tag ten million Turkish tweets from which they obtained 8,000 potentia"
L18-1107,L16-1658,0,0.0395301,"Missing"
L18-1107,D08-1102,0,0.0386352,"two authors contributed equally to this work. Shared Task for Language Identification in code-switched (CS) Data. We compare them in terms of the amount of bilingualism they contain and their code-switching rate – i.e., how frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on E"
L18-1107,D08-1110,0,0.0447593,"two authors contributed equally to this work. Shared Task for Language Identification in code-switched (CS) Data. We compare them in terms of the amount of bilingualism they contain and their code-switching rate – i.e., how frequently writers switch their language in the corpus. In Section 6.2. we train and test language ID taggers on our corpus and the Workshop corpus and compare their performance. We present our conclusions in Section 7. 2. Previous Work In the past few years there have been increasing efforts on a variety of tasks using code-switched data, including partof-speech tagging (Solorio and Liu, 2008b; Vyas et al., 2014; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on E"
L18-1107,W14-3907,1,0.858387,"; Jamatia et al., 2015; AlGhamdi et al., 2016), parsing (Goyal et al., 2003), language modeling (Franco and Solorio, 2007; Li and Fung, 2012; Adel et al., 2013b; Adel et al., 2013a; Li and Fung, 2014), code-switching prediction (Solorio and Liu, 2008a; Elfardy et al., 2014), sentiment analysis (Vilares et al., 2015; Lee and Wang, 2015) and even speech recognition (Ahmed and Tan, 2012; Lyudovyk and Pylypenko, 2014). The task that has received most of the attention has been Language Identification on code-switched data, thanks in part to the First and Second Shared Tasks on EMNLP 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). Many of the current state-of-the-art models for Language Identification perform sequence labeling using Conditional Random Fields (Al-Badrashiny and Diab, 2016) or Recurrent Neural Networks (Jaech et al., 2016b). In the 2016 Shared Task the best performing system on the MSA-DA dataset used a combination of both (Samih et al., 2016) on top of word and character-level embeddings, and the best performing system on the ES-EN dataset used logistic regression (Piergallini et al., 2016) and character n-gram features. On the task of finding and collecting code-switched data fro"
L18-1107,C12-1160,0,0.0651342,"Missing"
L18-1107,W15-2902,0,0.0436196,"Missing"
L18-1237,P82-1020,0,0.833356,"Missing"
mata-etal-2014-teenage,mata-etal-2014-prosodic,1,\N,Missing
N01-1027,A00-2028,1,\N,Missing
N01-1027,A00-2029,1,\N,Missing
N01-1027,P98-1122,0,\N,Missing
N01-1027,C98-1117,0,\N,Missing
N01-1027,P99-1040,1,\N,Missing
N06-2032,P03-1071,0,0.114987,"Missing"
N06-2032,J97-1003,0,0.535078,"dary prediction are divided into three types: lexical, acoustic and speaker-dependent. The value of even errorful lexical information in identifying story boundaries has been confirmed for many previous story segmentation systems (Beeferman et al., 1999; Stokes, 2003)). We include some previously-tested types of lexical features in our own system, as well as identifying our own ‘cue-word’ features from our training corpus. Our lexical features are extracted from ASR transcripts produced by the NIGHTINGALE system. They include lexical similarity scores calculated from the TextTiling algorithm.(Hearst, 1997), which determines the lexical similarity of blocks of text by analyzing the cosine similarity of a sequence of sentences; this algorithm tests the likelihood of a topic boundary between blocks, preferring locations between blocks which have minimal lexical similarity. For English, we stem the input before calculating these features, using an implementation of the Porter stemmer (Porter, 1980); we have not yet attempted to identify root forms for Mandarin or Arabic. We also calculate scores from (Galley et al., 2003)’s LCseg 1 JRip is implemented in the Weka (Witten et al., 1999) machine learn"
N06-2032,P93-1041,0,0.0692882,"nce: Video (TRECVID) research programs, automatic detection of story boundaries remains an elusive problem. State-of-the-art story segmentation error rates on English and Mandarin BN remain fairly high and Arabic is largely unstudied. The NIGHTINGALE system searches a diverse news corpus to return answers to user queries. For audio sources, the identification of story boundaries is crucial, to segment material to be searched and to provide interpretable results to the user. 2 Related work Previous approaches to story segmentation have largely focused lexical features, such as word similarily (Kozima, 1993), cue phrases (Passonneau and Litman, 1997), cosine similarity of lexical winThe training data used for NIGHTINGALE includes the TDT-4 and TDT5 corpora (Strassel and Glenn, 2003; Strassel et al., 2004). TDT-4 includes newswire text and broadcast news audio in English, Arabic and Mandarin; TDT-5 contains only text data, and is therefore not used by our system. The TDT-4 audio corpus includes 312.5 hours of English Broadcast News from 450 shows, 88.5 hours of Arabic news from 109 shows, and 134 hours of Mandarin broadcasts from 205 shows. This material was drawn from six English news shows – ABC"
N06-2032,W04-2906,0,0.0741739,"of Brodcast News in English, Mandarin and Arabic Andrew Rosenberg Computer Science Department Columbia University New York City, N.Y. 10027 Julia Hirschberg Computer Science Department Columbia University New York City, N.Y. 10027 amaxwell@cs.columbia.edu julia@cs.columbia.edu dows (Hearst, 1997; Galley et al., 2003), and adaptive language modeling (Beeferman et al., 1999). Segmentation of stories in BN have included some acoustic features (Shriberg et al., 2000; T¨ur et al., 2001). Work on non-English BN, generally use this combination of lexical and acoustic measures, such as (Wayne, 2000; Levow, 2004) on Mandarin. And (Palmer et al., 2004) report results from feature selection experiments that include Arabic sources, though they do not report on accuracy. TRECVID has also identified visual cues to story segmentation of video BN (cf. (Hsu et al., 2004; Hsieh et al., 2003; Chaisorn et al., 2003; Maybury, 1998)). Abstract In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction algorithm with"
N06-2032,P98-2135,0,0.032217,"., 2003), and adaptive language modeling (Beeferman et al., 1999). Segmentation of stories in BN have included some acoustic features (Shriberg et al., 2000; T¨ur et al., 2001). Work on non-English BN, generally use this combination of lexical and acoustic measures, such as (Wayne, 2000; Levow, 2004) on Mandarin. And (Palmer et al., 2004) report results from feature selection experiments that include Arabic sources, though they do not report on accuracy. TRECVID has also identified visual cues to story segmentation of video BN (cf. (Hsu et al., 2004; Hsieh et al., 2003; Chaisorn et al., 2003; Maybury, 1998)). Abstract In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction algorithm with automatically extracted acoustic and lexical features, we report success rates that are competitive with state-ofthe-art systems on each input language. We further demonstrate that features useful for English and Mandarin are not discriminative for Arabic. 3 The NIGHTINGALE Corpus 1 Introduction Broadcast News ("
N06-2032,N04-4023,0,0.0224508,"andarin and Arabic Andrew Rosenberg Computer Science Department Columbia University New York City, N.Y. 10027 Julia Hirschberg Computer Science Department Columbia University New York City, N.Y. 10027 amaxwell@cs.columbia.edu julia@cs.columbia.edu dows (Hearst, 1997; Galley et al., 2003), and adaptive language modeling (Beeferman et al., 1999). Segmentation of stories in BN have included some acoustic features (Shriberg et al., 2000; T¨ur et al., 2001). Work on non-English BN, generally use this combination of lexical and acoustic measures, such as (Wayne, 2000; Levow, 2004) on Mandarin. And (Palmer et al., 2004) report results from feature selection experiments that include Arabic sources, though they do not report on accuracy. TRECVID has also identified visual cues to story segmentation of video BN (cf. (Hsu et al., 2004; Hsieh et al., 2003; Chaisorn et al., 2003; Maybury, 1998)). Abstract In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction algorithm with automatically extracted acoustic and le"
N06-2032,J97-1005,0,0.0357605,"h programs, automatic detection of story boundaries remains an elusive problem. State-of-the-art story segmentation error rates on English and Mandarin BN remain fairly high and Arabic is largely unstudied. The NIGHTINGALE system searches a diverse news corpus to return answers to user queries. For audio sources, the identification of story boundaries is crucial, to segment material to be searched and to provide interpretable results to the user. 2 Related work Previous approaches to story segmentation have largely focused lexical features, such as word similarily (Kozima, 1993), cue phrases (Passonneau and Litman, 1997), cosine similarity of lexical winThe training data used for NIGHTINGALE includes the TDT-4 and TDT5 corpora (Strassel and Glenn, 2003; Strassel et al., 2004). TDT-4 includes newswire text and broadcast news audio in English, Arabic and Mandarin; TDT-5 contains only text data, and is therefore not used by our system. The TDT-4 audio corpus includes 312.5 hours of English Broadcast News from 450 shows, 88.5 hours of Arabic news from 109 shows, and 134 hours of Mandarin broadcasts from 205 shows. This material was drawn from six English news shows – ABC “World News Tonight”, CNN “Headline News”,"
N06-2032,J02-1002,0,0.0521011,"Missing"
N06-2032,N03-3009,0,0.0340552,"Missing"
N06-2032,J01-1002,0,0.260935,"Missing"
N06-2032,wayne-2000-multilingual,0,0.021307,"Segmentation of Brodcast News in English, Mandarin and Arabic Andrew Rosenberg Computer Science Department Columbia University New York City, N.Y. 10027 Julia Hirschberg Computer Science Department Columbia University New York City, N.Y. 10027 amaxwell@cs.columbia.edu julia@cs.columbia.edu dows (Hearst, 1997; Galley et al., 2003), and adaptive language modeling (Beeferman et al., 1999). Segmentation of stories in BN have included some acoustic features (Shriberg et al., 2000; T¨ur et al., 2001). Work on non-English BN, generally use this combination of lexical and acoustic measures, such as (Wayne, 2000; Levow, 2004) on Mandarin. And (Palmer et al., 2004) report results from feature selection experiments that include Arabic sources, though they do not report on accuracy. TRECVID has also identified visual cues to story segmentation of video BN (cf. (Hsu et al., 2004; Hsieh et al., 2003; Chaisorn et al., 2003; Maybury, 1998)). Abstract In this paper, we present results from a Broadcast News story segmentation system developed for the SRI NIGHTINGALE system operating on English, Arabic and Mandarin news shows to provide input to subsequent question-answering processes. Using a rule-induction a"
N06-2032,C98-2130,0,\N,Missing
N09-1045,W09-0807,1,0.835986,"Missing"
N09-1045,P05-1071,1,0.75991,"ages with complex letter-to-sound mappings, such dictionaries are typically written by hand. However, for morphologically rich languages, such as MSA,1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pronunciations. Fortunately, the relationship between orthography and pronunciation is relatively regular and well understood for MSA. Moreover, recent automatic techniques for morphological analysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process (Habash and Rambow, 2005; Habash and Rambow, 2007) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools. In Section 2, we briefly describe related work, including the baseline system we use. In Section 3, we outline the linguistic phenomena we believe are critical to improving MSA pronunciation dictionaries. In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenomena. In Section 5, we describe how these rules are used, together with MADA, to build our pronunciation dict"
N09-1045,P06-1086,1,0.645739,"Missing"
N09-1045,N07-2014,1,0.474331,"to-sound mappings, such dictionaries are typically written by hand. However, for morphologically rich languages, such as MSA,1 pronunciation dictionaries are difficult to create by hand, because of the large number of word forms, each of which has a large number of possible pronunciations. Fortunately, the relationship between orthography and pronunciation is relatively regular and well understood for MSA. Moreover, recent automatic techniques for morphological analysis and disambiguation (MADA) can also be useful in automating part of the dictionary creation process (Habash and Rambow, 2005; Habash and Rambow, 2007) Nonetheless, most documented Arabic ASR systems appear to handle only a subset of Arabic phonetic phenomena; very few use morphological disambiguation tools. In Section 2, we briefly describe related work, including the baseline system we use. In Section 3, we outline the linguistic phenomena we believe are critical to improving MSA pronunciation dictionaries. In Section 4, we describe the pronunciation rules we have developed based upon these linguistic phenomena. In Section 5, we describe how these rules are used, together with MADA, to build our pronunciation dictionaries for training and"
N09-1045,W04-1612,0,0.17158,"Missing"
N09-1045,P06-1073,0,0.0614355,"Missing"
N12-1002,P11-2020,1,0.77456,"monstrated that mimicry of posture and behavior led to increased liking between the dialogue participants as well as a smoother interaction. They also found that naturally empathetic individuals exhibited a greater degree of mimicry than did others. Nenkova et al. (2008) found that entrainment on high-frequency words was correlated with naturalness, task success, and coordinated turn-taking behavior. Natale (1975) showed that an individual’s social desirability, or “propensity to act in a social manner,” can predict the degree to which that individual will match her partner’s vocal intensity. Levitan et al. (2011) showed that entrainment on backchannel-preceding cues is correlated with shorter latency between turns, fewer interruptions, and a higher degree of task success. In a study of married couples discussing problems in their relationships, Lee et al. (2010) found that entrainment measures derived from pitch features were significantly higher in positive interactions than in negative interactions and were predictive of the polarity of the participants’ attitudes. These studies have been motivated by theoretical models such as Giles’ Communication Accommodation Theory (Giles & Coupland, 1991), whic"
N12-1002,P08-2043,1,0.590389,"of spoken language, including speakers’ choice of referring expressions (Brennan & Clark, 1996); linguistic style (Niederhoffer & Pennebaker, 2002; Danescu-Niculescu-Mizil et al., 2011); syntactic Entrainment in many of these dimensions has also been associated with different measures of dialogue success. For example, Chartrand and Bargh (1999) demonstrated that mimicry of posture and behavior led to increased liking between the dialogue participants as well as a smoother interaction. They also found that naturally empathetic individuals exhibited a greater degree of mimicry than did others. Nenkova et al. (2008) found that entrainment on high-frequency words was correlated with naturalness, task success, and coordinated turn-taking behavior. Natale (1975) showed that an individual’s social desirability, or “propensity to act in a social manner,” can predict the degree to which that individual will match her partner’s vocal intensity. Levitan et al. (2011) showed that entrainment on backchannel-preceding cues is correlated with shorter latency between turns, fewer interruptions, and a higher degree of task success. In a study of married couples discussing problems in their relationships, Lee et al. (2"
N18-1176,C08-1006,0,0.824591,"have been analyzed in many genres. Ott et al. (2011) compared approaches to automatically detecting deceptive opinion spam, using a crowdsourced dataset of fake hotel reviews. Several studies use a fake opinion paradigm for collecting data, instructing subjects to write or record deceptive and truthful opinions about controversial topics such as the death penalty or abortion, or about a person that they like/dislike (Newman et al., 2003; Mihalcea and Strapparava, 2009). Other research has focused on real-world data obtained from court testimonies and depositions (Fornaciari and Poesio, 2013; Bachenko et al., 2008; P´erez-Rosas et al., 2015). Real-world deceptive situations are highstakes, where there is much to be gained or lost if deception succeeds or fails; it is hypothesized that these conditions are more likely to elicit strong cues to deception. However, working with such data requires extensive research to annotate each utterance for veracity, so such datasets are often quite small and not always reliable. Linguistic features such as n-grams and language complexity have been analyzed as cues to deception (P´erez-Rosas and Mihalcea, 2015; Yancheva and Rudzicz, 2013). Syntactic features such as p"
N18-1176,P12-2034,0,0.195609,"much to be gained or lost if deception succeeds or fails; it is hypothesized that these conditions are more likely to elicit strong cues to deception. However, working with such data requires extensive research to annotate each utterance for veracity, so such datasets are often quite small and not always reliable. Linguistic features such as n-grams and language complexity have been analyzed as cues to deception (P´erez-Rosas and Mihalcea, 2015; Yancheva and Rudzicz, 2013). Syntactic features such as part of speech tags have also been found to be useful for structured data (Ott et al., 2011; Feng et al., 2012). Statement Analysis (Adams, 1996) is a text-based deception detection approach that combines lexical and syntactic features. An especially useful resource for text-based deception detection is the Linguistic Inquiry and Word Count (LIWC) (Pennebaker and King, 1999), which groups words into psychologically motivated categories. In addition to lexical features, some studies have examined acousticprosodic cues to deception (Rockwell et al., 1997; Enos, 2009; Mendels et al., 2017). (Benus et al., 2006) studied pause behavior in deceptive speech. This work is very promising, but it is more difficu"
N18-1176,P13-1093,0,0.155058,"tions (Fornaciari and Poesio, 2013; Bachenko et al., 2008; P´erez-Rosas et al., 2015). Real-world deceptive situations are highstakes, where there is much to be gained or lost if deception succeeds or fails; it is hypothesized that these conditions are more likely to elicit strong cues to deception. However, working with such data requires extensive research to annotate each utterance for veracity, so such datasets are often quite small and not always reliable. Linguistic features such as n-grams and language complexity have been analyzed as cues to deception (P´erez-Rosas and Mihalcea, 2015; Yancheva and Rudzicz, 2013). Syntactic features such as part of speech tags have also been found to be useful for structured data (Ott et al., 2011; Feng et al., 2012). Statement Analysis (Adams, 1996) is a text-based deception detection approach that combines lexical and syntactic features. An especially useful resource for text-based deception detection is the Linguistic Inquiry and Word Count (LIWC) (Pennebaker and King, 1999), which groups words into psychologically motivated categories. In addition to lexical features, some studies have examined acousticprosodic cues to deception (Rockwell et al., 1997; Enos, 2009;"
N18-1176,P09-2078,0,0.61811,"eans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 3 for future work. Data 3.1 2 Related Work Language-based cues to deception have been analyzed in many genres. Ott et al. (2011) compared approaches to automatically detecting deceptive opinion spam, using a crowdsourced dataset of fake hotel reviews. Several studies use a fake opinion paradigm for collecting data, instructing subjects to write or record deceptive and truthful opinions about controversial topics such as the death penalty or abortion, or about a person that they like/dislike (Newman et al., 2003; Mihalcea and Strapparava, 2009). Other research has focused on real-world data obtained from court testimonies and depositions (Fornaciari and Poesio, 2013; Bachenko et al., 2008; P´erez-Rosas et al., 2015). Real-world deceptive situations are highstakes, where there is much to be gained or lost if deception succeeds or fails; it is hypothesized that these conditions are more likely to elicit strong cues to deception. However, working with such data requires extensive research to annotate each utterance for veracity, so such datasets are often quite small and not always reliable. Linguistic features such as n-grams and lang"
N18-1176,P11-1032,0,0.929346,"ture sets we employ. In Section 5, we report on the results of our empirical study of indicators of deception and perceived deception, as well as gender and native language differences. Section 6 presents our machine learning classification results using the deception indicator feature sets. We conclude in Section 7 with a discussion and ideas 1941 Proceedings of NAACL-HLT 2018, pages 1941–1950 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 3 for future work. Data 3.1 2 Related Work Language-based cues to deception have been analyzed in many genres. Ott et al. (2011) compared approaches to automatically detecting deceptive opinion spam, using a crowdsourced dataset of fake hotel reviews. Several studies use a fake opinion paradigm for collecting data, instructing subjects to write or record deceptive and truthful opinions about controversial topics such as the death penalty or abortion, or about a person that they like/dislike (Newman et al., 2003; Mihalcea and Strapparava, 2009). Other research has focused on real-world data obtained from court testimonies and depositions (Fornaciari and Poesio, 2013; Bachenko et al., 2008; P´erez-Rosas et al., 2015). Re"
N18-1176,D15-1133,0,0.193227,"Missing"
P00-1030,P98-2155,0,0.01146,"ng on phonology, speech analysis and synthesis (Bolinger, 1989; Ladd, 1996). In general, syntactic features are the most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context in uences accent patterns. More speci cally, we investigate how word collocation in uences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors mus"
P00-1030,P98-2165,1,0.826184,"most widely used features in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context in uences accent patterns. More speci cally, we investigate how word collocation in uences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors must be found. There are some advantages in looking only at one word class. We eliminate the interaction between part-of-spee"
P00-1030,W99-0619,1,0.838722,"es in pitch accent predication. For example, partof-speech is traditionally the most useful single pitch accent predictor (Hirschberg, 1993). Function words, such as prepositions and articles, are less likely to be accented, while content words, such as nouns and adjectives, are more likely to be accented. Other linguistic features, such as inferred given/new status (Hirschberg, 1993; Brown, 1983), contrastiveness (Bolinger, 1961), and discourse structure (Nakatani, 1998), have also been examined to explain accent assignment in large speech corpora. In a previous study (Pan and McKeown, 1998; Pan and McKeown, 1999), we investigated how features such as deep syntactic/semantic structure and word informativeness correlate with accent placement. In this paper, we focus on how local context in uences accent patterns. More speci cally, we investigate how word collocation in uences whether nouns are accented or not. Determining which nouns are accented and which are not is challenging, since part-ofspeech information cannot help here. So, other accent predictors must be found. There are some advantages in looking only at one word class. We eliminate the interaction between part-of-speech and collocation, so t"
P00-1030,J98-4010,0,0.0236302,"Missing"
P00-1030,C98-2160,1,\N,Missing
P00-1030,C98-2150,0,\N,Missing
P01-1048,A00-2029,1,\N,Missing
P01-1048,N01-1027,1,\N,Missing
P01-1048,P98-1122,0,\N,Missing
P01-1048,C98-1117,0,\N,Missing
P04-1085,J96-1002,0,0.00482603,"Missing"
P04-1085,P97-1023,1,0.16656,"rk of (Hillard et al., 2003). We did not use acoustic features, since the main purpose of the current work is to explore the use of contextual information. Table 3 lists the features that were found most helpful at identifying agreements and disagreements. Regarding lexical features, we selected a list of lexical items we believed are instrumental in the expression of agreements and disagreements: agreement markers, e.g. “yes” and “right”, as listed in (Cohen, 2002), general cue phrases, e.g. “but” and “alright” (Hirschberg and Litman, 1994), and adjectives with positive or negative polarity (Hatzivassiloglou and McKeown, 1997). We incorporated a set of durational features that were described in the literature as good predictors of agreements: utterance length distinguishes agreement from disagreement, the latter tending to be longer since the speaker elaborates more on the reasons and circumstances of her disagreement than for an agreement (Cohen, 2002). Duration is also a good predictor of backchannels, since they tend to be quite short. Finally, a fair amount of silence and filled pauses is sometimes an indicator of disagreement, since it is a dispreferred response in most social contexts and can be associated wi"
P04-1085,N03-2012,1,0.55458,"sus decision is reached. Our ultimate goal is automated summarization of multi-participant meetings and we hypothesize that the ability to automatically identify agreement and disagreement between participants will help us in the summarization task. For example, a summary might resemble minutes of meetings with major decisions reached (consensus) along with highlighted points of the pros and cons for each decision. In this paper, we present a method to automatically classify utterances as agreement, disagreement, or neither. Previous work in automatic identification of agreement/disagreement (Hillard et al., 2003) demonstrates that this is a feasible task when various textual, durational, and acoustic features are available. We build on their approach and show that we can get an improvement in accuracy when contextual information is taken into account. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational and structural features that look both forward and backward in the discourse. This allows us to acquire, and subsequently process, knowledge about who speaks to whom. We hypothesize that pragmatic features that center around previous agreemen"
P04-1085,W02-1002,0,0.0100279,"beling. Their drawback is that, as most generative models, they are generally computed to maximize the joint likelihood of the training data. In order to define a probability distribution over the sequences of observation and labels, it is necessary to enumerate all possible sequences of observations. Such enumeration is generally prohibitive when the model incorporates many interacting features and long-range dependencies (the reader can find a discussion of the problem in (McCallum et al., 2000)). Conditional models address these concerns. Conditional Markov models (CMM) (Ratnaparkhi, 1996; Klein and Manning, 2002) have been successfully used in sequence labeling tasks incorporating rich feature sets. In a left-to-right CMM as shown in Figure 1(a), the probability of a sequence of L tags + !      is decomposed as: 1        E4            !   . is the vector of observations and each is the index of a spurt. The probability distribution  @  / !- associated with each state of the Markov chain only depends on the preceding tag F  and the local observation "" . However, in order to incorporate more than one label dependency and, in"
P04-1085,W03-1209,0,0.011538,"Missing"
P04-1085,W04-2319,1,0.693642,"Missing"
P04-1085,J00-3003,1,0.795799,"Missing"
P04-1085,J93-3003,1,\N,Missing
P07-1101,P84-1055,0,0.831053,"three discourse-pragmatic functions of the word okay. Results of a perception study show that contextual cues are stronger predictors of discourse function than acoustic cues. However, acoustic features capturing the pitch excursion at the right edge of okay feature prominently in disambiguation, whether other contextual cues are present or not. 1 Introduction C UE PHRASES (also known as DISCOURSE MARK ERS) are linguistic expressions that can be used to convey explicit information about the structure of a discourse or to convey a semantic contribution (Grosz and Sidner, 1986; Reichman, 1985; Cohen, 1984). For example, the word okay can be used to convey a ‘satisfactory’ evaluation of some entity in the discourse (the movie was okay); as a backchannel in a dialogue to indicate that one interlocutor is still attending to another; to convey acknowledgment or agreement; or, in its ‘cue’ use, to start or finish a discourse segment (Jefferson, 1972; Schegloff and Sacks, 1973; Kowtko, 1997; Ward and Tsukahara, 2000). A major question is how speakers indicate and listeners interpret such variation in meaning. From a practical perspective, understanding how speakers and listeners disambiguate cue phra"
P07-1101,J93-3003,1,\N,Missing
P07-1101,J86-3001,0,\N,Missing
P08-1092,N04-1015,0,0.0318901,"em-overlap (BlairGoldensohn et al., 2004b) to cluster the sentences classified as biographical by our classifier, and then select the sentence from each cluster that maximizes the confidence score returned by the classifier as the representative for that cluster. 4.3 Sentence Reordering It is essential for MDS systems in the extraction framework to choose the order in which sentences should be presented in the final summary. Presenting more important information earlier in a summary is a general strategy for most domains, although importance may be difficult to determine reliably. Similar to (Barzilay and Lee, 2004), we automatically learn how to order our biographical sentences by observing the typical order of presentation of information in a particular domain. We observe that our Wikipedia biographies tend to follow a general presentation template, in which birth information is mentioned before death information, information about current professional position and affiliations usually appear early in the biography, and nuclear family members are typically mentioned before more distant relations. Learning how to order information from these biographies however would require that we learn to identify pa"
P08-1092,H01-1065,0,0.0147827,"a corpus of free-text biographies about the same celebrities. are derived from predicate-argument structures deduced from parse trees, and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summaries and then regress numeric features (e.g., the centroid) from each sentence to this score. Barzilay and Lee (2004) use HMMs to capture topic shift within a particular domain; sequence of topic shifts then guides the subsequent ordering of sentences within the summary. 7 Discussion and Future Work In this paper, we describe a MDS system for producing"
P08-1092,doddington-etal-2004-automatic,0,0.0157782,"g., bio, education, nationality) and uses binary unigram and bigram lexical and unigram part-of-speech features for classification. Duboue et al. (2003) also address the problem of learning content selection rules for biography. They learn rules from two corpora, a semi-structured corpus with lists of biographical facts about show business celebrities and a corpus of free-text biographies about the same celebrities. are derived from predicate-argument structures deduced from parse trees, and semantic features are the set of biography-related relations and events defined in the ACE guidelines (Doddington et al., 2004). Sentences containing kernel facts are ranked using probabilities estimated from a corpus of manually created biographies, including Wikipedia, to estimate the conditional distribution of relevant material given a kernel fact and a background corpus. The problem of ordering sentences and preserving coherence in MDS is addressed by Barzilay et al. (2001), who combine chronological ordering of events with cohesion metrics. SVM regression has recently been used by (Li et al., 2007) for sentence ranking for general MDS. The authors calculated a similarity score for each sentence to the human summ"
P08-1092,W03-1016,0,0.0607481,"Missing"
P08-1092,H05-1015,1,0.922764,"Missing"
P08-1092,C00-1072,0,0.138779,"Missing"
P08-1092,N03-1020,0,0.0566295,"Missing"
P08-1092,N03-2024,0,0.0637919,"Missing"
P08-1092,W04-3256,0,0.868188,"biographies. Finally, the first reference to the target person in the initial sentence in the reordering is rewritten using the longest coreference in our hypothesis sentences which contains the target’s full name. We then trim the output to a threshold to produce a biography of a certain length for evaluation against the DUC2004 systems. 3 Training Data One of the difficulties inherent in automatic biography generation is the lack of training data. One might collect training data by manually annotating a suitable corpus containing biographical and nonbiographical data about a person, as in (Zhou et al., 2004). However, such annotation is labor intensive. To avoid this problem, we adopt an unsupervised approach. We use Wikipedia biographies as our corpus of ’biographical’ sentences. We collect our ‘nonbiographical’ sentences from the English newswire documents in the TDT4 corpus.1 While each corpus 1 http://projects.ldc.upenn.edu/TDT4 808 may contain positive and negative examples, we assume that most sentences in Wikipedia biographies are biographical and that the majority of TDT4 sentences are non-biographical. 3.1 Constructing the Biographical Corpus To automatically collect our biographical sen"
P08-2043,N06-2031,0,\N,Missing
P08-2043,P07-1102,0,\N,Missing
P11-2020,W09-3936,1,0.649409,"ork has proposed new metrics of synchrony and convergence (Edlund et al., 2009) and measures of similarity at a more local level (Heldner et al., 2010). While a number of dimensions of potential entrainment have been studied in the literature, entrainment in turn-taking behaviors has received little attention. In this paper we examine entrainment in a novel turn-taking dimension: backchannelpreceding cues (BPC)s.1 Backchannels are short segments of speech uttered to signal continued interest and understanding without taking the floor (Schegloff, 1982). In a study of the Columbia Games Corpus, Gravano and Hirschberg (2009; 2011) identify five speech phenomena that are significantly correlated with speech followed by backchannels. However, they also note that individual speakers produced different combinations of these cues and varied the way cues were expressed. In our work, we look for evidence that speaker pairs negotiate the choice of such cues and their realizations in a conversation – that is, they entrain to one another in their choice and production of such cues. We test for evidence both at the global and at the local level. In conversation, dialogue partners often become more similar to each other. Th"
P11-2020,P08-2043,1,0.528885,"that a backchannel is appropriate. We term these cues backchannel-preceding cues (BPC)s, and examine the Columbia Games Corpus for evidence of entrainment on such cues. Entrainment, the phenomenon of dialogue partners becoming more similar to each other, is widely believed to be crucial to conversation quality and success. Our results show that speaking partners entrain on BPCs; that is, they tend to use similar sets of BPCs; this similarity increases over the course of a dialogue; and this similarity is associated with measures of dialogue coordination and task success. 1 Introduction 2006; Nenkova et al., 2008). That is, interlocutors who entrain achieve better communication. However, the question of how best to measure this phenomenon has not been well established. Most research has examined similarity of behavior over a conversation, or has compared similarity in early and later phases of a conversation; more recent work has proposed new metrics of synchrony and convergence (Edlund et al., 2009) and measures of similarity at a more local level (Heldner et al., 2010). While a number of dimensions of potential entrainment have been studied in the literature, entrainment in turn-taking behaviors has"
P11-2020,N06-2031,0,0.0508442,"k for evidence that speaker pairs negotiate the choice of such cues and their realizations in a conversation – that is, they entrain to one another in their choice and production of such cues. We test for evidence both at the global and at the local level. In conversation, dialogue partners often become more similar to each other. This phenomenon, known in the literature as entrainment, alignment, accommodation, or adaptation has been found to occur along many acoustic, prosodic, syntactic and lexical dimensions in both human-human interactions (Brennan and Clark, 1996; Coulston et al., 2002; Reitter et al., 2006; Ward and Litman, 2007; Niederhoffer and Pennebaker, 2002; Ward and Mamidipally, 2008; Buder et al., 2010) and humancomputer interactions (Brennan, 1996; Bell et al., 2000; Stoyanchev and Stent, 2009; Bell et al., 2003) 1 Prior studies termed cues that precede backchannels, backand has been associated with dialogue success and channel-inviting cues. To avoid suggesting that such cues are a naturalness (Pickering and Garrod, 2004; Goleman, speaker’s conscious decision, we adopt a more neutral term. 113 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shor"
P11-2020,N09-2048,0,0.0616994,"e test for evidence both at the global and at the local level. In conversation, dialogue partners often become more similar to each other. This phenomenon, known in the literature as entrainment, alignment, accommodation, or adaptation has been found to occur along many acoustic, prosodic, syntactic and lexical dimensions in both human-human interactions (Brennan and Clark, 1996; Coulston et al., 2002; Reitter et al., 2006; Ward and Litman, 2007; Niederhoffer and Pennebaker, 2002; Ward and Mamidipally, 2008; Buder et al., 2010) and humancomputer interactions (Brennan, 1996; Bell et al., 2000; Stoyanchev and Stent, 2009; Bell et al., 2003) 1 Prior studies termed cues that precede backchannels, backand has been associated with dialogue success and channel-inviting cues. To avoid suggesting that such cues are a naturalness (Pickering and Garrod, 2004; Goleman, speaker’s conscious decision, we adopt a more neutral term. 113 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 113–117, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics In Section 2, we describe the Columbia Games Corpus, on which the current analysis was cond"
P14-5009,J09-3007,0,0.41745,"Missing"
P14-5009,W02-1503,0,0.0681326,"swers to produce a PC-PATR grammar (McConnel, 1995). The LinGO Grammar Matrix (Bender et al., 2002) is a similar tool developed for HPSG that uses a type hierarchy to represent cross-linguistic generalizations. The most commonly used resource for formally documenting semantics across languages is FrameNet (Filmore et al., 2003). FrameNets have been developed for many languages, including Spanish, Japanese, and Portuguese. Most start with English FrameNet and adapt it for the new language; a large portion of the frames end up being substantially the same across languages (Baker, 2008). ParSem (Butt et al., 2002) is a Vignette Semantics and VigNet To interpret input text, WordsEye uses a lexical resource called VigNet (Coyne et al., 2011a). VigNet is inspired by and based on FrameNet (Baker et al., 1998), a resource for lexical semantics. In FrameNet, lexical items are grouped together in frames according to their shared semantic structure. Every frame contains a number of frame elements (semantic roles) which are participants in this structure. The English FrameNet defines the mapping between syntax and semantics for a lexical item by providing lists of valence patterns that map syntactic functions t"
P14-5009,W11-0905,1,0.947055,"erring to objects. The system assembles scenes from a library of 2,500 3D objects and 10,000 images tied to an English lexicon of about 15,000 nouns. The system includes a user interface where the user can type simple sentences that are processed to produce a 3D scene. The user can then modify the text to refine the scene. In addition, individual objects and their parts can be selected and highlighted with a bounding box to focus attention. Several thousand real-world people have used WordsEye online (http://www.wordseye.com). It has also been used as a tool in education, to enhance literacy (Coyne et al., 2011b). In this paper, we describe how we are using WordsEye to create a comprehensive tool for field linguistics. Related Work One of the most widely-used computer toolkits for field linguistics is SIL Fieldworks. FieldWorks is a collection of software tools; the most relevant for our research is FLEx, Fieldworks Language Explorer. FLEx includes tools for eliciting and recording lexical information, dictionary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguist-friendly” morphological parser (Black and Si"
P14-5009,P98-1013,0,0.0594057,"generalizations. The most commonly used resource for formally documenting semantics across languages is FrameNet (Filmore et al., 2003). FrameNets have been developed for many languages, including Spanish, Japanese, and Portuguese. Most start with English FrameNet and adapt it for the new language; a large portion of the frames end up being substantially the same across languages (Baker, 2008). ParSem (Butt et al., 2002) is a Vignette Semantics and VigNet To interpret input text, WordsEye uses a lexical resource called VigNet (Coyne et al., 2011a). VigNet is inspired by and based on FrameNet (Baker et al., 1998), a resource for lexical semantics. In FrameNet, lexical items are grouped together in frames according to their shared semantic structure. Every frame contains a number of frame elements (semantic roles) which are participants in this structure. The English FrameNet defines the mapping between syntax and semantics for a lexical item by providing lists of valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in two ways in order to capture “graphical semantics’,’ the knowledge needed to generate graphical scenes from language. First, graphical semantics are a"
P14-5009,C98-1013,0,\N,Missing
P86-1021,H89-1033,0,0.0348377,"Missing"
P87-1023,P82-1005,0,0.0434929,"Missing"
P87-1023,J86-3001,0,0.235995,"Missing"
P87-1023,P84-1085,0,0.0795882,"Missing"
P87-1023,P86-1021,1,\N,Missing
P87-1023,P84-1055,0,\N,Missing
P88-1023,P83-1007,0,0.0169786,"can test two objects of that type for co-designation. For example, for purposes of reference or accenting we may want to treat &apos;street&apos; and &apos;avenue&apos; as similar. (defun d i s c - s e g - r o t a r y (list (make-sentence "" Y o u &apos; l l "" ""come"" "" t o "" (make-np-constil;uenl; &apos; ( "" r o t a r y "" ) :article :indefinite)) (make-conjunction-sentence (make-sentence ""Go"" ( r o t a r y - a n g l e - a m o u n t (get-info act &apos;rotary-angle)) ""eay . . . . around"" (make-anaphora n i l "" i t "" ) ) (make-sentence ""l;nrn"" ""onto"" (make-street-constituent Each DS has associated with it a focus space. Following [2], a focus space consists of a set of FORWARD-LOOKING CENTERS, potentially salient discourse entities and modifiers. Focus spaces are pushed and popped from the FOCUS STACK as the description is generated, according to the relationships among their associated DS&apos;s. As an example, the generator for the rotary act appears in figure 2. This schema generates two sentences, second of which is a conjunction. One slot in this schema is taken by an NP constituent for the rotary. The m a k e - n p - c o n s t i t u e n t routine handles agreement between the article and the noun. A second slot is filled"
P88-1023,P84-1065,0,0.0303345,"Missing"
P88-1023,J86-3001,0,0.104701,"screws&apos;, as well as the DS&apos;s DSP. The accessibilityof an entity m as, for pronominal reference m depends upon the depth of its containing focus space. Deeper spaces are Discourse and Intonation less accessible. Entitiesm a y be made inaccessibleif their focus space is popped from the stack. The theoreticalfoundations of the current work are three: Grosz and Sidner&apos;s theory of discourse structure, Pierrehumbert&apos;s theory of English intonation, and Hirschberg I n t o n a t i o n a l and Pierrehumbert&apos;s studies of intonation and discourse. t a t i o n Modeling Discourse Structure Grosz and Sidner[9] propose that discourse be understood in terms of the purposes that underly it (INTENTIONAL STRUCTURE) and the entities and attributes which are salient during it (ATTENTIONALSTRUCTURE).Ill this account, discourses are analyzed as hierarchies of segments, each of which has an underlying D i s c o u r s e S e g m e n t P u r p o s e (DSP) intended by the speaker. All DSPs contribute to the overall D i s c o u r s e P u r p o s e (DP) of the discourse. For example, a discourse might have as its DP something like &apos;intend that Hearer put together an air compressor&apos;, while individual segments might"
P88-1023,P86-1021,1,\N,Missing
P93-1007,H92-1003,0,0.108333,"Missing"
P93-1007,P92-1008,0,0.355959,"grammars, case frame grammars, pattern matching and deterministic parsers. Even if all words in a disfluent segment are correctly recognized, failure to detect a disfluency may lead to interpretation errors during subsequent processing, as in Example (3). 1Thepresence of a word fragment in examples is indicated by the diacritic &apos;-&apos;. Self-corrected portions of the utterance appear in boldface. All examples in this paper are drawn from the ATIScorpus described below. Recognition output shown in Example (2) is from the system described in (Lee et al., 1990). Recently, Shriberg et al. (1992) and Bear et al. (1992) have proposed a two-stage method for processing repairs. In the first stage, lexical pattern 46 matching rules operating on orthographic transcriptions would be used to retrieve candidate repair utterances. In the second, syntactic, semantic, and acoustic information would filter true repairs from false positives found by the pattern matcher. Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al., 1992): 309 of 406 utterance containing &apos;nontrivial&apos; repairs in their 10,718 utterance corpus were correctly identified, while 191 fluent utteranc"
P93-1007,H92-1085,0,0.110974,"Ns), network-based semantic grammars, case frame grammars, pattern matching and deterministic parsers. Even if all words in a disfluent segment are correctly recognized, failure to detect a disfluency may lead to interpretation errors during subsequent processing, as in Example (3). 1Thepresence of a word fragment in examples is indicated by the diacritic &apos;-&apos;. Self-corrected portions of the utterance appear in boldface. All examples in this paper are drawn from the ATIScorpus described below. Recognition output shown in Example (2) is from the system described in (Lee et al., 1990). Recently, Shriberg et al. (1992) and Bear et al. (1992) have proposed a two-stage method for processing repairs. In the first stage, lexical pattern 46 matching rules operating on orthographic transcriptions would be used to retrieve candidate repair utterances. In the second, syntactic, semantic, and acoustic information would filter true repairs from false positives found by the pattern matcher. Results of testing the first stage of this model, the lexical pattern matcher, are reported in (Bear et al., 1992): 309 of 406 utterance containing &apos;nontrivial&apos; repairs in their 10,718 utterance corpus were correctly identified, wh"
P93-1007,P83-1019,0,0.748425,"Missing"
P93-1007,J86-1002,0,\N,Missing
P93-1007,J80-2003,0,\N,Missing
P93-1007,J83-3003,0,\N,Missing
P93-1007,J83-3001,0,\N,Missing
P93-1007,J90-3003,0,\N,Missing
P93-1007,H90-1021,0,\N,Missing
P96-1038,J86-3001,0,0.978131,"tional linguistics that discourse structure plays an important role in Natural Language Understanding tasks such as identifying speaker intentions and resolving anaphoric reference. Previous research has found *The second author was partially supported by NSF Grants No. IRI-90-09018, No. IRI-93-08173, and No. CDA-94-01024 at Harvard University and by AT&T Bell Laboratories. 286 that discourse structural information can be inferred from orthographic cues in text, such as paragraphing and punctuation; from linguistic cues in text or speech, such as c u e PHI~.ASES1 (Cohen, 1984; Reichman, 1985; Grosz and Sidner, 1986; Passonneau and Litman, 1993; Passonneau and Litman, to appear) and other lexical cues (Hinkelman and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incor"
P96-1038,P89-1026,0,0.0174833,"h as identifying speaker intentions and resolving anaphoric reference. Previous research has found *The second author was partially supported by NSF Grants No. IRI-90-09018, No. IRI-93-08173, and No. CDA-94-01024 at Harvard University and by AT&T Bell Laboratories. 286 that discourse structural information can be inferred from orthographic cues in text, such as paragraphing and punctuation; from linguistic cues in text or speech, such as c u e PHI~.ASES1 (Cohen, 1984; Reichman, 1985; Grosz and Sidner, 1986; Passonneau and Litman, 1993; Passonneau and Litman, to appear) and other lexical cues (Hinkelman and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst,"
P96-1038,H92-1089,1,0.817265,"ed notion of discourse structure. With few exceptions, they rely on intuitive analyses of topic structure; operational definitions of discourse-level properties (e.g., interpreting paragraph breaks as discourse segment boundaries); or &apos;theory-neutral&apos; discourse segmentations, where subjects are given instructions to simply mark changes in topic. Recent studies have focused on the question of whether discourse structure itself can be empirically determined in a reliable manner, a prerequisite to investigating linguistic cues to its existence. An intention-based theory of discourse was used in (Hirschberg and Grosz, 1992; Grosz and Hirschberg, 1992) to identify intonational correlates of discourse structure in news stories read by a professional speaker. Discourse structural elements were determined by experts in the Grosz and Sidner (1986) theory of discourse structure, based on either text alone or text and speech. This study revealed strong correlations of aspects of pitch range, amplitude, and timing with features of global and local structure for both segmentation methods. Passonneau and Litman (to appear) analyzed correlations of pause, as well as cue phrases and referential relations, with discourse st"
P96-1038,M91-1036,0,0.010816,"ferring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwafiska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on prosodic/acoustic cues to discourse structure have investigated the contributions of features such as pitch range, pausal duration, amplitude, speaking rate, and intonational contour to signaling topic change. Variation in pitch range has often been seen as conveying &apos;topic structure&apos; in discourse. Brown et al. (1980) found that subjects typically started new topics relatively high in their pitch range and finished topics by compressing their range. Silverman (1987) found that manipulation of pitch range alone, or in conjunction with pa"
P96-1038,C90-2044,1,0.838868,"punctuation; from linguistic cues in text or speech, such as c u e PHI~.ASES1 (Cohen, 1984; Reichman, 1985; Grosz and Sidner, 1986; Passonneau and Litman, 1993; Passonneau and Litman, to appear) and other lexical cues (Hinkelman and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwafiska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on prosodic/acoustic cues to discourse structure have investigated the contributions of features such as pitch range, pausal duration, amplitude, speaking rate, and intonational contour to signaling topic change. Variation in pitch range has often been seen as conveying &apos;topic struct"
P96-1038,J91-1002,0,0.16333,"n and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwafiska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on prosodic/acoustic cues to discourse structure have investigated the contributions of features such as pitch range, pausal duration, amplitude, speaking rate, and intonational contour to signaling topic change. Variation in pitch range has often been seen as conveying &apos;topic structure&apos; in discourse. Brown et al. (1980) found that subjects typically started new topics relatively high in their pitch range and finished topics by compressing their range. Silverman (1987) found that manipulation of pitch"
P96-1038,P94-1050,0,0.143061,"y, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwafiska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on prosodic/acoustic cues to discourse structure have investigated the contributions of features such as pitch range, pausal duration, amplitude, speaking rate, and intonational contour to signaling topic change. Variation in pitch range has often been seen as conveying &apos;topic structure&apos; in discourse. Brown et al. (1980) found that subjects typically started new topics relatively high in their pitch range and finished topics by compressing their range. Silverman (1987) found that manipulation of pitch range alone, or in conjunction with pausal duration between utteran"
P96-1038,P88-1014,0,0.0488675,"was partially supported by NSF Grants No. IRI-90-09018, No. IRI-93-08173, and No. CDA-94-01024 at Harvard University and by AT&T Bell Laboratories. 286 that discourse structural information can be inferred from orthographic cues in text, such as paragraphing and punctuation; from linguistic cues in text or speech, such as c u e PHI~.ASES1 (Cohen, 1984; Reichman, 1985; Grosz and Sidner, 1986; Passonneau and Litman, 1993; Passonneau and Litman, to appear) and other lexical cues (Hinkelman and Allen, 1989); from variation in referring expressions (Linde, 1979; Levy, 1984; Grosz and Sidner, 1986; Webber, 1988; Song and Cohen, 1991; Passonneau and Litman, 1993), tense, and aspect (Schubert and Hwang, 1990; Song and Cohen, 1991); from knowledge of the domain, especially for taskoriented discourses (Grosz, 1978); and from speaker intentions (Carberry, 1990; Litman and Hirschberg, 1990; Lochbaum, 1994). Recent methods for automatic recognition of discourse structure from text have incorporated thesaurus-based and other information retrieval techniques to identify changes in topic (Morris and Hirst, 1991; Yarowsky, 1991; Iwafiska et al., 1991; Hearst, 1994; Reynar, 1994). Parallel investigations on pro"
P96-1038,H90-1008,0,\N,Missing
P96-1038,J96-2004,0,\N,Missing
P96-1038,P93-1020,0,\N,Missing
P96-1038,P84-1055,0,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
S17-1013,W13-4806,0,0.0279294,"large-scale (120-hour) corpus of deceptive and non-deceptive dialogues collected using a semi-structured inter1 The interview questions can be found here: http:// tinyurl.com/lzfa8zl 110 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 110–114, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics domain. This work draws upon the body of research on short-text semantic similarity (e.g. (Mihalcea et al., 2006; Kenter and de Rijke, 2015; Oliva et al., 2011)). It is also related to work on topic segmentation (e.g. (Cardoso et al., 2013; Dias et al., 2007) ), however here we focus on matching conversational turns to a fixed set of possible topics. While this work is done in support of our ongoing work on deception detection using speech and text-based features, we believe that our approach could be applied to other spontaneous transcribed speech or text corpora which were collected with some constraints on topics. 2 fined as a maximal sequence of IPUs from a single speaker without any interlocutor speech that is not a backchannel (a simple acknowledgment that is not an attempt to take the turn). For this work, we compiled 40"
S17-1013,W04-1013,0,0.00917584,"ne As a baseline for matching the 24 questions interviewers were instructed to ask with interviewer turns, we performed a simple two-pass question matching procedure for exact string matches between written questions and the transcripts. In the first pass, we searched for exact matches of strings with punctuation and spacing removed. With the remaining unmatched questions, we then performed another round of matching, with the transcript lemmatized and with filler words removed, to identify very close though not exact matches. 3.2 ROUGE ROUGE (Recall-Oriented Understudy for Gisting Evaluation)(Lin, 2004) is a package designed to evaluate computer-generated summaries against a human-written baseline using a simple n-gram comparison to find precision, recall, and f-score for each machine-human summary comparison. Using ROUGE, we evaluated matches for questions which had not been detected by the baseline. We created a ROUGE task for each unmatched question. For each task, the original question was used as the reference text. We then tested each interviewer turn in the conversation against the reference, using bi-gram matching. We thus matched the turn receiving the highest similarity score to th"
W01-1610,N01-1027,1,0.76379,"Missing"
W01-1610,P98-1122,0,0.0323677,"eir intended actions were carried out correctly or not, in particular when the dialogue system does not give appropriate feedback about its internal representation at the right moment. In addition, users' corrections may miss their goal, because corrections themselves are more dicult for the system to recognize and interpret correctly, which may lead to so-called cyclic (or spiral) errors. That corrections are dicult for ASR systems is generally explained by the fact that they tend to be hyperarticulated |higher, louder, longer . . . than other turns (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999; Shimojima et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a"
W01-1610,A00-2029,1,0.740134,"a et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a good predictor of misrecognitions (Litman et al., 2000; Hirschberg et al., 2000), is also useful to correctly classify corrections and aware sites. In this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics. More in particular, we address the question whether there is much variance in the way people react to system errors, and if so, to what extent this variance can be explained on the basis of particular properties of the dialogue system. In the following section we rst provide details on the TOOT corpus that we used for our analyses. Then we give information on the labels for corrections a"
W01-1610,P01-1048,1,0.191728,"which may lead to so-called cyclic (or spiral) errors. That corrections are dicult for ASR systems is generally explained by the fact that they tend to be hyperarticulated |higher, louder, longer . . . than other turns (Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999; Shimojima et al., 1999), where ASR models are not well adapted to handle this special speaking style. The current paper focuses on user corrections, and looks at places where people rst become aware of a system problem (aware sites""). In other papers (Swerts et al., 2000; Hirschberg et al., 2001; Litman et al., 2001), we have already given some descriptive statistics on corrections and aware sites and we have been looking at methods to automatically predict these two utterance categories. One of our major ndings is that prosody, which had already been shown to be a good predictor of misrecognitions (Litman et al., 2000; Hirschberg et al., 2000), is also useful to correctly classify corrections and aware sites. In this paper, we will elaborate more on the exact labeling scheme we used, and add further descriptive statistics. More in particular, we address the question whether there is much variance in the"
W01-1610,P99-1024,0,0.0369287,"ses. Then we give information on the labels for corrections and aware sites, and on the actual labeling procedure. The next section gives the results of some descriptive statistics on properties of corrections and aware sites and on their distributions. We will end the paper with a general discussion of our ndings. 2 The data 2.1 The TOOT corpus Our corpus consists of dialogues between human subjects and TOOT, a spoken dialogue system that allows access to train information from the web via telephone. TOOT was collected to study variations in dialogue strategy and in user-adapted interaction (Litman and Pan, 1999). It is implemented using an IVR (interactive voice response) platform developed at AT&T, combining ASR and textto-speech with a phone interface (Kamm et al., 1997). The system's speech recognizer is a speaker-independent hidden Markov model system with context-dependent phone models for telephone speech and constrained grammars de ning vocabulary at any dialogue state. The platform supports barge-in. Subjects performed four tasks with one of several versions of the system that di ered in terms of locus of initiative (system, user, or mixed), con rmation strategy (explicit, implicit, or none),"
W01-1610,C98-1117,0,\N,Missing
W09-0807,N09-1045,1,0.812703,"Missing"
W09-3936,J03-4003,0,0.00550832,"simplified POS tags (N, V, Adj, Adv, Other) of w and of the IPU-final bigram; number of words in the IPU; a binary flag indicating if w is a word fragment; size and type of the biggest (bp) and smallest (sp) phrase that end in w; binary flags indicating if each of bp and sp is a major phrase (NP, VP, PP, ADJP, ADVP); binary flags indicating if w is the head of each of bp and sp. We chose these features in order to capture as much lexical and syntactic information as possible from the transcripts. The syntactic features were computed using two different parsers: the Collins statistical parser (Collins, 2003) and CASS, a partial parser especially designed for use with noisy text (Abney, 1996). We experimented with the learners listed in Table 2, using the implementations provided in the W EKA ML toolkit (Witten and Frank, 2000). Table 2 shows the accuracy of the majority-class baseline and of each classifier, using 10-fold cross validation on the 400 training data points, and the mean pairwise agreement by the three human labelers. The linear-kernel support-vector-machine (SVM) classifier achieves the highest accuracy, significantly outperforming the baseline, and approaching the mean agreement of"
W09-3936,C08-2003,0,\N,Missing
W09-3936,W08-0101,0,\N,Missing
W11-2018,E09-1004,0,0.0171781,", Mean, Median, SDev, MAS Min, Max, Mean, SDev Prosodic Events Pitch accents, intermediate phrase, and intonational boundaries. Table 1: Feature Sets. RAP: Relative Average Perturbation. PPQ5: five-point Period Perturbation Quotient. APQn: n-point Amplitude Perturbation Quotient. NHR: Noise-to-Harmonics Ratio. MAS: Mean Absolute Slope. own emotional state. Its pleasantness (EE) score indicates the negative or positive valence of a word, rated on a scale from 1 to 3. For example, “abandon” scores 1.0, implying a fairly low level of pleasC(t, u) |U | V (t, u) = ∗log P antness. A previous study (Agarwal et al., 2009) C(v, u) u(t) ∗ (1 − |M eanLOI|) notes that one of the advantages of this dictionary Here, the Mean LOI score ranging from (0,1) is is that it has different scores for various forms of a the label of each utterance. Instead of summing root word. For example, the words “affect” and “afthe u(t) scores directly, we now assign a weight to fection” have very different meanings; if they were each utterance. The weight is (1 − |M eanLOI|) in given the same score, the lexical affect quantificaour task. The overall IDF score of words important tion might not be discriminative. To calculate an to identi"
W11-2018,P96-1038,1,0.340624,"le 1). Prosodic Event Features To examine the contribution of higher-level prosodic events, we have also experimented with AuToBI (Rosenberg, 2010) to automatically detect pitch accents, word boundaries, intermediate phrase boundaries, and intonational boundaries in utterances. AuToBI requires annotated word boundary information; since we do not have hand-annotated boundaries, we use the Penn Phonetics Lab Forced Aligner (Yuan and Liberman, 2008) to align each utterance with its transcription. We use AuToBI’s models, which were trained on the spontaneous speech Boston Directions Corpus (BDC) (Hirschberg and Nakatani, 1996), to identify prosodic events in our corpus. 3.3 Fusion Learning Approaches Assuming that our various lexical, acoustic and prosodic feature streams are informative to some extent when tested separately, we want to combine information from the streams in different domains to improve prediction. We experimented with several approaches, including Bag-of-Features, Sum Rule combination, Hierarchical Fusion, and a new approach. We present here results of each on our LOI prediction task. In the Bag-of-Features approach, a simple classification method includes all features in a single classifier. A p"
W11-2018,W00-1308,0,0.0736547,"Missing"
W11-2018,C10-1129,1,0.916195,"Missing"
W12-2103,H05-1045,0,0.0742181,"Missing"
W12-2103,P94-1013,0,0.615351,"Missing"
W12-2103,P95-1026,0,0.660571,"Missing"
W12-2103,P07-1055,0,0.0303979,"Missing"
W12-2103,W02-1011,0,0.0121785,"Missing"
W12-2103,P08-1068,0,0.107255,"Missing"
W13-4020,weiss-etal-2008-performance,0,\N,Missing
W13-4020,W09-3906,0,\N,Missing
W13-4020,N04-3002,0,\N,Missing
W13-4020,N03-1033,0,\N,Missing
W13-4020,W10-4355,0,\N,Missing
W13-4021,W09-3915,0,0.0722433,"Missing"
W13-4021,P11-4015,0,0.0373011,"Missing"
W13-4021,P06-2085,0,0.0231904,"cally use simple rejection (“I’m sorry. I didn’t understand you.”) when they have low confidence in a recognition hypothesis and explicit or implicit confirmation when confidence scores 1 This work was partially funded by DARPA HR0011-12C-0016 as a Columbia University subcontract to SRI International. 137 Proceedings of the SIGDIAL 2013 Conference, pages 137–141, c Metz, France, 22-24 August 2013. 2013 Association for Computational Linguistics are higher. Machine learning approaches have been successfully employed to determine dialogue strategies (Bohus and Rudnicky, 2005; Bohus et al., 2006; Rieser and Lemon, 2006), such as when to provide help, repeat a previous prompt, or move on to the next prompt. Reiser and Lemon (2006) use machine learning to determine an optimal clarification strategy in multimodal dialogue. Komatani et al. (2006) propose a method to generate a help message based on perceived user expertise. Corpus studies on human clarifications in dialogue indicate that users ask task-related questions and provide feedback confirming their hypothesis instead of giving direct indication of their misunderstanding (Skantze, 2005; Williams and Young, 2004; Koulouri and Lauria, 2009). In our work, w"
W13-4021,N03-1033,0,0.0634867,"Missing"
W14-2202,P98-1013,0,0.13163,"spoken by cul4 Elicitation with WELT WELT organizes elicitation sessions around a set of 3D scenes, which are created by inputting English text into WordsEye. Scenes can be imported and exported between sessions, so that useful scenes can be reused and data compared. WELT also provides tools for recording audio (which is automatically synced with open scenes), textual descriptions, glosses, and notes during a session. Screenshots are included in Figure 2. 4.1 Cultural Adaptation of VigNet To interpret input text, WordsEye uses VigNet (Coyne et al., 2011), a lexical resource based on FrameNet (Baker et al., 1998). As in FrameNet, lexical items are grouped in frames according to shared semantic structure. A frame contains a set of frame elements (semantic roles). FrameNet defines the mapping between syntax and semantics for a lexical item with valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in order to capture “graphical semantics”, a set of graphical constraints representing the position, orientation, size, color, texture, and poses of objects in the scene, 8 Figure 2: Screenshots of WELT elicitation interfaces people, it would instead invoke a woman sitting on"
W14-2202,I11-2002,0,0.0302543,"ary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguistfriendly” morphological parser (Black and Simons, 2006), which uses an underlying model of morphology familiar to linguists, is fully integrated into lexicon development and interlinear text analysis, and produces a human-readable grammar sketch as well as a machine-interpretable parser. The morphological parser is constructed “stealthily” in the background, and can help a linguist by predicting glosses for interlinear texts. Linguist’s Assistant (Beale, 2011) provides a corpus of semantic representations for linguists to use as a guide for elicitation. After eliciting the language data, a linguist writes rules translating these semantic representations into surface forms. The result is a description of the language that can be used to generate text from documents that have been converted into the semantic representation. Linguists are encouraged to collect their own elicitations and naturally occurring texts and translate them into the semantic representation. The LinGO Grammar Matrix (Bender et al., 2002) facilitates formal modeling of syntax by"
W14-2202,C12-2013,0,0.0236747,"ut knowledge of specific computational formalisms. This is similar to the way FLEx allows linguists to create a formal model of morphology while also documenting the lexicon of a language and glossing interlinear texts. Computational tools for field linguistics fall into two categories: tools for native speakers to use directly, without substantial linguist intervention, and tools for field linguists to use. Tools intended for native speakers include the PAWS starter kit (Black and Black, 2009), which uses the answers to a series of guided questions to produce a draft of a grammar. Similarly, Bird and Chiang (2012) describe a simplified workflow and supporting MT software that lets native speakers produce useable documentation of their language on their own. One of the most widely-used toolkits in the latter category is SIL FieldWorks (SIL FieldWorks, 2014), or specifically, FieldWorks Language Explorer (FLEx). FLEx includes tools for eliciting and recording lexical information, dictionary development, interlinearization of texts, analysis of discourse features, and morphological analysis. An important part of FLEx is its “linguistfriendly” morphological parser (Black and Simons, 2006), which uses an un"
W14-2202,J09-3007,0,0.251859,"Missing"
W14-2202,W11-0905,1,0.872107,"tion of other languages that are similar linguistically or spoken by cul4 Elicitation with WELT WELT organizes elicitation sessions around a set of 3D scenes, which are created by inputting English text into WordsEye. Scenes can be imported and exported between sessions, so that useful scenes can be reused and data compared. WELT also provides tools for recording audio (which is automatically synced with open scenes), textual descriptions, glosses, and notes during a session. Screenshots are included in Figure 2. 4.1 Cultural Adaptation of VigNet To interpret input text, WordsEye uses VigNet (Coyne et al., 2011), a lexical resource based on FrameNet (Baker et al., 1998). As in FrameNet, lexical items are grouped in frames according to shared semantic structure. A frame contains a set of frame elements (semantic roles). FrameNet defines the mapping between syntax and semantics for a lexical item with valence patterns that map syntactic functions to frame elements. VigNet extends FrameNet in order to capture “graphical semantics”, a set of graphical constraints representing the position, orientation, size, color, texture, and poses of objects in the scene, 8 Figure 2: Screenshots of WELT elicitation in"
W14-2202,C98-1013,0,\N,Missing
W14-3907,li-etal-2012-mandarin,1,0.84418,"Missing"
W14-3907,W14-3917,0,0.103569,"Missing"
W14-3907,W14-3909,0,0.0388692,"Missing"
W14-3907,W14-3915,0,0.0710818,"Missing"
W14-3907,D13-1084,0,0.207232,"Missing"
W14-3907,W14-3911,1,0.921109,"t exploiting. For instance, the NE lexicons might account for the best results in the NE class in both the Twitter data and the Surprise genre (see Table 4 last row for SPA-EN and second to last for SPAEN Surprise). Most systems showed considerable 67 F-measure 1 0.9 Baseline 0.894 0.892 0.888 0.838 0.8 0.7 0.6 (Jain and Bhat, 2014) (Lin et al., 2014) (Chittaranjan et al., 2014) (King et al., 2014) F-measure (a) MAN-EN Baseline Test1 0.4 Baseline Test2 0.3 0.196 0.2 0.152 0.118 0.1 (Chittaranjan et al., 2014) 0.417 0.360 0.338 0.260 (King et al., 2014) 0.095 0.048 0.044 (Jain and Bhat, 2014) (Elfardy et al., 2014) (Lin et al., 2014) F-measure (b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2 Baseline 1 0.952 0.962 (King et al., 2014) (Lin et al., 2014) 0.975 0.974 0.972 0.977 0.9 0.8 (Jain and Bhat, 2014) (Shrestha, 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (c) NEP-EN F-measure 1 0.9 Baseline 0.8 0.7 0.6 0.703 0.754 0.753 0.783 0.793 0.822 0.634 (Shrestha, 2014) (King et al., 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (Jain and Bhat, 2014) (Lin et al., 2014) (Bar and Dershowitz, 2014) (d) SPA-EN Figure 1: Prediction results on"
W14-3907,W14-3916,0,0.102327,"Missing"
W14-3907,W14-3910,0,0.0496265,"Missing"
W14-3907,C82-1023,0,0.500666,"olumbia.edu pascale@ece.ust.hk ayc2135@columbia.edu Abstract this direction. We define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably. In its spoken form, CS has probably been around ever since different languages first came in contact. Linguists have studied this phenomenon since the mid 1900s. In contrast, the Natural Language Processing (NLP) community has only recently started to pay attention to CS, with the earliest work in this area dating back to Joshi’s theoretical work proposing an approach to parsing CS data (Joshi, 1982) based on the Matrix and Embedded language framework. With the wide-spread use of social media, CS is now being used more and more in written language and thus we are seeing an increase in published papers dealing with CS. We are specifically interested in intrasentential code switched phenomena. As a result of this task, we have successfully created the first set of annotated data for several language pairs with a coherent set of labels across the languages. As the shared task results show, CS poses new research questions that warrant new NLP approaches, and thus we expect to see a significan"
W14-3907,P11-2007,0,0.0996179,"Missing"
W14-3907,N13-1131,0,0.144834,"Missing"
W14-3907,W14-3912,0,0.116613,"Missing"
W14-3907,W15-3116,0,\N,Missing
W14-3907,E14-1001,0,\N,Missing
W14-3907,W15-2902,0,\N,Missing
W14-3907,N15-1109,0,\N,Missing
W14-3907,W15-5936,0,\N,Missing
W14-4331,N03-1033,0,0.0152308,"Missing"
W14-4331,J06-3004,1,\N,Missing
W15-4644,N12-1002,1,0.808889,"ssion. 4 This finding is in line with recent research questioning the ubiquity of entrainment in the syntactic and semantic domains and calling for more refined analyses of entrainment behavior (Healey et al., 2014). As discussed above, negative synchrony may be termed “disentrainment” and interpreted as a distancing behavior. However, its prevalence in cooperative dialogues is an argument for a more neutral interpretation. This can be explored in future work by determining whether negative synchrony is associated with objective and subjective measures of partner engagement and liking, as in (Levitan et al., 2012). Another consistency found across languages is that mean intensity is the only feature to show significant positive synchrony in a plurality of sessions. In English, in fact, it only shows positive synchrony, the only feature to do so; in the other three languages it is more evenly split between instances of positive and negative synchrony. Discussion This analysis explored three kinds of local entrainment on eight features over a total of 58 sessions in four languages. Table 7 summarizes our findings. Out of all this data certain patterns emerge: Negative (complementary) synchrony is more pr"
W15-4644,P08-2043,1,0.90439,"der to attenuate or accentuate social differences, numerous studies have looked for links between entrainment and positive social behavior. Entrainment on various features and at all levels of communication has been linked, respectively, to liking (Chartrand and Bargh, 1999; Street, 1984), positive affect in conversations between “seriously and chronically distressed” married couples discussing a problem in their relationship (Lee et al., 2010), mutual romantic interest in speed dating transcripts (Ireland et al., 2011), cooperation in a prisoner’s dilemma (Manson et al., 2013), task success (Nenkova et al., 2008; Reitter and Moore, 2007; Friedberg et al., 2012; Thomason et al., 2013), and approvalseeking (Natale, 1975; Danescu-Niculescu-Mizil et al., 2012). Given that these social aspects are assumed to be culture-specific, and the fact that research on entrainment has been done mainly on English and other Germanic languages, the types and degree of entrainment in other languages and cultures should be explored. Although there are numerous studies documenting entrainment in different aspects of spoken dialogue in particular languages collected in particular circumstances, it has been difficult to com"
W15-4644,P07-1102,0,0.0198294,"centuate social differences, numerous studies have looked for links between entrainment and positive social behavior. Entrainment on various features and at all levels of communication has been linked, respectively, to liking (Chartrand and Bargh, 1999; Street, 1984), positive affect in conversations between “seriously and chronically distressed” married couples discussing a problem in their relationship (Lee et al., 2010), mutual romantic interest in speed dating transcripts (Ireland et al., 2011), cooperation in a prisoner’s dilemma (Manson et al., 2013), task success (Nenkova et al., 2008; Reitter and Moore, 2007; Friedberg et al., 2012; Thomason et al., 2013), and approvalseeking (Natale, 1975; Danescu-Niculescu-Mizil et al., 2012). Given that these social aspects are assumed to be culture-specific, and the fact that research on entrainment has been done mainly on English and other Germanic languages, the types and degree of entrainment in other languages and cultures should be explored. Although there are numerous studies documenting entrainment in different aspects of spoken dialogue in particular languages collected in particular circumstances, it has been difficult to compare entrainment across l"
W16-2609,D12-1032,0,0.0718744,"Missing"
W16-2609,baroni-bernardini-2004-bootcat,0,0.169719,"system uses multithreading to reduce the overhead of the many HTTP requests required in web data collection. Furthermore all the tools described above use the operating system file system to manage collected documents. As shown in section 4 we have found that using a production level database system is preferable in both performance and scale. Query Generation and Sources Topic and terminology-oriented corpora-building requires robust query generation (similar to our search producer step). It is preferable to fetch a specific subset of the documents available from the search engine. BootCat (Baroni and Bernardini, 2004) randomly generate ngram queries from the unigram seeding model. GrawlTCQ (De Groc et al., 2011) further develops the query generation process by modeling the links between documents, terms and queries. CorpusCollie (Hoogeveen and Pauw, 2011) uses a similar approach but also removes tokens that are considered to be stop-words in other languages. Our system queries only documents from specific sources that are most suitable for our corpus: blogs, forums, twitter and subtitles rather than the entire web. This choice is dictated by the fact that the ASR language modeling and keyword search tasks"
W16-2609,N03-2003,0,0.0727605,"ome a popular task, used for a wide variety of purposes in text and speech processing. However, to date, most of this data collection has been done for English and other High Resource Languages (HRLs). These languages are characterized by having extensive 72 Proceedings of the 10th Web as Corpus Workshop (WAC-X) and the EmpiriST Shared Task, pages 72–81, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics data in different domains (Iyer et al., 1997), particularly when that data closely matches the genre of the available training material and the task at hand (Bulyko et al., 2003). While earlier work focused on English, (Ng et al., 2005) extended this approach to the recognition of Mandarin conversational speech and Schlippe et al 2013 explored the use of web data to perform unsupervised language model adaptation for French Broadcast News using RSS feeds and Twitter data. Creutz et al. (2009) presented an efficient method for selecting queries to extract useful web text for general or user-dependent vocabularies. Most of this research has used perplexity to determine improvement resulting from the addition of web text to the original language model corpus (Bulyko et al"
W16-2609,J06-4003,0,0.0288803,"s in cases where it would be desirable. 6.2 Performance 7 Text Normalization As previously noted, we are collecting web data for the purpose of including it in the language models for ASR that will be used to transcribe data for a spoken keyword search task. Due to the noisy nature of text found on the web, we must clean our collected data to make it appropriate for this task. Our text normalization proceeds in three distinct steps: • Pre-normalization: a first pass in which nonstandard punctuation is standardized; • Sentence segmentation: which is accomplished using the Punkt module of NLTK (Kiss and Strunk, 2006); and • Post normalization: in which sentence-bysentence cleaning of any out-of-language text and standardization of numerals is done. 7.1 Pre-normalization During pre-normalization, we first remove list entries and titles, since those generally are not full sentences. We replace non-standard characters with a standard version: these include ellipses, whitespace, hyphens, and apostrophes. Hyphens and apostrophes are removed as extraneous punctuation, except word-internal cases such as hyphenated words or contractions. Finally, any characters not part of the language’s character set, the Latin"
W16-2609,W14-1303,0,0.0312313,"y via its built in map-reduce component. Using MongoDB provided significant improvements compared to saving documents as text files; for example, in a single task of counting the number of tokens in the entire data set we found that MongoDB was approximately three orders of magnitude faster than using ext4 FS on Ubuntu. By overriding MongoDB internal id field we also solve the issue of duplicates, which we encounter in many sources, especially Twitter data, where tweets are often Raw data that is collected is examined using our language identification multi-classifier, majority vote approach. Lui and Baldwin (2014) showed that using a majority vote over three independent language classifiers consistently outperforms any individual system, so we use the following classifiers: • LingPipe - A language identification classifier built from LingPipe (http://aliasi.com/lingpipe/), and described in Mendels et al. (2015) • TextCat - We implemented the TextCat algorithm (Cavnar et al., 1994) using precomputed counts from the Crubadan Project. (Scannell, 2007) • Google’s Compact Language Detector 2 1 CLD2 is a Nave Bayesian classifier that supports 83 languages. We implemented a Java native interface to the origin"
W16-2609,W14-5315,0,0.0342938,"Missing"
W16-2609,E09-1019,0,0.0243288,"the EmpiriST Shared Task, pages 72–81, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics data in different domains (Iyer et al., 1997), particularly when that data closely matches the genre of the available training material and the task at hand (Bulyko et al., 2003). While earlier work focused on English, (Ng et al., 2005) extended this approach to the recognition of Mandarin conversational speech and Schlippe et al 2013 explored the use of web data to perform unsupervised language model adaptation for French Broadcast News using RSS feeds and Twitter data. Creutz et al. (2009) presented an efficient method for selecting queries to extract useful web text for general or user-dependent vocabularies. Most of this research has used perplexity to determine improvement resulting from the addition of web text to the original language model corpus (Bulyko et al., 2007) although (Sarikaya et al., 2005) have also proposed the use of BLEU scores in augmenting language model training data for Spoken Dialogue Systems. IARPA Babel project and we describe its language resources. In Section 4 we describe the components of our web collection systems. In Section 5 we identify the we"
W16-2609,W06-1208,0,0.0738002,"Missing"
W16-2609,W11-1106,0,0.0441193,"Missing"
W16-2609,D12-1136,0,0.0279168,"Missing"
W16-2609,quasthoff-etal-2006-corpus,0,0.039756,".2. We compute the frequency of each token in the dataset and then remove all tokens shorter than 4 characters or tokens that occur in a standard English word list (SIL, 1999). The primary reason for removing these tokens is to reduce the number of English search results in later steps. We discovered that a query containing an English word is likely to produce mainly English results, even if that word is shared with another language, due to the heavy preponderance of English material on the web. The data for the unigram models is obtained from the Babel program; also from the Leipzig corpora (Quasthoff et al., 2006), a multilingual corpus collected from the web; and from the Crubadan project (Scannell, 2007), another multilingual corpus providing trigram counts for more than 2000 languages and dialects. Our system also supports generating bigram and trigram queries which improves accuracy of the target language results but lowers recall. 4.5 4.2 We use MongoDB, a noSQL document-oriented database system, to store the filtered data. MongoDB allows us to process the data easily via its built in map-reduce component. Using MongoDB provided significant improvements compared to saving documents as text files;"
W16-2609,W14-2712,0,0.0422743,"Missing"
W16-5812,K15-1005,1,0.840177,"to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identified as lang2 are processed by the monolingual lang2 POS tagger. Finally we integrate the POS tags from both monolingual taggers creating the POS tag sequence for the sentence. Figure-1 shows a diagram representing this approach for the MSA-EGY language pair. For MSA-EGY, we used the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. It takes plain Arabic text in Arabic UTF8 encoding or Buckwalter encoding as input and outputs: 1) Class Identification (CI) of the input text to specify whether the tokens are MSA, EGY, as well as other information such as name entity, foreign word, or unknown labels per token. Furthermore, it provides the results with a confidence score; 2)Dialect Classification (DC) of the input text to specify whether it is Egyptian. For SPA-ENG, we trained language models (LM) on English and Spanish data to assign Langu"
W16-5812,N13-1049,0,0.0602647,"Missing"
W16-5812,W14-3902,0,0.166421,"Missing"
W16-5812,W13-2301,0,0.0179214,"notated using the Buckwalter (BW) POS tag set. The BW POS 102 tag set is considered one of the most popular Arabic POS tagsets. It gains its popularity from its use in the Penn Arabic Treebank (PATB) (Maamouri et al., 2004; Alkuhlani et al., 2013). It can be used for tokenized and untokenized Arabic text. The tokenized tags that are used in the PATB are extracted from the untokenized tags. The number of untokenized tags is 485 tags and generated by BAMA (Buckwalter, 2004). Both tokenized and untokenized tags use the same 70 tags and sub-tags such as nominal suffix, ADJ, CONJ, DET, and, NSUFF (Eskander et al., 2013) (Alkuhlani et al., 2013). Combining the sub-tags can form almost 170 morpheme sub-tags such NSUFF FEM SG. This is a very detailed tagset for our purposes and also for cross CS language pair comparison, i.e. in order to compare between trends in the MSA-EGY setting and the SPA-ENG setting. Accordingly, we map the BW tagset which is the output of the MADAMIRA tools to the universal tagset (Petrov et al., 2011). We apply the mapping as follows: 1) Personal, relative, demonstrative, interrogative, and indefinite pronouns are mapped to Pronoun; 2)Acronyms are mapped to Proper Nouns; 3) Complementi"
W16-5812,R15-1033,0,0.132072,"Missing"
W16-5812,J93-2004,0,0.0788385,"rsion of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identi"
W16-5812,pasha-etal-2014-madamira,1,0.874077,"Missing"
W16-5812,W15-5936,0,0.176006,"Missing"
W16-5812,D08-1110,1,0.873698,"a relatively pure monolingual tagger per language variety (MSA or EGY), trained on informal genres for both MSA and EGY. Therefore, we retrained a new version of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. pro"
W16-5812,taule-etal-2008-ancora,0,0.108198,"Missing"
W16-5812,D14-1105,0,0.331016,"m. Typically people who code switch master two (or more) languages: a common first language (lang1) and another prevalent language as a second language (lang2). The languages could be completely distinct such as Mandarin and English, or Hindi and English, or they can be variants of one another such as in the case of Modern Standard Arabic (MSA) and Arabic regional dialects (e.g. Egyptian dialect– EGY). CS is traditionally prevalent in spoken language but with the proliferation of social media such as Facebook, Instagram, and Twitter, CS is becoming ubiquitous in written modalities and genres (Vyas et al., 2014; Danet and Herring, 2007; C´ardenas-Claros and Isharyanti, 2009) CS can be observed in different linguistic levels of representation for different language pairs: phonological, morphological, lexical, syntactic, semantic, and discourse/pragmatic. It may occur within (intra-sentential) or across utterances (inter-sentential). For example, the following Arabic excerpt exhibits both lexical and syntactic CS. The speaker alternates between two variants of Arabic MSA and EGY. Arabic Intra-sentential CS:1 wlkn AjhztnA AljnA}yp lAnhA m$ xyAl Elmy lm tjd wlw mElwmp wAHdp. English Translation: Since o"
W18-1301,S15-1009,1,0.909161,"Missing"
W18-1301,C10-2117,0,0.0331677,"hedging and non-hedging usages of the term. We use these definitions as the basis for the rules in our hedge classifier. This hedging dictionary is divided into relational and propositional hedges. As described in Prokofieva and Hirschberg (2014), relational hedges have to do with the speaker’s relation to the propositional content, while propositional hedges are those that introduce uncertainty into the propositional content itself. Consider the following: (3) I think the ball is blue. (4) The ball is sort of blue. 4 Committed Belief Tagger We employ the committed belief tagger described in Prabhakaran et al. (2010) and as Sytem C in Prabhakaran et al. (2015). This tagger uses a quadratic kernel SVM to train a model using lexical and syntactic features. Tags are assigned at the word level; the tagger identifies tokens denoting the heads of propositions and classifies each proposition as one of four belief types: • Committed belief (CB): the speaker-writer believes the proposition with certainty, e.g. In (3), think is a relational hedge. In (4), sort of is a propositional hedge. Our baseline hedge detector is a simple, dictionary-based one. Using our dictionary of potential hedge terms, we look up the lem"
W18-1301,W10-3002,0,0.0203483,"ple, In (1), around is used as a hedge, but not in (2). (1) She weighs around a hundred pounds. (2) Suddenly she turned around. Related Work Most work on hedge detection has focused on using machine learning models based on annotated data, primarily from the domain of academic writing. The CoNLL-2010 shared task on learning to detect hedges (Farkas et al., 2010) used the BioScope corpus (Vincze et al., 2008) of biomedical abstracts and articles and a Wikipedia corpus annotated for “weasel words.” Most CoNLL-2010 systems approach the task as a sequence labeling problem on the token level (e.g. Tang et al. (2010)); others approached it as a token-by-token classification problem (e.g. Vlachos and Craven (2010)) or as a sentence classification problem (e.g. Clausen (2010)). Our approach is closest to Velldal (2011), a follow-up to CoNLL-2010 which frames the task of identifying hedges as a disambiguation problem in which all potential hedge cues are located and then subsequently disambiguated according to whether they are used as a hedge or not. However, our work differs in that we use a set of manuallyconstructed rules to disambiguate potential hedges rather than a machine learning classifier. Using a"
W18-1301,W08-0606,0,0.234822,"Missing"
W18-1301,W10-3017,0,0.0215463,"ction has focused on using machine learning models based on annotated data, primarily from the domain of academic writing. The CoNLL-2010 shared task on learning to detect hedges (Farkas et al., 2010) used the BioScope corpus (Vincze et al., 2008) of biomedical abstracts and articles and a Wikipedia corpus annotated for “weasel words.” Most CoNLL-2010 systems approach the task as a sequence labeling problem on the token level (e.g. Tang et al. (2010)); others approached it as a token-by-token classification problem (e.g. Vlachos and Craven (2010)) or as a sentence classification problem (e.g. Clausen (2010)). Our approach is closest to Velldal (2011), a follow-up to CoNLL-2010 which frames the task of identifying hedges as a disambiguation problem in which all potential hedge cues are located and then subsequently disambiguated according to whether they are used as a hedge or not. However, our work differs in that we use a set of manuallyconstructed rules to disambiguate potential hedges rather than a machine learning classifier. Using a rule-based rather than machine-learning approach allows us to apply our hedge detection method to Currently there are few corpora annotated for hedging, and the"
W18-1301,W10-3003,0,0.0262215,"nds. (2) Suddenly she turned around. Related Work Most work on hedge detection has focused on using machine learning models based on annotated data, primarily from the domain of academic writing. The CoNLL-2010 shared task on learning to detect hedges (Farkas et al., 2010) used the BioScope corpus (Vincze et al., 2008) of biomedical abstracts and articles and a Wikipedia corpus annotated for “weasel words.” Most CoNLL-2010 systems approach the task as a sequence labeling problem on the token level (e.g. Tang et al. (2010)); others approached it as a token-by-token classification problem (e.g. Vlachos and Craven (2010)) or as a sentence classification problem (e.g. Clausen (2010)). Our approach is closest to Velldal (2011), a follow-up to CoNLL-2010 which frames the task of identifying hedges as a disambiguation problem in which all potential hedge cues are located and then subsequently disambiguated according to whether they are used as a hedge or not. However, our work differs in that we use a set of manuallyconstructed rules to disambiguate potential hedges rather than a machine learning classifier. Using a rule-based rather than machine-learning approach allows us to apply our hedge detection method to"
W18-1301,W10-3001,0,0.165248,"Missing"
W18-3201,W16-5812,1,0.943087,"with the following set of features: 1) word embeddings, 2) prefix and suffix embeddings of one, two and three characters, and 3) four boolean features that encode whether the word is all upper case, all lower case, formatted as a title, or contains any digits. In total, the input space consists of seven embeddings and four boolean features. For the embeddings, we compute word, prefix and suffix lexicons, excluding tokens that appear less than five times in the training set, and then assign a unique integer to each token. We also reserve two integers for the padding and out-of-lexicon symbols. AlGhamdi et al. (2016) tested seven different POS tagging strategies for CS data: four consisted of combinations of monolingual systems and the other three were integrated systems. They tested them on MSA-Egyptian Arabic and English-Spanish. The first three combined strategies consisted of running monolingual POS taggers and language ID taggers in different order and combining the outputs in a single multilingual prediction. The fourth approach involved training an SVM on the output of the monolingual taggers. The three integrated approaches trained a supervised model on a) the Miami Bangor corpus (which contains s"
W18-3201,P16-1231,0,0.062473,"Missing"
W18-3201,K17-3002,0,0.0379086,"Missing"
W18-3201,W14-3902,0,0.111402,"Missing"
W18-3201,W16-5804,0,0.355968,"Missing"
W18-3201,N12-1015,0,0.0314779,"Missing"
W18-3201,R15-1033,0,0.45953,"Missing"
W18-3201,W02-1001,0,0.438028,"Missing"
W18-3201,D08-1110,0,0.645564,"Missing"
W18-3201,J93-2004,0,0.0663065,"Missing"
W18-3201,P13-2017,0,0.0332049,"Missing"
W18-3201,D14-1105,0,0.156767,"Missing"
W18-3201,C14-1110,0,0.0318856,"Missing"
W18-3201,petrov-etal-2012-universal,0,0.364579,"Missing"
W18-3201,W15-5936,0,0.764303,"Missing"
W18-3201,P07-1096,0,0.0372873,"Missing"
W18-3201,silveira-etal-2014-gold,0,0.103992,"Missing"
W18-3219,N18-1127,1,0.877746,"Missing"
W18-3219,L16-1669,1,0.92618,"Missing"
W18-3219,W17-4419,1,0.850073,"e NE tokens in MSA-EGY dataset is higher than the percentage of the NE tokens in ESP-ENG dataset. 4 Approaches In this section, we briefly describe the systems of the participants and discuss their results as well as the final scores. • IIT BHU (Trivedi et al., 2018). They proposed a “new architecture based on gating of character- and word-based representation of a token”. They captured the character and the word representations using a CNN and a bidirectional LSTM, respectively. They also used the Multi-Task Learning on the output layer and transfer the learning to a CRF classifier following Aguilar et al. (2017). Moreover, they fed a gazetteers representation to their model. Figure 3: MSA-EGY Data Annotation (i.e., URL, Punctuation, Number, etc) in addition to named entities. Then, we extracted and prepared all the tweets that contained “ne” for annotation. As we mentioned earlier, the IOB scheme is used as an annotation scheme to identify multiple words as a single named entity. All the URLs, Punctuation and Numbers tags are deterministically converted to “O” tag, while the tweets that include “ne” tags were given to our in-lab annotators for validation and re-annotation if needed. . Quality checks"
W18-3219,W18-3217,0,0.0703829,"are the ones described in Table 3. As stated by (Derczynski et al., 2014), the idea of the Surface Form F1-score is to capture the novel and emerging aspects that are usually encountered in social media data. Those aspects describe a fast-moving language that constantly produces new entities challenging more the recall capabilities of state-of-the-art models than the precision side. They fed the CRF with features from both external and internal resources. Additionally, they incorporated the language identification labels of the datasets from the previous versions of this workshop. • semantic (Geetha et al., 2018). They jointly trained a Bidirectional LSTM with a Conditional Random Fields on the output layer. • BATs (Janke et al., 2018). They used a Conditional Random Fields with multiple features. Some of those features were also used for neural network, but they got better results with the CRF approach. • Fraunhofer FKIE (Claeser et al., 2018). They used a Support Vector Machine (SVM) classifier with a Radial Basis kernel. They handcrafted a lot of features and also included gazetteers. 5.2 Although all the scores reported by the participants outperformed the baselines in both ENGSPA and MSA-EGY lang"
W18-3219,K15-1005,1,0.918003,"Missing"
W18-3219,W18-3213,0,0.0614809,"Missing"
W18-3219,W16-5812,1,0.89568,"Missing"
W18-3219,W18-3212,0,0.0472178,"Missing"
W18-3219,N16-1030,0,0.203476,"Missing"
W18-3219,W15-3116,0,0.0485213,"Missing"
W18-3219,P16-1101,0,0.120177,"Missing"
W18-3219,W16-5805,1,0.914454,"Missing"
W18-3219,W16-5803,0,0.0287502,"Missing"
W18-3219,W18-3220,0,0.0799552,"Missing"
W18-3219,D11-1141,0,0.0941713,"nts as a title. This is an example of what we refer to heterogeneous entity type, mean• Clitic attachment can obscure tokens, e.g. ¢l ¤ wAllh “and-God” or ”swear”. • Clitic attachment can obscure tokens, e.g. Yn¤ wmnY “and-Mona” or ”swear”. 144 N 1 MSA-EGY Samples Buckwalter Encoding:[wAllh]PER OnA HAss bqhr In [ElA’ Ebd AlftAH]PER [wmnY]PER [syf ]PER bytHAkmwA wfy AlqfS Arabic: º®  rhq HFA A ¢l ¤ years (Sang and Meulder, 2003). More recently, however, the focus has drastically moved to social media data due to the great incidence that social networks have in our daily communication (Ritter et al., 2011; Augenstein et al., 2017). The workshop on Noisy User-generated Text (W-NUT) has been a great effort towards the study of named entity recognition on noisy data. In 2016, the organizers focused on named entities from different topics to evaluate the adaptation of models from one topic to another (Strauss et al., 2016). In 2017, the organizers introduced the Surface Form F1-score metric and collected data from multiple social media platforms (Derczynski et al., 2014). The challenge not only lies on the entity types and the social media noisy but also in the distribution of the datasets and the"
W18-3219,W15-2902,0,0.15871,"Missing"
W18-3219,W03-0419,0,0.626032,"Missing"
W18-3219,L16-1655,0,0.0740953,"Missing"
W18-3219,W18-3215,0,0.0617212,"Missing"
W18-3219,W18-3221,0,0.175264,"Missing"
W18-3219,W14-3907,1,0.913831,"Missing"
W18-3219,W18-3214,0,0.187561,"Missing"
W18-3219,D08-1102,1,0.830788,"Missing"
W18-3219,D08-1110,1,0.798454,"Missing"
W19-1607,P14-5010,0,0.00330562,"E=&quot;Figure&quot; arg=&quot;figure&quot;/&gt; &lt;/output&gt; &lt;/vignette&gt; 5 Using SpatialNet for Text-to-Scene Generation SpatialNet can be used in conjunction with the graphics generation component of the WordsEye text-to-scene system to produce a 3D scene from a spatial description which can be used to verify the spatial frames and vignettes defined in SpatialNet. Figure 5 shows an overview of our system for text-to-scene generation. Although SpatialNet focuses on semantics, the system also requires modules for morphological analysis and syntactic parsing. For English and German, we use the Stanford CoreNLP Toolkit (Manning et al., 2014). In this section, we describe how we use Stanford CoreNLP, SpatialNet, and WordsEye to convert text into a 3D scene. We illustrate using German sentences (b) and (c) from Figure 2. First, Stanford CoreNLP is used to perform Figure 3: Declarative format for spatial frames (top) and spatial vignettes (bottom) fine these spatial vignettes is shown in Figure 3 (bottom). A visual representation of the vignettes is shown in Figure 4 (top). The vignettes link the spatial frame ON - SURFACE to different SGPs based on features of the frame element fillers. The first vignette, ON - FRONT- SURFACE, adds"
W19-1607,P98-1013,0,0.710378,"f how a language expresses spatial relations, annotation of spatial roles does not provide a formal description of the link between surface realization and underlying semantics. Our work provides a formal deThe problem is again that the German preposition an corresponds to two different spatial configurations, ‘affixed to’ (painting) and ‘at/near’ (house). We address the issue of modeling crosslinguistic differences in the expression of spatial language by developing a deep semantic representation of spatial relations called SpatialNet. SpatialNet is based on two existing resources: FrameNet (Baker et al., 1998; Ruppenhofer et al., 2016), a lexical database linking semantic frames to manually annotated text, and VigNet (Coyne et al., 2011), a resource extending FrameNet by grounding abstract lexical semantics with concrete graphical relations. VigNet was developed as part of the WordsEye text-to-scene system (Coyne and Sproat, 2001). SpatialNet builds on both these resources to provide a formal description of the lexical semantics of spatial relations by linking linguistic expressions both to semantic frames and to actual spatial configurations. Because of the link to VigNet and WordsEye, SpatialNet"
W19-1607,W18-1405,0,0.119533,"spatial frames, and SGPs with spatial and graphical constraints from the ontology, grounding the meaning in a language-independent manner. In Section 2, we discuss related work. In Section 3, we provide background information on 62 The graphical semantics can be thought of as a semantic grounding; it is used by WordsEye to construct and render a 3D scene. Frames augmented with graphical semantics are called vignettes. scription and also a semantic grounding that tells us the actual spatial configuration denoted by a set of spatial roles. Also, our work extends to languages other than English. Petruck and Ellsworth (2018) advocate using FrameNet (Ruppenhofer et al., 2016) to represent spatial language. FrameNet uses frame semantics to encode lexical meaning. VigNet (Coyne et al., 2011) is an extension of FrameNet used in the WordsEye text-to-scene system (Coyne and Sproat, 2001). SpatialNet builds on both FrameNet and VigNet; we will describe FrameNet and VigNet in more detail in the next section. 3 The descriptions of the graphical semantics in vignettes make use of object-centric properties called affordances (Gibson, 1977; Norman, 1988). Affordances include any functional or physical property that allows an"
W19-1607,W11-0905,1,0.911498,"Missing"
W19-1607,W14-2202,1,0.891573,"Missing"
W19-1607,W97-0800,0,0.209629,"n University, 2019). An initial mapping was constructed as follows: For each lexicalized concept in VigNet, we looked up each of its linked lexical items in WordNet. If the word (with correct part of speech) was found in WordNet, we added mappings between the VigNet concept and each WordNet synset for that word. This resulted in a manyto-many mapping of VigNet concepts to WordNet synsets. We are currently working on manually correcting this automatically-created map. To obtain a lexical mapping for German, we use the VigNet–WordNet map in conjunction with GermaNet (Henrich and Hinrichs, 2010; Hamp and Feldweg, 1997). GermaNet includes mappings to Princeton WordNet 3.0. For a given German lexical item, we use the GermaNet links to Princeton WordNet to obtain a set of possible VigNet concepts from the VigNet–WordNet mapping. We are also experimenting with the Open German WordNet (Siegel, 2019), although in general we have found it to be less accurate. Open German WordNet includes links to the EuroWordNet Interlingual Index (ILI) (Vossen, 1998), which are in turn mapped to the Princeton English WordNet. Table 1 shows the VigNet concepts for German words used in the sentences in Figure 2, obtained using Germ"
W19-1607,P14-5009,1,0.884838,"Missing"
W19-1607,henrich-hinrichs-2010-gernedit,0,0.0364807,"WordNet of English (Princeton University, 2019). An initial mapping was constructed as follows: For each lexicalized concept in VigNet, we looked up each of its linked lexical items in WordNet. If the word (with correct part of speech) was found in WordNet, we added mappings between the VigNet concept and each WordNet synset for that word. This resulted in a manyto-many mapping of VigNet concepts to WordNet synsets. We are currently working on manually correcting this automatically-created map. To obtain a lexical mapping for German, we use the VigNet–WordNet map in conjunction with GermaNet (Henrich and Hinrichs, 2010; Hamp and Feldweg, 1997). GermaNet includes mappings to Princeton WordNet 3.0. For a given German lexical item, we use the GermaNet links to Princeton WordNet to obtain a set of possible VigNet concepts from the VigNet–WordNet mapping. We are also experimenting with the Open German WordNet (Siegel, 2019), although in general we have found it to be less accurate. Open German WordNet includes links to the EuroWordNet Interlingual Index (ILI) (Vossen, 1998), which are in turn mapped to the Princeton English WordNet. Table 1 shows the VigNet concepts for German words used in the sentences in Figu"
W19-1607,E17-5001,0,0.028089,"sentence may contain several noun phrases outside the prepositional phrase, making the choice of F IGURE ambiguous. FrameNet also does not provide a semantic grounding. To create SpatialNet, we adopt the concept of a FrameNet frame, including the definition of frame elements and lexical units. However, we modify the valence patterns to more precisely define syntactic patterns in a declarative format. In addition, to facilitating the use of SpatialNet across different languages, we specify syntactic constraints in valence patterns using labels from the Universal Dependencies project (Universal Dependencies, 2017). VigNet does provide a grounding in graphical 4.1 Ontology of Semantic Categories The ontology in VigNet consists of a hierarchy of semantic types (concepts) and a knowledge base containing assertions. SpatialNet uses the VigNet ontology and semantic concepts directly, under the assumption that the semantic types and assertions are language-independent. Thus far, our work on English and German has not required modification of the ontology; however, since it was de64 Lexical item Mauer VigNet concepts GermaNet ODE-WordNet WALL . N WALL . N RAMPARTWALL . N RAMPART. N Katze DOMESTIC - CAT. N DOM"
W19-4001,W02-0817,0,0.129618,"ring the CoNLL-2010 shared task (Farkas et al., 2010), along with synonyms of these terms. This list was further expanded and edited through consultation with the LDC and other linguists, to ensure representation of hedge terms from more informal text. The full list of hedge words and phrases in our dictionary is shown in Table 1. This hedging dictionary is divided into relational and propositional hedges. As described in Prokofieva and Hirschberg (2014), relational hedges have to do Crowdsourcing has been successfully used in the past for collecting annotations for word sense disambiguation. Chklovski and Mihalcea (2002) had users select the WordNet sense that most closely matched the definition of a word as used in a given sentence. Likewise, Akkaya et al. (2010) used Amazon Mechanical Turk (AMT) to annotate Subjectivity Word Sense Disambiguation (SWSD), a coarse-grained word sense disam2 Hedge Term about Hedge Definition • almost; approximately (“There are about 10 million packages in transit right now.”) practically • virtually; almost; nearly (“Their provisions were practically gone.” “It has rained practically every day.”) suppose • to believe or assume as true (“It is generally supposed that his death w"
W19-4001,W10-3001,0,0.0994849,"Missing"
W19-4001,D08-1027,0,0.203945,"Missing"
W19-4001,W08-0606,0,0.141476,"Missing"
