2020.emnlp-main.180,D15-1040,0,0.0224932,"oss-lingual transfer. Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser (Zeman and Resnik, 2008; McDonald et al., 2013) on one or more source languages by using either gold or predicted POS labels (Tiedemann, 2015) and applied it to target languages. Building on this, later work used additional features such as typological language properties (Naseem et al., 2012), syntactic embeddings (Duong et al., 2015), and cross-lingual word clusters (T¨ackstr¨om et al., 2012). Among lexicalized approaches, Vilares et al. (2016) learns a bilingual parser on a corpora obtained by merging harmonized treebanks. Ammar et al. (2016) trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT (Devlin et al., 2019), zero-shot transfer in dependency parsing was investigated (Wu and Dredze, 2019; Tran and Bisazza, 2019). Finally Kondratyuk and Straka (2019) trained a multilingual parser on the"
2020.emnlp-main.180,D18-1039,0,0.220448,"es (Johnson et al., 2017; Arivazhagan et al., 2019; Conneau et al., 2020), a problem also known as “the curse of multilinguality”. Generally speaking, a multilingual model without language-specific supervision is likely to suffer from over-generalization and perform poorly on high-resource languages due to limited capacity compared to the monolingual baselines, as verified by our experiments on parsing. In this paper, we strike a good balance between maximum sharing and language-specific capacity in multilingual dependency parsing. Inspired by recently introduced parameter sharing techniques (Platanios et al., 2018; Houlsby et al., 2019), we propose a new multilingual parser, UDapter, that learns to modify its language-specific parameters including the adapter modules, as a function of language embeddings. This allows the model to share parameters across languages, ensuring generalization and transfer ability, but also enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database (Litte"
2020.emnlp-main.180,W15-2137,0,0.0139498,"d to combine language and task adapters, small bottleneck layers (Rebuffi et al., 2018; Houlsby et al., 2019), to address the capacity issue which limits multilingual pre-trained models for cross-lingual transfer. Cross-Lingual Dependency Parsing The availability of consistent dependency treebanks in many languages (McDonald et al., 2013; Nivre et al., 2018) has provided an opportunity for the study of cross-lingual parsing. Early studies trained a delexicalized parser (Zeman and Resnik, 2008; McDonald et al., 2013) on one or more source languages by using either gold or predicted POS labels (Tiedemann, 2015) and applied it to target languages. Building on this, later work used additional features such as typological language properties (Naseem et al., 2012), syntactic embeddings (Duong et al., 2015), and cross-lingual word clusters (T¨ackstr¨om et al., 2012). Among lexicalized approaches, Vilares et al. (2016) learns a bilingual parser on a corpora obtained by merging harmonized treebanks. Ammar et al. (2016) trains a multilingual parser using multilingual word embeddings, token-level language information, language typology features and fine-grained POS tags. More recently, based on mBERT (Devlin"
2020.emnlp-main.180,J19-3005,0,0.110741,"Missing"
2020.emnlp-main.180,W17-0412,0,0.0295288,"ed Attachement Scores (LAS) on the high-resource set are given in Table 1. UDapter consistently outperforms both our monolingual and multilingual baselines in all languages, and beats the previous work, setting a new state of the art, in 9 out of 13 languages. Statistical significance testing8 applied between UDapter and multi/mono-udify confirms that UDapter’s performance is significantly better than the baselines in 11 out of 13 languages (all except en and it). 8 We used paired bootstrap resampling to check whether the difference between two models is significant (p < 0.05) by using Udapi (Popel et al., 2017). 10 difference (udapter, multi-udify) treebank size (K) 20 8 16 6 12 4 8 2 4 0 ko eu tr zh he ar sv fi ru ja hi it en 0 Figure 2: Difference in LAS between UDapter and multi-udify in the high-resource setting. Diamonds indicate the amount of sentences in the corresponding treebank. Among directly comparable baselines, multiudify gives the worst performance in the typologically diverse high-resource setting. This multilingual model is clearly worse than its monolingually trained counterparts mono-udify: 83.0 vs 86.0. This result resounds with previous findings in multilingual NMT (Arivazhagan"
2020.emnlp-main.180,N19-1393,0,0.0935715,"lso enables language-specific parameterization in a single multilingual model. Furthermore, we propose not to learn language embeddings from scratch, but to leverage a mix of linguistically curated and predicted typological features as obtained from the URIEL language typology database (Littell et al., 2017) which supports 3718 languages including all languages represented in UD. While the importance of typological features for cross-lingual parsing is known for both non-neural (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015) and neural approaches (Ammar et al., 2016; Scholivet et al., 2019), we are the first to use them 2302 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2302–2315, c November 16–20, 2020. 2020 Association for Computational Linguistics effectively as direct input to a neural parser, without manual selection, over a large number of languages in the context of zero-shot parsing where gold POS labels are not given at test time. In our model, typological features are crucial, leading to a substantial LAS increase on zero-shot languages and no loss on high-resource languages when compared to the language embeddings learne"
2020.emnlp-main.180,N12-1052,0,\N,Missing
2020.emnlp-main.180,P12-1066,0,\N,Missing
2020.emnlp-main.180,D15-1213,0,\N,Missing
2020.emnlp-main.180,P15-1166,0,\N,Missing
2020.emnlp-main.180,Q16-1023,0,\N,Missing
2020.emnlp-main.180,Q17-1024,0,\N,Missing
2020.emnlp-main.180,E17-2002,0,\N,Missing
2020.emnlp-main.180,D18-1103,0,\N,Missing
2020.emnlp-main.180,D18-1543,0,\N,Missing
2020.emnlp-main.180,K18-2020,0,\N,Missing
2020.emnlp-main.180,N19-1388,0,\N,Missing
2020.emnlp-main.180,N19-1423,0,\N,Missing
2020.emnlp-main.180,P13-2017,0,\N,Missing
2020.iwpt-1.16,2020.iwpt-1.21,0,0.391189,". This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and st"
2020.iwpt-1.16,2020.iwpt-1.24,0,0.181901,"dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an"
2020.iwpt-1.16,W17-6507,1,0.792227,"Missing"
2020.iwpt-1.16,2020.iwpt-1.20,0,0.157709,"Missing"
2020.iwpt-1.16,2020.iwpt-1.23,0,0.193264,"on that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependen"
2020.iwpt-1.16,2020.iwpt-1.26,0,0.254795,"Missing"
2020.iwpt-1.16,2020.iwpt-1.19,0,0.133706,"is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any"
2020.iwpt-1.16,2020.iwpt-1.18,0,0.167819,"its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-"
2020.iwpt-1.16,P80-1024,0,0.722615,"Missing"
2020.iwpt-1.16,2020.iwpt-1.17,0,0.156172,"Missing"
2020.iwpt-1.16,D19-1279,0,0.19421,"pendent with dependency label L2 has an empty node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into"
2020.iwpt-1.16,D19-1277,0,0.0922493,"Missing"
2020.iwpt-1.16,2020.lrec-1.497,1,0.848313,"Missing"
2020.iwpt-1.16,W18-6012,0,0.132583,"hether the same is true for enhanced dependency parsing. The challenge is both formal and practical. First, the enhanced representation is a connected graph, possibly containing cycles, while previous work on dependency parsing mostly dealt with rooted trees. Second, as some dependency labels incorporate the lemma of certain dependents and other additional information, the set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th Internation"
2020.iwpt-1.16,K19-2001,0,0.0535138,"Missing"
2020.iwpt-1.16,S15-2153,1,0.852196,"Missing"
2020.iwpt-1.16,S14-2008,1,0.812645,"e set of labels to be predicted is much larger and language-dependent. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Schuster et al., 2017; Nivre et al., 2018). Moreover, most state of the art work in dependency parsing uses a graph-based approach, where the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen et al., 2014, 2015, 2019) and could even lead to state of the art performance for downstream applications as shown by the results of the Extrinsic Evaluation Parsing shared-task (Oepen et al., 2017). 151 Proceedings of the 16th International Conference on Parsing Technologies and the IWPT 2020 Shared Task, pages 151–161 c Virtual Meeting, July 9, 2020. 2020 Association for Computational Linguistics 3 Enhanced Universal Dependencies conj conj UD version 22 states that apart from the morphological and basic dependency annotation layers, strings may be annotated with an additional, enhanced, dependency layer"
2020.iwpt-1.16,2020.acl-demos.14,0,0.0565369,"node i2.1 as parent which itself is an L1 dependent of i1, its dependency label will be expanded into a path i1:L1&gt;L2. This preserves the information that the dependent was an L2 dependent of ‘something’ that was itself an L1 dependent of i1, while at the same time removing the potentially conflicting i2.1 (Figure 5).6 7 Approaches There is quite a bit of variation in the way various teams have addressed the task. For the initial stages of the analysis (tokenization, lemmatization, POStagging) some version of UDPipe7 (Straka et al., 2016), Udify8 (Kondratyuk and Straka, 2019), and/or Stanza9 (Qi et al., 2020) is often involved. Several teams (Orange (Heinecke, 2020), FASTPARSE (Dehouck et al., 2020), UNIPI (Attardi et al., 2020), CLASP (Ek and Bernardy, 2020), ADAPT (Barry et al., 2020)) concentrate on parsing into standard UD, and then add hand-written enhancement rules, sometimes in combination with data-driven heuristics to improve robustness. TurkuNLP (Kanerva et al., 2020) transforms EUD into a representation that is compatible with standard UD by combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by"
2020.iwpt-1.16,L16-1680,0,0.0883819,"Missing"
2020.iwpt-1.16,2020.iwpt-1.22,0,0.144364,"combining multiple edges into a single edge with a complex label, and compiling edges involving empty nodes into complex edge labels (as is done by the evaluation script as well). The total number of edge-labels is reduced by de-lexicalising enhanced edge labels and storing a pointer to the dependent from which the lemma of an enhancement originates in the de-lexicalized edge label. A wide range of parsers (graph-based biaffine, transitionbased), and pre-trained embeddings (XLM-R or mBERT or language specific BERTs) is used. Finally, several teams (Emory NLP (He and Choi, 2020), ShanghaiTech (Wang et al., 2020), ADAPT, Køpsala (Hershcovich et al., 2020), RobertNLP (Gr¨unewald and Friedrich, 2020)) do not use conversion (or only to restore de-lexicalized labels), but instead use a graph-based parser that can directly produce enhanced dependency graphs. The output of the graph-based parser is often combined with information from a standard UD parser to ensure well-formedness and connectedness of the resulting graph. 6 8 Results We include two baseline results:10 baseline1 was obtained by taking gold basic UD trees and copying these into the enhanced layer without any modifications. Baseline2 uses UDPi"
2020.iwpt-1.16,K18-2001,1,0.884896,"Missing"
2021.iwpt-1.15,W17-6507,1,0.881486,"notation is richer than in UD 2.7. Besides improvements in the officially released versions of the individual treebanks, a few other things have changed in comparison to the IWPT 2020 task. The English data now includes the GUM treebank (its enhanced annotation was not present in UD 2.7 but it was being prepared for UD 2.8 and it was ready in time for the shared task). As in 2020, we include two French treebanks whose enhanced annotation is still not included in the official UD releases, but the annotation is more conservative this year, omitting the extra labels for diathesis neutralization (Candito et al., 2017) and surface vs deep syntax markers. Still, some enhancements in French go slightly beyond the official UD guidelines (see below for details). In Polish, we now harmonize the relation subtypes in the three treebanks so that merging them into one dataset is no longer an issue. Finally, we omit the Chukchi treebank, which is new in UD 2.7 and has enhanced graphs, but the graphs are there only 147 conj conj obj nsubj nummod punct orphan cc orphan Sue has 5 euros , Pat 6 and Kim 3 Figure 1: A basic tree of a gapping structure. conj conj obj nsubj nummod punct nsubj obj cc nsubj obj Sue has 5 euros"
2021.iwpt-1.15,2021.iwpt-1.24,0,0.0565891,"Missing"
2021.iwpt-1.15,2020.acl-main.747,0,0.0880101,"ng task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexic"
2021.iwpt-1.15,2021.iwpt-1.18,0,0.0205018,"This should be seen as a diagnostic only, and is intended to gain further insights into the capability of various systems to deal with challenging phenomena, such as the proper analysis of phenomena occurring in the context of coordination and ellipsis. 7 Approaches The predominant approach to obtaining the enhanced dependency graph is to use a biaffine function, i.e., predicting for each pair of nodes how likely it is that they are in a parent-child relation. There is wide variety in the way the final annotation graph is obtained, and ensuring that the result is valid (i.e. connected). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of th"
2021.iwpt-1.15,2021.iwpt-1.17,0,0.05987,"Missing"
2021.iwpt-1.15,2021.iwpt-1.16,0,0.0440323,"Missing"
2021.iwpt-1.15,2020.lrec-1.497,1,0.896496,"Missing"
2021.iwpt-1.15,2021.iwpt-1.22,0,0.0523448,"Missing"
2021.iwpt-1.15,W18-6012,0,0.397597,"Missing"
2021.iwpt-1.15,W13-3728,0,0.40752,"nd subsequent work have shown that considerable progress has been made in multilingual dependency parsing. For enhanced dependency parsing, there are additional challenges. The enhanced representation is a connected directed graph, possibly containing cycles, while the bulk of dependency parsing work still focuses on rooted trees. The set of labels to be predicted is also much larger, as some enhanced dependency labels incorporate the lemma of certain dependents. On the other hand, it has been shown that much of the enhanced annotation can be predicted on the basis of the basic UD annotation (Nyblom et al., 2013; Schuster et al., 2017; Nivre et al., 2018). Moreover, most state-of-the-art work in dependency parsing uses a graph-based approach, where 146 Proceedings of the 17th International Conference on Parsing Technologies (IWPT 2021), pages 146–157 Bangkok, Thailand (online), August 6, 2021. ©2021 Association for Computational Linguistics the assumption that the output must form a tree is only used in the final step from predicted links to final output. And finally, work on deep-syntax and semantic parsing has shown that accurate mapping of strings into rich graph representations is possible (Oepen"
2021.iwpt-1.15,2020.conll-shared.0,0,0.0794056,"Missing"
2021.iwpt-1.15,K19-2001,0,0.064997,"Missing"
2021.iwpt-1.15,S15-2153,1,0.846515,"Missing"
2021.iwpt-1.15,S14-2008,1,0.870583,"Missing"
2021.iwpt-1.15,2020.acl-demos.14,0,0.236593,"mpare the approaches taken by participating teams and discuss the results of the shared task, also in comparison with the first edition of this task. 1 2 Introduction Universal Dependencies (UD) (Nivre et al., 2020) is a framework for cross-linguistically consistent treebank annotation that has so far been applied to 114 languages. UD defines two levels of annotation, the basic trees and the enhanced graphs (EUD) (Schuster and Manning, 2016). There are several good parsers that can predict the basic trees (including tokenization and morphology) for previously unseen text (Straka et al., 2016; Qi et al., 2020). Two large shared tasks on basic UD parsing were organized at CoNLL (Zeman et al., 2017, 2018). Enhanced UD parsing attracted comparatively less attention until the shared task organized at IWPT 2020 (Bouma et al., 2020). The present paper describes a second instance of that task, organized as a part of the 17th International Conference on Parsing Technologies1 (IWPT), collocated with ACL-IJCNLP 2021. Like in the previous year, the evaluation was done on datasets covering 17 languages from four language familiies. This paper is a follow-up of the overview paper of the previous instance of the"
2021.iwpt-1.15,L16-1376,0,0.284241,"Missing"
2021.iwpt-1.15,2021.iwpt-1.23,0,0.012184,"cted). GREW (Guillaume and Perrier, 2021) uses manually constructed rewrite rules to map basic UD into EUD, while FASTPARSE (Anderson and Gómez-Rodríguez, 2021) and NUIG (Choudhary and O’riordan, 2021) reformulate the task as a sequence-labeling task. For the initial stages of the analysis (sentence splitting, tokenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021"
2021.iwpt-1.15,L16-1680,0,0.0641786,"Missing"
2021.iwpt-1.15,2021.eacl-demos.10,0,0.427708,"Missing"
2021.iwpt-1.15,2021.iwpt-1.20,0,0.0398853,"kenization, lemmatization, POStagging) most teams use Stanza (Qi et al., 2020) or Trankit (Van Nguyen et al., 2021) or similar methods. In a post-evaluation experiment, the DCUEPFL team (Barry et al., 2021) obtained improved scores using Trankit instead of Stanza, while the TGIF team (Shi and Lee, 2021) uses a variation of the Trankit and Stanza systems to obtain the best pre-processing results, especially for sentencesplitting. A wide variety of monolingual and multilingual pre-trained language models is used, with XML-R (Conneau et al., 2020) being the most popular. The ShanghaiTech system (Wang et al., 2021) learns an input representation from a combination of pretrained language models where the various representations are concatenated into a single vector and masking is used to learn a weighting for various components of the combined vector. Both COMBO (Klimaszewski and Wróblewska, 2021) and UNIPI (Attardi et al., 2021) use a method that learns weights for the scores obtained from various layers of the BERT model to be used as input for the biaffine parser. Most teams reduce the number of edge labels during training by de-lexicalizing edge labels. Dependency paths involving an empty node are us"
2021.iwpt-1.15,K18-2001,1,0.752485,"Missing"
A00-2040,J95-4004,0,0.0234464,"urther improvements would definitely have been possible at this point, it becomes increasingly difficult to do this on the basis of linguistic knowledge alone. That is, most of the rules which have to be added deal with highly idiosyncratic cases (often related to loan-words) which can only be discov3It s h o u l d be n o t e d t h a t we only considered words which do n o t c o n t a i n diacritics. I n c l u d i n g t h o s e is u n p r o b l e m a t i c in principle, b u t would lead to a slight increase of t h e n u m b e r of rules. 306 Transformation-based grapheme to phoneme conversion Brill (1995) demonstrates that accurate part-ofspeech tagging can be learned by using a two-step process. First, a simple system is used which assigns the most probable tag to each word. The results of the system are aligned with the correct tags for some corpus of training data. Next, (contextsensitive) transformation rules are selected from a pool of rule patterns, which replace erroneous tags by correct tags. The rule with the largest benefit on the training data (i.e. the rule for which the number of corrections minus the number of newly introduced mistakes, is the largest) is learned and applied to t"
A00-2040,E99-1017,0,0.021278,"Missing"
A00-2040,C00-1048,0,0.0220852,"Missing"
A00-2040,J94-3001,0,0.0167302,"Missing"
A00-2040,P95-1003,0,0.0194581,"ansducer which maps all strings in A to all strings in B. identity: the transducer which maps each element, in A onto itself. composition of the transducers T and U. use Term as an abbreviation for R (where Term and R may contain variables). Figure 1: A fragment of FSA regular expression syntax. A and B are regular expressions denoting recognizers, T and U transducers, and R can be either. ators. The part of FSA's built-in regular expression syntax relevant to this paper, is listed in figure 1. One particular useful extension of the basic syntax of regular expressions is the replace-operator. Karttunen (1995) argues that many phonological and morphological rules can be interpreted as rules which replace a certain portion of the input string. Although several implementations of the replaceoperator are proposed, the most relevant case for our purposes is so-called 'leftmost longest-match' replacement. In case of overlapping rule targets in the input, this operator will replace the leftmost target, and in cases where a rule target contains a prefix which is also a potential target, the longer sequence will be replaced. Gerdemann and van Noord (1999) implement leftmost longest-match replacement in FSA"
A00-2040,J97-2003,0,0.0359875,"Missing"
A00-2040,W99-0705,0,\N,Missing
A00-2040,E93-1007,0,\N,Missing
A00-2040,J95-2004,0,\N,Missing
A00-2040,P98-2188,0,\N,Missing
A00-2040,C98-2183,0,\N,Missing
bouma-2010-cross,P00-1064,0,\N,Missing
bouma-2010-cross,2006.jeptalnrecital-invite.2,0,\N,Missing
bouma-2010-cross,J07-4005,0,\N,Missing
bouma-2010-cross,W07-0908,0,\N,Missing
bouma-kloosterman-2002-querying,ide-etal-2000-xces,0,\N,Missing
bouma-kloosterman-2002-querying,W97-1502,0,\N,Missing
bouma-kloosterman-2002-querying,C00-2144,0,\N,Missing
bouma-kloosterman-2002-querying,oostdijk-2000-spoken,0,\N,Missing
C92-1018,C86-1050,0,\N,Missing
C92-1018,C86-1045,0,\N,Missing
C92-1018,J92-2003,1,\N,Missing
C94-1039,J91-3003,0,0.102308,"an take part in unbounded dependency construelions. Lexical treatments of the kind presented in Pollard and Sag (in press), chapter 9 assume that a lexlcal rule is responsible for &apos;moving&apos; Rather than formalizing the &apos;add-adjuncts&apos; rule as a lexical rule we propose to use recursive constraints on lexical categories. Such lexical constraints are then processed using delayed ewduation techniques, a Such an approach is more promising than an off-line approach that precomputes the effect lcf. Miller (1992) for a similar suggestions concerning French. 2inspired by Kasper (in preparation) aRefer to Carpenter (1991) for a proof of TurilLg equivalence of simple eategorial grammar with recurslve lexical rules. VI,;RBAI, VERBAL sc : P •S sere 1 :~ sc:l&apos;.( wod : ar~l : Sem~) : Sere0 ) .S va/: Sere .N(?&apos;Gq,: ~OIll Figure 3: A lexieal rule t h a t adds a single adjunct to the sul)cat list of &gt;t verb. In the. case of n ~uljuncts the rule applies n times. O P_.AI)Vt,;RIIIA L RI,~ST I1.~.1) V E ll.n I A L arglnuc : ~ttod : val[nuc [ : Soa arg resh&quot; : [ qI*oa : Q ] m~d : va&quot;nuclI ,&apos;~,~t,&quot; : 0 Figure 4: A restrictive, adverbial and an olmrator a(lverl)ial. Restrictive adverbials (such as locatives and time adverbia"
E89-1003,P87-1011,0,0.660894,"k with categorial grammars, which contain restricted versions of composition and raising. Although they can be processed efficiently, there is linguistic evidence that they are not fully adequate for analysis of such phenomena as coordination. Since atomic categories can in general not be raised in these grammars, sentence (2b) (in which the category n p has to be raised) cannot be derived. Furt herm ore, since composition is not generalized, as in Ades & Steedman (1982), a sentence such as J o h n There have been two proposals for eliminating spurious ambiguity from the grammar. The first is Wittenburg (1987). In this paper, a categorial grammar with composition and heavily restricted versions of raising (for subject n p's only) is considered. Wittenburg proposes to eliminate spurious ambiguity by redefining composition. His predictive composition rules apply only in those cases where they are really needed to make a derivation possible. A disadvantage of this method, noticed by Wittenburg, is that one may have to add special predictive composition rules for all general combinasold but Mary donated a book to the library would not be derivable. The possibilities for left-to-right, incremental, proc"
E89-1003,P87-1012,0,0.407911,"t to canonical constituent structures, it also admits various other constituent structures. Therefore, the sentences in (2) can be considered to be ordinary instances of coordination (of two categories slap and ( v p / a p )  v p , respectively). (2) a. John sold and Mary boughta book s/vp vp/np s/np s/vp vp/np np s/np left-directional functor respectively, looking for an argument of category B. - 19- tory rules in the grammar. Some careful rewriting of the original grammar has to take place, before things work as desired. b. J. loves Mary madly and Sue wildly vp/np np vpvp np vpvp Mary np Pareschi & Steedman (1987) propose an efficient chart-parsing algorithm for categorial grammars with spurious ambiguity. Instead of the usual strategy, in which all possible subconstituents are added to the chart, Pareschi & Steedman restrict themselves to adding only those constituents that may lead to a difference in semantics. Thus, in (3) only the underlined constituents are in the chart. The &quot;---&quot; constituent is not. madly vpvp (vplnp)vp (vp/np)Wp A somewhat different type of argument for flexible phrase structure is based on the way humans process natural language. In Ades & Steedman (1982) it is pointed out th"
E91-1031,P85-1018,0,0.6562,"r unbounded dependency constructions (Bouma case : n o m hum : sg - 179 - 1987). Finally, instead of fully instantiated categorystructures, one may choose to work with polymorphic categories (Karttunen 1989, Zeevat et al. 1987). Consequently, CUG not only shows resemblances with traditional categorial grammar, but also with Head-driven Phrase Structure G r a m m a r (Pollard &: Sag, 1987), another lexicalist and unification-based framework. C H A R T PARSING OF UNIFICATION G R A M M A R (UG). Parsing methods for context-free grammar can be extended to unification-based grammar formalisms (see Shieber, 1985 or Haas, 1989), and therefore they can in principle be used to parse C U G . A chart-parser scans a sentence from left to right, while entering items, representing (partial) derivations, in a chart. Assume that items are represented as Prolog terms of the form item(Begin, End, LH S, Parsed, ToParse), where LHS is a feature-structure and Parsed and ToParse contain lists of feature-structures. An item(O, 1, [S],[NP], [V, N P ] ) represents a partial derivation ranging from position 0 to 1 of a constituent with feature-structure S, of which a daughter N P has been found and of which daughters V"
E91-1031,C86-1045,0,0.016524,"r both bottom-up and top-down parsing. The combinatory rules of classical CG, A ~ A / B B (rightward application) and A ---, B B  A (leftward application), can be encoded as highly schematic rewrite rules associated with an attribute-value graph: Rightward Application Rule : Xo ~ XI X2 X o : &lt; [- 1 &gt; 1. X~ : &lt; 2 &gt; Leftward 1 dir : right arg :&lt; 2 &gt; X l : ] cat : Application Rule : X0 --* X1 X2 X0:&lt; 1&gt; X1 : &lt; 2 &gt; Preliminaries CATEGORIAL UNIFICATION GRAMMAR.. Unificationbased versions of Categorial Grammar, known as CUG or UCG, have attracted considerable attention recently (see, for instance, Uszkoreit, 1986, Karttunen, 1986, Bouma, 1988, Bouma et al., 1988, and Calder et al., 1988). The categories of Categorial Grammar (CG) can be encoded easily as feature-structures, in which the attribute &lt; cat &gt; dominates either an atomic value (in case of an atomic category) or a structure with attributes &lt; v a l &gt;, &lt; d i r &gt; and &lt; arg &gt; (in case of a complex category). Morphosyntactic information can be added by introducing additional labels. An example of such a category represented as attribute-value matrix is presented below. N P[+nom]/N[+nom, val : dir : left arg :&lt; 2 &gt; CUG is a lexicalist theory: langu"
E91-1031,C88-1018,0,0.0140407,"al CG, A ~ A / B B (rightward application) and A ---, B B  A (leftward application), can be encoded as highly schematic rewrite rules associated with an attribute-value graph: Rightward Application Rule : Xo ~ XI X2 X o : &lt; [- 1 &gt; 1. X~ : &lt; 2 &gt; Leftward 1 dir : right arg :&lt; 2 &gt; X l : ] cat : Application Rule : X0 --* X1 X2 X0:&lt; 1&gt; X1 : &lt; 2 &gt; Preliminaries CATEGORIAL UNIFICATION GRAMMAR.. Unificationbased versions of Categorial Grammar, known as CUG or UCG, have attracted considerable attention recently (see, for instance, Uszkoreit, 1986, Karttunen, 1986, Bouma, 1988, Bouma et al., 1988, and Calder et al., 1988). The categories of Categorial Grammar (CG) can be encoded easily as feature-structures, in which the attribute &lt; cat &gt; dominates either an atomic value (in case of an atomic category) or a structure with attributes &lt; v a l &gt;, &lt; d i r &gt; and &lt; arg &gt; (in case of a complex category). Morphosyntactic information can be added by introducing additional labels. An example of such a category represented as attribute-value matrix is presented below. N P[+nom]/N[+nom, val : dir : left arg :&lt; 2 &gt; CUG is a lexicalist theory: language specific information about word order, subcategorization, agreement, cas"
E91-1031,J89-4001,0,0.675983,"dency constructions (Bouma case : n o m hum : sg - 179 - 1987). Finally, instead of fully instantiated categorystructures, one may choose to work with polymorphic categories (Karttunen 1989, Zeevat et al. 1987). Consequently, CUG not only shows resemblances with traditional categorial grammar, but also with Head-driven Phrase Structure G r a m m a r (Pollard &: Sag, 1987), another lexicalist and unification-based framework. C H A R T PARSING OF UNIFICATION G R A M M A R (UG). Parsing methods for context-free grammar can be extended to unification-based grammar formalisms (see Shieber, 1985 or Haas, 1989), and therefore they can in principle be used to parse C U G . A chart-parser scans a sentence from left to right, while entering items, representing (partial) derivations, in a chart. Assume that items are represented as Prolog terms of the form item(Begin, End, LH S, Parsed, ToParse), where LHS is a feature-structure and Parsed and ToParse contain lists of feature-structures. An item(O, 1, [S],[NP], [V, N P ] ) represents a partial derivation ranging from position 0 to 1 of a constituent with feature-structure S, of which a daughter N P has been found and of which daughters V and N P are sti"
E93-1010,W89-0206,0,0.130698,"Missing"
E93-1010,E91-1006,0,0.10174,"Missing"
E93-1010,P80-1024,0,0.0691737,"Missing"
E93-1010,W89-0205,0,0.231034,"Missing"
E93-1010,P85-1018,0,0.188651,"Missing"
E93-1010,P91-1015,1,0.879486,"Missing"
E93-1010,P91-1000,0,0.287769,"Missing"
E93-1010,E91-1031,1,\N,Missing
hendrickx-etal-2008-coreference,J98-2001,0,\N,Missing
hendrickx-etal-2008-coreference,W96-0102,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1005,0,\N,Missing
hendrickx-etal-2008-coreference,N01-1008,0,\N,Missing
hendrickx-etal-2008-coreference,A88-1003,0,\N,Missing
hendrickx-etal-2008-coreference,W02-1008,0,\N,Missing
hendrickx-etal-2008-coreference,W05-0303,0,\N,Missing
hendrickx-etal-2008-coreference,C02-1139,0,\N,Missing
hendrickx-etal-2008-coreference,N06-1025,0,\N,Missing
hendrickx-etal-2008-coreference,J01-4004,0,\N,Missing
hendrickx-etal-2008-coreference,P98-2143,0,\N,Missing
hendrickx-etal-2008-coreference,C98-2138,0,\N,Missing
hendrickx-etal-2008-coreference,hoste-de-pauw-2006-knack,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1006,0,\N,Missing
hendrickx-etal-2008-coreference,M95-1011,0,\N,Missing
hendrickx-etal-2008-coreference,P03-1023,0,\N,Missing
I05-7011,alfonseca-manandhar-2002-improving,0,\N,Missing
I05-7011,W02-0908,0,\N,Missing
I05-7011,P90-1034,0,\N,Missing
I05-7011,P94-1019,0,\N,Missing
I05-7011,P89-1010,0,\N,Missing
I05-7011,P98-2127,0,\N,Missing
I05-7011,C98-2122,0,\N,Missing
I05-7011,N04-1041,0,\N,Missing
I05-7011,P99-1004,0,\N,Missing
I05-7011,P03-1001,0,\N,Missing
J92-2003,P90-1021,1,0.92456,"= (Xi head) to every rule of the form Xo --~ X1... Xn (with Xi (1 K i &lt; n) the head of the rule and assuming all head features to be collected under head) will not do, as it rules out the possibility of exceptions altogether. Shieber (1986b) therefore proposes to add this constraint conservatively, which means that, if the rule already contains conflicting information for some head feature f, the constraint is replaced by a set of constraints (Xo head f&apos;) = (Xi head f&apos;), for all head features f&apos; # f. • Structuring the Lexicon. Flickinger, Pollard, and Wasow (1985), Flickinger (1987), De Smedt (1990), Daelemans (1988), and others, have argued that the encoding and maintenance of the detailed lexical descriptions typical for lexicalist grammar formalisms benefits greatly from the use of (nonmonotonic) inheritance. In Flickinger, Pollard, and Wasow (1985), for instance, lexical information is organized in the form of frames, which are comparable to the templates (i.e., feature structures that may be used as part of the definition of other feature structures) of PATR-II (Shieber 1986a). A frame or specific lexical entry may inherit from more general frames. Frames can be used to encode infor"
J92-2003,J90-1002,0,0.0185217,"es of attribute-value pairs where values are either atoms or feature structures themselves and, furthermore, values m a y be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) (g f) (g f) = = = a a Igg) f&apos;a Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, ~, P, 6, q0, F, &quot;~/where: 1. Q is a n o n e m p t y finite set of states, 2. G is a countable set (the alphabet), 3. 1~ is a countable set (the output alphabet), 4. ~ : Q x G --* Q is a finite partial function (the transition function), 5. q0EQ, 6. FC_Q, 7. )~ : F --* P is a total function (the output function), 8. the directed graph (Q, E) is acyclic, where pEq iff for some l E Y~,6(p,I) = q, 9. for every q E Q, there exists a directed path from q0 to q in (Q, E), and 10. for every q E F, 6(q, I) is not defined for any l. We will freq"
J92-2003,E89-1009,0,0.72773,"ailable for capturing lexical generalizations are templates (see above) and lexical rules. Lexical rules may transform the feature structure of a lexical entry. An example is the rule for agentless passive (Shieber 1986a, p. 62), which transforms the feature structure for transitive past participles into a feature structure for participles occurring in agentless passive constructions. Lexical rules can only change the feature structure of a lexical entry, not its word form, and thus, the scope of these rules is rather restricted. While the examples in Flickinger, Pollard, and Wasow (1985) and Evans and Gazdar (1989a,b) suggest that the latter restriction can be easily removed, it is not so obvious how a unification-based grammar formalism can cope with the combination of rules and exceptions typical for (inflectional) morphology. For instance, it is possible to formulate a rule that describes past tense formation in English, but it is not so easy to exclude the application of this rule to irregular verbs and to describe (nonredundantly) past tense formation of these irregular verbs. Evans and Gazdar (1989a,b) present the DATR-formalism, which, among other things, contains a nonmonotonic inference system"
J92-2003,P85-1032,0,0.385065,"Missing"
J92-2003,P86-1038,0,0.0127366,"tive to consider an operation that subsumes the effects of the proposals so far. Default Unification, as defined below, is an attempt to provide such an operation. 187 Computational Linguistics Volume 18, Number 2 3. Feature Structures and Unification Feature structures are often depicted as matrices of attribute-value pairs where values are either atoms or feature structures themselves and, furthermore, values m a y be shared by different attributes in the feature structure. Feature structures can be defined using a description language, such as the one found in PATR-II (Shieber 1986a) or in Kasper and Rounds (1986; 1990). For instance, 4a is a description of 4b. Example 4 a. ( (f) (g f) (g f) = = = a a Igg) f&apos;a Following the approach of Kasper and Rounds (1986; 1990), and others, we represent feature structures formally as finite (acyclic) automata (the definition below is taken from Dawar and Vijay-Shanker 1990): Definition A finite acyclic automaton A is a 7-tuple (Q, ~, P, 6, q0, F, &quot;~/where: 1. Q is a n o n e m p t y finite set of states, 2. G is a countable set (the alphabet), 3. 1~ is a countable set (the output alphabet), 4. ~ : Q x G --* Q is a finite partial function (the transition function),"
J92-2003,C86-1050,0,0.0869938,"formal semantics of the operation and demonstrate how some of the phenomena used to motivate nonmonotonic extensions of unification-based formalisms can be handled. 1. Introduction While monotonicity is often desirable from a formal and computational perspective, it is at odds with a considerable body of linguistic work. Default principles, default rules, and default feature-values can be found in many linguistic formalisms and are used prominently in work on phonology, morphology, and syntax. In spite of their great expressive power and flexibility, unification-based grammar formalisms (see Shieber 1986a, for an introduction) are in general not very successful in modeling such devices. Unification is an information-combining, monotonic, operation on feature structures, whereas the implementation of default devices typically requires some form of nonmonotonicity. In this paper, we present a nonmonotonic operation on feature structures, which enables us to implement the effects of a number of default devices used in linguistics. As the operation is defined in terms of feature structures only, an important characteristic of unification-based formalisms, namely that linguistic knowledge is encod"
kis-etal-2004-new,moiron-2004-discarding,0,\N,Missing
kis-etal-2004-new,varadi-2002-hungarian,0,\N,Missing
P10-1135,N03-1011,0,0.0789663,"Missing"
P10-1135,J06-1005,0,0.0223612,"al modelling is the part-whole relation, which exists between parts and the wholes they compise (Winston et al., 1987; Gerstl and Pribbenow, 1995). Different types of part-whole relations, classified in various taxonomies, are mentioned in literature (Winston et al., 1987; Odell, 1994; Gerstl and Pribbenow, 1995; Keet and Artale, 2008). The taxonomy of Keet and Artale (2008), for instance, distinguishes part-whole relations based on their transitivity, and on the semantic classes of entities they sub-categorize. Part-whole relations are also crucial for many information extraction (IE) tasks (Girju et al., 2006). Annotated corpora and semantic dictionaries used in IE, such as the ACE corpus1 and WordNet (Fellbaum, 1998), include examples of part-whole relations. Also, previous relation extraction work, 1 http://projects.ldc.upenn.edu/ace/ 1. Is information extraction (IE) harder when learning the individual types of part-whole relations? That is, we determine whether the performance of state-of-the-art IE systems in learning the individual part-whole relation types increases (due to more coherency in the relations’ linguistic realizations) or drops (due to fewer examples), compared to the traditional"
P10-1135,C92-2082,0,0.308327,"Missing"
P10-1135,P03-1054,0,0.00305894,"ure, including ontologically-motivated mereological relations and linguistically-motivated meronymic ones. We adopt a 3-step approach to address our questions from section 1. 1. Define prototypical seeds (part-whole tuples) as follows: • (Separate) sets of seeds for each type of part-whole relation in Keet’s taxonomy. • A single set that mixes seeds denoting all the different part-whole relations types. 2. Part-whole relations extraction from a corpus by initializing a minimally-supervised IE algorithm with the seed-sets We parsed the English and Dutch corpora respectively with the Stanford3 (Klein and Manning, 2003) and the Alpino4 (van Noord, 2006) parsers, and formalized the relations between terms (entities) as dependency paths. A dependency path is the shortest path of lexico-syntactic elements, i.e. shortest lexico-syntactic pattern, connecting entities (proper and common nouns) in their parsetrees. Such a formalization has been successfully employed in previous IE tasks (see Stevenson and Greenwood (2009) for an overview). Compared to traditional surface-pattern representations, used by Pantel and Pennacchiotti (2006), dependency paths abstract from surface texts to capture long range dependencies"
P10-1135,P09-1045,0,0.283585,"ce of IE algorithms, with mereological seeds being in general more fertile than their meronymic counterparts, and generating higher-precision tuples; 2) the precision achieved when initializing IE algorithms with a general set, which mixes 1332 seeds of heterogeneous part-whole relation types, is comparable to the best results obtained with individual sets of specialized seeds, denoting specific part-whole relations. An evaluation of the patterns and tuples extracted indicated considerable precision drop between successive iterations of our algorithm. This appears to be due to semantic drift (McIntosh and Curran, 2009), where highly-ambiguous patterns promote incorrect tuples , which in turn, compound the precision loss. 4.2 Types of Extracted Relations Initializing our algorithm with seeds of a particular type always led to the discovery of tuples characterizing other types of part-whole relations in the English corpus. This can be explained by prototypical patterns, e.g. “include”, generated regardless of the seeds’ types, and which are highy correlated with, and hence, trigger tuples denoting other part-whole relation types. An almost similar observation was made for the Dutch corpus, except that tuples"
P10-1135,P99-1008,0,0.193479,"Missing"
P10-1135,H93-1061,0,0.0287782,"Missing"
P10-1135,J90-1003,0,0.0615314,"chieves state-of-the-art performance when initialized with relatively small seedsets over the Acquaint corpus (∼ 6M words). Recall is improved with web search queries as additional source of information. Espresso extracts surface patterns connecting the seeds (tuples) in a corpus. The reliability of a pattern p, r(p), given a set of input tuples I, is computed using (3), as its average strength of association with each tuple,i, weighted by each tuple’s reliability, rι (i).  P (3) i∈I rπ (p) = pmi(i,p) maxpmi  ×rι (i) |I| In this equation, pmi(i, p) is the pointwise mutual information score (Church and Hanks, 1990) between a pattern, p (e.g. consist-of), and a tuple, i (e.g. engine-car), and maxpmi is the maximum PMI score between all patterns and tuples. The reliability of the initializing seeds is set to 1. The top-k most reliable patterns are selected to find new tuples. The reliability of each tuple i, rι (i) is computed according to (4), where P is the set of harvested patterns. The top-m most reliable tuples are used to infer new patterns. P (4) rι (i) = i∈I  pmi(i,p)  maxpmi ×rπ (p) |P | The recursive discovery of patterns from tuples and vice-versa is repeated until a threshold number of patte"
P10-1135,J93-1003,0,0.091389,"Missing"
P10-1135,N07-2032,0,0.0688219,"Missing"
P10-1135,P06-1015,0,0.365425,"the individual part-whole relation types? That is, we determine whether 1328 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1328–1336, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a wider variety of unique patterns and tuples are extracted when IE systems target the different types of part-whole relations instead of considering a single part-whole relation that subsumes all the different types. To answer these questions, we bootstrapped a minimally-supervised relation extraction algorithm, based on Espresso (Pantel and Pennacchiotti, 2006), with different seed-sets for the various types of part-whole relations, and analyzed the harvested tuples and patterns. 2 Previous Work Investigations on the part-whole relations span across many disciplines, such as conceptual modeling (Artale et al., 1996; Keet, 2006; Keet and Artale, 2008), which focus on the ontological aspects, and linguistics and cognitive sciences, which focus on natural language semantics. Several linguistically-motivated taxonomies (Odell, 1994; Gerstl and Pribbenow, 1995), based on the work of Winston et al. (1987), have been proposed to clarify the semantics of th"
P10-1135,W09-1301,0,0.0657226,"Missing"
P10-1135,2006.jeptalnrecital-invite.2,0,0.0486717,"Missing"
P90-1021,E89-1009,0,0.13281,"o n e d in the n o n - d e f a u l t p a r t o f t h e d e f i n i t i o n , Is percolated u p to the value automatically. b. will : ( <cat val> = <cat arg> <cat rag> = VP <cat val> =VP 1<cat arg vform> ffi bse l<cat val vform> ffi fin ). T h e lexical f e a t u r e specification d e f a u l t s of GPSG c a n also be incorporated. Certain information holds for m o s t lexlcal items o f a certain category, b u t n o t for p h r a s e s o f t h l s c a t e g o r y . A uniflclatlon-based g r a m m a r t h a t i n c l u d e s a morphological c o m p o n e n t (see, for instance, Calder, 1989 and Evans & Gazdar, 1989), would p r o b a b l y list only (regular) root f o r m s a s lexlcal items. For regular n o u n s , for instance, I, EXICAL D E F A U L T S . Note t h a t the equation <cat val> = <cat arg> will c a u s e all a d d i t i o n a l f e a t u r e s on the a r g u m e n t which are not explicitly mentioned In the n o n - d e f a u l t p a r t of the definition to percolate u p to the value. 170 only the singularform would be listed in the lexicon. S u c h information can be added to lexicon definitions b y m e a n s of a lexlcal default rule: {22) v. N ==> ( 3SG <class> = regular} b. (x~v ffi REF"
P90-1021,1993.eamt-1.1,0,0.0625842,"Missing"
P90-1021,C86-1050,0,\N,Missing
P90-1021,C86-1129,0,\N,Missing
P90-1021,C86-1045,0,\N,Missing
P90-1021,C88-1018,0,\N,Missing
P90-1021,P88-1035,0,\N,Missing
P90-1021,P85-1032,0,\N,Missing
P90-1021,P86-1038,0,\N,Missing
P90-1021,E89-1008,0,\N,Missing
P94-1021,E93-1016,0,0.0255646,"position and scope of adjuncts in such constructions. Delayed evaluation is used to process grammars containing recursive constraints. 1 Recursive 2.1 Introduction Subject-verb agreement Consider a categorial treatment of subject-verb agreement with intransitive ( NP[NOM]S ) and transitive ((NP[NOM]S)/NP[ACC]) verbs defined as follows: Combinations of Categorial Grammar (co) and unification naturally lead to the introduction of polymorphic categories. Thus, Karttunen (1989) categorizes NP&apos;s as X/X, where x is a verbal category, Zeevat el al. (1987) assign the category X/(NPX) to NP&apos;s, and Emms (1993) extends the Lambek-calculus with polymorphic categories to account for coordination, quantifier scope, and extraction. (1) lez(walks,X):iv(X). /ez(kisses, X) :- tv(X). vat[ eat s ] The role of polymorphism has been restricted, however, by the fact that in previous work categories were defined as feature structures using the simple, nonrecursive, constraints familiar from feature description languages such as PATR. Relational constraints can be used to define a range of polymorphic categories that are beyond the expressive capabilities of previous approaches. iv( dir&apos;&apos; arg [ catnp ] )&quot; case n"
P94-1021,J91-3003,0,\N,Missing
P94-1021,J93-4001,0,\N,Missing
R19-1140,D16-1250,0,0.147197,"We perform experiments on Turkish and Finnish as a pair of morphologically complex languages and compare our approach with the baseline models. 2 The Morpheme-Based Alignment Model Figure 1: Morpheme-based cross-lingual alignment model that contains a source side word encoder for morphemes. The encoder is trained to learn morpheme representations in the target space Baselines In this paper, we consider two baseline models. As the first baseline, we employ a simple projection-based CLE method which learns a mapping between embedding spaces by solving the Procrustes problem (Smith et al., 2017; Artetxe et al., 2016). This method first learns a linear transformation matrix to minimize the distance between vectors of word pairs in a seed dictionary by imposing the orthogonality constraint (Gower et al., 2004) and then it uses this matrix to transform the source language embedding space to represent both languages in a shared embedding space. The baseline method is denoted by Procrustes in this paper. As the second baseline, we use relaxed crossdomain similarity local scaling (RCSLS) (Joulin et al., 2018). RCSLS optimizes the transformation matrix by maximizing the cross-domain similarity 1 Code available a"
R19-1140,Q17-1010,0,0.158277,"in similarity local scaling (RCSLS) (Joulin et al., 2018). RCSLS optimizes the transformation matrix by maximizing the cross-domain similarity 1 Code available at: https://bitbucket.org/ ahmetustunn/morphology-sensitive-cle local scaling (CSLS) score, instead of minimizing the distance between word pairs in the training dictionary. CSLS is a modification of cosine similarity commonly used in information retrieval. In this way, RCSLS relaxes the orthogonality constraint used in Procrustes according to a retrieval criterion. Note that for the both baseline models and our model, we use fastText (Bojanowski et al., 2017) to generate monolingual word embeddings. Fasttext represents words as sequence of character ngrams but in many cases this is suboptimal since ¨ un not all character n-grams are morphemes (Ust¨ et al., 2018). Besides that the aim of this study is to incorporate morphology into cross-lingual training, whereas fastText is designed for monolingual training. Morpheme-based Model In the morphemebased model, we extend the projection-based baseline (Procrustes) in order to exploit subword (morpheme) level information for the crosslingual mapping. Our model starts by splitting all words in the source"
R19-1140,W16-1603,0,0.0220837,"Missing"
R19-1140,D17-1070,0,0.210942,"In the figure, x denotes the fixed length word representation generated by the word encoder through morphemes and y represents the target side word embedding. The source encoder is trained to mimic target word embeddings in the bilingual dictionary by minimizing the loss function: Lalign = dist(x, y) − λ(dist(xc , y) + dist(x, yc )) where (x, y) corresponds to the source and target word embeddings, (xc , yc ) is a contrastive 1223 term. λ2 controls the effect of the negative samples in the alignment loss. We use the cosine similarity for the distance measure. For the encoder model, following (Conneau et al., 2017a), we use bidirectional LSTMs with max pooling. It encodes the words in both the forward and the backward direction to capture unidirectional information, then it combines the resulting numbers to form a fixed-size vector by selecting the maximum value over each dimension of the hidden units. Figure 2 shows the encoder model. In the figure, each word vector u is computed from morphemes mn through the bidirectional LSTM encoder. features for both languages by using the Universal Dependency Treebanks (Nivre et al., 2016) and the Universal Morphology (Sylak-Glassman, 2016) project.3 Each word pa"
R19-1140,D18-1269,0,0.0426043,"Missing"
R19-1140,P15-1119,0,0.0733669,"Missing"
R19-1140,D18-1330,0,0.0258817,"Missing"
R19-1140,C12-1089,0,0.0880734,"Missing"
R19-1140,W15-1844,0,0.0343659,"Missing"
R19-1140,P18-1072,0,0.0437631,"Missing"
R19-1140,W18-3019,1,0.859248,"Missing"
R19-1140,P15-2118,0,0.026681,"Missing"
R19-1140,N16-1156,0,0.0306812,"Missing"
redeker-etal-2012-multi,W97-0703,0,\N,Missing
redeker-etal-2012-multi,hendrickx-etal-2008-coreference,1,\N,Missing
redeker-etal-2012-multi,J91-1002,0,\N,Missing
redeker-etal-2012-multi,W04-0213,0,\N,Missing
redeker-etal-2012-multi,W09-3909,0,\N,Missing
redeker-etal-2012-multi,J05-2005,0,\N,Missing
redeker-etal-2012-multi,P09-2020,0,\N,Missing
redeker-etal-2012-multi,P09-1076,0,\N,Missing
redeker-etal-2012-multi,prasad-etal-2008-penn,0,\N,Missing
redeker-etal-2012-multi,J04-3003,0,\N,Missing
W06-1802,W06-2609,1,0.825868,"s to frequent question types can be based on dependency patterns and coreference resolution (Bouma et al., 2005; Mur and van der Plas, 2006), leading to higher recall (compared to systems using surface patterns). Closed-domain (medical) QA can benefit from the fact that dependency relations allow answers to be identified for questions which are not restricted to specific named entity classes, i.e. definitions, causes, symptoms, etc. Answering definition questions, for instance, is a task which has motivated approaches that go well beyond the techniques used for answering factoid questions. In Fahmi and Bouma (2006) it is shown that syntactic patterns can be used to extract potential definition sentences from Wikipedia, and that syntactic features of these sentences (in combination with obvious clues such as the position of the sentence in the document) can be used to improve the accuracy of an automatic classifier which distinguishes definitions from non-definitions in the extracted data set. 4 Joost Joost is a QA system for Dutch which incorporates the features mentioned above, using the Alpino parser for Dutch to parse (offline) the document collections as well as (interactively) user questions. It ha"
W06-1802,H05-1118,0,0.020248,"hich has been used for both open (CLEF) and closed domain QA. 1 Linguistically Informed IR Information retrieval is used in most QA systems to filter out relevant passages from large document collections to narrow down the search for answer extraction modules in a QA system. Given a full syntactic analysis of the text collection, it becomes feasible to exploit linguistic information as a knowledge source for IR. Using Apache’s IR system Lucene, we can index the document collection along various linguistic dimensions, such as part of speech tags, named entity classes, and dependency relations. Tiedemann (2005) uses a genetic algorithm to optimize the use of such an extended IR index, and shows that it leads to significant improvements of IR performance. 2 Acquisition of Lexical Knowledge Syntactic similarity measures can be used for automatic acquisition of lexical knowledge required for QA, as well as for answer extraction and ranking. For instance, in van der Plas and Bouma (2005) it is shown that automatically acquired class-labels for named entities improve the accuracy of answering general WH-questions (i.e. Which ferry sank in the Baltic Sea?) and questions which ask for the definition of a n"
W06-1802,I05-7011,1,0.780113,"Missing"
W06-1802,U04-1002,0,\N,Missing
W06-1802,W02-0908,0,\N,Missing
W06-1802,2006.jeptalnrecital-invite.2,0,\N,Missing
W06-1802,P94-1019,0,\N,Missing
W06-1802,C04-1188,0,\N,Missing
W06-1802,P98-2127,0,\N,Missing
W06-1802,C98-2122,0,\N,Missing
W06-1802,N04-1041,0,\N,Missing
W06-1802,P03-1001,0,\N,Missing
W06-2609,H05-1041,0,0.407739,"ept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use of machine learning techniques can be found in Miliaraki and Androutsopoulos (2004) and Androutsopoulos and Galanis (2005) They use similar patterns as (Joho and Sanderson, 2000) to construct training attributes. Sager and L’Homme (1994) note that the definition of a term should at least always contain genus (term’s category) and species (term’s properties). BlairGoldensohn et al. (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both a genus and species phrase for a given term. There is an intuition that most of definition sentences are located at the beginning of documents. This lead to the use of sentence number as a good indicator of potential definition"
W06-2609,C92-2082,0,0.0538134,"Missing"
W06-2609,N04-1007,0,0.030529,"ttern extractor for their Definder system using a tagger and a finite state grammar. Joho and Sanderson (2000) retrieve descriptive phrases (dp) of query nouns (qn) from text to answer definition questions like Who is qn? Patterns such as ‘dp especially qn’, as utilized by Hearst (1992), are used to extract names and their descriptions. Similar patterns are also applied by Liu et al. (2003) to mine definitions of topic-specific concepts on the Web. As an additional assumption, specific documents dedicated to the concepts can be identified if they have particular HTML and hyperlink structures. Hildebrandt et al. (2004) exploit surface patterns to extract as many relevant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use"
W06-2609,C04-1199,0,0.0929452,"levant ”nuggets” of information of a concept as possible. Similar to our work, a copular pattern NP1 be NP2 is used as one of the extraction patterns. Nuggets which do not begin with a determiner are discarded to filter out spurious nuggets (e.g., progressive tense). Nuggets extracted from every article in a corpus are then stored in a relational database. In the end, answering definition questions becomes as simple as looking up relevant terms from the database. This strategy is similar to our approach for answering definition questions. The use of machine learning techniques can be found in Miliaraki and Androutsopoulos (2004) and Androutsopoulos and Galanis (2005) They use similar patterns as (Joho and Sanderson, 2000) to construct training attributes. Sager and L’Homme (1994) note that the definition of a term should at least always contain genus (term’s category) and species (term’s properties). BlairGoldensohn et al. (2004) uses machine learning and manually crafted lexico-syntactic patterns to match sentences containing both a genus and species phrase for a given term. There is an intuition that most of definition sentences are located at the beginning of documents. This lead to the use of sentence number as a"
W07-1503,bouma-kloosterman-2002-querying,1,0.741949,"Similar studies were carried out by Cassidy (2002) (for an early version of XQuery) and Mayo et al. (2006), who compare the NITE Query Language and XQuery. Below, we first illustrate a task that requires use of XPath only, and then move on to tasks that require the additional functionality of XQuery. 8 www.ims.uni-stuttgart.de/projekte/ TIGER/ 9 www.w3.org/TR/xslt20 10 www.w3.org/TR/xquery 11 www.w3.org/TR/xpath20 12 e.g. exist.sourceforge.net, monetdb.cwi.nl, www.oracle.com/database/berkeley-db/xml 13 See Kay (2005) for a thorough comparison. 20 4.1 Corpus exploration with XPath As argued in Bouma and Kloosterman (2002), XPath provides a powerful query language for formulating linguistically relevant queries, provided that the XML encoding of the treebank reflects the syntactic structure of the trees. Inherent reflexive verbs, for instance, are verbal heads with a rel=’se’ dependent. A verb with an inherently reflexive can therefore be found as follows (remember that in Alpino dependency trees, dependents are actually siblings of the head): //node[@pos=""verb"" and @rel=""hd"" and ../node[@rel=""se""] ] The double slash (’//’) ensures that we search for nodes anywhere within the XML document. The material in brack"
W07-1503,cassidy-2002-xquery,0,0.0117953,"formations of documents, XQuery is primarily intended for extraction of information from XML databases. XQuery is in many respects similar to SQL and is rapidly becoming the standard for XML database systems.12 A distinctive difference between the XSLT and XQuery is the fact that XSLT documents are themselves XML documents, whereas this is not the case for XQuery. This typically makes XQuery more concise and easier to read than XSLT.13 These considerations made us experiment with XQuery as a language for data extraction from syntactically annotated corpora. Similar studies were carried out by Cassidy (2002) (for an early version of XQuery) and Mayo et al. (2006), who compare the NITE Query Language and XQuery. Below, we first illustrate a task that requires use of XPath only, and then move on to tasks that require the additional functionality of XQuery. 8 www.ims.uni-stuttgart.de/projekte/ TIGER/ 9 www.w3.org/TR/xslt20 10 www.w3.org/TR/xquery 11 www.w3.org/TR/xpath20 12 e.g. exist.sourceforge.net, monetdb.cwi.nl, www.oracle.com/database/berkeley-db/xml 13 See Kay (2005) for a thorough comparison. 20 4.1 Corpus exploration with XPath As argued in Bouma and Kloosterman (2002), XPath provides a pow"
W07-1503,W06-2704,0,0.0851495,"tion of information from XML databases. XQuery is in many respects similar to SQL and is rapidly becoming the standard for XML database systems.12 A distinctive difference between the XSLT and XQuery is the fact that XSLT documents are themselves XML documents, whereas this is not the case for XQuery. This typically makes XQuery more concise and easier to read than XSLT.13 These considerations made us experiment with XQuery as a language for data extraction from syntactically annotated corpora. Similar studies were carried out by Cassidy (2002) (for an early version of XQuery) and Mayo et al. (2006), who compare the NITE Query Language and XQuery. Below, we first illustrate a task that requires use of XPath only, and then move on to tasks that require the additional functionality of XQuery. 8 www.ims.uni-stuttgart.de/projekte/ TIGER/ 9 www.w3.org/TR/xslt20 10 www.w3.org/TR/xquery 11 www.w3.org/TR/xpath20 12 e.g. exist.sourceforge.net, monetdb.cwi.nl, www.oracle.com/database/berkeley-db/xml 13 See Kay (2005) for a thorough comparison. 20 4.1 Corpus exploration with XPath As argued in Bouma and Kloosterman (2002), XPath provides a powerful query language for formulating linguistically rele"
W07-1503,oostdijk-2000-spoken,0,0.0285956,"7 Proceedings of the Linguistic Annotation Workshop, pages 17–24, c Prague, June 2007. 2007 Association for Computational Linguistics that requires XQuery. We demonstrate that much of the complexity of advanced tasks can be avoided by providing users with a corpus specific module, that makes available common concepts and functions. 2 The Alpino Treebank format As part of the development of the Alpino parser (Bouma et al., 2001), a number of manually annotated dependency treebanks have been created (van der Beek et al., 2002). Annotation guidelines were adopted from the Corpus of Spoken Dutch (Oostdijk, 2000), a large corpus annotation project for Dutch. In addition, large corpora (e.g. the 80M word Dutch CLEF2 corpus, the 500M word Twente News corpus3 , and Dutch Wikipedia4 ) have been annotated automatically. Both types of treebanks have been used for corpus linguistics (van der Beek, 2005; Villada Moir´on, 2005; Bouma et al., 2007). The automatically annoted treebanks have been used for lexical acquisition (van der Plas and Bouma, 2005), and form the core of a Dutch QA system (Bouma et al., 2005). The format of Alpino dependency trees is illustrated in figure 1. The (somewhat simplified) XML fo"
W07-1503,I05-7011,1,0.824949,"Missing"
W07-1503,I05-6008,0,0.0358828,"Missing"
W07-1503,U04-1002,0,\N,Missing
W07-1503,H05-1091,0,\N,Missing
W09-0107,bouma-kloosterman-2002-querying,1,0.808842,"structures are generated by the lexicon and grammar rules as the value of a dedicated feature. The dependency structures are based on CGN (Corpus Gesproken Nederlands, Corpus of Spoken Dutch) (Hoekstra et al., 2003), D-Coi and LASSY (van Noord et al., 2006). Dependency structures are stored in XML. Advantages of the use of XML include the availability of general purpose search and visualization software. For instance, we exploit XPATH (standard XML query language) to search in large sets of dependency structures, and Xquery to extract information from such large sets of dependency structures (Bouma and Kloosterman, 2002; Bouma and Kloosterman, 2007). 2 Figure 2: Dependency structure for Lager was de koers dan gisteren An anonymous reviewer criticized the analysis, because the extraposition principle would also allow the rightward extraction of comparative phrases licensed by comparatives in topic position. The extraposition principle would have to allow for this in the light of examples such as Extraposition of comparative objects out of topic The first illustration of our thesis that parsed corpora provide an interesting new resource for linguists, constitutes more of an anecdote than a systematic study. We"
W09-0107,van-noord-etal-2006-syntactic,1,0.894225,"Missing"
W09-0107,W07-1503,1,0.855808,"he lexicon and grammar rules as the value of a dedicated feature. The dependency structures are based on CGN (Corpus Gesproken Nederlands, Corpus of Spoken Dutch) (Hoekstra et al., 2003), D-Coi and LASSY (van Noord et al., 2006). Dependency structures are stored in XML. Advantages of the use of XML include the availability of general purpose search and visualization software. For instance, we exploit XPATH (standard XML query language) to search in large sets of dependency structures, and Xquery to extract information from such large sets of dependency structures (Bouma and Kloosterman, 2002; Bouma and Kloosterman, 2007). 2 Figure 2: Dependency structure for Lager was de koers dan gisteren An anonymous reviewer criticized the analysis, because the extraposition principle would also allow the rightward extraction of comparative phrases licensed by comparatives in topic position. The extraposition principle would have to allow for this in the light of examples such as Extraposition of comparative objects out of topic The first illustration of our thesis that parsed corpora provide an interesting new resource for linguists, constitutes more of an anecdote than a systematic study. We include the example, presente"
W09-0107,2006.jeptalnrecital-invite.2,1,0.916168,"Missing"
W09-0107,J07-4004,0,0.0260151,"mous improvements have been achieved in this area. Parsers based on constraint-based formalisms such as HPSG, LFG, and CCG are now fast enough for many applications; they are robust; and they perform much more accurately than previously by incorporating, typically, a statistical disambiguation component. As a consequence, such parsers now obtain competitive, if not superior, performance. Zaenen (2004), for instance, points out that the (LFGbased) XLE parser is fast, has a statistical disambiguation component, and is robust, and thus allows full parsing to be incorporated in many applications. Clark and Curran (2007) show that both accurate and highly efficient parsing is possible using a CCG. As a consequence of this development, massive amounts of parsed sentences now become available. Such large collections of syntactically annotated but not manually verified syntactic analyses are a very useful resource for many purposes. In this position paper we focus on one purpose: linguistic analysis. Our claim is, that very large parsed corpora are an important resource for linguists. Such very large parsed corpora can be used to search systematically for specific infrequent syntactic configurations of interest,"
W09-0107,van-der-wouden-etal-2002-syntactic,0,\N,Missing
W09-1604,W06-2810,0,0.0835815,"Missing"
W09-1604,P04-3018,0,0.016788,"ic]] | ideas = [[Boolean algebra]] }} Figure 1: Infobox and (simplified) Wikimedia source all templates into account. We plan to use information mined from Wikipedia for Question Answering and related tasks. In 2007 and 2008, the CLEF question answering track1 used Wikipedia as text collection. While the usual approach to open domain question answering relies on information retrieval for selecting relevant text snippets, and natural language processing techniques for answer extraction, an alternative stream of research has focussed on the potential of on-line data-sets for question answering (Lita et al., 2004; Katz et al., 2005). In Bouma et al. (2008) it is suggested that information harvested from infoboxes can be used for question answering in CLEF. For instance, the answer to questions such as How high is the Matterhorn?, Where was Piet Mondriaan born?, and What is the area of the country Suriname? can in principle be found in infoboxes. However, in practice the number of questions that is answered by their Dutch QA -system by means information from infoboxes is small. One reason for this is the lack of coverage of infoboxes in Dutch Wikipedia. In the recent GIKICLEF task2 systems have to find"
W09-1604,W08-0409,0,0.0588543,"Missing"
W09-1604,N07-2032,0,0.0144087,"the extracted information, on linking the information with other on-line data repositories, and on interactive access. It contains 274M facts about 2.6M entities (November, 2008). An important component of DbPedia is harvesting of the information present in infoboxes. However, as Wu and Weld (2007) note, not all relevant pages have (complete) infoboxes. The information present in infoboxes is typically also present in the running text of a page. One line of research has concentrated on using the information obtained from infoboxes as seeds for systems that learn relation extraction patterns (Nguyen et al., 2007). Wu and Weld (2007) go one step further, and concentrate on learning to complete the infoboxes themselves. They present a system which first learns to predict the appropriate infobox for a page (using text classification). Next, they learn relation extraction patterns using the information obtained from existing infoboxes as seeds. Finally, the learned patterns are applied to text of pages for which a new infobox has been predicted, to assign values to infobox attributes. A recent paper by Adar et al. (2009) (which only came to our attention at the time of writing) starts from the same observ"
W13-5637,afantenos-etal-2010-learning,0,0.400429,"nto paragraph-size chunks reflecting the topic structure (e.g. Hearst (1997) and Eisenstein (2009)), which is also sometimes called discourse segmentation. We describe a rule-based approach to identify EDUs in Dutch sentences using syntactic and lexical information, and present the results of applying the algorithm to an annotated corpus that contains texts from four different genres. 2 Related work Several successful segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al., 2010; Bach et al., 2012). For Dutch, there is insufficient annotat"
W13-5637,W12-1623,0,0.0595447,"urse segmentation, not to be confused with segmenting a text into paragraph-size chunks reflecting the topic structure (e.g. Hearst (1997) and Eisenstein (2009)), which is also sometimes called discourse segmentation. We describe a rule-based approach to identify EDUs in Dutch sentences using syntactic and lexical information, and present the results of applying the algorithm to an annotated corpus that contains texts from four different genres. 2 Related work Several successful segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al.,"
W13-5637,W10-4311,1,0.842183,"project of Dark & Light Blind Care this disorder was discovered in time][and treated.] (FL14) We do not treat restrictive relative clauses, appositives, and complement clauses as separate EDUs. In line with Tofiloski et al. (2009) we do not consider speech parentheticals ( e.g. &quot;...&quot;, he said) as EDUs. For more details about our segmentation principles, see van der Vliet et al. (2011). 4 Implementation The input for the EDU identification algorithm is a text file containing the text that needs to be segmented. Before applying our segmentation rules we use Alpino (Van Noord et al., 2006), a 1 Borisova and Redeker (2010) show that the identification of SAME-UNIT pseudo-relations in a boundary-segmented corpus can be problematic. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 413 of 474] Dutch dependency parser, to segment the input text into sentences and to create a syntactic tree for each sentence of the text. We then use the syntactic analysis of the sentences to identify the EDUs for each sentence and produce a text file containing all the EDUs of all sentences. For each sentence we start to identify possible ED"
W13-5637,N09-1040,0,0.0283974,"dded to a text automatically with relatively high accuracy. This paper addresses the first step in automatic discourse analysis, namely the identification of suitable Elementary Discourse Units (EDUs) in a text. This process involves segmenting the text into sentences, and decomposing complex sentences into smaller units (typically clauses) that express states of affairs and form the basis for discourse analysis. The task thus concerns finegrained discourse segmentation, not to be confused with segmenting a text into paragraph-size chunks reflecting the topic structure (e.g. Hearst (1997) and Eisenstein (2009)), which is also sometimes called discourse segmentation. We describe a rule-based approach to identify EDUs in Dutch sentences using syntactic and lexical information, and present the results of applying the algorithm to an annotated corpus that contains texts from four different genres. 2 Related work Several successful segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that goo"
W13-5637,P07-1062,0,0.0239704,"(e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al., 2010; Bach et al., 2012). For Dutch, there is insufficient annotated data to apply machine learning techniques. In van der Vliet (2010) we present a basic rule-based segmenter that makes use of syntactic information and punctuation for the automatic identification of segment boundaries in Dutch text. This automatic discourse segmentation algorithm uses the common approach of identifying EDUs by determining the segment boundaries in sentences. A well-known complication for this type of approach is the occurrence of embedded discourse u"
W13-5637,J97-1003,0,0.358701,"relations can be added to a text automatically with relatively high accuracy. This paper addresses the first step in automatic discourse analysis, namely the identification of suitable Elementary Discourse Units (EDUs) in a text. This process involves segmenting the text into sentences, and decomposing complex sentences into smaller units (typically clauses) that express states of affairs and form the basis for discourse analysis. The task thus concerns finegrained discourse segmentation, not to be confused with segmenting a text into paragraph-size chunks reflecting the topic structure (e.g. Hearst (1997) and Eisenstein (2009)), which is also sometimes called discourse segmentation. We describe a rule-based approach to identify EDUs in Dutch sentences using syntactic and lexical information, and present the results of applying the algorithm to an annotated corpus that contains texts from four different genres. 2 Related work Several successful segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their"
W13-5637,W10-4327,0,0.027137,"mbedded discourse units. We use syntactic and lexical information to decompose sentences into EDUs. Experimental results show that our algorithm for EDU identification performs well on texts of various genres. KEYWORDS: discourse analysis, elementary discourse units, segmentation. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 411 of 474] 1 Introduction Discourse structures can be useful as input for several other tasks, such as automatic summarization (Marcu, 2000; Thione et al., 2004; Bosma, 2008; Louis et al., 2010), question answering (Verberne et al., 2007) and information extraction (Maslennikov and Chua, 2007). However, the manual analysis of discourse relations in a text is a time-consuming task. The use of discourse relations for other applications thus presupposes that these relations can be added to a text automatically with relatively high accuracy. This paper addresses the first step in automatic discourse analysis, namely the identification of suitable Elementary Discourse Units (EDUs) in a text. This process involves segmenting the text into sentences, and decomposing complex sentences into s"
W13-5637,P07-1075,0,0.0233233,"o EDUs. Experimental results show that our algorithm for EDU identification performs well on texts of various genres. KEYWORDS: discourse analysis, elementary discourse units, segmentation. Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 411 of 474] 1 Introduction Discourse structures can be useful as input for several other tasks, such as automatic summarization (Marcu, 2000; Thione et al., 2004; Bosma, 2008; Louis et al., 2010), question answering (Verberne et al., 2007) and information extraction (Maslennikov and Chua, 2007). However, the manual analysis of discourse relations in a text is a time-consuming task. The use of discourse relations for other applications thus presupposes that these relations can be added to a text automatically with relatively high accuracy. This paper addresses the first step in automatic discourse analysis, namely the identification of suitable Elementary Discourse Units (EDUs) in a text. This process involves segmenting the text into sentences, and decomposing complex sentences into smaller units (typically clauses) that express states of affairs and form the basis for discourse ana"
W13-5637,redeker-etal-2012-multi,1,0.889431,"Missing"
W13-5637,W09-3813,0,0.36559,"(2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al., 2010; Bach et al., 2012). For Dutch, there is insufficient annotated data to apply machine learning techniques. In van der Vliet (2010) we present a basic rule-based segmenter that makes use of syntactic information and punctuation for the automatic identification of segment boundaries in Dutch text. This automatic discourse segmentation algorithm uses the common approach of identifying EDUs by determining the segment boundaries in sentences. A well-known complication for this type of approach is the occurrence of embedded discourse units that interrupt an ongoing EDU, as in"
W13-5637,N03-1030,0,0.0490139,"segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al., 2010; Bach et al., 2012). For Dutch, there is insufficient annotated data to apply machine learning techniques. In van der Vliet (2010) we present a basic rule-based segmenter that makes use of syntactic information and punctuation for the automatic identification of segment boundaries in Dutch text. This automatic discourse segmentation algorithm uses the common approach of identifying EDUs by determining the segment boundaries in sentences. A well-known complication for this type o"
W13-5637,H05-1033,0,0.0273209,"been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba and Di Eugenio, 2007; Sagae, 2009; Hernault et al., 2010; Bach et al., 2012). For Dutch, there is insufficient annotated data to apply machine learning techniques. In van der Vliet (2010) we present a basic rule-based segmenter that makes use of syntactic information and punctuation for the automatic identification of segment boundaries in Dutch text. This automatic discourse segmentation algorithm uses the common approach of identifying EDUs by determining the segment boundaries in sentences. A well-known complication for this type of approach is the occurrence"
W13-5637,W04-1009,0,0.0245546,"Missing"
W13-5637,P09-2020,0,0.259637,"course analysis. The task thus concerns finegrained discourse segmentation, not to be confused with segmenting a text into paragraph-size chunks reflecting the topic structure (e.g. Hearst (1997) and Eisenstein (2009)), which is also sometimes called discourse segmentation. We describe a rule-based approach to identify EDUs in Dutch sentences using syntactic and lexical information, and present the results of applying the algorithm to an annotated corpus that contains texts from four different genres. 2 Related work Several successful segmentation systems have been developed for English (e.g. Tofiloski et al. (2009), Subba and Di Eugenio (2007) and Bach et al. (2012)), German (Lüngen et al., 2006) and French (Afantenos et al., 2010) written text. Tofiloski et al. (2009) show for their English data that good segmentation results can be obtained by a rule-based approach. For English, the RST Discourse Treebank (Carlson et al., 2002), a substantial corpus segmented and manually annotated for discourse structure, has been widely used for the development of machine learning approaches to discourse segmentation for English text (Soricut and Marcu, 2003; Sporleder and Lapata, 2005; Fisher and Roark, 2007; Subba"
W13-5637,2006.jeptalnrecital-invite.2,0,\N,Missing
W17-0403,W15-2103,0,0.0275219,"n crucially relies on the fact that we can use the conversion script to convert Alpino output to UD. 2 Conversion Process Conversion of a manually verified treebank to UD is possible if the underlying annotation contains the information that is required to do a mapping from the original annotation to POS-tags and bilexical dependencies that is conformant with the annotation guidelines of the UD project. By doing an automatic conversion, we follow a strategy that has been used to create many of the other UD treebanks as well (Zeman et al., 2014; Johannsen et al., 2015; Øvrelid and Hohle, 2016; Ahrenberg, 2015; Lynn and Foster, 2016). Conversion of Lassy to UD POS-tags can be achieved by means of a simple set of case statements that refer to the original POS-tag and a small set of morphological feature values. The only case that is more involved is the distinction between verbs and auxiliaries. This distinction is missing in the POS-tags and morphological features of the Lassy treebanks, but can be reconstructed using the lemma and valency of the verb (i.e., a limited set of verbs that select for only a subject and a 2 Currently, no secondary edges are used in the Small. 20 UD Lassy Lassy UD Interp"
W17-0403,P16-1231,0,0.040033,"Missing"
W17-0403,W06-2920,0,0.0789443,"nt cases are listed in Table 1. We have used the conversion script to create UD 3 All sentences from the training section containing the adverb ook (also). 21 root predicative phrase as xcomp, and a case of an incomplete word (part of a coordination) marked as X (in accordance with the original annotation but not the best option according to UD), We also tried to compare Lassy Small with the UD Dutch corpus that has been included in UD since v1.2. The latter corpus is a conversion of the Alpino treebank (van der Beek et al., 2002). It was used in the CONLL X shared task on dependency parsing (Buchholz and Marsi, 2006) and converted at that point to CONLL format. The UD version is based on a conversion to HamleDT to UD (Zeman et al., 2014). The various conversion steps have lead to loss of information,4 and apparent mistakes,5 and the quality of this corpus in general seems to be lower than the UD Lassy Small corpus. A more systematic comparison will be possible once we have been able to reconstruct the original sources of the material included in the Alpino treebank fragment used for CONLL. At that point, it will also be possible to create an improved version of the data using the current conversion script"
W17-0403,Q16-1032,0,0.0434811,"Missing"
W17-0403,meyers-etal-2004-annotating,0,0.0709743,"ociated with the verbal domain as well as dependents associated with the nominal domain, as in example (2). Here, a verb clearly heads a nominal phrase, as it is introduced by a determiner. Yet, at the same time, it selects an inherent reflexive pronoun, something that is not possible for nouns. The dependency annotation for this example in Figure 2 also shows that the PP phrase is labeled nmod, giving preference to the nominal interpretation of verzekeren. Note that the the parallelism between (1) and (2) suggests that it could perhaps also have been labeled obl. In fact, the NomBank corpus (Meyers et al., 2004) adopts the rule that the same semantic role labels should be used as much as possible for verbs and nominalised versions of these verbs. Cross-lingual comparison The inventory of dependency labels in UD is a mixed functional-structural system, which distinguishes oblique arguments, for instance, on the basis of their part-of-speech, i.e. a PP dependent is labeled obl, a dependent clause advcl, and an adverbial advmod. Also, attachment to predicates is differentiated from attachment to nominals. The original Lassy Small treebank has both phrasal categories and dependency labels, and seems to m"
W17-0403,L16-1250,0,0.0217454,"y treebank. The comparison crucially relies on the fact that we can use the conversion script to convert Alpino output to UD. 2 Conversion Process Conversion of a manually verified treebank to UD is possible if the underlying annotation contains the information that is required to do a mapping from the original annotation to POS-tags and bilexical dependencies that is conformant with the annotation guidelines of the UD project. By doing an automatic conversion, we follow a strategy that has been used to create many of the other UD treebanks as well (Zeman et al., 2014; Johannsen et al., 2015; Øvrelid and Hohle, 2016; Ahrenberg, 2015; Lynn and Foster, 2016). Conversion of Lassy to UD POS-tags can be achieved by means of a simple set of case statements that refer to the original POS-tag and a small set of morphological feature values. The only case that is more involved is the distinction between verbs and auxiliaries. This distinction is missing in the POS-tags and morphological features of the Lassy treebanks, but can be reconstructed using the lemma and valency of the verb (i.e., a limited set of verbs that select for only a subject and a 2 Currently, no secondary edges are used in the Small. 20 UD Lass"
W17-0403,2006.jeptalnrecital-invite.2,1,0.851114,"Missing"
W18-6003,D17-1137,0,0.298496,"Missing"
W18-6003,L16-1262,1,0.89472,"Missing"
W18-6003,W05-0406,0,0.430106,"D treebanks, and present recommendations for the annotation of expletives so that more consistent annotation can be achieved in future releases. 1 Introduction Universal Dependencies (UD) is a framework for morphosyntactic annotation that aims to provide useful information for downstream NLP applications in a cross-linguistically consistent fashion (Nivre, 2015; Nivre et al., 2016). Many such applications require an analysis of referring expressions. In co-reference resolution, for example, it is important to be able to separate anaphoric uses of pronouns such as it from non-referential uses (Boyd et al., 2005; Evans, 2001; Uryupina et al., 2016). Accurate translation of pronouns is another challenging problem, sometimes relying on coreference resolution, and where one of the choices is to not translate a pronoun at all. The latter situation occurs for instance when translating from a 2 What is an Expletive? The UD initiative aims to provide a syntactic annotation scheme that can be applied cross18 Proceedings of the Second Workshop on Universal Dependencies (UDW 2018), pages 18–26 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics pronoun, and there, identical to"
W18-6003,W17-1505,0,0.0151815,"Gosse Bouma∗◦ Jan Hajic†◦ Dag Haug‡◦ Joakim Nivre•◦ Per Erik Solberg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of expletive elements in treebanks"
W18-6003,guillou-etal-2014-parcor,0,0.0313846,"berg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of expletive elements in treebanks and the UD annotation scheme introduces a dedicated dep"
W18-6003,W15-2501,0,0.0255507,"sal Dependency Treebanks Gosse Bouma∗◦ Jan Hajic†◦ Dag Haug‡◦ Joakim Nivre•◦ Per Erik Solberg‡◦ Lilja Øvrelid?◦ ∗ University of Groningen, Centre for Language and Cognition Charles University in Prague, Faculty of Mathematics and Physics, UFAL ‡ University of Oslo, Department of Philosophy, Classics, History of Arts and Ideas • Uppsala University, Department of Linguistics and Philology ? University of Oslo, Department of Informatics ◦ Center for Advanced Study at the Norwegian Academy of Science and Letters † Abstract language that has expletives into a language that does not use expletives (Hardmeier et al., 2015; Werlen and Popescu-Belis, 2017). The ParCor co-reference corpus (Guillou et al., 2014) distinguishes between anaphoric, event referential, and pleonastic use of the English pronoun it. Lo´aiciga et al. (2017) train a classifier to predict the different uses of it in English using among others syntactic information obtained from an automatic parse of the corpus. Being able to distinguish referential from non-referential noun phrases is potentially important also for tasks like question answering and information extraction. Applications like these motivate consistent and explicit annotation of"
W19-4206,W13-3520,0,0.0695643,"Missing"
W19-4206,W16-2002,0,0.0246736,"tional Linguistics architecture for both lemmatization and tagging which are described in Section 3.2 and 3.3. using dataset embeddings. 2 Related work 3 Our system is based on three main approaches which are heavily studied in existing literature. These are sequence-to-sequence learning, multitask learning and multi-lingual learning. Recent work on computational morphology showed that neural sequence-to-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given c"
W19-4206,D18-1335,0,0.0550576,"Missing"
W19-4206,N19-1155,0,0.0141909,"nt state of the decoder ht , the character attention cct and the morphological tags ti,1:γ of the target word. The probability of the output lemma characters are then predicted through a softmax layer. (8) e ht = decoder(ht , cct , ti,1:γ ) p(li,t |e ht ) = softmax(e ht ) (9) 3.3 (15) In order to exploit morphological features during lemmatization, we give the morphological tags ti:γ which are predicted by the tag decoder, as part of input to the lemma decoder. Independent of their order, the entire set of the tags are encoded by a simple feed-forward layer as described in the baseline model (Malaviya et al., 2019) and the resulting vector is concatenated with the input embeddings for each target word. The last part of the lemma decoder is the attention network which is the same character-level attention model as in the tag decoder. The character attention mechanism allows the lemma decoder to compute an attention vector cct based on the output states of the word encoder. The attention vector is then passed into a concatenation layer to generate the output state e ht of the decoder for each lemma character li,t . (11) (12) j e ht = tanh(Wcc [cct ; ht ]) (14) (10) Together with the context attention, we"
W19-4206,W18-6011,0,0.0251833,"ve epochs (patience). 4.1 Dataset Embeddings Experiments In this section, we will describe the data used in our experiments as well as evaluate the effectiveness of our external embeddings setup and the dataset embeddings with in a variety of settings. In all experiments we use +E and -E to indicate the model with and without external embeddings, and +D and -D for dataset embeddings. 5.1 Data The test data of SIGMORPHON 2019 task 2 consists of a collection of datasets released in the Universal Dependencies project (Nivre et al., 2018), which are automatically converted to the UniMorph Schema (McCarthy et al., 2018). In total, we evaluate our model on 107 datasets, covering 66 languages. After empirically looking at the trade-off between data-size and training time, we decided to limit each dataset to its first 250,000 tokens for all experiments. This speeded up the training considerably, with almost no loss in performance. For the tuning of our model, we selected a sub-set of datasets from the main benchmark. More specifically, we aimed to get a diversion of language-family, size, and morphological richness (here proxied by the average amount of morphological tags per word). To ensure we do not overfit"
W19-4206,W14-1701,0,0.0217034,"then separately generate lemmas and morphological tags using these representations by using multiple attention mechanisms. Our contributions are threefold: Introduction This paper presents our model for the SIGMORPHON 2019 Task 2 on morphological analysis and lemmatization in context (McCarthy et al., 2019). The task is to generate a lemma and a sequence of morphological tags, which are called morphosyntactic descriptions (MSD), for each word in a given sentence. This task is important because it can be used to improve several downstream NLP applications such as grammatical error correction (Ng et al., 2014), machine translation (Conforti et al., 2018) and multilingual parsing (Zeman et al., 2018). Table 1 shows the lemma and morphological tags of: Johnny likes cats. The first sub-task, Lemmatization, is to transform an inflected word form to its lemma which is its base-form (or dictionary form), as in the example of likes to like. The second sub-task, morphological tagging, is to predict morphological properties of words as a sequence of tags, including a part of speech tag. These morphological tags specify the inflections encoded in word-forms. In the • We introduce the use of multiple attentio"
W19-4206,P16-2067,0,0.0231974,"models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given context, and use a shared encoder within a multi-task architecture consisting of multiple decoder similar to our model. Multi-lingual learning approaches which benefit from joint learning for multiple languages is also studied on various tasks with different architectures. Ammar et al. (2016) uses a language embedding that contains information considering the language, word-order properties and typological properties for"
W19-4206,K18-2011,0,0.0503291,"Missing"
W19-4206,P16-2038,0,0.0278568,"uence-to-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014) have yielded new stateof-the-art performance on various tasks including morphological reinflection and lemmatization (Cotterell et al., 2016, 2017, 2018). Building on this, Dayanık et al. (2018) utilize different levels of representations such as character-level, word-level and sentence-level in the encoder of their seq2seq architecture based on previous work (Heigold et al., 2017). Multi-task learning approaches for jointly learning related tasks have been successfully employed on syntactic and semantic tasks (Søgaard and Goldberg, 2016; Plank et al., 2016). In the context of morphological analysis, this has been used by Kementchedjhieva et al. (2018), who jointly learn morphosyntactic tags and inflections for a word in a given context, and use a shared encoder within a multi-task architecture consisting of multiple decoder similar to our model. Multi-lingual learning approaches which benefit from joint learning for multiple languages is also studied on various tasks with different architectures. Ammar et al. (2016) uses a language embedding that contains information considering the language, word-order properties and typolo"
W19-4206,D18-1473,0,0.0538249,"Missing"
W97-0614,P96-1009,0,0.0261893,"d by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are described. Yet, as part of this it also des"
W97-0614,H91-1034,0,0.0202964,"is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. whole sentences are des"
W97-0614,H89-1043,0,0.0331708,"easible in terms of accuracy and computational resources, and thus is a viable alternative to pure concept spotting. Although the added benefit of grammatical analysis over concept spotting is not clear for our relatively simple application, the grammatical approach may become essential as soon as the application is extended in such a way that mor~ complicated grammatical constructions need to be recognized. In that case, simple concept spotting may not be able to correctly process all constructions, whereas the capabilities of the grammatical approach extend much further. Whereas some (e.g. (Moore et al., 1989)) argue that grammatical analysis may improve recognition accuracy, our current experiments have as yet not been able to reveal a clear advantage in this respect. As the basis for our implementation we have chosen definite-clause grammars (DCGs) (Pereira and Warren, 1980), a flexible formalism which is related to various kinds of common linguistics description, and which allows application of various parsing algorithms. DCGs can be translated directly into Prolog, for which interpreters and compilers exist that are fast enough to handle real-time processing of spoken input. The grammar for OVI"
W97-0614,J97-3004,1,0.875354,"Missing"
W97-0614,H89-1018,0,0.0360997,"ation, which is a version of a German system developed by Philips Dialogue Systems in Aachen (Aust et al., 1995), adapted to Dutch. This German system processes spoken input using ""concept spotting"", which means that the smallest information-carrying units in the input are extracted, such as names of train stations and expressions of time, and these are translated more or less individually into updates of the internal database representing the dialogue state. The words between the concepts thus perceived are ignored. The use of concept spotting is common in spokenlanguage information systems (Ward, 1989; Jackson et al., 1991; Aust et al., 1995; Allen et al., 1996). Arguments in favour of this kind of shallow parsing is that it is relatively easy to develop the NLP component, since larger sentence constructs do not have 66 to be taken into account, and that the robustness of the parser is enhanced, since sources of ungrammaticality occurring between concepts are skipped and therefore do not hinder the translation of the utterance to updates. The prototype presently under construction departs from the use of concept spotting. The grammar for OVIS describes grarnrnat&apos;icaluser utterances, i.e. w"
W97-1513,E93-1010,1,0.858596,"Missing"
W97-1513,P94-1021,1,0.886061,"Missing"
W97-1513,C94-1039,1,0.88654,"Missing"
W97-1513,C96-2197,0,0.067302,"Missing"
W99-0802,H94-1109,0,0.0184743,"olog. The native language of practically all students is Dutch. The aim of the introductory course is to provide a overview of language technology applications, of the concepts and techniques used to develop such applications, and to let students gain practical experience in developing (components) of these applications. The second course focuses on computational semantics and the construction of natural language interfaces using computational grammars. Course material for computational linguistics exists primarily in the form of text books, such as Allen (1987), Gazdar and Mellish (1989) and Covington (1994). They focus primarily on basic concepts and techniques (finite state automata, definite clause grammar, parsing algorithms, construction of semantic representations, etc.) and the implementation of toy systems for experimenting with these techniques. If course-ware is provided, it consists of the code and grammar fragments discussed in the text-material. The language used for illustration is primarily English. While attention for basic concepts and techniques is indispensable for any course in this field, one may wonder whether implementation issues need to be so prominent as they are in the"
W99-0802,P95-1003,0,0.0619598,"Missing"
W99-0802,W97-1513,1,0.898777,"Missing"
