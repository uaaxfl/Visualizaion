2021.sigdial-1.9,{ARTA}: Collection and Classification of Ambiguous Requests and Thoughtful Actions,2021,-1,-1,3,1,1438,shohei tanaka,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Human-assisting systems such as dialogue systems must take thoughtful, appropriate actions not only for clear and unambiguous user requests, but also for ambiguous user requests, even if the users themselves are not aware of their potential requirements. To construct such a dialogue agent, we collected a corpus and developed a model that classifies ambiguous user requests into corresponding system actions. In order to collect a high-quality corpus, we asked workers to input antecedent user requests whose pre-defined actions could be regarded as thoughtful. Although multiple actions could be identified as thoughtful for a single user request, annotating all combinations of user requests and system actions is impractical. For this reason, we fully annotated only the test data and left the annotation of the training data incomplete. In order to train the classification model on such training data, we applied the positive/unlabeled (PU) learning method, which assumes that only a part of the data is labeled with positive examples. The experimental results show that the PU learning method achieved better performance than the general positive/negative (PN) learning method to classify thoughtful actions given an ambiguous user request."
2021.iwslt-1.1,{FINDINGS} {OF} {THE} {IWSLT} 2021 {EVALUATION} {CAMPAIGN},2021,-1,-1,14,0,832,antonios anastasopoulos,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"The evaluation campaign of the International Conference on Spoken Language Translation (IWSLT 2021) featured this year four shared tasks: (i) Simultaneous speech translation, (ii) Offline speech translation, (iii) Multilingual speech translation, (iv) Low-resource speech translation. A total of 22 teams participated in at least one of the tasks. This paper describes each shared task, data and evaluation metrics, and reports results of the received submissions."
2021.iwslt-1.3,{NAIST} {E}nglish-to-{J}apanese Simultaneous Translation System for {IWSLT} 2021 Simultaneous Text-to-text Task,2021,-1,-1,9,1,5723,ryo fukuda,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,This paper describes NAIST{'}s system for the English-to-Japanese Simultaneous Text-to-text Translation Task in IWSLT 2021 Evaluation Campaign. Our primary submission is based on wait-k neural machine translation with sequence-level knowledge distillation to encourage literal translation.
2021.iwslt-1.24,On Knowledge Distillation for Translating Erroneous Speech Transcriptions,2021,-1,-1,2,1,5723,ryo fukuda,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"Recent studies argue that knowledge distillation is promising for speech translation (ST) using end-to-end models. In this work, we investigate the effect of knowledge distillation with a cascade ST using automatic speech recognition (ASR) and machine translation (MT) models. We distill knowledge from a teacher model based on human transcripts to a student model based on erroneous transcriptions. Our experimental results demonstrated that knowledge distillation is beneficial for a cascade ST. Further investigation that combined knowledge distillation and fine-tuning revealed that the combination consistently improved two language pairs: English-Italian and Spanish-English."
2021.iwslt-1.27,Large-Scale {E}nglish-{J}apanese Simultaneous Interpretation Corpus: Construction and Analyses with Sentence-Aligned Data,2021,-1,-1,2,0,5729,kosuke doi,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the construction of a new large-scale English-Japanese Simultaneous Interpretation (SI) corpus and presents the results of its analysis. A portion of the corpus contains SI data from three interpreters with different amounts of experience. Some of the SI data were manually aligned with the source speeches at the sentence level. Their latency, quality, and word order aspects were compared among the SI data themselves as well as against offline translations. The results showed that (1) interpreters with more experience controlled the latency and quality better, and (2) large latency hurt the SI quality."
2021.humeval-1.5,Is This Translation Error Critical?: Classification-Based Human and Automatic Machine Translation Evaluation Focusing on Critical Errors,2021,-1,-1,1,1,1440,katsuhito sudoh,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"This paper discusses a classification-based approach to machine translation evaluation, as opposed to a common regression-based approach in the WMT Metrics task. Recent machine translation usually works well but sometimes makes critical errors due to just a few wrong word choices. Our classification-based approach focuses on such errors using several error type labels, for practical machine translation evaluation in an age of neural machine translation. We made additional annotations on the WMT 2015-2017 Metrics datasets with fluency and adequacy labels to distinguish different types of translation errors from syntactic and semantic viewpoints. We present our human evaluation criteria for the corpus development and automatic evaluation experiments using the corpus. The human evaluation corpus will be publicly available upon publication."
2020.iwslt-1.21,{NAIST}{'}s Machine Translation Systems for {IWSLT} 2020 Conversational Speech Translation Task,2020,-1,-1,2,1,5723,ryo fukuda,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes NAIST{'}s NMT system submitted to the IWSLT 2020 conversational speech translation task. We focus on the translation disfluent speech transcripts that include ASR errors and non-grammatical utterances. We tried a domain adaptation method by transferring the styles of out-of-domain data (United Nations Parallel Corpus) to be like in-domain data (Fisher transcripts). Our system results showed that the NMT model with domain adaptation outperformed a baseline. In addition, slight improvement by the style transfer was observed."
2020.coling-main.234,Improving Spoken Language Understanding by Wisdom of Crowds,2020,-1,-1,3,0.40217,1439,koichiro yoshino,Proceedings of the 28th International Conference on Computational Linguistics,0,"Spoken language understanding (SLU), which converts user requests in natural language to machine-interpretable expressions, is becoming an essential task. The lack of training data is an important problem, especially for new system tasks, because existing SLU systems are based on statistical approaches. In this paper, we proposed to use two sources of the {``}wisdom of crowds,{''} crowdsourcing and knowledge community website, for improving the SLU system. We firstly collected paraphrasing variations for new system tasks through crowdsourcing as seed data, and then augmented them using similar questions from a knowledge community website. We investigated the effects of the proposed data augmentation method in SLU task, even with small seed data. In particular, the proposed architecture augmented more than 120,000 samples to improve SLU accuracies."
2020.coling-main.319,Incorporating Noisy Length Constraints into Transformer with Length-aware Positional Encodings,2020,-1,-1,3,0,5724,yui oka,Proceedings of the 28th International Conference on Computational Linguistics,0,"Neural Machine Translation often suffers from an under-translation problem due to its limited modeling of output sequence lengths. In this work, we propose a novel approach to training a Transformer model using length constraints based on length-aware positional encoding (PE). Since length constraints with exact target sentence lengths degrade translation performance, we add random noise within a certain window size to the length constraints in the PE during the training. In the inference step, we predict the output lengths using input sequences and a BERT-based length prediction model. Experimental results in an ASPEC English-to-Japanese translation showed the proposed method produced translations with lengths close to the reference ones and outperformed a vanilla Transformer (especially in short sentences) by 3.22 points in BLEU. The average translation results using our length prediction model were also better than another baseline method using input lengths for the length constraints. The proposed noise injection improved robustness for length prediction errors, especially within the window size."
2020.acl-srw.8,Reflection-based Word Attribute Transfer,2020,-1,-1,2,0,22487,yoichi ishibashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop,0,"Word embeddings, which often represent such analogic relations as king - man + woman queen, can be used to change a word{'}s attribute, including its gender. For transferring king into queen in this analogy-based manner, we subtract a difference vector man - woman based on the knowledge that king is male. However, developing such knowledge is very costly for words and attributes. In this work, we propose a novel method for word attribute transfer based on reflection mappings without such an analogy operation. Experimental results show that our proposed method can transfer the word attributes of the given words without changing the words that do not have the target attributes."
2020.acl-main.327,Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model,2020,-1,-1,2,0,6008,kosuke takahashi,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance."
W19-4106,Conversational Response Re-ranking Based on Event Causality and Role Factored Tensor Event Embedding,2019,29,0,3,1,1438,shohei tanaka,Proceedings of the First Workshop on NLP for Conversational AI,0,"We propose a novel method for selecting coherent and diverse responses for a given dialogue context. The proposed method re-ranks response candidates generated from conversational models by using event causality relations between events in a dialogue history and response candidates (e.g., {``}be stressed out{''} precedes {``}relieve stress{''}). We use distributed event representation based on the Role Factored Tensor Model for a robust matching of event causality relations due to limited event causality knowledge of the system. Experimental results showed that the proposed method improved coherency and dialogue continuity of system responses."
D19-5601,Findings of the Third Workshop on Neural Generation and Translation,2019,25,1,8,0,4384,hiroaki hayashi,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language."
Y18-3001,Overview of the 5th Workshop on {A}sian Translation,2018,0,5,2,0,283,toshiaki nakazawa,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
W18-2711,Multi-Source Neural Machine Translation with Missing Data,2018,11,2,2,0,28399,yuta nishimura,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"Multi-source translation is an approach to exploit multiple inputs (e.g. in two different languages) to increase translation accuracy. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in TED talks, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol {\textless}NULL{\textgreater}. These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the {\textless}NULL{\textgreater} tokens achieved higher translation accuracies measured by BLEU than those by any one-to-one NMT systems."
W17-5712,A Simple and Strong Baseline: {NAIST}-{NICT} Neural Machine Translation System for {WAT}2017 {E}nglish-{J}apanese Translation Task,2017,0,0,2,1,296,yusuke oda,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"This paper describes the details about the NAIST-NICT machine translation system for WAT2017 English-Japanese Scientific Paper Translation Task. The system consists of a language-independent tokenizer and an attentional encoder-decoder style neural machine translation model. According to the official results, our system achieves higher translation accuracy than any systems submitted previous campaigns despite simple model architecture."
W17-4709,Tree as a Pivot: Syntactic Matching Methods in Pivot Translation,2017,0,2,3,0,31611,akiva miura,Proceedings of the Second Conference on Machine Translation,0,None
W17-3208,An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation,2017,6,3,5,0,303,makoto morishita,Proceedings of the First Workshop on Neural Machine Translation,0,"Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling."
W16-4607,Neural Reordering Model Considering Phrase Translation and Word Alignment for Phrase-based Translation,2016,13,2,2,0,23269,shin kanouchi,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper presents an improved lexicalized reordering model for phrase-based statistical machine translation using a deep neural network. Lexicalized reordering suffers from reordering ambiguity, data sparseness and noises in a phrase table. Previous neural reordering model is successful to solve the first and second problems but fails to address the third one. Therefore, we propose new features using phrase translation and word alignment to construct phrase vectors to handle inherently noisy phrase translation pairs. The experimental results show that our proposed method improves the accuracy of phrase reordering. We confirm that the proposed method works well with phrase pairs including NULL alignments."
W16-4621,{C}hinese-to-{J}apanese Patent Machine Translation based on Syntactic Pre-ordering for {WAT} 2016,2016,-1,-1,1,1,1440,katsuhito sudoh,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper presents our Chinese-to-Japanese patent machine translation system for WAT 2016 (Group ID: ntt) that uses syntactic pre-ordering over Chinese dependency structures. Chinese words are reordered by a learning-to-rank model based on pairwise classification to obtain word order close to Japanese. In this year{'}s system, two different machine translation methods are compared: traditional phrase-based statistical machine translation and recent sequence-to-sequence neural machine translation with an attention mechanism. Our pre-ordering showed a significant improvement over the phrase-based baseline, but, in contrast, it degraded the neural machine translation baseline."
C16-1021,Exploring Text Links for Coherent Multi-Document Summarization,2016,34,11,4,1,35692,xun wang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Summarization aims to represent source documents by a shortened passage. Existing methods focus on the extraction of key information, but often neglect coherence. Hence the generated summaries suffer from a lack of readability. To address this problem, we have developed a graph-based method by exploring the links between text to produce coherent summaries. Our approach involves finding a sequence of sentences that best represent the key information in a coherent way. In contrast to the previous methods that focus only on salience, the proposed method addresses both coherence and informativeness based on textual linkages. We conduct experiments on the DUC2004 summarization task data set. A performance comparison reveals that the summaries generated by the proposed system achieve comparable results in terms of the ROUGE metric, and show improvements in readability by human evaluation."
W15-5012,{C}hinese-to-{J}apanese Patent Machine Translation based on Syntactic Pre-ordering for{WAT} 2015,2015,5,3,1,1,1440,katsuhito sudoh,Proceedings of the 2nd Workshop on {A}sian Translation ({WAT}2015),0,None
P15-2023,Discriminative Preordering Meets Kendall{'}s $\\tau$ Maximization,2015,23,7,3,1,37418,sho hoshino,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper explores a simple discriminative preordering model for statistical machine translation. Our model traverses binary constituent trees, and classifies whether children of each node should be reordered. The model itself is not extremely novel, but herein we introduce a new procedure to determine oracle labels so as to maximize Kendallxe2x80x99s xcfx84 . Experiments in Japanese-to-English translation revealed that our simple method is comparable with, or superior to, state-of-the-art methods in translation accuracy."
N15-1030,Empty Category Detection With Joint Context-Label Embeddings,2015,21,2,2,1,35692,xun wang,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This paper presents a novel technique for empty category (EC) detection using distributed word representations. A joint model is learned from the labeled data to map both the distributed representations of the contexts of ECs and EC types to a low dimensional space. In the testing phase, the context of possible EC positions will be projected into the same space for empty category detection. Experiments on Chinese Treebank prove the effectiveness of the proposed method. We improve the precision by about 6 points on a subset of Chinese Treebank, which is a new state-ofthe-art performance on CTB."
2014.iwslt-evaluation.18,{NTT}-{NAIST} syntax-based {SMT} systems for {IWSLT} 2014,2014,26,0,1,1,1440,katsuhito sudoh,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents NTT-NAIST SMT systems for English-German and German-English MT tasks of the IWSLT 2014 evaluation campaign. The systems are based on generalized minimum Bayes risk system combination of three SMT systems using the forest-to-string, syntactic preordering, and phrase-based translation formalisms. Individual systems employ training data selection for domain adaptation, truecasing, compound word splitting (for GermanEnglish), interpolated n-gram language models, and hypotheses rescoring using recurrent neural network language models."
2014.amta-researchers.18,{J}apanese-to-{E}nglish patent translation system based on domain-adapted word segmentation and post-ordering,2014,35,0,1,1,1440,katsuhito sudoh,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"This paper presents a Japanese-to-English statistical machine translation system specialized for patent translation. Patents are practically useful technical documents, but their translation needs different efforts from general-purpose translation. There are two important problems in the Japanese-to-English patent translation: long distance reordering and lexical translation of many domain-specific terms. We integrated novel lexical translation of domain-specific terms with a syntax-based post-ordering framework that divides the machine translation problem into lexical translation and reordering explicitly for efficient syntax-based translation. The proposed lexical translation consists of a domain-adapted word segmentation and an unknown word transliteration. Experimental results show our system achieves better translation accuracy in BLEU and TER compared to the baseline methods."
Y13-1026,Effects of Parsing Errors on Pre-Reordering Performance for {C}hinese-to-{J}apanese {SMT},2013,19,3,4,1,33211,dan han,"Proceedings of the 27th Pacific Asia Conference on Language, Information, and Computation ({PACLIC} 27)",0,"Linguistically motivated reordering methods have been developed to improve word alignment especially for Statistical Machine Translation (SMT) on long distance language pairs. However, since they highly rely on the parsing accuracy, it is useful to explore the relationship between parsing and reordering. For Chinese-toJapanese SMT, we carry out a three-stage incremental comparative analysis to observe the effects of different parsing errors on reordering performance by combining empirical and descriptive approaches. For the empirical approach, we quantify the distribution of general parsing errors along with reordering qualities whereas for the descriptive approach, we extract seven influential error patterns and examine their correlation with reordering errors."
W13-2806,Using unlabeled dependency parsing for pre-reordering for {C}hinese-to-{J}apanese statistical machine translation,2013,24,6,4,1,33211,dan han,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"Chinese and Japanese have a different sentence structure. Reordering methods are effective, but need reliable parsers to extract the syntactic structure of the source sentences. However, Chinese has a loose word order, and Chinese parsers that extract the phrase structure do not perform well. We propose a framework where only POS tags and unlabeled dependency parse trees are necessary, and linguistic knowledge on structural difference can be encoded in the form of reordering rules. We show significant improvements in translation quality of sentences from news domain, when compared to state-of-the-art reordering methods."
P13-2119,Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation,2013,23,62,3,0.555928,5136,kevin duh,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Data selection is an effective approach to domain adaptation in statistical machine translation. The idea is to use language models trained on small in-domain text to select similar sentences from large general-domain corpora, which are then incorporated into the training data. Substantial gains have been demonstrated in previous works, which employ standard ngram language models. Here, we explore the use of neural language models for data selection. We hypothesize that the continuous vector representation of words in neural language models makes them more effective than n-grams for modeling unknown word contexts, which are prevalent in general-domain text. In a comprehensive evaluation of 4 language pairs (English to German, French, Russian, Spanish), we found that neural language models are indeed viable tools for data selection: while the improvements are varied (i.e. 0.1 to 1.7 gains in BLEU), they are fast to train on small in-domain data and can sometimes substantially outperform conventional n-grams."
I13-1147,Two-Stage Pre-ordering for {J}apanese-to-{E}nglish Statistical Machine Translation,2013,15,12,3,1,37418,sho hoshino,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,We propose a new rule-based pre-ordering method for Japanese-to-English statistical machine translation that employs heuristic rules in two-stages. This two-stage framework contributes to experimental results that our method outperforms conventional rule-based methods in BLEU and RIBES.
D13-1021,Noise-Aware Character Alignment for Bootstrapping Statistical Machine Transliteration from Bilingual Corpora,2013,8,8,1,1,1440,katsuhito sudoh,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,This paper proposes a novel noise-aware character alignment method for bootstrapping statistical machine transliteration from automatically extracted phrase pairs. The model is an extension of a Bayesian many-to-many alignment method for distinguishing nontransliteration (noise) parts in phrase pairs. It worked effectively in the experiments of bootstrapping Japanese-to-English statistical machine transliteration in patent domain using patent bilingual corpora.
D13-1139,Shift-Reduce Word Reordering for Machine Translation,2013,15,4,2,0.666667,12296,katsuhiko hayashi,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,This paper presents a novel word reordering model that employs a shift-reduce parser for inversion transduction grammars. Our model uses rich syntax parsing features for word reordering and runs in linear time. We apply it to postordering of phrase-based machine translation (PBMT) for Japanese-to-English patent tasks. Our experimental results show that our method achieves a significant improvement of 3.1 BLEU scores against 30.15 BLEU scores of the baseline PBMT system.
2013.iwslt-evaluation.12,{NTT}-{NAIST} {SMT} systems for {IWSLT} 2013,2013,25,1,1,1,1440,katsuhito sudoh,Proceedings of the 10th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents NTT-NAIST SMT systems for English-German and German-English MT tasks of the IWSLT 2013 evaluation campaign. The systems are based on generalized minimum Bayes risk system combination of three SMT systems: forest-to-string, hierarchical phrase-based, phrasebased with pre-ordering. Individual SMT systems include data selection for domain adaptation, rescoring using recurrent neural net language models, interpolated language models, and compound word splitting (only for German-English)."
W12-4207,Head Finalization Reordering for {C}hinese-to-{J}apanese Machine Translation,2012,31,10,2,1,33211,dan han,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In Statistical Machine Translation, reordering rules have proved useful in extracting bilingual phrases and in decoding during translation between languages that are structurally different. Linguistically motivated rules have been incorporated into Chinese-to-English (Wang et al., 2007) and English-to-Japanese (Isozaki et al., 2010b) translation with significant gains to the statistical translation system. Here, we carry out a linguistic analysis of the Chinese-to-Japanese translation problem and propose one of the first reordering rules for this language pair. Experimental results show substantially improvements (from 20.70 to 23.17 BLEU) when head-finalization rules based on HPSG parses are used, and further gains (to 24.14 BLEU) were obtained using more refined rules."
W12-4213,Zero Pronoun Resolution can Improve the Quality of {J}-{E} Translation,2012,12,12,2,0,2121,hirotoshi taira,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"In Japanese, particularly, spoken Japanese, subjective, objective and possessive cases are very often omitted. Such Japanese sentences are often translated by Japanese-English statistical machine translation to the English sentence whose subjective, objective and possessive cases are omitted, and it causes to decrease the quality of translation. We performed experiments of J-E phrase based translation using Japanese sentence, whose omitted pronouns are complemented by human. We introduced 'antecedent F-measure' as a score for measuring quality of the translated English. As a result, we found that it improves the scores of antecedent F-measure while the BLEU scores were almost unchanged. Every effectiveness of the zero pronoun resolution differs depending on the type and case of each zero pronoun."
P12-2020,A Comparative Study of Target Dependency Structures for Statistical Machine Translation,2012,21,2,2,0.714286,6319,xianchao wu,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"This paper presents a comparative study of target dependency structures yielded by several state-of-the-art linguistic parsers. Our approach is to measure the impact of these non-isomorphic dependency structures to be used for string-to-dependency translation. Besides using traditional dependency parsers, we also use the dependency structures transformed from PCFG trees and predicate-argument structures (PASs) which are generated by an HPSG parser and a CCG parser. The experiments on Chinese-to-English translation show that the HPSG parser's PASs achieved the best dependency and translation accuracies."
P12-1001,Learning to Translate with Multiple Objectives,2012,37,11,2,0.702348,5136,kevin duh,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality.n n Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization."
I11-1004,Extracting Pre-ordering Rules from Predicate-Argument Structures,2011,26,20,2,0.714286,6319,xianchao wu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Word ordering remains as an essential problem for translating between languages with substantial structural differences, such as SOV and SVO languages. In this paper, we propose to automatically extract pre-ordering rules from predicateargument structures. A pre-ordering rule records the relative position mapping of a predicate word and its argument phrases from the source language side to the target language side. We propose 1) a lineartime algorithm to extract the pre-ordering rules from word-aligned HPSG-tree-tostring pairs and 2) a bottom-up algorithm to apply the extracted rules to HPSG trees to yield target language style source sentences. Experimental results are reported for large-scale English-to-Japanese translation, showing significant improvements of BLEU score compared with the baseline SMT systems."
I11-1153,Generalized Minimum {B}ayes Risk System Combination,2011,19,11,2,0.76981,5136,kevin duh,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Minimum Bayes Risk (MBR) has been used as a decision rule for both singlesystem decoding and system combination in machine translation. For system combination, we argue that common MBR implementations are actually not correct, since probabilities in the hypothesis space cannot be reliably estimated. These implementations achieve the effect of consensus decoding (which may be beneficial in its own right), but does not reduce Bayes Risk in the true Bayesian sense. We introduce Generalized MBR, which parameterizes the loss function in MBR and allows it to be optimized in the given hypothesis space of multiple systems. This extension better approximates the true Bayes Risk decision rule and empirically improves over MBR, even in cases where the combined systems are of mixed quality."
2011.mtsummit-papers.11,Alignment Inference and {B}ayesian Adaptation for Machine Translation,2011,-1,-1,2,0.76981,5136,kevin duh,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.34,Extracting Pre-ordering Rules from Chunk-based Dependency Trees for {J}apanese-to-{E}nglish Translation,2011,-1,-1,2,0.714286,6319,xianchao wu,Proceedings of Machine Translation Summit XIII: Papers,0,None
2011.mtsummit-papers.36,Post-ordering in Statistical Machine Translation,2011,-1,-1,1,1,1440,katsuhito sudoh,Proceedings of Machine Translation Summit XIII: Papers,0,None
W10-1736,Head Finalization: A Simple Reordering Rule for {SOV} Languages,2010,18,64,2,0,36870,hideki isozaki,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"English is a typical SVO (Subject-Verb-Object) language, while Japanese is a typical SOV language. Conventional Statistical Machine Translation (SMT) systems work well within each of these language families. However, SMT-based translation from an SVO language to an SOV language does not work well because their word orders are completely different. Recently, a few groups have proposed rule-based preprocessing methods to mitigate this problem (Xu et al., 2009; Hong et al., 2009). These methods rewrite SVO sentences to derive more SOV-like sentences by using a set of handcrafted rules. In this paper, we propose an alternative single reordering rule: Head Finalization. This is a syntax-based preprocessing approach that offers the advantage of simplicity. We do not have to be concerned about part-of-speech tags or rule weights because the powerful Enju parser allows us to implement the rule at a general level. Our experiments show that its result, Head Final English (HFE), follows almost the same order as Japanese. We also show that this rule improves automatic evaluation scores."
W10-1757,N-Best Reranking by Multitask Learning,2010,37,9,2,0.76981,5136,kevin duh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We propose a new framework for N-best reranking on sparse feature sets. The idea is to reformulate the reranking problem as a Multitask Learning problem, where each N-best list corresponds to a distinct task.n n This is motivated by the observation that N-best lists often show significant differences in feature distributions. Training a single reranker directly on this heteroge-nous data can be difficult.n n Our proposed meta-algorithm solves this challenge by using multitask learning (such as e1/e2 regularization) to discover common feature representations across N-best lists. This meta-algorithm is simple to implement, and its modular approach allows one to plug-in different learning algorithms from existing literature. As a proof of concept, we show statistically significant improvements on a machine translation system involving millions of features."
W10-1762,Divide and Translate: Improving Long Distance Reordering in Statistical Machine Translation,2010,27,29,1,1,1440,katsuhito sudoh,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper proposes a novel method for long distance, clause-level reordering in statistical machine translation (SMT). The proposed method separately translates clauses in the source sentence and reconstructs the target sentence using the clause translations with non-terminals. The non-terminals are placeholders of embedded clauses, by which we reduce complicated clause-level reordering into simple word-level reordering. Its translation model is trained using a bilingual corpus with clause-level alignment, which can be automatically annotated by our alignment algorithm with a syntactic parser in the source language. We achieved significant improvements of 1.4% in BLEU and 1.3% in TER by using Moses, and 2.2% in BLEU and 3.5% in TER by using our hierarchical phrase-based SMT, for the English-to-Japanese translation of research paper abstracts in the medical domain."
D10-1092,Automatic Evaluation of Translation Quality for Distant Language Pairs,2010,14,155,4,0,36870,hideki isozaki,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Automatic evaluation of Machine Translation (MT) quality is essential to developing high-quality MT systems. Various evaluation metrics have been proposed, and BLEU is now used as the de facto standard metric. However, when we consider translation between distant language pairs such as Japanese and English, most popular metrics (e.g., BLEU, NIST, PER, and TER) do not work well. It is well known that Japanese and English have completely different word orders, and special care must be paid to word order in translation. Otherwise, translations with wrong word order often lead to misunderstanding and incomprehensibility. For instance, SMT-based Japanese-to-English translators tend to translate 'A because B' as 'B because A.' Thus, word order is the most important problem for distant language translation. However, conventional evaluation metrics do not significantly penalize such word order mistakes. Therefore, locally optimizing these metrics leads to inadequate translations. In this paper, we propose an automatic evaluation metric based on rank correlation coefficients modified with precision. Our meta-evaluation of the NTCIR-7 PATMT JE task data shows that this metric outperforms conventional metrics."
C10-1050,Hierarchical Phrase-based Machine Translation with Word-based Reordering Model,2010,23,11,3,0.666667,12296,katsuhiko hayashi,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Hierarchical phrase-based machine translation can capture global reordering with synchronous context-free grammar, but has little ability to evaluate the correctness of word orderings during decoding. We propose a method to integrate word-based reordering model into hierarchical phrase-based machine translation to overcome this weakness. Our approach extends the synchronous context-free grammar rules of hierarchical phrase-based model to include reordered source strings, allowing efficient calculation of reordering model scores during decoding. Our experimental results on Japanese-to-English basic travel expression corpus showed that the BLEU scores obtained by our proposed system were better than those obtained by a standard hierarchical phrase-based machine translation system."
2010.iwslt-papers.5,Analysis of translation model adaptation in statistical machine translation,2010,15,15,2,0.76981,5136,kevin duh,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"Numerous empirical results have shown that combining data from multiple domains often improve statistical machine translation (SMT) performance. For example, if we desire to build SMT for the medical domain, it may be beneficial to augment the training data with bitext from another domain, such as parliamentary proceedings. Despite the positive results, it is not clear exactly how and where additional outof-domain data helps in the SMT training pipeline. In this work, we analyze this problem in detail, considering the following hypotheses: out-of-domain data helps by either (a) improving word alignment or (b) improving phrase coverage. Using a multitude of datasets (IWSLT-TED, EMEA, Europarl, OpenSubtitles, KDE), we show that sometimes outof-domain data may help word alignment more than it helps phrase coverage, and more flexible combination of data along different parts of the training pipeline may lead to better results."
2010.iwslt-evaluation.19,{NTT} statistical {MT} system for {IWSLT} 2010,2010,8,0,1,1,1440,katsuhito sudoh,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2008.iwslt-evaluation.13,{NTT} statistical machine translation system for {IWSLT} 2008.,2008,18,3,1,1,1440,katsuhito sudoh,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,The NTT Statistical Machine Translation System consists of two primary components: a statistical machine translation decoder and a reranker. The decoder generates k-best translation canditates using a hierarchical phrase-based translation based on synchronous context-free grammar. The decoder employs a linear feature combination among several real-valued scores on translation and language models. The reranker reorders the k-best translation candidates using Ranking SVMs with a large number of sparse features. This paper describes the two components and presents the results for the evaluation campaign of IWSLT 2008.
2007.iwslt-1.16,Larger feature set approach for machine translation in {IWSLT} 2007,2007,28,2,3,0,128,taro watanabe,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"The NTT Statistical Machine Translation System employs a large number of feature functions. First, k-best translation candidates are generated by an efficient decoding method of hierarchical phrase-based translation. Second, the k-best translations are reranked. In both steps, sparse binary features {---} of the order of millions {---} are integrated during the search. This paper gives the details of the two steps and shows the results for the Evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT) 2007."
P06-1078,Incorporating Speech Recognition Confidence into Discriminative Named Entity Recognition of Speech Data,2006,18,16,1,1,1440,katsuhito sudoh,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper proposes a named entity recognition (NER) method for speech recognition results that uses confidence on automatic speech recognition (ASR) as a feature. The ASR confidence feature indicates whether each word has been correctly recognized. The NER model is trained using ASR results with named entity (NE) labels as well as the corresponding transcriptions with NE labels. In experiments using support vector machines (SVMs) and speech data from Japanese newspaper articles, the proposed method outperformed a simple application of text-based NER to ASR results in NER F-measure by improving precision. These results show that the proposed method is effective in NER for noisy inputs."
