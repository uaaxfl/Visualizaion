2007.sigdial-1.8,W06-3001,0,0.148651,"Missing"
2007.sigdial-1.8,W04-2504,0,0.629109,"itions (there called focus shifts). For each node type, they list certain focus shift candidates, i.e., the items that are likely to come into focus in a coherent discourse (cf. Table 1). While their list of focus shift targets for the different node types is comprehensive, this is at the same time a major problem when it comes to a practical implementation: it is not at all clear how to (algorithmically) determine the correct node types, and thus the viable candidate targets for informational transitions. In a related approach that targets IQA dialogues rather than single-speaker discourse, (Chai and Jin, 2004) define informational transitions between subsequent user questions in IQA dialogues in terms of the question “topic”. The topic is either of type entity or activity and closely resembles the object and activity node types given in Table 1. While the informational state is now described in terms of only two types of elements (entity/object and activity/action) instead of the five postulated by (McCoy and Cheng, 1991), the rich set of discourse roles that these elements can introduce would still render an automatic construction of a representation of the informational state extremely difficult."
2007.sigdial-1.8,J05-1004,0,0.024287,"atedness From (McCoy and Cheng, 1991), we adopt the general idea of introducing candidate focus shift targets that represent coherent continuations of the discourse (or in our case, dialogue). To avoid the difficulty of choosing between up to five different node types that could represent the current focus of attention, we restrict ourselves to just action-type nodes. This is advantageous in two ways. On the one hand, actions correspond to verbs, which are inherently connected to some argument structure defining the verb’s semantic roles. By querying available lexical resources like PropBank (Palmer et al., 2005), we can retrieve the verb’s arguments. The corresponding semantic roles of the verb yield possible topics of follow-up questions. Thus, we can take advantage of existing lexical resources to automatically find focus nodes that represent followup questions involving any of the semantic roles of the verb. On the other hand, we conjecture that actions/verbs form a suitable and robust basis for describing the (informational) meaning of utterances in IQA, since most user utterances include a predicate (or an implicit reference to some predicate in the dialogue history), and syntactic parsers can b"
2014.lilt-9.5,P98-1013,0,0.0943933,"margin remains high. Its bounds are given by the accuracy of the structures the parser has learned from. Better structures in the learning sample should lead to better parsing across the whole corpus to be parsed, and it is possible that in the future incorporating DS measures in parsing preferences might lead to better results, perhaps to the point of modeling human garden-path e↵ects. See Manning (2011) for similar considerations with respect to part-of-speech tagging. 5 An alternative approach is to rely on lexical resources that contain rich semantic information about content words (e.g., Baker et al. 1998; Fellbaum 1998; Kipper et al. 2008). We find the corpus route more appealing because it is not a priori Frege in Space / 247 meaning representations should be objects that compose together to form more complex meanings, while accounting for how composition causes more or less sytematic shifts in word meaning, as in the cocomposition, co-predication and coercion examples above. Moreover, the meaning of content words as we can extract them from a corpus should be able to combine with the meaning of grammatical words, formal semantics’ special focus, in ways that account for the importance of st"
2014.lilt-9.5,E12-1004,1,0.180531,"Ps, involving nouns from the same domain and mostly the same or a related quantifying determiner. Note moreover how the neighbours of the count usage of some in some cats are, consistently, other expressions involving counting of distinct individuals. The mass usage with co↵ee, on the other hand, tends to attract other constructions involving quantifying amounts of substances. It should be possible to learn, by regressing on training examples of this sort, that some has a di↵erent meaning when modifying a count or a mass noun, as illustrated in the toy SOME matrix in the Section 3.4 above. In Baroni et al. (2012), we have shown that corpus-harvested distributional vectors for DPs with a quantifying determiner contain enough information for a statistical algorithm to correctly learn and generalize the entailment status of pairs of DPs represented distributionally. For example, if we extract from the corpus distributional vectors for a few thousand entailing (each dog|=some dog) and non-entailing (many cats6|=all cats) pairs, and we feed them as labeled training data to a ma286 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli chine learning program, the program is then able, given an arbitrary"
2014.lilt-9.5,J10-4006,1,0.852432,"ot an informative quantity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o"
2014.lilt-9.5,D10-1115,1,0.119557,"essions, distributional semantics must find ways to extract from finite evidence an estimate of how their distributional profile would look if we had an infinite corpus available. For words, a large but finite corpus provides a sample of possible contexts of sufficient size to constitute a decent surrogate of infinity; for most phrases and sentences, it does not (given that there is an infinite number of possible phrases and sentences), and we need a di↵erent, compositional strategy to come up with indirect estimates of their distributional profiles. Building on what we originally proposed in Baroni and Zamparelli (2010), we present an approach to compositional distributional semantics that relies on Frege’s (1892) distinction between “complete” and “incomplete” expressions. Specifically, we distinguish between words whose meaning is directly determined by their distributional behaviour, e.g. nouns, and words that act as functions transforming the distributional profile of other words (e.g., verbs). As discussed in Section 2, representations for the former can be directly induced from their patterns of co-occurrence in a corpus. We add to this standard practice a new view on the incomplete expressions and tre"
2014.lilt-9.5,basile-etal-2012-developing,0,0.00721642,"Ps, involving nouns from the same domain and mostly the same or a related quantifying determiner. Note moreover how the neighbours of the count usage of some in some cats are, consistently, other expressions involving counting of distinct individuals. The mass usage with co↵ee, on the other hand, tends to attract other constructions involving quantifying amounts of substances. It should be possible to learn, by regressing on training examples of this sort, that some has a di↵erent meaning when modifying a count or a mass noun, as illustrated in the toy SOME matrix in the Section 3.4 above. In Baroni et al. (2012), we have shown that corpus-harvested distributional vectors for DPs with a quantifying determiner contain enough information for a statistical algorithm to correctly learn and generalize the entailment status of pairs of DPs represented distributionally. For example, if we extract from the corpus distributional vectors for a few thousand entailing (each dog|=some dog) and non-entailing (many cats6|=all cats) pairs, and we feed them as labeled training data to a ma286 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli chine learning program, the program is then able, given an arbitrary"
2014.lilt-9.5,W11-1304,0,0.00751678,"nstructions with a specific verb. These matrices are estimated from corpus-extracted examples of &lt;subject, subject verb object&gt; vector pairs (picking, of course, subject-verb-object structures that occur with a certain frequency in the corpus, in order to be able to extract meaningful distributional vectors for them). After estimating a suitable number of such matrices for a variety of objects of the same verb (e.g., “eat cake”, “eat meat”, “eat snacks”), we use pairs of corpus-derived object vectors and the has recently been proposed as a benchmarking task for distributional semantic models (Biemann and Giesbrecht 2011). 53 Georgiana Dinu (p.c.) has developed a method to estimate higher-order tensors in just one step: However, the method requires the same training data as the multistep method, that is conceptually simpler. 54 In the conclusion, we will come back to some important issues pertaining to this treatment of verbs, such as how to handle changes in argument structure. 55 Like Grefenstette et al. (2013), we ignore for purposes of all examples discussed in this subsection the inflection of the verb and number of nouns and DPs. 290 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli STEP 1: ESTIM"
2014.lilt-9.5,D12-1050,0,0.0783256,"successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correctly, all function words – determiners, negation, etc. – in the test sentences). T"
2014.lilt-9.5,S12-1023,0,0.075745,"nly) content words. On the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 20"
2014.lilt-9.5,D12-1112,0,0.0645636,"nly) content words. On the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 20"
2014.lilt-9.5,W11-2503,1,0.744028,"ling that DSMs have been applied with some success to the Voynich Manuscript, a 15th century text written in an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly"
2014.lilt-9.5,J06-1003,0,0.00467225,"ion when developing a DSM pertains to defining what is a context for purposes of counting cooccurrences. Definitions of context range from simple ones (such as documents or the occurrence of another word inside a fixed window from the target word) to more linguistically sophisticated ones (such as the occurrence of words of certain syntactic categories connected to the target by special syntactic relations) (Curran and Moens 2002a; Grefenstette 1994; Pad´o and Lapata 2007; Sahlgren 2006; Turney and Pantel 2010). Di↵erent contexts capture di↵erent kinds of semantic similarity or “relatedness” (Budanitsky and Hirst 2006). At the two extremes, counting documents as contexts captures “topical” relations (the words war and Afghanistan will have a high cosine, because they often co-occur in documents), whereas DSMs based on word cooccurrence within narrow windows or constrained by syntactic relations tend to capture tighter “taxonomic” relations (such as the one between dog and hyena). Unsurprisingly, no single definition of context is appropriate for all tasks, and the jury on the “best” context model is still out (Sahlgren 2008). Next, raw target-context counts are typically transformed into association scores"
2014.lilt-9.5,J07-4004,0,0.135481,"Natural language parsers, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing than for syntax. As a consequence, data-driven semantics—which would in principle be a way to address the vastness of lexical meanings—has not advanced as rapidly as datadriven syntax. What sort of data-driven"
2014.lilt-9.5,P07-2009,0,0.179443,"on; assign to the lexical entries syntactic categories that correspond to their semantic types. This procedure allows one to proceed in parallel in the composition of the syntactic and semantic constructions. Besides this theoretical advantage, employing a CG framework has practical benefits, because of the existence of a fast and wide-coverage syntactic parser, namely C&C parser(Clark and Curran 2007), based on (Combinatory) Categorial Grammar(Steedman 2000). This parser is also integrated with Boxer, a system that builds a referential semantic tier using Discourse Representation Structures (Curran et al. 2007), thus allowing us to maintain the same large-scale approach that characterizes lexical DSMs in our compositional component, and providing a concrete infrastructure for the possibility of parallel distributional/referential representations built from the same semantic structures. Of course, other lexicalized formal grammars could also be considered; CG is just the one that might allow the integration in the most 294 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli straightforward way.58 In denotational semantics the semantic types give the type of the domain of denotation (e.g., the d"
2014.lilt-9.5,W02-0908,0,0.158862,"2.2), and then turn to some theoretical issues pertaining to them (Sections from 2.3 to 2.6). More thorough recent introductions to DSMs are provided by Clark (2013b), Erk (2012) and Turney and Pantel (2010). 2.1 Parameters of DSMs Most research on DSMs focuses on the many parameters of the pipeline to extract distributional vectors from corpora.8 Surprisingly, there is relatively little research on how the nature of the source corpus a↵ects the quality of the resulting vectors, but, as in many other areas of computational linguistics, the general consensus is that “more data is better data” (Curran and Moens 2002b). The most popular data source 8 Bullinaria and Levy (2007, 2012) provide a sysstematic evaluation of how some of the pipeline parameters a↵ect DSM quality. Frege in Space / 251 is the British National Corpus,9 a 100 million word corpus attempting to provide a “balanced” sample of various registers and genres of both written and spoken English. More recently, larger corpora (in the order of a few billion words), often made up of Web documents (including Wikipedia pages), are also widely used.10 Probably the most important decision when developing a DSM pertains to defining what is a context"
2014.lilt-9.5,P02-1030,0,0.289837,"2.2), and then turn to some theoretical issues pertaining to them (Sections from 2.3 to 2.6). More thorough recent introductions to DSMs are provided by Clark (2013b), Erk (2012) and Turney and Pantel (2010). 2.1 Parameters of DSMs Most research on DSMs focuses on the many parameters of the pipeline to extract distributional vectors from corpora.8 Surprisingly, there is relatively little research on how the nature of the source corpus a↵ects the quality of the resulting vectors, but, as in many other areas of computational linguistics, the general consensus is that “more data is better data” (Curran and Moens 2002b). The most popular data source 8 Bullinaria and Levy (2007, 2012) provide a sysstematic evaluation of how some of the pipeline parameters a↵ect DSM quality. Frege in Space / 251 is the British National Corpus,9 a 100 million word corpus attempting to provide a “balanced” sample of various registers and genres of both written and spoken English. More recently, larger corpora (in the order of a few billion words), often made up of Web documents (including Wikipedia pages), are also widely used.10 Probably the most important decision when developing a DSM pertains to defining what is a context"
2014.lilt-9.5,D10-1113,0,0.188269,"nted in a lower-dimensionality space whose components (deriving from the original ones via a statis9 http://www.natcorp.ox.ac.uk/ 10 See for example http://wacky.sslmit.unibo.it/. A potential problem with Web corpora is their systematic skewness, as in the probable overassociation of page with home. This can presumably be addressed with better sampling and filtering techniques (see Fletcher 2004, 2012). 252 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli tical process that considers their correlation patterns) should capture more robust “latent” aspects of meaning (Blei et al. 2003; Dinu and Lapata 2010; Griffiths et al. 2007; Landauer and Dumais 1997; Sahlgren 2005; Sch¨ utze 1997). Although it is not strictly a parameter in the construction of DSMs, researchers measure distributional (and thus semantic) similarity of pairs of target words with di↵erent similarity functions. The already introduced cosine of the angle formed by vectors is the most natural, geometrically justified and widely used of these functions.11 We conclude this short survey of DSM engineering by observing that, while in our discussion below we assume that the components of a semantic space can be interpreted as a distr"
2014.lilt-9.5,W10-2803,0,0.0126382,"n the face of it, standard DSMs do not address the issue of polysemy, since they represent each word with a single distributional vector. In the common case in which a word has more than one facet of meaning (ranging from full-fledged instances of homonymy such as river bank vs. central bank to subtler alternations such as chicken in the farm vs. chicken in the oven or the cases of co-predication and coercion discussed in the introduction), the distributional vector will be a summary of these facets. There has however been a lot of work on handling polysemy in DSMs (e.g., Boleda et al. 2012a; Erk 2010; Pantel and Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; K"
2014.lilt-9.5,D08-1094,0,0.507855,"Missing"
2014.lilt-9.5,P10-2017,0,0.0643032,"Missing"
2014.lilt-9.5,N10-1011,0,0.078307,"Missing"
2014.lilt-9.5,N10-3005,0,0.00967894,"ns that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose here of using tensors to encode composition functions. Our view on the syntax-semantics relations traces back to the traditional type-logical approach introduced by van Benthem in 1986, following which we have exploited the Curry-Howard correspondence between logical rules and lambda-calculus rules to obtain a proof term for a parsed structure."
2014.lilt-9.5,W13-0112,0,0.193813,"efine a composition function generating the distributional vector of some co↵ee from that of co↵ee, it stands to reason that we define a function that approximates the actual distributional vector of some co↵ee. Of course, not many corpus-extracted phrases (and very few sentences) are common enough to find enough occurrences of them in a corpus to extract meaningful distributional vectors (that’s why we want composition in the first place). However, we only need a few, reasonably frequent examples for each composition function to be learned by regression. In the transitive verb experiments of Grefenstette et al. (2013), good results were obtained with as little as 10 training examples per verb. Corpus-extracted phrase vectors as targets of learning Given the centrality of learning from phrase examples for our approach, we have collected various forms of empirical evidence that, at least for adjective-noun constructions (ANs) and DPs, phrase vectors directly extracted from the corpus make good semantic sense. It is thus reasonable to use them as our target of learning. In Baroni and Zamparelli (2010), we have presented qualitative ev284 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli TABLE 4: The 3"
2014.lilt-9.5,D11-1129,0,0.062427,"hus, the (cosine of the) angle of the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if"
2014.lilt-9.5,W11-2507,0,0.0720733,"hus, the (cosine of the) angle of the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if"
2014.lilt-9.5,W10-2805,0,0.10123,"ity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) an"
2014.lilt-9.5,W12-1702,0,0.0096488,"ns, bats, lionesses), which makes them difficult to handle in straightforward logical quantification terms. Two aspects of generics suggest that distributional semantics might have something to contribute to this task. First, sentences such as “lions have a mane” express a facet of our general knowledge about a concept – in this case, that of a lion (Carlson 2009, draws an explicit connection between generics and conceptual knowledge). Not surprisingly, DSMs derived from large corpora are good at extracting general world knowledge about concepts (Almuhareb and Poesio 2005; Baroni et al. 2010; Kelly et al. 2012), so it is reasonable to expect their compositional extension to capture valid statements about properties of concepts. Second, acceptability judgments about generic statements are not sharp. “Lions live in Africa” is perfect, “lions live in Europe” sounds funny but it is not nearly as bizarre and obviously false as “lions live on the moon”. Again, it is easier to model this sort of gradience in the geometric framework of distributional semantics than in truth-functional terms. In particular, there might be a relation between the way in which we just proposed to handle semantic acceptability a"
2014.lilt-9.5,E12-1024,0,0.0123192,"ork that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose here of using tensors to encode composition functions. Our view on the syn"
2014.lilt-9.5,W11-0607,0,0.0122606,"w frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines be"
2014.lilt-9.5,I11-1162,0,0.0100776,"an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly grasp the meaning of the word dog). The dominant tradition in formal semantics stresses instead another type of rel"
2014.lilt-9.5,P11-1060,0,0.00973588,"2008, and Section 5.1 for our view of Garrette et al. 2013 and Turney 2012). Here, we limit our discussion to work that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from compositi"
2014.lilt-9.5,J93-2004,0,0.0471575,"he problem of assigning reasonable (if not exhaustive) syntactic structures to arbitrary, real-life sentences is perhaps equally hard. Here, however, technology has provided an important part of the answer: Natural language parsers, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing"
2014.lilt-9.5,P04-1003,0,0.0342518,"y the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines between vectors of word pairs can predict whether"
2014.lilt-9.5,P08-1028,0,0.416123,"Lin 2002; Sch¨ utze 1998) showing that these models are actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; Kintsch 2001; Mitchell and Lapata 2008; Reisinger and Mooney 2010; Thater et al. 2009) has focused on the goal of modeling word meaning in context. There is a clear connection between distributional models of word meaning in context and distributional models of compositionality, which is the main topic of this article. For example, Mitchell and Lapata (2008) discriminate between the senses of running in water runs vs. horse runs by composing vectors representing the two phrases, whereas Erk and Pad´ o (2008) and others approach the same task in terms of how the runs vector changes due to contextual e↵ects triggered by the presence"
2014.lilt-9.5,D09-1045,0,0.0139036,"aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correctly, all function words – determiners, negation, etc. – in the test sentences). The ML models are also very well-suited (and empirically e↵ective) for tasks that we will not consider here under the rubric of compositionality but which do involve looking at sentences and larger passages, such as measuring textual coherence (Foltz et al. 1998) or predicting the next word that will be uttered (Mitchell and Lapata 2009). Besides their good empirical performance, the ML models are extremely easy to implement, which makes them, undoubtedly, the best current choice for practical applications. Criticism of vector-mixture models There are principled reasons, however, to believe that the ML models can only account for the simple phrases made of content words (nouns, verbs, adjectives) that they have been generally tested on, and that they will not scale up to represent the meanings of sentences, or even sub-sentential constituents with more complex internal structure. One important limitation stems from the fact t"
2014.lilt-9.5,S12-1019,0,0.00951421,"idence that statistical patterns of co-occurrence influence subjects’ intuitions about the meaning of nonce words just as they do in DSMs (McDonald and Ramscar 2001).16 Fourth and last, in neuroscience there is strong support for the view that concepts are represented in the brain as patterns of neural activation over broad areas (Haxby et al. 2001), and vectors are a natural way to encode such patterns (Huth et al. 2012); this suggests intriguing similarities between neural and distributional representations of meaning. Indeed, recent work in brain-computer interaction (Mitchell et al. 2008; Murphy et al. 2012) has shown that corpus-based distributional vectors are good predictors of the patterns of brain activation recorded in subjects thinking of a concept. This suggests, albeit in a very speculative way, that there might be a direct link between the distributional information encoded in DSMs and the way in which concepts are evoked in the brain. Of course, we do not want to suggest that DSMs are models of 14 Bloom and others emphasize the role of interaction and attention in language acquisition. Rather than providing extra cues to meaning, however, these cognitive functions help learners to focu"
2014.lilt-9.5,P10-1142,0,0.0125153,"review of their approach. 260 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli ambiguous than descriptive ones, and this causes a proliferation of apparent inconsistencies. There are di↵erent strategies to cope with this problem. One is to abandon the attempt to distinguish one John from another and focus on what descriptive content remains to names and deictics, for instance the fact that John will appear in contexts suitable for an anglophone male human being. At the other end of the spectrum, one could preprocess the input corpus with a (cross-document) anaphora resolution system (Ng 2010; Poesio et al. 2010) to try to unify those names and deictics that are likely to be coreferential.18 At least for the time being, we will just treat denotational semantics and DSMs as covering complementary aspects of meaning. To exemplify, suppose we hear the sentence A dog is barking. Our distributionalfeature-based representation of its constituents will provide us with a sketch of typical contexts in which it can be uttered truthfully, which can orient our perceptual system to pick up the relevant cues to determine if a dog is indeed barking right now, so that we can evaluate the referent"
2014.lilt-9.5,W03-3017,0,0.0429029,"s, which automatically assign a syntactic structure to sentences, have made great advances in recent years by exploiting probabilistic information about parts of speech (POS tags) and syntactic attachment preferences. This in turn has been made possible by the availability of medium-sized corpora annotated for POS and syntactic information, such as the Penn Treebank (Marcus et al. 1993), that serve as the basis for extracting probabilistic information from. Today’s state-of-the-art parsers can process dozens of unannotated, possibly noisy real-life sentences per second (Clark and Curran 2007; Nivre 2003).4 Learning from pre-annotated data has been less directly applicable to the goal of providing a semantic representation for sentences because there are few learning samples marked for meaning (but see Basile et al. 2012). Moreover, the range, variety and often ‘fuzzy’ nature of semantic phenomena makes the prospect of manual semantic markup of text data a lot less appealing than for syntax. As a consequence, data-driven semantics—which would in principle be a way to address the vastness of lexical meanings—has not advanced as rapidly as datadriven syntax. What sort of data-driven methods coul"
2014.lilt-9.5,J07-2002,0,0.216393,"Missing"
2014.lilt-9.5,D07-1042,0,0.0350195,"Missing"
2014.lilt-9.5,rapp-2004-freely,0,0.0231441,"roni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) and others have shown how the cosines between vectors of word pairs can predict whether the corresponding words will “prime” each other or not (that is, whether a subject will recognize the second word faster when the first one has just been presented). Kotlerman et al. (2010) and others use DSMs to predict lexical entailment (discovering whether the concept denoted by one word implies the one denoted by another; for example, dog entails animal ). Pad´ o et al. (2007) show that (simplifying somewhat) the cosine between a vector representing the typical subject or obj"
2014.lilt-9.5,W11-1511,0,0.0106477,"e most similar, or accurately predict which sentences could contain the word daisy. Still, since the DSM has never seen a daisy and so it has never experienced its color, its shape, etc., we might be reluctant to admit that the model truly “knows” the meaning of the word daisy (references for the grounding debate in relation to DSMs include Andrews et al. 2009; Burgess 2000; Glenberg and Robertson 2000; Louwerse 2011; Riordan and Jones 2011). It is indeed quite telling that DSMs have been applied with some success to the Voynich Manuscript, a 15th century text written in an unreadable script (Reddy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata"
2014.lilt-9.5,N10-1013,0,0.0867913,"ity. For example, since it is a function of how frequently the word represented by the vector was encountered in the corpus (modulo possible statistical transformations of the input values), it is a measure of the reliability of the distributional evidence encoded in the vector. Finally, note that Euclidean distance and cosine are in a bijective functional relation if Euclidean distance is computed on vectors that have been normalized to length 1. Frege in Space / 253 intuitions about the thematic fit of verb arguments and even spotting the alternation classes of verbs (Baroni and Lenci 2010; Baroni et al. 2010; Landauer and Dumais 1997; Lenci 2011; Lund and Burgess 1996; McDonald and Brew 2004; Pad´o and Lapata 2007; Pad´o et al. 2007, and references there). For example, starting with the classic work of Landauer and Dumais (1997), researchers have shown that cosines in distributional space predict which word, among a set of candidates, is the synonym of a target item (e.g., DSMs pick pinnacle as synonym of zenith over the foils completion, outset and decline). DSM performance on this task approximates that of native English speakers with a college education (Rapp 2004). Pad´ o and Lapata (2007) an"
2014.lilt-9.5,D12-1137,0,0.00913497,"sense and reference distinction (e.g., Dummett 1981) that see the sense of a linguistic expression as the manner in which we determine the referent (this would be the job of its distributional representation), whereas the denotational meaning is the referent itself. Bridging denotational and distributional semantics to account for the semantic interpretation of episodic statements is an exciting research program, but it is probably too early to pursue it. However, 18 An exciting new development that might be useful for these purposes is that of distributional methods to geo-locate documents (Roller et al. 2012): The John Smith referred to in a document from Bakersfield is relatively unlikely to be the same John Smith mentioned in an article from Hyderabad. Frege in Space / 261 there are many other aspects of semantics that can be captured independently of the ability to pick up reference from the current state of the world. We reviewed above the many lexical semantic tasks where DSMs representing single words have been very e↵ective despite their lack of direct real-world referencing capabilities (spotting synonyms and other semantic relations, measuring verb-argument plausibility, etc.), and we wil"
2014.lilt-9.5,W03-0902,0,0.014883,"ill-understood topic. DSMs, on the other hand, are extracted from large corpora where proper names, common nouns and other predicates refer to states of the world and events spanning a large chunk of time and space, reflecting di↵erent points of view, etc. So, if they are able to extract any factual information at all, this is very likely to take the form of generic knowledge. Indeed, a typical application of corpus-based semantics is the extraction of commonsense-knowledge “factoids” that are generally useful while not universally true: bananas are yellow, birds fly, etc. (e.g., Eslick 2006; Schubert and Tong 2003). Statistical notions are simFrege in Space / 259 ply not native to the formal semantics setup, but they are at the heart of the DSM approach. We will return to the issue of generic knowledge in Section 5.2 below. Two things should be noted here. First, it is perfectly possible to give a reference-based interpretation of DSMs, the question is what it tells us.17 Suppose we characterize the meaning of a word w in the corpus C in terms of its sentential context, expressed via a vector of binary values: for every word g, vectorw (g) = 1 if g appears along with w in some sentence in C, 0 otherwise"
2014.lilt-9.5,J98-1004,0,0.746674,"Missing"
2014.lilt-9.5,D12-1130,0,0.00874156,"dy and Knight 2011)—a case of ‘semantic analysis’ on a document of unknown content. We believe that the current limitations of DSMs to linguistic contexts are more practical than theoretical. Indeed, by exploiting recent advances in image analysis, a new generation of DSMs integrates text data with visual features automatically extracted from pictures that cooccur with the target words, to attain a more perceptually grounded view of distributional word meaning (Bruni et al. 2011, 2012; Feng and 258 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli Lapata 2010; Leong and Mihalcea 2011; Silberer and Lapata 2012). With research continuing in this direction, DSMs might be the first symbolic semantic models (or even more generally the first fully implemented large-scale computational semantic models) to truly address the symbol grounding problem. 2.6 Meaning and reference The symbol grounding challenge raised by philosophers and cognitive scientists pertains to the perceptual underpinnings of our generic knowledge of concepts (you need to have seen a dog to truly grasp the meaning of the word dog). The dominant tradition in formal semantics stresses instead another type of relation between linguistic si"
2014.lilt-9.5,D12-1110,0,0.0932519,"Missing"
2014.lilt-9.5,W09-2506,0,0.0131424,"re actually very well-suited to capture various kinds of polysemy on a large scale. Note that polysemy is naturally modeled in 254 / Marco Baroni, Raffaella Bernardi and Roberto Zamparelli terms of the contexts in which a word appears: In a sentence containing words such as farm, free-range and outdoors, the word chicken is more likely to mean the animal than its meat (albeit an animal with a clear culinary destiny). Consequently, a large subset of work on polysemy in DSMs (e.g., Dinu and Lapata 2010; Erk and Pad´o 2008, 2010; Kintsch 2001; Mitchell and Lapata 2008; Reisinger and Mooney 2010; Thater et al. 2009) has focused on the goal of modeling word meaning in context. There is a clear connection between distributional models of word meaning in context and distributional models of compositionality, which is the main topic of this article. For example, Mitchell and Lapata (2008) discriminate between the senses of running in water runs vs. horse runs by composing vectors representing the two phrases, whereas Erk and Pad´ o (2008) and others approach the same task in terms of how the runs vector changes due to contextual e↵ects triggered by the presence of water vs. horse. Mitchell and Lapata constru"
2014.lilt-9.5,E12-1003,0,0.0091094,"of Garrette et al. 2013 and Turney 2012). Here, we limit our discussion to work that share our goal and that uses approaches either very close or radically di↵erent from ours. Before going into the details of this work, it is worth mentioning that there is a rich tradition of corpus-based statistical semantics methods producing compositional representations that are di↵erent from the classic logic-based ones, but are not distributional in our sense. This line of research includes the corpus-based induction of semantic parsers suitable for question answering (Liang et al. 2011), role labeling (Titov and Klementiev 2012) and modeling semantic and syntactic acquisition (Kwiatkowski et al. 2012). The output of such semantic parsers could be tried as an alternative to the purely syntactic CG input used in our current work. Recently, there also has been much interest in higher-order tensors for distributional semantics (e.g., Baroni and Lenci 2010; Giesbrecht 2010; Turney 2007; Van de Cruys 2010; Widdows 2008). However, even when this line of work tackles the issue of compositionality, it looks at tensors as a way to represent larger structures that result from composition, rather than taking the view we propose"
2014.lilt-9.5,W11-1301,1,0.455046,"the composed vector with any other vector will stay the same. 22 The other successful model of Mitchell and Lapata (2010), namely the dilation model, can be seen as a special way to estimate the weights of the weighted additive model, and we consider it as a special case of the latter here. Frege in Space / 265 Other studies have confirmed that the ML methods, in particular the multiplicative model, are very competitive in various composition tasks that involve simple phrases and do not test for word order or di↵erent syntactic structures (Erk and Pad´o 2008; Grefenstette and Sadrzadeh 2011a; Vecchi et al. 2011; Boleda et al. 2012b). Interestingly and surprisingly, Blacoe and Lapata (2012) recently found that the ML models reach performance close to the one of knowledge-intensive state-of-the-art systems on a full-sentence paraphrasing task. Given the weaknesses of the models we will present below, we can only conjecture that the sentences in this data set fail to test for some crucial syntactic aspects of language (a suspicion that is strengthened by the fact that Blacoe and Lapata obtain excellent results with versions of the additive and multiplicative models that ignore, if we understand correct"
2014.lilt-9.5,C10-1142,0,0.0298247,"cke et al. formalism with ours by adopting the regression-based learning method we explained in this article (the empirical results of Grefenstette et al. 2013, are reviewed in Section 4.2 above). We share the idea of learning composition functions by regression on corpus-extracted examples of their inputs and outputs with Guevara (2010), who, however, treats all linguistic expressions as vectors without distinguishing them into atomic and functional types. The importance of exploiting input and output training data for building compositional distributional semantic models is also stressed by Zanzotto et al. (2010), who present a model similar to the one of Guevara, but cleverly exploit dictionary definitions to extract both positive and negative training examples. Another model that is closely related to ours is that of Socher et al. (2012), who also implement function application in terms of operations on matrices and vectors. However, di↵erently from us, they treat each word equally, as both a vector and a matrix. Distributional composition is doubled – each word matrix is composed with the lexical vector of the other word in a phrase – and the result is still a pair of a vector and a matrix. Since S"
2014.lilt-9.5,J12-1002,0,\N,Missing
2014.lilt-9.5,C98-1013,0,\N,Missing
2020.alvr-1.4,P19-1004,0,0.0183536,"cts as done in previous work (Lee et al., 2018; Ray et al., 2019; Xie et al., 2020; Murahari et al., 2019). In this paper we focus on whether a question is effective and referential considering the dialogue history and the visual context. One of the motivations for referential visual dialogue is to provide robots with the ability to identify objects through dialogue with a human as the robot moves. The task we address in this paper is a simplification. In our setup, the view of the robot is static, it is a picture. For our work we use the GuessWhat?! dataset (de Vries et al., 2017). Recently, Sankar et al. (2019) showed that several end-to-end dialogue systems do not take dialogue history into account. In this paper we are particularly interested in the GuessWhat?! models 3 Dataset and Evaluation Metrics In this section we briefly introduce the dataset and we define the evaluation metrics that we use. 3.1 GuessWhat?! GuessWhat?! (de Vries et al., 2017) is a cooperative game where two players talk in order to identify an object in an image. The player known as the Questioner has to guess the referent by asking yes/no questions. The other player, the Oracle, knows the referent object and answers the que"
2020.alvr-1.4,P19-1181,0,0.0609948,"Missing"
2020.alvr-1.4,C18-1104,1,0.660335,"Missing"
2020.alvr-1.4,J12-1006,0,0.105129,"Missing"
2020.alvr-1.4,N19-1265,1,0.62463,"Missing"
2020.alvr-1.4,P19-1646,0,0.0773276,". 1 Introduction GuessWhat?! (de Vries et al., 2017) is a cooperative two-player referential visual dialogue game. One player (the Oracle) is assigned a referent object in an image, the other player (the Questioner) has to guess the referent by asking yes/no questions. Referential visual dialogue has a clear task success metric: whether the Questioner is able to correctly identify the referent at the end of the dialogue. The need of going beyond this metric to evaluate the quality of the dialogues has already been observed. So far attention has been put on the linguistic skills of the models (Shukla et al., 2019; Shekhar et al., 2019) and their dialogue strategies (Shekhar et al., 2018; Pang and Wang, 2020). But still the models are evaluated without considering how much each question contributes to the goal. We propose two new metrics for evaluating questions. First, a question is effective if it rules out 2 Previous work Despite recent progress in the area of vision and language, recent work (Jain et al., 2019) in the navigation task (VLN) argues that current research leaves unclear how much of a role language plays in this task. They point out that dominant evaluation metrics have focused on goal"
2020.alvr-1.4,D19-1152,0,0.085568,"assessing performance, or even building trust. Following this idea of not only focusing on goal completion but on evaluating how much each step contributes to the goal, in this paper we propose two new metrics for referential dialogue. We agree with (Thomason et al., 2019) that incremental evaluation metrics such as ours should look further back into the dialogue history. We believe that language and vision systems should also be evaluated on aspects such as grammatically, truthfulness, diversity and other aspects as done in previous work (Lee et al., 2018; Ray et al., 2019; Xie et al., 2020; Murahari et al., 2019). In this paper we focus on whether a question is effective and referential considering the dialogue history and the visual context. One of the motivations for referential visual dialogue is to provide robots with the ability to identify objects through dialogue with a human as the robot moves. The task we address in this paper is a simplification. In our setup, the view of the robot is static, it is a picture. For our work we use the GuessWhat?! dataset (de Vries et al., 2017). Recently, Sankar et al. (2019) showed that several end-to-end dialogue systems do not take dialogue history into acc"
2020.alvr-1.4,D19-1596,0,0.0255515,"viour, whether for fault detection, assessing performance, or even building trust. Following this idea of not only focusing on goal completion but on evaluating how much each step contributes to the goal, in this paper we propose two new metrics for referential dialogue. We agree with (Thomason et al., 2019) that incremental evaluation metrics such as ours should look further back into the dialogue history. We believe that language and vision systems should also be evaluated on aspects such as grammatically, truthfulness, diversity and other aspects as done in previous work (Lee et al., 2018; Ray et al., 2019; Xie et al., 2020; Murahari et al., 2019). In this paper we focus on whether a question is effective and referential considering the dialogue history and the visual context. One of the motivations for referential visual dialogue is to provide robots with the ability to identify objects through dialogue with a human as the robot moves. The task we address in this paper is a simplification. In our setup, the view of the robot is static, it is a picture. For our work we use the GuessWhat?! dataset (de Vries et al., 2017). Recently, Sankar et al. (2019) showed that several end-to-end dialogue sys"
2020.findings-emnlp.248,C18-1197,0,0.0229723,"ady provided by the environment; similarly, listeners leverage information from various modalities, such as vision, to interpret the linguistic message. Integrating information from multiple modalities is indeed crucial for attention and perception (Partan and Marler, 1999) since combined information from concurrent modalities can give rise to different messages (McGurk and MacDonald, 1976). The argument that language and vision convey different, possibly complementary aspects of meaning has been largely made to motivate the need for multimodal semantic representations of words (Baroni, 2016; Beinborn et al., 2018). However, computational approaches to language and vision typically do not fully explore this complementarity. To illustrate, given an image (e.g., the one depicted in Figure 1), popular tasks involve describing it in natural language, e.g., “A tennis player about to hit the ball” (Image Captioning; see Bernardi et al., 2016); answering questions that are grounded in it, e.g., Q: “What sport is he playing?”, A: “Tennis” (Visual Question Answering; see Antol et al., 2015); having a dialogue on its entities, e.g., Q: “Is the person holding a racket?”, A: “Yes.” (visuallygrounded dialogue; see D"
2020.findings-emnlp.248,W18-1503,0,0.0234858,"ntion has been paid to understand what makes a question “difficult” for a model (Bhattacharya et al., 2019; Terao et al., 2020). Despite impressive progress, current approaches to VQA do not tackle one crucial limitation of the task: the answer to a question is given by the alignment of language and vision rather than their complementary integration. Moving from objects to actions, several tasks have been proposed to mimic more realistic settings where a higher degree of integration between modalities is required. One is visual storytelling (Huang et al., 2016; Gonzalez-Rico and Pineda, 2018; Lukin et al., 2018), where models have to understand the action depicted in each photo and their relations to generate a story. Similar abilities are required in the task of generating non-grounded, human-like questions about an image (Mostafazadeh et al., 2016; Jain et al., 2017), and in that of asking discriminative questions over pairs of similar scenes (Li et al., 2017). Related tasks are also those of predicting motivations of visually-grounded actions (Vondrick et al., 2016) or generating explanations for a given answer (Park et al., 2018; Hendricks et al., 2018). An even higher level of understanding of v"
2020.findings-emnlp.248,D18-1167,0,0.0255869,"generating non-grounded, human-like questions about an image (Mostafazadeh et al., 2016; Jain et al., 2017), and in that of asking discriminative questions over pairs of similar scenes (Li et al., 2017). Related tasks are also those of predicting motivations of visually-grounded actions (Vondrick et al., 2016) or generating explanations for a given answer (Park et al., 2018; Hendricks et al., 2018). An even higher level of understanding of vision and language is required in the tasks of filling the blank with the correct answer (Yu et al., 2015); answering questions from videos and subtitles (Lei et al., 2018); having a dialogue on objects (De Vries et al., 2017; Das et al., 2017) or events (Mostafazadeh et al., 2017); answering and justifying commonsense questions (Zellers et al., 2019). However, all these tasks require making commonsense inferences over the two modalities rather than integrating their complementary information to answer a grounded question. More akin to ours are the approaches by Iyyer et al. (2017), which aims to predict the subsequent scene and dialogue in a comic strip, and Kruk et al. (2019), where the goal is to compute the communicative intent of a social media post. Though"
2020.findings-emnlp.248,P16-1170,0,0.0280494,"e answer to a question is given by the alignment of language and vision rather than their complementary integration. Moving from objects to actions, several tasks have been proposed to mimic more realistic settings where a higher degree of integration between modalities is required. One is visual storytelling (Huang et al., 2016; Gonzalez-Rico and Pineda, 2018; Lukin et al., 2018), where models have to understand the action depicted in each photo and their relations to generate a story. Similar abilities are required in the task of generating non-grounded, human-like questions about an image (Mostafazadeh et al., 2016; Jain et al., 2017), and in that of asking discriminative questions over pairs of similar scenes (Li et al., 2017). Related tasks are also those of predicting motivations of visually-grounded actions (Vondrick et al., 2016) or generating explanations for a given answer (Park et al., 2018; Hendricks et al., 2018). An even higher level of understanding of vision and language is required in the tasks of filling the blank with the correct answer (Yu et al., 2015); answering questions from videos and subtitles (Lei et al., 2018); having a dialogue on objects (De Vries et al., 2017; Das et al., 201"
2020.findings-emnlp.248,D18-1009,0,0.0491848,"Missing"
2020.splu-1.4,2020.acl-main.728,0,0.20493,"cond one as absolute. A further distinction is made between referring expressions that are singular (e.g. “the rabbit in the hat”) and those that are plural (e.g. “the three rabbits on the table”) and refer to a group (see e.g., Lønning (1997); Gatt and van Deemter (2007); Krahmer and van Deemter (2012)). In this paper, we classify GuessWhat?! spatial questions using absolute, relational and group distinctions and examine how LXMERT performs for each type of spatial question. We also conduct an error analysis and an attention analysis taking these categories into consideration. Recent work by Agarwal et al. (2020) shows that in current visual dialogue datasets the dialogue history rarely matters. The authors ask crowdsourcers whether they can confidently answer a question by looking at the image and the question, without seeing the dialogue history. In our qualitative analysis we check whether history plays a role for the spatial questions of the GuessWhat?! game that LXMERT fails to answer. Answering visual questions is a task that has received increasing attention during the last years. Interesting exploratory analysis has been carried out to understand Visual Question Answering (VQA) systems which h"
2020.splu-1.4,W18-1406,0,0.0249429,"fication helps to gain a better understanding of the skills required to answer different spatial questions. R ELATIONAL is it the bus on the left? No is the boat next to a car? No Introduction G ROUP Visual Dialogues are a useful testbed to study how models ground natural language and in particular how they ground spatial language, which is the focus of our analysis. Visual Dialogues have been the aim of early work on natural language understanding (NLU) (Winograd, 1972) and are now studied by a very active community at the interplay between computer vision and computational linguistics (e.g. Baldridge et al. (2018); Ilinykh et al. (2019); Haber et al. (2019)). Recently, important progress has been made on visual dialogue systems thanks to the release of datasets like VisDial (Das et al., 2017) and GuessWhat?! (de Vries et al., 2017). The former contains chit-chat conversations about an image whereas the latter is a visual game, hence its dialogues are goal-oriented. In both cases, one agent asks questions and the is one of the two in the back? Yes Figure 1: A vast amount of questions asked by humans in the GuessWhat?! game (de Vries et al., 2017) are spatial. We classify them as absolute, relational, an"
2020.splu-1.4,Q18-1010,0,0.0161883,"the Oracle, no work has been carried out to evaluate and improve it. We aim to fill this gap. Shekhar et al. (2019) show that GuessWhat?! human players ask quite a lot spatial questions. It has been observed that capturing the spatial relation about objects is challenging for neural network models. Kelleher and Dobnik (2017) argue that Convolutional Neural Network (CNN) do not ground spatial information properly: since they discard location information through the pooling mechanism, their embeddings can only capture rough relative positions of objects within a scene. In line with this claim, Collell and Moens (2018) show that linguistic features are more spatially informative than CNN visual features. New multimodal models, like LXMERT, start from positional aware embeddings. We therefore study how well they handle the spatial questions asked by Guess3 Models In this section we present the models that we compare. We also explain how we adapted LXMERT to the Oracle task. The models are trained on successful games. LSTM is the baseline model proposed in de Vries et al. (2017). It does not have access to the raw image features. It receives as input embeddings of the target object’s category, 31 its spatial"
2020.splu-1.4,J12-1006,0,0.0228951,"Missing"
2020.splu-1.4,P19-1648,0,0.0190102,"y the question refers to and answer accordingly. Moreover, differently from VQA, the GuessWhat?! dataset has been collected in a more naturalistic environment, by letting humans play the games. We adapt LXMERT (Tan and Bansal, 2019), a multimodal universal encoder State-of-theArt in VQA, to accomplish the Oracle’s challenge. After the introduction of the supervised baseline models (de Vries et al., 2017), several models have been proposed for the Questioner, which are mostly based on reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b; Zhao and Tresp, 2018; Zhang et al., 2018a; Gan et al., 2019; Yang et al., 2019; Pang and Wang, 2020). For these models, the role of the Oracle is even more salient than for models based on supervised or cooperative learning (Shekhar et al., 2019) since they are reinforced to ask those questions the Oracle is good at answering. Despite this important role of the Oracle, no work has been carried out to evaluate and improve it. We aim to fill this gap. Shekhar et al. (2019) show that GuessWhat?! human players ask quite a lot spatial questions. It has been observed that capturing the spatial relation about objects is challenging for neural network models."
2020.splu-1.4,D07-1011,0,0.111581,"Missing"
2020.splu-1.4,P14-5010,0,0.00453362,"Missing"
2020.splu-1.4,W17-6808,0,0.0257213,"ed within the referring expression generation community. In this area, earlier work (Paraboni et al., 2007) has suggested that, in ordered domains (e.g., a document divided into sections and subsections), referring expressions that include spatial information, even when redundant, lead to a significant reduction in the amount of search that is needed to identify the referent. It has been argued that spatial information reduces the cognitive load (measured by eye tracking) necessary for resolving a referring expression (Paraboni et al., 2017). This research area (Krahmer and van Deemter, 2012; Ghanimifard and Dobnik, 2017) distinguishes between spatial referring expressions that involve another object in the description (e.g. “the rabbit in the hat”) from those that do not (e.g. “the rabbit on the left”). The first group of expressions is known as relational, while we shall refer to the second one as absolute. A further distinction is made between referring expressions that are singular (e.g. “the rabbit in the hat”) and those that are plural (e.g. “the three rabbits on the table”) and refer to a group (see e.g., Lønning (1997); Gatt and van Deemter (2007); Krahmer and van Deemter (2012)). In this paper, we cla"
2020.splu-1.4,P19-1184,0,0.0280805,"the skills required to answer different spatial questions. R ELATIONAL is it the bus on the left? No is the boat next to a car? No Introduction G ROUP Visual Dialogues are a useful testbed to study how models ground natural language and in particular how they ground spatial language, which is the focus of our analysis. Visual Dialogues have been the aim of early work on natural language understanding (NLU) (Winograd, 1972) and are now studied by a very active community at the interplay between computer vision and computational linguistics (e.g. Baldridge et al. (2018); Ilinykh et al. (2019); Haber et al. (2019)). Recently, important progress has been made on visual dialogue systems thanks to the release of datasets like VisDial (Das et al., 2017) and GuessWhat?! (de Vries et al., 2017). The former contains chit-chat conversations about an image whereas the latter is a visual game, hence its dialogues are goal-oriented. In both cases, one agent asks questions and the is one of the two in the back? Yes Figure 1: A vast amount of questions asked by humans in the GuessWhat?! game (de Vries et al., 2017) are spatial. We classify them as absolute, relational, and group based on how they many objects are i"
2020.splu-1.4,2020.alvr-1.4,1,0.780852,"Missing"
2020.splu-1.4,D19-1152,0,0.0273578,"ved and how they are related. The red box marks the object(s) involved in the question, while the green box marks the target of the game. Relational and group questions need more than one object, whereas absolute do not. 29 Proceedings of the Third International Workshop on Spatial Language Understanding (SpLU 2020), pages 29–38 c November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 other, which we call the Oracle, answers. For VisDial most of the work focused on the answerer, but in-depth evaluation has been carried out on the questioner too (eg., Murahari et al. (2019); Testoni et al. (2019)). For GuessWhat?!, instead, work has been done mostly, if not only, on the questioner. Current models trained with reinforcement learning achieve high task success; they adapt to the oracle limitations and end-up asking questions that are linguistically simpler than those asked by humans (Shekhar et al., 2019; Pang and Wang, 2020). challenging for neural networks since quite often they require models to put attention on more regions simultaneously and spot the relation holding among them (e.g., the car and the boat in Figure 1, middle). LXMERT is a transformer-based neu"
2020.splu-1.4,D19-1514,0,0.330462,"eceives the gold standard entity of the target (that is, the category label, e.g. “giraffe” or “boat”) as input. Furthermore, it answers questions without seeing either the image or the visual features of the target, but instead it simply relies on the category label of the target and its coordinates. Important progress on multimodal encoders has been obtained since the GuessWhat?! release; hence, we study the effect of using models that ground the question into the image and do not have access to the gold standard category label of the target. We adapt a multimodal universal encoder, LXMERT (Tan and Bansal, 2019), to play the role of the Oracle and compare it with the baseline model. The paper proceeds as follows. Section 2 reviews previous work on visual question answering and on spatial referring expressions. Section 3 presents the models providing information on how we adapt LXMERT for the Oracle task. Section 4 describes the dataset and our classification of spatial questions. In Section 5 we compare the accuracy of the models reporting a fine-grained evaluation by question type and zoom into the subset of spatial questions. We further analysed this subset through a manual inspection of LXMERT att"
2020.splu-1.4,J07-2004,0,0.0561576,"Missing"
2020.splu-1.4,2020.acl-demos.14,0,0.0133112,"on the left?”) instead contain a location word either in the x axis (e.g. right, middle, left), or the y (top, bottom), or the z (e.g. front, back) axis. We also consider absolute those questions that include a spatial adjective in its superlative form (e.g. “the leftmost one?”). Finally, we consider group questions those containing a number which may indicate order (e.g. “right to left, is it the first one?”) or groups (e.g. “in the back among four women?”). We have automatically annotated spatial questions by identifying nouns, prepositions and number using the Part of Speech tagger Stanza (Qi et al., 2020). When a question is not assigned to any of the three groups, we include it in the “Other” category.3 We tried identifying objects using the entity recognizers included in Stanford core NLP (Manning et al., 2014) and Stanza (Qi et al., 2020) but the coverage was not good. In the next section, we will first compare models using the multi-label classification reported in Table 1, then we will zoom into the spatial questions which together with the entity questions constitute the large majority of questions asked by humans. In order to understand strength and limits of multimodal models in answer"
2020.splu-1.4,H89-1033,0,0.24961,"Missing"
2020.splu-1.4,C18-1104,1,0.82823,"ses might create more context dependenTable 4: Accuracy of the sub-type of spatial questions (successful games, questions assigned only one type) more challenging: the model has to locate the regions corresponding to the two related words and understand the relation holding among them. The group questions may require “counting” skills that go beyond the scope of this paper. 6 Error Analysis Qualitative Analysis As a first step towards a deeper understanding of LXMERT performance, we use a linear logistic regression model for the task of predicting whether a question was answered correctly. In Shekhar et al. (2018) it has been shown that unsuccessful games contain more objects in the image than successful ones, and that the target size area is smaller. We use these two features as predictor variables together with the length of the question and the turn in which it was asked in the full dialogue. We observe that the number of objects in the image and the question turn play a significant role in predicting the model behaviour. This might be due to the fact that models do not receive the dialogue history as input. Below we run an error analysis based on the three spatial sub-type questions described above"
2020.splu-1.4,W18-5015,0,0.0222972,"t has to compare the target’s properties with those of the entity the question refers to and answer accordingly. Moreover, differently from VQA, the GuessWhat?! dataset has been collected in a more naturalistic environment, by letting humans play the games. We adapt LXMERT (Tan and Bansal, 2019), a multimodal universal encoder State-of-theArt in VQA, to accomplish the Oracle’s challenge. After the introduction of the supervised baseline models (de Vries et al., 2017), several models have been proposed for the Questioner, which are mostly based on reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b; Zhao and Tresp, 2018; Zhang et al., 2018a; Gan et al., 2019; Yang et al., 2019; Pang and Wang, 2020). For these models, the role of the Oracle is even more salient than for models based on supervised or cooperative learning (Shekhar et al., 2019) since they are reinforced to ask those questions the Oracle is good at answering. Despite this important role of the Oracle, no work has been carried out to evaluate and improve it. We aim to fill this gap. Shekhar et al. (2019) show that GuessWhat?! human players ask quite a lot spatial questions. It has been observed that capturing the spatial re"
2020.splu-1.4,P17-1024,1,0.831159,"rarely matters. The authors ask crowdsourcers whether they can confidently answer a question by looking at the image and the question, without seeing the dialogue history. In our qualitative analysis we check whether history plays a role for the spatial questions of the GuessWhat?! game that LXMERT fails to answer. Answering visual questions is a task that has received increasing attention during the last years. Interesting exploratory analysis has been carried out to understand Visual Question Answering (VQA) systems which highlight their weaknesses and strengths, e.g. (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017; Kafle and Kanan, 2017). VQA datasets contain both wh- and Y/N-questions. But the kind of Y/N visual questions the Oracle needs to answer are different than those of the VQA datasets: it has to check whether the target has or does not have the questioned property. Hence, it has to compare the target’s properties with those of the entity the question refers to and answer accordingly. Moreover, differently from VQA, the GuessWhat?! dataset has been collected in a more naturalistic environment, by letting humans play the games. We adapt LXMERT (Tan and Bansal, 2019), a multimo"
2020.splu-1.4,N19-1265,1,0.699821,"ember 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 other, which we call the Oracle, answers. For VisDial most of the work focused on the answerer, but in-depth evaluation has been carried out on the questioner too (eg., Murahari et al. (2019); Testoni et al. (2019)). For GuessWhat?!, instead, work has been done mostly, if not only, on the questioner. Current models trained with reinforcement learning achieve high task success; they adapt to the oracle limitations and end-up asking questions that are linguistically simpler than those asked by humans (Shekhar et al., 2019; Pang and Wang, 2020). challenging for neural networks since quite often they require models to put attention on more regions simultaneously and spot the relation holding among them (e.g., the car and the boat in Figure 1, middle). LXMERT is a transformer-based neural network and as such it heavily exploits attentionbased mechanisms. In this paper, we run a qualitative analysis of the attention LXMERT exhibits for the different types of location questions and run an in-depth error analysis of its results. To sum up, we make the following contributions: It is interesting to understand where cu"
2020.splu-1.4,P17-2034,0,0.0293198,"thors ask crowdsourcers whether they can confidently answer a question by looking at the image and the question, without seeing the dialogue history. In our qualitative analysis we check whether history plays a role for the spatial questions of the GuessWhat?! game that LXMERT fails to answer. Answering visual questions is a task that has received increasing attention during the last years. Interesting exploratory analysis has been carried out to understand Visual Question Answering (VQA) systems which highlight their weaknesses and strengths, e.g. (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017; Kafle and Kanan, 2017). VQA datasets contain both wh- and Y/N-questions. But the kind of Y/N visual questions the Oracle needs to answer are different than those of the VQA datasets: it has to check whether the target has or does not have the questioned property. Hence, it has to compare the target’s properties with those of the entity the question refers to and answer accordingly. Moreover, differently from VQA, the GuessWhat?! dataset has been collected in a more naturalistic environment, by letting humans play the games. We adapt LXMERT (Tan and Bansal, 2019), a multimodal universal encod"
2021.acl-srw.11,N19-1423,0,0.0138336,"uesser module. We fine-tune the pre-trained LXMERT on GuessWhat?!. Again, we use this model only to generate dialogues. VLP. Finally, we develop a Questioner model based on VLP (Zhou et al., 2020), a powerful multimodal Encoder-Decoder Transformer architecture pre-trained on image captioning. VLP is a single BL GDSE LXMERT-GDSE VLP HUMAN CHAIR-s 29.53 30.31 14.98 10.78 7.45 CHAIR-i 27.32 16.57 8.83 6.60 4.11 Table 1: CHAIR results on human and machinegenerated dialogues on the GuessWhat?! test set. stream unified encoder-decoder architecture: its Transformer backbone is the same as BERT-base (Devlin et al., 2019). VLP represents each input image as 100 object regions extracted from a variant of Faster RCNN (Ren et al., 2016) pre-trained on Visual Genome (Krishna et al., 2017; Anderson et al., 2018), together with the class likelihood on the 1600 object categories defined in Anderson et al. (2018) as region object labels. During pre-training, the model uses a masked language modelling objective. During inference, in order to generate a sequence token-by-token, VLP masks sequentially each token by appending a special token [SEP] at the end of the sequence. VLP is trained to predict a [STOP] token at the"
2021.acl-srw.11,P19-1184,0,0.0458705,"Missing"
2021.acl-srw.11,W17-3204,0,0.162895,"the output generated by deep neural network architectures. Although it is not easy to evaluate the output of natural language generation systems, some features clearly deteriorate their value, making these systems hardly employable in real-world scenarios. Crucially, state-of-the-art models are shown to generate words that are not consistent with the source inputs. This issue is generally referred to as hallucination. This phenomenon applies to different NLP tasks and neural architectures. It has been explored in summarization (Kryscinski et al., 2020; Nan et al., 2021), machine translation (Koehn and Knowles, 2017; Nguyen and Chiang, 2018), and image captioning (Rohrbach et al., 2018). Hallucinating entities is particularly harmful in multimodal systems. MacLeod et al. (2017) study how blind people experience automatically generated captions describing images. The authors found that many participants in this study value more the correctness of the caption compared to a fine-grained description of the image, thus providing evidence that hallucination represents a major issue. The problem of generating hallucinated entities is thus a relevant challenge for the community, but it is an understudied problem"
2021.acl-srw.11,2020.emnlp-main.750,0,0.0656202,"Recent years have witnessed important progress in the quality of the output generated by deep neural network architectures. Although it is not easy to evaluate the output of natural language generation systems, some features clearly deteriorate their value, making these systems hardly employable in real-world scenarios. Crucially, state-of-the-art models are shown to generate words that are not consistent with the source inputs. This issue is generally referred to as hallucination. This phenomenon applies to different NLP tasks and neural architectures. It has been explored in summarization (Kryscinski et al., 2020; Nan et al., 2021), machine translation (Koehn and Knowles, 2017; Nguyen and Chiang, 2018), and image captioning (Rohrbach et al., 2018). Hallucinating entities is particularly harmful in multimodal systems. MacLeod et al. (2017) study how blind people experience automatically generated captions describing images. The authors found that many participants in this study value more the correctness of the caption compared to a fine-grained description of the image, thus providing evidence that hallucination represents a major issue. The problem of generating hallucinated entities is thus a releva"
2021.acl-srw.11,L18-1602,0,0.0184189,"rce input (Koehn and Knowles, 2017; Nguyen and Chiang, 2018). A recent work (M¨uller et al., 2020) found that neural machine translation systems evaluated on out-of-domain test sets generate translations that are fluent but unrelated to the source sentence. These works focus on words belonging to different parts of speech, like proper nouns, adjectives, and verbs, while we only focus on entity hallucination and leave for future work the analysis of attribute hallucination. Hallucination in Vision & Language. The generation of hallucinations affects also Multimodal Machine Translation systems. Lala and Specia (2018) highlight the issues that may arise while translating ambiguous or polysemic words given a visual context. Rohrbach et al. (2018) investigate the problem of object hallucination in image captioning, the closest task to our work. The authors propose a new metric (CHAIR) to quantify the extent to which machine-generated captions contain hallucinated entities. The authors found overreliance on language priors as a plausible cause of hallucinated tokens in the generated captions. Moreover, they found that models with a more reliable visual representation hallucinate less, suggesting that a robust"
2021.acl-srw.11,I17-1047,0,0.0191234,"visual input is important for reducing hallucination. We use the CHAIR metric to evaluate different models, and look at the role of different visual representations. A recent work (Xiao and Wang, 2021) investigates the relationship between hallucinations and predictive uncertainty in image captioning and datato-text generation. The authors found that higher predictive uncertainty leads to a higher chance of hallucinating entities. We leave this kind of analysis for future work. 102 Visual Dialogues Evaluation. Among the visual dialogue datasets and tasks available (e.g., de Vries et al. 2017; Mostafazadeh et al. 2017; Das et al. 2017; Haber et al. 2019), we chose a task-oriented referential game, GuessWhat?! (de Vries et al., 2017). Task-oriented conversational agents generate dialogues to reach a goal, thus the presence of hallucinations considerably hurt the performance of such systems. We chose GuessWhat?! because of the simplicity of its dialogue structure (polar question-answer pairs). Recent work in the literature highlights the inability of the accuracy in the guessing task to serve as a good proxy of the quality of the underlying dialogues, with a particular focus on surface-level features such as"
2021.acl-srw.11,2020.amta-research.14,0,0.0191485,"Missing"
2021.acl-srw.11,D19-1152,0,0.0238697,"sk-oriented referential game, GuessWhat?! (de Vries et al., 2017). Task-oriented conversational agents generate dialogues to reach a goal, thus the presence of hallucinations considerably hurt the performance of such systems. We chose GuessWhat?! because of the simplicity of its dialogue structure (polar question-answer pairs). Recent work in the literature highlights the inability of the accuracy in the guessing task to serve as a good proxy of the quality of the underlying dialogues, with a particular focus on surface-level features such as the presence of repetitions (Shekhar et al., 2019; Murahari et al., 2019; Testoni et al., 2019). We extend this claim by looking at hallucination, an under-studied but crucial issue in Visual Dialogues. 3 Task and Metrics Task The GuessWhat?! game (de Vries et al., 2017) is a cooperative two-player game in English based on a referential communication task where two players collaborate to identify a referent object in an image. This setting has been extensively used in human-human collaborative dialogue (e.g., Clark 1996; Yule 2013). GuessWhat?! is an asymmetric game involving two human participants who see a real-world image. One of the participants (the Oracle) i"
2021.acl-srw.11,N18-1031,0,0.118147,"deep neural network architectures. Although it is not easy to evaluate the output of natural language generation systems, some features clearly deteriorate their value, making these systems hardly employable in real-world scenarios. Crucially, state-of-the-art models are shown to generate words that are not consistent with the source inputs. This issue is generally referred to as hallucination. This phenomenon applies to different NLP tasks and neural architectures. It has been explored in summarization (Kryscinski et al., 2020; Nan et al., 2021), machine translation (Koehn and Knowles, 2017; Nguyen and Chiang, 2018), and image captioning (Rohrbach et al., 2018). Hallucinating entities is particularly harmful in multimodal systems. MacLeod et al. (2017) study how blind people experience automatically generated captions describing images. The authors found that many participants in this study value more the correctness of the caption compared to a fine-grained description of the image, thus providing evidence that hallucination represents a major issue. The problem of generating hallucinated entities is thus a relevant challenge for the community, but it is an understudied problem in multimodal conversatio"
2021.acl-srw.11,D18-1437,0,0.144952,"s not easy to evaluate the output of natural language generation systems, some features clearly deteriorate their value, making these systems hardly employable in real-world scenarios. Crucially, state-of-the-art models are shown to generate words that are not consistent with the source inputs. This issue is generally referred to as hallucination. This phenomenon applies to different NLP tasks and neural architectures. It has been explored in summarization (Kryscinski et al., 2020; Nan et al., 2021), machine translation (Koehn and Knowles, 2017; Nguyen and Chiang, 2018), and image captioning (Rohrbach et al., 2018). Hallucinating entities is particularly harmful in multimodal systems. MacLeod et al. (2017) study how blind people experience automatically generated captions describing images. The authors found that many participants in this study value more the correctness of the caption compared to a fine-grained description of the image, thus providing evidence that hallucination represents a major issue. The problem of generating hallucinated entities is thus a relevant challenge for the community, but it is an understudied problem in multimodal conversational agents. Apart from sharing similarities wi"
2021.acl-srw.11,N19-1265,1,0.881602,"Missing"
2021.acl-srw.11,2021.eacl-main.183,0,0.0376892,"Missing"
2021.acl-srw.11,2020.coling-main.95,0,0.0551953,"Missing"
2021.acl-srw.11,D19-1514,0,0.021705,"the target (Guesser), and asking questions (Questioner). 4.1 Oracle We use the baseline Oracle model proposed in de Vries et al. (2017). The model receives as input the embedding of the target object category, its spatial coordinates, and the question to be answered encoded by a dedicated Long-Short-Term Memory (LSTM) network. These three embeddings are concatenated and fed to a Multi-Layer Perceptron (MLP) that gives an answer (Yes, No, N/A). 4.2 Guesser We use the state-of-the-art multimodal Guesser model proposed in Greco et al. (2021a) (Figure 2 bottom).1 This Guesser is based on LXMERT (Tan and Bansal, 2019), a powerful multimodal Transformer model that is fine-tuned on the GuessWhat?! guesser task using successful human dialogues. LXMERT represents the visual input by the set of position-aware object embeddings for the 36 most salient regions detected by a Faster R-CNN network, and the text by position-aware randomlyinitialized word embeddings. LXMERT has selfattention and cross-attention layers to merge and enhance the information coming from the two modalities to create a joint representation. LXMERT uses a special tokens CLS and the embedding corresponding to this token is considered a repres"
2021.acl-srw.11,2021.eacl-main.178,1,0.786074,"machinegenerated dialogues, we found that the baseline model (which is shown to generate many hallucinations – Table 1) outperforms the others. We believe that this result is due to the over-reliance of the baseline model on location questions, as highlighted in Shekhar et al. (2019). These questions, though are helpful for the model to identify the target object, make its dialogues sound unnatural when asked too often. We think this confirms the failure of the overall accuracy to serve as a proxy for the quality of the generated dialogues, as recently highlighted in Shekhar et al. (2019) and Testoni and Bernardi (2021). In order to understand this discrepancy between accuracy and hallucination, we compared dialogues that contain at least one hallucinated entity with dialogues not affected by this issue. We found that the presence of hallucinations clearly deteriorates the accuracy in the game: as shown in Table 2, dialogues containing at least one hallucinated token lead to lower accuracy in guessing the target object compared to games that do not contain 3 Simultaneously, Suglia et al. (2021) have adapted VLP to the GuessWhat?! game; they use a different training regime, and they focus on VQA as a downstre"
2021.acl-srw.11,2021.eacl-main.236,0,0.0324794,"the closest task to our work. The authors propose a new metric (CHAIR) to quantify the extent to which machine-generated captions contain hallucinated entities. The authors found overreliance on language priors as a plausible cause of hallucinated tokens in the generated captions. Moreover, they found that models with a more reliable visual representation hallucinate less, suggesting that a robust processing of the visual input is important for reducing hallucination. We use the CHAIR metric to evaluate different models, and look at the role of different visual representations. A recent work (Xiao and Wang, 2021) investigates the relationship between hallucinations and predictive uncertainty in image captioning and datato-text generation. The authors found that higher predictive uncertainty leads to a higher chance of hallucinating entities. We leave this kind of analysis for future work. 102 Visual Dialogues Evaluation. Among the visual dialogue datasets and tasks available (e.g., de Vries et al. 2017; Mostafazadeh et al. 2017; Das et al. 2017; Haber et al. 2019), we chose a task-oriented referential game, GuessWhat?! (de Vries et al., 2017). Task-oriented conversational agents generate dialogues to"
2021.eacl-main.178,W17-5534,0,0.0165026,"d Work Task-oriented models can be evaluated based on their task success, but this is not enough to know whether the generated dialogues are human-like. The development of quantitative metrics to evaluate the quality of dialogues generated by conversational agents is a difficult challenge (Liu et al., 2016), and it is under investigation for chit-chat dialogue systems. For instance, Guo et al. (2017) study topic diversity in the conversational flow, which is rather important in chit-chat and opendomain dialogues, but less so for task-oriented ones; Kannan and Vinyals (2016), Li et al. (2017), Bruni and Fernández (2017) propose to use adversarial evaluation, whereas Lowe et al. (2017), See et al. (2019), and Hashimoto et al. (2019) propose automatic systems that build upon human evaluation. All these efforts are still preliminary and are not easily employable for new datasets or new models. Since no standard and unique metric has been proposed to evaluate the quality of taskoriented (grounded) conversational dialogues, we consider a mixture of metrics used independently in various studies, and we provide a comparative analysis across models and tasks based on the same set of linguistic metrics. Neural Networ"
2021.eacl-main.178,D18-2029,0,0.0169961,"evel aspects: overall vocabulary usage (H, GR), diversity of questions/phrases within a dialogue (MO, GRQ), and similarity of content word usage with respect to human dialogues (LRd). There could be some correlation between metrics capturing similar aspects of language quality, but this does not affect the validity of the proposed LD metric. 4 Models For both visual dialogue games, GuessWhat and GuessWhich, supervised learning has been compared with other learning paradigms. After the random questions taken from other dialogues. Embeddings are obtained by using Universal Sentence Encoder-USE (Cer et al., 2018). We found that novel and low-MO questions are more similar to their dialogue than the random ones, confirming the effectiveness of these metrics. introduction of the supervised baseline model (de Vries et al., 2017), several models have been proposed for GuessWhat. They exploit either reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b,a; Zhao and Tresp, 2018; Gan et al., 2019; Pang and Wang, 2020) or cooperative learning (Shekhar et al., 2019; Pang and Wang, 2020); in both cases, the model is first trained with the supervised learning regime and then the new paradigm is applie"
2021.eacl-main.178,P19-1648,0,0.0195343,"nd GuessWhich, supervised learning has been compared with other learning paradigms. After the random questions taken from other dialogues. Embeddings are obtained by using Universal Sentence Encoder-USE (Cer et al., 2018). We found that novel and low-MO questions are more similar to their dialogue than the random ones, confirming the effectiveness of these metrics. introduction of the supervised baseline model (de Vries et al., 2017), several models have been proposed for GuessWhat. They exploit either reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b,a; Zhao and Tresp, 2018; Gan et al., 2019; Pang and Wang, 2020) or cooperative learning (Shekhar et al., 2019; Pang and Wang, 2020); in both cases, the model is first trained with the supervised learning regime and then the new paradigm is applied. This two-step process has been shown to reach higher task success than the supervised approach. For GuessWhich, after the supervised model introduced in Das et al. (2017a), new models based on reinforcement learning have been proposed, too (Das et al., 2017b; Murahari et al., 2019; Zhou et al., 2019), but their task success is comparable if not lower than the one achieved by using only sup"
2021.eacl-main.178,P19-1184,0,0.204038,"ising both for chitchat (Vinyals and Le, 2015) and task-oriented dialogues (Lewis et al., 2017), and it has been further extended to develop agents that can communicate through natural language about visual content (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). Several dialogue tasks have been proposed as referential guessing games in which an agent (the Q-bot) asks questions to another agent (the A-bot) and has to guess the referent (e.g., a specific object depicted in the image) they have been speaking about (de Vries et al., 2017; Das et al., 2017b; He et al., 2017; Haber et al., 2019; Ilinykh et al., 2019; Udagawa and Aizawa, 2019). We are interested in understanding the interplay between the learning processes behind these two sub-tasks: generating questions and guessing the referent. Shekhar et al. (2019) have compared models on GuessWhat and have shown that task success (TS) does not correlate with the quality of machine-generated dialogues. First of all, we check whether this result is task-agnostic by carrying out a comparative analysis of models playing different referential games. We choose a task in which visual grounding happens during question generation (GuessW"
2021.eacl-main.178,N19-1169,0,0.0153977,"generated dialogues are human-like. The development of quantitative metrics to evaluate the quality of dialogues generated by conversational agents is a difficult challenge (Liu et al., 2016), and it is under investigation for chit-chat dialogue systems. For instance, Guo et al. (2017) study topic diversity in the conversational flow, which is rather important in chit-chat and opendomain dialogues, but less so for task-oriented ones; Kannan and Vinyals (2016), Li et al. (2017), Bruni and Fernández (2017) propose to use adversarial evaluation, whereas Lowe et al. (2017), See et al. (2019), and Hashimoto et al. (2019) propose automatic systems that build upon human evaluation. All these efforts are still preliminary and are not easily employable for new datasets or new models. Since no standard and unique metric has been proposed to evaluate the quality of taskoriented (grounded) conversational dialogues, we consider a mixture of metrics used independently in various studies, and we provide a comparative analysis across models and tasks based on the same set of linguistic metrics. Neural Networks have been shown to generate text that sounds unnatural due to the presence of repeated utterances, poor vocabul"
2021.eacl-main.178,P17-1162,0,0.0565032,"Missing"
2021.eacl-main.178,W19-8621,0,0.0263898,"hat (Vinyals and Le, 2015) and task-oriented dialogues (Lewis et al., 2017), and it has been further extended to develop agents that can communicate through natural language about visual content (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). Several dialogue tasks have been proposed as referential guessing games in which an agent (the Q-bot) asks questions to another agent (the A-bot) and has to guess the referent (e.g., a specific object depicted in the image) they have been speaking about (de Vries et al., 2017; Das et al., 2017b; He et al., 2017; Haber et al., 2019; Ilinykh et al., 2019; Udagawa and Aizawa, 2019). We are interested in understanding the interplay between the learning processes behind these two sub-tasks: generating questions and guessing the referent. Shekhar et al. (2019) have compared models on GuessWhat and have shown that task success (TS) does not correlate with the quality of machine-generated dialogues. First of all, we check whether this result is task-agnostic by carrying out a comparative analysis of models playing different referential games. We choose a task in which visual grounding happens during question generation (GuessWhat, de Vries et al. 2"
2021.eacl-main.178,D17-1259,0,0.0212855,"valuated on their task success and it is common practice to choose the best model based only on the task success metric. We explore whether this choice prevents the system from learning better linguistic skills. Important progress has been made on the development of such conversational agents. The boost is mostly due to the introduction of the encoderdecoder framework (Sutskever et al., 2014) which allows learning directly from raw data to both understand and generate utterances. The framework has been found to be promising both for chitchat (Vinyals and Le, 2015) and task-oriented dialogues (Lewis et al., 2017), and it has been further extended to develop agents that can communicate through natural language about visual content (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). Several dialogue tasks have been proposed as referential guessing games in which an agent (the Q-bot) asks questions to another agent (the A-bot) and has to guess the referent (e.g., a specific object depicted in the image) they have been speaking about (de Vries et al., 2017; Das et al., 2017b; He et al., 2017; Haber et al., 2019; Ilinykh et al., 2019; Udagawa and Aizawa, 2019). We are interested in under"
2021.eacl-main.178,D17-1230,0,0.225382,"age proficiency takes longer than learning the guessing task. By comparing models playing different games (GuessWhat, GuessWhich, and Mutual Friends), we show that this discrepancy is model- and taskagnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set. 1 Introduction A good dialogue model should generate utterances that are indistinguishable from human dialogues (Liu et al., 2016; Li et al., 2017). This holds for both chit-chat, open-domain, and task-oriented dialogues. While chit-chat dialogue systems are usually evaluated by analysing the quality of their dialogues (Lowe et al., 2017; See et al., 2019), task-oriented dialogue models are evaluated on their task success and it is common practice to choose the best model based only on the task success metric. We explore whether this choice prevents the system from learning better linguistic skills. Important progress has been made on the development of such conversational agents. The boost is mostly due to the introduction of the encode"
2021.eacl-main.178,D16-1230,0,0.0937521,"Missing"
2021.eacl-main.178,P17-1103,0,0.167199,"and taskagnostic. We investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set. 1 Introduction A good dialogue model should generate utterances that are indistinguishable from human dialogues (Liu et al., 2016; Li et al., 2017). This holds for both chit-chat, open-domain, and task-oriented dialogues. While chit-chat dialogue systems are usually evaluated by analysing the quality of their dialogues (Lowe et al., 2017; See et al., 2019), task-oriented dialogue models are evaluated on their task success and it is common practice to choose the best model based only on the task success metric. We explore whether this choice prevents the system from learning better linguistic skills. Important progress has been made on the development of such conversational agents. The boost is mostly due to the introduction of the encoderdecoder framework (Sutskever et al., 2014) which allows learning directly from raw data to both understand and generate utterances. The framework has been found to be promising both for chitc"
2021.eacl-main.178,I17-1047,0,0.102145,"this choice prevents the system from learning better linguistic skills. Important progress has been made on the development of such conversational agents. The boost is mostly due to the introduction of the encoderdecoder framework (Sutskever et al., 2014) which allows learning directly from raw data to both understand and generate utterances. The framework has been found to be promising both for chitchat (Vinyals and Le, 2015) and task-oriented dialogues (Lewis et al., 2017), and it has been further extended to develop agents that can communicate through natural language about visual content (Mostafazadeh et al., 2017; Das et al., 2017a; de Vries et al., 2017). Several dialogue tasks have been proposed as referential guessing games in which an agent (the Q-bot) asks questions to another agent (the A-bot) and has to guess the referent (e.g., a specific object depicted in the image) they have been speaking about (de Vries et al., 2017; Das et al., 2017b; He et al., 2017; Haber et al., 2019; Ilinykh et al., 2019; Udagawa and Aizawa, 2019). We are interested in understanding the interplay between the learning processes behind these two sub-tasks: generating questions and guessing the referent. Shekhar et al. ("
2021.eacl-main.178,D19-1152,0,0.138365,"ity of machine-generated dialogues. First of all, we check whether this result is task-agnostic by carrying out a comparative analysis of models playing different referential games. We choose a task in which visual grounding happens during question generation (GuessWhat, de Vries et al. 2017); a task in which it happens only in the guessing phase (GuessWhich, Das et al. 2017b), and a task that is only based on language (MutualFriends, He et al. 2017). We introduce a linguistic metric, Linguistic Divergence (LD), that, by assembling various metrics used in the literature (Shekhar et al., 2019; Murahari et al., 2019; van Miltenburg et al., 2019), measures how much the language generated by computational models differs, on the surface level, from the one used by humans. We consider LD to be a proxy of the quality of machine-generated dialogues. For each task, we compare State-Of-The-Art (SOTA) models against their TS and LD. In the 2071 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2071–2082 April 19 - 23, 2021. ©2021 Association for Computational Linguistics core part of the paper, we study the relationship between the learning process"
2021.eacl-main.178,D19-1596,0,0.0277413,"upon human evaluation. All these efforts are still preliminary and are not easily employable for new datasets or new models. Since no standard and unique metric has been proposed to evaluate the quality of taskoriented (grounded) conversational dialogues, we consider a mixture of metrics used independently in various studies, and we provide a comparative analysis across models and tasks based on the same set of linguistic metrics. Neural Networks have been shown to generate text that sounds unnatural due to the presence of repeated utterances, poor vocabulary, and inconsistency in word usage (Ray et al., 2019). Various improvements have been proposed to mitigate these weaknesses. To prevent the decoder from choosing words based simply on their frequency, Li et al. (2019) replace its maximum likelihood estimation objective, while others change the sampling search strategy (Holtzman et al., 2020; Wu et al., 2019; See et al., 2019); these changes aim to reduce the number of repeated questions, to increase the variety of words and their distribution. Attempts have been made to provide the conversational models with a reasoning module based on Bayesian inference (Abbasnejad et al., 2019) or Rational Spe"
2021.eacl-main.178,N19-1170,0,0.160178,"e investigate whether and when better language quality could lead to higher task success. We show that in GuessWhat, models could increase their accuracy if they learn to ground, encode, and decode also words that do not occur frequently in the training set. 1 Introduction A good dialogue model should generate utterances that are indistinguishable from human dialogues (Liu et al., 2016; Li et al., 2017). This holds for both chit-chat, open-domain, and task-oriented dialogues. While chit-chat dialogue systems are usually evaluated by analysing the quality of their dialogues (Lowe et al., 2017; See et al., 2019), task-oriented dialogue models are evaluated on their task success and it is common practice to choose the best model based only on the task success metric. We explore whether this choice prevents the system from learning better linguistic skills. Important progress has been made on the development of such conversational agents. The boost is mostly due to the introduction of the encoderdecoder framework (Sutskever et al., 2014) which allows learning directly from raw data to both understand and generate utterances. The framework has been found to be promising both for chitchat (Vinyals and Le"
2021.eacl-main.178,N19-1265,1,0.43771,"deh et al., 2017; Das et al., 2017a; de Vries et al., 2017). Several dialogue tasks have been proposed as referential guessing games in which an agent (the Q-bot) asks questions to another agent (the A-bot) and has to guess the referent (e.g., a specific object depicted in the image) they have been speaking about (de Vries et al., 2017; Das et al., 2017b; He et al., 2017; Haber et al., 2019; Ilinykh et al., 2019; Udagawa and Aizawa, 2019). We are interested in understanding the interplay between the learning processes behind these two sub-tasks: generating questions and guessing the referent. Shekhar et al. (2019) have compared models on GuessWhat and have shown that task success (TS) does not correlate with the quality of machine-generated dialogues. First of all, we check whether this result is task-agnostic by carrying out a comparative analysis of models playing different referential games. We choose a task in which visual grounding happens during question generation (GuessWhat, de Vries et al. 2017); a task in which it happens only in the guessing phase (GuessWhich, Das et al. 2017b), and a task that is only based on language (MutualFriends, He et al. 2017). We introduce a linguistic metric, Lingu"
2021.eacl-main.178,P19-1646,0,0.0225235,"improvements have been proposed to mitigate these weaknesses. To prevent the decoder from choosing words based simply on their frequency, Li et al. (2019) replace its maximum likelihood estimation objective, while others change the sampling search strategy (Holtzman et al., 2020; Wu et al., 2019; See et al., 2019); these changes aim to reduce the number of repeated questions, to increase the variety of words and their distribution. Attempts have been made to provide the conversational models with a reasoning module based on Bayesian inference (Abbasnejad et al., 2019) or Rational Speech Act (Shuklar et al., 2019) frameworks that should lead to more informative and coherent questions. Here, we do not propose new models, but rather aim to better understand the strengths and weaknesses of current models. 3 Games and Metrics Our focus is on task-oriented dialogues. We consider a task that relies on grounding language into vision during question generation, i.e. GuessWhat (de Vries et al., 2017), a task that requires grounding only at the guessing phase, i.e. GuessWhich (Das et al., 2017b), and a task based only on language, i.e. MutualFriends, (He et al., 2017). Games As illustrated by the snippets report"
2021.eacl-main.178,D19-1014,0,0.0173926,"reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b,a; Zhao and Tresp, 2018; Gan et al., 2019; Pang and Wang, 2020) or cooperative learning (Shekhar et al., 2019; Pang and Wang, 2020); in both cases, the model is first trained with the supervised learning regime and then the new paradigm is applied. This two-step process has been shown to reach higher task success than the supervised approach. For GuessWhich, after the supervised model introduced in Das et al. (2017a), new models based on reinforcement learning have been proposed, too (Das et al., 2017b; Murahari et al., 2019; Zhou et al., 2019), but their task success is comparable if not lower than the one achieved by using only supervised learning (see Testoni et al. 2019). Below, we briefly describe the models we have compared in our analysis. For each task, we have chosen generative models trained with different learning paradigms and for which the code is available; for each paradigm, we have tried to choose the best performing ones or those that obtain a task success near to state-of-the art and could help better understand the interplay between task success and dialogue quality. GuessWhat We use the A-Bot introduced in de Vri"
2021.eacl-main.178,P19-1375,0,0.0226368,"in various studies, and we provide a comparative analysis across models and tasks based on the same set of linguistic metrics. Neural Networks have been shown to generate text that sounds unnatural due to the presence of repeated utterances, poor vocabulary, and inconsistency in word usage (Ray et al., 2019). Various improvements have been proposed to mitigate these weaknesses. To prevent the decoder from choosing words based simply on their frequency, Li et al. (2019) replace its maximum likelihood estimation objective, while others change the sampling search strategy (Holtzman et al., 2020; Wu et al., 2019; See et al., 2019); these changes aim to reduce the number of repeated questions, to increase the variety of words and their distribution. Attempts have been made to provide the conversational models with a reasoning module based on Bayesian inference (Abbasnejad et al., 2019) or Rational Speech Act (Shuklar et al., 2019) frameworks that should lead to more informative and coherent questions. Here, we do not propose new models, but rather aim to better understand the strengths and weaknesses of current models. 3 Games and Metrics Our focus is on task-oriented dialogues. We consider a task tha"
2021.eacl-main.178,W18-5015,0,0.021523,"s For both visual dialogue games, GuessWhat and GuessWhich, supervised learning has been compared with other learning paradigms. After the random questions taken from other dialogues. Embeddings are obtained by using Universal Sentence Encoder-USE (Cer et al., 2018). We found that novel and low-MO questions are more similar to their dialogue than the random ones, confirming the effectiveness of these metrics. introduction of the supervised baseline model (de Vries et al., 2017), several models have been proposed for GuessWhat. They exploit either reinforcement learning (Sang-Woo et al., 2019; Zhang et al., 2018b,a; Zhao and Tresp, 2018; Gan et al., 2019; Pang and Wang, 2020) or cooperative learning (Shekhar et al., 2019; Pang and Wang, 2020); in both cases, the model is first trained with the supervised learning regime and then the new paradigm is applied. This two-step process has been shown to reach higher task success than the supervised approach. For GuessWhich, after the supervised model introduced in Das et al. (2017a), new models based on reinforcement learning have been proposed, too (Das et al., 2017b; Murahari et al., 2019; Zhou et al., 2019), but their task success is comparable if not lo"
2021.emnlp-main.736,D18-1437,0,0.063119,"Missing"
2021.emnlp-main.736,2021.eacl-main.219,0,0.0242623,"in beam search. We show that the task accuracy of both the conversational agent and human subjects increases when receiving the dialogues generated by the Confirm-it re-ranking algorithm. timize for output with high probability (like beam search) lead to highly deteriorated texts, since the highest scores are often assigned to generic, incoherent, and repetitive sequences. Several works propose reranking strategies on the set of hypotheses produced by the beam search following different criteria (Dušek and Jurˇcíˇcek, 2016; Blain et al., 2017; Agarwal et al., 2018; Borgeaud and Emerson, 2020; Hargreaves et al., 2021) to improve both the performance on a given task and the quality of the output. In this work, we present a cognitivelyinspired reranking technique for a visual dialogue questioner agent. In visual dialogue systems, the quality of the output has been improved mainly by aiming at reducing repetitions in the output. This goal has been achieved through Reinforcement Learning by adding auxiliary objective functions (Murahari et al., 2019), intermediate rewards (Zhang et al., 2018), regularized information gain techniques (Shukla et al., 2019), or intermediate probabilities with an attention mechani"
2021.splurobonlp-1.3,2020.coling-main.509,0,0.0522702,"Missing"
2021.splurobonlp-1.3,P18-1128,0,0.0272946,"te questions. This is reflected both by the baseline and the SOTA model: LSTM drops from 77.31 (All) to 70.45 (Absolute) to 67.11 (Group) and similarly does LXMERT – from 82.40 to 79.42 to 74.48. Even the accuracy of the best When looking at the context-dependent questions, the standard-deviation among the accuracies reached by the three runs is rather high, hence in order to understand its effect on the comparison between models when receiving just the question and the question together with the previous turn, we have run a statistical significance paired t-test (following the suggestions in Dror et al. (2018).) The result shows that the difference between the two settings is never significant, except for LSTM/LSTMDH on the absolute questions (p-value &lt; 0.05). This shows that model performance is rather unstable and hence the selection of the binary answer is not properly grounded. This instability is not due to the size of the set: we have computed accuracy of the three runs of LXMERT on subsets of 500 and 100 randomly chosen questions and obtained a very low standard deviation. Since the questions we have accurately selected are context-dependent, ideally a model should increase confidence in its"
2021.splurobonlp-1.3,2020.acl-main.728,0,0.188922,"e 1: Through the dialogue the focus shifts from all the mandarins to just one. To answer Q4 (the zoomer), “top” needs to be interpreted relatively to the group of two mandarins identified by Q3 (the trigger). both the visual and language contexts. As such this multi-folded challenge is rather ambitious. Our work focuses on identifying Follow-up Questions (FuQs) in Visual Dialogue. Namely, our goal is to construct a dataset of questions that we know require grounding both on the visual input and the dialogue history. Work carried out on modeling the role of dialogue history in visual dialogue (Agarwal et al., 2020) has used the chit-chat dialogues of VisDial (Das et al., 2017) as a case study. However, it has been shown that in this dataset the role of grounding the question on the dialogue history is limited: models that take history into account do better, but the dataset contains a small percentage of questions that require dialogue history to be interpreted correctly. Based on these findings, Agarwal et al point out the need for data which captures dialogue history dependence. Our work is a contribution to this data collection challenge. We aim to identify FuQs which require (part of) the dialogue h"
2021.splurobonlp-1.3,D19-1514,0,0.139056,", the issue of dialogue history needed by the Oracle has never been considered. Since the first baseline model (de Vries et al., 2017), the Oracle receives just the question without the previous turns. Furthermore, this baseline model is blind: it takes the question, the target’s category and its location as inputs. This simple model has been widely used as the Oracle agent by all work on the Questioner (eg. (Strub et al., 2017; Shekhar et al., 2019; Pang and Wang, 2020).) Testoni et al. (2020) compared the LSTM baseline with a visually grounded LSTM (V-LSTM) and with an adaptation of LXMERT (Tan and Bansal, 2019). They show LXMERT based Oracle improves over the baseline achieving a new SOTA for the GuessWhat?! Oracle. Yet the model does not use dialog history as an input. We evaluate LSTM, VLSTM and LXMERT against our dataset of contextdependent questions. Agarwal et al. (2020) argues that although complex models that encode history for visual dialogs have been proposed (Yang et al., 2019), such work has not demonstrated that history matters for visual dialogs. Agarwal et al. propose and apply a new methodology for evaluating history dependence of questions in visual dialog. They show crowdsourcers a"
2021.splurobonlp-1.3,N19-1265,1,0.917127,"target. The interpretation of a question depends on its QuD. Most of the work on the GuessWhat?! game has focused attention on the Questioner player; as a consequence, the issue of dialogue history needed by the Oracle has never been considered. Since the first baseline model (de Vries et al., 2017), the Oracle receives just the question without the previous turns. Furthermore, this baseline model is blind: it takes the question, the target’s category and its location as inputs. This simple model has been widely used as the Oracle agent by all work on the Questioner (eg. (Strub et al., 2017; Shekhar et al., 2019; Pang and Wang, 2020).) Testoni et al. (2020) compared the LSTM baseline with a visually grounded LSTM (V-LSTM) and with an adaptation of LXMERT (Tan and Bansal, 2019). They show LXMERT based Oracle improves over the baseline achieving a new SOTA for the GuessWhat?! Oracle. Yet the model does not use dialog history as an input. We evaluate LSTM, VLSTM and LXMERT against our dataset of contextdependent questions. Agarwal et al. (2020) argues that although complex models that encode history for visual dialogs have been proposed (Yang et al., 2019), such work has not demonstrated that history ma"
2021.splurobonlp-1.3,2020.splu-1.4,1,0.70639,"ends on its QuD. Most of the work on the GuessWhat?! game has focused attention on the Questioner player; as a consequence, the issue of dialogue history needed by the Oracle has never been considered. Since the first baseline model (de Vries et al., 2017), the Oracle receives just the question without the previous turns. Furthermore, this baseline model is blind: it takes the question, the target’s category and its location as inputs. This simple model has been widely used as the Oracle agent by all work on the Questioner (eg. (Strub et al., 2017; Shekhar et al., 2019; Pang and Wang, 2020).) Testoni et al. (2020) compared the LSTM baseline with a visually grounded LSTM (V-LSTM) and with an adaptation of LXMERT (Tan and Bansal, 2019). They show LXMERT based Oracle improves over the baseline achieving a new SOTA for the GuessWhat?! Oracle. Yet the model does not use dialog history as an input. We evaluate LSTM, VLSTM and LXMERT against our dataset of contextdependent questions. Agarwal et al. (2020) argues that although complex models that encode history for visual dialogs have been proposed (Yang et al., 2019), such work has not demonstrated that history matters for visual dialogs. Agarwal et al. propo"
2021.splurobonlp-1.3,H89-1033,0,0.686935,"set the role of grounding the question on the dialogue history is limited: models that take history into account do better, but the dataset contains a small percentage of questions that require dialogue history to be interpreted correctly. Based on these findings, Agarwal et al point out the need for data which captures dialogue history dependence. Our work is a contribution to this data collection challenge. We aim to identify FuQs which require (part of) the dialogue history to be interpreted. Introduction The development of multimodal conversation agents is a long standing challenge (e.g. (Winograd, 1972)). In recent years, much has been achieved on the challenge of Visual Question Answering (VQA) (e.g. (Antol et al., 2015; Goyal et al., 2017).) The rapid advancements have brought researchers to further increase the difficulty of the task by proposing Visual Dialogue datasets (e.g. (Das et al., 2017; de Vries et al., 2017)) suitable to train multimodal dialogue systems. With this switch from VQA to Visual Dialogue, the challenge has increased in difficulty. First of all, while VQA involves only understanding the multimodal input (image and question), Visual Dialogues also require visual questi"
bernardi-etal-2006-pos,J99-2004,0,\N,Missing
bernardi-etal-2006-pos,U05-1025,1,\N,Missing
bernardi-etal-2006-pos,bosco-etal-2000-building,0,\N,Missing
bernardi-etal-2006-pos,W00-0717,0,\N,Missing
bernardi-etal-2006-pos,P93-1034,0,\N,Missing
bernardi-etal-2006-pos,gimenez-marquez-2004-svmtool,0,\N,Missing
bernardi-etal-2006-pos,P93-1024,0,\N,Missing
bernardi-etal-2010-context,de-marneffe-etal-2006-generating,0,\N,Missing
bernardi-etal-2010-context,W06-3005,0,\N,Missing
bernardi-etal-2010-context,P03-1054,0,\N,Missing
bernardi-etal-2010-context,J95-2003,0,\N,Missing
bernardi-etal-2010-context,P87-1022,0,\N,Missing
bernardi-etal-2010-context,J04-3003,0,\N,Missing
bernardi-etal-2010-context,poesio-kabadjov-2004-general,0,\N,Missing
C18-1104,W10-4342,0,\N,Missing
C18-1104,E09-1081,0,\N,Missing
C18-1104,W13-4065,0,\N,Missing
C18-1104,W15-4610,0,\N,Missing
C18-1104,D16-1127,0,\N,Missing
C18-1104,W16-3630,0,\N,Missing
C18-1104,W16-3631,0,\N,Missing
C18-1104,W16-3643,0,\N,Missing
C18-1104,I17-1047,0,\N,Missing
C18-1104,W17-5539,0,\N,Missing
C18-1104,D12-1008,0,\N,Missing
C18-1104,W15-4643,0,\N,Missing
C18-1104,N16-1014,0,\N,Missing
C18-1199,D15-1075,0,0.293942,"(interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant “world” or “situation”). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare “blind” and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing “grounding” in an optimal fashion. 1 Introduction Evaluating the ability to infer information from a text is a crucial test of the capability of models to grasp meaning. As a result, the computational linguistics community has invested huge efforts into developing textual entailment (TE) datasets. After formal semanticists developed FraCas in the mid ’90 (Cooper e"
C18-1199,W17-6809,0,0.0391296,"Missing"
C18-1199,N18-2017,0,0.0195464,"ng point the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), the largest natural language inference dataset available with sentence pairs labelled with entailment, contradiction and neutral relations. We augmented this dataset with images. It has been shown very recently that SNLI contains language bias, such that a simple classifier can achieve high accuracy in predicting the three classes just by having as input the hypothesis sentence. A subset of the SNLI test set with ‘hard’ cases, where such a simplistic classifier fails (hereafter SNLIhard ) has been released (Gururangan et al., 2018). Hence, in this paper we will report our results on both the full dataset and the hard 2356 test set, but then zoom in on SNLIhard to understand the models’ behaviour. We briefly introduce SNLI and the new test set and compare them through our annotation of linguistic phenomena. 3.1 Dataset construction SNLI and SNLIhard test set The SNLI dataset (Bowman et al., 2015) was built through Amazon Mechanical Turk. Workers were shown captions of photographs without the photo and were asked to write a new caption that (a) is definitely a true description of the photo (entailment); (b) might be a tru"
C18-1199,D17-1305,0,0.0112777,"l vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks. We share with this work the idea that the image can be taken as a possible world. However, we don’t use sets of images to obtain the visual denotation of text in order to check whether entailment is logically valid/highly likely. Rather, we take the image to be"
C18-1199,E17-1068,0,0.0142869,"on between linguistic and visual vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spaces improves model performance on those three tasks. We share with this work the idea that the image can be taken as a possible world. However, we don’t use sets of images to obtain the visual denotation of text in order to check whether entailment is logically valid/highly likely. Rather, we"
C18-1199,J93-2004,0,0.0604657,"otation of a subset of the SNLI test set. Linguistic phenomena Following the error analysis approach described in recent work (Nangia et al., 2017; Williams et al., 2018), we compiled a new list of linguistic features that can be of interest when contrasting SNLI and SNLIhard , as well as for evaluating RTE models. Some of these were detected automatically, while others were assigned manually. Automatic tags included S YNONYM and A NTONYM, which were detected using WordNet (Miller, 1995). Q UANTIFIER, P RONOUN, D IFF T ENSE, S UPERLATIVE and BARE NP were identified using Penn treebank labels (Marcus et al., 1993), while labels such as N EGATION were found with a straightforward keyword search. The tag L ONG has been assigned to sentence pairs with a premise containing more than 30 tokens, or a hypothesis with more than 16 tokens. Details about the tags used in the manual annotation are presented in Table 3. We examined the differences in the tags distributions between the SNLI and SNLIhard test sets (Table 4). Interestingly, the hard sentence pairs from our random sample include proportionately more antonyms but fewer pronouns, as well as examples with different verb tenses in the premise and hypothes"
C18-1199,marelli-etal-2014-sick,1,0.892995,"the computational linguistics community has invested huge efforts into developing textual entailment (TE) datasets. After formal semanticists developed FraCas in the mid ’90 (Cooper et al., 1996), an increase in statistical approaches to computational semantics gave rise to the need for suitable evaluation datasets. Hence, Recognizing Textual Entailment (RTE) shared tasks have been organized regularly (Sammons et al., 2012). Recent work on compositional distributional models has motivated the development of the SICK dataset of sentence pairs in entailment relations for evaluating such models (Marelli et al., 2014). Further advances with Neural Networks (NNs) have once more motivated efforts to develop a large natural language inference dataset, SNLI (Bowman et al., 2015), since NNs need to be trained on big data. However, meaning is not something we obtain just from text and the ability to reason is not unimodal either. The importance of enriching meaning representations with other modalities has been advocated by cognitive scientists, (e.g., (Andrews et al., 2009; Barsalou, 2010)) and computational linguists (e.g., (Glavaˇs et al., 2017)). While efforts have been put into developing multimodal dataset"
C18-1199,passonneau-etal-2006-inter,0,0.025145,"Missing"
C18-1199,D14-1162,0,0.0809103,"Missing"
C18-1199,P17-2034,0,0.0230488,"d. A suitable GTE model therefore has to perform two sub-tasks: (a) it needs to ground its linguistic representations of P, H or both in non-linguistic (visual) data; (b) it needs to reason about the possible relationship between P and H (modulo the visual information). 2 Related Work Grounding language through vision has recently become the focus of several tasks, including Image Captioning (IC, e.g. (Hodosh et al., 2013; Xu et al., 2015)) and Visual Question Answering (VQA, eg. (Malinowski and Fritz, 2014; Antol et al., 2015)), and even more recently, Visual Reasoning (Johnson et al., 2017; Suhr et al., 2017) and Visual Dialog (Das et al., 2017). Our focus is on Grounded Textual Entailment (GTE). While the literature on TE is rather vast, GTE is still rather unexplored territory. Textual Entailment Throughout the history of Computational Linguistics various datasets have been built to evaluate Computational Semantics models on the TE task. Usually they contain data divided into entailment, contradiction or unknown classes. The “unknown” label has sometimes been replaced with the “unrelated” or “neutral” label, capturing slightly different types of phenomena. Interestingly, the “entailment” and “co"
C18-1199,N18-1101,0,0.0129406,"e book is old. P: A woman is applying lip makeup to another woman, H: The man is ready to fight. P: A group of people are taking a fun train ride, H: People ride the train. P: A crowd gathered on either side of a Soap Box Derby, H: The people are at the race. P: Kids being walked by an adult, H: An adult is escorting some children. P: A woman walks in front of a giant clock, H: The clock walks in front of the woman. Table 3: Tags used in manual annotation of a subset of the SNLI test set. Linguistic phenomena Following the error analysis approach described in recent work (Nangia et al., 2017; Williams et al., 2018), we compiled a new list of linguistic features that can be of interest when contrasting SNLI and SNLIhard , as well as for evaluating RTE models. Some of these were detected automatically, while others were assigned manually. Automatic tags included S YNONYM and A NTONYM, which were detected using WordNet (Miller, 1995). Q UANTIFIER, P RONOUN, D IFF T ENSE, S UPERLATIVE and BARE NP were identified using Penn treebank labels (Marcus et al., 1993), while labels such as N EGATION were found with a straightforward keyword search. The tag L ONG has been assigned to sentence pairs with a premise co"
C18-1199,Q14-1006,0,0.0547872,"given an encoding of the premise (Kolesnyk et al., 2016; Starc and Mladeni´c, 2017). Vision and Textual Entailment In recent years, several models have been proposed to integrate the language and vision modalities; usually the integration is operationalized by element-wise multiplication between linguistic and visual vectors. Though the interest in these modalities has spread in an astonishing way thanks to various multimodal tasks proposed, including the IC, VQA, Visual Reasoning and Visual Dialogue tasks mentioned above, very little work has been done on grounding entailment. Interestingly, Young et al. (2014) has proposed the idea of considering images as the “possible worlds” on which sentences find their denotation. Hence, they released a “visual denotation graph” which associates sentences with their denotation (sets of images). The idea has been further exploited by Lai and Hockenmaier (2017) and Han et al. (2017). Vendrov et al. (2016) look at hypernymy, textual entailment and image captioning as special cases of a single visual-semantic hierarchy over words, sentences and images, and they claim that modelling the partial order structure of this hierarchy in visual and linguistic semantic spa"
C18-1199,W17-5301,0,\N,Missing
D13-1072,J10-4006,0,0.272769,"nefit from natural language processing as learning from images requires a prohibitively expensive annotation effort. A major goal of natural language processing is to obtain general knowledge from text and in this paper we test which model provides the best knowledge for use in the visual domain. Within the two visual scenarios, we compare three state-of-the-art language models and a knowledge base: (1) A window-based model, which counts co-occurrence frequencies within a fixed window; (2) R-LDA (S´eaghdha, 2010), an extension of LDA that enables generation of joint probabilities; (3) TypeDM (Baroni and Lenci, 2010), a strong Distributional Memory model; (4) ConceptNet (Speer and Havasi, 2013), an automatically generated semantic graph containing concepts with their relations. We test the language models in two ways: (1) We directly compare the statistics of the linguistic models with statistics extracted from the visual domain. 769 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 769–779, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics (2) We compare the linguistic models inside the two computer vision applicatio"
D13-1072,P12-1015,0,0.0247552,"abilities using their frequency counts. For example, to compute the conditional probability of an object given a scene P (oi |sj ), we extract all triples having the form <object, rel, scene>, where “rel” can be “AtLocation”, “LocatedNear”, etc. P (oi |sj ) = P f req(< oi , rel, sj >) om ∈O f req< om , rel, sj > (9) 4.2 Window model One of the most famous and basic statistical model is based on counting co-occurrences within a window of fixed width, which follows the tradition of hyperspace analogue to language (Lund and Burgess, 1996). We took the Window2, 20 models which have been built in (Bruni et al., 2012) using the ukWaC (1.9B tokens) and Wackypedia (820M tokens). As the Window2 model only looks at 2 words on the left and right of the current one, it reflects the relationships between words occurring near each other, while the Window20 searches for a broader view of how words are related to each other. The weights of each pairs of words are calculated using the Local Mutual Information (LMI). To compute the conditional probabilities, we use the LMI scorLocatedNear oil car seatbelt car chair your bottom chair school plant everywhere muzzle dog trailer car dog bark bone salt table horse cowboy s"
D13-1072,P10-1045,0,0.0753564,"Missing"
D13-1072,D11-1041,0,\N,Missing
E12-1004,W03-1812,0,0.0177654,"rds tend to share similar contexts, DS has been very successful in tasks that require quantifying semantic similarity among words, such as synonym detection and concept clustering (Turney and Pantel, 2010). Recently, there has been a flurry of interest in DS to model meaning composition: How can we derive the DS representation of a composite phrase from that of its constituents? Although the general focus in the area is to perform algebraic operations on word semantic vectors (Mitchell and Lapata, 2010), some researchers have also directly examined the corpus contexts of phrases. For example, Baldwin et al. (2003) studied vector extraction for phrases because they were interested in the decomposability of multiword expressions. Baroni and Zamparelli (2010) and Guevara (2010) look at corpus-harvested phrase vectors to learn composition functions that should derive such composite vectors automatically. Baroni and Zamparelli, in particular, showed qualitatively that directly corpus-harvested vectors for AN constructions are meaningful; for example, the vector of young husband has nearest neighbors small son, small daughter and mistress. Following up on this approach, we show here quantitatively that corpu"
E12-1004,W11-2501,1,0.303507,"N usually does not entail another N, we can create negative examples (AN1 6|= N2 ) just by randomly permuting the Ns. Of course, such unsupervised data would be slightly noisy, especially because some of the most frequent adjectives are not restrictive. To collect cleaner data and to be sure that we are really examining the phenomenon of entailment, we took a mere few moments of manual effort to select the 256 restrictive adjectives from the most frequent 300 adjectives in the corpus. We then took the Cartesian product of these 256 adjectives with the 200 concrete nouns in the BLESS data set (Baroni and Lenci, 2011). Those nouns were chosen to avoid highly polysemous words. From the Cartesian product, we obtain a total of 1246 AN sequences, such as big cat, that occur more than 100 times in the corpus. These AN sequences encompass 190 of the 256 adjec26 tives and 128 of the 200 nouns. The process results in 1246 positive instances of AN |= N entailment, which we use as training data. To create a comparable amount of negative data, we randomly permuted the nouns in the positive instances to obtain pairs of AN1 6|= N2 (e.g., big cat 6|= dog). We manually double-checked that all positive and negative exampl"
E12-1004,D10-1115,1,0.92743,"a, but it has been largely limited to the lexical domain. On the other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in wh"
E12-1004,W07-1427,0,0.0641273,"Missing"
E12-1004,J90-1003,0,0.0700654,"ord phrases and single words, and more precisely to: those AN and QN sequences that are in the data sets (see next subsections), the adjectives, quantifiers and nouns contained in those sequences, and the most frequent (9.8K) nouns and (8.1K) adjectives in the corpus. The first step is to count the content words (more precisely, the most frequent 9.8K nouns, 8.1K adjectives, and 9.6K verbs in the corpus) that occur in the same sentence as phrases of interest. In the second step, following standard practice, the co-occurrence counts are converted into pointwise mutual information (PMI) scores (Church and Hanks, 1990). The result of this step is a sparse matrix (with both positive and negative entries) with 48K rows (one per phrase of interest) and 27K columns (one per content word). 3.2 The AN |= N data set To characterize entailment between nouns using their semantic vectors, we need data exemplifying which noun entails which. This section introduces one cheap way to collect such a training data set exploiting semantic vectors for composed expressions, namely AN sequences. We rely on the linguistic fact that ANs share a syntactic category and semantic type with plain common nouns (big cat shares syntacti"
E12-1004,W09-3711,0,0.101211,"o real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference. Here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally. In a first"
E12-1004,P05-1014,0,0.850495,"challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expressions bear on inference. Here we focus on phrasal vectors harvested directly from the corpus rather than obtained compositionally. In a first experiment, we exploit t"
E12-1004,D11-1129,0,0.0134247,"mited to the lexical domain. On the other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of"
E12-1004,W10-2805,0,0.0908867,"e other hand, FS has provided sophisticated models of sentence meaning, but it has been largely limited to hand-coded models that do not scale up to real-life challenges by learning from data. Given these complementary strengths, we naturally ask if DS and FS can address each other’s limitations. Two recent strands of research are bringing DS closer to meeting core FS challenges. One strand attempts to model compositionality with DS methods, representing both primitive and composed linguistic expressions as distributional vectors (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Guevara, 2010; Mitchell and Lapata, 2010). The other strand attempts to reformulate FS’s notion of logical inference in terms that DS can capture (Erk, 2009; Geffet and Dagan, 2005; Kotlerman et al., 2010; Zhitomirsky-Geffet and Dagan, 2010). In keeping with the lexical emphasis of DS, this strand has focused on inference at the word level, or lexical entailment, that is, discovering from distributional vectors of hyponyms (dog) that they entail their hypernyms (animal). This paper brings these two strands of research together by demonstrating two ways in which the distributional vectors of composite expre"
E12-1004,C92-2082,0,0.612514,"notion just described, whereas the entailment relations |=N and |=QP among nouns and quantifier phrases are the inclusion relations among sets of entities and sets of sets of entities respectively. Our results in Section 5 show that DS needs to treat |=N and |=QP differently as well. Empirical, corpus-based perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data spa"
E12-1004,N04-1041,0,0.0124637,"|=QP differently as well. Empirical, corpus-based perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally deri"
E12-1004,2003.mtsummit-papers.42,0,0.0525299,"Missing"
E12-1004,P06-1101,0,0.122522,"perspectives on entailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally derived) encode the information needed to"
E12-1004,C08-1114,0,0.0120661,"ailment Until recently, the corpus-based research tradition has studied entailment mostly at the word level, with applied goals such as classifying lexical relations and building taxonomic WordNet-like resources automatically. The most popular approach, first adopted by Hearst (1992), extracts lexical relations from patterns in large corpora. For instance, from the pattern N1 such as N2 one learns that N2 |= N1 (from insects such as beetles, derive beetles |= insects). Several studies have refined and extended this approach (Pantel and Ravichandran, 2004; Snow et al., 2005; Snow et al., 2006; Turney, 2008). While empirically very successful, the patternbased method is mostly limited to single content words (or frequent content-word phrases). We are interested in entailment between phrases, where it is not obvious how to use lexico-syntactic patterns and cope with data sparsity. For instance, it seems hard to find a pattern that frequently connects one QP to another it entails, as in all beetles PATTERN many beetles. Hence, we aim to find a more general method and investigate whether DS vectors (whether corpus-harvested or compositionally derived) encode the information needed to account for phr"
E12-1004,C04-1146,0,0.885377,"Missing"
E12-1004,W07-1412,0,0.0147927,"ing common sense into account. Instead of a relation from the truth of the consequent to the truth of the antecedent in any circumstance, the applied view looks at entailment in terms of plausibility: φ |= ψ if a human who reads (and trusts) φ would most likely infer that ψ is also true. Entailment systems have been compared under this new perspective in various evaluation campaigns, the best known being the Recognizing Textual Entailment (RTE) initiative (Dagan et al., 2009). Most RTE systems are based on advanced NLP components, machine learning techniques, and/or syntactic transformations (Zanzotto et al., 2007; Kouleykov and Magnini, 2005). A few systems exploit deep FS analysis (Bos and Markert, 2006; Chambers et al., 2007). In particular, the FS results about QP properties that affect entailment have been exploited by Chambers et al, who complement a core broad-coverage system with a Natural Logic module to trade lower recall for higher precision. For instance, they exploit the monotonicity properties of no that cause the following reversal in entailment direction: some beetles |= some insects but no insects |= no beetles. To investigate entailment step by step, we address here a much simpler and"
E12-1004,W07-1400,0,\N,Missing
E12-1004,J09-3004,0,\N,Missing
E17-2054,P14-1023,0,0.0423073,"datasets as balanced as possible. When designing the combinations for ‘few’ and ‘most’, for example, we controlled for the proportion of targets in the scene, in order to avoid making one of the two easier to learn. Also, combinations were thought to avoid biasing cardinals toward fixed proportions of targets/distractors. Building the scenarios We use images from ImageNet (Deng et al., 2009). Starting from the full list of 203 concepts and corresponding images extracted by Cassani (2014), we discarded those concepts whose corresponding word had low/null frequency in the large corpus used in (Baroni et al., 2014). To get rid of issues related to concept identification, we used a single representation for each of the 188 selected concepts. Technically, we computed a centroid vector by averaging the 4096-dimension visual features of the corresponding images, which were extracted from the fc7 of a CNN (Simonyan and Zisserman, 2014). We used the VGG-19 model pretrained on the ImageNet ILSVRC data (Russakovsky et al., 2015) implemented in the MatConvNet toolbox (Vedaldi and Lenc, 2015). Centroid vectors were reduced to 100-d via PCA and further normalized to length 1 before being used to build the scenario"
E17-2054,W16-3211,1,0.361394,"iation for Computational Linguistics: Volume 2, Short Papers, pages 337–342, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics of targets/distractors (§ 3.2). As hypothesized, the two quantification mechanisms turn out to be better accounted for by models capitalizing on the expected similarity measures. scenes, and to more recent work aimed at counting either everyday objects in natural images (Chattopadhyay et al., 2016) or geometrical objects with attributes in synthetic scenes (Johnson et al., 2016). With respect to quantifiers, our approach is similar to (Sorodoc et al., 2016), who use quantifiers no, some, and all to quantify over sets of colored dots. Differently from ours, however, all these works tackle the issue as either a classification problem or a Visual Question Answering task, with less focus on learning the meaning representation of each cardinal/quantifier. To our knowledge, this is the first attempt to jointly investigate both mechanisms and to obtain the meaning representaton of each cardinal/quantifier as resulting from a language-to-vision mapping. 2 Data In order to test our hypothesis, we need a dataset of visual scenes which crucially include mu"
J16-4003,2014.lilt-9.5,1,0.893455,"Missing"
J16-4003,P14-1023,1,0.773388,"Missing"
J16-4003,W11-2501,1,0.792201,"sibly evoke the alternative This is a cat?,” we ask the more intuitive question: “How plausible is the sentence This is not a dog, it is a cat?” The corresponding THERE context is There is no dog here, but there is a cat, with but added as it makes the sentence more natural. 4.2 Selecting Potential Alternatives As just discussed, our stimuli contain exactly two content words—a noun either in a predicative or in an existential negated position, and its potential alternative in a comparable position. To construct the noun pairs, we took as negated elements 50 randomly selected items from BLESS (Baroni and Lenci 2011), and paired them with potential alternatives from several sources. We picked alternatives from different sources in order to take a variety of relations that might affect alternativehood into account. However, we make no claim about the exhaustiveness of the phenomena we are considering, and we do not present theoretical conjectures about how lexical relations affect the likelihood of being a plausible alternative. One of the sources was WordNet,2 from which we extracted lists of cohyponyms, hyponyms, and hypernyms. For the 50 negated items, there are almost 4K WordNet cohyponyms, many of the"
J16-4003,D10-1115,1,0.875636,"Missing"
J16-4003,W13-3206,1,0.907722,"Missing"
J16-4003,W10-2803,0,0.0254291,"y symmetric (There is no cat here, but there is an animal is plausible, but ?There is no animal here, but there is a cat sounds odd). Inverted pairs will obviously be a challenge to the symmetric cosine measure. We expect that, once we enrich the data set with such cases, plain DS similarity will no longer suffice, and we will have to rely on supervised approaches that currently seem almost superfluous. Similarly, compositional methods were only very moderately useful to account for our current skeletal sentential contexts. Compositional, or, more generally, wordmeaning-in-context approaches (Erk 2010) will have a better chance to prove their worth if we extend the empirical base to include more informative contexts, and let longer constituents be under the scope of negation and/or in the alternative set (cf., There is no bachelor here, but there is a. . . married man, ??unmarried man). From an empirical point of view, it will of course also be important to study conversational negations in more natural set-ups than the highly controlled experiment we designed. From a theoretical point of view, the most pressing and interesting issue pertains to integrating what we observed empirically into"
J16-4003,D08-1094,0,0.0759667,"Missing"
J16-4003,J10-4007,0,0.0402758,"Missing"
J16-4003,S13-1001,0,0.259002,"defined a priori, attempting to mimic the logical properties of negation, rather than being induced from distributional data. Clark, Coecke, and Sadrzadeh (2008) explore the idea that sentences live in a space spanned by a single “truth-denoting” basis (~1), with the origin (~0) corresponding to “false.” A sentence like John likes Mary is represented by ~1 if the sentence is true, ~0 otherwise. In this framework, further elaborated by Coecke, Sadrzadeh, and Clark (2010), negation is elegantly modeled as a swap matrix. Related approaches have been presented by Preller and Sadrzadeh (2011) and Grefenstette (2013). All this work, however, is purely theoretical, and it is not clear how the proposed models would be implemented in practice. Moreover, treating negation as a swapping operator only makes sense in the abstract scenario of a vector space representing truth values. If vectors of sentences and other expressions are instead distributional in nature (e.g., representing distributions over possible contexts), it is far from clear that swapped vectors would capture linguistic negation. Indeed, Hermann, Grefenstette, and Blunsom (2013) argue that negation should not affect all dimensions in a vector,"
J16-4003,W10-2805,0,0.0245924,"y. We considered two other approaches to composition that we ruled out based on poor development set performance. These were: ignoring negation (e.g., using the same composition function for this/it is a hawk and this/it is not a hawk), and modeling negation as a separate composition step (e.g., deriving this is not a hawk from the application of two composition functions: not(this is(hawk))). We did not attempt to model here and, more importantly, but in the THERE context (there is no X here, but there is a Y). To estimate the function parameters (matrix weights), we followed the approach of Guevara (2010), Baroni and Zamparelli (2010), and Dinu, Pham, and Baroni (2013), as implemented in the DISSECT toolkit.11 Specifically, we extracted from our corpus distributional vectors for sufficiently frequent phrases matching the target template (e.g., this/it is cat), and estimated the corresponding function by optimizing (in the leastsquares sense) the mapping from the vectors of the nouns in these phrases (cat) onto the corpus-extracted phrase vectors. Theoretical and empirical justifications for this method can be found in Baroni, Bernardi, and Zamparelli (2014), or any of the articles mentioned he"
J16-4003,W13-3209,0,0.0677149,"Missing"
J16-4003,W10-3109,0,0.0755906,"Missing"
J16-4003,N15-1097,1,0.843147,". We can conclude, then, that word and phrase vectors co-exist in the same space and, at the same time, that they have, sensibly, different distributions: phrase vectors, because of their shared semantics, tend to be more similar to each other than word vectors are. 5.1.3 Supervised Regression. The distributional vectors of negated items and their alternatives (either noun or composed phrase vectors) were also fed to a supervised regression algorithm, in order to tune similarity to the specific factors that determine felicitous alternativehood. We explored all the neural-network techniques of Kruszewski and Baroni (2015) across a wide range of hyperparameters, but found, on the development set, that it was best to use support vector regression (Drucker et al. 1996) with an RBF kernel. We also exploited the development set to tune the hyperparameters of this model through a broad grid search, conducted separately for noun vs. phrase inputs and IT vs. THERE ratings. We had to decide how to merge the vectors representing a negated item and its candidate alternative in order to feed them together to the supervised regression algorithm. Following recent work that addressed the same problem in the context of superv"
J16-4003,Q15-1016,0,0.0385939,"Missing"
J16-4003,Q13-1015,0,0.0316184,"the broader context (e.g., emphasizing the container components of cup) by means of standard word-meaning-in-context methods (Erk and Pado´ 2008). 6. Discussion and Future Work This special issue addresses the integration of formal and distributional models. This is a challenging task, because the two strands of research rely on different research cultures, methodologies, and simplifying assumptions inevitable in any scientific work. Some studies on finding a unifying perspective have been carried out on both empirical and theoretical fronts (Garrette, Erk, and Mooney 2013; Grefenstette 2013; Lewis and Steedman 2013; McNally and Boleda, to appear). We started our work on this article with the firm conviction that distributional models can do more, inspiring and helping to tackle new tasks that are both theoretically interesting and empirically challenging. The pragmatic contribution of negation is just such a task. Our success at modeling alternatives under conversational negation shows that distributional models are immediately applicable to theoretically interesting problems beyond arguably trivial ones, such as identifying near-synonyms. Cosine in a distributional semantic space turned out to be a ver"
J16-4003,N13-1090,0,0.0977992,"Missing"
J16-4003,C14-1097,0,0.0551762,"Missing"
J16-4003,D12-1130,0,0.0501069,"Missing"
J16-4003,C14-1212,0,0.0178886,"as best to use support vector regression (Drucker et al. 1996) with an RBF kernel. We also exploited the development set to tune the hyperparameters of this model through a broad grid search, conducted separately for noun vs. phrase inputs and IT vs. THERE ratings. We had to decide how to merge the vectors representing a negated item and its candidate alternative in order to feed them together to the supervised regression algorithm. Following recent work that addressed the same problem in the context of supervised entailment detection with distributional vectors (Roller, Erk, and Boleda 2014; Weeds et al. 2014), we explored the options of concatenating and subtracting the input vectors. We picked the second strategy as it consistently produced better development set results across all settings. Note that subtraction, unlike concatenation, explicitly encodes the knowledge that we are comparing two feature vectors living in the same space, and what matters should be how the two vectors differ on a feature-by-feature basis. Furthermore, because supervision should zero in on the information that is not already captured by direct distributional similarity, we explored the possibility to train supervised"
J16-4003,P03-1018,0,0.0966414,"and cold). A survey of the relevant DS literature is provided by Mohammad et al. (2013). The consensus view is that contrasting words tend to occur in similar contexts (Mohammad et al. even 639 Computational Linguistics Volume 42, Number 4 propose a “distributional hypothesis of highly contrasting pairs” stating that highly contrasting pairs occur in similar contexts more often than non-contrasting word pairs). Thus, it is impossible to distinguish them from non-contrasting related words (e.g., synonyms) using standard distributional similarity measures, and ad hoc strategies must be devised. Widdows (2003) presented a pioneering study of explicit negation in DS. The assumption of that work is that negated word meanings should be orthogonal, which is to say that they should not share any common feature. Specifically, Widdows proposes a binary negation operator, NOT(A, B), which projects the vector representing A onto the orthogonal space of the B vector. In logical terms, this can be seen as conjunction with a negated predicate (A ∧ ¬B). The orthogonality assumption makes perfect sense for the information retrieval applications envisioned by Widdows (web NOT internet), but it is too strict to ch"
J16-4003,J13-3004,0,\N,Missing
marelli-etal-2014-sick,D11-1129,0,\N,Missing
marelli-etal-2014-sick,D10-1115,1,\N,Missing
marelli-etal-2014-sick,D12-1110,0,\N,Missing
marelli-etal-2014-sick,D08-1027,0,\N,Missing
marelli-etal-2014-sick,W07-1431,0,\N,Missing
marelli-etal-2014-sick,P08-1028,0,\N,Missing
marelli-etal-2014-sick,S12-1051,0,\N,Missing
marelli-etal-2014-sick,S14-2001,1,\N,Missing
marelli-etal-2014-sick,S13-2005,1,\N,Missing
marelli-etal-2014-sick,S12-1053,1,\N,Missing
N09-3003,W06-3001,0,0.0348781,"Missing"
N09-3003,E89-1039,0,0.412479,"Missing"
N09-3003,2007.sigdial-1.8,1,0.89346,"Missing"
N09-3003,W06-3005,0,0.0708439,"Missing"
N18-1039,D17-1206,0,0.0649381,"Missing"
N18-1039,E17-2026,0,0.0458913,"Missing"
N18-1039,D16-1044,0,0.0878921,"asing complex2 2.1 Related Work Quantities in Language & Vision In recent years, the task of extracting quantity information from visual scenes has been tackled via Visual Question Answering (VQA). Given a real image and a natural language question, a VQA computational model is asked to understand the image, the linguistic query, and their interaction to provide the correct answer. So-called count questions, i.e. ‘How many Xs have the property Y?’, are very frequent and have been shown to be particularly challenging for any model (Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015; Fukui et al., 2016). The difficulty of the task has been further confirmed by the similarly poor performance achieved even on the ‘diagnostic’ datasets, which include synthetic visual scenes depicting geometric shapes (Johnson et al., 2017; Suhr et al., 2017). Using Convolutional Neural Networks (CNN), a number of works in Computer Vision (CV) have proposed specific architectures for counting digits (Segu´ı et al., 2015), people in the crowd (Zhang et al., 2015a), and penguins (Arteta et al., 2016). With a more cognitive flavor, Chattopadhyay et al. (2017) employed a ‘divide-and-conquer’ strategy to split the im"
N18-1039,P16-2038,0,0.0700036,"Missing"
N18-1039,W16-3211,1,0.559819,"Missing"
N18-1039,E17-2054,1,0.775333,"Missing"
N18-1039,P17-2034,0,0.0940541,"uestion, a VQA computational model is asked to understand the image, the linguistic query, and their interaction to provide the correct answer. So-called count questions, i.e. ‘How many Xs have the property Y?’, are very frequent and have been shown to be particularly challenging for any model (Antol et al., 2015; Malinowski et al., 2015; Ren et al., 2015; Fukui et al., 2016). The difficulty of the task has been further confirmed by the similarly poor performance achieved even on the ‘diagnostic’ datasets, which include synthetic visual scenes depicting geometric shapes (Johnson et al., 2017; Suhr et al., 2017). Using Convolutional Neural Networks (CNN), a number of works in Computer Vision (CV) have proposed specific architectures for counting digits (Segu´ı et al., 2015), people in the crowd (Zhang et al., 2015a), and penguins (Arteta et al., 2016). With a more cognitive flavor, Chattopadhyay et al. (2017) employed a ‘divide-and-conquer’ strategy to split the image into subparts and count the objects in each subpart by mimicking the ‘subitizing’ mechanism (i.e. numerosities up to 3-4 can be rapidly and accurately appreciated). Inspired by 1 The dataset and the code can be downloaded from github.co"
N19-1265,N16-1014,0,0.0429711,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,D16-1127,0,0.0416633,"e state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks t"
N19-1265,W18-5045,0,0.0315275,"otated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer questions about an image."
N19-1265,C18-1104,1,0.831206,"Missing"
N19-1265,N15-1020,0,0.0310712,"symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in"
N19-1265,P17-1062,0,0.0143494,"e and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two partic"
N19-1265,W13-4065,0,0.0202117,"sier to train than RL. • A first in-depth study to compare cooperative learning to a state-of-the-art RL system. Our study shows that the linguistic skills of the models differ dramatically, despite approaching comparable task success levels. This underlines the importance of linguistic analysis to complement solely numeric evaluation. 2 Related Work Task-oriented dialogue systems The conventional architecture of task-oriented dialogue systems includes a pipeline of components, and the task of tracking the dialogue state is typically modelled as a partially-observable Markov decision process (Williams et al., 2013; Young et al., 2013; Kim et al., 2014) that operates on a symbolic dialogue state consisting of predefined variables. The use of symbolic representations to characterise the state of the dialogue has some advantages (e.g., ease of interfacing with knowledge bases), but it has also some key disadvantages: the variables to be tracked have to be defined in advance and the system needs to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neur"
N19-1265,L16-1019,1,0.80446,"Missing"
N19-1265,W16-3601,0,0.0150046,"to be trained on data annotated with explicit state configurations. 2 Code and supplementary material are available at https://vista-unitn-uva.github.io. Given these limitations, there has been a shift towards neural end-to-end systems that learn their own representations. Early works focus on nongoal-oriented chatbots (Vinyals and Le, 2015; Sordoni et al., 2015; Serban et al., 2016; Li et al., 2016a,b). Bordes et al. (2017) propose a memory network to adapt an end-to-end system to taskoriented dialogue. Recent works combine conventional symbolic with neural approaches (Williams et al., 2017; Zhao and Eskenazi, 2016; Rastogi et al., 2018), but all focus on language-only dialogue. We propose a visually grounded taskoriented end-to-end dialogue system which, while maintaining the crucial aspect of the interaction of the various modules at play in a conversational agent, grounds them through vision. Visual dialogue agents In recent years, researchers in computer vision have proposed tasks that combine visual processing with dialogue interaction. Pertinent datasets created by Das et al. (2017a) and de Vries et al. (2017) include VisDial and GuessWhat?!, respectively, where two participants ask and answer que"
P13-2010,D10-1115,1,0.949648,"u + βv, where α and β are two scalars. Finally, in the dilation model, the output vector is obtained by first decomposing one of the input vectors, say v, into a vector parallel to u and an orthogonal vector. Following this, the parallel vector is dilated by a factor λ before re-combining. This results in: p = (λ − 1)hu, viu + hu, uiv. A more general form of the additive model (fulladd) has been proposed by Guevara (2010) (see also Zanzotto et al. (2010)). In this approach, the two vectors to be added are pre-multiplied by weight matrices estimated from corpus-extracted examples: p = Au + Bv. Baroni and Zamparelli (2010) and Coecke et al. (2010) take inspiration from formal semantics to characterize composition in terms of function application. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 4 Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex arch"
P13-2010,E12-1004,1,0.876171,"cation. The former model adjective-noun phrases by treating the adjective as a function from nouns onto modified nouns. Given that linear functions can be expressed by matrices and their application by matrix-by-vector multiplication, a 4 Other approaches to composition in DSMs have been recently proposed by Socher et al. (2012) and Turney (2012). We leave their empirical evaluation on DPs to further work, in the first case because it is not trivial to adapt their complex architecture to our setting; in the other because it is not clear how Turney would extend his approach to represent DPs. 2 Baroni et al. (2012), like us, study determiner phrases with distributional methods, but they do not model them compositionally. 3 Dataset and code available from clic.cimec. unitn.it/composes. 54 noun duel homeless polygamy opulence target DP two opponents no home several wives too many goods same-N foil 1 various opponents too few homes most wives some goods same-N foil 2 three opponents one home fewer wives no goods same-D foil two engineers no incision several negotiators too many abductions D foil two no several too many N foil opponents home wives goods Table 1: Examples from the noun-DP relatedness benchma"
P13-2010,C10-1142,0,0.412805,"e make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 functor (such as the adjective) is represented by a matrix U to be multiplied with the argument vector v (e.g., the noun vector): p = Uv. Adjective matrices are estimated from corpus-extracted examples of noun vectors and corresponding output adjective-noun phrase vectors, similarly to Guevara’s approach.4 3 The noun-DP relatedness benchmark Paraphrasing a single word with a phrase is a natural task for models of compositionality (Turney, 2012; Zanzotto et al., 2010) and determiners sometimes play a crucial role in defining the meaning of a noun. For example a trilogy is composed of three works, an assemblage includes several things and an orchestra is made of many musicians. These examples are particularly interesting, since they point to a “conceptual” use of determiners, as components of the stable and generic meaning of a content word (as opposed to situation-dependent deictic and anaphoric usages): for these determiners the boundary between content and grammatical word is somewhat blurred, and they thus provide a good entry point for testing DSM repr"
P13-2010,P12-1015,1,0.766679,"eparate words, since number clearly plays an important role in DP semantics. Finally, for each of the target determiners we added to the space the 2K most frequent DPs containing that determiner and a target noun. Co-occurrence statistics were collected from the concatenation of ukWaC, a mid-2009 dump of the English Wikipedia and the British National Corpus,5 with a total of 2.8 billion tokens. We use a bag-of-words approach, counting co-occurrence with all context words in the same sentence with a target item. We tuned a number of parameters on the independent MEN word-relatedness benchmark (Bruni et al., 2012). This led us to pick the top 20K most frequent content word lemmas as context items, Pointwise Mutual Information as weighting scheme, and dimensionality reduction by Non-negative Matrix Factorization. Except for the parameter-free mult method, parameters of the composition methods are estimated by minimizing the average Euclidean distance between the model-generated and corpusextracted vectors of the 20K DPs we consider.6 For the lexfunc model, we assume that the determiner is the functor and the noun is the argument, accuracy 39.3 34.7 34.1 31.8 23.1 method noun random mult determiner accur"
P13-2010,D11-1129,0,0.267774,"Missing"
P13-2010,W10-2805,0,0.347546,"cus almost entirely on phrases made of two or more content words (e.g., adjective-noun or verb-noun combinations) and completely ignore grammatical words, to the point that even the test set of transitive sentences proposed by Grefenstette and Sadrzadeh (2011) contains only 1 Some linguists refer to what we call DPs as noun phrases or NPs. We say DPs simply to emphasize our focus on determiners. 53 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 53–57, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics elli (2010), Guevara (2010) and Mitchell and Lapata (2010). Thus, we can straightforwardly extend the methods already proposed for adjective-noun phrases to DPs. We introduce a new task, a similarity-based challenge, where we consider nouns that are strongly conceptually related to certain DPs and test whether cDSMs can pick the most appropriate related DP (e.g., monarchy is more related to one ruler than many rulers).2 We make our new dataset publicly available, and we hope that it will stimulate further work on the distributional semantics of grammatical elements.3 2 functor (such as the adjective) is represented by a"
P13-2010,P08-1028,0,0.700392,"“the table shows no results” since the two sentences contain the same content words, or that “to kill many rats” and “to kill few rats” are equally good paraphrases of “to exterminate rats”. We focus here on how cDSMs handle determiners and the phrases they form with nouns (determiner phrases, or DPs).1 While determiners are only a subset of grammatical words, they are a large and important subset, constituting the natural stepping stone towards sentential distributional semantics: Compositional methods have already been successfully applied to simple noun-verb and noun-verb-noun structures (Mitchell and Lapata, 2008; Grefenstette and Sadrzadeh, 2011), and determiners are just what is missing to turn these skeletal constructions into full-fledged sentences. Moreover, determiner-noun phrases are, in superficial syntactic terms, similar to the adjective-noun phrases that have already been extensively studied from a cDSM perspective by Baroni and ZamparDistributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic mo"
P13-2010,D09-1045,0,0.0588046,"Missing"
P13-2010,D12-1110,0,\N,Missing
P16-1144,D15-1075,0,0.0561594,"n about one fifth of the cases, the annotators could not guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sen"
P16-1144,P82-1020,0,0.82838,"Missing"
P16-1144,C14-3002,0,0.0424602,"Missing"
P16-1144,P15-1007,0,0.0212447,"Missing"
P16-1144,D13-1020,0,0.058654,"guess the word even when the broader context was given. Thus, only a small portion of the CBT passages are really probing the model’s ability to understand the broader context, which is instead the focus of LAMBADA. The idea of a book excerpt completion task was originally introduced in the MSRCC dataset (Zweig and Burges, 2011). However, the latter limited context to single sentences, not attempting to measure broader passage understanding. Of course, text understanding can be tested through other tasks, including entailment detection (Bowman et al., 2015), answering questions about a text (Richardson et al., 2013; Weston et al., 2015) and measuring inter-clause coherence (Yin and Sch¨utze, 2015). While different tasks can provide complementary insights into the models’ abilities, we find word prediction particularly attractive because of its naturalness (it’s easy to norm the data with non-expert humans) and simplicity. Models just need to be trained to predict the most likely word given the previous context, following the classic language modeling paradigm, which is a much simpler setup than the one required, say, to determine whether two sentences entail each other. Moreover, models can have access"
P16-1144,N15-1020,0,0.0259137,"ately, the system responses are appropriate for the respective questions. However, when taken together, they are incoherent. The system behaviour is somewhat parrot-like. It can locally produce perfectly sensible language fragments, but it fails to take the meaning of the broader discourse context into account. Much research effort has consequently focused on designing systems able to keep information from the broader context into memory, and possibly even perform simple forms of reasoning about it (Hermann et al., 2015; Hochreiter and Schmidhuber, 1997; Ji et al., 2015; Mikolov et al., 2015; Sordoni et al., 2015; Sukhbaatar et al., 2015; Wang and Cho, 2015, a.o.). In this paper, we introduce the LAMBADA dataset (LAnguage Modeling Broadened to Account for Discourse Aspects). LAMBADA proposes a word prediction task where the target item is difficult to guess (for English speakers) when only the sentence in which it appears is available, but becomes easy when a broader context is presented. Consider Example (1) in Figure 1. The sentence Do you honestly think that I would want you to have a ? has a multitude of possible continuations, but the broad context clearly indicates that the missing word is misca"
P17-1024,D16-1203,0,0.0848112,"utilising language cues is not enough to model FOIL-COCO and that it challenges the state-of-the-art by requiring a fine-grained understanding of the relation between text and image. 1 Figure 1: Is the caption correct or foil (T1)? If it is foil, where is the mistake (T2) and which is the word to correct the foil one (T3)? models are actually learning. There is an emerging feeling in the community that the VQA task should be revisited, especially as many current dataset can be handled by ‘blind’ models which use language input only, or by simple concatenation of language and vision features (Agrawal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016a). In IC too, Hodosh and Hockenmaier (2016) showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to distinguish between a correct and incorrect caption. Such results indicate that in current datasets, language provides priors that make LaVi models successful without truly understanding and integrating language and vision. But problems do not stop at biases. Johnson et al. (2016) also point out that current data ‘conflate multiple sources of error, making it hard"
P17-1024,W16-3203,0,0.560135,"he-art by requiring a fine-grained understanding of the relation between text and image. 1 Figure 1: Is the caption correct or foil (T1)? If it is foil, where is the mistake (T2) and which is the word to correct the foil one (T3)? models are actually learning. There is an emerging feeling in the community that the VQA task should be revisited, especially as many current dataset can be handled by ‘blind’ models which use language input only, or by simple concatenation of language and vision features (Agrawal et al., 2016; Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2016a). In IC too, Hodosh and Hockenmaier (2016) showed that, contrarily to what prior research had suggested, the task is far from been solved, since IC models are not able to distinguish between a correct and incorrect caption. Such results indicate that in current datasets, language provides priors that make LaVi models successful without truly understanding and integrating language and vision. But problems do not stop at biases. Johnson et al. (2016) also point out that current data ‘conflate multiple sources of error, making it hard to pinpoint model weaknesses’, thus highlighting the need for diagnostic datasets. Thirdly, existing IC"
P17-1024,P14-2074,0,0.0297077,"tween a correct and incorrect caption. Such results indicate that in current datasets, language provides priors that make LaVi models successful without truly understanding and integrating language and vision. But problems do not stop at biases. Johnson et al. (2016) also point out that current data ‘conflate multiple sources of error, making it hard to pinpoint model weaknesses’, thus highlighting the need for diagnostic datasets. Thirdly, existing IC evaluation metrics are sensitive to n-gram overlap and there is a need for measures that better simulate human judgments (Hodosh et al., 2013; Elliott and Keller, 2014; Anderson et al., 2016). Our paper tackles the identified issues by proposing an automatic method for creating a Introduction Most human language understanding is grounded in perception. There is thus growing interest in combining information from language and vision in the NLP and AI communities. So far, the primary testbeds of Language and Vision (LaVi) models have been ‘Visual Question Answering’ (VQA) (e.g. Antol et al. (2015); Malinowski and Fritz (2014); Malinowski et al. (2015); Gao et al. (2015); Ren et al. (2015)) and ‘Image Captioning’ (IC) (e.g. Hodosh et al. (2013); Fang et al. (2"
P17-1024,Q14-1006,0,0.0461178,"Missing"
P18-2019,P16-1144,1,0.802829,"Missing"
P18-2019,E17-2068,0,0.0447499,"Missing"
P18-2019,N16-1174,0,0.0122522,"whether humans and the models make similar errors, we look into the distribution of responses in 3-Sent (val), which is the most comparable setting with respect to accuracy. Table 3 reports responses by humans (top) and AttCon-LSTM (bottom). Human errors generally involve quantifiers that display a similar magnitude as the correct one. To illustrate, ‘some’ is chosen in place of ‘a few’, and ‘most’ in place of either ‘almost all’ or ‘more than half’. A similar pattern is observed in the model’s predictions, AttCon-LSTM LSTM augmented with an attention mechanism using a learned context vector (Yang et al., 2016). LSTM states are weighted by cosine similarity to the context vector. 5 Results Table 2 reports the accuracy of all models and humans in both conditions. We have three main results. (1) Broader context helps humans to perform the task, but hurts model performance. This can be seen by comparing the 4-point increase of human accuracy from 1-Sent (0.22) to 3-Sent (0.26) with the generally worse performance of all models (e.g. AttCon-LSTM, from 0.34 to 0.27 chance BoW-conc BoW-sum fastText CNN LSTM bi-LSTM Att-LSTM AttCon-LSTM Humans 1-Sent val test 0.111 0.111 0.270 0.238 0.308 0.290 0.305 0.271"
P18-2019,P14-1023,0,\N,Missing
P19-1350,P15-2123,0,0.0320022,"those learned by the model trained on Wh-q independently: Y/N questions result in a big hard-to-distinguish “blob”, and are confused with Wh-q about size, as visible in Fig. 2 and the confusion matrix analysis (in the SM). In contrast, Rehearsal remembers how to distinguish among all kinds of Wh-q and between Wh-q and Y/N-q. The error analysis confirms that the model hardly makes any mistakes related to task confusion. However, despite the higher performance than EWC, Rehearsal is still not able to discern well between different kinds of Y/N-q. 5 Related Work Early work on life-long learning (Chen et al., 2015; Mitchell et al., 2015) is related to ours, but typically concerns a single task (e.g., relation extraction). Lee (2017) aims to transfer conversational skills from a synthetic domain to a customer-specific application in dialogue agents, while Yogatama et al. (2019) show that current models for different NLP tasks are not able to properly reuse previously learned knowledge. In general, continual learning has been mostly studied in computer vision. To the best of our knowledge, little has been done on catastrophic forgetting in VQA. A study on forgetting in the context of VQA and closest to o"
R13-1061,D08-1094,0,0.0307605,"ic approach is based on the idea that the meaning of a word relies heavily on its context. Hence, the meaning of a word can be represented as a vector of its co-occurrence frequency with the neighbouring words. There have been several works that explore ways to improve the representation of word meaning by incorporating syntactic information in the context vector, dependency relations between words being the commonly used syntactic features. Dependencybased Distributional Semantic Models (DSMs) have been tested against several tasks and shown to be among the best performing word space models (Erk and Pado, 2008; Cruys, 2008; Baroni and Lenci, 2009; Baroni and Lenci, 2010). In this paper we investigate an alternative view on the syntactic features that can be used to enrich the context vector, namely Combinatory Categorial Grammar (CCG) categories, which provide a transparent relation between syntactic category and semantic type of a linguistic expression. 2 “Supertags” for Distributional Semantic Models We propose to investigate the role of constituent structures and features encoding tense information, by building a distributional model with dimensions tagged by “supertags”, namely by Combinatory C"
R13-1061,W09-0201,0,0.0212381,"that the meaning of a word relies heavily on its context. Hence, the meaning of a word can be represented as a vector of its co-occurrence frequency with the neighbouring words. There have been several works that explore ways to improve the representation of word meaning by incorporating syntactic information in the context vector, dependency relations between words being the commonly used syntactic features. Dependencybased Distributional Semantic Models (DSMs) have been tested against several tasks and shown to be among the best performing word space models (Erk and Pado, 2008; Cruys, 2008; Baroni and Lenci, 2009; Baroni and Lenci, 2010). In this paper we investigate an alternative view on the syntactic features that can be used to enrich the context vector, namely Combinatory Categorial Grammar (CCG) categories, which provide a transparent relation between syntactic category and semantic type of a linguistic expression. 2 “Supertags” for Distributional Semantic Models We propose to investigate the role of constituent structures and features encoding tense information, by building a distributional model with dimensions tagged by “supertags”, namely by Combinatory Categorial Grammar (CCG) categories. C"
R13-1061,J01-3003,0,0.283274,"ed by the classification originally proposed in Levin (1993) and further revised in Vinson and Vigliocco (2008), the organizers of the ESSLLI 2008 Workshop have proposed a data set of 45 verbs classified into 9 semantic classes (communication, mentalState, motionManner, motionDirection, changeLocation, bodySense, bodyAction, exchange, and changeState), further grouped into 5 classes: communication and mentalState into cognition; motionManner, motionDirection, and changeLocation into motion; bodySense and bodyAction into body; exchange; and changeState. Verbs (argument structure distinctions): Merlo and Stevenson (2001) consider thematic relations to be crucial for verb classification, and hence propose a classification of verbs that is coarser than the one proposed by Levin and considered to be appropriate for numerous language engineering tasks. In particular, the relevant features to be considered are causativity, animacy, the passive vs. active voice, and the use of pastparticiple vs. simple past. They consider the argument-structure, which is the thematic roles assigned by the verbs, to be the discriminative main property. To this end, three classes of verbs are defined: unergative, unaccusative, and ob"
R13-1061,J10-4006,0,0.0318285,"rd relies heavily on its context. Hence, the meaning of a word can be represented as a vector of its co-occurrence frequency with the neighbouring words. There have been several works that explore ways to improve the representation of word meaning by incorporating syntactic information in the context vector, dependency relations between words being the commonly used syntactic features. Dependencybased Distributional Semantic Models (DSMs) have been tested against several tasks and shown to be among the best performing word space models (Erk and Pado, 2008; Cruys, 2008; Baroni and Lenci, 2009; Baroni and Lenci, 2010). In this paper we investigate an alternative view on the syntactic features that can be used to enrich the context vector, namely Combinatory Categorial Grammar (CCG) categories, which provide a transparent relation between syntactic category and semantic type of a linguistic expression. 2 “Supertags” for Distributional Semantic Models We propose to investigate the role of constituent structures and features encoding tense information, by building a distributional model with dimensions tagged by “supertags”, namely by Combinatory Categorial Grammar (CCG) categories. CCG is the categorial gram"
R13-1061,J07-4004,0,0.0158545,"pression. 2 “Supertags” for Distributional Semantic Models We propose to investigate the role of constituent structures and features encoding tense information, by building a distributional model with dimensions tagged by “supertags”, namely by Combinatory Categorial Grammar (CCG) categories. CCG is the categorial grammar version studied by the Edinburgh research group led by Steedman (2000), which has been used to theoretically analyse several linguistic phenomena. It has been used for building a CCGbank (Hockenmaier, 2003) and has been implemented into an efficient and wide coverage parser (Clark and Curran, 2007)1 . Below we will briefly describe the CCG categories, without going into details about the grammar. 1 However, for our experiments we used the revised version presented in Honnibal et al. (2007) 467 Proceedings of Recent Advances in Natural Language Processing, pages 467–474, Hissar, Bulgaria, 7-13 September 2013. CCG language consists of atomic and complex categories where the latter are built out of the former by means of the directional implication operators OutputInput and Output/Input. For instance, an intransitive verb is assigned the category SNP , which means that it wants an NP arg"
R13-1061,P10-1022,0,\N,Missing
S14-2001,D10-1115,1,0.672361,"ss and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentenc"
S14-2001,S12-1051,0,0.383512,"Missing"
S14-2001,S14-2013,0,0.0692411,"Missing"
S14-2001,S14-2141,0,0.0602478,"78.5 S UIO-Lien run1 77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compo"
S14-2001,S14-2125,0,0.0284327,"NLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, tho"
S14-2001,S14-2024,0,0.0394666,"S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully composi"
S14-2001,marelli-etal-2014-sick,1,0.808802,"re also welcome. Besides being of intrinsic interest, the latter systems’ performance will serve to situate CDSM performance within the broader landscape of computational semantics. 3 Data Set Creation The SICK Data Set The SICK data set, consisting of about 10,000 English sentence pairs annotated for relatedness in meaning and entailment, was used to evaluate the systems participating in the task. The data set creation methodology is outlined in the following subsections, while all the details about data generation and annotation, quality control, and interannotator agreement can be found in Marelli et al. (2014). 1 http://nlp.cs.illinois.edu/HockenmaierGroup/data.html http://www.cs.york.ac.uk/semeval2012/task6/index.php?id=data 2 2 Relatedness score Example 1.6 A: “A man is jumping into an empty pool” B: “There is no biker jumping in the air” 2.9 A: “Two children are lying in the snow and are making snow angels” B: “Two angels are making snow on the lying children” 3.6 A: “The young boys are playing outdoors and the man is smiling nearby” B: “There is no boy playing outdoors and there is no man smiling” 4.9 A: “A person in a black jacket is doing tricks on a motorbike” B: “A man in a black jacket is"
S14-2001,S14-2085,0,0.0831811,"Missing"
S14-2001,P08-1028,0,0.15047,"ectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the"
S14-2001,S14-2114,0,0.327729,"Missing"
S14-2001,S14-2093,0,0.0223856,"results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by assigning meanings to i"
S14-2001,D12-1110,0,0.111915,"btasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs). 1 Introduction Distributional Semantic Models (DSMs) approximate the meaning of words with vectors summarizing their patterns of co-occurrence in corpora. Recently, several compositional extensions of DSMs (CDSMs) have been proposed, with the purpose of representing the meaning of phrases and sentences by composing the distributional representations of the words they contain (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Mitchell and Lapata, 2010; Socher et al., 2012). Despite the ever increasing interest in the field, the development of adequate benchmarks for CDSMs, especially at the sentence level, is still lagging. Existing data sets, such as those introduced by Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011), are limited to a few hundred instances of very short sentences with a fixed structure. In the last ten years, several large 2 The Task The Task involved two subtasks. (i) Relatedness: predicting the degree of semantic similarity between two sentences, and (ii) Entailment: detecting the entailment relation holding between them Thi"
S14-2001,S14-2047,0,0.0241507,"S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionall"
S14-2001,D11-1129,0,0.0154758,"Missing"
S14-2001,S14-2044,0,0.216165,"Missing"
S14-2001,S14-2139,0,0.0534159,"Missing"
S14-2001,S14-2131,0,0.156692,"Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositional features, and divide the former into ‘fully compositional’ (systems that compositionally computed the meaning of the full sentences, though not necessarily by"
S14-2001,S14-2055,0,0.250575,"77.1 77.0 FBK-TR run3 P 75.4 StanfordNLP run5 S 74.5 UTexas run1 P/S 73.2 Yamraj run1 70.7 asjai run5 S 69.8 haLF run2 S 69.4 RTM-DCU run1 UANLPCourse run2 67.2 S 48.7 Table 6: Primary run results for the entailment subtask. The table also shows whether a system exploits composition information at either the phrase (P) or sentence (S) level. Approaches A summary of the approaches used by the systems to address the task is presented in Table 8. In the table, systems in bold are those for which the authors submitted a paper (Ferrone and Zanzotto, 2014; Bjerva et al., 2014; Beltagy et al., 2014; Lai and Hockenmaier, 2014; Alves et al., 2014; Le´on et al., 2014; Bestgen, 2014; Zhao et al., 2014; Vo et al., 2014; Bic¸ici and Way, 2014; Lien and Kouylekov, 2014; Jimenez et al., 2014; Proisl and Evert, 2014; Gupta et al., 2014). For the others, we used the brief description sent with the system’s results, double-checking the information with the authors. In the table, “E” and “R” refer to the entailment and relatedness task respectively, and “B” to both. Almost all systems combine several kinds of features. To highlight the role played by composition, we draw a distinction between compositional and non-compositio"
S14-2001,S14-2021,0,0.0355515,"Missing"
S14-2001,W07-1401,0,\N,Missing
U05-1025,bosco-etal-2000-building,0,0.212624,"et of types. In the following sections we will refer to such pairs as Potential PoS (PPoS); (iii) finally, we prune the obtained inclusion chart by highlighting those paths that relate pairs which are significantly similar, where the similarity is measured in terms of frequency of types and words. The pruning results in a forest of trees whose leaves form sets identifying the induced PoS tags. Figure 1 shows a flow chart which summarizes the three phases of our algorithm. 3.1 Dependency Structures Our dependency structures are derived from a sub-treebank of TUT, The Turin University Treebank (Bosco et al., 2000; Bosco, 2003). The treebank currently includes 1500 sentences organized in different sub-corpora from which we converted 441 dependency trees, maintaining only the basic syntactic information required for this study. More specifically, we mantained information on Head-Dependent relations by distinguishing each dependent either as an Argument or as an Adjunct. Moreover, words are marked as N (nouns), V (verbs) or X (all others) according to the results obtained in (Tamburini et al., 2002). We use &lt; &gt; to mark Head-Argument relation and  and  to mark Head-Adjunct relation where the arrows poin"
U05-1025,W00-0717,0,0.292258,"notated corpus. Our aim is to automatically derive an empirically founded PoS classification making no a priori assumptions about the PoS classes to be distinguished. Early approaches to this problem were based on the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts. There are a number of studies based on this hypothesis in the fields of both computational linguistics and cognitive science aiming at building automatic or semiautomatic procedures for clustering words (Brill and Marcus, 1992; Pereira et al., 1993; Sch¨ utze, 1993; Clark, 2000; Redington et al., 1998).These papers examine the distributional behaviour of some target words by comparing the lexical distribution of their respective collocates and by using quantitative measures of distributional similarity. The main drawback of these techniques is the Proceedings of the Australasian Language Technology Workshop 2005, pages 176–183, Sydney, Australia, December 2005. 176 limited context of analysis. Information is collected from a restricted context, of for instance 3 words, which can conceal syntactic dependencies longer than the context interval. Our approach to solve t"
U05-1025,P93-1024,0,0.286433,"of conclusions one can draw from the annotated corpus. Our aim is to automatically derive an empirically founded PoS classification making no a priori assumptions about the PoS classes to be distinguished. Early approaches to this problem were based on the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts. There are a number of studies based on this hypothesis in the fields of both computational linguistics and cognitive science aiming at building automatic or semiautomatic procedures for clustering words (Brill and Marcus, 1992; Pereira et al., 1993; Sch¨ utze, 1993; Clark, 2000; Redington et al., 1998).These papers examine the distributional behaviour of some target words by comparing the lexical distribution of their respective collocates and by using quantitative measures of distributional similarity. The main drawback of these techniques is the Proceedings of the Australasian Language Technology Workshop 2005, pages 176–183, Sydney, Australia, December 2005. 176 limited context of analysis. Information is collected from a restricted context, of for instance 3 words, which can conceal syntactic dependencies longer than the context int"
U05-1025,P93-1034,0,0.569577,"from the annotated corpus. Our aim is to automatically derive an empirically founded PoS classification making no a priori assumptions about the PoS classes to be distinguished. Early approaches to this problem were based on the hypothesis that if two words are syntactically and semantically different, they will appear in different contexts. There are a number of studies based on this hypothesis in the fields of both computational linguistics and cognitive science aiming at building automatic or semiautomatic procedures for clustering words (Brill and Marcus, 1992; Pereira et al., 1993; Sch¨ utze, 1993; Clark, 2000; Redington et al., 1998).These papers examine the distributional behaviour of some target words by comparing the lexical distribution of their respective collocates and by using quantitative measures of distributional similarity. The main drawback of these techniques is the Proceedings of the Australasian Language Technology Workshop 2005, pages 176–183, Sydney, Australia, December 2005. 176 limited context of analysis. Information is collected from a restricted context, of for instance 3 words, which can conceal syntactic dependencies longer than the context interval. Our approa"
U05-1025,J99-2004,0,\N,Missing
W08-1604,2007.sigdial-1.8,1,0.858797,"ous dialogue and a FU Q. Salient transitions between two consecutive questions are defined in (Chai and Jin, 2004) under the name of “informational transitions”. The authors aim to describe how the topic within a di3 3.1 Preliminary Observations What “things” do users focus on? For all forthcoming examples of dialogues, questions and answers, we will base our discussion on an actual prototype IQA system we have been developing; this system is supposed to provide library-related information in a university library setting. In the dialogues collected via an earlier Wizardof-Oz (WoZ) experiment (Kirschner and Bernardi, 2007), we observed that users either seem to have some specific library-related task (action, e.g. “search”) in mind that they want to ask the system about, or they want to retrieve information on some specific entity (e.g., “guided tour”). People tend to use FU Qs to “zoom into” (i.e., find out more about) either of the two. In line with this analysis, the focus of a FU Q might move from the task (action/verb) to the entities that are possible fillers of the verb’s semantic argument slots. Based on these simple observations, we propose a task/entity-based model for describing the focus of question"
W08-1604,J05-1004,0,0.0192626,"ind out more about) either of the two. In line with this analysis, the focus of a FU Q might move from the task (action/verb) to the entities that are possible fillers of the verb’s semantic argument slots. Based on these simple observations, we propose a task/entity-based model for describing the focus of questions and answers in our IQA setting. Our theory of focus structure is related to the task-based theory of (Grosz, 1977). Tasks correspond to verbs, which are inherently connected to an argument structure defining the verb’s semantic roles. By consulting lexical resources like PropBank (Palmer et al., 2005), we can use existing knowledge about possible semantic arguments of 1 This definition is in line with how focus has been used in Computational Linguistics and Artificial Intelligence (hence, “AI focus”), originating in the work of Grosz and Sidner on discourse entity salience. We follow Lecœuche et al. in that focused elements could also be actions/tasks. We see the most salient focused element (corresponding to the “Backwardlooking center“ in Centering Theory) as the topic of the utterance. Accordingly, in the following we will use the terms focus and topic interchangeably; cf. (Vallduvi, 19"
W08-1604,W06-3001,0,0.122269,"Missing"
W10-4359,bernardi-etal-2010-context,1,0.857027,"FU Qs. We have used Logistic Regression Models (LRMs), both for learning which aspects of dialogue structure are relevant to answering FU Qs, and for comparing the accuracy with which the resulting IQA systems can correctly answer these questions. Unlike much of the related research in IQA, which used artificial collections of user questions, our work has been based on real user-system dialogues we collected via a chatbot-inspired help-desk IQA system deployed on the web site of our University Library. Previously, our experiments used a selection of shallow (Kirschner et al., 2009) and deep (Bernardi et al., 2010) features, all of which describe specific relations holding between two utterances (i.e., user questions or system answers). In this paper we present additional features derived from automatically collected dialogue meta-data from our chatbot’s dialogue management component. We use Principal Component Analysis (PCA) to combine the benefits of all these information sources, as opposed to using only certain hand-selected features as in our previous work. The main goal of this paper is to learn from data a new typology of FU Qs; we then compare it to an existing typology based on hand-annotated F"
W10-4359,P87-1022,0,0.385344,"Missing"
W10-4359,W04-2504,0,0.565626,"Section 3 then introduces our collection of realistic IQA dialogues which we will use in all our experiments; the section includes descriptions of meta information in the form of dialogue management features and post-hoc human annotations. In Section 4 we introduce our experimental framework, based on inter-utterance features and LRMs. Our experimental results are presented in Section 5, which is followed by our conclusions. 2 Related work Much of previous work on dialogue processing in the domain of contextual or interactive Question Answering (QA) (Bertomeu, 2008; van Schooten et al., 2009; Chai and Jin, 2004; Yang et al., 2006) has been based on (semi-)artificially devised sets of context questions. However, the importance of evaluating IQA against real user questions and the need to consider preceding system answers has already been emphasized (Bernardi and Kirschner, 2010). The corpus of dialogues we deal with consists of real logs in which actual library users were conversing (by typing) with a chat-bot to obtain information in a help-desk scenario. (Yang et al., 2006) showed that shallow similarity features between a FU Q and the preceding utterances are useful to determine whether the FU Q i"
W10-4359,W06-3005,0,0.0507553,"Missing"
W10-4359,W06-3001,0,\N,Missing
W10-4359,J95-2003,0,\N,Missing
W12-3304,W11-4103,1,0.385496,"information (Shen et al., 2006a), (Broder et al., 2007). This study will focus on QC in art, culture and history domain, using the Bridgeman art library1 , although our framework is general enough to be used in different domains. Manually creating a training 1 http://www.bridgemanart.com/ set of queries to build a classifier in a specific domain is very time-consuming. In this study, we will describe our method of automatically creating a training set based on the click-through links and how we build an SVM (Support Vector Machine) classifier with the integration of enriched information. In (Le et al., 2011), it has been shown that click-through information and topic models are useful for query enrichment when the ultimate goal is query classification. We will follow this enrichment step, but integrate this information into a SVM classifier instead of using matching and ranking between queries and categories as in (Le et al., 2011). The purpose of this paper is to determine (1) whether the query enrichment with click-though information and hidden topics is useful for a machine learning query classification system using SVM; and (2) whether integrating this enriched information into a machine lear"
W13-0603,E12-1004,1,0.838773,"ntence similarity considers word order seriously. We do not exclude that in real-world tasks systems which ignore word order may still attain satisfactory results (as the results of Blacoe and Lapata 2012 suggest), but this will not be evidence of having truly captured compositionality. Moreover, a hidden conclusion (or, better, assumption!) of the evaluations conducted so far on CDSMs seems to be that grammatical words, in particular determiners, play no role in sentence meaning and hence sentence similarity and paraphrase detection. A first study on this class of words has been presented in Baroni et al. (2012) where it is shown that DSMs can indeed capture determiner meaning and their role in the entailment between quantifier phrases. The data sets used in Mitchell and Lapata (2008) and Grefenstette and Sadrzadeh (2011b) focus on verb meaning and its sense disambiguation within context, and consider sentences where determiners are just place-holders to simply guarantee grammaticality, but do not play any role neither in the human judgments nor in the model evaluation – in which they are simply ignored. Similarly, Blacoe and Lapata (2012) evaluate the compositional models on full sentences but again"
W13-0603,D12-1050,0,0.463977,"rb were extracted from the BNC and sentences in simple past form (with articles if necessary) were generated. For example, starting from met, the two sentences “The system met the criterion” and “The child met the house” were generated. For each sentence, two new versions were created by replacing the verb with two synonyms representing two verb senses (e.g., “The system visited the criterion” and “The system satisfied the criterion”). The data set consists of 200 pairs of sentences annotated with human similarity judgments. Large-scale full sentence paraphrasing data Socher et al. (2011) and Blacoe and Lapata (2012) tackle the challenging task of evaluating CDSMs against large-scale full sentence data. They use the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) as data set. The corpus consists of 5800 pairs of sentences extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The evaluation experiments conducted against these data sets seem to support the following conclusions: • “the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations),"
W13-0603,P11-1020,0,0.0371892,"g whether a DSM is “cheating” in accomplishing this task, or, better, if it does detect paraphrases but does not properly capture compositionality. Paraphrase set The sentences are grouped into sets of paraphrases. Some groups are rather similar to each other (for instance they are about someone playing some instrument) though they clearly describe a different situation (the player is a man vs. a woman or the instrument is a guitar vs. a violin), as it happens for the sentences in Group 1 and Group 2 listed in Table 1. We took as starting point the Microsoft Research Video Description Corpus (Chen and Dolan, 2011) considering only those sentences that could be simplified to fit in the CFG described above. We have obtained 20 groups of sentences. Groups which were left with just a few sentences after grammar-based trimming have been extended adding sentences with the nouns modified by an attributive adjective (chosen so that it would not distort the meaning of the sentence, e.g. we have added tall as modifier of person in “A tall person makes a cake”, if there is no original sentence in the group that would describe the person differently), or adding sentences with a determiner similar to the one used i"
W13-0603,C04-1051,0,0.052391,"system met the criterion” and “The child met the house” were generated. For each sentence, two new versions were created by replacing the verb with two synonyms representing two verb senses (e.g., “The system visited the criterion” and “The system satisfied the criterion”). The data set consists of 200 pairs of sentences annotated with human similarity judgments. Large-scale full sentence paraphrasing data Socher et al. (2011) and Blacoe and Lapata (2012) tackle the challenging task of evaluating CDSMs against large-scale full sentence data. They use the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) as data set. The corpus consists of 5800 pairs of sentences extracted from news sources on the web, along with human annotations indicating whether each pair captures a paraphrase/semantic equivalence relationship. The evaluation experiments conducted against these data sets seem to support the following conclusions: • “the model should be sensitive to the order of the words in a phrase (for composition) or a word pair (for relations), when the order affects the meaning.” (Turney, 2012) • “experimental results demonstrate that the multiplicative models are superior to the additive alternative"
W13-0603,W13-0112,0,0.748012,"e seriously, we built a data set of intransitive and transitive sentences in which word order and determiners have the chance to prove their worth in sentence similarity and paraphrase detection. 2 Compositional Distributional Semantic Models In this section we won’t present a proper overview of CDSMs, but focus only on those models we will be testing in our experiments, namely the multiplicative and additive models of Mitchell and Lapata (2008, 2009, 2010), and the lexical function model that represents the work carried out by Baroni and Zamparelli (2010), Grefenstette and Sadrzadeh (2011b), Grefenstette et al. (2013). We leave a re-implementation of Socher et al. (2012), another approach holding much promise for distributional composition with grammatical words, to future work. Multiplicative and additive models While Mitchell and Lapata (2008, 2009, 2010) propose a general framework that encompasses most of the CDSMs currently available, their empirical work focuses on two simple but effective models where the components of the vector resulting from the composition of two input vectors contain (functions of) geometric or additive averages of the corresponding input components. Given input vectors u and v"
W13-0603,D11-1129,0,0.299539,"Missing"
W13-0603,W11-2507,0,0.467995,"Missing"
W13-0603,W10-2805,0,0.0970267,"ive models, but it is not clear how to extend them to composition of more than two words. Non-terminals (Grammar) S → DP V P DP → DET N DP → N N → ADJ N V P → IV V P → T V DP Terminals (Lexicon) DET → a; some; the; one; two; three; no N → man; lady; violin; guitar; . . . ADJ → big; large; acoustic; . . . IV → performs; drinks; flies; . . . T V → cuts; eats; plays; . . . Figure 1: CFG of the fragment of English in our data set Adjective matrices are estimated from corpus-extracted examples of input noun vectors and the corresponding output adjective-noun phrase vectors, an idea also adopted by Guevara (2010). The approach of Baroni and Zamparelli, termed lexfunc (because specific lexical items act as functors), is actually a specific instantiation of the DisCoCat formalism (Clark, 2012; Coecke et al., 2010), that looks at the general case of n-ary composition functions, encoded in higher-order tensors, with function application modeled by tensor contraction, a generalization of matrix-by-vector multiplication to tensors of arbitrary order. The DisCoCat approach has also been applied to transitive verbs by Grefenstette and Sadrzadeh (2011a) and Grefenstette and Sadrzadeh (2011b). The regression me"
W13-0603,P08-1028,0,0.825558,"o similarity (Mitchell and Lapata, 2010), Turney (2012) obtains an extended version including word order variations of the original phrases, which are automatically judged to have a very low similarity (e.g., certain circumstance and case particular). Sentence similarity: Intransitive Sentences One of the first proposals to look at verb-argument composition traces back to Kintsch (2001) who was interested in capturing the different verb senses activated by different arguments, e.g., “The color ran” vs. “The horse ran”, but the model was tested only on a few sentences. Starting from this work, Mitchell and Lapata (2008) made an important step forward by developing a larger data set of subject+intransitive-verb sentences. They began with frequent noun-verb tuples (e.g., horse ran) extracted from the British National Corpus (BNC) and paired them with sentences with two synonyms of the verb, representing distinct verb senses, one compatible and the other incompatible with the argument (e.g., horse galloped and horse dissolved). The tuples were converted into simple sentences (in past tense form) and articles were added to nouns when appropriate. The final data set consists of 120 sentences with 15 original verb"
W13-0603,D09-1045,0,0.0388762,"Missing"
W13-0603,D12-1110,0,0.206824,"ive sentences in which word order and determiners have the chance to prove their worth in sentence similarity and paraphrase detection. 2 Compositional Distributional Semantic Models In this section we won’t present a proper overview of CDSMs, but focus only on those models we will be testing in our experiments, namely the multiplicative and additive models of Mitchell and Lapata (2008, 2009, 2010), and the lexical function model that represents the work carried out by Baroni and Zamparelli (2010), Grefenstette and Sadrzadeh (2011b), Grefenstette et al. (2013). We leave a re-implementation of Socher et al. (2012), another approach holding much promise for distributional composition with grammatical words, to future work. Multiplicative and additive models While Mitchell and Lapata (2008, 2009, 2010) propose a general framework that encompasses most of the CDSMs currently available, their empirical work focuses on two simple but effective models where the components of the vector resulting from the composition of two input vectors contain (functions of) geometric or additive averages of the corresponding input components. Given input vectors u and v, the multiplicative model (mult) returns a composed v"
W13-0603,D10-1115,1,\N,Missing
W14-5403,J10-4006,0,0.0252689,"between human-object positions in images and prepositions that link human, verb and object in language. Intuitively, if an action implies a strong positional relation between the human and the object, we expect to find specific, distinguishing prepositions in language. For example, in language you usually say “sit on chair”, where the preposition on suggests a specific spatial relation between the human and the chair. When an action does not imply a strong positional relation, such as “play”, we expect no specific prepositions. Links in language models To test this hypothesis, we use TypeDM (Baroni and Lenci, 2010), a distributional memory that has been built from large scale text corpora. This model contains weighted &lt;word-link-word> tuples extracted from a dependency parse of corpora. The relations between words are characterized by their “link”. Some of these links are prepositions that connect verbs and objects together. Examples of some tuples with word-link-word and their weights are provided in Figure 3. Number of links and link entropy We want to determine whether there is any correlation between human-object relative positions in images and the associated prepositions from language models. To d"
W14-5418,D10-1115,0,0.197781,"Missing"
W15-2712,W13-3209,0,0.122551,"ifferent, identical, brown cardboard boxes. Adrian accidentally puts a pair of red socks in the box containing blue objects, and Barbara remarks ‘no, no, these belong in the red box’. Thus, even if red when modifying box (or indeed any noun denoting a physical ob(1) a. If you ate some of the cookies, then I won’t have enough for the party. ; some and possibly all b. A: Did you eat all the cookies? B: I ate some. ; some but not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is."
W15-2712,W13-0104,1,0.883712,"in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase or sentence from the meaning of its component units. For instance, compositional distributional semantics accounts for how the generic distributional representation of, say, red makes different contributions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2"
W15-2712,W13-3214,0,0.0237673,"utions when composed with nouns like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — w"
W15-2712,D10-1113,0,0.0256717,"on discourse and dialogue context and then consider the dynamic meaning of sentences as context-change potential. 2.1 Word and Phrase Meaning As is well known, standard distributional models provide a single meaning representation for a word, which implicitly encodes all its possible senses and meaning nuances in general. A few recent models do account for some contextual effects within the scope of a sentence: For instance, the different shades of meaning that an adjective like red takes depending on the noun it modifies (e.g., car vs. cheek). However, such models, e.g. Erk and Pad´o (2008), Dinu and Lapata (2010), and Erk et al. (2013), typically use just a single word or sentence as context. They do not look into how word meaning gets progressively constrained by the common ground of the speakers as the discourse unfolds. A prominent type of “meaning adjustment” in discourse and dialogue is the interaction with the properties of the referent a particular word is associated to. For example, when we use a word like box, which a priori can be used for entities with very different properties, we typically use it to refer to a specific box in a given context, and this constrains its interpretation. The re"
W15-2712,D08-1094,0,0.0821638,"Missing"
W15-2712,P15-1029,0,0.0176232,"part. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in different contexts, where the task would be to pick the best image for each context. A related task would be reference resolution in a situated visual dialogue context (which can be seen as a situated version of image retrieval). This task has recently been tackled by Kennington and Schlangen (2015), who present an incremental acWe propose to take a different look on what the distributional meaning of a sentence is. Sentences are part of larger communicative situations and, as highlighted in the Dynamic Semantic tradition, can be considered relations between the discourse so far and what is to come next. We thus challenge the distributional semantics community to develop dynamic distributional semantic models that are able to encode the “context change potential” that sentences and utterances bring about as well as their coherence within a discourse context, including but not limited to"
W15-2712,J13-3003,0,0.107601,"rnardi∗ Gemma Boleda∗ Raquel Fern´andez† Denis Paperno∗ ∗ Center for Mind/Brain Sciences University of Trento † Institute for Logic, Language and Computation University of Amsterdam Abstract their unawareness of the unfolding discourse context. Standardly, distributional models are constructed from large amounts of data in batch mode by aggregating information into a vector that synthesises the general distributional meaning of an expression. Some of the recent distributional models account for contextual effects within the scope of a phrase or a sentence, (e.g., (Baroni and Zamparelli, 2010; Erk et al., 2013)), but they are not intended to capture how the meaning depends on the incrementally built discourse context where an expression is used. Since words and sentences are not used in isolation but are typically part of a discourse, the traditional distributional view is not sufficient. We argue that, to grow into an empirically adequate, full-fledged theory of meaning and interpretation, distributional models must evolve to provide meaning representations for actual language use in discourse and dialogue. Specifically, we discuss how the type of information they encode needs to be extended, and p"
W15-2712,P14-1132,0,0.0273218,"nd and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks related to some of the main phenomena we have discussed, against which new models could be evaluated. In Section 2.1 we have considered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a co"
W15-2712,D11-1129,0,0.157329,"Missing"
W15-2712,N15-1016,0,0.289155,"idered the need to interface conceptual meaning with referential meaning incrementally built up as a discourse unfolds. A good testbed for evaluating these aspects is offered by the recent development of crossmodal distributional semantic frameworks that are able to map between language and vision (Karpathy et al., 2014; Lazaridou et al., 2014; Socher et al., 2014). Current models have shown that images representing a concept can be retrieved by mapping a word vector into a visual space, and more recently image generation systems that create images from word vectors have also been introduced (Lazaridou et al., 2015a; Lazaridou et al., 2015b). These frameworks could be used to test whether an incrementally constructed, discoursecontextualised word vector is able to retrieve and generate different, more contextually appropriate images than its out-of-context vector counterpart. For instance, a vector for a phrase like red box in a context where red refers to the box’ contents should be mapped to different types of images depending on whether it has been constructed by a pragmatically aware model or not. Such a dataset could be constructed by creating images of referents of the same phrase used in differen"
W15-2712,J86-3001,0,0.186101,"tributional models mainly capture the latter stable conventions. The challenge is thus to be able to also capture the former, discourse-dependent meaning. Moreover, even function words, which are notreferential and are usually considered to have a precise (logical) meaning, are subject to pragmatic effects. For instance, the meaning of the determiner some is typically taken to be that of an existential quantifier (i.e., there exists at least one object with certain properties). Yet, its ‘at least one’ meaning may be refined in particular discourse contexts, as shown in the following examples: Grosz and Sidner, 1986; Kamp and Reyle, 1993; Asher and Lascarides, 2003; Ginzburg, 2012), namely, that the meaning of an expression consists in its context-change potential, where context is incrementally built up as a discourse proceeds. We contend that a distributional semantics for language use should account for the discourse context-dependent, dynamic, and incremental nature of language. Generic semantic knowledge won’t suffice: one needs to encode somehow the discourse state or common ground, which will enable modeling discourse and dialogue coherence. In this section, we first look into examples that illust"
W15-2712,W14-4321,1,0.874434,"Missing"
W15-2712,D12-1110,0,0.447686,"should capture, and propose concrete tasks on which they could be tested. 1 Introduction Distributional semantics has revolutionised computational semantics by representing the meaning of linguistic expressions as vectors that capture their co-occurrence patterns in large corpora (Turney et al., 2010; Erk, 2012). This strategy has been shown to be very successful for modelling word meaning, and it has recently been expanded to capture the meaning of phrases and even sentences in a compositional fashion (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012). Distributional semantic models are often presented as a robust alternative to representing meaning, compared to symbolic and logic-based approaches in formal semantics, thanks to their flexible representations and their data-driven nature. However, current models fail to account for aspects of meaning that are central in formal semantics, such as the relation between linguistic expressions and their referents or the truth conditions of sentences. In this position paper we focus on one of the main limitations of current distributional approaches, namely, 2 Meaning in Discourse As we just poin"
W15-2712,S14-2001,1,0.826578,"e elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, i"
W15-2712,D13-1170,0,0.00520688,"d Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena discussed above is certainly challenging. However, we believe that, given the many recent advances in the field, the distributional semantics community is ready to take up this challenge. We have argued that, in order to account for the dynamics of situated common ground and coherence, it is critical to capture the discourse context-dependent and incremental nature of meaning. Here we sketch out a series of tasks rela"
W15-2712,S07-1009,0,0.0220861,"fic word occurrence in context. These approaches offer a very valuable starting point, but their scope differs from ours. In particular, we can identify the following three main traditions: (1) Word Sense Disambiguation (Navigli, 2009, offers an overview), which aims to assign one of the predefined list of word senses to a given word, depending on the context. These are typically dictionary senses, and so do not capture semantic nuances that depend on the specific use of the word in a given discourse or dialogue context. (2) Word meaning in context as modeled in the lexical substitution task (McCarthy and Navigli, 2007; Erk et al., 2013), which predicts one or more paraphrases for a word in a given sentence. Unlike Word Sense Disambiguation, word meaning in context is specific to a given use of a word, that is, it doesn’t assume a pre-defined list of senses and can account for highly specific contextual effects. However, in this tradition context is restricted to one sentence, so the semantic phenomena modeled do not extend to discourse or dialogue. (3) Compositional distributional semantics (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; Boleda et al., 2013), which predicts the meaning of a phrase"
W15-2712,N15-1020,0,0.0682292,"Missing"
W15-2712,D14-1079,0,0.0132662,"like army, wine, cheek, or car, by modeling the resulting phrase. However, these methods are again limited to intrasentential context and only yield one single interpretation per phrase (presumably, the most typical one), thus not accounting for context-dependent interpretations of the red box type, discussed in In Section 2.2, we have highlighted the context update potential of utterances as a feature that should be captured by compositional distributional models beyond the word/phrase level. Recent work has evaluated such models on dialogue act tagging tasks (Kalchbrenner and Blunsom, 2013; Milajevs et al., 2014). However, these approaches consider utterances in isolation and rely on a predefined set of dialogue act types that are to a large extent arbitrary, and in any case of a metalinguistic nature. Similar comments would apply to the task of identifying discourse relations connecting isolated pairs of sentences. Instead, we argue that pragmatically-aware distributional models should help us to induce dialogue acts in an unsupervised way and to model them as context update functions. Thus, we suggest to adopt tasks that target coherence and the evolution of common ground — which is what discourse r"
W15-2712,P14-1009,1,0.848888,"not all Distributional models have so far not been particularly successful in modelling the meaning of function words (but see Baroni et al. (2012); Bernardi et al. (2013); Hermann et al. (2013)). We believe that discourse-aware distributional semantics may fare better in this respect. We elaborate on this idea further in the next subsection since their impact is seen beyond words and phrases level. 2.2 Beyond Words and Phrases Following formal semantics, so far distributional semantics has modelled sentences as a product of a compositional function (Baroni et al., 2014; Socher et al., 2012; Paperno et al., 2014). The main focus has been on evaluating which compositional operation performs best against tasks 96 b. A: the shape of a banana is not- it’s not really handy. B: Yes it is. such as classifying sentence pairs in an entailment relation, evaluating sentence similarity (Marelli et al., 2014), or predicting the so-called “sentiment” (positive, negative, or neutral orientation) of phrases and sentences (Socher et al., 2013). None of these tasks have considered sentence pairs within a wider discourse or dialogue context. 3 Tasks Developing distributional semantic models that can tackle the phenomena"
W15-2712,D10-1115,0,\N,Missing
W15-2712,E12-1004,1,\N,Missing
W15-2712,P83-1007,0,\N,Missing
W15-2712,Q14-1017,0,\N,Missing
W15-2712,2014.lilt-9.5,1,\N,Missing
W15-2712,P13-2010,1,\N,Missing
W15-2712,W15-0120,0,\N,Missing
W16-3208,P14-1023,0,0.0289256,"2015). The model includes multiple convolutional layers followed by max pooling and the top of these are fully connected layers (f c6, f c7, f c8). We used 4096-dimensional visual vectors extracted from the f c6 layer, which has shown better performance 1 The dataset is publicly available and can be downloaded at https://github.com/shekharRavi/ 62 Linguistic Features Each word in the dataset is represented by a 400-dimension vector extracted from a semantic space2 built with the CBOW architecture implemented in the word2vec toolkit (Mikolov et al., 2013) and the best-performing parameters in Baroni et al. (2014). 5 Evaluation Measures To evaluate the compositionality of each NNcompound, we measure the extent to which the composed vector is similar to the corresponding observed one, ie. the vector directly extracted from either texts or the selected image. Hence, first of all we use the standard Cosine similarity measure. The higher the similarity, the better the composition. It could be the case that the composed vector is however less similar to the observed one than it is the closest N-constituent. Thus, similarity by its own is not informative of whether the composition function has provided addit"
W16-3208,W14-5418,1,0.843799,"vision-based models in different tasks. Multimodal representations have been also used for exploring compositionality in visual objects (Vendrov et al., 2015), but compositionality was intended as combining two or more objects in a visual scene (eg., an apple and a banana) and not as obtaining the representation of a new concept based on two or more existing concepts. Even though some research in visual compositionality has been carried out for part segmentation tasks (Wang and Yuille, 2015), we focus on a rather unexplored avenue. To our knowledge, the closest work to ours is represented by Nguyen et al. (2014), who used a compositional model of distributional semantics for generating adjectivenoun phrases (eg., a red car given the vectors of red and car) both in language and vision. According to their results, a substantial correlation can be found between observed and composed representations in the visual modality. Moving from these results, the present study addresses the issue of whether, and to which extent, a compositional model can be applied to vision for obtaining nounnoun combinations, without relying on linguistic information. 3 Dataset To test our hypothesis, we used the publicly availa"
W16-3211,D13-1202,0,0.0303441,"e show that memory networks perform well in this task, and that explicit counting is not necessary to the system’s performance, supporting psycholinguistic evidence on the acquisition of quantifiers. 1 Introduction Multimodal representations of meaning have recently gained a lot of attention in the computational semantics literature. It has been shown, in particular, that the meaning of content words can be modelled in a cognitively – and even neuroscientifically – plausible way by learning representations from both the linguistic and visual contexts in which a lexical item has been observed (Anderson et al., 2013; Lazaridou et al., 2015). Such work has been crucial to advance the development of both a) a computational theory of meaning rooted in situated language use, as pursued by the field of Distributional Semantics (Clark, 2012; Erk, 2012) and b) vision-based applications such as image caption generation and visual question answering (Antol et al., 2015), going towards genuine image understanding. Both distributional semantics and visual applications, however, struggle with providing plausible representations for function words. This has theoretical and practical consequences. On the ∗ This projec"
W16-3211,S13-1001,0,0.235575,", current vision systems are forced to rely on background language models instead of truly interpreting the words of a query or caption in the given visual context. As a consequence, if e.g. the sentence I see some cats is more frequent than I see no cat, language modelbased applications will tend to generate the first even when the second would be more appropriate. In this paper, we start remedying this situation by investigating one important class of function words: natural language quantifiers (e.g. no, some, all). Quantifiers are an emerging field of research in distributional semantics (Grefenstette, 2013; Herbelot and Vecchi, 2015) and, so far, haven’t been studied in relation with visual data and grounding. We make a first step in this direction by asking whether the meaning of quantifier words can be learnt by observing their use in the presence of visual information. We observe that in grounded contexts, children learn to make quantification estimates before being able to count (Feigenson et al., 2004; Mazzocco et al., 2011), using their Approximate Number Sense (ANS). We ask whether Neural Networks (NNs) can model this ability, and we evaluate several neural network models, with and witho"
W16-3211,D15-1003,1,0.759671,"tems are forced to rely on background language models instead of truly interpreting the words of a query or caption in the given visual context. As a consequence, if e.g. the sentence I see some cats is more frequent than I see no cat, language modelbased applications will tend to generate the first even when the second would be more appropriate. In this paper, we start remedying this situation by investigating one important class of function words: natural language quantifiers (e.g. no, some, all). Quantifiers are an emerging field of research in distributional semantics (Grefenstette, 2013; Herbelot and Vecchi, 2015) and, so far, haven’t been studied in relation with visual data and grounding. We make a first step in this direction by asking whether the meaning of quantifier words can be learnt by observing their use in the presence of visual information. We observe that in grounded contexts, children learn to make quantification estimates before being able to count (Feigenson et al., 2004; Mazzocco et al., 2011), using their Approximate Number Sense (ANS). We ask whether Neural Networks (NNs) can model this ability, and we evaluate several neural network models, with and without numerical processing abil"
W17-6938,W04-3205,0,0.0932672,"preposition (a relation between objects) or an adverb (a manner of action). In total, we produce 196,284 datapoints, each corresponding to an <image, original, foil&gt; triple. The starting point for images and correct captions is Microsoft’s Common Objects in Context (MS-COCO)(Lin et al. (2014)). 3.1 Creating new target/foil pairs We describe below our procedure to expand the original dataset with new parts-of-speech. Verbs: We use three resources: a) VerbOcean, a semi-automatically generated broad-coverage semantic network of verbs extracted from the Web by exploiting a pattern-based approach (Chklovski and Pantel (2004)); b) Computing Lexical Contrast (CLC), a resource of contrasting words selected from direct and indirect WordNet opposites, (Mohammad et al. (2013)); c) SimLex999, a set of related word pairs rated with respect to their similarity (Hill et al. (2016)). From VerbOcean and CLC, we extract all antonyms (e.g., pull-push). From SimLex999, we select those pairs with a similarity score lower than the average in the database (e.g., allowing- preventing). We end up with 902, 44, and 30 verb pairs from VerbOcean, CLC and SimLex999 respectively. Adjectives and adverbs: As in the verb case, we use antony"
W17-6938,C16-1093,1,0.86445,"Missing"
W17-6938,J13-3004,0,0.015754,"ginal, foil&gt; triple. The starting point for images and correct captions is Microsoft’s Common Objects in Context (MS-COCO)(Lin et al. (2014)). 3.1 Creating new target/foil pairs We describe below our procedure to expand the original dataset with new parts-of-speech. Verbs: We use three resources: a) VerbOcean, a semi-automatically generated broad-coverage semantic network of verbs extracted from the Web by exploiting a pattern-based approach (Chklovski and Pantel (2004)); b) Computing Lexical Contrast (CLC), a resource of contrasting words selected from direct and indirect WordNet opposites, (Mohammad et al. (2013)); c) SimLex999, a set of related word pairs rated with respect to their similarity (Hill et al. (2016)). From VerbOcean and CLC, we extract all antonyms (e.g., pull-push). From SimLex999, we select those pairs with a similarity score lower than the average in the database (e.g., allowing- preventing). We end up with 902, 44, and 30 verb pairs from VerbOcean, CLC and SimLex999 respectively. Adjectives and adverbs: As in the verb case, we use antonyms from CLC and we select pairs from SimLex999 which have a similarity score lower than average. We extract 46 and 127 adjectives pairs from CLC and"
W17-6938,P17-1024,1,0.395794,"A little girl trying to push a skateboard with other standing around FOIL : A little girl trying to pull a skateboard with other standing around Verb Figure 1: Sample image, corresponding original caption and the generated foil caption for the different parts of speech. The model has to be able to classify the caption as ‘correct’ or ‘foil’ (Task 1); detect the foil word in the foil caption (see words highlighted in red) (Task 2); and correct the foil word with an appropriate replacement (see words highlighted in green) (Task 3). 2 The FOIL methodology We follow the methodology highlighted in Shekhar et al. (2017), which consists of replacing a single word in a human-generated caption with a ‘foil’ item, making the caption unsuitable to describe the original image. Given such replacements, the system should be able to perform three tasks: a) a classification task (T1): given an image and a caption, the model has to predict whether the caption is correct or inappropriate for the image (evaluating whether the model has a coarse understanding of the linguistic and visual inputs and their relations); b) a foil word detection task (T2): given an image and a foil caption, detect the foil word in the caption"
W17-6939,P15-2020,0,0.0285248,"guistic information (for a brief review see the introduction in Zanini et al. (2016)). To evaluate 1 our hypothesis, we employ a computational model trained to classify objects in images. We test whether mass-substance images are internally (i.e. among the various regions of the same image) more homogeneous, and externally (i.e. among the various instances of the same entity) more consistent compared to entities denoted by count nouns (see Figure 1). In other words, ‘substances’ should be distinguished from ‘objects’ by means of the lower variance of their visual features (somewhat similar to Kiela et al. (2015) in a lexical entailment detection task). Though similar with respect to shape, entities denoted by count nouns are likely to be very different with respect to many other low-level visual features (surface, texture, color, etc.). As a consequence, they would require higher-level operations to be recognized and classified as belonging to a particular entity class. Figure 1: Left: images representing the count noun ‘building’. Right: images representing the mass noun ‘flour’. As can be noted, the former exhibits much more variability compared to the latter, both internally (i.e. among regions of"
W17-6939,L16-1447,0,0.0442887,"Missing"
W19-0418,K17-1037,0,0.0232022,"reach their best performance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer"
W19-0418,W18-6402,0,0.061223,"Missing"
W19-0418,D18-1119,0,0.0317612,"sis of the multimodal semantic spaces learned by the encoders In this section, we analyse the encoders by comparing the similarity of the multimodal spaces they learn and by comparing the learned multimodal spaces to the visual and linguistic representations they receive as input in terms on nearest neighbours. Representation similarity analysis Representation Similarity Analysis (RSA) is a technique from neuroscience (Kriegeskorte et al., 2008) that has been recently leveraged in computational linguistics, for example to compare the semantic spaces learned by artificial communicating agents (Bouchacourt and Baroni, 2018). It compares different semantic spaces by comparing their internal similarity relations, given a common set N of input data points. Each input k ∈ N is processed by an encoder for a given task T i, producing vector hkT i . Let HTNi be the set of vector representations created by the encoder of T i for all the items in N ; and let HTNj be the corresponding set of representations by the encoder of task T j. These two semantic spaces, HTNi and HTNj , are not directly comparable as they have been produced independently. RSA remedies this by instead comparing their structure in terms of internal s"
W19-0418,D18-2029,0,0.0186085,"n to a task-specific component: an MLP in the case of the pre-training retrieval tasks and a fully connected layer in the case of the FOIL classification task. common initial base across models and diminishes the effects of using different datasets for each specific task (the datasets are described in Section 5). Visual and language embeddings To represent visual data, we use ResNet152 features (He et al., 2016), which yield state of the art performance in image classification tasks and can be computed efficiently. To represent linguistic data, we use Universal Sentence Encoder (USE) vectors (Cer et al., 2018) since they yield near state-of-the-art results on several NLP tasks and are suitable both for short texts (such as the descriptions in ReferIt) and longer ones (such as the dialogues in GuessWhat).2 In order to gain some insight into the semantic spaces that emerge from these visual and linguistic representations, we consider a sample of 5K datapoints sharing the images across the three tasks and use average cosine similarity as a measure of space density. We find that the semantic space of the input images is denser (0.57 average cosine similarity) than the semantic space of the linguistic i"
W19-0418,P18-2074,0,0.0283991,"when the same set of inputs is given. We give as input 5,000 data points from the FOIL test set, randomly sampled from only the ones with original captions and containing unique images, and compare the representations produced by the encoders under investigation. Figure 4 shows that the semantic space produced by the encoder fully trained on FOIL is rather different from all the other models, and that the VQA semantic space is very similar to the one produced by the randomly initialized encoder. Nearest neighbour overlap We analyse the encoder representations using nearest neighbour overlap. Collell and Moens (2018) proposed this measure to compare the structure of functions that map concepts from an input to a target space. It is defined as the number of k nearest neighbours that two paired vectors share in their respective semantic space. For instance, if k = 3 and the 3 nearest neighbours of the vector for ‘cat’ vcat in space V are {vdog , vtiger , vlion }, and those of the vector of ‘cat’ zcat in space Z are {vmouse , vtiger , vlion }, the nearest neighbour overlap (NN) is 2. The value is then normalized with respect to the number of data points and the number of k nearest neighbours. k=1 ResNet152 U"
W19-0418,D17-1070,0,0.0291437,", the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as in computational linguistics (e.g., Conneau et al. (2017)). However, little has been done in this respect for visually grounded natural language processing models. In this work, we combine these different research lines and explore transfer learning techniques in the domain of language and vision tasks. In particular, we use the FOIL diagnostic dataset (Shekhar et al., 2017) and investigate to what extent skills learned through different multimodal tasks transfer. While transfering the knowledge learned by a pre-trained model can be useful in principle, Conneau et al. (2018) found that randomly initialized models provide strong baselines that can ev"
W19-0418,P18-1198,0,0.0227038,"We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as"
W19-0418,W18-1401,0,0.0668427,"Missing"
W19-0418,D14-1086,0,0.101691,"assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities. 1 Introduction In recent years, a lot of progress has been made within the emerging field at the intersection of computational linguistics and computer vision thanks to the use of deep neural networks. The most common strategy to move the field forward has been to propose different multimodal tasks—such as visual question answering (Antol et al., 2015), visual question generation (Mostafazadeh et al., 2016), visual reference resolution (Kazemzadeh et al., 2014), and visual dialogue (Das et al., 2017)—and to develop task-specific models. The benchmarks developed so far have put forward complex and distinct neural architectures, but in general they all share a common backbone consisting of an encoder which learns to merge the two types of representation to perform a certain task. This resembles the bottom-up processing in the ‘Hub and Spoke’ model proposed in Cognitive Science to represent how the brain processes and combines multi-sensory inputs (Patterson and Ralph, 2015). In this model, a ‘hub’ module merges the input processed by the sensor-specif"
W19-0418,Q16-1037,0,0.0313427,"ta size they need to reach their best performance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved"
W19-0418,N18-2069,0,0.0225537,"tion sets and compare the results to chance performance. Given the number of candidate answers and objects per task in our datasets, chance P@1 is 0.055 for VQA and 0.05 for ReferIt and GuessWhat. Our task-specific models obtain P@1 values of 0.14 for VQA (mean rank 2.84), 0.12 for ReferIt (mean rank 3.32), and 0.08 for GuessWhat (mean rank 4.14). Not surprisingly given the challenging nature of these tasks, the results are not high. Nevertheless, the representations learned by the models allow them to perform above chance level and thus provide a reasonable basis for further investigation. 3 Madhysastha et al. (2018) found that an earlier version of the FOIL dataset was biased. We have used the latest version of the dataset available at https://foilunitn.github.io/, which does not have this problem. 6.1 Analysis via diagnostic task In this first analysis, we assess the quality of the multimodal representations learned by the three multimodal tasks considered in terms of their potential to perform the FOIL task, i.e., to spot semantic (in)congruence between an image and a caption. Besides comparing the models with respect to task accuracy, we also investigate how they learn to adapt to the FOIL task over t"
W19-0418,P16-1170,0,0.0253913,"anguage tasks on an existing diagnostic task designed to assess multimodal semantic understanding. Second, we carry out a battery of analyses aimed at studying how the encoder merges and exploits the two modalities. 1 Introduction In recent years, a lot of progress has been made within the emerging field at the intersection of computational linguistics and computer vision thanks to the use of deep neural networks. The most common strategy to move the field forward has been to propose different multimodal tasks—such as visual question answering (Antol et al., 2015), visual question generation (Mostafazadeh et al., 2016), visual reference resolution (Kazemzadeh et al., 2014), and visual dialogue (Das et al., 2017)—and to develop task-specific models. The benchmarks developed so far have put forward complex and distinct neural architectures, but in general they all share a common backbone consisting of an encoder which learns to merge the two types of representation to perform a certain task. This resembles the bottom-up processing in the ‘Hub and Spoke’ model proposed in Cognitive Science to represent how the brain processes and combines multi-sensory inputs (Patterson and Ralph, 2015). In this model, a ‘hub’"
W19-0418,P17-1024,1,0.822556,"ficial multimodal systems, with the goal of assessing its ability to compute multimodal representations that are useful beyond specific tasks. While current visually grounded models perform remarkably well on the task they have been trained for, it is unclear whether they are able to learn representations that truly merge the two modalities and whether the skill they have acquired is stable enough to be transferred to other tasks. In this paper, we investigate these questions in detail. To do so, we evaluate an encoder trained on different multimodal tasks on an existing diagnostic task—FOIL (Shekhar et al., 2017)—designed to assess multimodal semantic understanding and carry out an in-depth analysis to study how the encoder merges and exploits the two modalities. We also exploit two techniques to investigate the structure of the learned semantic spaces: Representation Similarity Analysis (RSA) (Kriegeskorte et al., 2008) and Nearest Neighbour overlap (NN). We use RSA to compare the outcome of the various encoders given the same vision-and-language input and NN to compare the multimodal space produced by an encoder with the ones built with the input visual and language embeddings, respectively, which a"
W19-0418,P17-2034,0,0.0302679,"ers give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et al. (2014)) as well as in computational linguistics (e.g., Conneau et al. (2017)). However, little has been done in this respect for visually grounded natural language processing models. In this work, we combine these different research lines and explore transfer lear"
W19-0418,W18-5448,0,0.131622,"mance on the FOIL task. We also observe that the semantic spaces learned by the encoders trained on the ReferIt and GuessWhat tasks are closer to each other than to the semantic space learned by the VQA encoder. Despite these asymmetries among tasks, we find that all encoders give more weight to the visual input than the linguistic one. 2 Related Work Our work is part of a recent research trend that aims at analyzing, interpreting, and evaluating neural models by means of auxiliary tasks besides the task they have been trained for (Adi et al., 2017; Linzen et al., 2016; Alishahi et al., 2017; Zhang and Bowman, 2018; Conneau et al., 2018). Within language and vision research, the growing interest in having a better understanding of what neural models really learn has led to the creation of several diagnostic datasets (Johnson et al., 2017; Shekhar et al., 2017; Suhr et al., 2017). Another research direction which is relevant to our work is transfer learning, a machine learning area that studies how the skills learned by a model trained on a particular task can be transferred to learn a new task better, faster, or with less data. Transfer learning has proved successful in computer vision (e.g. Razavian et"
W19-2912,D16-1044,0,0.0445381,"es in the auditory input. This way, the model exploits prior associations between modalities. We show that the model profits from the prior knowledge and outperforms the auditory-only setting. Figure 1: Learning to quantify through a ‘Hub and Spoke’ model enhanced with prior knowledge. The Hub learns to integrate multisensory inputs, whose representations (Spokes) are affected by such integration and can be ‘hallucinated’ by prior knowledge. We focus on how this prior knowledge hallucinates the visual representation (signalled by the dotted arrow). rather challenging (Malinowski et al., 2015; Fukui et al., 2016). While this work paid little attention to quantifiers, a few recent studies specifically investigated their computational learning from visual inputs (Sorodoc et al., 2016; Pezzelle et al., 2017). These works built on the evidence that (part of) the meaning of quantifiers is grounded in perception. However, they only experimented with the visual modality, though the numerical representations humans derive from sensory inputs have been shown to be shared across modalities, e.g., vision and sound (Feigenson et al., 2004). Introduction Quantifiers (words like ‘some’, ‘most’, ‘all’) have long bee"
W19-2912,E12-1004,1,0.778224,"hown to be shared across modalities, e.g., vision and sound (Feigenson et al., 2004). Introduction Quantifiers (words like ‘some’, ‘most’, ‘all’) have long been the holy grail of formal semanticists (see Peters et al. (2006) for an overview). More recently, they have caught the attention of cognitive scientists, who showed that these expressions are handled by children quite early in life (Halberda et al., 2008), even before developing the ability to count (Hurewitz et al., 2006). Though some effort has been paid to model these highfrequency expressions from their use in big corpora of texts (Baroni et al., 2012; Herbelot and Vecchi, 2015), relatively little work has focused on the models’ ability to quantify using these words. In computer vision, some focus to the task of extracting quantities from images has been expressed through visual question answering, whose benchmark dataset (Antol et al., 2015) contains ‘count questions’ (e.g., ‘How many Xs have the property Y?’) that repeatedly turned out to be In the literature on multisensory integration it is well established that redundant information conveyed through different sensory inputs leads to a better performance on semantic tasks (McGurk and M"
W19-2912,D18-2029,0,0.0169819,"‘feline’, ‘mammal’).2 (b) For each entity present in the audio-visual scene, we randomly picked one of the three nouns. (c) For each noun, we counted the number of entities present in the audio-visual input, assigned that number to the noun and pluralized it, if necessary. (d) In order to account for more variability, we started the linguistic caption by choosing one of six possible starting phrases.3 We obtained captions with on average 10.5 nouns (standard deviation: 4.53). As for the linguistic scenes, for each caption we extracted the features through the Universal Sentence Encoder (USE) (Cer et al., 2018) producing 512 dimensional vectors for each sentence. Alternatively, we could have used LSTM modules to process from scratch both the linguistic and acoustic inputs exploiting their sequential nature. We rejected this alternative mainly to avoid that, during the training process, the neural network learns task-dependent representations and arbitrary associations. It has been shown (e.g., in Cer et al. (2018)) that USE provides sentence-level embeddings with strong transfer performance on several NLP tasks. We consider this point as a strong motivation for our choice: in this way, we get more c"
W19-2912,P17-1057,0,0.027851,"on from various modalities. Attention has been mostly on language and vision, for which various tasks have been proposed, i.e. image captioning (Hodosh et al., 2013), visual question answering (Antol et al., 2015; Goyal et al., 2017), visual reasoning (Andreas et al., 2016; Johnson et al., 2017; Suhr et al., 2017), visual storytelling (Huang et al., 2016; Gonzalez-Rico and Fuentes-Pineda, 2018), and visual dialogue (De Vries et al., 2017). While all this work combines images with written text, some other studies employed spoken language to perform various tasks, such as image-audio retrieval (Chrupała et al., 2017; Harwath et al., 2018). Overall, these works repeatedly showed that combining information from language and vision leads to representations that are beneficial in virtually any task. A relatively recent strand of research focused on the integration of visual and sound information, where the latter is, e.g., the ‘roar’ of a fast car (Owens et al., 2016, 2018; Zhao et al., 2018). More akin to our work is Aytar et al. (2017), who jointly investigated language, vision, and sound. By training a deep convolutional network • Using priors hallucinating the visual representation improves the performan"
W19-2912,D15-1003,0,0.019985,"oss modalities, e.g., vision and sound (Feigenson et al., 2004). Introduction Quantifiers (words like ‘some’, ‘most’, ‘all’) have long been the holy grail of formal semanticists (see Peters et al. (2006) for an overview). More recently, they have caught the attention of cognitive scientists, who showed that these expressions are handled by children quite early in life (Halberda et al., 2008), even before developing the ability to count (Hurewitz et al., 2006). Though some effort has been paid to model these highfrequency expressions from their use in big corpora of texts (Baroni et al., 2012; Herbelot and Vecchi, 2015), relatively little work has focused on the models’ ability to quantify using these words. In computer vision, some focus to the task of extracting quantities from images has been expressed through visual question answering, whose benchmark dataset (Antol et al., 2015) contains ‘count questions’ (e.g., ‘How many Xs have the property Y?’) that repeatedly turned out to be In the literature on multisensory integration it is well established that redundant information conveyed through different sensory inputs leads to a better performance on semantic tasks (McGurk and MacDonald, 1976). These findi"
W19-2912,E17-2054,1,0.909244,"Figure 1: Learning to quantify through a ‘Hub and Spoke’ model enhanced with prior knowledge. The Hub learns to integrate multisensory inputs, whose representations (Spokes) are affected by such integration and can be ‘hallucinated’ by prior knowledge. We focus on how this prior knowledge hallucinates the visual representation (signalled by the dotted arrow). rather challenging (Malinowski et al., 2015; Fukui et al., 2016). While this work paid little attention to quantifiers, a few recent studies specifically investigated their computational learning from visual inputs (Sorodoc et al., 2016; Pezzelle et al., 2017). These works built on the evidence that (part of) the meaning of quantifiers is grounded in perception. However, they only experimented with the visual modality, though the numerical representations humans derive from sensory inputs have been shown to be shared across modalities, e.g., vision and sound (Feigenson et al., 2004). Introduction Quantifiers (words like ‘some’, ‘most’, ‘all’) have long been the holy grail of formal semanticists (see Peters et al. (2006) for an overview). More recently, they have caught the attention of cognitive scientists, who showed that these expressions are han"
W19-2912,N18-1039,1,0.75123,"t another and to perform a more/less task. Similar high-level cognitive abilities are required to humans to use vague quantifiers such as few, many, or most, whose meaning is heavily dependent on contextual factors. Using visual scenes as context, a recent strand of work has focused on the computational learning of quantifiers with neural networks. One approach tackled the task in a visual question answering fashion (Sorodoc et al., 2018), while another aimed at learning to apply the correct quantifier to a given scene (Sorodoc et al., 2016; Pezzelle et al., 2017). More related to our work is Pezzelle et al. (2018b), which tested a model in the task of predicting the probability of each quantifier to be used in a given scene. The network was trained with probabilities from human participants by Pezzelle 3.2 Datasets Following Pezzelle et al. (2018a), our datasets consist of scenes containing animals and artifacts with a minimum of 3 and a maximum of 20 entities in total. There are in total 17 proportions, out of which 8 contain more animals than artifacts, 8 contain more artifacts than animals, and 1 contains an equal number of them.1 For each proportion 1 The proportions obtained by having min. 3 max"
W19-2912,W16-3211,1,0.911449,"uditory-only setting. Figure 1: Learning to quantify through a ‘Hub and Spoke’ model enhanced with prior knowledge. The Hub learns to integrate multisensory inputs, whose representations (Spokes) are affected by such integration and can be ‘hallucinated’ by prior knowledge. We focus on how this prior knowledge hallucinates the visual representation (signalled by the dotted arrow). rather challenging (Malinowski et al., 2015; Fukui et al., 2016). While this work paid little attention to quantifiers, a few recent studies specifically investigated their computational learning from visual inputs (Sorodoc et al., 2016; Pezzelle et al., 2017). These works built on the evidence that (part of) the meaning of quantifiers is grounded in perception. However, they only experimented with the visual modality, though the numerical representations humans derive from sensory inputs have been shown to be shared across modalities, e.g., vision and sound (Feigenson et al., 2004). Introduction Quantifiers (words like ‘some’, ‘most’, ‘all’) have long been the holy grail of formal semanticists (see Peters et al. (2006) for an overview). More recently, they have caught the attention of cognitive scientists, who showed that t"
W19-2912,P17-2034,0,0.152241,"anguage-vision association prior). We show that • Using congruent audio visual inputs increases the performance of the model in learning quantifiers within single-sensory models; Fueled by the explosion of deep learning, much effort has been paid in recent years to develop models that exploit information from various modalities. Attention has been mostly on language and vision, for which various tasks have been proposed, i.e. image captioning (Hodosh et al., 2013), visual question answering (Antol et al., 2015; Goyal et al., 2017), visual reasoning (Andreas et al., 2016; Johnson et al., 2017; Suhr et al., 2017), visual storytelling (Huang et al., 2016; Gonzalez-Rico and Fuentes-Pineda, 2018), and visual dialogue (De Vries et al., 2017). While all this work combines images with written text, some other studies employed spoken language to perform various tasks, such as image-audio retrieval (Chrupała et al., 2017; Harwath et al., 2018). Overall, these works repeatedly showed that combining information from language and vision leads to representations that are beneficial in virtually any task. A relatively recent strand of research focused on the integration of visual and sound information, where the l"
