2020.acl-main.105,I13-1062,1,0.869269,"Missing"
2020.acl-main.105,C16-1277,1,0.850295,"Missing"
2020.acl-main.105,W18-2501,0,0.0205763,"(Meng et al., 2017) is a sequenceto-sequence model with attention, augmented with a copying mechanism (Gu et al., 2016) to predict phrases that rarely occur. The model is trained with document-keyphrase pairs and uses beam search decoding for inference. seq2seq+corr (Chen et al., 2018) extends the aforementioned model with correlation constraints. It employs a coverage mechanism (Tu et al., 2016) that diversifies attention distributions to increase topic coverage, and a review mechanism to avoid generating duplicates. We implemented the models in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). Models are trained on the KP20k dataset (Meng et al., 2017), which contains 567,830 scientific abstracts with gold-standard, author-assigned keywords (5.3 per doc. on avg.). We use the parameters suggested by the authors for each model. To validate the effectiveness of our implementations, we conducted an intrinsic evaluation by counting the number of exact matches between predicted and gold keyphrases. We adopt the standard 1119 metric and compute the f-measure at top 5, as it corresponds to the average number of keyphrases in KP20k and NTCIR-2, that is, 5.3 and 4.8, respectively. We also e"
2020.acl-main.105,P16-1154,0,0.0238486,"evious work that formulates this task as an extraction problem (a.k.a. keyphrase extraction), which can be seen as ranking phrases extracted from a document, recent neural models for keyphrase generation are based on sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2014), thus potentially allowing them to generate any phrase, also beyond those that appear verbatim in the text. In this study, we consider the following two neural keyphrase generation models: seq2seq+copy (Meng et al., 2017) is a sequenceto-sequence model with attention, augmented with a copying mechanism (Gu et al., 2016) to predict phrases that rarely occur. The model is trained with document-keyphrase pairs and uses beam search decoding for inference. seq2seq+corr (Chen et al., 2018) extends the aforementioned model with correlation constraints. It employs a coverage mechanism (Tu et al., 2016) that diversifies attention distributions to increase topic coverage, and a review mechanism to avoid generating duplicates. We implemented the models in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). Models are trained on the KP20k dataset (Meng et al., 2017), which contains 567,830 scientific ab"
2020.acl-main.105,Y11-1063,1,0.506613,"ng-kg. 1 Introduction With the exponential growth of the scientific literature (Bornmann and Mutz, 2015), retrieving relevant scientific papers becomes increasingly difficult. Keywords, also referred to as keyphrases, provide an effective way to supplement paper indexing and improve retrieval effectiveness in scientific digital libraries (Barker et al., 1972; Zhai, 1997; Gutwin et al., 1999; Lu and Kipp, 2014). However, only few documents have assigned keyphrases, and those who do were, for the most part, selflabeled by their authors, thus exhibiting annotation inconsistencies (Strader, 2011; Suzuki et al., 2011). This has motivated an active line of research on automatic keyphrase extraction (see Hasan and Ng (2014) for an overview) and, more recently, keyphrase generation (Meng et al., 2017), where the task is to find a set of words and phrases that represents the main content of a document. Although models for predicting keyphrases have been extensively evaluated on their ability to reproduce author’s keywords, it still remains unclear whether they can be usefully applied in information retrieval. One reason for this lack of evidence may have been their relatively low performance discouraging attem"
2020.acl-main.105,N06-1052,0,0.167069,"Missing"
2020.acl-main.105,P16-1008,0,0.0214297,"et al., 2014), thus potentially allowing them to generate any phrase, also beyond those that appear verbatim in the text. In this study, we consider the following two neural keyphrase generation models: seq2seq+copy (Meng et al., 2017) is a sequenceto-sequence model with attention, augmented with a copying mechanism (Gu et al., 2016) to predict phrases that rarely occur. The model is trained with document-keyphrase pairs and uses beam search decoding for inference. seq2seq+corr (Chen et al., 2018) extends the aforementioned model with correlation constraints. It employs a coverage mechanism (Tu et al., 2016) that diversifies attention distributions to increase topic coverage, and a review mechanism to avoid generating duplicates. We implemented the models in PyTorch (Paszke et al., 2017) using AllenNLP (Gardner et al., 2018). Models are trained on the KP20k dataset (Meng et al., 2017), which contains 567,830 scientific abstracts with gold-standard, author-assigned keywords (5.3 per doc. on avg.). We use the parameters suggested by the authors for each model. To validate the effectiveness of our implementations, we conducted an intrinsic evaluation by counting the number of exact matches between p"
2020.acl-main.105,C08-1122,0,0.105347,"Missing"
2020.acl-main.105,A97-1046,0,0.430277,"this framework, we point out and discuss the difficulties encountered with supplementing documents with –not present in text– keyphrases, and generalizing models across domains. Our code is available at https:// github.com/boudinfl/ir-using-kg. 1 Introduction With the exponential growth of the scientific literature (Bornmann and Mutz, 2015), retrieving relevant scientific papers becomes increasingly difficult. Keywords, also referred to as keyphrases, provide an effective way to supplement paper indexing and improve retrieval effectiveness in scientific digital libraries (Barker et al., 1972; Zhai, 1997; Gutwin et al., 1999; Lu and Kipp, 2014). However, only few documents have assigned keyphrases, and those who do were, for the most part, selflabeled by their authors, thus exhibiting annotation inconsistencies (Strader, 2011; Suzuki et al., 2011). This has motivated an active line of research on automatic keyphrase extraction (see Hasan and Ng (2014) for an overview) and, more recently, keyphrase generation (Meng et al., 2017), where the task is to find a set of words and phrases that represents the main content of a document. Although models for predicting keyphrases have been extensively e"
2020.acl-main.105,P19-1515,0,0.0326067,"Missing"
2020.coling-main.368,N16-1014,0,0.0740857,"onsidered as sensible. This quality is comprised of relevancy, consistency, common sense, and more. However, in this work, we do not focus on these sub-qualities. Instead, we consider the sensibleness quality as a whole. Please note that although a sensible response can be generic, short, and boring, it is rather important for the response to be on-topic than to have unique tokens. Thus, we place this quality on the second level of the hierarchy. Likability quantifies how much a set of one or more qualities makes a response more likable for a particular task. These qualities can be diversity (Li et al., 2016), sentiment (Rashkin et al., 2019), specificity (Ke et al., 2018), engagement (Yi et al., 2019), fluency (Kann et al., 2018) and more. A likable response may or may not be sensible to the context. For example, a diverse response may contain many unique words, although it might be off-topic or completely incomprehensible. However, when combining with sensibleness, it can quantify how likable a sensible response is. Due to the enhancement that the likability aspect has on the response, we position it on the highest level of the hierarchy. 3.2 USL-H Metric Each aspect, by itself, is not enough to"
2020.coling-main.368,I17-1099,0,0.253464,"yers in the hierarchy. Based on these properties, we propose a simple method to combine scores of each aspect to obtain USL-H score, which stands for Understandability, Sensibleness, and Likability in Hierarchy. USL-H can be modified to add or remove a quality and to replace a metric with a more optimal alternative. This configurability removes the barrier of requiring a single complicated model and instead enables a combination of multiple sub-metrics. For simplicity, we demonstrate the configurability using only specificity as our likability aspect. Experimenting on the DailyDialog dataset (Li et al., 2017), we show that valid utterance prediction, next utterance prediction, and masked language models have good correlations with human judgments on understandability, sensibleness, and specificity, respectively. Moreover, combining these sub-metrics as a single metric using our USL-H also correlates well with overall quality on both Pearson and Spearman correlations. Alternatively, we replace specificity with the empathy aspect, recombine the sub-metrics, and put it up against other metrics to select the most empathetic responses. We find that this new configuration can detect better empathetic re"
2020.coling-main.368,W04-1013,0,0.337402,"h are both time-consuming and cost-ineffective (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020). To cope with this, various metrics have been proposed to score the overall quality of a dialogue response. Word overlap-based metrics, which were adopted from the MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018). However, Liu et al. (2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context. Recently, learning-based metrics, which aim to predict the overall quality of a response, have a better correlation with human judgment compared with word overlap-based metrics. Various training settings have also been explored. For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learning to regress on human judgments, whereas PONE (Lan et al., 2020) is trained with next utterance prediction task with sophisticated samplings. However, these metrics are not"
2020.coling-main.368,D16-1230,0,0.0594923,"Missing"
2020.coling-main.368,P17-1103,0,0.112283,"sed to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018). However, Liu et al. (2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context. Recently, learning-based metrics, which aim to predict the overall quality of a response, have a better correlation with human judgment compared with word overlap-based metrics. Various training settings have also been explored. For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learning to regress on human judgments, whereas PONE (Lan et al., 2020) is trained with next utterance prediction task with sophisticated samplings. However, these metrics are not configurable and may suffer from several limitations. First, they may not capture a certain quality that is essential for a particular task. As shown in Table 1, BERTScore 1 The implementation of our metrics is available at https://github.com/vitouphy/usl_dialogue_metric. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:"
2020.coling-main.368,2020.acl-main.64,0,0.166315,"corresponding response r, and label them as a valid pair. Then, we keep ui as the context and select a random utterance uj from a pool of all the utterances in the training set and label that pair (ui , uj ) as the invalid pair. To fine-tune the BERT model, we first merge a context-response pair into a single array of tokens (w1 , w2 , . . . , wt ). Then, we use the same approach as BERT-VUP metric to obtain the score sS . Metric for Specificity For simplicity of studying the configurability of our proposed metric, we select specificity as our likable quality. Following the use of Roberta in Mehri and Eskenazi (2020) to compute the mask language model (MLM) metric, we use a BERT-based model for consistency with the BERT-VUP and BERT-NUP metrics. Moreover, instead of using both (c, r), as in Mehri and Eskenazi (2020), we only use the response r to ensure the independence from the context c. Therefore, for a response r with m words, we sequentially mask one word at a time and feed it into BERT-MLM to predict negative log-likelihood (MLM-Likelihood) of all masked words. We also investigate negative cross-entropy (MLM-NCE), perplexity (MLM-PPL), and MLM-SLOR (Kann et al., 2018) to verify if they can be used f"
2020.coling-main.368,1997.mtsummit-papers.22,0,0.378671,"Missing"
2020.coling-main.368,P02-1040,0,0.116627,"that are essential for a task. We also propose a simple method to composite metrics of each aspect to obtain a single metric called USL-H, which stands for Understandability, Sensibleness, and Likability in Hierarchy1 . We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics. 1 Introduction Evaluating a dialogue response is crucial for the development of open-domain dialogue systems. It allows comparison between different systems, which is similar to how the machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019). Without automatic evaluation metrics, many studies tend to rely on either expert or crowdsourced platform to score the responses, which are both time-consuming and cost-ineffective (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020). To cope with this, various metrics have been proposed to score the overall quality of a dialogue response. Word overlap-based metrics, which were adopted from the MT community"
2020.coling-main.368,P19-1534,0,0.0115974,"uality is comprised of relevancy, consistency, common sense, and more. However, in this work, we do not focus on these sub-qualities. Instead, we consider the sensibleness quality as a whole. Please note that although a sensible response can be generic, short, and boring, it is rather important for the response to be on-topic than to have unique tokens. Thus, we place this quality on the second level of the hierarchy. Likability quantifies how much a set of one or more qualities makes a response more likable for a particular task. These qualities can be diversity (Li et al., 2016), sentiment (Rashkin et al., 2019), specificity (Ke et al., 2018), engagement (Yi et al., 2019), fluency (Kann et al., 2018) and more. A likable response may or may not be sensible to the context. For example, a diverse response may contain many unique words, although it might be off-topic or completely incomprehensible. However, when combining with sensibleness, it can quantify how likable a sensible response is. Due to the enhancement that the likability aspect has on the response, we position it on the highest level of the hierarchy. 3.2 USL-H Metric Each aspect, by itself, is not enough to evaluate the overall quality of a"
2020.coling-main.368,W12-2018,0,0.183413,"judgments. 2 Related Work Automatic Evaluation Metrics Many automatic evaluation metrics have been proposed to evaluate the overall quality of a response. BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE (Lin, 2004) metrics are word overlap-based approaches, which utilize the model’s responses and reference responses to compute the number of overlapping words. The more words overlap, the higher the scores are. However, Liu et al. (2016) showed that these metrics have a weak correlation with human judgment. Alternatively, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020a) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score (Zhang et al., 2018; Zeng et al., 2019). However, due to many possible responses to a context, it is inaccurate to use these metrics. Learning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020). The model learns to determine whether or not a context-response pair is valid. It is typically trained with contex"
2020.coling-main.368,P16-1162,0,0.0101997,"r Understandability, Sensibleness, and Likability in Hierarchy1 . We demonstrated that USL-H score achieves good correlations with human judgment and maintains its configurability towards different aspects and metrics. 1 Introduction Evaluating a dialogue response is crucial for the development of open-domain dialogue systems. It allows comparison between different systems, which is similar to how the machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019). Without automatic evaluation metrics, many studies tend to rely on either expert or crowdsourced platform to score the responses, which are both time-consuming and cost-ineffective (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020). To cope with this, various metrics have been proposed to score the overall quality of a dialogue response. Word overlap-based metrics, which were adopted from the MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zha"
2020.coling-main.368,2020.acl-main.220,0,0.510385,"Missing"
2020.coling-main.368,2020.acl-main.183,0,0.0180866,"lity (Table 1). Another limitation is the difficulty in enhancing only a particular aspect of the metric. Suppose there is a single metric that can capture both sensibleness and specificity, and a new state-of-the-art metric on the latter quality is subsequently developed; it would be complicated to modify the existing metrics (i.e., BLEU or ADEM) to include this new SOTA metric. Aside from evaluating a response using only a single overall score, some studies evaluate the response on multiple aspects, i.e., fluency, relevancy, specificity, and empathy (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020). The limitation of this approach is that with multiple scores to consider, it becomes unclear to determine which response is better. Is a specific response more preferable than an empathetic one? To address these issues, we first propose simplifying the various qualities by grouping them into three main aspects: understandability (N¨ubel, 1997), sensibleness (Adiwardana et al., 2020), and likability. We assume these groups have hierarchical properties in the following way: (i) a response first needs to be understandable; (ii) then it needs to make sense to the context; (iii) other qualities,"
2020.coling-main.368,N15-1020,0,0.0109309,"15; Sennrich et al., 2016; Aharoni et al., 2019). Without automatic evaluation metrics, many studies tend to rely on either expert or crowdsourced platform to score the responses, which are both time-consuming and cost-ineffective (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020). To cope with this, various metrics have been proposed to score the overall quality of a dialogue response. Word overlap-based metrics, which were adopted from the MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018). However, Liu et al. (2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because there are many possible responses to reply to a given context. Recently, learning-based metrics, which aim to predict the overall quality of a response, have a better correlation with human judgment compared with word overlap-based metrics. Various training settings have also been explored. For example, ADEM (Lowe et al., 2017a) is trained to predict the score by learn"
2020.coling-main.368,W18-5713,0,0.0119931,"r the specificity quality (Table 1). Another limitation is the difficulty in enhancing only a particular aspect of the metric. Suppose there is a single metric that can capture both sensibleness and specificity, and a new state-of-the-art metric on the latter quality is subsequently developed; it would be complicated to modify the existing metrics (i.e., BLEU or ADEM) to include this new SOTA metric. Aside from evaluating a response using only a single overall score, some studies evaluate the response on multiple aspects, i.e., fluency, relevancy, specificity, and empathy (Zhang et al., 2018; Weston et al., 2018; Smith et al., 2020). The limitation of this approach is that with multiple scores to consider, it becomes unclear to determine which response is better. Is a specific response more preferable than an empathetic one? To address these issues, we first propose simplifying the various qualities by grouping them into three main aspects: understandability (N¨ubel, 1997), sensibleness (Adiwardana et al., 2020), and likability. We assume these groups have hierarchical properties in the following way: (i) a response first needs to be understandable; (ii) then it needs to make sense to the context; (i"
2020.coling-main.368,D19-1124,0,0.0201178,"UGE (Lin, 2004) metrics are word overlap-based approaches, which utilize the model’s responses and reference responses to compute the number of overlapping words. The more words overlap, the higher the scores are. However, Liu et al. (2016) showed that these metrics have a weak correlation with human judgment. Alternatively, embedding-based metrics (Wieting et al., 2016; Rus and Lintean, 2012; Forgues et al., 2014; Zhang et al., 2020a) are used as measurements in the previous studies, for which the embeddings of context and response are used to obtain the similarity score (Zhang et al., 2018; Zeng et al., 2019). However, due to many possible responses to a context, it is inaccurate to use these metrics. Learning-based metrics have been explored recently, especially with the next utterance prediction setting (Tao et al., 2018; Ghazarian et al., 2019; Lan et al., 2020). The model learns to determine whether or not a context-response pair is valid. It is typically trained with context-response pairs that appear in the dialogue as the positive examples. Then, negative sampling is used to obtain negative examples. Training using this setting demonstrates a moderate correlation with human judgment. Howeve"
2020.coling-main.368,P18-1102,0,0.212783,"ction Evaluating a dialogue response is crucial for the development of open-domain dialogue systems. It allows comparison between different systems, which is similar to how the machine translation community uses BLEU (Papineni et al., 2002) to evaluate the overall quality of the translation and determines whether a system is the state-of-the-art (Bahdanau et al., 2015; Sennrich et al., 2016; Aharoni et al., 2019). Without automatic evaluation metrics, many studies tend to rely on either expert or crowdsourced platform to score the responses, which are both time-consuming and cost-ineffective (Zhang et al., 2018; Zhan et al., 2019; Adiwardana et al., 2020). To cope with this, various metrics have been proposed to score the overall quality of a dialogue response. Word overlap-based metrics, which were adopted from the MT community to measure the overlapping words between reference and candidate sentences, have been used to evaluate the dialogue responses (Sordoni et al., 2015; Zhang et al., 2018). However, Liu et al. (2016) showed that these metrics, i.e., BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), or ROUGE score (Lin, 2004), do not correlate well with human judgements, because t"
2020.coling-main.368,2020.acl-demos.30,0,0.495727,"tively. Figure 1: Decomposition the structure of a response quality. and BERT-RUBER tend to assign a relatively high score to the unspecific response. This might be due to the complexity of the overall score. Generally, a single overall score is usually comprised of different qualities, such as readability, specificity, and empathy, and the importance of each aspect differs according to the task. For example, specificity is preferred in food-ordering chatbots, whereas fluency is preferred in language-teaching chatbots. However, the existing metrics are not flexible to such changes. BERTScore (Zhang et al., 2020a), for example, relies on using pre-trained BERT embedding (Devlin et al., 2019) to compute similarity between reference and candidate responses; thus this does not guarantee good correlation for the specificity quality (Table 1). Another limitation is the difficulty in enhancing only a particular aspect of the metric. Suppose there is a single metric that can capture both sensibleness and specificity, and a new state-of-the-art metric on the latter quality is subsequently developed; it would be complicated to modify the existing metrics (i.e., BLEU or ADEM) to include this new SOTA metric. A"
2020.coling-main.580,D13-1160,0,0.0573825,"op questions in MRC domain Currently, four multi-hop MRC datasets proposed for textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). Recently, Chen et al. (2020) introduced the 6617 HybridQA dataset—a multi-hop question answering over both tabular and textual data. The dataset was created by crowdsourcing based on Wikipedia tables and Wikipedia articles. Multi-hop questions in KB domain Question answering over the knowledge graph has been investigated for decades. However, most current datasets (Berant et al., 2013; Bordes et al., 2015; Yih et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop). Zhang et al. (2018b) introduced the METAQA dataset that contains both single-hop and multi-hop questions. Abujabal et al. (2017) introduced the ComplexQuestions dataset comprising 150 compositional questions. All of these datasets are solved by using the KB only. Our dataset is constructed based on the intersection between Wikipedia and Wikidata. Therefore, it can be solved by using structured or unstructured data. Compositional Knowledge Base Inference Extracting Horn rules from the KB"
2020.coling-main.580,N19-1405,0,0.018769,"(from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the reasoning and inference skills of the model. Further, data analyses (Chen and Durrett, 2019; Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve. Recently, to evaluate the internal reasoning of the reading comprehension system, Inoue et al. (2020) proposed a new dataset R4 C that requires systems to provide an answer and derivations. A derivation This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://rajpurkar.github.io/SQuAD-explorer/ License details: http:// 6609 Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–66"
2020.coling-main.580,P17-1171,0,0.0172741,"for bridge questions in our dataset. (3) Post-process Generated Data: We randomly selected two entities to create a question when generating the data; therefore, a large number of no questions exist in the yes/no questions. We performed post-processing to finalize the dataset that balances the number of yes and no questions. Questions could have several true answers in the real world. To ensure one sample has only one answer, we discarded all ambiguous cases in the dataset (Appendix A.7). Collect Distractor Paragraphs: Following Yang et al. (2018) and Min et al. (2019), we used bigram tf-idf (Chen et al., 2017) to retrieve the top-50 paragraphs from Wikipedia that are most similar to the question. Then, we used the entity type of the two gold paragraphs (four gold paragraphs for bridgecomparison question) to select the top-8 paragraphs (top-6 for bridge-comparison question) and considered it as a set of distractor paragraphs. We shuffled the 10 paragraphs (including gold and distractor paragraphs) and obtained a context. Dataset Statistics (A Benchmark Setting): We used a single-hop model (Section 5.1) to split the train, dev, and test sets. We conducted five-fold cross-validation on all data. The a"
2020.coling-main.580,2020.findings-emnlp.91,0,0.0398754,"Missing"
2020.coling-main.580,N19-1423,0,0.0282641,"s of a model. We carefully design a pipeline and a set of templates when generating a question–answer pair that guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. 1 Introduction Machine reading comprehension (MRC) aims at teaching machines to read and understand given text. Many current models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1 . However, such performances do not indicate that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-"
2020.coling-main.580,2020.acl-main.602,0,0.160115,"ethod, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the r"
2020.coling-main.580,C16-1278,0,0.0281822,"Missing"
2020.coling-main.580,D17-1215,0,0.0206477,"ough experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. 1 Introduction Machine reading comprehension (MRC) aims at teaching machines to read and understand given text. Many current models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1 . However, such performances do not indicate that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). The fi"
2020.coling-main.580,P14-5010,0,0.00640319,"Missing"
2020.coling-main.580,P19-1416,0,0.265845,"with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a binary classification task that is incapable of evaluating the reasoning and inference skills of the model. Further, data analyses (Chen and Durrett, 2019; Min et al., 2019) revealed that many examples in HotpotQA do not require multi-hop reasoning to solve. Recently, to evaluate the internal reasoning of the reading comprehension system, Inoue et al. (2020) proposed a new dataset R4 C that requires systems to provide an answer and derivations. A derivation This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://rajpurkar.github.io/SQuAD-explorer/ License details: http:// 6609 Proceedings of the 28th International Conference on Computational Linguistics, pages 6609–6625 Barcelona, Spain"
2020.coling-main.580,P19-1487,0,0.0413725,"Missing"
2020.coling-main.580,D16-1264,0,0.0294627,"hat guarantees the multi-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. 1 Introduction Machine reading comprehension (MRC) aims at teaching machines to read and understand given text. Many current models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1 . However, such performances do not indicate that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a q"
2020.coling-main.580,P18-2124,0,0.025941,"-hop steps and the quality of the questions. We also exploit the structured format in Wikidata and use logical rules to create questions that are natural but still require multi-hop reasoning. Through experiments, we demonstrate that our dataset is challenging for multi-hop models and it ensures that multi-hop reasoning is required. 1 Introduction Machine reading comprehension (MRC) aims at teaching machines to read and understand given text. Many current models (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) have defeated humans on the performance of SQuAD (Rajpurkar et al., 2016; Rajpurkar et al., 2018), as shown on its leaderboard1 . However, such performances do not indicate that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there"
2020.coling-main.580,D10-1106,0,0.0385876,"7) introduced the ComplexQuestions dataset comprising 150 compositional questions. All of these datasets are solved by using the KB only. Our dataset is constructed based on the intersection between Wikipedia and Wikidata. Therefore, it can be solved by using structured or unstructured data. Compositional Knowledge Base Inference Extracting Horn rules from the KB has been studied extensively in the Inductive Logic Programming literature (Quinlan, 1990; Muggleton, 1995). From the KB, there are several approaches that mine association rules (Agrawal et al., 1993) and several mine logical rules (Schoenmackers et al., 2010; Gal´arraga et al., 2013). We observed that these rules can be used to test the reasoning skill of the model. Therefore, in this study, we utilized the logical rules in the form: r1 (a, b) ∧ r2 (b, c) ⇒ r(a, c). ComplexWebQuestions and QAngaroo datasets are also utilized KB when constructing the dataset, but they do not utilize the logical rules as we did. RC datasets with explanations Table 8 presents several existing datasets that provide explanations. HotpotQA and R4 C are the most similar works to ours. HotpotQA provides a justification explanation (collections of evidence to support the"
2020.coling-main.580,N18-1059,0,0.0909986,"dicate that these models can completely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classify"
2020.coling-main.580,Q18-1021,0,0.103867,"ely understand the text. Specifically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a bin"
2020.coling-main.580,D18-1259,0,0.340761,"ically, using an adversarial method, Jia and Liang (2017) demonstrated that the current models do not precisely understand natural language. Moreover, Sugawara et al. (2018) demonstrated that many datasets contain a considerable number of easy instances that can be answered based on the first few words of the questions. Multi-hop MRC datasets require a model to read and perform multi-hop reasoning over multiple paragraphs to answer a question. Currently, there are four multi-hop datasets over textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). The first two datasets were created by incorporating the documents (from Web or Wikipedia) with a knowledge base (KB). Owing to their building procedures, these datasets have no information to explain the predicted answers. Meanwhile, the other two datasets were created mainly based on crowdsourcing. In HotpotQA, the authors introduced the sentence-level supporting facts (SFs) information that are used to explain the predicted answers. However, as discussed in Inoue et al. (2020), the task of classifying sentence-level SFs is a binary classification task that i"
2020.coling-main.580,P15-1128,0,0.0277961,"multi-hop MRC datasets proposed for textual data: ComplexWebQuestions (Talmor and Berant, 2018), QAngaroo (Welbl et al., 2018), HotpotQA (Yang et al., 2018), and R4 C (Inoue et al., 2020). Recently, Chen et al. (2020) introduced the 6617 HybridQA dataset—a multi-hop question answering over both tabular and textual data. The dataset was created by crowdsourcing based on Wikipedia tables and Wikipedia articles. Multi-hop questions in KB domain Question answering over the knowledge graph has been investigated for decades. However, most current datasets (Berant et al., 2013; Bordes et al., 2015; Yih et al., 2015; Diefenbach et al., 2017) consist of simple questions (single-hop). Zhang et al. (2018b) introduced the METAQA dataset that contains both single-hop and multi-hop questions. Abujabal et al. (2017) introduced the ComplexQuestions dataset comprising 150 compositional questions. All of these datasets are solved by using the KB only. Our dataset is constructed based on the intersection between Wikipedia and Wikidata. Therefore, it can be solved by using structured or unstructured data. Compositional Knowledge Base Inference Extracting Horn rules from the KB has been studied extensively in the Ind"
2020.findings-emnlp.420,W14-4012,0,0.0673636,"Missing"
2020.findings-emnlp.420,N19-1423,0,0.0243099,"nal layer of the 1/2 resolution map, C2 is of 1/4, ..., C5 is of 1/32. Top-down Reconstruction FPN makes feature pyramids from the stages of a backbone in a topdown manner. Suppose that CNV(2) , ..., CNV(5) are trainable convolutional layers and P2, ..., P5 (P stands for pyramid) are the reconstructed feature maps on each stage2 . Then Pi can be represented as follows: 5.1 Language Encoder LCFP requires a fixedlength vector of language information to generate input for FiLM blocks. We can use any fixed vector, such as the last hidden layers of RNNs or transformer-based language models such as Devlin et al. (2019). Our proposal adopts gated recurrent unit (GRU) (Cho et al., 2014) in accordance with the FiLM paper (Perez et al., 2018). Suppose that dlang is the dimension of hidden layer, Pi = CNV(i) (Ci) + Resize2 (P(i + 1)) (2). hlang = GRU(text) ∈ Rdlang . where P6 = 0 and Resize2 denotes the operation to enlarge the image twice. This means that Pi contains information about higher and coarser stages, which hold more complex semantics in general because of their wider receptive fields. 5 Visual Feature Extractor We use ResNet as our backbone. In addition to the C2-C5 described in Section 4.2, we use C"
2020.findings-emnlp.420,D13-1038,0,0.0161008,"cture fuses linguistic features with each visual feature. Introduction Referring expressions are a ubiquitous part of human communication (Krahmer and Van Deemter, 2012) that must be studied in order to create machines that work smoothly with humans. Much effort has been taken to improve methods of creating visual common ground between machines, which have limited means of expression and knowledge about the real world, and humans, from the perspectives of both referring expression comprehension and generation (Moratz et al., 2002; Tenbrink and Moratz, 2003; Funakoshi et al., 2004, 2005, 2006; Fang et al., 2013). Even now, researchers are exploring possible methods of designing more realistic scenarios for applications, such as in visual dialogue games (De Vries et al., 2017; Haber et al., 2019; Udagawa and Aizawa, 2019). Many models have been proposed for referring expression comprehension so far. As image recognition matured, Guadarrama et al. (2014) studied object retrieval methods based on category labels predicted by the recognition models. Hu et al. (2016b) extended this approach to broader natural language expression including categories of objects, their attributes, positional configurations,"
2020.findings-emnlp.420,C04-1096,0,0.118981,"f their visual features. Our architecture fuses linguistic features with each visual feature. Introduction Referring expressions are a ubiquitous part of human communication (Krahmer and Van Deemter, 2012) that must be studied in order to create machines that work smoothly with humans. Much effort has been taken to improve methods of creating visual common ground between machines, which have limited means of expression and knowledge about the real world, and humans, from the perspectives of both referring expression comprehension and generation (Moratz et al., 2002; Tenbrink and Moratz, 2003; Funakoshi et al., 2004, 2005, 2006; Fang et al., 2013). Even now, researchers are exploring possible methods of designing more realistic scenarios for applications, such as in visual dialogue games (De Vries et al., 2017; Haber et al., 2019; Udagawa and Aizawa, 2019). Many models have been proposed for referring expression comprehension so far. As image recognition matured, Guadarrama et al. (2014) studied object retrieval methods based on category labels predicted by the recognition models. Hu et al. (2016b) extended this approach to broader natural language expression including categories of objects, their attrib"
2020.findings-emnlp.420,W06-1411,0,0.159281,"Missing"
2020.findings-emnlp.420,P19-1184,0,0.0438822,"Missing"
2020.findings-emnlp.420,D14-1086,0,0.0289828,"matches the expression a gray dot, it does not capture information about the triangle consisting of three dots to the upper left. Conversely, the largest receptive field (in the panel to the right) includes the triangle, but contains too much information to determine the color of the gray dot. Thus, linguistic and visual features have an optimum size of receptive field for fusion. Few existing models, however, use fusion of linguistic features with visual features with different receptive field sizes. This is possibly because major datasets for referring expression comprehension, for example, Kazemzadeh et al. (2014); Plummer et al. (2015); Mao et al. (2016); Yu et al. (2016), use photographs and weigh expressions related to object category more often than positional relationships. Tenbrink and Moratz (2003); Tanaka et al. (2004); Liu et al. (2012, 2013) reveal that people often use group-based expressions (relative positional relationships of multiple objects) when there is no clear difference between objects; therefore, these expressions are not so unusual. Further investigation should be done on methods that handle referring expressions based on positional relationships. For this reason, we focus on th"
2020.findings-emnlp.420,J12-1006,0,0.0681295,"Missing"
2020.findings-emnlp.420,W12-1621,0,0.0336143,"information to determine the color of the gray dot. Thus, linguistic and visual features have an optimum size of receptive field for fusion. Few existing models, however, use fusion of linguistic features with visual features with different receptive field sizes. This is possibly because major datasets for referring expression comprehension, for example, Kazemzadeh et al. (2014); Plummer et al. (2015); Mao et al. (2016); Yu et al. (2016), use photographs and weigh expressions related to object category more often than positional relationships. Tenbrink and Moratz (2003); Tanaka et al. (2004); Liu et al. (2012, 2013) reveal that people often use group-based expressions (relative positional relationships of multiple objects) when there is no clear difference between objects; therefore, these expressions are not so unusual. Further investigation should be done on methods that handle referring expressions based on positional relationships. For this reason, we focus on the OneCommon corpus (Udagawa and Aizawa, 2019), a recently proposed corpus on a visual dialogue game using composite images of simple figures. It captures various expressions based on positional relationships, such as group-based expres"
2020.findings-emnlp.420,W13-4010,0,0.0376932,"Missing"
2020.findings-emnlp.420,D18-1287,0,0.0520643,"Missing"
2020.findings-emnlp.67,W17-5526,0,0.060795,"Missing"
2020.findings-emnlp.67,P19-1648,0,0.0282022,"illiams et al., 2016; El Asri et al., 2017). However, most prior work focus on dialogues where information is not grounded in external, perceptual modality such as vision. In this work, we propose an effective method of analyzing linguistic structures in visually grounded dialogues. Recent years have witnessed an increasing attention in visually grounded dialogues (Zarrieß et al., 2016; de Vries et al., 2018; Alamri et al., 2019; Narayan-Chen et al., 2019). Despite the impressive progress on benchmark scores and model architec751 tures (Das et al., 2017b; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Shukla et al., 2019; Niu et al., 2019; Zheng et al., 2019; Kang et al., 2019; Murahari et al., 2019; Pang and Wang, 2020), there have also been critical problems pointed out in terms of dataset biases (Goyal et al., 2017; Chattopadhyay et al., 2017; Massiceti et al., 2018; Chen et al., 2018; Kottur et al., 2019; Kim et al., 2020; Agarwal et al., 2020) which obscure such contributions. For instance, Cirik et al. (2018) points out that existing dataset of reference resolution may be largely solvable without recognizing the full referring expressions (e.g. based on object categories only). To c"
2020.findings-emnlp.67,W18-1405,0,0.0129193,"ble context. In this dataset, the visual contexts are kept simple and controllable to remove undesirable biases while enhancing linguistic variety. In total, 5,191 natural dialogues are collected and fully annotated with referring expressions (which they called markables) and their referents, which can be leveraged for further linguistic analysis. • To capture the linguistic structures in these dialogues, we propose to annotate spatial expressions which play a central role in visually grounded dialogues. We take inspiration from the existing annotation frameworks (Pustejovsky et al., 2011a,b; Petruck and Ellsworth, 2018; Ulinski et al., 2019) but also make several simplifications and modifications to improve coverage, 750 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 750–765 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1: Example dialogue from OneCommon Corpus with reference resolution annotation (left) and our spatial expression annotation (right). We consider spatial expressions as predicates and annotate their arguments as well as modifiers. For further details of the original dataset and our annotation schema, see Section 3. efficiency and"
2020.findings-emnlp.67,W18-1403,0,0.0167052,"us on canonicalizing the central spatial relations in this work: we do not canonicalize spatial attributes manually, since we can canonicalize the central spatial attributes automatically (c.f. Subsubsection 4.2.1). According to Landau (2017), there are 2 classes of relations in spatial language: functional class whose core meanings engage force-dynamic relationship (such as on, in) and geometric class whose core meanings engage geometry (such as left, above). Since functional relations are less common in this dataset and more difficult to define due to their vagueness and context dependence (Platonov and Schubert, 2018), we focus on the following 5 categories of geometric relations and attribute comparisons, including a total of 24 canonical relations which can be defined explicitly. Direction requires the subjects and objects to be placed in certain orientation: left, right, above, below, horizontal, vertical, diagonal. Proximity is related to distance between subjects, objects or other entities: near, far, alone. 753 Region restricts the subjects to be in a certain region specified by the objects: interior, exterior. Color comparison is related to comparison of color between subjects and objects: lighter,"
2020.findings-emnlp.67,D19-1462,0,0.0283894,"mon to facilitate future model development and analyses. 2 Related Work Linguistic structure plays a critical role in dialogue research. From theoretical aspects, various dialogue structures have been studied, including discourse structure (Stent, 2000; Asher et al., 2003), speech act (Austin, 1962; Searle, 1969) and common grounding (Clark, 1996; Lascarides and Asher, 2009). In dialogue system engineering, various linguistic structures have been considered and applied, including syntactic dependency (Davidson et al., 2019), predicate-argument structure (PAS) (Yoshino et al., 2011), ellipsis (Quan et al., 2019; Hansen and Søgaard, 2020), intent recognition (Silva et al., 2011; Shi et al., 2016), semantic representation/parsing (Mesnil et al., 2013; Gupta et al., 2018) and frame-based dialogue state tracking (Williams et al., 2016; El Asri et al., 2017). However, most prior work focus on dialogues where information is not grounded in external, perceptual modality such as vision. In this work, we propose an effective method of analyzing linguistic structures in visually grounded dialogues. Recent years have witnessed an increasing attention in visually grounded dialogues (Zarrieß et al., 2016; de Vri"
2020.findings-emnlp.67,D15-1022,0,0.0460295,"Missing"
2020.findings-emnlp.67,2020.acl-main.442,0,0.0622034,"s. For instance, Cirik et al. (2018) points out that existing dataset of reference resolution may be largely solvable without recognizing the full referring expressions (e.g. based on object categories only). To circumvent these issues, we focus on OneCommon Corpus where the visual contents are simple (exploitable categories are removed) and well-balanced (by sampling from uniform distributions) to minimize dataset biases. Although various probing methods have been proposed for models and datasets in NLP (Belinkov and Glass, 2019; Geva et al., 2019; Kaushik et al., 2020; Gardner et al., 2020; Ribeiro et al., 2020), fine-grained analyses of visually grounded dialogues have been relatively limited. Instead, Kottur et al. (2019) proposed a diagnostic dataset to investigate model’s language understanding: however, their dialogues are generated artificially and may not reflect the true nature of visual dialogues. Shekhar et al. (2019) also acknowledges the importance of linguistic analysis but only dealt with coarse-level features that can be computed automatically (such as dialogue topic and diversity). Most similar and related to our research are Yu et al. (2019); Udagawa and Aizawa (2020), where they con"
2020.findings-emnlp.67,N19-1265,0,0.0311775,"Missing"
2020.findings-emnlp.67,N16-1176,0,0.0539799,"Missing"
2020.findings-emnlp.67,P19-1646,0,0.0255154,"16; El Asri et al., 2017). However, most prior work focus on dialogues where information is not grounded in external, perceptual modality such as vision. In this work, we propose an effective method of analyzing linguistic structures in visually grounded dialogues. Recent years have witnessed an increasing attention in visually grounded dialogues (Zarrieß et al., 2016; de Vries et al., 2018; Alamri et al., 2019; Narayan-Chen et al., 2019). Despite the impressive progress on benchmark scores and model architec751 tures (Das et al., 2017b; Wu et al., 2018; Kottur et al., 2018; Gan et al., 2019; Shukla et al., 2019; Niu et al., 2019; Zheng et al., 2019; Kang et al., 2019; Murahari et al., 2019; Pang and Wang, 2020), there have also been critical problems pointed out in terms of dataset biases (Goyal et al., 2017; Chattopadhyay et al., 2017; Massiceti et al., 2018; Chen et al., 2018; Kottur et al., 2019; Kim et al., 2020; Agarwal et al., 2020) which obscure such contributions. For instance, Cirik et al. (2018) points out that existing dataset of reference resolution may be largely solvable without recognizing the full referring expressions (e.g. based on object categories only). To circumvent these issue"
2020.findings-emnlp.67,W19-1607,0,0.0294429,"the visual contexts are kept simple and controllable to remove undesirable biases while enhancing linguistic variety. In total, 5,191 natural dialogues are collected and fully annotated with referring expressions (which they called markables) and their referents, which can be leveraged for further linguistic analysis. • To capture the linguistic structures in these dialogues, we propose to annotate spatial expressions which play a central role in visually grounded dialogues. We take inspiration from the existing annotation frameworks (Pustejovsky et al., 2011a,b; Petruck and Ellsworth, 2018; Ulinski et al., 2019) but also make several simplifications and modifications to improve coverage, 750 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 750–765 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Figure 1: Example dialogue from OneCommon Corpus with reference resolution annotation (left) and our spatial expression annotation (right). We consider spatial expressions as predicates and annotate their arguments as well as modifiers. For further details of the original dataset and our annotation schema, see Section 3. efficiency and reliability. 1 As show"
2020.findings-emnlp.67,W11-2008,0,0.0315107,"tps: //github.com/Alab-NII/onecommon to facilitate future model development and analyses. 2 Related Work Linguistic structure plays a critical role in dialogue research. From theoretical aspects, various dialogue structures have been studied, including discourse structure (Stent, 2000; Asher et al., 2003), speech act (Austin, 1962; Searle, 1969) and common grounding (Clark, 1996; Lascarides and Asher, 2009). In dialogue system engineering, various linguistic structures have been considered and applied, including syntactic dependency (Davidson et al., 2019), predicate-argument structure (PAS) (Yoshino et al., 2011), ellipsis (Quan et al., 2019; Hansen and Søgaard, 2020), intent recognition (Silva et al., 2011; Shi et al., 2016), semantic representation/parsing (Mesnil et al., 2013; Gupta et al., 2018) and frame-based dialogue state tracking (Williams et al., 2016; El Asri et al., 2017). However, most prior work focus on dialogues where information is not grounded in external, perceptual modality such as vision. In this work, we propose an effective method of analyzing linguistic structures in visually grounded dialogues. Recent years have witnessed an increasing attention in visually grounded dialogues"
2020.findings-emnlp.67,D19-1516,0,0.0242962,"k et al., 2020; Gardner et al., 2020; Ribeiro et al., 2020), fine-grained analyses of visually grounded dialogues have been relatively limited. Instead, Kottur et al. (2019) proposed a diagnostic dataset to investigate model’s language understanding: however, their dialogues are generated artificially and may not reflect the true nature of visual dialogues. Shekhar et al. (2019) also acknowledges the importance of linguistic analysis but only dealt with coarse-level features that can be computed automatically (such as dialogue topic and diversity). Most similar and related to our research are Yu et al. (2019); Udagawa and Aizawa (2020), where they conducted additional annotation of reference resolution in visual dialogues: however, they still do not capture more sophisticated linguistic structures such as PAS, modification and ellipsis. tial prepositions in single sentences. To the best of our knowledge, our work is the first to apply, model and analyze spatial expressions in visually grounded dialogues at full scale. 3 Annotation 3.1 Dataset Our work extends OneCommon Corpus originally proposed in Udagawa and Aizawa (2019). In this task, two players A and B are given slightly different, overlappi"
2020.findings-emnlp.67,L16-1019,0,0.066426,"Missing"
2020.lrec-1.212,W15-0915,0,0.0143419,"r, evaluation datasets are expensive and time-consuming to build. To alleviate this issue, only smaller portions of papers, such as the abstract (Wu et al., 2006; Hirohata et al., 2008; Dayrell et al., 2012) or introduction (Pendar and Cotos, 2008), were annotated, and a limited number of disciplines were used (Cortes, 2013; Mizumoto et al., 2017). The bottom-up approach is not much better, because there is no established evaluation dataset for detecting formulaic expressions. Previous work, therefore, relied on domain experts to manually assess the quality of extracted formulaic expressions (Brooke et al., 2015; Iwatsuki and Aizawa, 2018), which, in addition to being costly, hinders replicability. Overall, the unavailability of annotated resources for both communicative functions and formulaic expressions has hindered the development of automated methods for detecting communicative functions. There are, nonetheless, closely related resources for academic writing, in which examples of phrases and wordings are collected and classified into communicative functions. Academic Phrasebank (Morley, 2014) is one of them. However, the use of this resource as a ground-truth dataset is not 1712 straightforward,"
2020.lrec-1.212,N15-3022,0,0.0221273,"er systems have been proposed to automate this process. In most cases, phrases or wordings were extracted from linguistic resources and recorded in a database in advance, and a system searches for one of them based on the users’ writing. In order to extract frequently used word n-grams, Jeong et al. (2014) relied on PubMed structured abstracts as a resource, in which sentences are labelled with the following functions: introduction, methods, results and discussion. However, this convention of writing abstracts is specific to PubMed; thus, this work will not be applicable to other disciplines. Chang and Chang (2015) and Yen et al. (2015) extracted grammatical phrase patterns from an English dictionary, rather than word n-grams from corpora. The system4 they proposed is useful to find a correct usage of specific words. Liu et al. (2016) extracted frequent word n-grams from Elsevier’s ScienceDirect and paraphrased them using WordNet synonyms to extend their database. Despite the differences in the methods used to create databases, the method of recommendation of phrases and wordings is similar among the systems mentioned here. When a user writes something, all systems show examples or phrases that follow t"
2020.lrec-1.212,L18-1269,0,0.0138162,"u et al. (2017) found that even a supervised method trained on a dataset for natural language inference yielded universal sentence representations that perform well on various tasks. The current trend in the acquisition of sentence representations is the use of outputs from pre-trained language models such as BERT (Devlin et al., 2019). In any case, sentence representations for general purposes do not always contain every aspect of languages. Hence, it is important to investigate which linguistic aspects they contain and comprehensive evaluation benchmarks have been proposed for this purpose (Conneau and Kiela, 2018; Wang et al., 2018). These benchmarks can well evaluate sentence representations in terms of semantic factors such as semantic relatedness, paraphrases and caption-image retrieval as well as logical factors such as entailment. Communicative functions, which the present paper focuses on, are another perspective related to rhetorical structure. Basically, the discourse structure is realised in multiple sentences, but a sentence can play the role of a rhetorical unit to make discourse. Therefore, rhetorical information embedded in sentence representation is worth evaluating. 3. 3.1. Most researc"
2020.lrec-1.212,D17-1070,0,0.0145633,"s of linguistic units ranging from multiple sentences to phrases. Furthermore, Lor´es (2004) investigated the usage of Theme and Rheme in abstracts of scholarly articles. 2.3. Sentence Representations and Their Evaluation Since the sentence is one of the fundamental units of languages, vector representations of sentences have attracted much research attention. Following successful word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), unsupervised methods to acquire sentence embeddings, such as Skip-Thought Vectors (Kiros et al., 2015) have been proposed. Conneau et al. (2017) found that even a supervised method trained on a dataset for natural language inference yielded universal sentence representations that perform well on various tasks. The current trend in the acquisition of sentence representations is the use of outputs from pre-trained language models such as BERT (Devlin et al., 2019). In any case, sentence representations for general purposes do not always contain every aspect of languages. Hence, it is important to investigate which linguistic aspects they contain and comprehensive evaluation benchmarks have been proposed for this purpose (Conneau and Kie"
2020.lrec-1.212,dayrell-etal-2012-rhetorical,0,0.108872,"thor’s purpose or intention (Swales, 1990; Swales, 2004). For example, the formulaic expression ‘in this paper we propose’ has the communicative function ‘stating the aim of the paper’. There is, however, no consensus as to what constitutes the minimal text span that realises a communicative function. For example, to convey the function ‘describing the limitations of current research’, some may regard ‘beyond the scope’ as the minimal formulaic expression, while others may consider a larger span such as ‘is beyond the scope of this paper’. Here, we follow past research (Hirohata et al., 2008; Dayrell et al., 2012; Fiacco et al., 2019) and deal with this issue by regarding the whole sentence as the minimal unit of a communicative function. Formulaic expressions and their communicative functions have been investigated mainly in academic writing research to help people write papers more rapidly and fluently (Cortes, 2013; Mizumoto et al., 2017; Omidian et al., 2018). There even exist some computer systems for academic-writing assistance12 that rely on these communicative functions to improve the user’s writing skills by suggesting commonly-used, alternative formulaic expressions. This is especially helpf"
2020.lrec-1.212,N19-1423,0,0.0670733,"have attracted much research attention. Following successful word embeddings such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), unsupervised methods to acquire sentence embeddings, such as Skip-Thought Vectors (Kiros et al., 2015) have been proposed. Conneau et al. (2017) found that even a supervised method trained on a dataset for natural language inference yielded universal sentence representations that perform well on various tasks. The current trend in the acquisition of sentence representations is the use of outputs from pre-trained language models such as BERT (Devlin et al., 2019). In any case, sentence representations for general purposes do not always contain every aspect of languages. Hence, it is important to investigate which linguistic aspects they contain and comprehensive evaluation benchmarks have been proposed for this purpose (Conneau and Kiela, 2018; Wang et al., 2018). These benchmarks can well evaluate sentence representations in terms of semantic factors such as semantic relatedness, paraphrases and caption-image retrieval as well as logical factors such as entailment. Communicative functions, which the present paper focuses on, are another perspective r"
2020.lrec-1.212,I08-1050,0,0.226388,", which indicate the author’s purpose or intention (Swales, 1990; Swales, 2004). For example, the formulaic expression ‘in this paper we propose’ has the communicative function ‘stating the aim of the paper’. There is, however, no consensus as to what constitutes the minimal text span that realises a communicative function. For example, to convey the function ‘describing the limitations of current research’, some may regard ‘beyond the scope’ as the minimal formulaic expression, while others may consider a larger span such as ‘is beyond the scope of this paper’. Here, we follow past research (Hirohata et al., 2008; Dayrell et al., 2012; Fiacco et al., 2019) and deal with this issue by regarding the whole sentence as the minimal unit of a communicative function. Formulaic expressions and their communicative functions have been investigated mainly in academic writing research to help people write papers more rapidly and fluently (Cortes, 2013; Mizumoto et al., 2017; Omidian et al., 2018). There even exist some computer systems for academic-writing assistance12 that rely on these communicative functions to improve the user’s writing skills by suggesting commonly-used, alternative formulaic expressions. Th"
2020.lrec-1.212,C18-1227,1,0.813161,"s are expensive and time-consuming to build. To alleviate this issue, only smaller portions of papers, such as the abstract (Wu et al., 2006; Hirohata et al., 2008; Dayrell et al., 2012) or introduction (Pendar and Cotos, 2008), were annotated, and a limited number of disciplines were used (Cortes, 2013; Mizumoto et al., 2017). The bottom-up approach is not much better, because there is no established evaluation dataset for detecting formulaic expressions. Previous work, therefore, relied on domain experts to manually assess the quality of extracted formulaic expressions (Brooke et al., 2015; Iwatsuki and Aizawa, 2018), which, in addition to being costly, hinders replicability. Overall, the unavailability of annotated resources for both communicative functions and formulaic expressions has hindered the development of automated methods for detecting communicative functions. There are, nonetheless, closely related resources for academic writing, in which examples of phrases and wordings are collected and classified into communicative functions. Academic Phrasebank (Morley, 2014) is one of them. However, the use of this resource as a ground-truth dataset is not 1712 straightforward, as it was made with the pur"
2020.lrec-1.212,W08-0908,0,0.0542873,"om-up approach, in which formulaic expressions are first extracted from a corpus and their communicative functions are subsequently identified. With either approach, problems arise when computational methods are applied to create the lists. For the top-down approach, no evaluation dataset is publicly available for classifying sentences into communicative functions. Moreover, evaluation datasets are expensive and time-consuming to build. To alleviate this issue, only smaller portions of papers, such as the abstract (Wu et al., 2006; Hirohata et al., 2008; Dayrell et al., 2012) or introduction (Pendar and Cotos, 2008), were annotated, and a limited number of disciplines were used (Cortes, 2013; Mizumoto et al., 2017). The bottom-up approach is not much better, because there is no established evaluation dataset for detecting formulaic expressions. Previous work, therefore, relied on domain experts to manually assess the quality of extracted formulaic expressions (Brooke et al., 2015; Iwatsuki and Aizawa, 2018), which, in addition to being costly, hinders replicability. Overall, the unavailability of annotated resources for both communicative functions and formulaic expressions has hindered the development o"
2020.lrec-1.212,D14-1162,0,0.0825409,"Missing"
2020.lrec-1.212,W18-5446,0,0.0258209,"t even a supervised method trained on a dataset for natural language inference yielded universal sentence representations that perform well on various tasks. The current trend in the acquisition of sentence representations is the use of outputs from pre-trained language models such as BERT (Devlin et al., 2019). In any case, sentence representations for general purposes do not always contain every aspect of languages. Hence, it is important to investigate which linguistic aspects they contain and comprehensive evaluation benchmarks have been proposed for this purpose (Conneau and Kiela, 2018; Wang et al., 2018). These benchmarks can well evaluate sentence representations in terms of semantic factors such as semantic relatedness, paraphrases and caption-image retrieval as well as logical factors such as entailment. Communicative functions, which the present paper focuses on, are another perspective related to rhetorical structure. Basically, the discourse structure is realised in multiple sentences, but a sentence can play the role of a rhetorical unit to make discourse. Therefore, rhetorical information embedded in sentence representation is worth evaluating. 3. 3.1. Most researchers investigating X"
2020.lrec-1.212,P06-4011,0,0.0458941,"laic expressions are subsequently extracted from the sentences, and 2) the bottom-up approach, in which formulaic expressions are first extracted from a corpus and their communicative functions are subsequently identified. With either approach, problems arise when computational methods are applied to create the lists. For the top-down approach, no evaluation dataset is publicly available for classifying sentences into communicative functions. Moreover, evaluation datasets are expensive and time-consuming to build. To alleviate this issue, only smaller portions of papers, such as the abstract (Wu et al., 2006; Hirohata et al., 2008; Dayrell et al., 2012) or introduction (Pendar and Cotos, 2008), were annotated, and a limited number of disciplines were used (Cortes, 2013; Mizumoto et al., 2017). The bottom-up approach is not much better, because there is no established evaluation dataset for detecting formulaic expressions. Previous work, therefore, relied on domain experts to manually assess the quality of extracted formulaic expressions (Brooke et al., 2015; Iwatsuki and Aizawa, 2018), which, in addition to being costly, hinders replicability. Overall, the unavailability of annotated resources fo"
2020.lrec-1.212,P15-4024,0,0.0197708,"ed to automate this process. In most cases, phrases or wordings were extracted from linguistic resources and recorded in a database in advance, and a system searches for one of them based on the users’ writing. In order to extract frequently used word n-grams, Jeong et al. (2014) relied on PubMed structured abstracts as a resource, in which sentences are labelled with the following functions: introduction, methods, results and discussion. However, this convention of writing abstracts is specific to PubMed; thus, this work will not be applicable to other disciplines. Chang and Chang (2015) and Yen et al. (2015) extracted grammatical phrase patterns from an English dictionary, rather than word n-grams from corpora. The system4 they proposed is useful to find a correct usage of specific words. Liu et al. (2016) extracted frequent word n-grams from Elsevier’s ScienceDirect and paraphrased them using WordNet synonyms to extend their database. Despite the differences in the methods used to create databases, the method of recommendation of phrases and wordings is similar among the systems mentioned here. When a user writes something, all systems show examples or phrases that follow the user’s input. For e"
2020.sdp-1.16,E06-1002,0,0.00895393,"description for an output vector. In our grounding, instead of directly associating each math word to a text description, we put an intermediate procedure: making groups each of which consists of math words referring to an identical mathematical concept. For instance in Figure 1, the first and the third y belong to a group because they both refer to the same function, while the second one is in another group because it refers to a vector. It is notable that the grounding is similar to some established tasks, namely coreference resolution (Sukthanker et al., 2020) and named entity recognition (Bunescu and Pasca, 2006). In this work, we checked the feasibility of the proposing task direction for the grounding. For this purpose, we made a long annotated scientific paper in which all formulae are annotated with math word spans and text descriptions of the corresponding mathematical concepts. The math words in the annotated paper which refer to the same mathematical concept are tied together in a group. We did the annotation for an entire paper rather than small fragments of texts to disclose the flexibility of math word usage. Through the analysis of our annotated paper, we revealed that the meanings of math"
2020.sdp-1.16,N18-1028,0,0.0278746,"the natural logarithm of the value ?”. Though their purpose is close to ours, we annotated not only descriptions but also a few pieces of additional information, i.e., affix types and group information (what concept the word refer to). In the terms of linguistics, these two can be regarded respectively as word spans and coreference information. Additionally, we did the annotation for a longer document than their target papers with coherency. We were especially interested in longer documents so that we can analyze how diverse meanings of mathematical concepts can be. The variable typing task (Stathopoulos et al., 2018) is also closely relevant to our goal. Their task is simply associating mathematical type (technical terms referring to mathematical concepts) to each variable in STEM documents. For example, for a sentence 139 Let ? be a parabolic subgroup of GL(?) with Levi decomposition ? = ? ?, where ? is the unipotent radical. (Stathopoulos et al., 2018) they assigned the “parabolic subgroup” and “unipotent radical” respectively to variables ? and ? as their mathematical types. Based on arXMLiv, they introduced their own dataset, which includes 33,524 labeled variables in 7,803 sentences. Their work reThe"
2020.wosp-1.1,C14-1008,0,0.0169248,"re not updated or re-classified. Finally, for many domains, expert classifications simply do not exist. With VCP, the problems could be overcome. (Virtual) citation proximity (1) covers many types of relatedness; (2) allows documents to be in unlimited numbers of co-citation clusters; (3) has no limitations for the number of clusters; (4) is dynamic; and (5) can be learned for any domain that uses citations. In recent years advances in deep-learning have shown the ability to identify complex patterns in text based data in areas such as translation (Wu et al., 2016) and sentiment analysis (Dos Santos and Gatti, 2014). A document embedding (Le and Mikolov, 2014; Dai et al., 2015) is an embedding representing an entire document trained using a paragraph embedding model. Document embedding vectors have been shown to be superior to other text representations such as bag-of-words as they take into account the relative positions of the words in the text, although experimental they may be an interesting feature representation to train VCP. Overall, papers with success in using machine learning for dealing with larger passages of text more limited in number (Liu et al., 2018), compared to longer texts (Lopez and"
2021.acl-short.109,D15-1075,0,0.0410029,"quality data. More recent research, however, has drawn our attention towards the inevitability of ambiguity, and the necessity to take it into consideration when working on natural language understanding tasks (Pavlick and Kwiatkowski, 2019; Chen et al., 2020; Nie et al., 2020; Swayamdipta et al., 2020). This ambiguity stems from the lack of proper context or differences in background knowledge between annotators, and leads to a large number of examples where the correctness of labels can be debated. ChaosNLI (Nie et al., 2020) is a dataset created by manually annotating a subset of the SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and αNLI (Bhagavatula et al., 2020) datasets. Each of the total 4,645 samples received 100 annotations. Through this data, they were able to generate a probability distribution over the labels for these samples, which they call the human agreement distribution, with the goal of using it to evaluate the ability of current state-of-the-art models to capture ambiguity. The divergence scores between the model’s predicted probability distribution and the true target distribution is computed and compared against random and human baselines. They showed that models trai"
2021.acl-short.109,2020.acl-main.774,0,0.0668617,"Missing"
2021.acl-short.109,N19-1423,0,0.0257081,"ere is no overlap with ChaosNLI. We apply a simple linear approach to convert the UNLI regression value p into a probability distribution zNLI , as described in the following composed function (its plot can be found in the Appendix A): 1 https://github.com/mariomeissner/ AmbiNLI 863 zNLI ( (0, 2p, 1 − 2p) = (2p − 1, 2 − 2p, 0) p &lt; 0.5 p ≥ 0.5. The resulting AmbiNLI dataset has 18,152 SNLI examples, 18,048 MNLI examples, and 55,517 UNLI examples, for a total of 91,717 premisehypothesis pairs with an ambiguity distribution as the target label. 3 Experiments In our experiments, we use BERT-base (Devlin et al., 2019) with pre-trained weights and a softmax classification head. We use a batch size of 128 and learning rate of 1e-5. Learning to capture question ambiguity. In our main experiment, we aim to judge whether is is possible to learn how to capture the human agreement distribution. We first obtain a base model in the same manner as Nie et al. (2020), by pretraining it for 3 epochs on the gold-labels of the SNLI and MNLI training sets. We observed that this pre-training step is necessary to provide the model with a general understanding of the NLI task to compensate for the low amount of ambiguity dat"
2021.acl-short.109,D16-1062,0,0.0350787,"Missing"
2021.acl-short.109,P11-1015,0,0.0702775,"Missing"
2021.acl-short.109,2020.emnlp-main.734,0,0.119562,"nstead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks. 1 Introduction Ambiguity is intrinsic to natural language, and creating datasets free of this property is a hard if not impossible task. Previously, it was common to disregard it as noise or as a sign of poor quality data. More recent research, however, has drawn our attention towards the inevitability of ambiguity, and the necessity to take it into consideration when working on natural language understanding tasks (Pavlick and Kwiatkowski, 2019; Chen et al., 2020; Nie et al., 2020; Swayamdipta et al., 2020). This ambiguity stems from the lack of proper context or differences in background knowledge between annotators, and leads to a large number of examples where the correctness of labels can be debated. ChaosNLI (Nie et al., 2020) is a dataset created by manually annotating a subset of the SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and αNLI (Bhagavatula et al., 2020) datasets. Each of the total 4,645 samples received 100 annotations. Through this data, they were able to generate a probability distribution over the labels for these samples, which they ca"
2021.acl-short.109,Q19-1043,0,0.0183646,"of data but targeting the ambiguity distribution instead of gold-labels can result in models that achieve higher performance and learn better representations for downstream tasks. 1 Introduction Ambiguity is intrinsic to natural language, and creating datasets free of this property is a hard if not impossible task. Previously, it was common to disregard it as noise or as a sign of poor quality data. More recent research, however, has drawn our attention towards the inevitability of ambiguity, and the necessity to take it into consideration when working on natural language understanding tasks (Pavlick and Kwiatkowski, 2019; Chen et al., 2020; Nie et al., 2020; Swayamdipta et al., 2020). This ambiguity stems from the lack of proper context or differences in background knowledge between annotators, and leads to a large number of examples where the correctness of labels can be debated. ChaosNLI (Nie et al., 2020) is a dataset created by manually annotating a subset of the SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and αNLI (Bhagavatula et al., 2020) datasets. Each of the total 4,645 samples received 100 annotations. Through this data, they were able to generate a probability distribution over the la"
2021.acl-short.109,2020.emnlp-main.746,0,0.0616926,"Missing"
2021.acl-short.109,N18-1101,0,0.0331012,"search, however, has drawn our attention towards the inevitability of ambiguity, and the necessity to take it into consideration when working on natural language understanding tasks (Pavlick and Kwiatkowski, 2019; Chen et al., 2020; Nie et al., 2020; Swayamdipta et al., 2020). This ambiguity stems from the lack of proper context or differences in background knowledge between annotators, and leads to a large number of examples where the correctness of labels can be debated. ChaosNLI (Nie et al., 2020) is a dataset created by manually annotating a subset of the SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), and αNLI (Bhagavatula et al., 2020) datasets. Each of the total 4,645 samples received 100 annotations. Through this data, they were able to generate a probability distribution over the labels for these samples, which they call the human agreement distribution, with the goal of using it to evaluate the ability of current state-of-the-art models to capture ambiguity. The divergence scores between the model’s predicted probability distribution and the true target distribution is computed and compared against random and human baselines. They showed that models trained using gold-labels have ver"
2021.acl-srw.21,P19-1620,0,0.134235,"(Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). Such systems usually use neural models, which require a substantial number of question-answer (QA) pairs for training. To reduce the considerable manual cost of dataset creation, there has been a resurgence of studies on automatic QA pair generation (QAG), consisting of a pipeline of answer extraction (AE) and 1 Our code and data are available at https://github. com/KazutoshiShinoda/VQAG. question generation (QG), to augment question answering (QA) datasets (Yang et al., 2017a; Du and Cardie, 2018; Subramanian et al., 2018; Alberti et al., 2019). For the downstream QA task, most existing studies have evaluated QAG methods using a test set from the same distribution as a training set (Yang et al., 2017a; Zhang and Bansal, 2019; Liu et al., 2020). However, when a QA model is evaluated only on an in-distribution test set, it is difficult to verify that the model is not exploiting unintended biases in a dataset (Geirhos et al., 2020). Exploiting an unintended bias can degrade the robustness of a QA model, which is problematic in real-world applications. For example, recent studies have observed that a QA model does not generalize to othe"
2021.acl-srw.21,2020.acl-main.421,0,0.0211531,"AD dev set with a linguistic rulebased method.8 • AddSent (Add) & AddOneSent (AddO) (Jia and Liang, 2017): Adversarial SQuAD dataset created using handcrafted rules designed for fooling a QA model. The sizes of Add and AddO are 3,560 and 1,787, respectively. • Quoref (Quo) (Dasigi et al., 2019): 2,418 questions requiring coreference resolution created by humans. We used the dev set. • Natural Machine Translation Noise (MT) (Ravichander et al., 2021): A subset of NoiseQA, consisting of 1,190 English translated questions produced by Google’s commercial translation system from the XQuAD dataset (Artetxe et al., 2020). This creation introduces naturally occurring noise caused by machine translation. • Natural Automatic Speech Recognition Noise (ASR) (Ravichander et al., 2021): Another subset of NoiseQA, consisting of 1,190 questions that include automatic speech recognition error. Challenge Sets We assessed the robustness of the QA models to the following 12 challenge sets, as well as SQuADDu test . • NewsQA (News) (Trischler et al., 2017): 5,166 QA pairs created from CNN articles by crowdworkers, transformed into the SQuAD format following Sen and Saffari (2020). • Natural Questions (NQ) (Kwiatkowski et a"
2021.acl-srw.21,C18-1142,0,0.0195324,"iu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017; Yang et al., 2017a; Yao et al., 2018; Kumar et al., 2019). Dong et al. (2019) used a pretrained language model. The diversity of questions has been tackled using variational attention (Bahuleyan et al., 2018), a conditional variational autoencoder (CVAE) (Yao et al., 2018), and top p nucleus sampling (Sultan et al., 2020). Our study is different from these studies wherein we study QAG by introducing variational methods into both AE and QG. Lee et al. (2020) is the closest to our study in terms of the modeling choice. While Lee et al. (2020) introduced an information-maximizing term to improve the consistency of QA pairs, our study uniquely controls 198 the diversity by explicitly controlling KL values. Despite the potential of data augmentation with QAG to mitigate the sparsity of QA datasets and"
2021.acl-srw.21,K16-1002,0,0.647181,"earch Workshop, pages 197–214 August 5–6, 2021. ©2021 Association for Computational Linguistics a variational QAG model (VQAG). We introduce two independent latent random variables into our model to learn the two one-to-many relationships in AE and QG by utilizing neural variational inference (Kingma and Welling, 2013). Incorporating the randomness of these two latent variables enables our model to generate diverse answers and questions separately. We also study the effect of controlling the Kullback–Leibler (KL) term in the variational lower bound for mitigating the posterior collapse issue (Bowman et al., 2016), where the model ignores latent variables and generates outputs that are almost the same. We evaluate our approach on 12 challenge sets that are unseen during training to assess the improved robustness of the QA model. In summary, our contributions are three-fold: • We propose a variational question-answer pair generation model with explicit KL control to generate significantly diverse answers and questions. • We construct synthetic QA datasets using our model to boost the QA performance in an in-distribution test set, achieving comparable scores with existing QAG methods. • We discover that"
2021.acl-srw.21,P16-1154,0,0.406831,"hat there should be question-worthy phrases that are not used as the gold answers in a manually created dataset. We aim to extract such phrases. 2.2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017;"
2021.acl-srw.21,P16-1014,0,0.322666,"this study, we assume that there should be question-worthy phrases that are not used as the gold answers in a manually created dataset. We aim to extract such phrases. 2.2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used ("
2021.acl-srw.21,N10-1086,0,0.0533,"2019). However, these neural AE models are trained with maximum likelihood estimation; that is, each model is optimized to produce an answer set closest to the gold answers. In contrast, our model incorporates a latent random variable and is trained by maximizing the lower bound of the likelihood to extract diverse answers. In this study, we assume that there should be question-worthy phrases that are not used as the gold answers in a manually created dataset. We aim to extract such phrases. 2.2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importa"
2021.acl-srw.21,D17-1215,0,0.0603982,"-5 to zero. We conducted the training for one epoch using a synthetic dataset and two epochs using the original training set. In addition to the performance of Single models, we reported the performance of Ensemble models, where the output probabilities of three different QA models are simply averaged. In practice, the top 20 candidate answer spans predicted by each QA model were used for the final prediction. 4.6.3 • Implications (Imp) (Ribeiro et al., 2019): 13,371 QA pairs automatically derived from the SQuAD dev set with a linguistic rulebased method.8 • AddSent (Add) & AddOneSent (AddO) (Jia and Liang, 2017): Adversarial SQuAD dataset created using handcrafted rules designed for fooling a QA model. The sizes of Add and AddO are 3,560 and 1,787, respectively. • Quoref (Quo) (Dasigi et al., 2019): 2,418 questions requiring coreference resolution created by humans. We used the dev set. • Natural Machine Translation Noise (MT) (Ravichander et al., 2021): A subset of NoiseQA, consisting of 1,190 English translated questions produced by Google’s commercial translation system from the XQuAD dataset (Artetxe et al., 2020). This creation introduces naturally occurring noise caused by machine translation."
2021.acl-srw.21,W10-2910,0,0.0156696,"erence question. We do not report precision scores here because our motivation is to improve diversity. To measure diversity, we reported Dist-1 (D1), Entropy-4 (E4) (Serban et al., 2017; Zhang et al., 2018), and Self-BLEU-4 (SB4) (Zhu et al., 2018).3 Table 2: Results of answer-aware QG on the test set. One question per input is evaluated in the upper part, while 50 questions per input are evaluated in the lower part to assess their diversity. Metrics To measure the accuracy of multi-span extraction, we computed Proportional Overlap (Prop.) and Exact Match (Exact) metrics (Breck et al., 2007; Johansson and Moschitti, 2010; Du and Cardie, 2018) for each pair of extracted and ground truth answer spans, and then we report their precision and recall.2 Prop. is proportional to the amount of overlap between two phrases. Our models extracted 50 answers from each context. To measure the diversity, we defined a Dist score, which is the the total number of distinct contextanswer pairs. Baselines We used three baselines: named entity recognition (NER), Harvesting QG (HarQG) (Du and Cardie, 2018), and InfoHCVAE (Lee et al., 2020). For NER, we used spaCy (Honnibal et al., 2020). For HarQG, we directly copied the scores fro"
2021.acl-srw.21,P17-1147,0,0.0252516,"he quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy.1 1 Introduction Machine reading comprehension has gained significant attention in the NLP community, whose goal is to devise systems that can answer questions about given documents (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). Such systems usually use neural models, which require a substantial number of question-answer (QA) pairs for training. To reduce the considerable manual cost of dataset creation, there has been a resurgence of studies on automatic QA pair generation (QAG), consisting of a pipeline of answer extraction (AE) and 1 Our code and data are available at https://github. com/KazutoshiShinoda/VQAG. question generation (QG), to augment question answering (QA) datasets (Yang et al., 2017a; Du and Cardie, 2018; Subramanian et al., 2018; Alberti et al., 2019). For the downstream QA task, most existing stu"
2021.acl-srw.21,W13-2114,0,0.0314521,"ral AE models are trained with maximum likelihood estimation; that is, each model is optimized to produce an answer set closest to the gold answers. In contrast, our model incorporates a latent random variable and is trained by maximizing the lower bound of the likelihood to extract diverse answers. In this study, we assume that there should be question-worthy phrases that are not used as the gold answers in a manually created dataset. We aim to extract such phrases. 2.2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input"
2021.acl-srw.21,D19-1326,0,0.0335322,"Missing"
2021.acl-srw.21,D14-1162,0,0.0868024,"Missing"
2021.acl-srw.21,D19-1614,0,0.0168189,"hods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017; Yang et al., 2017a; Yao et al., 2018; Kumar et al., 2019). Dong et al. (2019) used a pretrained language model. The diversity of questions has been tackled using variational attention (Bahuleyan et al., 2018), a conditiona"
2021.acl-srw.21,D16-1264,0,0.494417,"While most existing QAG methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy.1 1 Introduction Machine reading comprehension has gained significant attention in the NLP community, whose goal is to devise systems that can answer questions about given documents (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). Such systems usually use neural models, which require a substantial number of question-answer (QA) pairs for training. To reduce the considerable manual cost of dataset creation, there has been a resurgence of studies on automatic QA pair generation (QAG), consisting of a pipeline of answer extraction (AE) and 1 Our code and data are available at https://github. com/KazutoshiShinoda/VQAG. question generation (QG), to augment question answering (QA) datasets (Yang et al., 2017a; Du and Cardie, 2018; Subramanian et al., 2018; Alberti et al., 2019)."
2021.acl-srw.21,2021.eacl-main.259,0,0.481414,"to verify that the model is not exploiting unintended biases in a dataset (Geirhos et al., 2020). Exploiting an unintended bias can degrade the robustness of a QA model, which is problematic in real-world applications. For example, recent studies have observed that a QA model does not generalize to other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Other studies have found a lack of robustness to challenge sets, such as paraphrased questions (Gan and Ng, 2019), questions with low lexical overlap (Sugawara et al., 2018), and questions that include noise (Ravichander et al., 2021). While existing studies have proposed data augmentation methods targeting a particular challenge set, they are only effective at the expense of the indistribution accuracy (Gan and Ng, 2019; Ribeiro et al., 2019; Ravichander et al., 2021). These methods assume that the target distribution is given a priori. However, identifying the type of samples that a QA model cannot handle in advance is difficult in real-world applications. We conjecture that increasing the diversity of a training set with data augmentation, rather than augmenting QA pairs similar to the original training set, can improve"
2021.acl-srw.21,P19-1621,0,0.300798,". For example, recent studies have observed that a QA model does not generalize to other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Other studies have found a lack of robustness to challenge sets, such as paraphrased questions (Gan and Ng, 2019), questions with low lexical overlap (Sugawara et al., 2018), and questions that include noise (Ravichander et al., 2021). While existing studies have proposed data augmentation methods targeting a particular challenge set, they are only effective at the expense of the indistribution accuracy (Gan and Ng, 2019; Ribeiro et al., 2019; Ravichander et al., 2021). These methods assume that the target distribution is given a priori. However, identifying the type of samples that a QA model cannot handle in advance is difficult in real-world applications. We conjecture that increasing the diversity of a training set with data augmentation, rather than augmenting QA pairs similar to the original training set, can improve the robustness of QA models. Poor diversity in QA datasets has been shown to result in the poor robustness of QA models (Lewis and Fan, 2019; Geva et al., 2019; Ko et al., 2020), supporting our hypothesis. To th"
2021.acl-srw.21,D17-1066,0,0.0249959,"rs from different latent spaces makes sense because multiple questions can be created from a context-answer pair and multiple answer spans can be extracted from a context. Thus, we introduce two independent latent random variables to assign the roles of diversifying AE and QG to z and y, respectively. VAEs often suffer from posterior collapse, where the model learns to ignore latent variables and generates outputs that are almost the same (Bowman et al., 2016). Many approaches have been proposed to mitigate this issue, such as weakening the generators (Bowman et al., 2016; Yang et al., 2017b; Semeniuta et al., 2017), or modifying the objective functions (Tolstikhin et al., 2018; Zhao et al., 2017a; Higgins et al., 2017). To mitigate this problem, we use a variant of the modified β-VAE (Higgins et al., 2017) proposed by Burgess et al. (2018), which uses two hyperparameters to control the KL terms. Our modified objective function is: L = Eqφ (z,y|q,a,c) [log pθ (q|y, a, c) + log pθ (a|z, c)] − |DKL (qφ (z|a, c)||pθ (z|c)) − Ca | − |DKL (qφ (y|q, c)||pθ (y|c)) − Cq |, (1) where DKL is the KL divergence, θ (φ) is the parameters of the generative (inference) model, and Ca , Cq ≥ 0. See Appendix A for the deri"
2021.acl-srw.21,2020.emnlp-main.190,0,0.387749,"ted QAG methods using a test set from the same distribution as a training set (Yang et al., 2017a; Zhang and Bansal, 2019; Liu et al., 2020). However, when a QA model is evaluated only on an in-distribution test set, it is difficult to verify that the model is not exploiting unintended biases in a dataset (Geirhos et al., 2020). Exploiting an unintended bias can degrade the robustness of a QA model, which is problematic in real-world applications. For example, recent studies have observed that a QA model does not generalize to other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Other studies have found a lack of robustness to challenge sets, such as paraphrased questions (Gan and Ng, 2019), questions with low lexical overlap (Sugawara et al., 2018), and questions that include noise (Ravichander et al., 2021). While existing studies have proposed data augmentation methods targeting a particular challenge set, they are only effective at the expense of the indistribution accuracy (Gan and Ng, 2019; Ribeiro et al., 2019; Ravichander et al., 2021). These methods assume that the target distribution is given a priori. However, identifying the type of samples that a QA mod"
2021.acl-srw.21,N18-2090,0,0.0164993,"act such phrases. 2.2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017; Yang et al., 2017a; Yao et al., 2018; Kumar et al., 2019). Dong et al. (2019) used a pretrained language model. The diversity o"
2021.acl-srw.21,D18-1427,0,0.0351211,"Missing"
2021.acl-srw.21,P19-1485,0,0.118549,"sting studies have evaluated QAG methods using a test set from the same distribution as a training set (Yang et al., 2017a; Zhang and Bansal, 2019; Liu et al., 2020). However, when a QA model is evaluated only on an in-distribution test set, it is difficult to verify that the model is not exploiting unintended biases in a dataset (Geirhos et al., 2020). Exploiting an unintended bias can degrade the robustness of a QA model, which is problematic in real-world applications. For example, recent studies have observed that a QA model does not generalize to other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Other studies have found a lack of robustness to challenge sets, such as paraphrased questions (Gan and Ng, 2019), questions with low lexical overlap (Sugawara et al., 2018), and questions that include noise (Ravichander et al., 2021). While existing studies have proposed data augmentation methods targeting a particular challenge set, they are only effective at the expense of the indistribution accuracy (Gan and Ng, 2019; Ribeiro et al., 2019; Ravichander et al., 2021). These methods assume that the target distribution is given a priori. However, identifying the type"
2021.acl-srw.21,W17-2623,0,0.150083,"methods aim to improve the quality of synthetic examples, we conjecture that diversity-promoting QAG can mitigate the sparsity of training sets and lead to better robustness. We present a variational QAG model that generates multiple diverse QA pairs from a paragraph. Our experiments show that our method can improve the accuracy of 12 challenge sets, as well as the in-distribution accuracy.1 1 Introduction Machine reading comprehension has gained significant attention in the NLP community, whose goal is to devise systems that can answer questions about given documents (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). Such systems usually use neural models, which require a substantial number of question-answer (QA) pairs for training. To reduce the considerable manual cost of dataset creation, there has been a resurgence of studies on automatic QA pair generation (QAG), consisting of a pipeline of answer extraction (AE) and 1 Our code and data are available at https://github. com/KazutoshiShinoda/VQAG. question generation (QG), to augment question answering (QA) datasets (Yang et al., 2017a; Du and Cardie, 2018; Subramanian et al., 2018; Alberti et al., 2019). For the downstream QA ta"
2021.acl-srw.21,W18-2609,0,0.0688325,"ons about given documents (Rajpurkar et al., 2016; Trischler et al., 2017; Joshi et al., 2017). Such systems usually use neural models, which require a substantial number of question-answer (QA) pairs for training. To reduce the considerable manual cost of dataset creation, there has been a resurgence of studies on automatic QA pair generation (QAG), consisting of a pipeline of answer extraction (AE) and 1 Our code and data are available at https://github. com/KazutoshiShinoda/VQAG. question generation (QG), to augment question answering (QA) datasets (Yang et al., 2017a; Du and Cardie, 2018; Subramanian et al., 2018; Alberti et al., 2019). For the downstream QA task, most existing studies have evaluated QAG methods using a test set from the same distribution as a training set (Yang et al., 2017a; Zhang and Bansal, 2019; Liu et al., 2020). However, when a QA model is evaluated only on an in-distribution test set, it is difficult to verify that the model is not exploiting unintended biases in a dataset (Geirhos et al., 2020). Exploiting an unintended bias can degrade the robustness of a QA model, which is problematic in real-world applications. For example, recent studies have observed that a QA model does"
2021.acl-srw.21,N16-1170,0,0.0194572,"However, not all the named entities, noun phrases, verb phrases, adjectives, or clauses in the given documents are used as gold answer spans. As such, these rulebased methods are likely to extract many trivial phrases. Therefore, there have been studies on training neural models to identify question-worthy phrases. Du and Cardie (2018) framed AE as a sequence labeling task and used BiLSTM-CRF (Huang et al., 2015). Subramanian et al. (2018) treated the positions of answers as a sequence and used a pointer network (Vinyals et al., 2015). Wang et al. (2019) used a pointer network and Match-LSTM (Wang and Jiang, 2016, 2017). Alberti et al. (2019) made use of pretrained BERT (Devlin et al., 2019). However, these neural AE models are trained with maximum likelihood estimation; that is, each model is optimized to produce an answer set closest to the gold answers. In contrast, our model incorporates a latent random variable and is trained by maximizing the lower bound of the likelihood to extract diverse answers. In this study, we assume that there should be question-worthy phrases that are not used as the gold answers in a manually created dataset. We aim to extract such phrases. 2.2 Question Generation Trad"
2021.acl-srw.21,2020.acl-main.500,0,0.0268611,"rks studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017; Yang et al., 2017a; Yao et al., 2018; Kumar et al., 2019). Dong et al. (2019) used a pretrained language model. The diversity of questions has been tackled using variational attention (Bahuleyan et al., 2018), a conditional variational autoencoder (CVAE) (Yao et al., 2018), and top p nucleus sampling (Sultan et al., 2020). Our study is different from these studies wherein we study QAG by introducing variational methods into both AE and QG. Lee et al. (2020) is the closest to our study in terms of the modeling choice. While Lee et al. (2020) introduced an information-maximizing term to improve the consistency of QA pairs, our study uniquely controls 198 the diversity by explicitly controlling KL values. Despite the potential of data augmentation with QAG to mitigate the sparsity of QA datasets and avoid overfitting, not much is known about the robustness of QA models reinforced with QAG to more challenging test"
2021.acl-srw.21,D18-1424,0,0.0185479,".2 Question Generation Traditionally, QG was studied using rule-based methods (Mostow and Chen, 2009; Heilman and Smith, 2010; Lindberg et al., 2013; Labutov et al., 2015). After Du et al. (2017) proposed a neural sequence-to-sequence model (Sutskever et al., 2014) for QG, neural models that take context and answer as inputs have started to be used to improve question quality with attention (Bahdanau et al., 2014) and copying (Gulcehre et al., 2016; Gu et al., 2016) mechanisms. Most works focused on generating relevant questions from context-answer pairs (Zhou et al., 2018; Song et al., 2018; Zhao et al., 2018; Sun et al., 2018; Kim et al., 2019; Liu et al., 2019; Qiu and Xiong, 2019). These works showed the importance of answers as input features for QG. Other works studied predicting question types (Zhou et al., 2019; Kang et al., 2019), modeling a structured answer-relevant relation (Li et al., 2019), and refining generated questions (Nema et al., 2019). To further improve question quality, policy gradient techniques have been used (Yuan et al., 2017; Yang et al., 2017a; Yao et al., 2018; Kumar et al., 2019). Dong et al. (2019) used a pretrained language model. The diversity of questions has bee"
2021.deelio-1.14,P19-1635,0,0.247716,"19; Bouraoui et al., 2019; Zhou et al., 2019; Talmor et al., 2020). However, it has also been reported that current neural language models still perform poorly in natural language processing tasks that require NCS and a deep understanding of numerals, such as numerical reasoning, numerical question answer140 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 140–150 Online, June 10, 2021. ©2021 Association for Computational Linguistics ing, and numerical error detection/correction (Dua et al., 2019; Chen et al., 2019). Numerals appear frequently in various forms, such as dates, numbers of people, percentages, and so on, regardless of the domain of passages. Hence, the acquisition of numerical common sense by neural language models and the analysis of the acquired numerical common sense are essential research topics to support systems for reasoning on text containing numerals and smooth conversation with humans at a high level. One of the major problems that make it difficult for language models to understand the meaning of numerals and to acquire NCS is that naive language models treat numerals in text as"
2021.deelio-1.14,D19-1109,0,0.0245561,"following example sentence. “John is 200 cm tall."" In recent years, BERT, GPT-3, and other neural language models have achieved a level of performance on par with or better than human performance in many natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Brown et al., 2020). Moreover, several studies have recently been conducted to investigate whether pre-trained neural language models have commonsense knowledge, and these studies often conclude that the language models have been successful in acquiring some commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Bouraoui et al., 2019; Zhou et al., 2019; Talmor et al., 2020). However, it has also been reported that current neural language models still perform poorly in natural language processing tasks that require NCS and a deep understanding of numerals, such as numerical reasoning, numerical question answer140 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 140–150 Online, June 10, 2021. ©2021 Association for Computational Linguistics ing, and numerical error detection/correction (Dua et al., 2019; Ch"
2021.deelio-1.14,N19-1423,0,0.0189083,"knowledge about real-world numerical features for a deep understanding of natural language text containing numerals. Introduction Numerical common sense (NCS) is knowledge about the numerical features of objects in the real world, such as size, weight, or color, each of which has its own range and probability distribution (Yamane et al., 2020). Consider the following example sentence. “John is 200 cm tall."" In recent years, BERT, GPT-3, and other neural language models have achieved a level of performance on par with or better than human performance in many natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Brown et al., 2020). Moreover, several studies have recently been conducted to investigate whether pre-trained neural language models have commonsense knowledge, and these studies often conclude that the language models have been successful in acquiring some commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Bouraoui et al., 2019; Zhou et al., 2019; Talmor et al., 2020). However, it has also been reported that current neural language models still perform poorly in natural language processing tasks that require NCS and a deep understanding o"
2021.deelio-1.14,N19-1246,0,0.0317386,"Missing"
2021.deelio-1.14,P19-1388,0,0.019714,"ategories. Their categories include “Monetary,"" “Percentage,"" “Temporal"" (date and time), and so on. They used a convolutional neural network (CNN), long short-term memory (LSTM), and bidirectional LSTM in experiments and concluded that character-level CNN performed the best. We use the FinNum dataset in our experiments for the masked numeral prediction task. 3 3.1 NCS Two Types of NCS of datasets that collect the distributions of some attributes such as weight, length, and price of common objects as well as the verification of such NCS acquired by neural language models using these datasets (Elazar et al., 2019; Zhang et al., 2020; Yamane et al., 2020). In this paper, we aimed to acquire quantity-type NCS as well as token-type NCS with language models, focusing on the fact that there are these two types of NCS. 3.2 Masked Numeral Prediction 3.2.1 Task Description Masked numeral prediction is the task of predicting a masked numeral in an input natural language text from the words around the masked numeral (e.g., “The movie I saw yesterday was [MASK] minutes long."") (Spithourakis and Riedel, 2018; Lin et al., 2020). In this paper, we use this task as an indicator to evaluate the NCS acquired by langua"
2021.deelio-1.14,D19-1170,0,0.0209675,"Missing"
2021.deelio-1.14,2020.emnlp-main.557,0,0.131756,"ics to support systems for reasoning on text containing numerals and smooth conversation with humans at a high level. One of the major problems that make it difficult for language models to understand the meaning of numerals and to acquire NCS is that naive language models treat numerals in text as string tokens, just like any other word (Spithourakis and Riedel, 2018). This makes it difficult to obtain a mapping between the string tokens and the magnitudes of the numerals, which is needed to capture NCS. In this study, we use the masked numeral prediction task (Spithourakis and Riedel, 2018; Lin et al., 2020) to evaluate and verify the NCS acquired by neural language models. The task requires models to predict masked numerals in an input passage from their context. We use two types of evaluation metrics: hit@k accuracy (Lin et al., 2020) and MdAE and MdAPE (Spithourakis and Riedel, 2018) for this task. Hit@k accuracy calculates the percentage of predictions in which the groundtruth numeral is within the top k predicted numerals, and we can say that they evaluate language models in terms of the symbolic aspect of numerals. MdAE and MdAPE are calculated from the difference between the groundtruth nu"
2021.deelio-1.14,D19-1250,0,0.0234162,", 2020). Consider the following example sentence. “John is 200 cm tall."" In recent years, BERT, GPT-3, and other neural language models have achieved a level of performance on par with or better than human performance in many natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Brown et al., 2020). Moreover, several studies have recently been conducted to investigate whether pre-trained neural language models have commonsense knowledge, and these studies often conclude that the language models have been successful in acquiring some commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Bouraoui et al., 2019; Zhou et al., 2019; Talmor et al., 2020). However, it has also been reported that current neural language models still perform poorly in natural language processing tasks that require NCS and a deep understanding of numerals, such as numerical reasoning, numerical question answer140 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 140–150 Online, June 10, 2021. ©2021 Association for Computational Linguistics ing, and numerical error detection/correction"
2021.deelio-1.14,D16-1101,0,0.0674503,"Missing"
2021.deelio-1.14,P18-1196,0,0.395075,"orms, such as dates, numbers of people, percentages, and so on, regardless of the domain of passages. Hence, the acquisition of numerical common sense by neural language models and the analysis of the acquired numerical common sense are essential research topics to support systems for reasoning on text containing numerals and smooth conversation with humans at a high level. One of the major problems that make it difficult for language models to understand the meaning of numerals and to acquire NCS is that naive language models treat numerals in text as string tokens, just like any other word (Spithourakis and Riedel, 2018). This makes it difficult to obtain a mapping between the string tokens and the magnitudes of the numerals, which is needed to capture NCS. In this study, we use the masked numeral prediction task (Spithourakis and Riedel, 2018; Lin et al., 2020) to evaluate and verify the NCS acquired by neural language models. The task requires models to predict masked numerals in an input passage from their context. We use two types of evaluation metrics: hit@k accuracy (Lin et al., 2020) and MdAE and MdAPE (Spithourakis and Riedel, 2018) for this task. Hit@k accuracy calculates the percentage of prediction"
2021.deelio-1.14,2020.tacl-1.48,0,0.0179297,"rs, BERT, GPT-3, and other neural language models have achieved a level of performance on par with or better than human performance in many natural language processing tasks (Devlin et al., 2019; Liu et al., 2019; Lan et al., 2020; Brown et al., 2020). Moreover, several studies have recently been conducted to investigate whether pre-trained neural language models have commonsense knowledge, and these studies often conclude that the language models have been successful in acquiring some commonsense knowledge (Petroni et al., 2019; Davison et al., 2019; Bouraoui et al., 2019; Zhou et al., 2019; Talmor et al., 2020). However, it has also been reported that current neural language models still perform poorly in natural language processing tasks that require NCS and a deep understanding of numerals, such as numerical reasoning, numerical question answer140 Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 140–150 Online, June 10, 2021. ©2021 Association for Computational Linguistics ing, and numerical error detection/correction (Dua et al., 2019; Chen et al., 2019). Numerals appear frequently in various forms, s"
2021.deelio-1.14,D19-1534,0,0.0147395,"d at predicting numerals with different characteristics. In our experiments, to eliminate the negative effects of the sub-word approach, we do not split the numerals into sub-words. The sub-word approach splits words into shorter tokens called sub-words. It has the advantage that even low-frequency words can be represented by a combination of sub-words that appear in a text more frequently. However, unlike the case of words, sub-words derived from numerals often have little relationship to the meaning of the original numerals, which can make it difficult to understand the meaning of numerals (Wallace et al., 2019). All other words are separated into sub-words in our experiments. To summarize, in this work, we tackle the problem of dealing with numerals in naive language models on the masked numeral prediction task. Our contributions are as follows: 1. A masked word prediction model with a new loss function LossNUM that is based on the differences between the groundtruth numerals and predicted numerals; 2. A masked word prediction model, called the REG model, structured with an additional output layer to predict a numeral from an input passage containing a masked numeral. We use the BERT-based masked wo"
2021.eacl-main.137,2020.acl-main.463,0,0.149914,"achieve humanlevel understanding. For example, Jia and Liang (2017) use manually crafted adversarial examples to show that successful systems are easily distracted. Sugawara et al. (2020) show that a significant part of already solved questions is solvable even after shuffling the words in a sentence or dropping content words. These studies demonstrate that we cannot explain what type of understanding is required by the datasets and is actually acquired by models. Although benchmarking MRC is related to the intent behind questions and is critical to test hypotheses from a top-down viewpoint (Bender and Koller, 2020), its theoretical foundation is poorly investigated in the literature. In this position paper, we examine the prerequisites for benchmarking MRC based on the following two questions: (i) What does reading comprehension involve? (ii) How can we evaluate it? Our motivation is to provide a theoretical basis for the creation of MRC datasets. As Gilpin et al. (2018) indicate, interpreting the internals of a system is closely related to only the system’s architecture and is insufficient for explaining how the task is accomplished. This is because even if the internals of models can be interpreted, w"
2021.eacl-main.137,bentivogli-etal-2010-building,0,0.045129,"ortance of the requirement of various skills in MRC, which can serve as the units for the explanation of reading comprehension. Therefore, our motivation is to provide an overview of the skills as a hierarchical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency par"
2021.eacl-main.137,2020.emnlp-main.703,0,0.144309,"Missing"
2021.eacl-main.137,2020.emnlp-main.85,0,0.0380571,"Missing"
2021.eacl-main.137,W18-2607,0,0.0224664,"hical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu"
2021.eacl-main.137,D15-1075,0,0.0278114,"these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu et al., 2019), they are hardly required in NLU tasks including MRC. In the natural language inference task, McCoy et al. (2019) indicate that existing datasets (e.g., Bowman et al. (2015)) may fail to elucidate the syntactic understanding of given sentences. Although it is not obvious that these basic tasks should be included in MRC and it is not easy to circumscribe linguistic knowledge from concrete and abstract knowledge (Zaenen et al., 2005; Manning, 2006), we should always care about the capabilities of basic tasks (e.g., use of checklists 1595 Construct the global structure of propositions. Skills: creating a coherent representation and grounding it to other media. Situation model Textbase Surface structure Construct the local relations of propositions. Skills: recognizi"
2021.eacl-main.137,P17-1171,0,0.0184303,"a passage of text if, for any question regarding that text that can be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question. We overview various aspects of the task along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al."
2021.eacl-main.137,N19-1405,0,0.0377633,"Missing"
2021.eacl-main.137,W19-2008,0,0.0332615,"Missing"
2021.eacl-main.137,N18-2017,0,0.0647421,"Missing"
2021.eacl-main.137,N18-1175,0,0.0664065,"Missing"
2021.eacl-main.137,W18-2605,0,0.053332,"Missing"
2021.eacl-main.137,P16-1145,0,0.0714057,"Missing"
2021.eacl-main.137,D19-1243,0,0.146481,"Missing"
2021.eacl-main.137,D19-1259,0,0.040119,"Missing"
2021.eacl-main.137,P17-1147,0,0.0746664,"Missing"
2021.eacl-main.137,N18-1023,0,0.0136785,"E (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi et al., 2019)), and logical reasoning (ReClor (Yu et al., 2020)). 2.2 Benchmarking Issues In some datasets, the performance of machines has already reached human-level performance. However, Jia and Liang (2017) indicate that models can easily be fooled by manual injection of distracting sentences. Their study revealed that questions simply gathered by crowdsourcing without careful guideline"
2021.eacl-main.137,Q18-1023,0,0.0719978,"Missing"
2021.eacl-main.137,Q19-1026,0,0.039576,"y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et"
2021.eacl-main.137,P18-1077,0,0.0651417,"Missing"
2021.eacl-main.137,D17-1082,0,0.0330346,"on et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 201"
2021.eacl-main.137,D16-1062,0,0.0225099,"hat of other NLU tasks and measurements. 6. Consequential Value implications of score interpretation as a basis for the consequences of test use, especially regarding the sources of invalidity related to issues of bias, fairness, and distributive justice. Considering the model vulnerabilities to adversarial attacks and social biases of models and datasets to ensure the fairness of model outputs. Table 2: Aspects of the construct validity in psychometrics and corresponding features in MRC. may be useful if such ambiguity can be reflected in the evaluation (e.g., using the item response theory (Lalor et al., 2016)). As for model predictions, an issue may be the reproducibility of results (Bouthillier et al., 2019), which implies that the reimplementation of a system generates statistically similar predictions. For the reproducibility of models, Dror et al. (2018) emphasize statistical testing methods to evaluate models. For the reproducibility of findings, Bouthillier et al. (2019) stress it as the transferability of findings in a dataset/task to another dataset/task. In open-domain question answering, Lewis et al. (2021) point out that successful models might only memorize dataset-specific knowledge."
2021.eacl-main.137,2021.eacl-main.86,1,0.747166,"biguity can be reflected in the evaluation (e.g., using the item response theory (Lalor et al., 2016)). As for model predictions, an issue may be the reproducibility of results (Bouthillier et al., 2019), which implies that the reimplementation of a system generates statistically similar predictions. For the reproducibility of models, Dror et al. (2018) emphasize statistical testing methods to evaluate models. For the reproducibility of findings, Bouthillier et al. (2019) stress it as the transferability of findings in a dataset/task to another dataset/task. In open-domain question answering, Lewis et al. (2021) point out that successful models might only memorize dataset-specific knowledge. To facilitate this transferability, we need to have units of explanation that can be used in different datasets (Doshi-Velez and Kim, 2018). External Aspect This aspect refers to the relationship between a model’s scores on different tasks. Yogatama et al. (2019) point out that current models struggle to transfer their ability from a task originally trained on (e.g., MRC) to different unseen tasks (e.g., SRL). To develop a general NLU model, one would expect that a successful MRC model should show sufficient perf"
2021.eacl-main.137,D19-5808,0,0.0326017,"Missing"
2021.eacl-main.137,2020.acl-main.465,0,0.0196021,"ng that the questions comprehensively specify and evaluate textbase-level skills. • Evaluating the capability of the situation model in which propositions are coherently organized 4.2 and are grounded to non-textual information. Construct Validity in MRC Should MRC models mimic human text comprehension? In this paper, we do not argue that MRC models should mimic human text comprehension. However, when we design an NLU task and create datasets for testing human-like linguistic generalization, we can refer to the aforementioned features to frame the intended behavior to evaluate in the task. As Linzen (2020) discusses, the task design is orthogonal to how the intended behavior is realized at the implementation level (Marr, 1982). Table 2 also raises MRC features corresponding to the six aspects of construct validity. In what follows, we elaborate on these correspondings and discuss the missing aspects that are needed to achieve the construct validity of the current MRC. 4 Substantive Aspect This aspect appraises the evidence for the consistency of model behavior. We consider that this is the most important aspect for explaining reading comprehension, a process that subsumes various implicit and c"
2021.eacl-main.137,N19-1112,0,0.0176302,"18) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity recognition (NER), and semantic role labeling (SRL). Although these basic tasks can be accomplished by some recent pretraining-based neural language models (Liu et al., 2019), they are hardly required in NLU tasks including MRC. In the natural language inference task, McCoy et al. (2019) indicate that existing datasets (e.g., Bowman et al. (2015)) may fail to elucidate the syntactic understanding of given sentences. Although it is not obvious that these basic tasks should be included in MRC and it is not easy to circumscribe linguistic knowledge from concrete and abstract knowledge (Zaenen et al., 2005; Manning, 2006), we should always care about the capabilities of basic tasks (e.g., use of checklists 1595 Construct the global structure of propositions. Skills: c"
2021.eacl-main.137,P11-2057,0,0.0118386,"MRC, which can serve as the units for the explanation of reading comprehension. Therefore, our motivation is to provide an overview of the skills as a hierarchical taxonomy and to highlight the missing aspects in existing MRC datasets that are required for comprehensively covering the representation levels. Existing Taxonomies We first provide a brief overview of the existing taxonomies of skills in NLU tasks. For recognizing textual entailment (Dagan et al., 2006), several studies present a classification of reasoning and commonsense knowledge (Bentivogli et al., 2010; Sammons et al., 2010; LoBue and Yates, 2011). For scientific question answering, Jansen et al. (2016) categorize knowledge and inference for an elementary-level dataset. Similarly, Boratko et al. (2018) propose types of Surface Structure This level broadly covers the linguistic information and its semantic meaning, which can be based on the raw textual input. Although these features form a proposition according to psychology, it should be viewed as sentencelevel semantic representation in computational linguistics. This level includes part-of-speech tagging, syntactic parsing, dependency parsing, punctuation recognition, named entity re"
2021.eacl-main.137,D18-1260,0,0.040328,"Missing"
2021.eacl-main.137,P19-1416,0,0.0384813,"Missing"
2021.eacl-main.137,D18-1233,0,0.0538666,"Missing"
2021.eacl-main.137,P17-1075,1,0.843473,"eful for organizing such terms. We ground existing NLP technologies and tasks to different representation levels in the next section. 3.2 knowledge and reasoning for scientific questions in MRC (Clark et al., 2018). A limitation of both these studies is that the proposed sets of knowledge and inference are limited to the domain of elementary-level science. Although some existing datasets for MRC have their own classifications of skills, they are coarse and only cover a limited extent of typical NLP tasks (e.g., word matching and paraphrasing). In contrast, for a more generalizable definition, Sugawara et al. (2017) propose a set of 13 skills for MRC. Rogers et al. (2020) pursue this direction by proposing a set of questions with eight question types. In addition, Schlegel et al. (2020) propose an annotation schema to investigate requisite knowledge and reasoning. Dunietz et al. (2020) propose a template of understanding that consists of spatial, temporal, causal, and motivational questions to evaluate precise understanding of narratives with reference to human text comprehension. In what follows, we describe the three representation levels that basically follow the three representations of the CI model"
2021.eacl-main.137,Q19-1014,0,0.0123835,"urkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi et al., 2019)), and logical reasoning (ReClor (Yu et al., 2020)). 2.2 Benchmarking Issues In some datasets, the performance of machines has already reached human-level performance. However, Jia and Liang (2017) indicate that models can easily be fooled by manual injection of distracting sentences. Their study revealed that quest"
2021.eacl-main.137,N18-1140,0,0.0150031,"). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsourced (SQuAD v1.1 (Rajpurkar et al., 2016)), and collected from examinations (RACE (Lai et al., 2017)). Domains The most popular domain is Wikipedia articles (Natural Questions (Kwiatkowski et al., 2019)), but news articles are also used (Who-didWhat (Onishi et al., 2016)). CliCR (Suster and Daelemans, 2018) and emrQA (Pampari et al., 2018) are datasets in the clinical domain. DuoRC 1593 (Saha et al., 2018) uses movie scripts. Specific Skills Several recently proposed datasets require specific skills including unanswerable questions (SQuAD v2.0 (Rajpurkar et al., 2018)), dialogues (CoQA (Reddy et al., 2019), DREAM (Sun et al., 2019)), multiple-sentence reasoning (MultiRC (Khashabi et al., 2018)), multi-hop reasoning (HotpotQA (Yang et al., 2018)), mathematical and set reasoning (DROP (Dua et al., 2019)), commonsense reasoning (CosmosQA (Huang et al., 2019)), coreference resolution (QuoRef (Dasigi"
2021.eacl-main.137,N19-1421,0,0.0396611,"Missing"
2021.eacl-main.137,W17-2623,0,0.029479,"es (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). Later, fueling the development of neural models, most published datasets have more than a hundred thousand questions that are automatically created (CNN/Daily Mail (Hermann et al., 2015)), crowdsour"
2021.eacl-main.137,D19-1221,0,0.0224497,"ent unseen tasks (e.g., SRL). To develop a general NLU model, one would expect that a successful MRC model should show sufficient performance on other NLU tasks as well. To this end, Wang et al. (2019) propose an evaluation framework with ten different NLU tasks in the same format. Consequential Aspect This aspect refers to the actual and potential consequences of test use. In MRC, this refers to the use of a successful model in practical situations other than tasks, where we need to ensure the robustness of a model to adversarial attacks and the accountability for unintended model behaviors. Wallace et al. (2019) highlight this aspect by showing that existing NLP models are vulnerable to adversarial examples, thereby generating egregious outputs. Summary: Design of Rubric Given the validity aspects, our suggestion is to design a rubric (scoring guide used in education) of what reading comprehension we expect is evaluated in a dataset; this helps to inspect detailed strengths and weaknesses of models that cannot be obtained only by simple accuracy. The rubric should not only cover various linguistic phenomena (the content aspect) but also involve different levels of intermediate evaluation in the readi"
2021.eacl-main.137,Q18-1021,1,0.930753,"ing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are created by experts (QA4MRE (Sutcliffe et al., 2013)). L"
2021.eacl-main.137,D18-1257,0,0.0204048,"k along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) chosen from a candidate set of answers (multiple choice) (MCTest (Richardson et al., 2013)), or (iii) generated as a free-form text (description) (NarrativeQA (Koˇcisk´y et al., 2018)). Some datasets optionally allow answering by a yes/no reply (BoolQ (Clark et al., 2019)). Sourcing Methods Initially, questions in smallscale datasets are"
2021.eacl-main.137,D18-1259,0,0.341169,". Burges (2013) provides a general definition of MRC, i.e., a machine comprehends a passage of text if, for any question regarding that text that can be answered correctly by a majority of native speakers, that machine can provide a string which those speakers would agree both answers that question. We overview various aspects of the task along with representative datasets as follows. Existing datasets are listed in Appendix A. Context Styles A context can be given in various forms with different lengths such as a single passage (MCTest (Richardson et al., 2013)), a set of passages (HotpotQA (Yang et al., 2018)), a longer document (CBT (Hill et al., 2016)), or open domain (Chen et al., 2017). In some datasets, a context includes non-textual information such as images (RecipeQA (Yagcioglu et al., 2018)). Question Styles A question can be an interrogative sentence (in most datasets), a fill-in-theblank sentence (cloze) (CLOTH (Xie et al., 2018)), knowledge base entries (QAngaroo (Welbl et al., 2018)) and search engine queries (MSMARCO (Nguyen et al., 2016)). Answering Styles An answer can be (i) chosen from a text span of the given document (answer extraction) (NewsQA (Trischler et al., 2017)), (ii) c"
2021.eacl-main.137,D18-1009,0,0.0563267,"aset biases embedded by annotators. If machine learning models exploit such biases for answering questions, we cannot evaluate the precise NLU of models. Following Geirhos et al. (2020), we define shortcut-proof questions as ones that prevent models from exploiting dataset biases and learning decision rules (shortcuts) that perform well only on i.i.d. test examples with regard to its training examples. Gardner et al. (2019) also point out the importance of mitigating shortcuts in MRC. In this section, we view two different approaches for this challenge. Removing Unintended Biases by Filtering Zellers et al. (2018) propose a model-based adversarial filtering method that iteratively trains an ensemble of stylistic classifiers and uses them to filter out the questions. Sakaguchi et al. (2020) also propose filtering methods based on both machines and humans to alleviate dataset-specific and word-association biases. However, a major issue is the inability to discern knowledge from bias in a closed domain. When the domain is equal to a dataset, patterns that are valid only in the domain are called dataset-specific biases (or annotation artifacts in the labeled data). When the domain covers larger corpora, th"
2021.eacl-main.170,D15-1162,0,0.0292874,"Missing"
2021.eacl-main.170,D10-1101,0,0.0517926,"ions between opinion targets and the corresponding opinion words. • We propose a novel attention-based relational graph convolutional network, ARGCN, an extension of R-GCNs suited to encode syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABSA, target-oriented word extraction, aiming to extract the corresponding op"
2021.eacl-main.170,P19-1285,0,0.0605958,"Missing"
2021.eacl-main.170,C10-1074,0,0.0592666,"ts and the corresponding opinion words. • We propose a novel attention-based relational graph convolutional network, ARGCN, an extension of R-GCNs suited to encode syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABSA, target-oriented word extraction, aiming to extract the corresponding opinion words for th"
2021.eacl-main.170,N19-1259,0,0.24605,"models on four benchmark datasets. Extensive analysis also demonstrates the effectiveness of each component of our models. Our code is available at https: //github.com/wcwowwwww/towe-eacl. 1 The food is tasty and portion sizes are appropriate. Target: food Opinion: tasty The food is tasty and portion sizes are appropriate. Target: portion size Opinion: appropriate Figure 1: Examples of TOWE task. The words highlighted in orange represent the given opinion targets, whereas the words in blue represent the corresponding opinion words. Introduction Target-oriented opinion words extraction (TOWE) (Fan et al., 2019) is a subtask of aspect-based sentiment analysis (ABSA) (Hu and Liu, 2004; Pontiki et al., 2016). Given a review and an opinion target in the sentence, the objective of TOWE is to extract the corresponding opinion words describing or evaluating the opinion targets from the review. Opinion targets are the words or phrases representing features or entities toward which users express their attitudes, whereas opinion words referring to * These authors contributed equally to this work; the order is random. † Corresponding author. Figure 1 shows two examples of TOWE. In the review “The food is tasty"
2021.eacl-main.170,D17-1310,0,0.0150218,"syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABSA, target-oriented word extraction, aiming to extract the corresponding opinion words for the given opinion targets in a review. They released four benchmark datasets for evaluation, designed a target-fused model, and achieved excellent performance. Wu et a"
2021.eacl-main.170,D12-1123,0,0.0224392,"ency graphs of sentences and establish the relations between opinion targets and the corresponding opinion words. • We propose a novel attention-based relational graph convolutional network, ARGCN, an extension of R-GCNs suited to encode syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABSA, target-oriented"
2021.eacl-main.170,D15-1168,0,0.0267955,"last hidden states of the pre-trained BERT (Devlin et al., 2018) as word representations and fine-tuned it jointly. Inspired by Xu et al. (2018), we fine-tuned the GloVe vectors during training to obtain a domain-specific representation. The dimension of target embedding was 3 and 100 1991 • Early Solutions: Some early solutions including rule-based methods and trivial deep learning methods are assigned to the first group. Inspired by Hu and Liu (2004) and Zhuang et al. (2006), Fan et al. (2019) proposed the Distance-rule and Dependency-rule as two representative rule-based methods. Following Liu et al. (2015) and Tang et al. (2016), Fan et al. (2019) proposed LSTM/BiLSTM and the TC-BiLSTM as some trivial deep learning methods. Besides, Fan et al. (2019) combined BiLSTM and Distance-rule method to complete TOWE in a pipelined way, which is named as Pipeline in the experiments. • TOWE models: IOG is the first TOWE model proposed by Fan et al. (2019). It adopts six different positional and directional LSTMs to extract the opinion words. PE-BiLSTM is the base model of the LOTN (Wu et al., 2020). They introduced target information of TOWE by position embedding and extracted opinion words with a BiLSTM."
2021.eacl-main.170,N18-2078,0,0.127968,"ver, both studies only introduce parts of target information (the position information of targets). In this paper, we introduce the target-aware representation to fully exploit opinion target information in a concise way, which is especially important when our models are used for real-world reviews. Becase TOWE can be viewed as a syntactic task, a natural solution is analysing the relationship between opinion targets and opinion words by dependency parsing. Recently, owing to the great success of graph convolutional networks (GCNs) in various fields (Kipf and Welling, 2016; Chen et al., 2018; Marcheggiani et al., 2018), a few researchers have attempted to encode the syntactic 1986 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1986–1997 April 19 - 23, 2021. ©2021 Association for Computational Linguistics dependency information with GCNs to build a robust dependency encoder. For example, GCNs over the dependency tree have been exploited to perform semantic role labelling (Marcheggiani and Titov, 2017) and named entity recognition (Cetoli et al., 2017).In addition, several studies explore GCNs over a dependency graph to complete the ABSA task"
2021.eacl-main.170,D17-1159,0,0.0623428,"Missing"
2021.eacl-main.170,D14-1162,0,0.0855485,"Missing"
2021.eacl-main.170,S15-2082,0,0.0296354,"states of the pre-trained BERT (Devlin et al., 2018) as word representations and fine-tuned it jointly. Inspired by Xu et al. (2018), we fine-tuned the GloVe vectors during training to obtain a domain-specific representation. The dimension of target embedding was 3 and 100 1991 • Early Solutions: Some early solutions including rule-based methods and trivial deep learning methods are assigned to the first group. Inspired by Hu and Liu (2004) and Zhuang et al. (2006), Fan et al. (2019) proposed the Distance-rule and Dependency-rule as two representative rule-based methods. Following Liu et al. (2015) and Tang et al. (2016), Fan et al. (2019) proposed LSTM/BiLSTM and the TC-BiLSTM as some trivial deep learning methods. Besides, Fan et al. (2019) combined BiLSTM and Distance-rule method to complete TOWE in a pipelined way, which is named as Pipeline in the experiments. • TOWE models: IOG is the first TOWE model proposed by Fan et al. (2019). It adopts six different positional and directional LSTMs to extract the opinion words. PE-BiLSTM is the base model of the LOTN (Wu et al., 2020). They introduced target information of TOWE by position embedding and extracted opinion words with a BiLSTM."
2021.eacl-main.170,S14-2004,0,0.0460012,"er of ARGCN, and h0i l is the output of vi in l-th layer of ARGCN. Thus, hl+1 is the input of (l + 1)-th layer of i ARGCN. Sequential Layer and minimized during training. Here, the opinion word tags {O, B, I} are correspondingly numeralized as labels {0, 1, 2}, respectively, and yi denotes the gold label. 1990 4 Experiments 4.1 Datasets and Metrics Following the previous works (Fan et al., 2019; Wu et al., 2020), we evaluate the models on four benchmark datasets, including 14res, 14lap, 15res and 16res. Explicitly, the datasets 14res and 14lap are annotated from SemEval Challenge 2014 task 4 (Pontiki et al., 2014). The 15res and 16res are annotated from SemEval Challenge 2015 task 12 (Pontiki et al., 2015) and SemEval Challenge 2016 task 5 (Pontiki et al., 2016) respectively. The suffixes “res” and “lap” indicate they are collected from restaurant reviews and laptop reviews, respectively. Datasets Train Test Train 14lap Test Train 15res Test Train 16res Test 14res #sentences 1627 500 1158 343 754 325 1079 329 #targets 2643 865 1634 482 1076 436 1512 457 for our base model and GCNs-based models, respectively. We implemented our models with PyTorch (Paszke et al., 2019). We introduced 10 layers of ARGCN"
2021.eacl-main.170,J11-1002,0,0.0276594,"t syntactic dependency graphs of sentences and establish the relations between opinion targets and the corresponding opinion words. • We propose a novel attention-based relational graph convolutional network, ARGCN, an extension of R-GCNs suited to encode syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABS"
2021.eacl-main.170,D19-1569,0,0.223633,"a few researchers have attempted to encode the syntactic 1986 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1986–1997 April 19 - 23, 2021. ©2021 Association for Computational Linguistics dependency information with GCNs to build a robust dependency encoder. For example, GCNs over the dependency tree have been exploited to perform semantic role labelling (Marcheggiani and Titov, 2017) and named entity recognition (Cetoli et al., 2017).In addition, several studies explore GCNs over a dependency graph to complete the ABSA task (Sun et al. (2019), Zhang et al. (2019), Liang et al. (2020), Wang et al. (2020)). However, it is worth mentioning that TOWE is defined as a sequence labelling task, and the manner in which GCNs are applied to TOWE effectively is yet to be explored. In this study, we first construct a directed graph based on a dependency tree to be more suitable for TOWE. Subsequently, we propose ARGCN, which can enhance our model by encoding syntactic information. ARGCN can be seen as extending the Relational Graph Convolutional Networks (R-GCNs) (Schlichtkrull et al., 2018) with the distance-aware attention mechanism. ARGCN c"
2021.eacl-main.170,C16-1311,0,0.0260804,"the pre-trained BERT (Devlin et al., 2018) as word representations and fine-tuned it jointly. Inspired by Xu et al. (2018), we fine-tuned the GloVe vectors during training to obtain a domain-specific representation. The dimension of target embedding was 3 and 100 1991 • Early Solutions: Some early solutions including rule-based methods and trivial deep learning methods are assigned to the first group. Inspired by Hu and Liu (2004) and Zhuang et al. (2006), Fan et al. (2019) proposed the Distance-rule and Dependency-rule as two representative rule-based methods. Following Liu et al. (2015) and Tang et al. (2016), Fan et al. (2019) proposed LSTM/BiLSTM and the TC-BiLSTM as some trivial deep learning methods. Besides, Fan et al. (2019) combined BiLSTM and Distance-rule method to complete TOWE in a pipelined way, which is named as Pipeline in the experiments. • TOWE models: IOG is the first TOWE model proposed by Fan et al. (2019). It adopts six different positional and directional LSTMs to extract the opinion words. PE-BiLSTM is the base model of the LOTN (Wu et al., 2020). They introduced target information of TOWE by position embedding and extracted opinion words with a BiLSTM. Wu et al. (2020) propo"
2021.eacl-main.170,2020.acl-main.295,0,0.376122,"Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1986–1997 April 19 - 23, 2021. ©2021 Association for Computational Linguistics dependency information with GCNs to build a robust dependency encoder. For example, GCNs over the dependency tree have been exploited to perform semantic role labelling (Marcheggiani and Titov, 2017) and named entity recognition (Cetoli et al., 2017).In addition, several studies explore GCNs over a dependency graph to complete the ABSA task (Sun et al. (2019), Zhang et al. (2019), Liang et al. (2020), Wang et al. (2020)). However, it is worth mentioning that TOWE is defined as a sequence labelling task, and the manner in which GCNs are applied to TOWE effectively is yet to be explored. In this study, we first construct a directed graph based on a dependency tree to be more suitable for TOWE. Subsequently, we propose ARGCN, which can enhance our model by encoding syntactic information. ARGCN can be seen as extending the Relational Graph Convolutional Networks (R-GCNs) (Schlichtkrull et al., 2018) with the distance-aware attention mechanism. ARGCN can consider the semantic relevance and syntactic relevance bet"
2021.eacl-main.170,D16-1059,0,0.0173795,"s suited to encode syntactic dependency information. • We propose an ARGCN-based TOWE model. Experimental results show that it significantly outperforms the state-of-the-art model on all datasets of the TOWE task. 2 Related Work As subtasks of ABSA, a series of early studies focused on opinion targets extraction, including unsupervised/semi-supervised methods (Qiu et al., 2011; Liu et al., 2012, 2013) and supervised methods (Jakob and Gurevych, 2010; Li et al., 2010). Some recent studies extracted opinion targets and opinion words jointly in a uniform framework and achieved promising results (Wang et al., 2016; Li and Lam, 2017). However, they did not extract the corresponding relation between opinion targets and opinion words. Moreover, studies on extracting paired opinion relations are rare (Hu and Liu, 2004; Zhuang et al., 2006). Because it is important for downstream sentiment analysis and real-world applications, Fan et al. (2019) proposed a new subtask of ABSA, target-oriented word extraction, aiming to extract the corresponding opinion words for the given opinion targets in a review. They released four benchmark datasets for evaluation, designed a target-fused model, and achieved excellent p"
2021.eacl-main.170,P18-2094,0,0.012243,"classification task, we adopted commonly used evaluation metrics: precision, recall, and F1-score. An extraction is considered as correct only when the opinion words from the beginning to the end are all predicted exactly as the ground truth. 4.2 Compared Methods Experimental Settings For ARGCN and Target-BiLSTM, we adopted 300-dimension GloVe word embeddings (Pennington et al., 2014) as our word representations. For ARGCN-bert and Target-BiLSTM-bert, we adopted the last hidden states of the pre-trained BERT (Devlin et al., 2018) as word representations and fine-tuned it jointly. Inspired by Xu et al. (2018), we fine-tuned the GloVe vectors during training to obtain a domain-specific representation. The dimension of target embedding was 3 and 100 1991 • Early Solutions: Some early solutions including rule-based methods and trivial deep learning methods are assigned to the first group. Inspired by Hu and Liu (2004) and Zhuang et al. (2006), Fan et al. (2019) proposed the Distance-rule and Dependency-rule as two representative rule-based methods. Following Liu et al. (2015) and Tang et al. (2016), Fan et al. (2019) proposed LSTM/BiLSTM and the TC-BiLSTM as some trivial deep learning methods. Beside"
2021.eacl-main.170,D19-1464,0,0.0171568,"ave attempted to encode the syntactic 1986 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1986–1997 April 19 - 23, 2021. ©2021 Association for Computational Linguistics dependency information with GCNs to build a robust dependency encoder. For example, GCNs over the dependency tree have been exploited to perform semantic role labelling (Marcheggiani and Titov, 2017) and named entity recognition (Cetoli et al., 2017).In addition, several studies explore GCNs over a dependency graph to complete the ABSA task (Sun et al. (2019), Zhang et al. (2019), Liang et al. (2020), Wang et al. (2020)). However, it is worth mentioning that TOWE is defined as a sequence labelling task, and the manner in which GCNs are applied to TOWE effectively is yet to be explored. In this study, we first construct a directed graph based on a dependency tree to be more suitable for TOWE. Subsequently, we propose ARGCN, which can enhance our model by encoding syntactic information. ARGCN can be seen as extending the Relational Graph Convolutional Networks (R-GCNs) (Schlichtkrull et al., 2018) with the distance-aware attention mechanism. ARGCN can consider the seman"
2021.eacl-main.304,2020.acl-main.244,0,0.0216262,"Missing"
2021.eacl-main.304,dayrell-etal-2012-rhetorical,0,0.0251575,"or all four sections, i.e. introduction, methods, results, and discussion sections. Maswana et al. (2015) compared the usage of the CFs in five engineering fields and found that certain CFs are preferred depending on the discipline. Argumentative Zoning is a similar concept based on the rhetorical moves (Teufel, 1999). It had seven categories, which were later extended to 15 categories by Teufel et al. (2009) Previous studies on CF-based classification used conditional random fields (Hirohata et al., 2008), a classifier chain with sequential minimum optimisation, Rakel with the J48 algorithm (Dayrell et al., 2012), a Bayes classifier, and a decision tree (Soonklang, 2016). However, these studies only focused on abstracts of scientific papers. Therefore, existing CF-labelled FE lists were created by manually assigning CF labels (Table 1), complicating the construction of a large CF-labelled FE database. Recently, Fiacco et al. (2019) used a hierarchical FE-Extraction Methods Two approaches are used for extracting FEs: corpus- and sentence-level approaches. Based on the intuition that FEs appear frequently or words composing FE are strongly associated, most studies use the corpus-level approach, in which"
2021.eacl-main.304,I08-1050,0,0.0666967,") proposed the CF structure of all the sections in biochemistry papers. Cotos et al. (2015) proposed a CF set for all four sections, i.e. introduction, methods, results, and discussion sections. Maswana et al. (2015) compared the usage of the CFs in five engineering fields and found that certain CFs are preferred depending on the discipline. Argumentative Zoning is a similar concept based on the rhetorical moves (Teufel, 1999). It had seven categories, which were later extended to 15 categories by Teufel et al. (2009) Previous studies on CF-based classification used conditional random fields (Hirohata et al., 2008), a classifier chain with sequential minimum optimisation, Rakel with the J48 algorithm (Dayrell et al., 2012), a Bayes classifier, and a decision tree (Soonklang, 2016). However, these studies only focused on abstracts of scientific papers. Therefore, existing CF-labelled FE lists were created by manually assigning CF labels (Table 1), complicating the construction of a large CF-labelled FE database. Recently, Fiacco et al. (2019) used a hierarchical FE-Extraction Methods Two approaches are used for extracting FEs: corpus- and sentence-level approaches. Based on the intuition that FEs appear"
2021.eacl-main.304,N19-1423,0,0.0209798,"the collected FEs are appropriate. The contributions of our study are as follows: We carefully considered multidisciplinary problems in the classification. Although the development of a training dataset for every discipline in the world is obviously impossible, demonstrating a successful classification using a single disciplinary dataset is not sufficient for practical use. In this study, we determined whether a model trained on a corpus of one discipline can be applied to that of another discipline. Moreover, the effects of a pre-training dataset were examined by comparing SciBERT and BERT (Devlin et al., 2019). The experimental results show that the classifiers performed fairly well in terms of both in-discipline 3477 • we created and published the CF-labelled sentence dataset, which is the first dataset for training and evaluation of CF-based classification; • we showed that a simple SciBERT-based neural classifier performed reasonably well for the CF labelling problem; • we showed that the SciBERT classifier can be used even though the discipline of the training data is different from the inferred one; • we proposed an FE extraction method; and • we constructed a CF-labelled FE database with the"
2021.emnlp-main.167,W07-1401,0,0.063601,"1: Comparison of the structures in the controlled setup used in this study. L, T, R, B, and A in the location format column denote left, top, right, bottom, and area, respectively. Bugliarello et al. (2020)’s controlled setup unifies the use of the location format and global visual features, which were different in the original proposals. It consists of nine tasks: CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QQP3 , STS-B (Cer et al., 2017), MNLI (Nangia et al., 2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al., 2012). STS-B is a single-valued regression task, and the others are classification tasks. We train the controlled pre-trained models on the training sets and evaluate them with the development sets. Figure 1 (left) shows the number of the training sentences in the corpora and their word overlap between the corpus used in the V&L pre-training. 3.2 Implementation Details We fine-tuned pre-trained models published by Bugliarello et al. (2020) 4 . To use a script for the GLUE benchmark, we modified the model codes for Huggingface’s Transformer"
2021.emnlp-main.167,P19-1184,0,0.0521701,"Missing"
2021.emnlp-main.167,D16-1264,0,0.00995267,"l. (2020) not used not used Lu et al. (2019) Tan and Bansal (2019) Table 1: Comparison of the structures in the controlled setup used in this study. L, T, R, B, and A in the location format column denote left, top, right, bottom, and area, respectively. Bugliarello et al. (2020)’s controlled setup unifies the use of the location format and global visual features, which were different in the original proposals. It consists of nine tasks: CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QQP3 , STS-B (Cer et al., 2017), MNLI (Nangia et al., 2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al., 2012). STS-B is a single-valued regression task, and the others are classification tasks. We train the controlled pre-trained models on the training sets and evaluate them with the development sets. Figure 1 (left) shows the number of the training sentences in the corpora and their word overlap between the corpus used in the V&L pre-training. 3.2 Implementation Details We fine-tuned pre-trained models published by Bugliarello et al. (2020) 4 . To use a script for the"
2021.emnlp-main.167,P18-1238,0,0.0186555,"oint is the token type for the vision tokens. In the controlled setup, the token type is not added for ViLBERT and LXMERT because they have separate streams. Of the single-stream models, VisualBERT and Uniter use BERT’s token type ID to specify vision tokens, while VL-BERT adds a new embedding to represent vision tokens. 2.2 Pre-training We summarize the pre-training used in the controlled setup to train the five model structures described above. Note that we omit the detail of the pre-training used in each original paper here. The five models were pre-trained on Google’s Conceptual Captions (Sharma et al., 2018) corpus, which was collected from Web images and their alt-text HTML attributes. The corpus was filtered before training, and the size was approximately 2.7 M pairs as a result. Three tasks, masked language modelling (MLM), masked object classification (MOC), and ITM, were made from image-text pairs in the corpus. Given an image-text pair, the model predicts masked language tokens for MLM, the object class of masked vision tokens for MOC, and whether the pair is correct or not for ITM. The weights of the five models were initialized with the pre-trained weights of BERTBASE if the corresponding"
2021.emnlp-main.167,D13-1170,0,0.00314539,"each word head head Vision type ID from BERT from BERT Original paper Li et al. (2019) Chen et al. (2020) extended Su et al. (2020) not used not used Lu et al. (2019) Tan and Bansal (2019) Table 1: Comparison of the structures in the controlled setup used in this study. L, T, R, B, and A in the location format column denote left, top, right, bottom, and area, respectively. Bugliarello et al. (2020)’s controlled setup unifies the use of the location format and global visual features, which were different in the original proposals. It consists of nine tasks: CoLA (Warstadt et al., 2019), SST-2 (Socher et al., 2013), MRPC (Dolan and Brockett, 2005), QQP3 , STS-B (Cer et al., 2017), MNLI (Nangia et al., 2017), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), and WNLI (Levesque et al., 2012). STS-B is a single-valued regression task, and the others are classification tasks. We train the controlled pre-trained models on the training sets and evaluate them with the development sets. Figure 1 (left) shows the number of the training sentences in the corpora and their word overlap between the corpus used in the V&L pre-training. 3"
2021.emnlp-main.167,P19-1644,0,0.290555,"at the single/dual modification alone has little effect on the GLUE scores, indicating the performance degradation is primarily caused by pre-training. We also see how the V&L models changed from the source LM by analyzing the changes in the model parameters and the problem sets that each model can solve. Our results Pre-trained vision-and-language (V&L) models improve the performance of tasks that require an understanding of the V&L grounding, including visual question answering (Antol et al., 2015), referring expression comprehension (Kazemzadeh et al., 2014), and image-text matching (ITM) (Suhr et al., 2019). Recent V&L tasks, such as multimodal reading comprehension (Kembhavi et al., 2017; Yagcioglu et al., 2018; Hannan et al., 2020; Tanaka et al., 2021) and dialogue (Ilinykh et al., 2019; Haber et al., 2019; Udagawa and Aizawa, 2019), require a deeper NLU as well as the grounding. Extending pre-trained language models (LMs) 1 is an option for those tasks as this allows V&L The source code for our experiments is available at models to inherit language knowledge from their https://github.com/Alab-NII/eval_vl_glue 2189 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Pro"
2021.emnlp-main.167,D19-1514,0,0.0459136,"ore effective to adopt a single stream, and devise pre-training strategies for maintaining language knowledge. 2 Controlled V&L Models In this section, we describe the pre-trained V&L models used in our experiments. Bugliarello et al. (2020) proposed a framework for V&L models that consider a sequence of tokens in sentences as language information, and a sequence of recognized object regions as visual information. In their framework, they reproduced five existing models, VisualBERT (Li et al., 2019), Uniter (Chen et al., 2020), VL-BERT (Su et al., 2020), ViLBERT (Lu et al., 2019), and LXMERT (Tan and Bansal, 2019), and made their controlled versions by modifying some parts for a fairer and easier comparison. We use these controlled versions. 2.1 Structural Modification We describe streams and embeddings, which are the basic factors of the model structures. We summarize the model structures in the controlled setup used in this experiment in Table 1. Streams. V&L models can be divided into two categories based on how the vision and language sequences are encoded. Single-stream models, VisualBERT, Uniter, and VL-BERT, jointly process the vision and language sequences in a single encoder. Dual-stream model"
2021.findings-acl.446,P19-1080,0,0.0282527,"anguage understanding, dialog management, and natural language generation. Natural language understanding converts user utterances to a semantic frame which is considered a dialog state, and a popular method is slot filling (Mrkˇsi´c et al., 2017; Ramadan et al., 2018). The estimated dialog state is then passed on to dialog management to determine the next action, which is formulated as a partially-observable Markov decision process (POMDP) (Young, 2006). The action space is represented with hand-crafted dialog acts (Budzianowski et al., 2018; Stolcke et al., 2000) or meaning representations (Balakrishnan et al., 2019). Finally, a natural language generator generates a response, which is often realized with recurrent neural networks (Zhou et al., 2016; Tran and Nguyen, 2017). Our proposed model spans between dialog management and natural language generation, however, our model does not require any hand-crafted representation. Past works that applied reinforcement learning to dialog models have shown a huge performance improvement in task success (Lewis et al., 2017; He et al., 2018). Li et al. (2016b) proposed a dialog generation method by using deep reinforcement learning with words as action spaces. Altho"
2021.findings-acl.446,D18-1547,0,0.0222579,"Missing"
2021.findings-acl.446,D18-1256,0,0.0188206,"resented with hand-crafted dialog acts (Budzianowski et al., 2018; Stolcke et al., 2000) or meaning representations (Balakrishnan et al., 2019). Finally, a natural language generator generates a response, which is often realized with recurrent neural networks (Zhou et al., 2016; Tran and Nguyen, 2017). Our proposed model spans between dialog management and natural language generation, however, our model does not require any hand-crafted representation. Past works that applied reinforcement learning to dialog models have shown a huge performance improvement in task success (Lewis et al., 2017; He et al., 2018). Li et al. (2016b) proposed a dialog generation method by using deep reinforcement learning with words as action spaces. Although the rewards were carefully designed, it is reported that these models tend to generate incomprehensible responses. Zhao et al. (2019) solved the problem by using discrete latent variables as the action space. Wang et al. (2020a) have extended LaRL and applied hierarchical reinforcement learning technique to decouple the dialog policy and natural language generation. The model is composed of two policy networks; one is the high-level policy which acts on latent dial"
2021.findings-acl.446,D17-1259,0,0.151632,"space has improved flexibility and is effective for solving task-oriented dialogs. 1 Phrase-Level Action Space area sure , to eat type of food several options [restaurant_name] is a preference for what would you like I have hi , a table for you ? … I recommend do you prefer sure , what type of food do you prefer ? System Response Figure 1: Demonstration of phrase-level action. The model generates useful phrases and rearrange them to form a response. Introduction Dialog policy optimization is key research to efficiently solving real-world tasks (Rastogi et al., 2020; Budzianowski et al., 2018; Lewis et al., 2017). In neural response generation, which has made remarkable progress in recent years (Vinyals and Le, 2015; Li et al., 2016a; Serban et al., 2017; Bao et al., 2019), many methods that apply reinforcement learning (RL) have been proposed (Li et al., 2016b; Peng et al., 2018; Saleh et al., 2019; Zhao et al., 2019). In those studies, one major issue was how to define an action space. Early research proposed a method in which each word of the response is an action (Li et al., 2016b). However, this has a shortcoming that the generated responses deviate from natural human language (Zhao et al., 2019)"
2021.findings-acl.446,P16-1094,0,0.081583,"eat type of food several options [restaurant_name] is a preference for what would you like I have hi , a table for you ? … I recommend do you prefer sure , what type of food do you prefer ? System Response Figure 1: Demonstration of phrase-level action. The model generates useful phrases and rearrange them to form a response. Introduction Dialog policy optimization is key research to efficiently solving real-world tasks (Rastogi et al., 2020; Budzianowski et al., 2018; Lewis et al., 2017). In neural response generation, which has made remarkable progress in recent years (Vinyals and Le, 2015; Li et al., 2016a; Serban et al., 2017; Bao et al., 2019), many methods that apply reinforcement learning (RL) have been proposed (Li et al., 2016b; Peng et al., 2018; Saleh et al., 2019; Zhao et al., 2019). In those studies, one major issue was how to define an action space. Early research proposed a method in which each word of the response is an action (Li et al., 2016b). However, this has a shortcoming that the generated responses deviate from natural human language (Zhao et al., 2019). A possible reason is that the action space is huge, making it difficult to optimize with RL. Moreover, rewarding only th"
2021.findings-acl.446,D16-1127,0,0.152785,"eat type of food several options [restaurant_name] is a preference for what would you like I have hi , a table for you ? … I recommend do you prefer sure , what type of food do you prefer ? System Response Figure 1: Demonstration of phrase-level action. The model generates useful phrases and rearrange them to form a response. Introduction Dialog policy optimization is key research to efficiently solving real-world tasks (Rastogi et al., 2020; Budzianowski et al., 2018; Lewis et al., 2017). In neural response generation, which has made remarkable progress in recent years (Vinyals and Le, 2015; Li et al., 2016a; Serban et al., 2017; Bao et al., 2019), many methods that apply reinforcement learning (RL) have been proposed (Li et al., 2016b; Peng et al., 2018; Saleh et al., 2019; Zhao et al., 2019). In those studies, one major issue was how to define an action space. Early research proposed a method in which each word of the response is an action (Li et al., 2016b). However, this has a shortcoming that the generated responses deviate from natural human language (Zhao et al., 2019). A possible reason is that the action space is huge, making it difficult to optimize with RL. Moreover, rewarding only th"
2021.findings-acl.446,P17-1163,0,0.0219216,"Missing"
2021.findings-acl.446,P18-1203,0,0.0662807,"Missing"
2021.findings-acl.446,P18-2069,0,0.018985,"nts for an effective action space. Our code is available at https://github.com/Alab-NII/ PhraseRL. 2 Related Work A classical approach for realizing task-oriented dialog systems is the frame-based dialog system (Chen et al., 2017). This model generates a response in a pipeline fashion, by splitting the generation process into three modules: natural language understanding, dialog management, and natural language generation. Natural language understanding converts user utterances to a semantic frame which is considered a dialog state, and a popular method is slot filling (Mrkˇsi´c et al., 2017; Ramadan et al., 2018). The estimated dialog state is then passed on to dialog management to determine the next action, which is formulated as a partially-observable Markov decision process (POMDP) (Young, 2006). The action space is represented with hand-crafted dialog acts (Budzianowski et al., 2018; Stolcke et al., 2000) or meaning representations (Balakrishnan et al., 2019). Finally, a natural language generator generates a response, which is often realized with recurrent neural networks (Zhou et al., 2016; Tran and Nguyen, 2017). Our proposed model spans between dialog management and natural language generation"
2021.findings-acl.446,N19-1123,0,0.208737,"? System Response Figure 1: Demonstration of phrase-level action. The model generates useful phrases and rearrange them to form a response. Introduction Dialog policy optimization is key research to efficiently solving real-world tasks (Rastogi et al., 2020; Budzianowski et al., 2018; Lewis et al., 2017). In neural response generation, which has made remarkable progress in recent years (Vinyals and Le, 2015; Li et al., 2016a; Serban et al., 2017; Bao et al., 2019), many methods that apply reinforcement learning (RL) have been proposed (Li et al., 2016b; Peng et al., 2018; Saleh et al., 2019; Zhao et al., 2019). In those studies, one major issue was how to define an action space. Early research proposed a method in which each word of the response is an action (Li et al., 2016b). However, this has a shortcoming that the generated responses deviate from natural human language (Zhao et al., 2019). A possible reason is that the action space is huge, making it difficult to optimize with RL. Moreover, rewarding only the task accomplishment can cause biased improvement, which leads the model to ignore the comprehensibility of the generated response (Wang et al., 2020a). To overcome such issues, LaRL (Zhao"
2021.findings-acl.446,C16-1191,0,0.0547282,"Missing"
2021.findings-acl.446,J00-3003,0,0.742004,"Missing"
2021.findings-acl.446,K17-1044,0,0.0127854,"sidered a dialog state, and a popular method is slot filling (Mrkˇsi´c et al., 2017; Ramadan et al., 2018). The estimated dialog state is then passed on to dialog management to determine the next action, which is formulated as a partially-observable Markov decision process (POMDP) (Young, 2006). The action space is represented with hand-crafted dialog acts (Budzianowski et al., 2018; Stolcke et al., 2000) or meaning representations (Balakrishnan et al., 2019). Finally, a natural language generator generates a response, which is often realized with recurrent neural networks (Zhou et al., 2016; Tran and Nguyen, 2017). Our proposed model spans between dialog management and natural language generation, however, our model does not require any hand-crafted representation. Past works that applied reinforcement learning to dialog models have shown a huge performance improvement in task success (Lewis et al., 2017; He et al., 2018). Li et al. (2016b) proposed a dialog generation method by using deep reinforcement learning with words as action spaces. Although the rewards were carefully designed, it is reported that these models tend to generate incomprehensible responses. Zhao et al. (2019) solved the problem by"
2021.findings-acl.446,2020.acl-main.638,0,0.12551,"Peng et al., 2018; Saleh et al., 2019; Zhao et al., 2019). In those studies, one major issue was how to define an action space. Early research proposed a method in which each word of the response is an action (Li et al., 2016b). However, this has a shortcoming that the generated responses deviate from natural human language (Zhao et al., 2019). A possible reason is that the action space is huge, making it difficult to optimize with RL. Moreover, rewarding only the task accomplishment can cause biased improvement, which leads the model to ignore the comprehensibility of the generated response (Wang et al., 2020a). To overcome such issues, LaRL (Zhao et al., 2019) was proposed, which used a discrete global vector to represent dialog acts. In this method, reinforcement learning is performed only on those discrete latent variables, thus the policy optimization is achieved without affecting the language generation. However, LaRL depends on a single vector from the beginning to the end during the response generation, even though a response may often contain more than one dialog act and contents (Wang et al., 2020b). Due to this, a static, global vector tends to be an entangled representation of multiple"
2021.findings-acl.446,D18-1356,0,0.0348323,"Missing"
2021.findings-emnlp.101,N15-1171,0,0.349632,"en et al., 2020), their data sets created using crowdsourcing do not exhibit similar quality as our expert data set. In addition, we include five state-of-the-art neural Online news articles have, over time, started to replace traditional print and radio media as a primary source of information (Dallmann et al., 2015). A varying word choice may have a major effect on the public and individual perception of societal issues, especially since regular news consumers are mostly not fully aware of the degree and scope of bias (Spinde et al., 2020a). As shown in existing research (Park et al., 2009; Baumer et al., 2015), detecting and highlighting media bias might be relevant for media analysis and to mitigate the effects 1 We also provide another 1,000 yet unlabeled sentences of biased reports on readers. Also, the detection for future work. We have not labeled them to date due to of media bias can assist journalists and publishers resource restrictions. 2 in their work (Spinde et al., 2021b). To date, only With the 1,700 stemming from MBIC (Spinde et al., a few research projects focus on the detection and 2021c). 1166 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1166–1177 No"
2021.findings-emnlp.101,2020.nlpcss-1.16,0,0.457951,"g topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1 -score of 0.804, outperforming existing methods. 1 Introduction aggregation of bias (Lim et al., 2020; Spinde et al., 2020c). Even though bias embodies a complex structure, contributions (Hube and Fetahu, 2019; Chen et al., 2020) often neglect annotator background and use crowdsourcing to collect annotations. Therefore, existing data sets exhibit low annotator agreement and inferior quality. Our study holds both theoretical and practical significance. We propose BABE (Bias Annotations By Experts), a data set of media bias annotations, which is built on top of the MBIC data set (Spinde et al., 2021c). MBIC offers a balanced content selection, annotations on a word and sentence level, and is with 1,700 annotated sentences one of the largest data sets available in the domain. BABE improves MBIC, and other data sets, in t"
2021.findings-emnlp.101,N19-1423,0,0.00542909,"sk can be optimized by minimizing the binary cross-entropy loss L := − N 1 X X fk (xi ) · log(fˆk (xi )). (1) N i=1 k={0,1} where fk (·) is a binary indicator triggering 0 in the case of neutral labels and 1 in the case of a biased sequence. fˆk (·) is a scalar representing the language model score for the given sequence. 4.2 Neural Models We fit fˆk (·) using a range of state-of-the-art language models. Central to the architectural design of these models is Vaswani et al. (2017)’s encoder stack of the Transformer relying solely on the attention mechanism. Specifically, we use the BERT model (Devlin et al., 2019) and its variants DistilBERT (Sanh et al., 2019) and RoBERTa (Liu et al., 2019) that learned bidirectional language representations from the unlabeled text. DistilBERT is a compressed model of the original BERT, and RoBERTa uses a slightly different loss function with more training data than its predecessor. We Further statistics on SG 1 and SG 2 such as bias/opinion distribution per news outlet and topic, also evaluate models built on the transformer architecture but differ in the training objective. While the connection between bias and opinion, and the overall topic distribution are provide"
2021.findings-emnlp.101,D19-1664,0,0.329171,"66 sentences labeled on the sentence level. However, their reported interrater-agreement (IRR) of Fleiss’ Kappa on different topics averages at zero. Baumer et al. (2015) classify framing in political news. Using crowdsourcing, they label 74 news articles from eight US news outlets, collected from politics-specific RSS feeds on two separate days. Chen et al. (2020) create a data set of 6,964 articles containing political bias, unfairness, and nonobjectivity labels at the article level. Altogether, they present 11 different topics such as “presidential election”, “politics”, and “white house”. Fan et al. (2019) present 300 news articles containing annotations for lexical and informational bias made by two experts. They define lexical bias as bias stemming from specific word choice, and informational bias as sentences conveying information tangential or speculative to sway readers’ opinions towards entities (Fan et al., 2019). Their data set, BASIL, allows for analysis at the token level and relative to the target, but only 448 sentences are available for lexical bias. Under the name MBIC, Spinde et al. (2021c) extract 1,700 sentences from 1,000 news articles. Crowdsource workers then label bias and"
2021.findings-emnlp.101,P18-1031,0,0.0164285,"controversial topics. The obtained corpus consisting of 83,143 neutral news headlines and 45,605 biased instances allows for the encoding of a sequence’s bias information in the embedded space. The news headlines corpus serves to learn more effective language representations, it is not suitable for evaluation purposes due to its noisy nature. We ensure that no overlap exists between the distant corpus and BABE to guarantee model to guarantee model integrity with respect to training and testing. Fine-tuning general language models on the target task has proven beneficial for many tasks in NLP (Howard and Ruder, 2018). The language model pre-training followed by fine-tuning allows models to incorporate the idiosyncrasies of the target cor- 5 Experiments pus. For text classification, the authors of ULMFiT (Howard and Ruder, 2018) demonstrated the supe- Training Protocol. We implement the neural modriority of task-specific word embeddings. Before els with HuggingFace’s Transformer API (Wolf fine-tuning, we introduce an additional pre-training et al., 2020). The model components are instantask to improve feature learning capabilities consid- tiated with their pre-trained parameters. Parameering media bias con"
2021.findings-emnlp.101,2020.lrec-1.184,0,0.170896,"etter annotation quality and higher inter-annotator agreement than existing work. It consists of 3,700 sentences balanced among topics and outlets, containing media bias labels on the word and sentence level. Based on our data, we also introduce a way to detect bias-inducing sentences in news articles automatically. Our best performing BERT-based model is pre-trained on a larger corpus consisting of distant labels. Fine-tuning and evaluating the model on our proposed supervised data set, we achieve a macro F1 -score of 0.804, outperforming existing methods. 1 Introduction aggregation of bias (Lim et al., 2020; Spinde et al., 2020c). Even though bias embodies a complex structure, contributions (Hube and Fetahu, 2019; Chen et al., 2020) often neglect annotator background and use crowdsourcing to collect annotations. Therefore, existing data sets exhibit low annotator agreement and inferior quality. Our study holds both theoretical and practical significance. We propose BABE (Bias Annotations By Experts), a data set of media bias annotations, which is built on top of the MBIC data set (Spinde et al., 2021c). MBIC offers a balanced content selection, annotations on a word and sentence level, and is wi"
2021.findings-emnlp.101,2021.ccl-1.108,0,0.0611639,"Missing"
2021.findings-emnlp.101,P13-1162,0,0.21877,"e a pre-training task helping the model to learn bias-specific embeddings by considering bias information when optimizing its loss function. For the classification presented in this paper, we focus on sentence level bias detection, which is the current standard in related work (Section 2)3 . We address future work on word level bias in Section 7. We publish all our code and resources on https://github.com /Media-Bias-Analysis-Group/Neura l-Media-Bias-Detection-Using-Dis tant-Supervision-With-BABE. 2 Related Work Media bias can be defined as slanted news coverage or internal news article bias (Recasens et al., 2013). While there are multiple forms of bias, e.g., bias by personal perception or by the omission of information (Puglisi and Snyder, 2015), our focus is on bias caused by word choice, in which different words refer to the same concept. For a detailed explanation of the types of media bias, we refer to Spinde et al. (2021b). In the following, we summarize the existing literature on bias data sets and media bias classification. 2.1 Media Bias Data Sets Lim et al. (2018) present 1,235 sentences labeled for word and sentence level bias by crowdsource workers. All the sentences in their data set focu"
2021.findings-emnlp.101,2020.acl-main.442,0,0.015682,"recurrence and attention-based models. Hand-crafted features like static dictionaries cannot adequately address the complexity and context-dependence of bias. We argue that standard metrics (e.g., accuracy and F1 ) provide a limited perspective into a model’s predictive power in case of a complex construct like media bias. Further research needs to tackle these pitfalls to propose systems with better generalization capabilities. A promising starting point might be a more refined evaluation scheme that decomposes the bias detection task into multiple sub-tasks, such as presented in CheckList (Ribeiro et al., 2020). This scheme will also allow us to understand how our system performs 19 on different types of bias (e.g., bias by context, In this Section, we show three decimal places to account for detailed model differences. by linguistics, by overall reporting). Additionally, 1173 we believe that current research on explainable artificial intelligence might increase users’ trust in neural-based classifiers. Existing research already presents ways to visualize Transformer-based models and make their results more accessible and interpretable (Vig, 2019). Lastly, combining neural methods with advances in l"
2021.findings-emnlp.101,P14-1146,0,0.132045,"sentences of biased reports on readers. Also, the detection for future work. We have not labeled them to date due to of media bias can assist journalists and publishers resource restrictions. 2 in their work (Spinde et al., 2021b). To date, only With the 1,700 stemming from MBIC (Spinde et al., a few research projects focus on the detection and 2021c). 1166 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1166–1177 November 7–11, 2021. ©2021 Association for Computational Linguistics models in our comparison and extend two of them in a distant supervision approach (Tang et al., 2014; Deriu et al., 2017). Leveraging large amounts of distantly labeled data, we formulate a pre-training task helping the model to learn bias-specific embeddings by considering bias information when optimizing its loss function. For the classification presented in this paper, we focus on sentence level bias detection, which is the current standard in related work (Section 2)3 . We address future work on word level bias in Section 7. We publish all our code and resources on https://github.com /Media-Bias-Analysis-Group/Neura l-Media-Bias-Detection-Using-Dis tant-Supervision-With-BABE. 2 Related W"
2021.findings-emnlp.101,S15-2079,0,0.163459,". All the sentences in their data set focus on one event. Another data set focusing on just one event is presented by Färber et al. (2020). It consists of 2,057 sentences from 90 news articles, annotated with bias labels on article and sentence levels, and contains labels such as overall bias, hidden assumption, and framing. The annotators agree with a Krippendorff’s α = -0.05. Lim et al. (2020) also provide a second data set with 966 sentences labeled on the sentence level. However, their reported interrater-agreement (IRR) of Fleiss’ Kappa on different topics averages at zero. Baumer et al. (2015) classify framing in political news. Using crowdsourcing, they label 74 news articles from eight US news outlets, collected from politics-specific RSS feeds on two separate days. Chen et al. (2020) create a data set of 6,964 articles containing political bias, unfairness, and nonobjectivity labels at the article level. Altogether, they present 11 different topics such as “presidential election”, “politics”, and “white house”. Fan et al. (2019) present 300 news articles containing annotations for lexical and informational bias made by two experts. They define lexical bias as bias stemming from"
2021.findings-emnlp.101,P19-3007,0,0.0206845,"ub-tasks, such as presented in CheckList (Ribeiro et al., 2020). This scheme will also allow us to understand how our system performs 19 on different types of bias (e.g., bias by context, In this Section, we show three decimal places to account for detailed model differences. by linguistics, by overall reporting). Additionally, 1173 we believe that current research on explainable artificial intelligence might increase users’ trust in neural-based classifiers. Existing research already presents ways to visualize Transformer-based models and make their results more accessible and interpretable (Vig, 2019). Lastly, combining neural methods with advances in linguistic bias theory (Spinde et al., 2021b) to explain a classifier’s decision to users will also be part of our future work. For this work, we focused on sentence level bias, which is often used in the media bias domain. Still, in addition to the 3,700 labeled sentences, we also include word level annotations in our data set to encourage solutions focusing on more granular characteristics. We believe that word level bias conveys strong explanatory and structural knowledge and see a detailed word level bias analysis and detection as a promi"
2021.findings-emnlp.101,2020.emnlp-demos.6,0,0.0350019,"Missing"
2021.mrqa-1.6,P17-1123,0,0.10935,"ed nor generated correctly by neural models. To investigate this phenomenon, we first analyze the QA and QG performance in terms of QCLO. • We found that not only QA but also QG models are biased in terms of question–context lexical overlap; that is, QG models fail to generate questions with low lexical overlap (§2). Experimental setups For QA, we use the finetuned BERT-base and -large models (Devlin et al., 2019). For QG, we use SemanticQG (Zhang and Bansal, 2019).3 For the dataset, we use the SQuAD-Du dataset; the train, dev, and test split of SQuAD v1.1 (Rajpurkar et al., 2016) proposed by Du et al. (2017), which we denote as SQuADDu train , Du , respectively. This split SQuADDu , SQuAD test dev • We discovered that data augmentation using recent neural QG models does not contribute to debias QA datasets; rather, it frequently degrades the QA performance on questions with low lexical overlap, while improving that on questions with high lexical overlap (§4). 2 • We demonstrated that the proposed simple data augmentation approach using synonym replacement (§3) for augmenting questions with low lexical overlap is effective to improve When computing lexical overlap, we do not exclude stop words bec"
2021.mrqa-1.6,2020.acl-main.253,0,0.0300601,"ce in a training set. Our proposed approach has the same motivation as these works. On the other hand, data augmentation can unintentionally introduce or amplify dataset bias. Backtranslation (Sennrich et al., 2016), which is the common data augmentation approach for machine translation, can introduce the translationese bias. That is, machine translation systems trained with back-translation, compared to ones without backtranslation, can enhance the BLEU scores when the input is translationese (i.e., human-translated texts) but harm the BLEU scores when the input is naturally occurring texts (Edunov et al., 2020; Marie et al., 2020). This phenomenon is analogous to the observation in our work, where we demonstrated that SQuAD QG models are biased towards generating questions with high QCLO, and this tendency can harm the QA performance on questions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust"
2021.mrqa-1.6,P19-1610,0,0.0175699,"phenomenon is analogous to the observation in our work, where we demonstrated that SQuAD QG models are biased towards generating questions with high QCLO, and this tendency can harm the QA performance on questions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased questions (Gan and Ng, 2019), implications derived from SQuAD (Ribeiro et al., 2019), questions with low lexical overlap (Sugawara et al., 2018), and other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Ko et al. (2020) showed that extractive QA model can suffer from positional bias and fail to generalize to different answer positions. The lack of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesqu"
2021.mrqa-1.6,P16-1154,0,0.0331645,"mplies that some under-represented features (e.g., multiple-sentence reasoning (Rajpurkar et al., 2016)) exist even in the Easy subset, and the existing neural QG models might amplify such features (possibly by copying many words from multiple sentences to formulate questions) and make it easy to capture them. Investigating what kind of features are learned by using data augmentation with neural QG models in more detail is future work. 6 QG are not freely available (Du and Cardie, 2018; Lee et al., 2020; Shinoda et al., 2021). The de facto standard of QG models is to utilize a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016). The tendency of QG models to copy words from textual contexts as indicated in Figure 2 is partially due to this copy mechanism. While the existing QG works have increased the BLEU scores on SQuAD8 and successfully generated fluent questions in terms of human scores, the bias regarding lexical overlap in QG has not received sufficient attention. Data Augmentation and Dataset Bias Data augmentation has been widely used in other domains to reduce dataset biases such as the background bias in person re-identification (McLaughlin et al., 2015), the gender bias in coreferen"
2021.mrqa-1.6,P16-1014,0,0.027033,"under-represented features (e.g., multiple-sentence reasoning (Rajpurkar et al., 2016)) exist even in the Easy subset, and the existing neural QG models might amplify such features (possibly by copying many words from multiple sentences to formulate questions) and make it easy to capture them. Investigating what kind of features are learned by using data augmentation with neural QG models in more detail is future work. 6 QG are not freely available (Du and Cardie, 2018; Lee et al., 2020; Shinoda et al., 2021). The de facto standard of QG models is to utilize a copy mechanism (Gu et al., 2016; Gulcehre et al., 2016). The tendency of QG models to copy words from textual contexts as indicated in Figure 2 is partially due to this copy mechanism. While the existing QG works have increased the BLEU scores on SQuAD8 and successfully generated fluent questions in terms of human scores, the bias regarding lexical overlap in QG has not received sufficient attention. Data Augmentation and Dataset Bias Data augmentation has been widely used in other domains to reduce dataset biases such as the background bias in person re-identification (McLaughlin et al., 2015), the gender bias in coreference resolution (Zhao et a"
2021.mrqa-1.6,N19-1423,0,0.659583,"models can contribute to debiasing QA models (i.e., improve the robustness of QA models to under-represented questions). In this study, we focus on question–context lexical overlap, inspired by the findings presented in Sugawara et al. (2018). Their work revealed that questions having low lexical overlap with context tend to require reasoning skills rather than superficial word matching, and existing QA models are not robust to these questions (Table 1). To see if data augmentation with recent neural QG models can improve the robustness to those questions, we analyze the performance of BERT (Devlin et al., 2019) trained on SQuAD v1.1 (Rajpurkar et al., 2016) augmented with them. Our analysis reveals that data augmentation with neural QG models frequently sacrifices the QA performance of the BERT-base model on questions with low lexical overlap, while improving that on questions with high lexical overlap. We conjecture that this is because neural QG models frequently generate questions with high lexical overlap as indicated in Table Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question–context lexical overlap. This hinders"
2021.mrqa-1.6,D17-1215,0,0.0715092,"Missing"
2021.mrqa-1.6,N18-2092,0,0.0179081,"y to unlabeled passages. 4 The distributions of the lexical overlap of these datasets are presented in Figure 2. We indicate that these methods are more biased towards high lexical overlap than SQuADDu train , which was used as the training set for these QG models. Experiments To determine the effect of data augmentation on improving the QA model robustness to questions with low QCLO, we conducted experiments with several QG approaches. Experimental Setups As in our previous experiment (§2), we used BERT-base and -large models, whose total number of parameters are 110M and 340M, respectively. Dhingra et al. (2018) proposed to pretrain a QA model using synthetic data composed of cloze-style questions and then finetune it on the ground-truth data. We adopted the pretrain-and-fine-tune approach for the neural QG approaches, which generated over 1.2 million questions. However, as discussed by Zhang and Bansal (2019), we observed that when the size of the synthetic data was small or similar to the ground-truth data, a performance gain could not be obtained by the pretrain-and-fine-tune approach. Thus, for the proposed approach, which generated 70k questions, we fine-tuned QA models on the ground-truth data"
2021.mrqa-1.6,P18-1177,0,0.0460714,"Missing"
2021.mrqa-1.6,2020.emnlp-main.84,0,0.0174894,"hile improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased questions (Gan and Ng, 2019), implications derived from SQuAD (Ribeiro et al., 2019), questions with low lexical overlap (Sugawara et al., 2018), and other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Ko et al. (2020) showed that extractive QA model can suffer from positional bias and fail to generalize to different answer positions. The lack of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesque, 2014). Therefore, evaluating QA models only on an in-distribution test set is not enough to evaluate the robustness of the QA models. Question Generation for Question Answering QG has been studied extensively in order to augme"
2021.mrqa-1.6,2021.acl-srw.21,1,0.860756,"wara2 Akiko Aizawa1,2 1 The University of Tokyo 2 National Institute of Informatics shinoda@is.s.u-tokyo.ac.jp {saku,aizawa}@nii.ac.jp Abstract and Liang, 2017; Sugawara et al., 2018; Gan and Ng, 2019; Ribeiro et al., 2019), which could be a serious problem in real-world applications. Question generation (QG) has also been extensively studied to augment QA datasets (Du et al., 2017; Du and Cardie, 2018). It is demonstrated that QG can improve not only the in-domain generalization but also the out-of-distribution generalization capability of QA models (Zhang and Bansal, 2019; Lee et al., 2020; Shinoda et al., 2021). In other areas, data augmentation techniques have been successfully used to reduce dataset biases and increase the performance of machine learning models on under-represented samples in vision (McLaughlin et al., 2015; Wong et al., 2016) and language (Zhao et al., 2018; Zhou and Bansal, 2020). Thus, we assume that QG is useful to debias QA models and improve its robustness by augmenting QA datasets. However, it has not been fully studied whether existing QG models can contribute to debiasing QA models (i.e., improve the robustness of QA models to under-represented questions). In this study,"
2021.mrqa-1.6,2020.acl-main.20,0,0.345747,"inoda1,2 Saku Sugawara2 Akiko Aizawa1,2 1 The University of Tokyo 2 National Institute of Informatics shinoda@is.s.u-tokyo.ac.jp {saku,aizawa}@nii.ac.jp Abstract and Liang, 2017; Sugawara et al., 2018; Gan and Ng, 2019; Ribeiro et al., 2019), which could be a serious problem in real-world applications. Question generation (QG) has also been extensively studied to augment QA datasets (Du et al., 2017; Du and Cardie, 2018). It is demonstrated that QG can improve not only the in-domain generalization but also the out-of-distribution generalization capability of QA models (Zhang and Bansal, 2019; Lee et al., 2020; Shinoda et al., 2021). In other areas, data augmentation techniques have been successfully used to reduce dataset biases and increase the performance of machine learning models on under-represented samples in vision (McLaughlin et al., 2015; Wong et al., 2016) and language (Zhao et al., 2018; Zhou and Bansal, 2020). Thus, we assume that QG is useful to debias QA models and improve its robustness by augmenting QA datasets. However, it has not been fully studied whether existing QG models can contribute to debiasing QA models (i.e., improve the robustness of QA models to under-represented ques"
2021.mrqa-1.6,2020.acl-main.532,0,0.0258237,"Our proposed approach has the same motivation as these works. On the other hand, data augmentation can unintentionally introduce or amplify dataset bias. Backtranslation (Sennrich et al., 2016), which is the common data augmentation approach for machine translation, can introduce the translationese bias. That is, machine translation systems trained with back-translation, compared to ones without backtranslation, can enhance the BLEU scores when the input is translationese (i.e., human-translated texts) but harm the BLEU scores when the input is naturally occurring texts (Edunov et al., 2020; Marie et al., 2020). This phenomenon is analogous to the observation in our work, where we demonstrated that SQuAD QG models are biased towards generating questions with high QCLO, and this tendency can harm the QA performance on questions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased quest"
2021.mrqa-1.6,P19-1485,0,0.0200043,"m the QA performance on questions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased questions (Gan and Ng, 2019), implications derived from SQuAD (Ribeiro et al., 2019), questions with low lexical overlap (Sugawara et al., 2018), and other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Ko et al. (2020) showed that extractive QA model can suffer from positional bias and fail to generalize to different answer positions. The lack of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesque, 2014). Therefore, evaluating QA models only on an in-distribution test set is not enough to evaluate the robustness of the QA models. Question Generation for Question Answering QG has"
2021.mrqa-1.6,D19-1670,0,0.0217767,"with low QCLO. 3 is commonly used in the QG literature (Du and Cardie, 2018; Zhang and Bansal, 2019) because the original test set is not released. The numbers of question, answer and context triples in SQuADDu train , Du Du SQuADdev , and SQuADtest , are 76k, 11k, and 12k, respectively. Method We assume that if we augment questions with low QCLO unlike existing neural QG approaches, the robustness of QA models to questions with low QCLO can be improved. In this section, we describe the proposed method for generating questions with low QCLO. We extend the idea of synonym replacement used in (Wei and Zou, 2019) to reduce the lexical overlap. The proposed method is as follows: Results We show the result in Figure 1. This indicates that the performance of the BERT models on the questions with lower QCLO is degraded compared to the questions with higher QCLO. For QG, the BLEU-4 score (Papineni et al., 2002) is highly correlated with QCLO, which means that the model fails to generate questions with low QCLO accurately. 1. List all the overlapping words between question and context. 2. Replace every word in the listed words other than predefined stop words with one of its synonyms chosen randomly from Wo"
2021.mrqa-1.6,P02-1040,0,0.109281,"d We assume that if we augment questions with low QCLO unlike existing neural QG approaches, the robustness of QA models to questions with low QCLO can be improved. In this section, we describe the proposed method for generating questions with low QCLO. We extend the idea of synonym replacement used in (Wei and Zou, 2019) to reduce the lexical overlap. The proposed method is as follows: Results We show the result in Figure 1. This indicates that the performance of the BERT models on the questions with lower QCLO is degraded compared to the questions with higher QCLO. For QG, the BLEU-4 score (Papineni et al., 2002) is highly correlated with QCLO, which means that the model fails to generate questions with low QCLO accurately. 1. List all the overlapping words between question and context. 2. Replace every word in the listed words other than predefined stop words with one of its synonyms chosen randomly from WordNet (Miller, 1995), and obtain a synthetic question. We also show the distributions in terms of QCLO of questions generated by recent neural QG models (HarvestingQG (Du and Cardie, 2018), SemanticQG (Zhang and Bansal, 2019), InfoHCVAE (Lee et al., 2020), and VQAG (Shinoda et al., 2021)) in Figure"
2021.mrqa-1.6,D16-1264,0,0.674854,"s (i.e., improve the robustness of QA models to under-represented questions). In this study, we focus on question–context lexical overlap, inspired by the findings presented in Sugawara et al. (2018). Their work revealed that questions having low lexical overlap with context tend to require reasoning skills rather than superficial word matching, and existing QA models are not robust to these questions (Table 1). To see if data augmentation with recent neural QG models can improve the robustness to those questions, we analyze the performance of BERT (Devlin et al., 2019) trained on SQuAD v1.1 (Rajpurkar et al., 2016) augmented with them. Our analysis reveals that data augmentation with neural QG models frequently sacrifices the QA performance of the BERT-base model on questions with low lexical overlap, while improving that on questions with high lexical overlap. We conjecture that this is because neural QG models frequently generate questions with high lexical overlap as indicated in Table Question answering (QA) models for reading comprehension have been demonstrated to exploit unintended dataset biases such as question–context lexical overlap. This hinders QA models from generalizing to underrepresente"
2021.mrqa-1.6,P19-1621,0,0.0266479,"ork, where we demonstrated that SQuAD QG models are biased towards generating questions with high QCLO, and this tendency can harm the QA performance on questions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased questions (Gan and Ng, 2019), implications derived from SQuAD (Ribeiro et al., 2019), questions with low lexical overlap (Sugawara et al., 2018), and other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Ko et al. (2020) showed that extractive QA model can suffer from positional bias and fail to generalize to different answer positions. The lack of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesque, 2014). Therefore, evaluating QA models only on an in-"
2021.mrqa-1.6,P17-1096,0,0.0174112,"k of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesque, 2014). Therefore, evaluating QA models only on an in-distribution test set is not enough to evaluate the robustness of the QA models. Question Generation for Question Answering QG has been studied extensively in order to augment QA datasets and boost the QA performance, which has been evaluated primarily on SQuAD (Du et al., 2017; Zhou et al., 2018; Yang et al., 2017; Zhang and Bansal, 2019). Question answer pair generation, which consists of answer candidate extraction and QG, has been also received attention because question-worthy answers for the input of We demonstrated that not only QA models but also QG models are biased in terms of the question– context lexical overlap. To determine the influence of the bias, we analyzed the QA performance with data augmentation using the recent QG models. We demonstrated that they frequently degraded the QA 7 https://rajpurkar.github.io/ SQuAD-explorer/ 8 http://aqleaderboard.tomhosking.co. uk/squad 7 69 Conclusio"
2021.mrqa-1.6,2020.emnlp-main.190,0,0.0191254,"uestions with low QCLO while improving that on questions with high QCLO. Related Work The Robustness of QA models Pretrained language models such as BERT (Devlin et al., 2019) have surpassed the human score on the SQuAD leaderboard.7 However, such powerful QA models have been shown to exhibit the lack of robustness. A QA model that is trained on SQuAD is not robust to paraphrased questions (Gan and Ng, 2019), implications derived from SQuAD (Ribeiro et al., 2019), questions with low lexical overlap (Sugawara et al., 2018), and other QA datasets (Yogatama et al., 2019; Talmor and Berant, 2019; Sen and Saffari, 2020). Ko et al. (2020) showed that extractive QA model can suffer from positional bias and fail to generalize to different answer positions. The lack of robustness demonstrated in these studies can be explained by shortcut learning of deep neural networks (Geirhos et al., 2020). A high score on an in-distribution test set can be achieved by just exploiting unintended dataset biases (Levesque, 2014). Therefore, evaluating QA models only on an in-distribution test set is not enough to evaluate the robustness of the QA models. Question Generation for Question Answering QG has been studied extensively"
2021.mrqa-1.6,P16-1009,0,0.0357614,"as been widely used in other domains to reduce dataset biases such as the background bias in person re-identification (McLaughlin et al., 2015), the gender bias in coreference resolution (Zhao et al., 2018), and the lexical bias in natural language inference (Zhou and Bansal, 2020). These works repeated training examples or added synthetic data to increase under-represented samples and reduce the imbalance in a training set. Our proposed approach has the same motivation as these works. On the other hand, data augmentation can unintentionally introduce or amplify dataset bias. Backtranslation (Sennrich et al., 2016), which is the common data augmentation approach for machine translation, can introduce the translationese bias. That is, machine translation systems trained with back-translation, compared to ones without backtranslation, can enhance the BLEU scores when the input is translationese (i.e., human-translated texts) but harm the BLEU scores when the input is naturally occurring texts (Edunov et al., 2020; Marie et al., 2020). This phenomenon is analogous to the observation in our work, where we demonstrated that SQuAD QG models are biased towards generating questions with high QCLO, and this tend"
2021.mrqa-1.6,D19-1253,0,0.131831,"cal Overlap Kazutoshi Shinoda1,2 Saku Sugawara2 Akiko Aizawa1,2 1 The University of Tokyo 2 National Institute of Informatics shinoda@is.s.u-tokyo.ac.jp {saku,aizawa}@nii.ac.jp Abstract and Liang, 2017; Sugawara et al., 2018; Gan and Ng, 2019; Ribeiro et al., 2019), which could be a serious problem in real-world applications. Question generation (QG) has also been extensively studied to augment QA datasets (Du et al., 2017; Du and Cardie, 2018). It is demonstrated that QG can improve not only the in-domain generalization but also the out-of-distribution generalization capability of QA models (Zhang and Bansal, 2019; Lee et al., 2020; Shinoda et al., 2021). In other areas, data augmentation techniques have been successfully used to reduce dataset biases and increase the performance of machine learning models on under-represented samples in vision (McLaughlin et al., 2015; Wong et al., 2016) and language (Zhao et al., 2018; Zhou and Bansal, 2020). Thus, we assume that QG is useful to debias QA models and improve its robustness by augmenting QA datasets. However, it has not been fully studied whether existing QG models can contribute to debiasing QA models (i.e., improve the robustness of QA models to unde"
2021.mrqa-1.6,N18-2003,0,0.132658,"lications. Question generation (QG) has also been extensively studied to augment QA datasets (Du et al., 2017; Du and Cardie, 2018). It is demonstrated that QG can improve not only the in-domain generalization but also the out-of-distribution generalization capability of QA models (Zhang and Bansal, 2019; Lee et al., 2020; Shinoda et al., 2021). In other areas, data augmentation techniques have been successfully used to reduce dataset biases and increase the performance of machine learning models on under-represented samples in vision (McLaughlin et al., 2015; Wong et al., 2016) and language (Zhao et al., 2018; Zhou and Bansal, 2020). Thus, we assume that QG is useful to debias QA models and improve its robustness by augmenting QA datasets. However, it has not been fully studied whether existing QG models can contribute to debiasing QA models (i.e., improve the robustness of QA models to under-represented questions). In this study, we focus on question–context lexical overlap, inspired by the findings presented in Sugawara et al. (2018). Their work revealed that questions having low lexical overlap with context tend to require reasoning skills rather than superficial word matching, and existing QA"
2021.mrqa-1.6,2020.acl-main.773,0,0.0912347,"generation (QG) has also been extensively studied to augment QA datasets (Du et al., 2017; Du and Cardie, 2018). It is demonstrated that QG can improve not only the in-domain generalization but also the out-of-distribution generalization capability of QA models (Zhang and Bansal, 2019; Lee et al., 2020; Shinoda et al., 2021). In other areas, data augmentation techniques have been successfully used to reduce dataset biases and increase the performance of machine learning models on under-represented samples in vision (McLaughlin et al., 2015; Wong et al., 2016) and language (Zhao et al., 2018; Zhou and Bansal, 2020). Thus, we assume that QG is useful to debias QA models and improve its robustness by augmenting QA datasets. However, it has not been fully studied whether existing QG models can contribute to debiasing QA models (i.e., improve the robustness of QA models to under-represented questions). In this study, we focus on question–context lexical overlap, inspired by the findings presented in Sugawara et al. (2018). Their work revealed that questions having low lexical overlap with context tend to require reasoning skills rather than superficial word matching, and existing QA models are not robust to"
C00-1058,J96-1001,0,\N,Missing
C00-1058,C94-1100,0,\N,Missing
C00-1058,E93-1015,0,\N,Missing
C00-1058,A94-1006,0,\N,Missing
C00-1058,P95-1032,0,\N,Missing
C00-1058,P93-1003,0,\N,Missing
C00-1058,C94-1084,0,\N,Missing
C00-1058,C96-2130,0,\N,Missing
C00-1058,H91-1026,0,\N,Missing
C00-1058,1996.amta-1.13,0,\N,Missing
C12-1107,2005.mtsummit-papers.11,0,0.0106927,"atistics were collected across all subjects for the 6 documents used during the experiments, to record eye-movements and obtain the linguistic features of those documents. With the intention of capturing all possible linguistic influence on reading behavior, we collected a set of lexical, syntactic and semantic linguistic features on every document. Among the lexical linguistic features, we measured word length, whether the word contains a digit or not, the presence of upper case letters or word unpredictability, as given by the perplexity from a 5-gram language model trained on a big corpus (Koehn, 2005) and smoothed using modified Kneser-Ney technique (Chen and Goodman, 1999). Syntactic features were also included in our model, such as whether a word is the head of a phrase, binary features indicating whether a word has a certain Part-of-Speech (POS) tag, total height of the parse tree of the sentence each word corresponds to, word position in the sentence, etc. We believe that semantic features also may influence greatly on the eye movements, since it is reasonable to think that an important part of the cognitive processing consists in an incremental integration of the information into the"
C12-1107,J08-1002,0,0.0537646,"Missing"
C12-1107,P10-1118,0,0.161387,"-movement data (Kennedy and Pynte, 2005) was also built to test previous computational models and paving the way to establish a common ground of computational model development. Following these ideas, we work under the assumption that eye-movements reflect cognitive processes and that the analysis of these eye-movements and the linguistic features of the text can be used to indirectly recognize the current mental state of the reader. Cognitive load is an important variable that has received a significant amount of attention since it is a good signal of task difficulty and cognitive demand. In Tomanek et al. (2010), variations in eye-movements were used to build a cognitive cost model to predict human annotation costs of named entities, while in Doherty et al. (2010), variations in eye-movements were used to recognize hidden linguistic features from machine translation output such as sentence understandability. Although these two works appear to share the same idea, they point at different directions. The former uses linguistic features and eye-movements to unveil hidden cognitive costs, while the latter uses certainty on cognitive load and eye-movements to recognize textual characteristics such as sent"
C16-1261,W08-0804,0,0.0372083,"an alphabet or a feature dictionary A : N → {0, ..., m − 1} such that A(fi ) = i for f0 , ..., fm−1 ∈ N. For example, if the 12-th element in a feature set 2776 represents “the current token is appropriate, the previous token is I, and the current label is VERB”, one possible formulation is f12 = utf8(""bigram[-1:0]=I:appropriate&label=VERB""), where utf8 denotes a non-negative integer given by the UTF-8 bit representation of a string. A weight value for this feature can then be accessed as θ A(f12 ) . There are two problems with the use of alphabets. First, they consume too much storage space (Ganchev and Dredze, 2008). Second, if implemented with a hash table (as is often the case), they are so slow that they become the bottleneck of the entire learning system (Bohnet, 2010). In the following, Z(A) denotes the size of an alphabet A. 4.1 Pipelined method Let us now introduce a pipelined method to reduce both O(mζ) and Z(A). Definition 4.1. Given inputs T , 1. Train the model by using L1-regularization according to the method of Ng (2004): (a) Divide the data T into a training set T1 of the size (1 − γ)T and a development set T2 of the size γT , for some γ ∈ R. (b) For B = 0, 1, 2, 4, ..., C, solve the optim"
C16-1261,C14-2009,0,0.0671695,"Missing"
C16-1261,N03-1028,0,0.110406,"formant to ECMAScript 6, the language is actually lower-level than C, and Mozilla claims it is only at most twice as slow as native code. We used the language because, by using a JavaScript-compatible language, it is possible to embed the resulting classifier in web pages for use as a fundamental library in web systems, not only on desktop machines but also on mobile platforms. 5.2 NP chunking µ ν 3 2 1 3 2 1 0.9720 0.9673 0.9470 0.9703 0.9681 0.9460 0.9621 0.9610 0.9392 Table 1: Macro-averaged F1 values under various µ and ν values for Qµ.ν encoding. To test our CRF implementation, following Sha and Pereira (2003), we performed an NP chunking task using the CoNLL-2000 text chunking task data (Tjong Kim Sang and Buchholz, 2000). In the shared task data, the labels B-NP and I-NP were retained, but all other labels were converted to O. Therefore, this is basically a sequential labeling problem with three labels. The features are the same as those used by Sha and Pereira (2003), except that they reformulated two consecutive labels as a new one, whereas we used the original labels. The total number of features is 1,015,621. The training dataset consists of 8,936 instances, and the test dataset contains 2,01"
C16-1261,D15-1288,0,0.039572,"Missing"
C16-1261,W00-0726,0,0.0801512,"as slow as native code. We used the language because, by using a JavaScript-compatible language, it is possible to embed the resulting classifier in web pages for use as a fundamental library in web systems, not only on desktop machines but also on mobile platforms. 5.2 NP chunking µ ν 3 2 1 3 2 1 0.9720 0.9673 0.9470 0.9703 0.9681 0.9460 0.9621 0.9610 0.9392 Table 1: Macro-averaged F1 values under various µ and ν values for Qµ.ν encoding. To test our CRF implementation, following Sha and Pereira (2003), we performed an NP chunking task using the CoNLL-2000 text chunking task data (Tjong Kim Sang and Buchholz, 2000). In the shared task data, the labels B-NP and I-NP were retained, but all other labels were converted to O. Therefore, this is basically a sequential labeling problem with three labels. The features are the same as those used by Sha and Pereira (2003), except that they reformulated two consecutive labels as a new one, whereas we used the original labels. The total number of features is 1,015,621. The training dataset consists of 8,936 instances, and the test dataset contains 2,012. Of the training data set, we used 7,148 instances (4/5) for training and the remaining 1,788 instances as develo"
C16-1261,P09-1054,0,0.0215356,"Missing"
C16-1261,P09-2086,0,0.0273547,"Missing"
C16-2029,D14-1082,0,0.0104866,"display the search results. The current system searches for terms on video and slide-sharing sites, and displays the top results as side-notes. 3 4 http://kmcs.nii.ac.jp/planetext/en/ https://github.com/KMCS-NII/mapPdfToXml 138 Figure 2: The screen shot of paper outline view (left) and dependency relation display (right). 4.3 Ability to seamlessly display linguistic annotation As an example of this workflow, we annotated the dependency structure in the body text of a paper. In this workflow, the XML file annotated with dependency information was generated using the Stanford dependency parser (Chen and Manning, 2014) as the NLP tool. An example of the dependency relation displayed on SideNoter is shown in Figure 2. Conventionally, the dependency information has been discussed only in terms of one-sentence units. However, by viewing the overall relationship in the whole document, each of the dependencies occurring in the document and the density of the relationships can be understood. 5 Conclusion In this paper, we presented a paper browsing system that displays a variety of information obtained from the body text of papers in side-note columns. In addition, we have developed a framework that displays anno"
C16-2029,councill-etal-2008-parscit,0,0.0307666,"t Word LaTeX PDF Original XML Plain text XML Language processing Linguistic tool annotation result (b) Linguistic annotation ﬂow XML mapPdfToXml XML/PDF with embedded linguistic annotation Figure 1: Overall workflow of the proposed system. 2 Related works There are many systems for searching for papers. In the field of NLP, web services such as CiteSeerX1 and ACL searchbench (Sch¨afer et al., 2011) are typical examples. To perform a flexible paper search in a specific field, it is necessary to extract the logical structure of the paper and its bibliographic information. Tools such as ParsCit (Councill et al., 2008), LA-PDFText (Ramakrishnan et al., 2012), PDFX (Constantin et al., 2013), and GROBID (Lopez, 2009) can be used to analyze the logical structure of papers. In addition, many frameworks that enable knowledge extraction from scholarly documents have been proposed, such as PDFMEF (Wu et al., 2015) and Dr. Inventor Project (Ronzano and Saggion, 2015). If the knowledge acquired from a paper could be displayed at the same time the paper is being read, readers’ understanding of the paper would improve significantly. However, because a special viewer is often used to read PDF, it is difficult for other"
C16-2029,W14-5205,1,0.823194,"r, the image files converted from the PDF are displayed. To facilitate changes to the background color of the page, we utilize a transparent PNG background image format. In addition, our system uses the PDFFigures tool (Clark and Divvala, 2015) to extract figures and tables from papers. This tool can also recognize the corresponding caption text. 3.2 Linguistic annotation flow We have developed a workflow to visualize and easily verify the annotation information generated by NLP tools on the page layout. First, the XML file of a paper is converted to plain text using our PlaneText framework3 (Hara et al., 2014). PlaneText facilitates application of any NLP tools to target real-world documents containing structured text. Currently, a tool is also being developed to convert XML-tagged text into plain text sequences that can be directly inputted to NLP tools. The annotation information is then applied to the resulting plain text using any of the NLP tools. In this case, the resulting generated file format is set to XML. Finally, the PDF layout information is embedded into the XML file using the mapPdfToXml4 tool we are currently developing. This tool generates a new XML document by combining an origina"
C16-2029,P11-4002,0,0.0251551,"Missing"
C18-1227,C14-1071,0,0.0151641,"arded as FEs because they have no distinctive function or meaning. Further, useful expressions are not always highly frequent. In fact, word sequences with high mutual information are rare in corpora because many are subject-specific. Vincent (2013) decomposed a candidate phrase into the phrasal core and its collocates. The phrasal core is a continuous or discontinuous word sequence occurring with high frequency. Candidate phrases including the core were first identified in a corpus. Then the collocates were sought. Brooke et al. (2015) used the technique for multi-word expression extraction (Brooke et al., 2014) to find FEs. They split a sentence into parts with a lexical predictability ratio. They pointed out that evaluating newly acquired FEs is difficult because there is no answer dataset. Aside from studies of FE extraction, several studies have addressed phrase extraction in particular for information extraction or text mining (Zhong et al., 2012; Zhang et al., 2013; Liu et al., 2015). However, these studies specifically addressed characteristic phrases that were informative. Therefore, FEs such as ‘in this paper’ were not considered target expressions. 2680 3 3.1 Semantic FE Search for Writing"
C18-1227,W15-0915,0,0.245591,"ed. Their results show that highly frequent word sequences alone cannot be regarded as FEs because they have no distinctive function or meaning. Further, useful expressions are not always highly frequent. In fact, word sequences with high mutual information are rare in corpora because many are subject-specific. Vincent (2013) decomposed a candidate phrase into the phrasal core and its collocates. The phrasal core is a continuous or discontinuous word sequence occurring with high frequency. Candidate phrases including the core were first identified in a corpus. Then the collocates were sought. Brooke et al. (2015) used the technique for multi-word expression extraction (Brooke et al., 2014) to find FEs. They split a sentence into parts with a lexical predictability ratio. They pointed out that evaluating newly acquired FEs is difficult because there is no answer dataset. Aside from studies of FE extraction, several studies have addressed phrase extraction in particular for information extraction or text mining (Zhong et al., 2012; Zhang et al., 2013; Liu et al., 2015). However, these studies specifically addressed characteristic phrases that were informative. Therefore, FEs such as ‘in this paper’ were"
C18-1227,N15-3022,0,0.252565,"ザに意味カテゴリを明示する手間が発生することで ある。 これらの課題を解決するため，本研究では定型表現の意味検索の新たな枠組み を提案し，既存の定型表現辞書を分野毎の英文コーパスと対応づけて利用するための 手法を提案する。 さらに，既存の定型表現辞書を拡張し，より網羅性が高く，かつ分 野特化型の定型表現辞書の構築を検討し，有効性を検証する。 1 Introduction Non-native English speakers writing scholarly papers often use the same expressions repeatedly or are not confident in the correctness of their usage of certain wording. Existing computer-based writing assistance systems do not always help them find better expressions than those they already know because such systems search a corpus by using simple pattern-matching based on input keywords (Chang et al., 2015; Chang and Chang, 2015; Jeong et al., 2014; Liu et al., 2016; Mizumoto et al., 2017). For instance, when a user wants to write about previous work and find expressions other than ‘little research was done This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 2678 Proceedings of the 27th International Conference on Computational Linguistics, pages 2678–2689 Santa Fe, New Mexico, USA, August 20-26, 2018. on’, the only one the user might know, a keyword-based search does not present expressions such as ‘there has been little"
C18-1227,Y15-2005,0,0.0834923,"テゴリを 予測することが困難であり，ユーザに意味カテゴリを明示する手間が発生することで ある。 これらの課題を解決するため，本研究では定型表現の意味検索の新たな枠組み を提案し，既存の定型表現辞書を分野毎の英文コーパスと対応づけて利用するための 手法を提案する。 さらに，既存の定型表現辞書を拡張し，より網羅性が高く，かつ分 野特化型の定型表現辞書の構築を検討し，有効性を検証する。 1 Introduction Non-native English speakers writing scholarly papers often use the same expressions repeatedly or are not confident in the correctness of their usage of certain wording. Existing computer-based writing assistance systems do not always help them find better expressions than those they already know because such systems search a corpus by using simple pattern-matching based on input keywords (Chang et al., 2015; Chang and Chang, 2015; Jeong et al., 2014; Liu et al., 2016; Mizumoto et al., 2017). For instance, when a user wants to write about previous work and find expressions other than ‘little research was done This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 2678 Proceedings of the 27th International Conference on Computational Linguistics, pages 2678–2689 Santa Fe, New Mexico, USA, August 20-26, 2018. on’, the only one the user might know, a keyword-based search does not present expressions such as"
C18-1227,P12-3027,0,0.0210865,"problem and show that the quality of the FEs extracted with our method is higher than that of those with previous methods. 2 2.1 Related Work Writing Assistance Systems Existing writing assistance systems are classified into three types. First, the most direct approach for computer-based writing assistance is that in which user-input sentences are used to retrieve example sentences. Search results are shown with concordances (Wu et al., 2006) or dependency structures (Kato et al., 2006). Another approach is similar to an input method in which users can input non-alphabetical languages. FLOW (Chen et al., 2012) suggests an English translation from words written in another language. WINGS (Dai et al., 2014) suggests full Chinese sentences and words from pinyin. Full sentences are suggested on the basis of searches for sentences that contain words that are the same as or similar to the input. The third approach is combined with an authoring system. With this approach, candidate English 2679 expressions that follow user input are listed; then the users can choose one of them (Jeong et al., 2014; Chang et al., 2015; Yen et al., 2015; Chang and Chang, 2015; Liu et al., 2016; Mizumoto et al., 2017). Some"
C18-1227,P14-5005,0,0.0163383,"se with previous methods. 2 2.1 Related Work Writing Assistance Systems Existing writing assistance systems are classified into three types. First, the most direct approach for computer-based writing assistance is that in which user-input sentences are used to retrieve example sentences. Search results are shown with concordances (Wu et al., 2006) or dependency structures (Kato et al., 2006). Another approach is similar to an input method in which users can input non-alphabetical languages. FLOW (Chen et al., 2012) suggests an English translation from words written in another language. WINGS (Dai et al., 2014) suggests full Chinese sentences and words from pinyin. Full sentences are suggested on the basis of searches for sentences that contain words that are the same as or similar to the input. The third approach is combined with an authoring system. With this approach, candidate English 2679 expressions that follow user input are listed; then the users can choose one of them (Jeong et al., 2014; Chang et al., 2015; Yen et al., 2015; Chang and Chang, 2015; Liu et al., 2016; Mizumoto et al., 2017). Some systems allow users to specify the categories of FEs. Such categories include the Introduction, M"
C18-1227,kato-etal-2006-corpus,0,0.0485781,"context information originally recorded in the existing FE dictionary. We reformulate FE extraction as a sequential-labelling problem and show that the quality of the FEs extracted with our method is higher than that of those with previous methods. 2 2.1 Related Work Writing Assistance Systems Existing writing assistance systems are classified into three types. First, the most direct approach for computer-based writing assistance is that in which user-input sentences are used to retrieve example sentences. Search results are shown with concordances (Wu et al., 2006) or dependency structures (Kato et al., 2006). Another approach is similar to an input method in which users can input non-alphabetical languages. FLOW (Chen et al., 2012) suggests an English translation from words written in another language. WINGS (Dai et al., 2014) suggests full Chinese sentences and words from pinyin. Full sentences are suggested on the basis of searches for sentences that contain words that are the same as or similar to the input. The third approach is combined with an authoring system. With this approach, candidate English 2679 expressions that follow user input are listed; then the users can choose one of them (Je"
C18-1227,P15-2070,0,0.0211133,"Missing"
C18-1227,P06-4011,0,0.240541,"-prediction accuracy, compared to using the context information originally recorded in the existing FE dictionary. We reformulate FE extraction as a sequential-labelling problem and show that the quality of the FEs extracted with our method is higher than that of those with previous methods. 2 2.1 Related Work Writing Assistance Systems Existing writing assistance systems are classified into three types. First, the most direct approach for computer-based writing assistance is that in which user-input sentences are used to retrieve example sentences. Search results are shown with concordances (Wu et al., 2006) or dependency structures (Kato et al., 2006). Another approach is similar to an input method in which users can input non-alphabetical languages. FLOW (Chen et al., 2012) suggests an English translation from words written in another language. WINGS (Dai et al., 2014) suggests full Chinese sentences and words from pinyin. Full sentences are suggested on the basis of searches for sentences that contain words that are the same as or similar to the input. The third approach is combined with an authoring system. With this approach, candidate English 2679 expressions that follow user input are list"
C18-1227,P15-4024,0,0.201985,"put method in which users can input non-alphabetical languages. FLOW (Chen et al., 2012) suggests an English translation from words written in another language. WINGS (Dai et al., 2014) suggests full Chinese sentences and words from pinyin. Full sentences are suggested on the basis of searches for sentences that contain words that are the same as or similar to the input. The third approach is combined with an authoring system. With this approach, candidate English 2679 expressions that follow user input are listed; then the users can choose one of them (Jeong et al., 2014; Chang et al., 2015; Yen et al., 2015; Chang and Chang, 2015; Liu et al., 2016; Mizumoto et al., 2017). Some systems allow users to specify the categories of FEs. Such categories include the Introduction, Method, Results and Discussion (IMRaD) structure (Jeong et al., 2014), argumentative zone (Teufel, 1999; Chang et al., 2015) and move-step structure (Swales, 1990; Mizumoto et al., 2017). Note that users must designate which category to use. Overall, existing writing assistance systems adopt keyword-based searches, and thus the number of suggested expressions is limited to ones that contain user-input keywords. 2.2 Definitions o"
chaimongkol-etal-2014-corpus,C12-2103,0,\N,Missing
chaimongkol-etal-2014-corpus,M95-1005,0,\N,Missing
chaimongkol-etal-2014-corpus,S10-1004,0,\N,Missing
chaimongkol-etal-2014-corpus,P09-1074,0,\N,Missing
chaimongkol-etal-2014-corpus,doddington-etal-2004-automatic,0,\N,Missing
chaimongkol-etal-2014-corpus,M98-1001,0,\N,Missing
D18-1453,D16-1203,0,0.087457,"Missing"
D18-1453,N18-1144,0,0.0185537,"atasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an"
D18-1453,P17-1168,0,0.139783,"whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of validity and required reasoning skills, such as word matching, knowledge inference, and multiple sentence reasoning. We examine 12 recently proposed MRC datasets (Table 1), which include answer extraction, description, and multiple-choice styles. We also observe differences based on these styles. For our baselines, we use two neural-based systems, namely, the Bidirectional Attention Flow (Seo et al., 2017) and the Gated-Attention Reader (Dhingra et al., 2017). In Section 5, we describe the advantages and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. A"
D18-1453,N18-2017,0,0.0586381,"Missing"
D18-1453,P16-1145,0,0.02858,"nswer extraction, description, and 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingr"
D18-1453,P99-1042,0,0.140082,"ned sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). I"
D18-1453,D17-1215,0,0.405835,"ng of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC datasets. Consider the question shown in Figure 1, for example. Although the question, starting with when, requires an answer that is expressed as a moment in time, there is only one such expression (i.e., November 2014) in the given text (we refer to the text as the context). In other words, the question has only a single candidate answer. The system can solve it merely by recognizing the entity type required by when. In addition to this, even"
D18-1453,P17-1147,0,0.0383334,"h regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which ski"
D18-1453,N18-1023,0,0.0976911,"ults, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved"
D18-1453,Q18-1023,0,0.0646454,"Missing"
D18-1453,D17-1082,0,0.118876,"et into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparis"
D18-1453,W04-1013,0,0.00928634,"It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baseline systems: The multiple-choice style can be transformed to answer extraction, as mentioned in Clark et al. (2018). However, in some datasets, many questions have no textual overlap to determine the correct answer span in the context. Therefore, in order to avoid underestimating the baseline performance of those datasets, we used the GA system which is applicable to multiple choice questions. We scored the performance using exact match (EM)/F1 (Rajpurkar et al., 2016), Rouge-L (Lin, 2004), and accuracy for the answer extraction, description, and multiple-choice datasets, respectively (henceforth, we refer to these collectively as the score, for simplicity). For the description datasets, we determined in advance the answer span of the context that gives the highest Rouge-L score to the human-generated gold answer. We computed the Rouge-L score between 2 The ARC Easy and Challenge were collected using different methods; hence, we treated them as different datasets (see Clark et al. (2018) for further details). the predicted span and the gold answer.3 Reproduction of the baseline"
D18-1453,P11-2057,0,0.0719576,"Missing"
D18-1453,P18-2124,0,0.0286545,"ultiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets"
D18-1453,P18-1160,0,0.0649339,"Missing"
D18-1453,D16-1264,0,0.521837,"lls to investigate which skills explain the difference between easy and hard questions. From this study, we observed that (i) the baseline performances for the hard subsets remarkably degrade compared to those of entire datasets, (ii) hard questions require knowledge inference and multiple-sentence reasoning in comparison with easy questions, and (iii) multiplechoice questions tend to require a broader range of reasoning skills than answer extraction and description questions. These results suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI"
D18-1453,N16-1098,0,0.0584165,"Missing"
D18-1453,W17-0906,0,0.0162088,"ults suggest that one might overestimate recent advances in MRC. 1 Figure 1: Example from the SQuAD dataset (Rajpurkar et al., 2016). The baseline system can answer the token-limited question and, even if there are other candidate answers, it can easily attend to the answercontained sentence (s1 ) by watching word overlaps. able for the detailed testing of NLU. Our motivation originates from studies that demonstrated unintended biases in the sourcing of other NLU tasks, in which questions contain simple patterns and systems can recognize these patterns to answer them (Gururangan et al., 2018; Mostafazadeh et al., 2017). Introduction Evaluating natural language understanding (NLU) systems is a long-established problem in AI (Levesque, 2014). One approach to doing so is the machine reading comprehension (MRC) task, in which a system answers questions about given texts (Hirschman et al., 1999). Although recent studies have made advances (Yu et al., 2018), it is still unclear to what precise extent questions require understanding of texts (Jia and Liang, 2017). In this study, we examine MRC datasets and discuss what is needed to create datasets suitWe conjecture that a situation similar to this occurs in MRC da"
D18-1453,D13-1020,0,0.317104,"ntences, only one sentence (i.e., s1 ) appears to be related to the question; thus, the system can easily determine the correct answer by attention, that is, by matching the words appearing both in the context and the ques4208 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4208–4219 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics tion. Therefore, this kind of question does not require a complex understanding of language—e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; U"
D18-1453,P15-1024,0,0.0244362,"y et al., 2018; Khashabi et al., 2018). Nonetheless, the description style is difficult to evaluate because the Rouge-L and BLEU scores are insufficient for testing NLU. Whereas it is easy to evaluate the performance on multiple-choice questions, generating multiple reasonable options requires considerable effort. Interpretation of our heuristics: When we regard the MRC task as recognizing textual entailment (RTE) (Dagan et al., 2006), the task requires the reader to construct one or more premises from the context and form the most reasonable hypothesis from the question and candidate answer (Sachan et al., 2015). Thus, easier questions are those (i) where the reader needs to generate only one hypothesis, and (ii) where the premises directly describe the correct hypothesis. Our two heuristics can also be seen as the formalizations of these criteria. Therefore, to make questions more realistic, we need to create multiple hypotheses that require complex reasoning to be distinguished. Moreover, the integration of premises should be complemented by external knowledge to provide sufficient information to verify the correct hypothesis. 6 Related Work Our heuristics and annotation were motivated by unintende"
D18-1453,P18-1156,0,0.0157401,"s to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the CNN/Daily Mail and Whodid-What datasets. Why we used different baselin"
D18-1453,D16-1241,0,0.0358516,"along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a"
D18-1453,P10-1122,0,0.0650563,"Missing"
D18-1453,L18-1564,0,0.0175734,"d examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and hard questions. We observed the following: • The baseline performances for the hard subsets remarkably degrade compared to those of entire datasets. • Our annotation study shows that hard questions require knowledge inference and multiplesentence reasoning in comparison with easy questions. • Compared to"
D18-1453,W17-0907,0,0.0253284,"Missing"
D18-1453,P16-1144,0,0.0228959,"nd 1 All scripts used in this study, along with the subsets of the datasets and the annotation results, are available at https://github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the"
D18-1453,P17-1075,1,0.886477,"Missing"
D18-1453,N18-1140,0,0.0171139,"github.com/Alab-NII/ mrc-heuristics. 4209 multiple choice (Table 1). Our aim was to select datasets varying in terms of corpus genre, context length, and question sourcing methods.2 Other datasets that are not covered in our study, but can be analyzed using the same method, include: QA4MRE (Sutcliffe et al., 2013), CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (Hill et al., 2016), bAbI (Weston et al., 2015), WikiReading (Hewlett et al., 2016), LAMBADA (Paperno et al., 2016), Who-did-What (Onishi et al., 2016), ProPara (Dalvi et al., 2018), MultiRC (Khashabi et al., 2018), CliCR (Suster and Daelemans, 2018), SQuAD (v2.0) (Rajpurkar et al., 2018), and DuoRC (Saha et al., 2018). 2.2 Baseline Systems We employed the following two widely used baselines. Bidirectional Attention Flow (BiDAF) (Seo et al., 2017) was used for the answer extraction and description datasets. BiDAF models bi-directional attention between the context and question. It achieved state-of-the-art performance on the SQuAD dataset. Gated-Attentive Reader (GA) (Dhingra et al., 2017) was used for the multiple-choice datasets. GA has a multi-hop architecture with an attention mechanism. It achieved state-of-the-artperformance on the"
D18-1453,W17-2623,0,0.328136,"es and disadvantages of different question styles with regard to evaluating NLU systems. We also interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and re"
D18-1453,K17-1028,0,0.0374338,"e.g., multiple-sentence reasoning, which is known as a more challenging task (Richardson et al., 2013). In Section 3, we define two heuristics, namely entity-type recognition and attention. We specifically analyze the differences in the performance of baseline systems for the following two configurations: (i) questions answerable or unanswerable with the first k tokens; and (ii) questions whose correct answer appears or does not appear in the context sentence that is most similar to the question (henceforth referred to as the most similar sentence). Although similar heuristics are proposed by Weissenborn et al. (2017), ours are utilized for question filtering, rather than system development; Using these simple heuristics, we split each dataset into easy and hard subsets for further investigation of the baseline performance. After conducting the experiments, we analyze the following two points in Section 4. First, we consider which questions are valid for testing, i.e., reasonably solvable. Second, we consider what reasoning skills are required and whether this exposes any differences among the subsets. To investigate these two concerns, we manually annotate sample questions from each subset in terms of val"
D18-1453,Q18-1021,0,0.0363293,"interpret our heuristics for constructing realistic MRC datasets. Our contributions are as follows: • • This study is the first large-scale investigation across recent 12 MRC datasets with three question styles. We propose to employ simple heuristics to split each dataset into easy and hard subsets and examine the performance of two baseline models for each of the subsets. Answer extraction (select a context span) 1. SQuAD (v1.1) (Rajpurkar et al., 2016) 2. AddSent (Jia and Liang, 2017) 3. NewsQA (Trischler et al., 2017) 4. TriviaQA (Wikipedia set) (Joshi et al., 2017) 5. QAngaroo (WikiHop) (Welbl et al., 2018) Description (generate a free-form answer) 6. MS MARCO (v2) (Nguyen et al., 2016) 7. NarrativeQA (summary) (Koˇcisk´y et al., 2018) Multiple choice (choose from multiple options) 8. MCTest (160 + 500) (Richardson et al., 2013) 9. RACE (middle + high) (Lai et al., 2017) 10. MCScript (Ostermann et al., 2018) 11. ARC Easy (ARC-E) (Clark et al., 2018) 12. ARC Challenge (ARC-C) (Clark et al., 2018) Table 1: Examined datasets. • We manually annotate questions sampled from each subset with both validity and requisite reasoning skills to investigate which skills explain the difference between easy and"
D18-1453,W16-0103,0,0.049189,"Missing"
E12-1070,2001.mtsummit-papers.26,0,0.0352842,"h set. Dorr (1997) created an LCS-based lexical resource as an interlingual representation for machine translation. This framework was also used for text generation (Habash et al., 2003). However, the problem of multiple-role assignment was not completely solved on the resource. As a comparison of different semantic structures, Dorr (2001) and Hajiˇcov´a and Kuˇcerov´a (2002) analyzed the connection between LCS and PropBank roles, and showed that the mapping between LCS and PropBank roles was many to many correspondence and roles can map only by comparing a whole argument structure of a verb. Habash and Dorr (2001) tried to map LCS structures into thematic roles by using their thematic hierarchy. 3 Multiple role expression using lexical conceptual structure Lexical conceptual structure is an approach to describe a generalized structure of an event or state represented by a verb. A meaning of a verb is represented as a structure composed of several primitive predicates. For example, the LCS structure for the verb “throw” is shown in Figure 1 and includes the predicates cause, affect, go, from, fromward, toward, locate, in, and at. The arguments of primitive predicates are filled by core arguments of the"
E12-1070,hajicova-kucerova-2002-argument,0,0.0906746,"Missing"
E12-1070,kawahara-kurohashi-2006-case,0,0.19127,"Missing"
E12-1070,kingsbury-palmer-2002-treebank,0,0.221764,"role annotation which solves these problems by extending the theory of lexical conceptual structure (LCS). By comparing our framework with that of existing resources, including VerbNet and FrameNet, we demonstrate that our extended LCS framework can give a formal definition of semantic role labels, and that multiple roles of arguments can be represented strictly and naturally. 1 Introduction Recent developments of large semantic resources have accelerated empirical research on semantic processing (M`arquez et al., 2008). Specifically, corpora with semantic role annotations, such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Ruppenhofer et al., 2006), are indispensable resources for semantic role labeling. However, there are two topics we have to carefully take into consideration regarding role assignment frameworks: (1) clarity of semantic role meanings and (2) the constraint that a single semantic role is assigned to each syntactic argument. While these resources are undoubtedly invaluable for empirical research on semantic process[John] Agent Source Arg0 Agent Agent threw [a ball] Patient Theme Arg1 Theme Theme [from the window] . Source/Path Arg2 Source Source Table 1: Examples of single role as"
E12-1070,J08-2001,0,0.0617362,"Missing"
E12-1070,N07-1069,0,0.0318867,"cal research on semantic process[John] Agent Source Arg0 Agent Agent threw [a ball] Patient Theme Arg1 Theme Theme [from the window] . Source/Path Arg2 Source Source Table 1: Examples of single role assignments with existing resources. ing, current usage of semantic labels for SRL systems is questionable from a theoretical viewpoint. For example, most of the works on SRL have used PropBank’s numerical role labels (Arg0 to Arg5). However, the meanings of these numbers depend on each verb in principle and PropBank does not expect semantic consistency, namely on Arg2 to Arg5. Moreover, Yi et al. (2007) explicitly showed that Arg2 to Arg5 are semantically inconsistent. The reason why such labels have been used in SRL systems is that verb-specific roles generally have a small number of instances and are not suitable for learning. However, it is necessary to avoid using inconsistent labels since those labels confuse machine learners and can be a cause of low accuracy in automatic processing. In addition, clarity of the definition of roles are particularly important for users to rationally know how to use each role in their applications. For this reasons, well-organized and generalized labels g"
I13-1098,W02-0808,0,0.263029,"Aizawa 2,3 1 The Graduate University for Advanced Studies, Tokyo, Japan 2 The University of Tokyo, Tokyo, Japan 3 National Institute of Informatics, Tokyo, Japan {nqminh, giovanni, goran topic, aizawa}@nii.ac.jp Abstract One major issue in early research on machine understanding of mathematical terms found in text was the lack of evaluation datasets. A previous study (Wolska et al., 2011) was based on a small evaluation set of 200 mathematical expressions annotated by experts. Clearly, large samples of sense-tagged data would require significant human annotation and labor. Fortunately, then, Ide et al. (2002) showed that sense distinctions derived from cross-lingual information are at least as reliable as those made by human annotators. The novel research described in our paper presents a fully automated method for generating large samples of mathematical terms with sensetagged data. As part of the effort described here to address mathematical term sense disambiguation (MTSD), we first propose a method that uses a MathML parallel markup corpus to generate training and testing datasets. Second, we propose heuristics that improve alignment results for the parallel markup corpus. Third, we present a"
I13-1098,S10-1003,0,0.0721937,"Missing"
I13-1098,P11-2055,0,0.0296915,"Missing"
I13-1098,J03-1002,0,0.00537703,"Missing"
I13-1098,P10-1032,0,0.013472,"shold (0.2). In Equation 1, Pchild and Cchild , respectively, refer to the child nodes of treeP and treeC . The blue lines in Figure 3 represent the expanded alignments between subtrees. Processed expressions Figure 2: Steps for generating the data for MTSD. GIZA++ (Och and Ney, 2003) to obtain alignment between the Presentation terms and Content terms. Developed to train word-based translation models, the GIZA++ toolkit is not directly applicable to a tree-based corpus. One common solution is to convert the tree into a sentence by extracting the leaf nodes of the tree and to form a sequence (Sun et al., 2010). While this approach works well for natural language text, it is less effective with mathematical expressions, since the intermediate nodes of these expressions contain layout information. Before using GIZA++, to enhance alignment precision, we apply two heuristic rules to the presentation tree based on information on its structure. The first heuristic rule converts the intermediate layout nodes (except mrow) to leaves on the tree by moving them to the position of their first child. When moving an intermediate layout node, we create a temporary (‘temp’) node to replace the moved node and to k"
I13-1098,D07-1007,0,0.035258,"Missing"
I13-1098,2007.mtsummit-papers.62,0,0.0199088,"nctions.wolfram.com/01.14.03.0001.01 810 MathML parallel markup expressions Preprocess (heuristic rules) alignment Node-to-node aligned data GIZA++ alignment Subtree-to-subtree Ambiguous term extraction Data for MTSD Subtree-to-subtree aligned data and after applying heuristic rules for the expression arctan(0)=0. To extract more complex mathematical terms, we expand the node-to-node alignments to subtree-to-subtree alignments. In this study, we expanded the subtree alignment only to the parent of the mi nodes. The criteria used here to achieve subtree aligned pair are similar to that used by Tinsley et al. (2007). First, a node can be linked only once. Second, descendants of a presentation node can link only to descendants of its content counterpart. Third, ancestors of a presentation node can link only to ancestors of its content counterpart (a node counts as its own ancestor). If one presentation node links to more than one content node, we keep only the link with the highest alignment score, as given by Equation 1. The number of alignments between the presentation tree treeP and the content tree treeC is the sum of (1) the number of alignments from the leaf children of treeP to the leaf children of"
I13-1098,C04-1192,0,0.0626475,"Missing"
I13-1098,P02-1033,0,0.0588619,"Missing"
I13-1195,W10-1001,0,0.0583009,"Missing"
I13-1195,P08-1092,0,0.0578027,"Missing"
I13-1195,E99-1042,0,0.18479,"Missing"
I13-1195,D12-1043,0,0.069566,"Missing"
I13-1195,N07-1058,0,0.0503959,"Missing"
I13-1195,C10-1062,0,0.0539122,"Missing"
I13-1195,P03-1054,0,0.00909759,"Missing"
I13-1195,J08-1002,0,0.0333774,"Missing"
I13-1195,D10-1048,0,0.0209143,"Missing"
I13-1195,N10-1063,0,0.0310954,"Missing"
I13-1195,W05-1616,0,0.0821983,"Missing"
K16-2018,P09-2004,0,0.0371265,"e, and passed to the next process. Thus, we can use this substitution to reduce the difference between Explicit and AltLex such that it can be ignored, which contributes to the reusability of the components. 3.2 4. the MPQA subjectivity lexicon: each token in the argument texts is classified into nine groups according to the MPQA lexicon, and the number of tokens was counted, ignoring words not in the lexicon. These are chiefly general-purpose features and widely used in various NLP tasks, and actually a subset of the features used in several previous studies including (Lin et al., 2014) and (Pitler and Nenkova, 2009). 3.3 Implicit and EntRel Sense Classifier Similar to the Explicit and AltLex sense classification, the Implicit and EntRel sense classification is also a two-step process: first it is determined whether the type is Implicit or EntRel, and then a sense is assigned if classified as Implicit. Connective words themselves cannot be used as features in Implicit and EntRel sense classification; therefore, other features need to be prepared. There are many candidates for the features. Here, to simplify the implementation, and also because we cannot afford the time for task-specific feature engineerin"
K16-2018,K15-2003,0,0.0250645,"Missing"
K16-2018,K15-2002,0,0.0829315,"-1-2 Hitotsubashi, Chiyoda-ku Tokyo, Japan aizawa@nii.ac.jp Yusuke Kido The University of Tokyo 7-3-1 Hongo, Bunkyo-ku Tokyo, Japan y.k@is.s.u-tokyo.ac.jp Abstract introduction of a new evaluation criterion based on partial argument matching in the CoNLL2016 Shared Task. On the other hand, the sense classification components, which assign a sense to each discourse relation, continue to perform poorly. In particular, Non-Explicit sense classification is a difficult task, and even the best system achieved an F1 score of only 0.42 given the gold standard argument pairs without error propagation (Wang and Lan, 2015). In response to this situation, Discourse Relation Sense Classification has become a separate task in the CoNLL-2016 Shared Task (Xue et al., 2016). In this task, participants implement a system that takes gold standard argument pairs and assigns a sense to each of them. To tackle this task, we first analyzed the characteristics of the discourse relation data. We then implemented a classification system based on the analysis. One of the distinctive points of our system is that, compared to existing systems, it uses smaller number of features, which enables the source code to be quite short an"
K16-2018,K15-2001,0,0.0797053,"ork with and describes the system we submitted to the CoNLL2016 Shared Task. Our system uses two sets of two-step classifiers for Explicit and AltLex relations and Implicit and EntRel relations, respectively. Regardless of the simplicity of the implementation, it achieves competitive performance using minimalistic features. The submitted version of our system ranked 8th with an overall F1 score of 0.5188. The evaluation on the test dataset achieved the best performance for Explicit relations with an F1 score of 0.9022. 1 Introduction In the CoNLL-2015 Shared Task on Shallow Discourse Parsing (Xue et al., 2015), all the participants adopted some variation of the pipeline architecture proposed by Lin et al. (2014). Among the components of the architecture, the main challenges are the exact argument extraction and Non-Explicit sense classification (Lin et al., 2014). Argument extraction is a task to identify two argument spans for a given discourse relation. Although the reported scores were relatively low for these components this is partially because of the “quite harsh” evaluation1 . This led to the 2 1 CoNLL 2016 Shared Task Official Blog http://conll16st.blogspot.com/2016/04/ partial-scoring-and-"
K17-2011,coltekin-2010-freely,0,0.0311547,"n, 1983; Koskenniemi, 1984; Kaplan and Kay, 1994). The advantages of the approach are that rules are often hand-crafted and thus suitable in low-resource settings and that it is relatively easy and direct to incorporate the linguistic knowledge of specialists. On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorpho for the languages of the Horn of Africa (Gasser, 2011). In this decade, machine learning for morphological inflection became a hot topic. One direction is to exploit the paradigmatic nature of inflection (Durrett and DeNero, 2013; Ahlberg et al., 2015). For example, Durrett and DeNero (2013) proposed a multi-step supervised learning approach. The first phase tries to extract transformational rules from data sets and consists of three sub-steps: the alignment of words in training data, merging spans across the resulting alignments, and rule extraction from these intermediary inf"
K17-2011,K17-2001,0,0.0353388,"Missing"
K17-2011,N13-1138,0,0.0852367,"knowledge of specialists. On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorpho for the languages of the Horn of Africa (Gasser, 2011). In this decade, machine learning for morphological inflection became a hot topic. One direction is to exploit the paradigmatic nature of inflection (Durrett and DeNero, 2013; Ahlberg et al., 2015). For example, Durrett and DeNero (2013) proposed a multi-step supervised learning approach. The first phase tries to extract transformational rules from data sets and consists of three sub-steps: the alignment of words in training data, merging spans across the resulting alignments, and rule extraction from these intermediary information. And then the second phase tries to learn the position and the type of transformation application. The advantage of this approach is that we can obtain concrete paradigms of inflection. Another recent innovation in this field (Faruqui e"
K17-2011,N15-1107,0,0.087595,"On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorpho for the languages of the Horn of Africa (Gasser, 2011). In this decade, machine learning for morphological inflection became a hot topic. One direction is to exploit the paradigmatic nature of inflection (Durrett and DeNero, 2013; Ahlberg et al., 2015). For example, Durrett and DeNero (2013) proposed a multi-step supervised learning approach. The first phase tries to extract transformational rules from data sets and consists of three sub-steps: the alignment of words in training data, merging spans across the resulting alignments, and rule extraction from these intermediary information. And then the second phase tries to learn the position and the type of transformation application. The advantage of this approach is that we can obtain concrete paradigms of inflection. Another recent innovation in this field (Faruqui et al., 2016; Kann and S"
K17-2011,N16-1077,0,0.415454,"ero, 2013; Ahlberg et al., 2015). For example, Durrett and DeNero (2013) proposed a multi-step supervised learning approach. The first phase tries to extract transformational rules from data sets and consists of three sub-steps: the alignment of words in training data, merging spans across the resulting alignments, and rule extraction from these intermediary information. And then the second phase tries to learn the position and the type of transformation application. The advantage of this approach is that we can obtain concrete paradigms of inflection. Another recent innovation in this field (Faruqui et al., 2016; Kann and Sch¨utze, 2016b) is the use of the sequence-to-sequence (seq2seq) model (Sutskever et al., 2014) (also known as the encoder-decoder model (Cho et al., 2014)). Notably, Kann and Sch¨utze (2016a) applied the attention-based version of seq2seq models (Bahdanau et al., 2015) to the SIGMORPHON 2016 Shared Task (Cotterell et al., 2016) and showed that their system can learn morphological reinflection even for extremely morphologically rich languages such as Maltese and became the winner of the year. 3 3.1 Basic architecture 3.1.1 Seq2seq model Fig. 1 shows the basic archicture of our syst"
K17-2011,P15-2111,0,0.0347906,"Missing"
K17-2011,E09-2008,0,0.0395563,"adition in natural language processing (NLP). The earliest studies used finite-state transducers (Karttunen, 1983; Koskenniemi, 1984; Kaplan and Kay, 1994). The advantages of the approach are that rules are often hand-crafted and thus suitable in low-resource settings and that it is relatively easy and direct to incorporate the linguistic knowledge of specialists. On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorpho for the languages of the Horn of Africa (Gasser, 2011). In this decade, machine learning for morphological inflection became a hot topic. One direction is to exploit the paradigmatic nature of inflection (Durrett and DeNero, 2013; Ahlberg et al., 2015). For example, Durrett and DeNero (2013) proposed a multi-step supervised learning approach. The first phase tries to extract transformational rules from data sets and consists of three sub-steps: the alignment of words in tra"
K17-2011,D16-1097,0,0.0283726,"Missing"
K17-2011,W10-1401,0,0.0861424,"Missing"
K17-2011,W16-2010,0,0.0311049,"Missing"
K17-2011,P16-2090,0,0.120229,"Missing"
K17-2011,J94-3001,0,0.590683,"of our approach. In Section 4, we present environmental settings used in our experiments and the main results of our work. In Section 5, we discuss the error analysis of our results. 100 Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 100–109, c Vancouver, Canada, August 3–4, 2017. 2017 Association for Computational Linguistics 2 Related Work account of the first author 1 . Morphological inflection has a long-tradition in natural language processing (NLP). The earliest studies used finite-state transducers (Karttunen, 1983; Koskenniemi, 1984; Kaplan and Kay, 1994). The advantages of the approach are that rules are often hand-crafted and thus suitable in low-resource settings and that it is relatively easy and direct to incorporate the linguistic knowledge of specialists. On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorpho for the languages of"
K17-2011,P84-1038,0,0.423575,"system description of our approach. In Section 4, we present environmental settings used in our experiments and the main results of our work. In Section 5, we discuss the error analysis of our results. 100 Proceedings of the CoNLL SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection, pages 100–109, c Vancouver, Canada, August 3–4, 2017. 2017 Association for Computational Linguistics 2 Related Work account of the first author 1 . Morphological inflection has a long-tradition in natural language processing (NLP). The earliest studies used finite-state transducers (Karttunen, 1983; Koskenniemi, 1984; Kaplan and Kay, 1994). The advantages of the approach are that rules are often hand-crafted and thus suitable in low-resource settings and that it is relatively easy and direct to incorporate the linguistic knowledge of specialists. On the other hand, manual crafting of such rules is often expensive and usually language specialists are not easily available. General purpose open-source libraries for this approach include OpenFST (Allauzen et al., 2007) and Foma (Hulden, 2009). In addition, there are several language-specific systems such as TRMorph for Turkish (C¸o¨ ltekin, 2010) and HornMorp"
L16-1607,anick-etal-2014-identification,0,0.0203449,"time mobile devices, Mac lower bound, cost, sentence Asia, space, between human, Eugene Charniak CRF, algorithm qualitative, new five, two-fold, several can, cannot, need to it, they Miyao and Tsujii 2008, [1] English, natural language NLP, biomedicine ERLA, universities F=0.98 Table 1: Entity tags, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such a"
L16-1607,I11-1001,0,0.225982,"based analysis such as information extraction (IE) concerning the methodological aspects of research papers and patents for analyzing technical trends and discovering emerging research fields. Their focus is on determining how things such as systems and data are developed and used. Consequently, in the annotated corpora used for establishing the systems for these purposes, things described in a document are labeled and classified according to their role in a certain context, such as application domain, method, and product. Some studies attach role-based labels to entity mentions. For example, Gupta and Manning (2011), in establishing a method for identifying the technical trends from abstracts in the ACL anthology 2 , extracted the FOCUS (main contribution of the article), DOMAIN (application domain), and TECHNIQUE (a method or tool used to achieve the FOCUS). The corpus used for the study attaches these labels directly to mentions of the corresponding entities. Similarly, Fukuda et al. (2012) annotated and classified entities in patent documents as TECHNOLOGY 2 3836 https://aclweb.org/anthology/ Type THING OCCURRENT PROCESS TIME CONTINUANT ARTIFACT DATA-ITEM LOCATION PERSON PLAN QUALITY QUANTITY MODALITY"
L16-1607,S10-1004,0,0.0371118,"DOMAIN (an area of study); and FORMULA (a mathematical formula). In addition, we defined the following compound or “ambiguous” types to handle systematic ambiguity in 3 Annotated Data Our dataset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indic"
L16-1607,P03-1054,0,0.00501384,"riginal annotation on the same part of the abstract shown in Figure 1, converted to standoff format for displaying in brat. Their annotation is sparser than ours (Figure 1), annotating only terms related to the topic of the paper as a whole. For extraction, they used heuristic rules based on trigger words and Stanford dependencies such as “A term is FOCUS if it is the direct object of the verb present” as seed rules. Then, the rule set was enhanced by iteratively adding the head words of extracted phrases as the triggers. The abstracts were tokenized using the Stanford parser (version 3.4.1) (Klein and Manning 2003), and the tokens are labeled with binary labels for inclusion in GuptaManning terms for each topic class (FOCUS, DOMAIN, and TECHNIQUE). Then, the support vector classifier from the python scikit-learn 0.17 package (Pedregosa et al., 2011) with a linear kernel was used to predict the labels. We tested several combinations of the features from the Stanford parser and our annotation. The features from the Stanford parser were parts of speech (P in Table 5) and the triplet of type, direction (head or argument), and the part of speech of the token it depends/depended on, for each dependency involv"
L16-1607,J08-1002,1,0.854996,"Missing"
L16-1607,D14-1200,0,0.0358364,"Missing"
L16-1607,W09-3716,0,0.0330807,"otactic component (...). The three components mentioned are entities of different types, i.e., lexicon is a dataset (DATA-ITEM), and the others are program functions (PLAN). The current convention uses IS-A relation to relate components and lexicon etc., which is impossible without violating Rule-IS. This suggests that we need to define a new relation for role-playing and a new type or types for “role” words such as components. We also found that ambiguity and metonymic constructions cause annotation difficulty. These violations suggest a need for a type-coercion mechanism, such as dot-types (Pustejovsky et al. 2009). For example, when a process uses parameters, the names of the parameters can denote “the invocation of the process with the parameters” (e. g. RM pairs extracted can perform the mapping, where RM pairs extracted denotes a process using the pairs as parameters) and the name of data structure is used for both the data structure itself and the content of the data (Bigrams and trigrams are commonly used in statistical natural language processing). They lead to the annotation of APPLY-TO relation between DATAITEM and PROCESS, which violates Rule-APP. Another type of the problem is the ambiguity b"
L16-1607,W04-2401,0,0.0369304,"Missing"
L16-1607,W15-0403,0,0.0213946,"gs, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (20"
L16-1607,W14-2410,0,0.0296543,"ressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (2013), using Related Work sections from the proceedings of the Association for the Advancement of Artificial Intelligence (AAAI2010), identified the papertopic relation along with the me"
L16-1607,D07-1111,0,0.0605503,"Missing"
L16-1607,E12-2021,1,0.768208,"taset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indicate that software (PLAN, PLAN-OR-PROCESS) is more frequently discussed than hardware (ARTIFACT) in both the general computer science/technology domain (ACM) and the natural language processing sub"
L16-1607,W13-2318,1,0.777851,"Missing"
L16-1607,tateisi-etal-2014-annotation,1,0.890874,"gence (AAAI2010), identified the papertopic relation along with the method-purpose relation among concepts described in the paper in order to construct a network representing the methods developed in one study and used by others and to evaluate the influence of the research. Nassour-Kassis et al. (2015) identified the mentions of tasks and attributes and linked them with one of 6 types (Means-End, Instance-of, Consists-of, Associated-with, Contributes-to, and Compares-to) of relations, using ten articles on summarization for building a conceptual map in the natural language processing domain. Tateisi et al. (2014) developed a corpus on research articles from Journal of Information Processing Society of Japan (IPSJ Journal) where relationship among OBJECTS (named entities), MEASURE (judgment and evaluation, including numbers), and TERM (general technical concepts other than OBJECT and MEASURE) are identified and labeled with one of 16 types such as Apply-to (method-purpose), Evaluate (evaluation objectevaluation result), and Attribute (object-attribute), and developed a prototype of a keyword-based search system in which results can be filtered according to the relations involving the keyword. Those wor"
L16-1635,2014.amta-wptp.6,0,0.640881,"Missing"
L16-1635,W15-1825,0,0.030624,"nterpretation, where the interpreter hears a text and speaks out the translation (e.g., during conference interpreting) and conventional translation by which a written source text is translated mainly using the keyboard. It is close to sight translation, but while sight translation is usually done in the moment; there are – in principle - no time constraints in translation dictation. Translation dictation was used in some translation bureaus in the 1960s and 1970s (Gingold, 1978) but it has been used less frequently since the mid-80s, as professional translators started using micro-computers (Zapata and Kirkedal, 2015). Already the ALPAC report (Pierce et al., 1966) mentioned that “productivity of human translators might be as much as four times higher when dictating” as compared to writing. Others (e.g. Reddy and Rose, 2010, Rodriguez et al., 2012) are less optimistic about the time efficiency of dictation, but with increasing quality of voice recognition this mode of translation is becoming a valid alternative to ‘conventional’ translation typing (Ciobanu, 2014) and even to machine translation post-editing. See also Martinez et al, (2014) who experiment with integrating speech recognition into an online C"
L18-1373,K17-2001,0,0.0654038,"Missing"
L18-1373,L16-1262,0,0.0830227,"Missing"
L18-1373,W17-0417,1,0.49089,"and alienability. 1. Introduction The project of Universal Dependencies (UD) (Nivre et al., 2016) marks a milestone in the history of natural language processing (NLP), as it unifies syntactic annotation schemes across languages/corpora and enables crosslingual processing. At the CoNLL Shared Task 2017 for UD, 33 teams participated in the task, proving that the community is quickly thriving (Zeman et al., 2017). We are currently working on the UD annotation scheme and corpus of Ainu, a language spoken by the Ainu people, an ethnic minority in Japan. A part of the work was presented elsewhere (Senuma and Aizawa, 2017). The task aims to tackle two purposes: 1. to promote the language revitalization of Ainu, as it is a highly endangered language but has a rich amount of the records of oral literature, and 2. to serve as a basis to test if universal specifications in NLP such as UD and UniMorph (UM) (SylakGlassman, 2016) can encode the world’s languages. The first issue is urgent because Ethnologue classifies the languages as nearly extinct (Lewis et al., 2016), and scholars have been alerting the status of language usages (DeChicchis, 1995; Sato, 2012). To mitigate the situation, Bugaeva (2011) created a fre"
L18-1373,K17-3001,0,0.0603486,"Missing"
matsubayashi-etal-2012-building,kingsbury-palmer-2002-treebank,0,\N,Missing
matsubayashi-etal-2012-building,W07-1522,0,\N,Missing
matsubayashi-etal-2012-building,E12-1070,1,\N,Missing
matsubayashi-etal-2012-building,kawahara-kurohashi-2006-case,0,\N,Missing
N15-3019,P14-1023,0,0.0218795,"and phrases to retrieve English sentences that are similar to a given phrase or word in a different language (query). These vector representations not only allow for efficient crosslingual lookups in databases consisting of millions of sentences, but can also be employed to visualize intralingual and interlingual semantic relationships between phrases. 2 Related Work Various types of neural network models have been proposed to induce distributed word representations and leveraging these word embeddings as features has proven viable in achieving state-of-the-art results for a variety of tasks (Baroni et al., 2014; Collobert and Weston, 2008). Recently, methods that attempt to compose embeddings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resou"
N15-3019,W04-2209,0,0.0437915,"es in the word/phrase vector space as a measure of semantic relatedness. Currently, our system supports the lookup of Japanese and English queries in English text. Our system encourages refining retrieved results and viewing relations in different contexts by supporting multiple queries. All queries and their corresponding results are visualized together to aid a better understanding of their relationships. To illustrate the differences to phrase vector-based sentence retrieval, we also offer a retrieval option based on direct word-to-text matching using the EDICT Japanese-English dictionary (Breen, 2004) and Apache Lucene1 for sentence retrieval. To the best of our knowledge, our system is the first to provide writing assistance using vector representations of words and phrases. 3.1 Inducing Crosslingually Constrained Word Representations We employ the approach presented in Soyer et al. (2015) to learn bilingually constrained representations of Japanese and English words. The method draws from sentence-parallel bilingual text to constrain word vectors crosslingually, handles text on a phrase level ensuring the compositionality of the induced word embeddings, and is agnostic to how phrase repr"
N15-3019,P14-1006,0,0.0209294,"dings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resources to constrain vector representations crosslingually. (Soyer et al., 2015; Hermann and Blunsom, 2014; Cho et al., 2014; Chandar A P et al., 2014). Embeddings learned using these methods not only possess meaningful properties within a language, but also interlingually. 3 Crosslingual Vector-Based Writing Assistance (CroVeWA) Our system harnesses crosslingually constrained word and phrase representations to retrieve and visualize sentences related to given queries, using distances in the word/phrase vector space as a measure of semantic relatedness. Currently, our system supports the lookup of Japanese and English queries in English text. Our system encourages refining retrieved results and vi"
N15-3019,D11-1014,0,0.0268484,"efficient crosslingual lookups in databases consisting of millions of sentences, but can also be employed to visualize intralingual and interlingual semantic relationships between phrases. 2 Related Work Various types of neural network models have been proposed to induce distributed word representations and leveraging these word embeddings as features has proven viable in achieving state-of-the-art results for a variety of tasks (Baroni et al., 2014; Collobert and Weston, 2008). Recently, methods that attempt to compose embeddings not only of words but of whole phrases (Le and Mikolov, 2014; Socher et al., 2011) have enabled vector representations to be applied for tasks that are defined over phrases, sentences, or even documents. The most relevant work for this paper are recent approaches that allow for the induction of 91 Proceedings of NAACL-HLT 2015, pages 91–95, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics word and phrase embeddings not only from monolingual text but using bilingual resources to constrain vector representations crosslingually. (Soyer et al., 2015; Hermann and Blunsom, 2014; Cho et al., 2014; Chandar A P et al., 2014). Embeddings learn"
N15-3019,D14-1179,0,\N,Missing
nanba-etal-2012-automatic,W04-3219,0,\N,Missing
nanba-etal-2012-automatic,W03-2003,0,\N,Missing
nanba-etal-2012-automatic,N06-1057,0,\N,Missing
nanba-etal-2012-automatic,N06-1003,0,\N,Missing
nanba-etal-2012-automatic,P98-2127,0,\N,Missing
nanba-etal-2012-automatic,C98-2122,0,\N,Missing
nanba-etal-2012-automatic,P08-1116,0,\N,Missing
nanba-etal-2012-automatic,P99-1004,0,\N,Missing
P11-2083,C02-1166,1,0.917691,"Missing"
P11-2083,W97-0119,0,0.134854,"ng tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for w"
P11-2083,P98-1069,0,0.437,"ical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as wel"
P11-2083,W09-1117,0,0.012,"se it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in t"
P11-2083,P04-1067,1,0.9116,"Missing"
P11-2083,C10-1070,0,0.0648492,"P 0 , P 1 , P 2 , 1 2 P and P . A clear fact from the figure is that both the precision and the recall scores increase according to the increase of the N values, which coincides with our intuition. As one can note, our method consistently outperforms the previous work and also the original corpus on all the values considered for N . P2 P2' P1 P1' P0 0.8 Precision 0.6 0.4 0.2 0.0 0 100 200 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus pri"
P11-2083,C10-1073,1,0.449903,"f and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010) an approach based on improving the comparability of the corpus under consideration, prior to extracting bilingual lexicons. This approach is interesting since there is no point in trying to extract lexicons from a corpus with a low degree of comparability, as the probability of finding translations of any given word is low in such cases. We follow here the same general idea and aim, in a first step, at improving the comparability of a given corpus while preserving most of its vocabulary. However, unlike the previous work, we show here that it is possible to guarantee a certain degree of homog"
P11-2083,P07-1084,0,0.230398,"Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have r"
P11-2083,P06-1011,0,0.0476767,"sistently outperforms the previous work and also the original corpus on all the values considered for N . P2 P2' P1 P1' P0 0.8 Precision 0.6 0.4 0.2 0.0 0 100 200 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it. However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which finally leads to comparable corpora of higher quality. In turn suc"
P11-2083,N04-1034,0,0.0136395,"one can note, our method consistently outperforms the previous work and also the original corpus on all the values considered for N . P2 P2' P1 P1' P0 0.8 Precision 0.6 0.4 0.2 0.0 0 100 200 4 Discussion As previous studies on bilingual lexicon extraction from comparable corpora radically differ on resources used and technical choices, it is very difficult to compare them in a unified framework (Laroche and Langlais, 2010). We compare in this section our method with some ones in the same vein (i.e. enhancing bilingual corpora prior to extracting bilingual lexicons from them). Some works like (Munteanu et al., 2004) and (Munteanu and Marcu, 2006) propose methods to extract parallel fragments from comparable corpora. However, their approach only focuses on a very small part of the original corpus, whereas our work aims at preserving most of the vocabulary of the original corpus. We have followed here the general approach in (Li and Gaussier, 2010) which consists in enhancing the quality of a comparable corpus prior to extracting information from it. However, despite this latter work, we have shown here a method which ensures homogeneity of the obtained corpus, and which finally leads to comparable corpora"
P11-2083,J03-1002,0,0.00385611,"ns extracted from comparable corpora. We introduce a clustering-based approach for enhancing corpus comparability which exploits the homogeneity feature of the corpus, and finally preserves most of the vocabulary of the original corpus. Our experiments illustrate the well-foundedness of this method and show that the bilingual lexicons obtained from the homogeneous corpus are of better quality than the lexicons obtained with previous approaches. 1 Introduction Bilingual lexicons are an important resource in multilingual natural language processing tasks such as statistical machine translation (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et a"
P11-2083,P99-1067,0,0.164812,"tion (Och and Ney, 2003) and cross-language information retrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use"
P11-2083,E06-1029,0,0.0203324,"etrieval (Ballesteros and Croft, 1997). Because it is expensive to manually build bilingual lexicons adapted to different domains, researchers have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These appro"
P11-2083,P10-1011,0,0.050965,"ically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have proposed in (Li and Gaussier, 2010)"
P11-2083,N09-2031,0,0.013052,"have tried to automatically extract bilingual lexicons from various corpora. Compared with parallel corpora, it is much easier to build high-volume comparable corpora, i.e. corpora consisting of documents in different languages covering overlapping information. Several studies have focused on the extraction of bilingual lexicons from comparable corpora (Fung and McKeown, 1997; Fung and Yee, 1998; Rapp, 1999; D´ejean et al., 2002; Gaussier et al., 2004; Robitaille et al., 2006; Morin et al., 2007; Garera et al., 2009; Akiko Aizawa National Institute of Informatics Tokyo, Japan aizawa@nii.ac.jp Yu and Tsujii, 2009; Shezaf and Rappoport, 2010). The basic assumption behind most studies on lexicon extraction from comparable corpora is a distributional hypothesis, stating that words which are translation of each other are likely to appear in similar context across languages. On top of this hypothesis, researchers have investigated the use of better representations for word contexts, as well as the use of different methods for matching words across languages. These approaches seem to have reached a plateau in terms of performance. More recently, and departing from such traditional approaches, we have propos"
P11-2083,C98-1066,0,\N,Missing
P17-1075,W10-1001,0,0.0295681,"sarily correlate with those used in RC systems. Therefore, in any system analysis, ideally we would have to consult a variety of features. 4 Readability Metrics Annotation of Reading Comprehension Datasets We annotated six existing RC datasets with the prerequisite skills. We explain the annotation procedure in Section 4.1 and the annotated RC datasets in Section 4.2. In this study, we evaluated the readability of texts based on metrics in NLP. Several studies have examined readability in various applications, such as second-language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and from various aspects, such as development measures in second-language acquisition (Vajjala and Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008). Of these, we adopted the classification of linguistic features proposed by Vajjala and Meurers (2012). This was because they presented a comparison of a wide range of linguistic features focusing on second-language acquisition and their method can be applied to plain text.3 We list the readability metrics in Table 1, which were reported by Vajjala and Meurers (2012) as 4.1 Annotation Procedure We prepared annotation guidelines a"
P17-1075,P99-1042,0,0.113684,"ly used to evaluate the competence of understanding RC questions as contextual entailments. The remainder of this paper is divided into the following sections. First, we discuss related work in Section 2. Next, we specify our two classes of metrics in Section 3. In Section 4, we annotate existing RC datasets with the prerequisite skills. Section 5 gives the results of our dataset analysis and Section 6 discusses their implications. Section 7 presents our conclusions. 2 2.1 Related Work Reading Comprehension Datasets In this section, we present a short history of RC datasets. To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC. Their dataset comprised reading materials for grades 3–6 with simple 5W (wh-) questions. Subsequent investigations into questions of natural language understanding focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and 1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets."
P17-1075,bentivogli-etal-2010-building,0,0.208163,"Missing"
P17-1075,P16-1223,0,0.0464317,"Missing"
P17-1075,P11-2057,0,0.48195,"soning. This skill is a renamed version of mathematical operations. 3. Coreference resolution∗ : this skill has a small modification to include an anaphora (Dagan et al., 2013). It is similar to direct reference (Clark, 1975). 4. Logical reasoning∗ : we identified this skill as the understanding of predicate logic, e.g., conditionals, quantifiers, negation, and transitivity. Note that this skill, together with mathematical reasoning, is intended to align with the offline skills described by Graesser et al. (1994). 5. Analogy∗ : understanding of metaphors including metonymy and synecdoche (see LoBue and Yates (2011) for examples of synecdoche.) 6. Causal relation: understanding of causality that is represented by explicit expressions such as “why,” “because,” and “the reason for” (only if they exist). 7. Spatiotemporal relation: understanding of spatial and/or temporal relationships between multiple entities, events, and states. In addition, we propose the following four categories by refining the “commonsense reasoning” category proposed originally in Sugawara et al. (2017). 8. Ellipsis† : recognizing implicit/omitted information (argument, predicate, quantifier, time, or place). This skill is inspired"
P17-1075,C10-1039,0,0.0224721,"mprehensive understanding of natural language texts needs a better grasp of global coherence (e.g., the main point or moral of the text, the goal of a story, or the intention of characters) from the broad context (Graesser et al., 1994). Most questions in current use require only local coherence (e.g., referential relations and thematic roles) within a narrow context. An example of a question based on global coherence would be to give a summary of the text, as used in Hermann et al. (2015). It could be generated automatically by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010). Annotation issues: we found questions for which there were disagreements regarding nonsense decisions. For example, some questions can be solved by external knowledge without even seeing their context. Therefore, we should clarify what constitutes a “solvable” or “reasonable” question for RC. In addition, annotators reported that the prerequisite skills did not easily treat questions whose answer was “none of the above” in QA4MRE. We considered these “no answer” questions difficult, in that systems have to decide not to select any of the candidate answers, and our methodology failed to speci"
P17-1075,D16-1241,0,0.167252,"Missing"
P17-1075,P16-1144,0,0.0983128,"Missing"
P17-1075,W16-6001,1,0.913073,"ds to one of the functions of an NLP system, which has to be capable of that functionality. Our second class defines metrics for “text ease of processing,” namely the difficulty of reading the text. We regard it as readability of the text in terms of syntactic and lexical complexity. From among readability studies in NLP, we adopt a wide range of linguistic features proposed by Vajjala and Meurers (2012), which can be used for texts with no available annotations. The contributions of this paper are as follows. 3. We annotate six existing RC datasets, compared to the two datasets considered in Sugawara and Aizawa (2016), with our organized metrics being used in the comparison. We have made the results publicly available1 and report on the characteristics of the datasets and the differences between them. We should note that, in this study, RC datasets with different task formulations were annotated with prerequisite skills under the same conditions. Annotators first saw a context, a question, and its answer. They selected the sentences required to provide the answer, and then annotated them with appropriate prerequisite skills. That is, the datasets were annotated from the point of view of whether the context"
P17-1075,D08-1020,0,0.034156,"nnotation of Reading Comprehension Datasets We annotated six existing RC datasets with the prerequisite skills. We explain the annotation procedure in Section 4.1 and the annotated RC datasets in Section 4.2. In this study, we evaluated the readability of texts based on metrics in NLP. Several studies have examined readability in various applications, such as second-language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and from various aspects, such as development measures in second-language acquisition (Vajjala and Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008). Of these, we adopted the classification of linguistic features proposed by Vajjala and Meurers (2012). This was because they presented a comparison of a wide range of linguistic features focusing on second-language acquisition and their method can be applied to plain text.3 We list the readability metrics in Table 1, which were reported by Vajjala and Meurers (2012) as 4.1 Annotation Procedure We prepared annotation guidelines according to Sugawara et al. (2017). The guidelines include the definitions and examples of the skills and annotation instructions. Four annotators were asked to simul"
P17-1075,D16-1264,0,0.0879715,"ing to each metric and the correlation between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer. 1 ID: MCTest, mc160.dev.8 Context: Sara wanted to play on a baseball team. She had never tried to swing a bat and hit a baseball before. Her Dad gave her a bat and together they went to the park to practice. Question: Why was Sara practicing? Answer: She wanted to play on a team Figure 1: Examples of RC questions from SQuAD (Rajpurkar et al., 2016) and MCTest (Richardson et al., 2013) (the Contexts are excerpts). situations, it is difficult to obtain an accurate assessment of the RC system. Introduction A major goal of natural language processing (NLP) is to develop agents that can understand natural language. Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them. Constructing systems with RC competence is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and gene"
P17-1075,R15-1068,0,0.0201828,"proposed for human readability, they do not necessarily correlate with those used in RC systems. Therefore, in any system analysis, ideally we would have to consult a variety of features. 4 Readability Metrics Annotation of Reading Comprehension Datasets We annotated six existing RC datasets with the prerequisite skills. We explain the annotation procedure in Section 4.1 and the annotated RC datasets in Section 4.2. In this study, we evaluated the readability of texts based on metrics in NLP. Several studies have examined readability in various applications, such as second-language learning (Razon and Barnden, 2015) and text simplification (Aluisio et al., 2010), and from various aspects, such as development measures in second-language acquisition (Vajjala and Meurers, 2012) and discourse relations (Pitler and Nenkova, 2008). Of these, we adopted the classification of linguistic features proposed by Vajjala and Meurers (2012). This was because they presented a comparison of a wide range of linguistic features focusing on second-language acquisition and their method can be applied to plain text.3 We list the readability metrics in Table 1, which were reported by Vajjala and Meurers (2012) as 4.1 Annotatio"
P17-1075,W12-2019,0,0.129024,"the question. Based on this observation, in this work, we assume that the number of skills required to answer a question is a reasonable indication of the difficulty of the question. This is because each skill corresponds to one of the functions of an NLP system, which has to be capable of that functionality. Our second class defines metrics for “text ease of processing,” namely the difficulty of reading the text. We regard it as readability of the text in terms of syntactic and lexical complexity. From among readability studies in NLP, we adopt a wide range of linguistic features proposed by Vajjala and Meurers (2012), which can be used for texts with no available annotations. The contributions of this paper are as follows. 3. We annotate six existing RC datasets, compared to the two datasets considered in Sugawara and Aizawa (2016), with our organized metrics being used in the comparison. We have made the results publicly available1 and report on the characteristics of the datasets and the differences between them. We should note that, in this study, RC datasets with different task formulations were annotated with prerequisite skills under the same conditions. Annotators first saw a context, a question, a"
P17-1075,D13-1020,0,0.51805,"on between the two classes. Our dataset analysis suggests that the readability of RC datasets does not directly affect the question difficulty and that it is possible to create an RC dataset that is easy to read but difficult to answer. 1 ID: MCTest, mc160.dev.8 Context: Sara wanted to play on a baseball team. She had never tried to swing a bat and hit a baseball before. Her Dad gave her a bat and together they went to the park to practice. Question: Why was Sara practicing? Answer: She wanted to play on a team Figure 1: Examples of RC questions from SQuAD (Rajpurkar et al., 2016) and MCTest (Richardson et al., 2013) (the Contexts are excerpts). situations, it is difficult to obtain an accurate assessment of the RC system. Introduction A major goal of natural language processing (NLP) is to develop agents that can understand natural language. Such an ability can be tested with a reading comprehension (RC) task that requires the agent to read open-domain documents and answer questions about them. Constructing systems with RC competence is challenging because RC comprises multiple processes including parsing, understanding cohesion, and inference with linguistic and general knowledge. Clarifying what a syst"
P17-1075,D15-1044,0,0.00340332,"l understanding, comprehensive understanding of natural language texts needs a better grasp of global coherence (e.g., the main point or moral of the text, the goal of a story, or the intention of characters) from the broad context (Graesser et al., 1994). Most questions in current use require only local coherence (e.g., referential relations and thematic roles) within a narrow context. An example of a question based on global coherence would be to give a summary of the text, as used in Hermann et al. (2015). It could be generated automatically by techniques of abstractive text summarization (Rush et al., 2015; Ganesan et al., 2010). Annotation issues: we found questions for which there were disagreements regarding nonsense decisions. For example, some questions can be solved by external knowledge without even seeing their context. Therefore, we should clarify what constitutes a “solvable” or “reasonable” question for RC. In addition, annotators reported that the prerequisite skills did not easily treat questions whose answer was “none of the above” in QA4MRE. We considered these “no answer” questions difficult, in that systems have to decide not to select any of the candidate answers, and our meth"
P17-1075,D07-1003,0,0.00592074,"s with the prerequisite skills. Section 5 gives the results of our dataset analysis and Section 6 discusses their implications. Section 7 presents our conclusions. 2 2.1 Related Work Reading Comprehension Datasets In this section, we present a short history of RC datasets. To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC. Their dataset comprised reading materials for grades 3–6 with simple 5W (wh-) questions. Subsequent investigations into questions of natural language understanding focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and 1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets. 1 http://www-al.nii.ac.jp/rc_dataset_ analysis 2. We revise a previous classification of pre807 textual entailment (Bentivogli et al., 2010; Sammons et al., 2010; Dagan et al., 2006). One of the RC tasks of the time was QA4MRE (Sutcliffe et al., 2013). The highest accuracy achieved for this task was"
P17-1075,P10-1122,0,0.0921939,"Missing"
P17-1075,D15-1237,0,0.0264355,"existing RC datasets with the prerequisite skills. Section 5 gives the results of our dataset analysis and Section 6 discusses their implications. Section 7 presents our conclusions. 2 2.1 Related Work Reading Comprehension Datasets In this section, we present a short history of RC datasets. To our knowledge, Hirschman et al. (1999) were the first to use NLP methods for RC. Their dataset comprised reading materials for grades 3–6 with simple 5W (wh-) questions. Subsequent investigations into questions of natural language understanding focused on other formulations, such as question answering (Yang et al., 2015; Wang et al., 2007; Voorhees et al., 1999) and 1. We adopt two classes of evaluation metrics to show the qualitative features of RC datasets. Through analyses of RC datasets, we demonstrate that there is only a weak correlation between the difficulty of questions and the readability of context texts in RC datasets. 1 http://www-al.nii.ac.jp/rc_dataset_ analysis 2. We revise a previous classification of pre807 textual entailment (Bentivogli et al., 2010; Sammons et al., 2010; Dagan et al., 2006). One of the RC tasks of the time was QA4MRE (Sutcliffe et al., 2013). The highest accuracy achieved"
P17-1075,D15-1197,0,\N,Missing
P17-2080,D17-1066,0,0.0359631,"ther specific areas. 2 Models higher-level context vector is the concatenation of both status vectors. We will show later that SPHRED not only well keeps individual features, but also provides a better holistic representation for the response decoder than normal HRED. To provide a better dialog context, we build a hierarchical recurrent encoder-decoder with separated context models (SPHRED). This section first introduces the concept of SPHRED, then explains the conditional variational framework and two application scenarios. 2.2 VAEs have been used for text generation in (Bowman et al., 2015; Semeniuta et al., 2017), where texts are synthesized from latent variables. Starting from this idea, we assume every utterance wn comes with a corresponding label yn and latent variable zn . The generation of zn and wn are conditioned on the dialog context provided by SPHRED, and this additional class label yn . This includes 2 situations, where the label of the next sequence is known (like for Scenario 1 in Section 2.3) or not (Section 2.4). For each utterance, the latent variable zn is first sampled from a prior distribution. The whole dialog can be explained by the generative process: 2.1 SPHRED We decomposes a d"
P17-2080,P15-1152,0,0.0834311,"Missing"
P17-2080,P15-2073,0,0.00814574,"Missing"
P17-2080,N15-1020,0,0.0192508,"Missing"
P17-2080,D16-1230,0,0.00490343,"also experimented by substituting the encoder with a normal HRED, the resulting model cannot predict the correct sentiment at all because the context information is highly mingled for both speakers. The embedding based scores of our framework are still comparable with SPHRED and even better than VHRED. Imposing an external label didn’t bring any significant quality decline. Evaluation Accurate automatic evaluation of dialog generation is difficult (Galley et al., 2015; Pietquin and Hastie, 2013). In our experiment, we conducted three embedding-based evaluations (average, greedy and extrema) (Liu et al., 2016) on all our models, which map responses into vector space and compute the cosine similarity. Though not necessarily accurate, the embedding-based metrics can to a large extent measure the semantic similarity and test the ability of successfully generating a response sharing a similar topic with the golden answer. The results of a GRU language model (LM), HRED and VHRED were also provided for comparison. For the two scenarios of our framework, we further measured the percentage of generated responses matching the correct labels (accuracy). In (Liu et al., 2016), current popular metrics are show"
P17-2080,W15-4640,0,0.00336433,"nt tag of next utterance as the average of the preceding two ones. Namely, if one is positive and the other is negative, the next response would be neutral. The label y represents the sentiment tag, which is unknown at test time and needs to be predicted from the context. The probability qφ (yn |w1n−1 ) is modeled by feedforward neural networks. This scenario is designed to demonstrate our framework can successfully learn the manually defined rules to predict the proper label and decode responses conforming to this label. 3 Experiments We conducted our experiments on the Ubuntu dialog Corpus (Lowe et al., 2015), which contains about 500,000 multi-turn dialogs. The vocabulary was set as the most frequent 20,000 words. All the letters are transferred to lowercase and the Outof-Vocabulary (OOV) words were preprocessed as &lt;unk> tokens. 3.1 Training Procedures Model hyperparameters were set the same as in VHRED model except that we reduced by half the context RNN dimension. The encoder, context and decoder RNNs all make use of the Gated Recurrent Unit (GRU) structure (Cho et al., 2014). Labels were mapped to embeddings with size 100 and word vectors were initialized with the pubic Word2Vec embeddings tra"
P18-2028,A00-1043,0,0.188962,"tively generate more readable compression, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a largescale dataset, setting a new baseline for the future research. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sen"
P18-2028,P16-1231,0,0.0149942,"esearch. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong baselines. In short, our contributions are two-fold: (i) an effective syntaxbased evaluator is built as a post-hoc checker, yielding compression with better quality base"
P18-2028,P11-1049,0,0.085838,"Missing"
P18-2028,E06-1038,0,0.0646979,"rior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a largescale dataset, setting a new baseline for the future research. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong baselines. In sh"
P18-2028,P16-2055,0,0.158652,"baseline for the future research. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong baselines. In short, our contributions are two-fold: (i) an effective syntaxbased evaluator is built as a post-hoc checker, yielding compression with"
P18-2028,E17-1096,0,0.0215472,"next word token. We observed that the prediction of the next word could not only be based on the previous word but also the syntactic components, e.g., for the part-of-speech tag, the noun is often followed by a verb instead of an adjective or adverb and the integration of the part-ofspeech tag allows the model to learn such correct word collocations. Figure 2 shows the graphical illustration of the evaluator-SLM where the input is xi = [e(wi ); p(wi ); d(wi )], followed by a bi-directional RNN whose last layer is the Softmax layer used to represent word probability distribution. Similar to (Mousa and Schuller, 2017), we added two special tokens, <S> and </S> in the input so as to stagger the hidden vectors, thus avoiding self-prediction. Finally, we have the following formula as one part of the reward functions in the learning framework. 171 W1 W2 X2 Wn-1 X3 <S> Xn X1 Wn compression. Further, we asked two near native English speakers to create 200 extractive compressions for the first 200 sentences of this dataset; using it as the testing set, the first 1,000 sentences (excluding the testing set) is the development set, and the remainder is the training set. To assess the inter-assessor agreements, we co"
P18-2028,briscoe-carroll-2002-robust,0,0.0668754,"P-F1 54.9 60.3 58.0 65.1 60.3 64.1 64.5 67.3 65.0 69.6 Annotator 2 F1 RASP-F1 58.6 64.6 61.0 70.9 64.1 69.2 66.9 72.2 68.2 73.9 CR 0.53 0.55 0.51 0.50 0.51 Table 1: F1 and RASP-F1 results for Gigaword dataset. 4 consider the labels yielded by our dependencytree-based method as pseudo labels and employ LSTMs as a baseline. This section demonstrates the experimental results on both datasets. As the Gigaword dataset has no ground truth, we evaluated the baseline and our method on the 200-sentence test sets created by two human annotators. For the automatic evaluation, we employed F1 and RASP-F1 (Briscoe and Carroll, 2002) to measure the performances. The latter compares grammatical relations (such as ncsubj and dobj ) found in the system compressions with those found in the gold standard, providing a means to measure the semantic aspects of the compression quality. For the human evaluation, we asked two near native English speakers to assess the quality of 50 compressed sentences out of the 200-sentence test set in terms of readability and informativeness. Here are our observations: Furthermore, for a comprehensive comparison, we applied the sequence-to-sequence with attention method widely used in abstractive"
P18-2028,N16-1012,0,0.0931675,"nd dobj ) found in the system compressions with those found in the gold standard, providing a means to measure the semantic aspects of the compression quality. For the human evaluation, we asked two near native English speakers to assess the quality of 50 compressed sentences out of the 200-sentence test set in terms of readability and informativeness. Here are our observations: Furthermore, for a comprehensive comparison, we applied the sequence-to-sequence with attention method widely used in abstractive text summarization for sentence compression. Previous works such as (Rush et al., 2015; Chopra et al., 2016) have shown promising results with this framework, although the focus was generationbased summarization rather than extractive summarization. More specifically, the source sequence of this framework is the original sentence, while the target sequence is a series of zeros and ones (zeros represents REMOVE and ones represents RETAIN). Further, we incorporated dependency labels and part-of-speech tag features in the source side of the sequence-to-sequence method. 3.3 Gigaword $1 LSTMs $2 SLM Readability 3.56 4.16† Inf ormativeness 3.10 3.16 Training Table 2: Human Evaluation for Gigaword dataset."
P18-2028,W11-1611,0,0.411472,"Missing"
P18-2028,D15-1044,0,0.104741,"s (such as ncsubj and dobj ) found in the system compressions with those found in the gold standard, providing a means to measure the semantic aspects of the compression quality. For the human evaluation, we asked two near native English speakers to assess the quality of 50 compressed sentences out of the 200-sentence test set in terms of readability and informativeness. Here are our observations: Furthermore, for a comprehensive comparison, we applied the sequence-to-sequence with attention method widely used in abstractive text summarization for sentence compression. Previous works such as (Rush et al., 2015; Chopra et al., 2016) have shown promising results with this framework, although the focus was generationbased summarization rather than extractive summarization. More specifically, the source sequence of this framework is the original sentence, while the target sequence is a series of zeros and ones (zeros represents REMOVE and ones represents RETAIN). Further, we incorporated dependency labels and part-of-speech tag features in the source side of the sequence-to-sequence method. 3.3 Gigaword $1 LSTMs $2 SLM Readability 3.56 4.16† Inf ormativeness 3.10 3.16 Training Table 2: Human Evaluation"
P18-2028,P06-2019,0,0.0464832,"ssion, comparable or superior to several strong baselines. Furthermore, we introduce a 200-sentence test set for a largescale dataset, setting a new baseline for the future research. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong"
P18-2028,P17-1127,0,0.338517,"sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong baselines. In short, our contributions are two-fold: (i) an effective syntaxbased evaluator is built as a post-hoc checker, yielding compression with better quality based upon the evaluation metrics; (ii) a l"
P18-2028,D15-1042,0,0.601065,"Missing"
P18-2028,D13-1155,0,0.250842,"ine; h(wi )=1 otherwise. (3) the dependency relations, ROOT, dobj, nsubj, pobj, should be retained as they are the skeletons of a sentence. (4) the sentence length should be over than α but less than β. (5) the depth of the node (word), λdep(wi ), in the dependency tree. (6) the word with the dependency relation amod is to be removed. It is noteworthy that the method is unsupervised. The second method is the long short-term memory networks (LSTMs) which showed strong promise in sentence compression by (Filippova et al., 2015). The labels were obtained using the dependency tree pruning method (Filippova and Altun, 2013) and the LSTMs were applied in a supervised manner. Following their works, we also (2) t=1 Where at ∈ {RETAIN, REMOVE}, is the action token by the policy network, and St refers to hidden state of the network, [hfi ; hbi ] (section 2.1). 3 Experiments 3.1 Comparison Methods Data As neural network-based methods require a large amount of training data, we for the first time considered using Gigaword2 , a news domain corpus. More specifically, the first sentence and the headline of each article are extracted. After data cleansing, we finally compiled 1.02 million sentence and headline pairs (see d"
P18-2028,W08-1105,0,0.516832,"more, we introduce a 200-sentence test set for a largescale dataset, setting a new baseline for the future research. 1 Introduction Deletion-based sentence compression aims to delete unnecessary words from source sentence to form a short sentence (compression) while retaining grammatical and faithful to the underlying meaning of the source sentence. Previous works used either machine-learning-based approach or syntactic-tree-based approaches to yield most readable and informative compression (Jing, 2000; Knight and Marcu, 2000; Clarke and Lapata, 2006; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008; BergKirkpatrick et al., 2011; Filippova et al., 2015; Bingel and Søgaard, 2016; Andor et al., 2016; Zhao et al., 2017; Wang et al., 2017). For example, (Clarke and Lapata, 2008) proposed a syntactictree-based method that considers the sentence 170 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 170–175 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics preserve the source sentences, comparable or superior to several strong baselines. In short, our contributions are two-fold: (i) an effective"
P19-1216,J08-4004,0,0.0319326,"n, which were used as reference compression in the automatic evaluation. Two annotators5 were asked to generate one single reduced grammatical compression that satisfies two conditions:(1) conveys the important content of all the input sentences and (2) should be grammatically correct. We are interested in how the human annotators will perform this task without vocabulary constraints; hence, we did not tell them to introduce as little new vocabulary as possible in their compression as several previous works did (Boudin and Morin, 2013; Luong et al., 2015). Inter-agreement score Fleiss’ Kappa (Artstein and Poesio, 2008) was also computed. The score was 0.43, demonstrating that moderate agreement was reached. 3 Methodology Figure 1 illustrates our rewriting approach consisting of three steps. 3.1 Step.1 (A→B) Given m input sentences, s1 , s2 , ..., sm , called A, we use the keyphrase word graph approach (Boudin and Morin, 2013) to obtain coarsegrained compression, called B. 3.2 Step.1 (B→C) C is yielded by substituting words and phrases in B with synonyms. We first identified all the multiword expressions in a sentence and determined all the synonyms in WordNet 3.06 . Keep in mind that our goal is to shorten"
P19-1216,J05-3002,0,0.238308,"Missing"
P19-1216,N13-1030,0,0.0953512,"Giga-MSC Dataset Annotation We randomly selected 150 sentences for human annotation, which were used as reference compression in the automatic evaluation. Two annotators5 were asked to generate one single reduced grammatical compression that satisfies two conditions:(1) conveys the important content of all the input sentences and (2) should be grammatically correct. We are interested in how the human annotators will perform this task without vocabulary constraints; hence, we did not tell them to introduce as little new vocabulary as possible in their compression as several previous works did (Boudin and Morin, 2013; Luong et al., 2015). Inter-agreement score Fleiss’ Kappa (Artstein and Poesio, 2008) was also computed. The score was 0.43, demonstrating that moderate agreement was reached. 3 Methodology Figure 1 illustrates our rewriting approach consisting of three steps. 3.1 Step.1 (A→B) Given m input sentences, s1 , s2 , ..., sm , called A, we use the keyphrase word graph approach (Boudin and Morin, 2013) to obtain coarsegrained compression, called B. 3.2 Step.1 (B→C) C is yielded by substituting words and phrases in B with synonyms. We first identified all the multiword expressions in a sentence and d"
P19-1216,C08-1018,0,0.124051,"Missing"
P19-1216,C18-1102,0,0.0370675,"Missing"
P19-1216,P16-1009,0,0.118676,"Missing"
P19-1216,C08-1122,0,0.0412024,"y and word embedding were shared between both backward and forward models. We expect that because the grammatical C ′ accepts the majority of 4 Experiments 4.1 Datasets We used two datasets to evaluate the model performance. First is the Giga-MSC dataset detailed in Section 2. A total of 150 annotated sentences were used as the ground truth for testing. Second is the Cornell dataset (McKeown et al., 2010). We considered (#1) the word graph approach (Filippova, 2010), and an advanced version (#2) keyphrase-based word graph model (Boudin and Morin, 2013) augmented with keyphrase identification (Wan and Xiao, 2008), as our word graph baselines. Additionally, (#3) the hard paraphrasing (Hard-Para) approach directly substituted words and phrases with their shorter synonyms by using WordNet and PPDB 2.0 (size M is chosen with 463,433 paraphrasing pairs). (#4) Seq2seq model was trained using (B, C) pairs. We considered both of them as comparison approaches as well. The training details are presented in Appendix 1. We release the source code here8 . 4.3 Out-of-Vocabulary (OOV) Word Handling Both datasets were from the news domain; hence, there are lots of organizations and names that are out of vocabulary. W"
P19-1216,K18-1040,0,0.127129,"ing by these extraction-based approaches. In fact, human annotators tend to compress a sentence through several rewriting operations, such as substitution and rewording (Cohn and Lapata, 2008). Despite some research works that attempt to do the lexical substitution, it is often inappropriate without the consideration of context information. To tackle the above-mentioned problems, we present herein an unsupervised rewriter to improve the grammaticality of compression while introducing an appropriate amount of novel words. Inspired by the unsupervised machine translation (Sennrich et al., 2015; Fevry and Phang, 2018), we adopted the back-translation technique to our setting. Unlike machine translation, in the case of compression task, multiple input sentences and single output compression usually do not have semantic equivalence, which complicates the application of the back-translation technique. Thus, we propose a rewriting scheme that first exploits word graph approach to produce coarse-grained compression (B), based on which we substitute words with their shorter synonyms to yield paraphrased sentence (C). A neural rewriter is subsequently applied to the semantically equivalent (B, C) pairs 2235 Proce"
P19-1216,C10-1037,0,0.22546,"ession (MSC) aims to generate a single shorter and grammatical sentence that preserves important information from a group of related sentences. Over the past decade, multisentence compression has attracted considerable attention owing to its potential applications, such as compressing the content to be displayed on screens with limited size (e.g., mobile devices) and benefiting other natural language processing tasks, such as multi-document summarization (Banerjee et al., 2015), opinion summarization, and text simplification. Most existing works rely on the word graph approach initialized in (Filippova, 2010), which offers a simple solution that copies fragments from different input sentences and concatenates them to form the final compression. Later on, a bunch of subsequent research works (Boudin and Morin, 2013; Banerjee et al., 2015; Luong et al., 2015; ShafieiBavani et al., 2016; Pontes et al., 2018; Nayeem et al., 2018) attempted to improve the word graph approach using a variety of strategies, such as keyphrase re-ranking. However, such extraction-based approach may yield nonfluent or ungrammatical compression. A previous study (Nayeem and Chali, 2017) has shown that word graph approaches p"
P19-1216,N10-1044,0,0.161077,"words in compression. Our contributions are two-folds:(i) we present a neural rewriter for multi-sentence compression without any parallel data. This rewriter significantly improves the grammaticality and novel word rate, while maintaining the information coverage (informativeness) according to automatic evaluation and (ii) a large-scale multi-sentence compression corpus is introduced along with a manually created test set for future research. We release source code and data here1 . 2 Dataset Construction The largest existing English corpus for multisentence compression is the Cornell corpus (McKeown et al., 2010), which has only 300 instances. We introduce herein a large-scale dataset by compiling the English Gigaword2 . After preprocessing (e.g., filtering strange punctuations, etc.), 1.37 million news articles were yielded to group related sentences. The full procedure for the dataset construction is available here3 . 2.1 Group Related Sentences The prerequisite for multi-sentence compression is that all input sentences should be related to the same topic or event. Inspired by (McKeown et al., 2010), if the sentences are too similar, one of the input sentences could be directly treated as a compress"
S18-1126,C14-1008,0,0.0753278,"Missing"
S18-1126,S18-1111,0,0.0671588,"Missing"
S18-1126,D14-1181,0,0.0115647,"Missing"
S18-1126,P15-2060,0,0.0359472,"Missing"
S18-1126,C14-1220,0,0.0473647,"Word table layer Firstly, we determined n as the maximum sentence length in the training dataset. Those sentences with lengths shorter than n are padded with an auxiliary token ”0”. After that, we assigned a randomly initialized vector for each different word, creating thus a word embedding matrix: We ∈ R|V |×me where V is the vocabulary size and me is the word embedding dimension. Finally, we obtained a matrix x = [x1 ; x2 ; ...; xn ] for each instance where the words are represented by their corresponding word embedding vectors. In addition, we used the word position embedding described in (Zeng et al., 2014), which Table 1: Instances of a sentence in the corpus after applying the pre-processing phase with entity blinding. 1 Instances 190 326 234 72 18 483 17887 19210 http://www.nltk.org 794 The &lt;e1>classification accuracy&lt;/e1> of the method is evaluated on &lt;e2>spoken language system domains&lt;/e2> Preprocessing Sentence: POS: distance1: distance2: the entity1 of the method is evaluated on entity2 DT NN IN DT NN VBZ VBN IN NN -1 0 6 7 1 2 3 4 5 -8 -7 -6 -5 -1 0 -4 -3 -2 WPOS We Wd1 Wd2 |P| 2n-1 |V| mPOS md md me Word embeddings n POS embeddings Position embeddings Position embeddings the entity1 of"
tateisi-etal-2014-annotation,liakata-etal-2010-corpora,0,\N,Missing
tateisi-etal-2014-annotation,brants-2000-inter,0,\N,Missing
tateisi-etal-2014-annotation,D11-1025,0,\N,Missing
tateisi-etal-2014-annotation,P11-4002,0,\N,Missing
tateisi-etal-2014-annotation,varga-etal-2012-unsupervised,0,\N,Missing
tateisi-etal-2014-annotation,mori-etal-2014-flow,0,\N,Missing
tateisi-etal-2014-annotation,I11-1001,0,\N,Missing
tateisi-etal-2014-annotation,W13-2001,0,\N,Missing
tateisi-etal-2014-annotation,E12-2021,0,\N,Missing
tateisi-etal-2014-annotation,W13-2318,1,\N,Missing
W10-3910,N06-1025,0,\N,Missing
W10-3910,P09-1051,0,\N,Missing
W12-4905,W09-1113,0,0.15189,"uman reads a piece of text, especially for the first time, it is important that his/her eye movements are optimized for rapid understanding of the text. Humans typically perform this optimization unconsciously, which is reflected in the gaze data. Eye movements while reading text have long been explored in the field of psycholinguistics (Rayner, 1998), and the accumulated knowledge of human eye movements has been reflected in various eye movement models (Reichle et al., 1998, 2003, 2006). Reinterpretation of the knowledge from an NLP perspective, however, has not been thoroughly investigated (Nilsson and Nivre, 2009, 2010; Martínez-Gómez et al., 2012). One possible reason for this could be that eye movements inevitably contain individual differences among readers as well as unstable movements caused by various external or internal factors, which make it difficult to extract general reading strategies from gaze data obtained from different readers or even from a single reader. In this research, we explore whether this difficulty can be overcome. We aim to predict whether each word in the text is fixated by training conditional random field (CRF) models on existing gaze data (Kennedy, 2003), and then exami"
W12-4905,W10-2008,0,0.0128946,"ne such model. The E-Z Reader was developed to explain how eye movements are generated for the target gaze data, and not to predict eye movements when reading text for the first time. These models are optimized for the target gaze data by adjusting certain parameters without including any machine learning approaches. On the other hand, the work presented in (Nilsson and Nivre, 2009) was, as the authors stated, the first work that incorporated a machine learning approach to model human eye movements. The authors predicted word-based fixations for unseen text using a transition-based model. In (Nilsson and Nivre, 2010), temporal features were also considered to predict the duration of fixations. There are important differences between the two approaches mentioned above, other than the way in which the parameters are adjusted and the purpose of the modeling. The former approach modeled the average eye movement of the subjects, while the latter trained the model for each subject. The key point here is that the former approach attempts to generalize human eye-movement strategies, while the latter attempts to capture individual characteristics. Our final goal is not only to explain or predict human eye movement"
W12-4905,W07-2208,0,0.013964,"ect of this on gaze behavior, words in other tools were treated in the same manner. For the same reason, we left the capitalization of words unchanged. To train the CRF models, we utilized CRFsuite (Okazaki, 2007) ver. 0.12. We used a sentence as an input/output unit, since many of the existing NLP technologies are based on sentence-level processing, and we intend to associate outputs of the CRF models with NLP technologies in our future work. To obtain input sentences, five 80-character lines in each screen were split into sentences using the sentence splitter implemented in the Enju parser (Ninomiya et al., 2007)1 . In training the CRF models, we selected the option of maximizing the logarithm of the training data with an L1 regularization term, since this would effectively eliminate useless features, thereby highlighting those features that really contributed to capturing the gaze data. The coefficient for L1 regularization in each model was adjusted in the test data to examine to what extent we could explain the given data using our features. We next explain the features utilized for training our CRF models. 5.2 Features utilized for training CRF models Based on the observation in Section 4.2, we se"
W12-4905,N03-1028,0,0.0282518,"eferred to as peripheral fields. 57 t h r e a t e n i n g ● 96ms t h e i r ● 232ms v e r y e x i s t e n c e ? ● 168ms 335ms 173ms 188ms 88ms : fixation : saccade : regression Figure 1: Character-based gaze data in the Dundee Corpus 3.2 Conditional random fields CRFs (Lafferty et al., 2001) are a type of discriminative undirected probabilistic graphical model. Theoretically, CRFs can deal with various types of graph structures although we use CRFs for sequential labeling of whether each word is fixated. We therefore, explain CRFs with respect to sequences only, borrowing the explanation from (Sha and Pereira, 2003). CRFs define the conditional probability distributions p(Y |X ) of label sequences Y given input sequences X . We assume that random variable sequences X and Y have the same length, and that the generic input and label sequences are x = x 1 · · · x n and y = y1 · · · yn , respectively. A CRF on (X, Y ) is specified by a vector f of local features and a corresponding weight vector λ. Each local feature is either a state feature s( y, x , i) or a transition feature t( y, y ′ , x , i) where y, y ′ are labels, x is an input sequence, and i is an input position. Typically, features depend on the i"
W13-2318,E12-2021,0,0.139895,"Missing"
W13-2318,D09-1155,0,0.0572349,"tionships between a technique and its applications (Gupta and Manning, 2011). Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy. (2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition. 140 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140–148, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics al., 2012), and chemistry and computational linguistics (Teufel et al., 2009). Zoning provides a sentence-based information structure of papers to help identify the components such as the proposed method and the results obtained in the study. As such, zoning can narrow down the sections of a paper in which the answer to a query can be found. However, zoning alone cannot always capture the relation between the concepts described in the sections as it focuses on relation at a sentence level. For example, the examples (1), (2), (3) in the previous section require intra-sentence analysis to capture the relation between CRF and POS tagging. Our annotation scheme, which can"
W13-2318,brants-2000-inter,0,0.553416,"Missing"
W13-2318,varga-etal-2012-unsupervised,0,0.398072,"Missing"
W13-2318,D11-1025,0,0.0716884,"resented in this study can be viewed as a starting point for research focusing on representative schemata of human activities. 2 Related Work Traditionally, research on searching research papers has focused more on the social aspects of papers and their authors, such as citation links and co-authorship analysis implemented in the aforementioned services. Recently, research on content-based analysis of research papers has been emerging. For example, methods of document zoning have been proposed for research papers in biomedicine (Mizuta et al., 2006; Agarwal and Yu, 2009; Liakata et al., 2010; Guo et al., 2011; Varga et 141 In this paper, we propose a novel strategy for parallel preconditioning of large scale linear systems by means of a two-level approximate inverse technique with AISM method. According to the numerical results on an origin 2400 by using MPI, the proposed parallel technique of computing the approximate inverse makes the speedup of about 136.72 times with 16 processors. mation that we also want to capture, such as how the author evaluates current systems and methods or the previous efforts of others. An attempt to identify the evaluation and other meta-aspects of scientific papers"
W13-2318,I11-1001,0,0.200657,"nce proceedings such as the ACL Anthology. These services focus on simple keywordbased searches as well as extralinguistic relations among research papers, authors, and research topics. However, because contemporary research is becoming increasingly complicated and interrelated, intelligent content-based search systems are desired (Banchs, 2012). A typical query in computational linguistics could be what tasks have CRFs been used for?, which includes the elements of a typical schema for searching research papers; researchers want to find relationships between a technique and its applications (Gupta and Manning, 2011). Answers to this query can be found in various forms in published papers, for example, (1) CRF-based POS tagging has achieved state-ofthe-art accuracy. (2) CRFs have been successfully applied to sequence labeling problems including POS tagging and named entity recognition. 140 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 140–148, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics al., 2012), and chemistry and computational linguistics (Teufel et al., 2009). Zoning provides a sentence-based information structure"
W13-2318,W11-1801,0,0.0439783,"Missing"
W13-2318,liakata-etal-2010-corpora,0,0.121121,"Thus, the framework presented in this study can be viewed as a starting point for research focusing on representative schemata of human activities. 2 Related Work Traditionally, research on searching research papers has focused more on the social aspects of papers and their authors, such as citation links and co-authorship analysis implemented in the aforementioned services. Recently, research on content-based analysis of research papers has been emerging. For example, methods of document zoning have been proposed for research papers in biomedicine (Mizuta et al., 2006; Agarwal and Yu, 2009; Liakata et al., 2010; Guo et al., 2011; Varga et 141 In this paper, we propose a novel strategy for parallel preconditioning of large scale linear systems by means of a two-level approximate inverse technique with AISM method. According to the numerical results on an origin 2400 by using MPI, the proposed parallel technique of computing the approximate inverse makes the speedup of about 136.72 times with 16 processors. mation that we also want to capture, such as how the author evaluates current systems and methods or the previous efforts of others. An attempt to identify the evaluation and other meta-aspects of"
W13-2318,P11-4002,0,0.0949154,"Missing"
W13-2318,W12-3200,0,\N,Missing
W13-2906,2011.iwslt-papers.7,0,0.0519518,"Missing"
W13-2906,R11-1021,0,0.021479,"F1 -score and commas chosen by our filter smoothen certain gaze behaviors. CRF model Parse Tree Linguistic Features Comma Distribution in General Text Treebank Rule-based Comma Filter Human Annotation + Gaze Features Comma Distribution for Readability Text with/without Commas Figure 1: Our approach However, although there are guidelines and research on the syntactic aspects of comma placement, prosodic aspects have not been explored, since they are more related with cognition. It is as yet unclear how comma placement should be optimized for reading, and it has thus far been up to the writer (Huang and Chen, 2011). In this research, we attempt to optimize comma placements by integrating the linguistic features of text and the gaze features of readers. Figure 1 illustrates our approach. First, we design a comma predictor for general Chinese text based on conditional random field (CRF) models with various linguistic features. Second, we build a rule-based filter for classifying commas in text into ones facilitating or obstructing readability, by comparing the gaze features of persons reading text with and without commas. These two steps are connected by applying our rule-based filter to commas predicted"
W13-2906,P11-2111,0,0.0123511,"other hand, seldom have such usages (Zeng, 2006). In Cases 3 and 4, commas instead of conjunctions sometimes connect two clauses in a relation of either coordination or subordination. English commas, on the other hand, are only required between independent clauses connected by conjunctions (Zeng, 2006). Liu et al. (2010) proved that Chinese commas can change the syntactic structures of sentences by playing lexical or syntactic roles. Ren and Yang (2010) claimed that inserting commas as clause boundaries shortens the fixation time in post-comma regions. Meanwhile, in computational linguistics, Xue and Yang (2011) showed 1 Some other features or tag formats which worked well in the previous research, such as bi-/tri-gram, a preceding word (L-1) or its POS (POS-1), and IO-style tag (Leaman and Gonzalez, 2008) were also examined, but they did not work that well, probably because of the difference in task settings. 51 No Comma commas Without (pixel) 160 160 120 120 80 80 40 40 L1 – L7 L1 – L7 T1 – T7 C1 – C8 Z1 – Z10 Trials (“Subject” + “Trial No.”) Figure 6: Fixation time per comma (pixel) 90 90 No Comma commas Without Saccade length (2) / comma With WithComma commas T1 – T7 C1 – C8 Z1 – Z10 Trials (“Sub"
W13-2906,D10-1018,0,0.0313092,"e in a sentence that a word belongs to Word length Length of fragment with specific depth in a parsing tree WS|POS|STAG|DIP|OIC|WL|LOD|IOB-tag Table 2: Features used in our CRF model 2 Related Work Previous work on Chinese punctuation prediction mostly focuses on sentence segmentation in automatic speech recognition (Shriberg et al., 2000; Huang and Zweig, 2002; Peitz et al., 2011). (a) (b) Figure 2: Example of a parse tree (a) and its corresponding training data (b) with the features Jin et al. (2002) classified commas for sentence segmentation and succeeded in improving parsing performance. Lu and Ng (2010) proposed an approach built on a dynamic CRF for predicting punctuations, sentence boundaries, and sentence types of speech utterances without prosodic cues. Zhang et al. (2006) suggested that a cascade CRF-based approach can deal with ancient Chinese prose punctuation better than a single CRF. Guo et al. (2010) implemented a three-tier maximum entropy model incorporating linguistically motivated features for generating commonly used Chinese punctuation marks in unpunctuated sentences output by a surface realizer. 3 Functions of Chinese Commas There are five main uses of commas in Chinese text"
W13-2906,P06-3008,0,0.0325989,"two steps are connected by applying our rule-based filter to commas predicted by our comma predictor. The experimental results for each step validate our approach. 1 Introduction Chinese is an ideographic language, with no natural apparent word boundaries, little morphology, and no case markers. Moreover, most Chinese sentences are quite long. These features make it especially difficult for Chinese learners to identify composition of a word or a clause in a sentence. Punctuation marks, especially commas, are allowed to be placed relatively arbitrarily to serve as important segmentation cues (Yue, 2006) for providing syntactic and prosodic boundaries in text; commas indicate not only phrase or clause boundaries but also sentence segmentations, and they capture some of the major aspects of a writer’s prosodic intent (Chafe, 1988). The combination of both aspects promotes cognition when reading text (Ren and Yang, 2010; Walker et al., 2001). ∗ + Related work is described in Section 2. The functions of Chinese commas are described in Section 3. Our CRF model-based comma predictor is examined in Section 4, and our rule-based comma filter is constructed and examined in Section 5 and 6. Section 7"
W13-2906,W04-1101,0,\N,Missing
W14-5205,andersen-etal-2008-bnc,0,0.0302135,"1, and ... . text sentence indexmark cite note Notice that … . sentence sentence (c) New UI is shown. The UI is more useful than XYZ Cite1, and ... . text indexmark cite note Notice that … . (d) &lt;sentence&gt;&lt;text&gt;New UI&lt;/text&gt; is shown.&lt;/sentence&gt; &lt;sentence&gt;The UI is more useful than XYZ&lt;indexmark&gt; … &lt;/indexmark&gt; in &lt;cite&gt;[…]&lt;/cite&gt;&lt;note&gt;&lt;sentence&gt;Notice that … . &lt;/sentence&gt;&lt;/note&gt;, and … .&lt;/sentence&gt; Figure 2: Example of executing our strategy not explain how the given text can be used in a target annotation process such as parsing. Some projects based on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano et al., 2011), and Kachako (Kano, 2012)2 , have developed systems where the connections between various documents and various tools are already established. Users, however, can utilize only the text and tool pairs that have already been integrated into the systems. GATE (Cunningham et al., 2013) is based on a similar concept to UIMA; it supports XML documents as its input, while the framework also requires integration of tools into the systems. In our framework, although availability of XML documents is assumed, a user can apply NLP tools to the documents without modifying th"
W14-5205,J07-4004,0,0.0136817,"plemented. In Section 4, the efficiency of our framework and the adequacy of the obtained text sequences for use in NLP tools are examined using several types of documents. 2 Related Work To the best of our knowledge, no significant work on a unified methodology for data conversion between target text and the input/output formats of NLP tools has been published. Some NLP tools provide scripts for extracting valid input text for the tools from real-world documents; however, even these scripts assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser (Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The POS-taggers assume plain text sentences as their input. As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations in an integrated framework. However, in this work, the authors merely proposed the framework and did 1 45 / [Enju]: http://kmcs.nii.ac.jp/enju/ [C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki Decoration Meta-info (a) &lt;text&gt;New UI&lt;/text&gt; is shown. The UI"
W14-5205,de-marneffe-etal-2006-generating,0,0.0285424,"Missing"
W14-5205,W07-2208,1,0.753574,"iency of our framework and the adequacy of the obtained text sequences for use in NLP tools are examined using several types of documents. 2 Related Work To the best of our knowledge, no significant work on a unified methodology for data conversion between target text and the input/output formats of NLP tools has been published. Some NLP tools provide scripts for extracting valid input text for the tools from real-world documents; however, even these scripts assume specific formats for the documents. For example, deep syntactic parsers such as the C&C Parser (Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, and therefore the distributed packages for the parsers1 contain POS-taggers together with the parsers. The POS-taggers assume plain text sentences as their input. As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotations in an integrated framework. However, in this work, the authors merely proposed the framework and did 1 45 / [Enju]: http://kmcs.nii.ac.jp/enju/ [C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki Decoration Meta-info (a) &lt;text&gt;New UI&lt;/text&gt; is shown. The UI is more useful than XYZ &lt;indexmar"
W14-7008,P14-1023,0,0.0388694,"itched. Finally, Rule 3 is applied in step 5 and affects only the Japanese text. いる and を, both labeled as function words by the parser. 2.1.2 Compositional Distributed Semantics Many machine learning algorithms require fixed length vectors as input. Of the various different ways to map words to vectors, neural network based models have proven very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural netwo"
W14-7008,D12-1050,0,0.0318483,"very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japanese/English language pair. As a first step in this direction, we integrate the model introduced by Hermann et al. (Hermann and Blunsom, 2014) into our baseline system, specifical"
W14-7008,J81-4005,0,0.775336,"Missing"
W14-7008,P14-1129,0,0.0359334,"ne translation should take word order into account and should be trained towards objectives that do not neglect syntactic features. The semantic similarity that can be captured by the model described in (Hermann and Blunsom, 2014) appears to be already sufficiently covered by the default features of our baseline system for the case of machine translation. Employing the recursive neural network language model introduced by Mikolov et al. (Mikolov et al., 2011a) has proven successful for reranking translation candidates and significantly improved the RIBES score in our experiments. Recent work (Devlin et al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014) proves, there is a lot of potential in utilizing neural network based models in the machine translation pipeline. With the lessons learned from our work we hope for successful applications of this combination to the Japanese-English language pair in the future. ample 2 shows a very natural structure, following the word order of the preordered Japanese source almost exactly. 5 Conclusion and Future Work In this work we investigated several modifications and insertions to the pipeline of our baseline statistical machine translation system"
W14-7008,D13-1139,0,0.0174431,"word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP to obtain chunked1 depe"
W14-7008,W14-7001,0,0.0417255,"ores into the translation model of our baseline system that are computed from semantically meaningful distributed vector representations. • As postprocessing, we utilize a recurrent neural network language model to re-score the 100 best translation candidates for each output sentence of our system. Being able to handle variable length context, it complements the n-gram based language model used within the pipeline. 2 System We built our baseline system with Moses (Koehn et al., 2007) as a phrase-based machine translation system loosely following the setup described by the WAT 2014 organizers (Nakazawa et al., 2014), with some modifications. We will quickly go through every step of our training. Introduction Modern models for statistical machine translation constitute a pipeline of different components. This pipeline usually involves a preprocessing part, a language model, a translation model and a postprocessing part. While this or a similar structure is basis of most systems and generally agreed upon, a lot of research has been focusing on modifying and extending the individual components. Many parts rely on probabilities acquired through word frequencies in fixed contexts, discarding additional syntac"
W14-7008,P14-1006,0,0.273852,"ral different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japanese/English language pair. As a first step in this direction, we integrate the model introduced by Hermann et al. (Hermann and Blunsom, 2014) into our baseline system, specifically, we use a slightly modified version of the “BI” model described in the paper. Figure 2: Illustration of model described in (Hermann and Blunsom, 2014), slight"
W14-7008,D10-1092,0,0.299884,"Missing"
W14-7008,C14-1112,0,0.0698528,"ill not be split when a predecessor chunk cx−1 is a noun. 2.1.1 Preordering Japanese Text We employ a preordering method for Japanese-toEnglish translation. We preorder the input text in the preprocessing stage to reduce the difference between the word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2"
W14-7008,W10-1736,0,0.112691,"nly marginally change the composed representation. This issue becomes particularly problematic for pairs of the form Japanese English トークン トークン トークン token token ! the token the the System averaged Kendall’s tau Baseline Baseline + Preordering 0.2990 0.3712 Table 2: Preordering Evaluation with Kendall’s tau. the neural model score can substitute the default Moses scores. 4.2 Preordering To separate the evaluation of our preordering method from the machine translation evaluation, we calculated an intrinsic quality measure specific to preordering. We apply the procedure previously introduced by (Isozaki et al., 2010b; Hoshino et al., 2014). In our baseline method we rely on MGiza++ to align Japanese and English sub-phrases. Without preordering MGiza++ will perform a lot of nonmonotonic alignments. The goal of preordering is to reduce the number of these non-monotonic alignments, in the best case leading to exclusively monotonic alignments. Utilizing Kendall’s tau we can compare the alignments resulting from input with and without preordering. The closer this coefficient is to 1.0, the more monotonic are the alignments and the higher is the intrinsic benefit of the preordering. Table 2 lists the averaged"
W14-7008,P14-1011,0,0.136009,"ditional similarity score computed by the neural model in the same way as described above for the feature function case. This does not require extending our baseline system, the system only needs to incorporate one more feature when reading the phrase table. 2.1.3 Reranking with the Recurrent Neural Network Language Model Neural Networks have been applied successfully in language modeling tasks over the past years (Mikolov et al., 2011a) and are increasingly often found in combination with or as an extension to statistical machine translation systems (Devlin et 58 al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014). We employ the Recurrent Neural Network Language Model Toolkit (RNNLM) (Mikolov et al., 2011b) to rerank the 100 best translation candidates for each sentence and use the best candidates for our submission. 3 Data Embeddings BLEU RIBES No No No None (Baseline) Feature Function Phrase Table (full) 19.16 18.95 18.82 63.41 63.48 63.23 Yes Yes Yes Feature Function None (Baseline) Phrase Table (full) 18.92 18.55 - 61.76 61.44 - No Phrase Table (1 col) 14.53 60.59 Table 1: Scores for different ways to include the Compositional Distributed Semantics model, with and without preorde"
W14-7008,W14-3335,0,0.0136998,"difference between the word order of the Japanese input sentence and the word order of the English target sentence. Parts of a Japanese sentence can be scrambled/shuffled without changing the meaning of the sentence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP"
W14-7008,D13-1141,0,0.0322374,"ntics Many machine learning algorithms require fixed length vectors as input. Of the various different ways to map words to vectors, neural network based models have proven very effective recently. Vector representations (embeddings) created by these models have been utilized as features to achieve state-of-the-art results in several different Natural Language Processing tasks (Collobert et al., 2011; Mikolov et al., 2013; Baroni et al., 2014), giving rise to many new variants. Methods have been developed that are capable of embedding words from different languages into the same vector space (Zou et al., 2013), there are models that can induce representations of whole phrases or sentences instead of only single tokens (Socher et al., 2010; Le and Mikolov, 2014; Blacoe and Lapata, 2012) and there are models implementing a mixture of these two ideas, embedding phrases from different languages into the same vector space (Hermann and Blunsom, 2014; Cho et al., 2014). Work has been published about integrating embeddings and neural network models into the statistical machine translation pipeline for various language combinations, however, we are not aware of any previous work attempting this for the Japa"
W14-7008,P07-2045,0,0.00420608,"apply preordering to the input text as a way of compensating for syntactic differences between English and Japanese. • We insert scores into the translation model of our baseline system that are computed from semantically meaningful distributed vector representations. • As postprocessing, we utilize a recurrent neural network language model to re-score the 100 best translation candidates for each output sentence of our system. Being able to handle variable length context, it complements the n-gram based language model used within the pipeline. 2 System We built our baseline system with Moses (Koehn et al., 2007) as a phrase-based machine translation system loosely following the setup described by the WAT 2014 organizers (Nakazawa et al., 2014), with some modifications. We will quickly go through every step of our training. Introduction Modern models for statistical machine translation constitute a pipeline of different components. This pipeline usually involves a preprocessing part, a language model, a translation model and a postprocessing part. While this or a similar structure is basis of most systems and generally agreed upon, a lot of research has been focusing on modifying and extending the ind"
W14-7008,P14-2091,0,0.0145239,"entence or making it grammatically incorrect. Therefore, one English sentence can potentially correspond to several shuffled variations of the same Japanese sentence. Yoshida et al. (2014) show that normalizing the word order of Japanese sentences can benefit readability. Taking scrambling into account not only increases readability, it also plays an important role in machine translation (Isozaki et al., 2014). Hayashi et al. (2013) were able to improve the results of statistical machine translation systems by generating English determiners in the Japanese input text and reordering its words. Kudo et al. (2014) applied preordering and generated zerosubjects to achieve state-of-the-art results on a web-text corpus. We employ the preordering rules introduced in (Hoshino et al., 2014) to reduce order ambiguity in the Japanese input text. This method has achieved state-of-the-art results on the NTCIR patent corpus. Following this method, we parse input sentences with the Japanese dependency parser KNP to obtain chunked1 dependency and coordination labels corresponding to the dependency. After that, we Rule 1-B Even if Rule 1-A is applied to a chunk cx , the chunk will be split into three new chunks (wor"
W14-7008,P14-1140,0,0.160292,"score computed by the neural model in the same way as described above for the feature function case. This does not require extending our baseline system, the system only needs to incorporate one more feature when reading the phrase table. 2.1.3 Reranking with the Recurrent Neural Network Language Model Neural Networks have been applied successfully in language modeling tasks over the past years (Mikolov et al., 2011a) and are increasingly often found in combination with or as an extension to statistical machine translation systems (Devlin et 58 al., 2014; Cho et al., 2014; Zhang et al., 2014; Liu et al., 2014). We employ the Recurrent Neural Network Language Model Toolkit (RNNLM) (Mikolov et al., 2011b) to rerank the 100 best translation candidates for each sentence and use the best candidates for our submission. 3 Data Embeddings BLEU RIBES No No No None (Baseline) Feature Function Phrase Table (full) 19.16 18.95 18.82 63.41 63.48 63.23 Yes Yes Yes Feature Function None (Baseline) Phrase Table (full) 18.92 18.55 - 61.76 61.44 - No Phrase Table (1 col) 14.53 60.59 Table 1: Scores for different ways to include the Compositional Distributed Semantics model, with and without preordering. In the Featur"
W15-3602,P14-1119,0,0.0261504,"Missing"
W15-3602,S10-1004,0,0.0906334,"eyphrases for the document and consequently that technical term extraction methods might also be useful in keyphrase extraction. We will show that features that capture the neology of term candidates can be used to extract technical terms, and that the basic assumptions that enable this extraction also hold true for keyphrases. This paper is organized as follows: We first discuss how technical terms and keyphrases differ from general language terms in terms of neology. We then define features that capture this difference and analyze these features statistically using the SemEval-2010 dataset (Kim et al., 2010) and a gold-standard for technical term extraction derived from the same dataset (Chaimongkol and Aizawa, 2013). Our analysis shows that the proposed features reliably separate positive from negative examples, both of technical terms and of keyphrases. Furthermore, the histograms for the proposed features are very similar when calculated for technical terms and keyphrases, suggesting that technical terms and keyphrases have very similar neological properties. Finally, we demonstrate that this statistical bias can be used to reliably extract technical terms in a gold-standard dataset, and that"
W15-3602,P12-3029,0,0.0155309,"and to select candidates that are thematically related to each other, Hulth et al. (2006) use a pregenerated domain ontology to select candidates whose synonyms, hypernyms, 3 Theoretical basis In this paper we will examine the use of external corpora in order to track the frequency of occurrence of n-grams over time, and use measures of neology as a way to extract technical terms. We are not aware of any previous attempts to use neology as a feature for technical term extraction, keyphrase extraction, or other kinds of natural language processing tasks. We will use the Google Ngrams dataset (Lin et al., 2012), where this information is already extracted. Although we use this dataset, mainly because of its convenience in our initial investigation, there is nothing keeping us from using other corpora consisting of raw documents, such as Pubmed. In particular, this 3 worm in the Google Ngrams dataset are likely to be of the biological variety, and it is consequently impossible to tell from the Google Ngrams dataset alone that the computer security term only appeared in the later half of the 20th century1 . Fortunately, the case where technical terms coincide with general language terms is rare, at le"
W15-3602,S10-1055,0,0.0434595,"Missing"
W15-3602,D09-1137,0,0.0306126,"Missing"
W15-3602,W11-0218,0,\N,Missing
W15-3602,W04-3252,0,\N,Missing
W15-3602,E99-1023,0,\N,Missing
W15-3602,W08-1404,0,\N,Missing
W15-3602,W03-1028,0,\N,Missing
W15-3602,W03-1805,0,\N,Missing
W15-3602,N09-1070,0,\N,Missing
W15-3602,S13-1035,0,\N,Missing
W16-6001,D13-1020,0,0.0529642,"ul and promising for decomposing and analyzing the RC process. Finally, we discuss ways to improve our approach based on the results of two extra annotations. 1 1. Define a set of prerequisite skills that are required for understanding documents (Section 2.1) 2. Annotate questions of an RC task with the skills (Section 2.2) 3. Analyze the performances of existing RC systems for the annotated questions to grasp the differences and limitations of their individual performances (Section 2.3) In Section 2, we present an example of our methodology, where we annotated MCTest (MC160 development set) (Richardson et al., 2013)1 for Step 2 and analyzed systems by Smith et al. (2015) for Step 3. In Section 3, we present two additional annotations in order to show the outlook for the development of our methodology in terms of the classification of skills and finer categories for each skill. In Section 4, we discuss our conclusions. Introduction Reading comprehension (RC) tasks require machines to understand passages and respond to questions about them. For the development of RC systems, precisely identifying what systems can and cannot understand is important. However, a critical problem is that the RC process is so c"
W16-6001,D15-1197,0,0.545085,". Finally, we discuss ways to improve our approach based on the results of two extra annotations. 1 1. Define a set of prerequisite skills that are required for understanding documents (Section 2.1) 2. Annotate questions of an RC task with the skills (Section 2.2) 3. Analyze the performances of existing RC systems for the annotated questions to grasp the differences and limitations of their individual performances (Section 2.3) In Section 2, we present an example of our methodology, where we annotated MCTest (MC160 development set) (Richardson et al., 2013)1 for Step 2 and analyzed systems by Smith et al. (2015) for Step 3. In Section 3, we present two additional annotations in order to show the outlook for the development of our methodology in terms of the classification of skills and finer categories for each skill. In Section 4, we discuss our conclusions. Introduction Reading comprehension (RC) tasks require machines to understand passages and respond to questions about them. For the development of RC systems, precisely identifying what systems can and cannot understand is important. However, a critical problem is that the RC process is so complicated that it is not easy to examine the performanc"
W16-6001,R11-1063,0,0.0765903,"Missing"
W16-6001,D16-1264,0,\N,Missing
W17-0417,W98-0501,0,0.0678086,"of linguistic theory, demonstrating the struggle by linguists to answer major questions such as what constitutes a construction, what are the differences between words and sentences, and what is the relation between meaning and form.” Ainu is one of the representative languages equipped with noun incorporation and polysynthesis (words are synthesized by many morphemes), and it has been used as an important tool to uncover the universal nature of human languages (Baker, Considering the recent revival of dependency grammar (Tesnière, 1959; Tesnière, 2015; Hays, 1960) by theoretical refinement (Järvinen and Tapanainen, 1998; Kuhlmann, 2013; Nivre, 2015) and success in statistical/neural dependency parsing (McDonald et al., 2005; Nivre et al., 2007; Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Universal Dependencies (UD) is a natural choice to syntactically annotate Ainu texts. One notable feature of modern dependency theory such as UD is its simplicity in the annotation scheme (Nivre, 2015). Because Ainu is a complex language the theoretical aspect of which is not yet fully understood, it is crucial to make its annotation as easy as possible. In addition, Tesnière (1959, Chapter 276) claimed that dep"
W17-0417,Q16-1032,0,0.0225941,"s, and what is the relation between meaning and form.” Ainu is one of the representative languages equipped with noun incorporation and polysynthesis (words are synthesized by many morphemes), and it has been used as an important tool to uncover the universal nature of human languages (Baker, Considering the recent revival of dependency grammar (Tesnière, 1959; Tesnière, 2015; Hays, 1960) by theoretical refinement (Järvinen and Tapanainen, 1998; Kuhlmann, 2013; Nivre, 2015) and success in statistical/neural dependency parsing (McDonald et al., 2005; Nivre et al., 2007; Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Universal Dependencies (UD) is a natural choice to syntactically annotate Ainu texts. One notable feature of modern dependency theory such as UD is its simplicity in the annotation scheme (Nivre, 2015). Because Ainu is a complex language the theoretical aspect of which is not yet fully understood, it is crucial to make its annotation as easy as possible. In addition, Tesnière (1959, Chapter 276) claimed that dependency diagrams are a useful tool for pedagogical purposes, e.g., deeper analysis of 133 Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies (UDW 2017), pages 133–139"
W17-0417,D14-1082,0,0.0557449,"tween words and sentences, and what is the relation between meaning and form.” Ainu is one of the representative languages equipped with noun incorporation and polysynthesis (words are synthesized by many morphemes), and it has been used as an important tool to uncover the universal nature of human languages (Baker, Considering the recent revival of dependency grammar (Tesnière, 1959; Tesnière, 2015; Hays, 1960) by theoretical refinement (Järvinen and Tapanainen, 1998; Kuhlmann, 2013; Nivre, 2015) and success in statistical/neural dependency parsing (McDonald et al., 2005; Nivre et al., 2007; Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Universal Dependencies (UD) is a natural choice to syntactically annotate Ainu texts. One notable feature of modern dependency theory such as UD is its simplicity in the annotation scheme (Nivre, 2015). Because Ainu is a complex language the theoretical aspect of which is not yet fully understood, it is crucial to make its annotation as easy as possible. In addition, Tesnière (1959, Chapter 276) claimed that dependency diagrams are a useful tool for pedagogical purposes, e.g., deeper analysis of 133 Proceedings of the NoDaLiDa 2017 Workshop on Universal Depen"
W17-0417,J13-2004,0,0.0276579,"ating the struggle by linguists to answer major questions such as what constitutes a construction, what are the differences between words and sentences, and what is the relation between meaning and form.” Ainu is one of the representative languages equipped with noun incorporation and polysynthesis (words are synthesized by many morphemes), and it has been used as an important tool to uncover the universal nature of human languages (Baker, Considering the recent revival of dependency grammar (Tesnière, 1959; Tesnière, 2015; Hays, 1960) by theoretical refinement (Järvinen and Tapanainen, 1998; Kuhlmann, 2013; Nivre, 2015) and success in statistical/neural dependency parsing (McDonald et al., 2005; Nivre et al., 2007; Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Universal Dependencies (UD) is a natural choice to syntactically annotate Ainu texts. One notable feature of modern dependency theory such as UD is its simplicity in the annotation scheme (Nivre, 2015). Because Ainu is a complex language the theoretical aspect of which is not yet fully understood, it is crucial to make its annotation as easy as possible. In addition, Tesnière (1959, Chapter 276) claimed that dependency diagrams"
W17-0417,de-marneffe-etal-2014-universal,0,0.0641837,"Missing"
W17-0417,H05-1066,0,0.115906,"a construction, what are the differences between words and sentences, and what is the relation between meaning and form.” Ainu is one of the representative languages equipped with noun incorporation and polysynthesis (words are synthesized by many morphemes), and it has been used as an important tool to uncover the universal nature of human languages (Baker, Considering the recent revival of dependency grammar (Tesnière, 1959; Tesnière, 2015; Hays, 1960) by theoretical refinement (Järvinen and Tapanainen, 1998; Kuhlmann, 2013; Nivre, 2015) and success in statistical/neural dependency parsing (McDonald et al., 2005; Nivre et al., 2007; Chen and Manning, 2014; Kiperwasser and Goldberg, 2016), Universal Dependencies (UD) is a natural choice to syntactically annotate Ainu texts. One notable feature of modern dependency theory such as UD is its simplicity in the annotation scheme (Nivre, 2015). Because Ainu is a complex language the theoretical aspect of which is not yet fully understood, it is crucial to make its annotation as easy as possible. In addition, Tesnière (1959, Chapter 276) claimed that dependency diagrams are a useful tool for pedagogical purposes, e.g., deeper analysis of 133 Proceedings of t"
W17-0417,W16-1719,0,0.0130121,"cs. See Section 4 for the topic. 3.3 they are a part of pronominal clitics. For example, sap=as “go=I” is tokenized as sap and =as rather than sap, =, and as. This style is consistent with Tamura (1996) and the glossary of FRPAC (2011) in which pronominal clitics are affixed with the delimiters in their entries. 5 Relative Clauses as Adjectival Expressions In light of syntactic categories (or parts-ofspeech), the most noticeable feature of Ainu is a lack of adjectives. Instead, the language employs relative clause constructions to obtain the same effect. This phenomenon is similar to Arapaho (Wagner et al., 2016). For example, since Ainu is SOV in most cases, Dictionary and referential textbook nsubj For dictionary and reference, we mainly referred to Tamura (1996) and Tamura (2000) (English translation of Tamura (1988)), respectively. Strictly speaking, Tamura (1996) deals with the Saru dialect while our text is in the Horobetsu dialect, but two dialects are so close that they have few differences in vocabulary and no grammatical differences. We also used other dictionaries, e.g., Nakagawa (1995), Kayano (2002), and Bugaeva (2011b). For reference, we also consulted Refsing (1986), Shibatani (1990), a"
W17-0417,P15-2111,0,0.0231189,"=Pluract. However, this phenomonenon clearly deviates from number, which basically serves as an agreement system between nouns, often purely syntactically, rather than semantically. In the literature of typology there seems to be a difference of opinion as to whether pluractionality is related to aspect or it is an independent feature. We will make further research on this point, and we may adopt either Aspect=Pluract or Pluract=Yes in the future. 7.3 To annotate this phenomenon, we used a language-specific feature Valency=0, Valency=1, etc. Possibly we may borrow the terminology of UniMorph (Sylak-Glassman et al., 2015) features, that is, Valency: DITR, IMPRS, INTR, TR in the future. 7.4 Inalienable possession The only inflectional system of Ainu nouns is by inalienable possession, with the form similar to ezāfe in Persian (Bugaeva, 2011a, p. 520). Only a handful of nouns are classified as being inalienable, e.g., body parts (including names) and family members. They are inflected if being possessed by someone. For instance, re “name” is inflected to réhe; and with 1sg pronominal clitic ku=, we obtain ku=réhe “my name”. Likewise, a class of nouns called locative nouns has possessed forms. For example, a loca"
Y11-1063,Y07-1048,1,0.658356,"ire document set regarding several perspectives determined by keyword lists. It can analyze stylistic categories as well as contentdependent ones; our index uses content words (keyword lists); that means it can be used for analyzing content-dependent categories. Furthermore, it is based on the rank-type information, instead of the token-number information; that means it can be used for analyzing genre-based categories, because the rank-type information is usually considered to represent styles (Tweedie and Baayen, 1998). Our index captures both stylistic and content-dependent characteristics (Suzuki and Kageura, 2007) based on this information. In addition, it can use different keyword lists and takes into consideration the relative position of a document in a document set; and it also uses the ranks of the types, i.e., the ranked frequencies of different words (keywords) appearing in a document. We assessed the effectiveness of our index in analyzing the different characteristics of four genres and three subfields in Japanese academic papers. The rest of this paper is organized as follows. We first explain the original h-index and its characteristics in Section 2. In Section 3, we explain the proposed met"
Y15-1056,D11-1052,0,0.388792,"oments that are not numerical spikes. For example, Lanagan and Smeaton (2011) reported that emotional upsurge overlaps with numerical spikes and those are useful for tagging key moments in sports matches. However, detecting numerical spikes on Twitter becomes difficult when a target event is not pre-defined or rarely tweeted by Twitter users because the number event-related tweets per unit time is not directly computable. In such cases, detecting upsurge of emotions becomes crucial. One characteristics of tweets is that expressions used in tweets entail many linguistic phenomena. For example, Brody and Diakopoulos (2011) analyzed occurrences of character repetitions in words from a sentiment dictionary. In this paper, we assume that such variations of language expressions are caused by real-world events. Table 1 shows that a character repetition (‘Goooal’, ‘Huraaay’) occurs in tweets during emotional upsurge rather than their canonical form (‘Goal’, ‘Hurray’). In contrast, a character repetition does not frequently occur in tweets during non-emotional upsurge. However, to our knowledge, there has not been an attempt to capture emotional upsurge using the linguistic characteristics of tweets. In this paper, we"
Y15-1056,J92-4003,0,0.341931,"ting emotional upsurge tend to get better. This is obvious for Supervised LM performing well on #momoclo hashtag because when testing on this hashtag, Supervised LM is built from the rest of five target hashtags including #agar and the suffix of the emotionally upsurging tweets from #momoclo are similar to that of #agqr. Specifically, those tweets include lots of “w”s which 6 Discussion We further investigate the impact of the tweet set size on the reliability of the perplexity estimation using language models. Perplexity is known to be affected by the amount of text used for the calculation (Brown et al., 1992). We analyzed the impact using the most tweeted minute in the hashtag #aibou. Figure 4 shows the transition of perplexity according to the number tweets used to calculate the perplexity in the hashtag #aibou. As a result, after the number of tweets from the same minute exceeds 11, the difference between the minimum and the maximum perplexity became less than 3. This result shows that the perplexity does not largely rely on the number of tweets from the same timestamp and implies that 6 Therefore, both language models score 1.0 on #agqr. 494 60 40 perplexity 20 is an Internet slang meaning “lol"
Y15-1056,P12-1056,0,0.0277949,"s significantly higher than non-retweeted tweets. DanescuNiculescu-Mizil et al. (2013) reported that users’ career in an online community correlates with the cross entropy between each user’s posts and the language used in the whole community. Lin et al. (2011) used multiple language models built from each hashtag to track broad topics. These researches show that language model is a powerful method to use on various applications. To our knowledge, there is no prior research focused on languages used in emotional spiking tweets. Many tasks on Twitter including burst detection (Kleinberg, 2003; Diao et al., 2012), first story detection (Petrovi´c et al., 2010), and topic tracking (Lau et al., 2012) failed to effectively incorporate the textual characteristics of tweets and regard it is out of their scope. Being able to characterize tweets from emotional upsurge would open a window to the identification of real-world events that emotionally influence Twitter users. 3 Language Model-based Detection of Emotional Upsurge 3.1 Outline of the Proposed Method The motivation of using characteristic expressions used in tweets to detect emotional upsurge is there are various ways to express users’ feelings. In t"
Y15-1056,N13-1037,0,0.0128243,"us489 ing characteristic expressions in a Japanese tweet. Our contribution is that a spiking tweet language model, which we constructed automatically from existing tweet dataset, captures characteristic expressions well and it is an effective approach for detecting emotional upsurge. 2 Related Work Our idea is related to many previous works on Twitter including the investigation toward non-standard languages used on Twitter, and various applications tackled using language models. The nature of using non-standard languages including word lengthening in tweets largely differ from other corpus (Eisenstein, 2013). As further mentioned by Eisenstein (2013), these languages are affected by many factors including the 140 characters length limit of tweets, social factors (e.g. age (Rosenthal and McKeown, 2011)), location (Wing and Baldridge, 2011), input devices (Gouws et al., 2011) of an author of a tweet. Word lengthening is known to be useful for sentiment analysis (Brody and Diakopoulos, 2011). One way to model these expressions is to use language models and many studies successfully captured various characteristics of tweets using language model. There are lots of applications for language models bui"
Y15-1056,W11-0704,0,0.0217859,"otional upsurge. 2 Related Work Our idea is related to many previous works on Twitter including the investigation toward non-standard languages used on Twitter, and various applications tackled using language models. The nature of using non-standard languages including word lengthening in tweets largely differ from other corpus (Eisenstein, 2013). As further mentioned by Eisenstein (2013), these languages are affected by many factors including the 140 characters length limit of tweets, social factors (e.g. age (Rosenthal and McKeown, 2011)), location (Wing and Baldridge, 2011), input devices (Gouws et al., 2011) of an author of a tweet. Word lengthening is known to be useful for sentiment analysis (Brody and Diakopoulos, 2011). One way to model these expressions is to use language models and many studies successfully captured various characteristics of tweets using language model. There are lots of applications for language models built from tweets or web texts. According to Liu et al. (2012), distant-supervised language modPACLIC 29 els are useful for sentiment analysis of tweets. Neubig and Duh (2013) showed that for 26 languages used on Twitter, entropy of content in a retweet, the Twitter version"
Y15-1056,D14-1011,0,0.0190938,"model is calculated by the following formula: P (t) = l ∏ P (ti |ti−1 , ..., ti−n+1 ). (1) i=1 We use SRILM (Stolcke, 2002) with Katz back-off smoothing (Katz, 1987) to build language models. We build a character n-gram language model following Neubig and Duh (2013). To build a word n-gram language model, word segmentation is necessary to build a word n-gram language model since Japanese is an unsegmented language. However, various studies reported that tokenization in unsegmented languages on Twitter is not reliable enough due to the spelling variations and unknown words (Wang and Kan, 2013; Kaji and Kitsuregawa, 2014). We set the value of n for a character n-gram language model to 7. This is because when we consider n-grams with n &gt; 5, the number of n-grams decreases which shows that the language model suffers from the sparsity problem. However, as reported by Brody and Diakopoulos (2011), word lengthening (e.g. coool) is a common phenomenon on Twitter. To accurately capture those phenomenon, we tried to use as long n-gram as possible and make it to 7-gram. 3.3 Perplexity To quantify the difference between tweets during emotional upsurge and non-emotional upsurge, we used perplexity, a measurement of infor"
Y15-1056,C12-1093,0,0.0400072,"Missing"
Y15-1056,N10-1021,0,0.123074,"Missing"
Y15-1056,P11-1077,0,0.0126024,"es characteristic expressions well and it is an effective approach for detecting emotional upsurge. 2 Related Work Our idea is related to many previous works on Twitter including the investigation toward non-standard languages used on Twitter, and various applications tackled using language models. The nature of using non-standard languages including word lengthening in tweets largely differ from other corpus (Eisenstein, 2013). As further mentioned by Eisenstein (2013), these languages are affected by many factors including the 140 characters length limit of tweets, social factors (e.g. age (Rosenthal and McKeown, 2011)), location (Wing and Baldridge, 2011), input devices (Gouws et al., 2011) of an author of a tweet. Word lengthening is known to be useful for sentiment analysis (Brody and Diakopoulos, 2011). One way to model these expressions is to use language models and many studies successfully captured various characteristics of tweets using language model. There are lots of applications for language models built from tweets or web texts. According to Liu et al. (2012), distant-supervised language modPACLIC 29 els are useful for sentiment analysis of tweets. Neubig and Duh (2013) showed that for 26 langu"
Y15-1056,P13-1072,0,0.0167412,"an n-gram language model is calculated by the following formula: P (t) = l ∏ P (ti |ti−1 , ..., ti−n+1 ). (1) i=1 We use SRILM (Stolcke, 2002) with Katz back-off smoothing (Katz, 1987) to build language models. We build a character n-gram language model following Neubig and Duh (2013). To build a word n-gram language model, word segmentation is necessary to build a word n-gram language model since Japanese is an unsegmented language. However, various studies reported that tokenization in unsegmented languages on Twitter is not reliable enough due to the spelling variations and unknown words (Wang and Kan, 2013; Kaji and Kitsuregawa, 2014). We set the value of n for a character n-gram language model to 7. This is because when we consider n-grams with n &gt; 5, the number of n-grams decreases which shows that the language model suffers from the sparsity problem. However, as reported by Brody and Diakopoulos (2011), word lengthening (e.g. coool) is a common phenomenon on Twitter. To accurately capture those phenomenon, we tried to use as long n-gram as possible and make it to 7-gram. 3.3 Perplexity To quantify the difference between tweets during emotional upsurge and non-emotional upsurge, we used perpl"
Y15-1056,P11-1096,0,0.036435,"is an effective approach for detecting emotional upsurge. 2 Related Work Our idea is related to many previous works on Twitter including the investigation toward non-standard languages used on Twitter, and various applications tackled using language models. The nature of using non-standard languages including word lengthening in tweets largely differ from other corpus (Eisenstein, 2013). As further mentioned by Eisenstein (2013), these languages are affected by many factors including the 140 characters length limit of tweets, social factors (e.g. age (Rosenthal and McKeown, 2011)), location (Wing and Baldridge, 2011), input devices (Gouws et al., 2011) of an author of a tweet. Word lengthening is known to be useful for sentiment analysis (Brody and Diakopoulos, 2011). One way to model these expressions is to use language models and many studies successfully captured various characteristics of tweets using language model. There are lots of applications for language models built from tweets or web texts. According to Liu et al. (2012), distant-supervised language modPACLIC 29 els are useful for sentiment analysis of tweets. Neubig and Duh (2013) showed that for 26 languages used on Twitter, entropy of conte"
