2005.mtsummit-osmtw.2,W04-0813,0,0.0554133,"Missing"
2005.mtsummit-osmtw.2,2001.mtsummit-papers.14,0,0.286088,"our group (Díaz de Ilarraza et al., 2000) but is now integrated in the OpenTrad initiative, a larger government-funded project shared 7 1. An open source shallow-transfer machine translation engine for the Romance languages of Spain (the main ones being Spanish, Catalan and Galician). The MT architecture proposed uses finite-state transducers for lexical processing, hidden Markov models for part-of-speech tagging, and finite-state based chunking for structural transfer, and is largely based upon that of systems already developed by the Transducens group such as InterNOSTRUM (Spanish-Catalan, Canals-Marote et al., 2001) and Traductor Universia (SpanishPortuguese, Garrido-Alenda, 2003). 2. A deeper-transfer engine for the Spanish— Basque pair, which will be described in this paper. Some of the components (modules, data formats and compilers) from the first architecture will also be useful for the second. Indeed, an important additional goal of this work is testing which modules from the first architecture can be integrated in deeper-transfer architectures for more difficult language pairs. We expect that the introduction of an open source MT architecture will help finding solutions for well known problems in"
2005.mtsummit-osmtw.2,carreras-etal-2004-freeling,1,0.827581,"m Spanish to Basque and generation of the Basque output. It is based on the previous work of our group (Díaz de Ilarraza et al., 2000) but with new features and a new aim: interoperability with other linguistic resources and convergence with the other engines in the OpenTrad project through the use of XML . The previous objectoriented architecture is being rewritten into a open source one which will use modules which are shared with other engines in the OpenTrad project and will comply with its format specifications. The main modules are five: de-formatter, Spanish analysis based on FreeLing (Carreras et al., 2004), Spanish-Basque transfer, Basque generation and re-formatter. De-Formatter --&gt; Spanish | Analyzer | | | Transfer | | De-Formatter <-- Generation The following sections describe each module. The transfer and generation phases work in three levels: lexical form (tagged as node), chunk and sentence. No semantic disambiguation is applied, but a large number of multi-word units representing collocations, named-entities and complex terms are being included in the bilingual dictionary in order to minimize this limitation. 2.1. The de-formatter The de-formatter separates the text to be translated fro"
2005.mtsummit-osmtw.2,2005.eamt-1.12,1,0.803665,"Missing"
A97-1013,W95-0101,0,0.0834397,"neural nets (Schmid, 1994). The system can use linguistic rules and corpusbased statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syntactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): Only one day was spent on writing the 107 syntactic disambiguation rules used by the linguistic parser. Most hybrid approaches combine statistical information with automatically extracted rule-based information (Brill, 1995; Daelemans et al., 1996). Relatively little attention has been paid to models where the statistical approach is combined with a truly linguistic model (i.e. one generated by a linguist). This paper reports one such approach: syntactic rules written by a linguist are combined with statistical information using the relaxation labelling algorithm. No human annotators were needed for annotating the training corpus (218,000 words of journalese) used by the data-driven learning modules of this system: the training corpus was annotated by (i) tagging it with the EngCG morphological tagger, (ii) maki"
A97-1013,C94-1027,0,0.0610792,"rpus. Also the contributions of the linguistic and statistical language models to the hybrid model are estimated. 1 Introduction The language models used by natural language analyzers are traditionally based on two approaches. In the linguistic approach, the model is based on hand-crafted rules derived from the linguist&apos;s general and/or corpus-based knowledge about the object language. In the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (Garside et al., 1987), local rules (Hindle, 1989) or neural nets (Schmid, 1994). The system can use linguistic rules and corpusbased statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syntactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): Only one day was spent on writing the 107 syntactic disambiguation rules used by the linguistic parser. Most hybrid approaches combine statistical information with automatically extracted rule-based information (Brill, 1995; Daelemans et"
A97-1013,E95-1021,0,0.0164593,"etter than the other two. • The small difference between the hybrid models suggest that some reasonable statistics provide enough disambiguation, and that not very sophisticated information is needed. 7This obviously depends on the flexibility of one&apos;s requirements. 86 Finally, a reservation must be made: what we have not investigated in this paper is how much of the extra work done with the statistical module could have been done equally well or even better by spending e.g. another day writing a further collection of heuristic rules. As suggested e.g. by Tapanainen and Voutilainen (1994) and Chanod and Tapanainen (1995), hand-coded heuristics may be a worthwhile addition to &apos;strictly&apos; grammar-based rules. Acknowledgements We wish to thank T i m o J/irvinen, Pasi Tapanalnen and two ANLP&apos;97 referees for useful comments on earlier versions of this paper. The first author benefited from the collaboration of Juha Heikkil~ in the development of the linguistic description used by the EngCG morphological tagger; the two-level compiler for morphological anMysis in EngCG was written by K i m m o Koskenniemi; the recent version of the Constraint G r a m m a r parser (CG-2) was written by Pasi Tapanainen. The Constraint"
A97-1013,A94-1008,1,0.852537,". The B T C hybrid model is slightly better than the other two. • The small difference between the hybrid models suggest that some reasonable statistics provide enough disambiguation, and that not very sophisticated information is needed. 7This obviously depends on the flexibility of one&apos;s requirements. 86 Finally, a reservation must be made: what we have not investigated in this paper is how much of the extra work done with the statistical module could have been done equally well or even better by spending e.g. another day writing a further collection of heuristic rules. As suggested e.g. by Tapanainen and Voutilainen (1994) and Chanod and Tapanainen (1995), hand-coded heuristics may be a worthwhile addition to &apos;strictly&apos; grammar-based rules. Acknowledgements We wish to thank T i m o J/irvinen, Pasi Tapanalnen and two ANLP&apos;97 referees for useful comments on earlier versions of this paper. The first author benefited from the collaboration of Juha Heikkil~ in the development of the linguistic description used by the EngCG morphological tagger; the two-level compiler for morphological anMysis in EngCG was written by K i m m o Koskenniemi; the recent version of the Constraint G r a m m a r parser (CG-2) was written b"
A97-1013,W96-0102,0,0.127332,"Schmid, 1994). The system can use linguistic rules and corpusbased statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syntactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): Only one day was spent on writing the 107 syntactic disambiguation rules used by the linguistic parser. Most hybrid approaches combine statistical information with automatically extracted rule-based information (Brill, 1995; Daelemans et al., 1996). Relatively little attention has been paid to models where the statistical approach is combined with a truly linguistic model (i.e. one generated by a linguist). This paper reports one such approach: syntactic rules written by a linguist are combined with statistical information using the relaxation labelling algorithm. No human annotators were needed for annotating the training corpus (218,000 words of journalese) used by the data-driven learning modules of this system: the training corpus was annotated by (i) tagging it with the EngCG morphological tagger, (ii) making the tagged text 80 the"
A97-1013,P89-1015,0,0.120061,"gainst a held-out benchmark corpus. Also the contributions of the linguistic and statistical language models to the hybrid model are estimated. 1 Introduction The language models used by natural language analyzers are traditionally based on two approaches. In the linguistic approach, the model is based on hand-crafted rules derived from the linguist&apos;s general and/or corpus-based knowledge about the object language. In the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (Garside et al., 1987), local rules (Hindle, 1989) or neural nets (Schmid, 1994). The system can use linguistic rules and corpusbased statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syntactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): Only one day was spent on writing the 107 syntactic disambiguation rules used by the linguistic parser. Most hybrid approaches combine statistical information with automatically extracted rule-based informati"
A97-1013,E95-1029,1,0.841224,"Missing"
A97-1013,C90-3030,0,0.0190027,"-crafted rules derived from the linguist&apos;s general and/or corpus-based knowledge about the object language. In the data-driven approach, the model is automatically generated from annotated text corpora, and the model can be represented e.g. as n-grams (Garside et al., 1987), local rules (Hindle, 1989) or neural nets (Schmid, 1994). The system can use linguistic rules and corpusbased statistics. Notable about the system is that minimal human effort was needed for creating its language models (the linguistic consisting of syntactic disambiguation rules based on the Constraint Grammar framework (Karlsson, 1990; Karlsson et al., 1995); the corpus-based consisting of bigrams and trigrams): Only one day was spent on writing the 107 syntactic disambiguation rules used by the linguistic parser. Most hybrid approaches combine statistical information with automatically extracted rule-based information (Brill, 1995; Daelemans et al., 1996). Relatively little attention has been paid to models where the statistical approach is combined with a truly linguistic model (i.e. one generated by a linguist). This paper reports one such approach: syntactic rules written by a linguist are combined with statistical inf"
A97-1013,C92-2070,0,0.0331663,"We tested linguistic, statistical and hybrid language models, using the CG-2 parser (Tapanainen, 1996) and the relaxation labelling algorithm described in Section 2. benchmark The statistical models were obtained from a training corpus of 218,000 words of journalese, syntactically annotated using the linguistic parser (see above). Although the linguistic CG-2 parser does not disambiguate completely, it seems to have an almost perfect recall (cf. Table 1 below), and the noise introduced by the remaining ambiguity is assumed to be sufficiently lower than the signal, following the idea used in (Yarowsky, 1992). The collected statistics were bigram and trigram For evaluating the systems, five roughly equal-sized benchmark corpora not used in the development of our parsers and taggers were prepared. The texts, totaling 6,500 words, were copied from the Gutenberg e-text archive, and they represent present-day American English. One text is from an article about AIDS; another concerns brainwashing techniques; the third describes guerilla warfare tactics; the fourth addresses the assassination of J. F. Kennedy; the last is an extract from a speech by Noam Chomsky. The texts were first analysed by a recen"
A97-1013,C96-2148,1,0.857645,"Missing"
alegria-etal-2014-tweetnorm,N13-1037,0,\N,Missing
alegria-etal-2014-tweetnorm,P11-1038,0,\N,Missing
alegria-etal-2014-tweetnorm,padro-stanilovsky-2012-freeling,1,\N,Missing
alonso-etal-2004-multiple,W02-1022,0,\N,Missing
alonso-etal-2004-multiple,N03-1003,0,\N,Missing
C10-2125,P95-1017,0,0.185643,"Research supported by the Spanish Science and Innovation Ministry, via the KNOW2 project (TIN200914715-C04-04) and from the European Community’s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement number 247762 (FAUST) Regarding the classification step, pioneer systems developed were based on pairwise classifiers. Given a pair of mentions, the process generates a feature vector and feeds it to a classifier. The resolution is done by considering each mention of the document as anaphor1 and looking backward until the antecedent is found or the beginning of the document is reached (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). A first approach towards groupwise classifiers is the twin-candidate model (Yang et al., 2003). The model faces the problem as a competition between two candidates to be the antecedent of the anaphor into account. Each candidate mention is compared with all the others in a round robin contest. Following the groupwise approach, rankers consider all the possible antecedent mentions at once (Denis and Baldridge, 2008). Rankers can obtain more accurate results due to a more informed context where all candidate mentions are considered at the same ti"
C10-2125,atserias-etal-2006-freeling,0,0.0842495,"Missing"
C10-2125,N07-1011,0,0.100217,"s may introduce. Although chain formation processes search for global consistency, the lack of contextual information in the classification step is propagated forward. Few works try to overcome the limitations of keeping classification and chain formation apart. Luo et al. (2004) search the most probable path comparing each mention with the partialentities formed so far using a Bell tree structure. McCallum and Wellner (2005) propose a graph partitioning cutting by distances, with the peculiarity that distances are learned considering coreferential chains of the labeled data instead of pairs. Culotta et al. (2007) combine a groupwise classifier with a clustering process in a First-Order probabilistic model. The approach presented in this paper follows the same research line of joining group classification and chain formation in the same step. Concretely, we propose a graph representation of the problem solved by a relaxation labeling process, reducing coreference resolution to a graph partitioning problem given a set of constraints. In this manner, decisions are taken considering the whole set of mentions, ensuring consistency and avoiding that classification decisions are independently taken. Our expe"
C10-2125,N07-1030,0,0.244747,"med after classification. Many systems form the chains by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking an anaphor only to its antecedent with maximum confidence value (Ng and Cardie, 2002). Some works propose more elaborated methods than single-link for chain formation. The approaches used are Integer Linear Programming 1 Typically a pair of coreferential mentions mi and mj (i &lt; j) are called antecedent and anaphor respectively, though mj may not be anaphoric. 1086 Coling 2010: Poster Volume, pages 1086–1094, Beijing, August 2010 (ILP) (Denis and Baldridge, 2007; Klenner and Ailloud, 2009; Finkel and Manning, 2008), graph partitioning (Nicolae and Nicolae, 2006), and clustering (Klenner and Ailloud, 2008). The main advantage of these types of post-processes is the enforcement of transitivity sorting out the contradictions that the previous classification process may introduce. Although chain formation processes search for global consistency, the lack of contextual information in the classification step is propagated forward. Few works try to overcome the limitations of keeping classification and chain formation apart. Luo et al. (2004) search the mos"
C10-2125,D08-1069,0,0.233603,"ach mention of the document as anaphor1 and looking backward until the antecedent is found or the beginning of the document is reached (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). A first approach towards groupwise classifiers is the twin-candidate model (Yang et al., 2003). The model faces the problem as a competition between two candidates to be the antecedent of the anaphor into account. Each candidate mention is compared with all the others in a round robin contest. Following the groupwise approach, rankers consider all the possible antecedent mentions at once (Denis and Baldridge, 2008). Rankers can obtain more accurate results due to a more informed context where all candidate mentions are considered at the same time. Coreference chains are formed after classification. Many systems form the chains by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking an anaphor only to its antecedent with maximum confidence value (Ng and Cardie, 2002). Some works propose more elaborated methods than single-link for chain formation. The approaches used are Integer Linear Programming 1 Typically a pair of coreferential mentions mi and mj (i &lt;"
C10-2125,P08-2012,0,0.255516,"by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking an anaphor only to its antecedent with maximum confidence value (Ng and Cardie, 2002). Some works propose more elaborated methods than single-link for chain formation. The approaches used are Integer Linear Programming 1 Typically a pair of coreferential mentions mi and mj (i &lt; j) are called antecedent and anaphor respectively, though mj may not be anaphoric. 1086 Coling 2010: Poster Volume, pages 1086–1094, Beijing, August 2010 (ILP) (Denis and Baldridge, 2007; Klenner and Ailloud, 2009; Finkel and Manning, 2008), graph partitioning (Nicolae and Nicolae, 2006), and clustering (Klenner and Ailloud, 2008). The main advantage of these types of post-processes is the enforcement of transitivity sorting out the contradictions that the previous classification process may introduce. Although chain formation processes search for global consistency, the lack of contextual information in the classification step is propagated forward. Few works try to overcome the limitations of keeping classification and chain formation apart. Luo et al. (2004) search the most probable path comparing each mention with the partia"
C10-2125,gimenez-marquez-2004-svmtool,0,0.0132611,"s at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. MUC scorer (Vilain et al., 1995) is not used in our experiments. Although it has been widely used in the state of the art, we consider the newer metrics have overcome some MUC limitations (Bagga and Baldwin, 1998; Luo, 2005; Klenner and Ailloud, 2008; Denis and Baldridge, 2008). Our preprocessing pipeline consists of FreeLing (Atserias et al., 2006) for sentence splitting and tokenization, SVMTool (Gimenez and Marquez, 2004) for part of speech tagging and BIO (Surdeanu et al., 2005) for named entity recognition and classification. No lemmatization neither syntactic analysis are used. 4.1 4.1.1 We evaluate our approach to coreference resolution using ACE-phase02 corpus, which is composed of three sections: Broadcast News (BNEWS), Newswire (NWIRE) and Newspaper (NPAPER). Each section is in turn composed of a training set and a test set. Figure 4 shows some statistics about this corpus. In our experiments, we consider the true mentions of ACE. This is because our focus is on evaluating pairwise approach versus the g"
C10-2125,E09-1051,0,0.0137605,"any systems form the chains by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking an anaphor only to its antecedent with maximum confidence value (Ng and Cardie, 2002). Some works propose more elaborated methods than single-link for chain formation. The approaches used are Integer Linear Programming 1 Typically a pair of coreferential mentions mi and mj (i &lt; j) are called antecedent and anaphor respectively, though mj may not be anaphoric. 1086 Coling 2010: Poster Volume, pages 1086–1094, Beijing, August 2010 (ILP) (Denis and Baldridge, 2007; Klenner and Ailloud, 2009; Finkel and Manning, 2008), graph partitioning (Nicolae and Nicolae, 2006), and clustering (Klenner and Ailloud, 2008). The main advantage of these types of post-processes is the enforcement of transitivity sorting out the contradictions that the previous classification process may introduce. Although chain formation processes search for global consistency, the lack of contextual information in the classification step is propagated forward. Few works try to overcome the limitations of keeping classification and chain formation apart. Luo et al. (2004) search the most probable path comparing e"
C10-2125,P04-1018,0,0.0832459,"(ILP) (Denis and Baldridge, 2007; Klenner and Ailloud, 2009; Finkel and Manning, 2008), graph partitioning (Nicolae and Nicolae, 2006), and clustering (Klenner and Ailloud, 2008). The main advantage of these types of post-processes is the enforcement of transitivity sorting out the contradictions that the previous classification process may introduce. Although chain formation processes search for global consistency, the lack of contextual information in the classification step is propagated forward. Few works try to overcome the limitations of keeping classification and chain formation apart. Luo et al. (2004) search the most probable path comparing each mention with the partialentities formed so far using a Bell tree structure. McCallum and Wellner (2005) propose a graph partitioning cutting by distances, with the peculiarity that distances are learned considering coreferential chains of the labeled data instead of pairs. Culotta et al. (2007) combine a groupwise classifier with a clustering process in a First-Order probabilistic model. The approach presented in this paper follows the same research line of joining group classification and chain formation in the same step. Concretely, we propose a"
C10-2125,H05-1004,0,0.239183,"document is taken into account by the constraints regardless of the graph representation. Our experiments confirm (Section 4) that placing first named entity mentions, then nominal mentions and finally the pronouns, the precision increases considerably. Inside of each of these groups, the order is the same order of the document. 4 Experiments and Results use true mentions. Moreover, details on mention identifier systems and their performances are rarely published by the systems based on automatic identification of mentions and it difficults the comparison. To evaluate our system we use CEAF (Luo, 2005) and B 3 (Bagga and Baldwin, 1998). CEAF is computed based on the best one-to-one map between key coreference chains and response ones. We use the mention-based similarity metric which counts the number of common mentions shared by key coreference chains and response ones. As we are using true mentions for the experiments, precision, recall and F1 are the same value and only F1 is shown. B 3 scorer is used for comparison reasons. B 3 algorithm looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the aver"
C10-2125,N07-1010,0,0.0587673,"value. The random initial state is also used in our experiments to test that our proposed configuration is better-informed than random. Given the equiprobability state, we add a random value to each probability to be in a partition: hil = L1i + il , ∀l = 0..Li −1 1 where il is a random value 2L ≤ il ≤ 2L . i i These little random differences may help the algorithm to avoid local minima. 3.4 Reordering The vertices of the graph would usually be placed in the same order as the mentions are found in the document (chronological). In this manner, vi corresponds to mi . However, as suggested by Luo (2007), there is no need to generate the model following that order. In our approach, the first variables have a lower number of possible labels. Moreover, an error in the first variables has more influence on the performance than an error in the later ones. Placing named entities at the beginning is reasonably to expect that is helpful for the algorithm, given that named entities are usually the most informative mentions. 1090 bnews train bnews test npaper train npaper test nwire train nwire test Tokens 66627 17463 68970 17404 70832 16772 Mentions 9937 2579 11283 2483 10693 2608 Entities 4408 1040"
C10-2125,P02-1014,0,0.700258,"account. Each candidate mention is compared with all the others in a round robin contest. Following the groupwise approach, rankers consider all the possible antecedent mentions at once (Denis and Baldridge, 2008). Rankers can obtain more accurate results due to a more informed context where all candidate mentions are considered at the same time. Coreference chains are formed after classification. Many systems form the chains by joining each positively-classified pair (i.e. singlelink) or with simple improvements such as linking an anaphor only to its antecedent with maximum confidence value (Ng and Cardie, 2002). Some works propose more elaborated methods than single-link for chain formation. The approaches used are Integer Linear Programming 1 Typically a pair of coreferential mentions mi and mj (i &lt; j) are called antecedent and anaphor respectively, though mj may not be anaphoric. 1086 Coling 2010: Poster Volume, pages 1086–1094, Beijing, August 2010 (ILP) (Denis and Baldridge, 2007; Klenner and Ailloud, 2009; Finkel and Manning, 2008), graph partitioning (Nicolae and Nicolae, 2006), and clustering (Klenner and Ailloud, 2008). The main advantage of these types of post-processes is the enforcement o"
C10-2125,J01-4004,0,0.981618,"tion Ministry, via the KNOW2 project (TIN200914715-C04-04) and from the European Community’s Seventh Framework Programme (FP7/2007-2013) under Grant Agreement number 247762 (FAUST) Regarding the classification step, pioneer systems developed were based on pairwise classifiers. Given a pair of mentions, the process generates a feature vector and feeds it to a classifier. The resolution is done by considering each mention of the document as anaphor1 and looking backward until the antecedent is found or the beginning of the document is reached (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). A first approach towards groupwise classifiers is the twin-candidate model (Yang et al., 2003). The model faces the problem as a competition between two candidates to be the antecedent of the anaphor into account. Each candidate mention is compared with all the others in a round robin contest. Following the groupwise approach, rankers consider all the possible antecedent mentions at once (Denis and Baldridge, 2008). Rankers can obtain more accurate results due to a more informed context where all candidate mentions are considered at the same time. Coreference chains are formed after classifi"
C10-2125,M95-1005,0,0.536484,"the best one-to-one map between key coreference chains and response ones. We use the mention-based similarity metric which counts the number of common mentions shared by key coreference chains and response ones. As we are using true mentions for the experiments, precision, recall and F1 are the same value and only F1 is shown. B 3 scorer is used for comparison reasons. B 3 algorithm looks at the presence/absence of mentions for each entity in the system output. Precision and recall numbers are computed for each mention, and the average gives the final precision and recall numbers. MUC scorer (Vilain et al., 1995) is not used in our experiments. Although it has been widely used in the state of the art, we consider the newer metrics have overcome some MUC limitations (Bagga and Baldwin, 1998; Luo, 2005; Klenner and Ailloud, 2008; Denis and Baldridge, 2008). Our preprocessing pipeline consists of FreeLing (Atserias et al., 2006) for sentence splitting and tokenization, SVMTool (Gimenez and Marquez, 2004) for part of speech tagging and BIO (Surdeanu et al., 2005) for named entity recognition and classification. No lemmatization neither syntactic analysis are used. 4.1 4.1.1 We evaluate our approach to cor"
C10-2125,P03-1023,0,0.233194,"enth Framework Programme (FP7/2007-2013) under Grant Agreement number 247762 (FAUST) Regarding the classification step, pioneer systems developed were based on pairwise classifiers. Given a pair of mentions, the process generates a feature vector and feeds it to a classifier. The resolution is done by considering each mention of the document as anaphor1 and looking backward until the antecedent is found or the beginning of the document is reached (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). A first approach towards groupwise classifiers is the twin-candidate model (Yang et al., 2003). The model faces the problem as a competition between two candidates to be the antecedent of the anaphor into account. Each candidate mention is compared with all the others in a round robin contest. Following the groupwise approach, rankers consider all the possible antecedent mentions at once (Denis and Baldridge, 2008). Rankers can obtain more accurate results due to a more informed context where all candidate mentions are considered at the same time. Coreference chains are formed after classification. Many systems form the chains by joining each positively-classified pair (i.e. singlelink"
C10-2125,S10-1001,1,\N,Missing
C18-1236,P14-5010,0,0.00310785,"s Eng. Storeh. Storeh. Storeh. Sales Eng. Add. Data Reject Accept – – – Wheel Breaks Breaks Accept – Table 1: Part of an event log for the bicycle manufacturing process. modeled behavior. 2.4 NLP Capabilities and Frameworks There are several existing NLP technologies that can be more or less straightforwardly applied to BPM. As illustrated in Fig. 2, a textual description of a bussiness process is a text where several actors are mentioned, and their actions and interactions described. Thus, out-of-the-box analyzers can be used to structure the content of the text. Tools such as Stanford Core (Manning et al., 2014), FreeLing (Padr´o and Stanilovsky, 2012), Apache OpenNLP1 , or NLTK (Bird et al., 2009) can be used to extract predicates and involved actors via SRL, perform WSD to identify domain objects that may be mentioned using different words, solve coreferences to decide which of the mentioned actors correspond to the same entity, decide which order the described actions must follow or whether there are choices or loops, etc. However, existing analysis pipelines are still below optimal performance, specially in the more semantic tasks (WSD, coreference, SRL, temporal relation extraction, etc.) and on"
C18-1236,padro-stanilovsky-2012-freeling,1,0.866471,"Missing"
C18-1236,E12-2021,0,0.0925662,"Missing"
C96-2148,H92-1022,0,0.0684889,"Missing"
C96-2148,C94-1027,0,0.331207,"Missing"
C96-2148,H93-1061,0,\N,Missing
C98-2159,P97-1032,0,0.0306717,"uracy, since correct answers are computed as wrong and vice-versa. In following sections we will show how this uncertainty in the evaluation may be, in some cases, larger than the reported improvements from one system to another, so invalidating the conclusions of the comparison. 3 Model since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to s t u d y the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant- absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. W h a t we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. OK/~OK stand for a right/wrong tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus have the correct tag, the tag is correctly evaluated a.s right. When the te"
C98-2159,W96-0208,0,\N,Missing
C98-2159,P97-1010,0,\N,Missing
C98-2159,P97-1031,1,\N,Missing
carreras-etal-2004-freeling,A92-1018,0,\N,Missing
carreras-etal-2004-freeling,A00-1031,0,\N,Missing
carreras-etal-2004-freeling,carreras-padro-2002-flexible,1,\N,Missing
carreras-padro-2002-flexible,W01-1013,1,\N,Missing
carreras-padro-2002-flexible,A97-2017,0,\N,Missing
cuadros-etal-2012-highlighting,alvez-etal-2008-complete,1,\N,Missing
cuadros-etal-2012-highlighting,J98-1006,0,\N,Missing
cuadros-etal-2012-highlighting,agirre-de-lacalle-2004-publicly,0,\N,Missing
cuadros-etal-2012-highlighting,W06-1663,1,\N,Missing
cuadros-etal-2012-highlighting,C02-1144,0,\N,Missing
cuadros-etal-2012-highlighting,E09-1005,0,\N,Missing
cuadros-etal-2012-highlighting,C00-1072,0,\N,Missing
cuadros-etal-2012-highlighting,P05-1016,0,\N,Missing
cuadros-etal-2012-highlighting,P04-1036,0,\N,Missing
cuadros-etal-2012-highlighting,P06-1101,0,\N,Missing
cuadros-etal-2012-highlighting,magnini-cavaglia-2000-integrating,0,\N,Missing
D19-1346,W18-6402,0,0.0140303,"simple deep learning architecture to learn semantic relatedness between word-to-word and word-to-sentence pairs, and show how it outperforms other semantic similarity scorers when used to re-rank candidate answers in the Text Spotting problem. In the future, we plan using the same approach to tackle similar problems, including lexical selection in Machine Translation, or word sense disambiguation (Lala and Specia, 2018). We believe our approach could also be useful in multimodal machine translation, where an image caption must be translated using not only the text but also the image content (Barrault et al., 2018). Tasks that lie at the intersection of computer vision and NLP, such as the challenges posed in the new BreakingNews dataset (popularity prediction, automatic text illustration) could also benefit from our results (Ramisa et al., 2018). Acknowledgments We would like to thank Jos´e Fonollosa, Marta Ruiz Costa-Juss`a and the anonymous reviewers for discussion and feedback. This work was supported by the KASP Scholarship Program and by the MINECO project HuMoUR TIN2017-90086-R. 3455 of the second argument model.similarity('ae','stadium') /home/asabir/anaconda2/envs/py36/lib/python3.6/site-packag"
D19-1346,D18-2029,0,0.069159,"eraging semantic information. Our rationale is that the text to be spotted is often related to the image context in which it appears (word pairs such as Delta–airplane, or quarters–parking are not similar, but are clearly related). We show how learning a word-to-word or word-to-sentence relatedness score can improve the performance of text spotting systems up to 2.9 points, outperforming other measures in a benchmark dataset. 1 Introduction Deep learning has been successful in tasks related to deciding whether two short pieces of text refer to the same topic, e.g. semantic textual similarity (Cer et al., 2018), textual entailment (Parikh et al., 2016) or answer ranking for Q&A (Severyn and Moschitti, 2015). However, other tasks require a broader perspective to decide whether two text fragments are related more than whether they are similar. In this work, we describe one of such tasks, and we retrain some of the existing sentence similarity approaches to learn this semantic relatedness. We also present a new Deep Neural Network (DNN) that outperforms existing approaches when applied to this particular scenario. 2 Text Hypothesis Extraction We use two pre-trained Text Spotting baselines to extract k"
D19-1346,D17-1070,0,0.0224545,"by the baseline. 4) MRR Mean Reciprocal Rank (MRR), which is 1 P|Q| 1 computed as follows: MRR = |Q| k=1 rankk , where rank k is the position of the correct answer in the hypotheses list proposed by the baseline. Comparing with sentence level model: We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset. We use cosine to compute the similarity between the caption and the candidate word. Word-to-sentence representations are computed with: Universal Sentence Encoder with the Transformer USE-T (Cer et al., 2018), and Infersent (Conneau et al., 2017) with glove (Pennington et al., 2014). The rest of the systems in Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words. The advantage of the attention mechanism is the ability to integrate information over time, and it allows the model to refer to spe3454 c1: street c2: downtown c3: a street sign with a sign on the side of it c1: bicycle"
D19-1346,E17-2068,0,0.0204086,"Missing"
D19-1346,L18-1602,0,0.0351378,"Missing"
D19-1346,L16-1147,0,0.0126967,"tion. Our proposed models achieve better performance than our TWE previous model (Sabir et al., 2018), that trained a word embedding (Mikolov et al., 2013) from scratch on the same task. Similarity to probabilities: After computing the cosine similarity we need to convert that score to probabilities. As we proposed in previous work (Sabir et al., 2018) we obtain the final probability combining (Blok et al., 2003) the similarity score, the probability of the detected context (provided by the object/place classifier), and the probability of the candidate word (estimated from a 5M token corpus) (Lison and Tiedemann, 2016). Effect of Unigram Probabilities: Ghosh et al. (2017) showed the utility of a language model (LM) when the data is too small for a DNN, obtaining significant improvements. Thus, we introduce a basic model of unigram probabilities with Out-of-vocabulary (OOV) words smoothing. The model is applied at the end, to re-rank out false positive short words, and has the main goal of reranking out less probable word overranked by the deep model. As seen in Table 1, the introduction of this unigram lexicon produces the best results. FutureWarning: Conversion 18 Human performance: To estimate an upper bo"
D19-1346,D16-1244,0,0.0704933,"Missing"
D19-1346,P82-1020,0,0.515437,"Missing"
D19-1346,D14-1162,0,0.0897632,"procal Rank (MRR), which is 1 P|Q| 1 computed as follows: MRR = |Q| k=1 rankk , where rank k is the position of the correct answer in the hypotheses list proposed by the baseline. Comparing with sentence level model: We compare the results of our encoder with several stateof-the-art sentence encoders, tuned or trained on the same dataset. We use cosine to compute the similarity between the caption and the candidate word. Word-to-sentence representations are computed with: Universal Sentence Encoder with the Transformer USE-T (Cer et al., 2018), and Infersent (Conneau et al., 2017) with glove (Pennington et al., 2014). The rest of the systems in Table 1 are trained in the same conditions that our model with glove initialization with dual-channel overlapping non-static pre-trained embedding on the same dataset. Our model FDCLSTM without attention achieves a better result in the case of the second baseline LSTM that full of false-positives and short words. The advantage of the attention mechanism is the ability to integrate information over time, and it allows the model to refer to spe3454 c1: street c2: downtown c3: a street sign with a sign on the side of it c1: bicycle c2: bus c3: a man and a women are st"
D19-1346,P16-1044,0,0.0330611,"Missing"
D19-1346,C16-1229,0,0.0774846,"Missing"
E03-1038,M95-1012,0,0.010507,"consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikhe"
E03-1038,A97-1029,0,0.13099,"slated into Catalan, including several entities. There is a wide consensus about that Named Entity Recognition and Classification (NERC) are Natural Language Processing tasks which may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–wr"
E03-1038,W02-2002,0,0.0199984,"L&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advant"
E03-1038,M98-1014,0,0.0543784,"performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El president"
E03-1038,W02-2004,1,0.764051,"ng data was left as unlabelled data. As evaluation method we use the common measures for recognition tasks: precision, recall and F1 . Precision is the percentage of NEs predicted by a system which are correct. Recall is the percentage of NEs in the data that a system correctly recognizes. Finally, the F1 measure computes the harmonic mean of precision (p) and recall (r) as 2 p • Op + r). 3 The Spanish NER System The Spanish NER system is based on the best system at CoNLL&apos;02, which makes use of a set of AdaBoost–based binary classifiers for recognizing the Named Entities in running text. See (Carreras et al., 2002) for details. The NE recognition task is performed as a sequence tagging problem through the well–known BIO labelling scheme. Here, the input sentence is treated as a word sequence and the output tagging codifies the NEs in the sentence. In particular, each word is tagged as either the beginning of a NE (B tag), a word inside a NE (I tag), or a word outside a NE (0 tag). In our case, a NER model is composed by: (a) a representation function, which maps a word and its context into a set of features, and (b) three binary classifiers (one corresponding to each tag) which, operating on the feature"
E03-1038,W99-0613,0,0.379481,"Missing"
E03-1038,M98-1015,0,0.0203914,"ssifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El pr"
E03-1038,W02-2019,0,0.0233796,"ly, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our goal in this paper is to develop a low–cost Named Entity recognition system for Catalan. To achieve this, we take advantage of the facts that Spanish and Catalan are two Romance languages with similar syntact"
E03-1038,W02-2020,0,0.0246992,"s between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos, 2002), Memory–based techniques (Tjong Kim Sang, 2002b) or Hidden Markov Models (Malouf, 2002), among others. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre–existing linguistic resources and/or limited funding possibilities. Our"
E03-1038,M98-1021,0,0.0255916,"1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER="
E03-1038,W02-2024,0,0.0701502,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2025,0,0.0420667,"ente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justicia]oRG.&quot; Figure 1: Example of a Spanish (top) and Catalan (bottom) sentence including several Named Entities between brackets (PER=person, Loc=location, oRG=organization). of the Conference on Natural Language Learning, CoNLL&apos;02 (Tjong Kim Sang, 2002a), where several machine–learning systems were compared at the NERC task. Usually, machine learning (ML) systems rely on algorithms that take as input a set of labelled examples for the target task and produce as output a model (which may take different forms, depending on the used algorithm) that can be applied to new examples to obtain a prediction. CoNLL&apos;02 participants used different state–of–the–art ML algorithms, such as Support Vector Machines (McNamee and Mayfield, 2002), AdaBoost (Can eras et al., 2002; Tsukamoto et al., 2002), Transformation–Based methods (Black and Vasilakopoulos,"
E03-1038,W02-2031,0,0.0471146,"Missing"
E03-1038,M95-1006,0,0.149935,"etc. Thus, interest on detecting and classifyPrevious work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC task. Some MUC systems rely on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]"
E03-1038,M98-1016,0,0.0312668,"on data–driven approaches, such as Nymble (Bikel et al., 1997) which uses Hidden Markov Models, or ALEMBIC (Aberdeen et al., 1995), based on Error Driven Transformation Based Learning. Others use only hand–coded knowledge, such as FACILE (Black et al., 1998) which relies on hand written unification context rules with certainty factors, or FASTUS (Appelt et al., 1995), PLUM (Weischedel, 1995) and NetOwl Extractor (Krupka and Hausman, 1998) which are based on cascaded finite state transducers or pattern matching. There are also hybrid systems combining corpus evidence and gazetteer information (Yu et al., 1998; Borthwick et al., 1998), or combining hand–written rules with Maximum Entropy models to solve correference (Mikheev et al., 1998). More recent approaches can be found in the proceedings of the shared task at the 2002 edition 43 &quot;El presidente del [Comite OlImpico Internacional]oRG, [Jose Antonio Samaranch]pER, se reuni6 el lunes en [Nueva Yorkkoc eon investigadores del [FBI]oRG y del [Departamento de JusticialoRG:&quot; &quot;El president del [Comite Olimpie Internacional]oRG, [Josep Antoni Samaranch]pER, es va reunir dilluns a [Nova York]Loc amb investigadors del [FBI]oRG i del [Departament de Justic"
E14-2003,W07-1702,1,0.860388,"Missing"
E14-2003,P05-1045,0,0.00899297,"ambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE recognition), and hun"
E14-2003,Q13-1018,1,0.896749,"Missing"
E14-2003,W14-0150,0,0.021713,"ervices following a lightweigth SOA architecture approach, and they are publically accessible and shared through META-SHARE.1 1 2 Linguistic Analyzers Apart from basic state-of-the-art tokenizers, lemmatizers, PoS/MSD taggers, and NE recognizers, each pipeline requires deeper processors able to build the target language-independent semantic representantion. For that, we rely on three steps: dependency parsing, semantic role labeling and word sense disambiguation. These three processes, combined with multilingual ontological resouces such as different WordNets and PredicateMatrix (L´opez de la Calle et al., 2014), a lexical semantics resource combining WordNet, FrameNet, and VerbNet, are the key to the construction of our semantic representation. Introduction Project XLike2 goal is to develop technology able to gather documents in a variety of languages and genres (news, blogs, tweets, etc.) and to extract language-independent knowledge from them, in order to provide new and better services to publishers, media monitoring, and business intelligence. Thus, project use cases are provided by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners"
E14-2003,P05-1012,0,0.0466071,"d by STA (Slovenian Press Agency) and Bloomberg, as well as New York Times as an associated partner. Research partners in the project are Joˇzef Stefan Institute (JSI), Karlsruhe Institute of Technology (KIT), Universitat Polit`ecnica de Catalunya (UPC), University of Zagreb (UZG), and Tsinghua University (THU). The Spanish company iSOCO is in charge of integration of all components developed in the project. This paper deals with the language technology developed within the project XLike to convert in2.1 Dependency Parsing We use graph-based methods for dependency parsing, namely, MSTParser3 (McDonald et al., 2005) is used for Chinese and Croatian, and Treeler4 is used for the other languages. Treeler is a library developed by the UPC team that implements several statistical methods for tagging and parsing. We use these tools in order to train dependency parsers for all XLike languages using standard available treebanks. 1 accessible and shared here means that the services are publicly callable, not that the code is open-source. 3 http://www.meta-share.eu 2 http://www.xlike.org 4 http://sourceforge.net/projects/mstparser http://treeler.lsi.upc.edu 9 Proceedings of the Demonstrations at the 14th Conferen"
E14-2003,N03-1033,0,0.0120884,"c, 2012). Word sense disambiguation is based on FreeLing. Frame extraction is rule-based since no SRL corpus is available for Croatian. • Chinese: Chinese shallow and deep processing is based on a word segmentation component ICTCLAS8 and a semantic dependency parser trained on CSDN corpus. Then, rulebased frame extraction is performed (no SRL corpus nor WordNet are available for Chinese). • Spanish, English, and Catalan: all modules are based on FreeLing (Padr´o and Stanilovsky, 2012) and Treeler. • German: German shallow processing is based on OpenNLP6 , Stanford POS tagger and NE extractor (Toutanova et al., 2003; Finkel et al., 2005). Dependency parsing, semantic role labeling, word sense disambiguation, and SRL-based frame extraction are based on FreeLing and Treeler. • Slovene: Slovene shallow processing is proˇ vided by JSI Enrycher7 (Stajner et al., 2010), which consists of the Obeliks morphosyntactic analysis library (Grˇcar et al., 2012), the LemmaGen lemmatizer (Jurˇsiˇc et al., 2010) ˇ and a CRF-based entity extractor (Stajner et al., 2012). Dependency parsing, word sense Each language analysis service is able to process thousands of words per second when performing shallow analysis (up to NE"
E14-2003,C12-2001,0,0.0402396,"Missing"
E14-2003,E09-1005,0,0.0627699,"s performed for all languages with a publicly available WordNet. This includes all languages in the project except Chinese. The goal of WSD is to map specific languages to a common semantic space, in this case, WN synsets. Thanks to existing connections between WN and other resources, SUMO and OpenCYC sense codes are also output when available. Thanks to PredicateMatrix, the obtained concepts can be projected to FrameNet, achieving a normalization of the semantic roles produced by the SRL (which are treebank-dependent, and thus, not the same for all languages). The used WSD engine is the UKB (Agirre and Soroa, 2009) implementation provided by FreeLing (Padr´o and Stanilovsky, 2012). 2.4 3 Cross-lingual Semantic Annotation This step adds further semantic annotations on top of the results obtained by linguistic processing. All XLike languages are covered. The goal is to map word phrases in different languages into the same semantic interlingua, which consists of resources specified in knowledge bases such as Wikipedia and Linked Open Data (LOD) sources. Cross-lingual semantic annotation is performed in two stages: (1) first, candidate concepts in the knowledge base are linked to the linguistic resources ba"
E14-2003,padro-stanilovsky-2012-freeling,1,\N,Missing
E17-2035,W06-1655,0,0.100025,"Missing"
E17-2035,P11-1004,0,0.0182457,"models this transformation as ( ) ∑ 1 exp pθ (s, l |w) = θ ⊤ f (si , ℓi , ℓi−1 ) , Zθ (w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The task entails breaking a word up into its constituent morphs. For example, the English word joblessness can be segmented as job+less+ness. When processing morphologically-rich languages, this helps reduce the sparsity created by the higher OOV rate due to productive morphology, and, empirically, has shown to be beneficial in a diverse variety of down-stream tasks, e.g., machine translation (Clifton and Sarkar, 2011), speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014) and parsing (Seeker and Özlem Çetinoğlu, 2015). Both supervised # Sentences 8600 4034 4550 5679 Segmentation i=1 where s is a segmentation, ℓ a labeling, θ ∈ Rd is the parameter vector, f is a feature function2 and the partition function Zθ (w) ensures the distribution is normalized. Note that each ℓi is taken from a set of labels L. In this work, we take L = {prefix, stem, suffix}. As an extension to the standard S-CRF Model, we allow for higher-order segment interactions (Nguyen et al., 2011). This allows f"
E17-2035,K15-1017,1,0.898953,"93 4730 4445 4183 Table 2: Per language breakdown of size of the POS portion and the morphological segmentation portion of DravMorph. All train / dev / test splits used in the experiments will be released with the corpus. and unsupervised approaches have been successful, but, when annotated data is available, supervised approaches typically greatly outperform unsupervised approaches (Ruokolainen et al., 2013). In light of this, we adopt a fully supervised model here. We apply semi-Markov Conditional Random Fields (S-CRFs) to the problem of morphological segmentation (Sarawagi and Cohen, 2004; Cotterell et al., 2015). S-CRFs have the ability to jointly model both a segmentation and a labeling. For example, consider the following the Malayalam word kūṭṭukāranmāruṭeyēāppam (കൂടടുകാരനമാരുടെയോപപം) (with (male) friends): labeled segmentation kūṭṭukāranmāruṭeyēāppam Z===========⇒ {z } | w [stem kūṭṭukāran] [suf mār] [suf uṭe] [suf yēāppam] . | {z } |{z } |{z } | {z } s1 ,ℓ1 s2 ,ℓ2 s3 ,ℓ3 s4 ,ℓ4 A S-CRF models this transformation as ( ) ∑ 1 exp pθ (s, l |w) = θ ⊤ f (si , ℓi , ℓi−1 ) , Zθ (w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The ta"
E17-2035,D16-1256,1,0.88989,"Missing"
E17-2035,N16-1080,1,0.863416,"Missing"
E17-2035,R15-1041,1,0.844831,"ik (2007) ap220 Marmot Marmot + seg Ka 86.35 88.04 Ma 88.77 90.44 Ta 89.04 91.64 Te 90.50 91.44 Table 4: Tagging results using the Marmot tagger on the four Dravidian languages studied in the paper. The results indicate strongly that morphological segmentation---rather than simple prefix and suffixes n-gram features---is a useful step in handling the agglutinative Dravidian languages. ply linear-chain CRFs for POS tagging of Bengali, Hindi and Telugu. Another approach that applied to POS tagging of Dravidian language is to use part-of-speech tagger of another similar languages. More recently, Kumar et al. (2015) applied adaptor grammars to unsupervised morphological segmentation of Kannada, Malayalam and Tamil. 6 Conclusion In this paper, we presented a higher-order semiCRF model for morphological segmentation for the Dravidian languages of South India. Our results show that the modeling of higher-order dependencies between segments and linguisticallyinspired features can greatly improve system performance. We also showed that segmentation is beneficial to the down-stream task of POS tagging. To promote research on the Dravidian family, we release hand-corrected corpora for both morphological segment"
E17-2035,D13-1032,0,0.0473167,"Missing"
E17-2035,D14-1095,0,0.0212216,"(w) 3 Morphological Segmentation We first examine the task of morphological segmentation in the Dravidian languages. The task entails breaking a word up into its constituent morphs. For example, the English word joblessness can be segmented as job+less+ness. When processing morphologically-rich languages, this helps reduce the sparsity created by the higher OOV rate due to productive morphology, and, empirically, has shown to be beneficial in a diverse variety of down-stream tasks, e.g., machine translation (Clifton and Sarkar, 2011), speech recognition (Afify et al., 2006), keyword spotting (Narasimhan et al., 2014) and parsing (Seeker and Özlem Çetinoğlu, 2015). Both supervised # Sentences 8600 4034 4550 5679 Segmentation i=1 where s is a segmentation, ℓ a labeling, θ ∈ Rd is the parameter vector, f is a feature function2 and the partition function Zθ (w) ensures the distribution is normalized. Note that each ℓi is taken from a set of labels L. In this work, we take L = {prefix, stem, suffix}. As an extension to the standard S-CRF Model, we allow for higher-order segment interactions (Nguyen et al., 2011). This allows for feature functions to look at multiple adjacent segments si , 218 2 Note we have om"
E17-2035,C04-1081,0,0.170602,"Missing"
E17-2035,petrov-etal-2012-universal,0,0.0886232,"Missing"
E17-2035,W96-0213,0,0.293853,"Missing"
E17-2035,W13-3504,0,0.151268,"re, we calculated inter-annotator agreement of two annotators for morphological labels and all datasets have Cohen's κ (Cohen, 1968) > 0.80. POS Tagging Lang Ka Ma Ta Te # Tokens 31364 34300 32400 30625 # Types 3593 4730 4445 4183 Table 2: Per language breakdown of size of the POS portion and the morphological segmentation portion of DravMorph. All train / dev / test splits used in the experiments will be released with the corpus. and unsupervised approaches have been successful, but, when annotated data is available, supervised approaches typically greatly outperform unsupervised approaches (Ruokolainen et al., 2013). In light of this, we adopt a fully supervised model here. We apply semi-Markov Conditional Random Fields (S-CRFs) to the problem of morphological segmentation (Sarawagi and Cohen, 2004; Cotterell et al., 2015). S-CRFs have the ability to jointly model both a segmentation and a labeling. For example, consider the following the Malayalam word kūṭṭukāranmāruṭeyēāppam (കൂടടുകാരനമാരുടെയോപപം) (with (male) friends): labeled segmentation kūṭṭukāranmāruṭeyēāppam Z===========⇒ {z } | w [stem kūṭṭukāran] [suf mār] [suf uṭe] [suf yēāppam] . | {z } |{z } |{z } | {z } s1 ,ℓ1 s2 ,ℓ2 s3 ,ℓ3 s4 ,ℓ4 A S-CR"
E17-2035,E14-4017,0,0.0597902,"Missing"
E17-2035,Q15-1026,0,0.067967,"Missing"
E17-2035,P15-2111,0,0.0232754,"Missing"
J13-4003,P95-1017,0,0.206257,"Missing"
J13-4003,W99-0211,0,0.0259285,"Missing"
J13-4003,N04-1038,0,0.0357171,"Missing"
J13-4003,D08-1031,0,0.248762,"Missing"
J13-4003,C10-1017,0,0.634231,"Westbrook, and Grishman (2005) Bengtson and Roth (2008) Stoyanov et al. (2009) Ng (2009) Uryupina (2009) Yang et al. (2003) Denis and Baldridge (2008) Yang et al. (2008) Rahman and Ng (2011b) Luo et al. (2004) Luo (2007) Volume 39, Number 4 Resolution Classification Model Linking process mention pairs backward search heuristic rankers entitymention Klenner and Ailloud (2008) Nicolae and Nicolae (2006) Denis and Baldridge (2007) Klenner (2007) Finkel and Manning (2008) Bean et al. (2004) Cardie and Wagstaff (1999) Ng (2008) two step Culotta, Wick, and McCallum (2007) Finley and Joachims (2005) Cai and Strube (2010) Yang et al. (2004) McCallum and Wellner (2005) Haghighi and Klein (2007) Poon and Domingos (2008) one step global optimization clustering graph partitioning mention pairs global optimization clustering entitymention hypergraph partitioning clustering graph partitioning global optimization Figure 5 A classification of coreference resolution approaches in state-of-the-art machine-learning systems. Given these prerequisites, we define an approach based on constraint satisfaction that represents the problem in a hypergraph and solves it by relaxation labeling, reducing coreference resolution to a"
J13-4003,W99-0611,0,0.18454,"Missing"
J13-4003,N07-1011,0,0.333017,"Missing"
J13-4003,N07-1030,0,0.108028,"Missing"
J13-4003,D08-1069,0,0.344726,"Missing"
J13-4003,P08-2012,0,0.354676,"Missing"
J13-4003,P07-1107,0,0.0935649,"Missing"
J13-4003,H05-1003,0,0.0613099,"Missing"
J13-4003,W11-1902,0,0.199855,"ach constraint, taking into account the precision of the constraint finding coreferent mentions. A machine learning process is applied to obtain the set of constraints. Constraints can also be added writing them by hand. Adding manual constraints is expensive, 862 ´ and Turmo Sapena, Padro, Constraint-Based Hypergraph Partitioning Coreference Resolution Figure 16 R ELAX C OR training process. however, given that it takes a group of linguistic experts many hours devoted to this task. An alternative option is to use constraints from other coreference resolution systems, such as the ones used in Lee et al. (2011). Our experiments are based on automatically learned constraints. Figure 16 shows the training process. First, a data selection process unbalances the training data set and then a machine learning process obtains the constraints. The learned constraints are then applied to the training data set and their precision is evaluated. The precision of each constraint determines its weight. The development process optimizes two parameters—balance and Nprune —in order to achieve maximum performance given a measure for the task. Figure 17 shows the development process. 4.3.1 Data Selection. Generating a"
J13-4003,H05-1004,0,0.451125,"Missing"
J13-4003,N07-1010,0,0.132746,"s without context, and a lack of information when evaluating pairs. Also, the approach would be more flexible if it could incorporate knowledge both automatically and manually. 851 Computational Linguistics Approach Aone and Bennett (1995) McCarthy and Lehnert (1995) Soon, Ng, and Lim (2001) Ponzetto and Strube (2006) Yang, Su, and Tan (2006) Ng and Cardie (2002) Ng (2005) Ng (2007) Ji, Westbrook, and Grishman (2005) Bengtson and Roth (2008) Stoyanov et al. (2009) Ng (2009) Uryupina (2009) Yang et al. (2003) Denis and Baldridge (2008) Yang et al. (2008) Rahman and Ng (2011b) Luo et al. (2004) Luo (2007) Volume 39, Number 4 Resolution Classification Model Linking process mention pairs backward search heuristic rankers entitymention Klenner and Ailloud (2008) Nicolae and Nicolae (2006) Denis and Baldridge (2007) Klenner (2007) Finkel and Manning (2008) Bean et al. (2004) Cardie and Wagstaff (1999) Ng (2008) two step Culotta, Wick, and McCallum (2007) Finley and Joachims (2005) Cai and Strube (2010) Yang et al. (2004) McCallum and Wellner (2005) Haghighi and Klein (2007) Poon and Domingos (2008) one step global optimization clustering graph partitioning mention pairs global optimization cluster"
J13-4003,P04-1018,0,0.234221,"Missing"
J13-4003,P05-1020,0,0.0590753,"Missing"
J13-4003,D08-1067,0,0.263726,"Missing"
J13-4003,N09-1065,0,0.185163,"Missing"
J13-4003,P10-1142,0,0.0361315,"s performed on-line. In this manner, mention-group and entity-mention models can be easily incorporated. Figure 5 summarizes the classification of several systems in the state of the art, up to 2011. Recently, the CoNLL-2012 shared task (Pradhan et al. 2012) offered an evaluation framework similar to that of CoNLL-2011. The second column specifies which resolution step is used. The third column shows the classification model used by the system, and the fourth column identifies the algorithm followed in the linking process. More details about supervised machine learning systems can be found in Ng (2010). 3. A Constraint-Based Hypergraph Partitioning Approach to Coreference Resolution One of the possible directions to follow in coreference resolution research is the incorporation of new information such as world knowledge and discourse coherence. In some cases, this information cannot be expressed in terms of pairs of mentions, that is, it is information that involves either several mentions at once or partial entities. Therefore, an experimental approach in this field needs the expressiveness of the entity-mention model as well as the mention-pair model in order to use the most typical menti"
J13-4003,P02-1014,0,0.747282,"Missing"
J13-4003,W99-0210,0,0.0405518,"Missing"
J13-4003,N06-1025,0,0.53574,"Missing"
J13-4003,D08-1068,0,0.107173,"Missing"
J13-4003,H05-1043,0,0.0503549,"Missing"
J13-4003,W12-4501,0,0.225416,"nt data (SemEval-2010). – CEAF B3 MUC language R P F1 R P F1 R P F1 ca es en-closed en-open 69.7 70.8 74.8 75.0 69.7 70.8 74.8 75.0 69.7 70.8 74.8 75.0 27.4 30.3 21.4 22.0 77.9 76.2 67.8 66.6 40.6 43.4 32.6 33.0 67.9 68.9 74.1 74.2 96.1 95.0 96.0 95.9 79.6 79.8 83.7 83.7 winner. In addition, R ELAX C OR achieved second position in the CoNLL-2011 Shared Task; Figure 20 reproduces the official table of results. Following sections describe the shared tasks in detail. Finally, the performance of R ELAX C OR is again compared with two other state-ofthe-art systems in M`arquez, Recasens, and Sapena (2012). 5.2.1 SemEval-2010. The goal of SemEval-2010 task 1 (Recasens et al. 2010) was to evaluate and compare automatic coreference resolution systems for six different languages in four evaluation settings and using four different evaluation measures. This complex scenario aimed at providing insight into several aspects of coreference resolution, including portability across languages, relevance of linguistic information at different levels, and behavior of alternative scoring measures. The task attracted considerable attention from a number of researchers, but only six teams submitted results. Mo"
J13-4003,W11-1901,0,0.318621,"Missing"
J13-4003,P11-1082,0,0.363969,"n et al. (2007), these results can be roughly generalized to any other system using similar information, and even other languages. Therefore, these classes require attention in order to improve global performance, and the fact that lexical, morphological, syntactic, and semantic levels are not very useful to deal with them encourages the research on adding world knowledge to coreference resolution systems. In stateof-the-art systems, we can find some attempts to add world knowledge to coreference resolution, using Wikipedia (Ponzetto and Strube 2006; Uryupina et al. 2011) or YAGO and Freenet (Rahman and Ng 2011a). Table 8 Description of the mention classes for English documents. Class PN E PN P PN N CN CN CN E P N P 1∪2 P 3G P 3U 874 Description NPs headed by a Proper Name that Exactly match (excluding case and the determiner) at least one preceding mention in the same coreference chain. NPs headed by a Proper Name that Partially match (i.e., head match or overlap, excluding case) at least one preceding mention in the same coreference chain. NPs headed by a Proper Name that do Not match any preceding mention in the same coreference chain. Same definitions as in PN E, PN P, and PN N, but referring to"
J13-4003,S10-1001,1,0.919309,"Missing"
J13-4003,C10-2125,1,0.916684,"Missing"
J13-4003,S10-1017,1,0.848518,"Missing"
J13-4003,W11-1903,1,0.901405,"Missing"
J13-4003,J01-4004,0,0.840199,"Missing"
J13-4003,P09-1074,0,0.229045,"Missing"
J13-4003,M95-1005,0,0.752497,"Missing"
J13-4003,P08-1096,0,0.351086,"Missing"
J13-4003,P06-1006,0,0.0408203,"Missing"
J13-4003,C04-1033,0,0.0686636,"Missing"
J13-4003,P03-1023,0,0.258366,"Missing"
J13-4003,W06-1633,0,\N,Missing
J13-4003,H05-2017,0,\N,Missing
J13-4003,J03-4003,0,\N,Missing
J13-4003,J14-4004,0,\N,Missing
J14-3001,W03-0403,0,0.127579,"his makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only identical parses, in their case to retrain the MaxEnt mo"
J14-3001,W06-2920,0,0.0285347,"e.html. 526 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences Figure 1 Derivation tree and dependency graph of Conceder licencias, cuando as´ı lo dispongan las ordenanzas [To grant licences, when so stipulated by ordinances]. format, also illustrated in Figure 1. In this target annotation, lexical elements are linked by asymmetrical dependency relations in which one of the elements is considered the head of the relation and the other one is its dependant. The conversion is a fully automatic and unambiguous process that produces the dependency structure in the CoNLL format (Buchholz and Marsi 2006). A deterministic conversion algorithm makes use of the identifiers of the phrase structure rules mentioned previously, in order to identify the heads, dependants, and some dependency types that are directly transferred onto the dependency structure (e.g., subject, specifier, and modifier). The identifiers of the lexical entries, which include the syntactic category of the subcategorized elements, enable the identification of the argument-related dependency functions.5 3.3 Dependency Parsing For dependency parsing, we use MaltParser (Nivre et al. 2007). To train it, we use manually disambiguat"
J14-3001,W97-1502,0,0.246513,"Missing"
J14-3001,W12-3602,0,0.0464821,"Missing"
J14-3001,I08-2097,0,0.058012,"Missing"
J14-3001,P07-1052,0,0.0137496,"making two annotators work on the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only iden"
J14-3001,W09-1120,0,0.0182701,"rors, a common strategy is to control inter-annotator agreement by making two annotators work on the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs"
J14-3001,D07-1111,0,0.0278777,"n the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only identical parses, in their"
J14-3001,W06-1604,0,0.0855075,"Missing"
L18-1057,agerri-etal-2014-ixa,0,0.0153089,"a behind RelaxCor and the main difficulties presented by its integration in FreeLing. Sections 4. and 5. describe an alternative set of handwritten constraints and compare its performance with the machine-learned model. Finally, Section 6. concludes. 2. Related Work There are several open-source suites other than FreeLing that offer state-of-the-art level NLP functionalities. The most remarkable, for being open-source, widely used and offering a set of functionalities comparable to FreeLing are: Stanford CoreNLP (Manning et al., 2014), Apache OpenNLP3 , NLTK (Bird et al., 2009) and IXA Pipes (Agerri et al., 2014). There are also other systems of NLP-related software, such as GATE or UIMA, which are not language analysis pipelines themselves, but architectures or frameworks to integrate existing components. The above mentioned suites largely differ with respect to the used programming language, offered APIs, processing speed, supported languages, customization or retraining capabilities, whether they are more developer-oriented or endhttp://nlp.lsi.upc.edu/freeling http://www.gnu.org/copyleft/agpl.html 3 376 https://opennlp.apache.org user oriented, etc. Thus, a detailed comparison is out of the scope"
L18-1057,W12-4502,0,0.0347906,"Missing"
L18-1057,J13-4004,0,0.190794,"y provided modules are available for Spanish and English. Finally, the latest FreeLing version offers coreferences for Spanish and English. The first attempt to establish a common evaluation framework for coreference systems was carried out in SemEval 2010 (Recasens et al., 2009), which offered data sets for 6 languages (including English and Spanish). Later, CoNLL2011 and CoNLL-2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) proposed similar tasks that have been a reference framework since then. The 2011 edition included only English, and was won by Stanford rule-based system (Lee et al., 2013). The 2012 edition included English, Arabic, and Chinese, and was won by a neural network based system (Fernandes et al., 2012), which has been the main trend in the state of the art since then. 3. 3.1. Integration of RelaxCor into FreeLing RelaxCor RelaxCor is a coreference resolution system based on constraint satisfaction. The coreference resolution problem is represented as a graph with mentions in the vertices which are connected to each other by edges. Edges are assigned a weight that indicates the confidence whether the mention pair corefers or not. More specifically, an edge weight is"
L18-1057,P14-5010,0,0.00697416,"next section briefly summarizes related work. Section 3. overviews the basic idea behind RelaxCor and the main difficulties presented by its integration in FreeLing. Sections 4. and 5. describe an alternative set of handwritten constraints and compare its performance with the machine-learned model. Finally, Section 6. concludes. 2. Related Work There are several open-source suites other than FreeLing that offer state-of-the-art level NLP functionalities. The most remarkable, for being open-source, widely used and offering a set of functionalities comparable to FreeLing are: Stanford CoreNLP (Manning et al., 2014), Apache OpenNLP3 , NLTK (Bird et al., 2009) and IXA Pipes (Agerri et al., 2014). There are also other systems of NLP-related software, such as GATE or UIMA, which are not language analysis pipelines themselves, but architectures or frameworks to integrate existing components. The above mentioned suites largely differ with respect to the used programming language, offered APIs, processing speed, supported languages, customization or retraining capabilities, whether they are more developer-oriented or endhttp://nlp.lsi.upc.edu/freeling http://www.gnu.org/copyleft/agpl.html 3 376 https://opennlp"
L18-1057,padro-stanilovsky-2012-freeling,1,0.870489,"Missing"
L18-1057,W11-1901,0,0.0125495,"stem in CoNLL 2011 shared tasks, supporting English and Chinese. Apache OpenNLP offers a basic support for English. IXA Pipes do not ship a coreference resolution module out-ofthe-box, but third-party provided modules are available for Spanish and English. Finally, the latest FreeLing version offers coreferences for Spanish and English. The first attempt to establish a common evaluation framework for coreference systems was carried out in SemEval 2010 (Recasens et al., 2009), which offered data sets for 6 languages (including English and Spanish). Later, CoNLL2011 and CoNLL-2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) proposed similar tasks that have been a reference framework since then. The 2011 edition included only English, and was won by Stanford rule-based system (Lee et al., 2013). The 2012 edition included English, Arabic, and Chinese, and was won by a neural network based system (Fernandes et al., 2012), which has been the main trend in the state of the art since then. 3. 3.1. Integration of RelaxCor into FreeLing RelaxCor RelaxCor is a coreference resolution system based on constraint satisfaction. The coreference resolution problem is represented as a graph with mentions i"
L18-1057,W12-4501,0,0.0128463,"red tasks, supporting English and Chinese. Apache OpenNLP offers a basic support for English. IXA Pipes do not ship a coreference resolution module out-ofthe-box, but third-party provided modules are available for Spanish and English. Finally, the latest FreeLing version offers coreferences for Spanish and English. The first attempt to establish a common evaluation framework for coreference systems was carried out in SemEval 2010 (Recasens et al., 2009), which offered data sets for 6 languages (including English and Spanish). Later, CoNLL2011 and CoNLL-2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) proposed similar tasks that have been a reference framework since then. The 2011 edition included only English, and was won by Stanford rule-based system (Lee et al., 2013). The 2012 edition included English, Arabic, and Chinese, and was won by a neural network based system (Fernandes et al., 2012), which has been the main trend in the state of the art since then. 3. 3.1. Integration of RelaxCor into FreeLing RelaxCor RelaxCor is a coreference resolution system based on constraint satisfaction. The coreference resolution problem is represented as a graph with mentions in the vertices which ar"
L18-1057,S10-1017,1,0.803436,"Missing"
L18-1057,W11-1903,1,0.890305,"Missing"
L18-1057,J13-4003,1,0.901505,"Missing"
lloberes-etal-2010-spanish,W98-0501,0,\N,Missing
lloberes-etal-2010-spanish,W07-1214,0,\N,Missing
lloberes-etal-2010-spanish,atserias-etal-2006-freeling,0,\N,Missing
P97-1031,H92-1022,0,0.0453582,"Missing"
P97-1031,W95-0101,0,0.0508296,"Missing"
P97-1031,A92-1018,0,0.116965,"Missing"
P97-1031,W96-0102,0,0.0262351,"es or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger architecture. We also present a constraint-acquisition algorithm that uses statistical decision trees to learn context constraints from annotated corpora and we use the acquired constraints to feed the POS tagger. The paper is organized as follows. In section 2 we describe our language model, in section 3 we describe the constraint acquisition algorithm, and in se"
P97-1031,P89-1015,0,0.0375431,"tested and evaluated on the WSJ corpus. 1 Introduction Language Model In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger arch"
P97-1031,C90-3030,0,0.0516042,"Missing"
P97-1031,C96-2148,1,0.915537,"Missing"
P97-1031,C94-1027,0,0.0530995,"J corpus. 1 Introduction Language Model In NLP, it is necessary to model the language in a representation suitable for the task to be performed. The language models more commonly used are based on two main approaches: first, the linguistic approach, in which the model is written by a linguist, generally in the form of rules or constraints (Voutilainen and Jgrvinen, 1995). Second, the automatic approach, in which the model is automatically obtained from corpora (either raw or annotated) 1, and consists of n-grams (Garside et al., 1987; Cutting et ah, 1992), rules (Hindle, 1989) or neural nets (Schmid, 1994). In the automatic approach we can distinguish two main trends: The low-level data trend collects statistics from the training corpora in the form of n-grams, probabilities, weights, etc. The high level data trend acquires more sophisticated information, such as context rules, constraints, or decision trees (Daelemans et al., 1996; M/~rquez and Rodriguez, 1995; Samuelsson et al., 1996). The acquisition methods range from supervised-inductivelearning-from-example algorithms (Quinlan, 1986; . I~:.i:;:;~: I / i.wcous le~ed |t wri.e. |... l Corpus Figure h Tagger architecture. We also present a co"
P97-1031,E95-1029,0,0.0215161,"Missing"
P97-1031,A97-1013,1,0.822392,"Missing"
P98-2164,P97-1010,0,0.026221,"test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus- but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort -space/time complexity- are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a multiple-trial experiment with statistical tests of significance. For these reasons, this paper calls for a discussion on POS taggers evaluation, aiming to establish a more rigorous test experimentation setting/designing, indispensable to extract reliable conclusions. As a starting point, we will focus only o"
P98-2164,P97-1031,1,0.87924,"Missing"
P98-2164,W96-0208,0,0.0310376,"ing tagger performances against a reference test corpus, and to make some criticism about common practices followed by the NLP researchers in this issue. The above mentioned factors can affect either the evaluation or the comparison process. Factors affecting the evaluation process are: (1) Training and test experiments are usually performed over noisy corpora which distorts the obtained results, (2) performance figures are too often calculated from only a single or very small number of trials, though average results from multiple trials are crucial to obtain reliable estimations of accuracy (Mooney, 1996), (3) testing experiments are usually done on corpora with the same characteristics as the training data -usually a small fresh portion of the training corpus- but no serious attempts have been done in order to determine the reliability of the results when moving from one domain to another (Krovetz, 1997), and (4) no figures about computational effort -space/time complexity- are usually reported, even from an empirical perspective. A factors affecting the comparison process is that comparisons between taggers are often indirect, while they should be compared under the same conditions in a mult"
P98-2164,P97-1032,0,0.0293534,"nce correct answers are c o m p u t e d as wrong and vice-versa. In following sections we will show how this uncertainty in the evaluation m a y be, in some cases, larger than the reported improvements from one system to another, so invalidating the conclusions of the comparison. 3 Model since the tagger error rate is getting too close to the error rate of the test corpus. Since we want to s t u d y the relationship between the tagger error rate and the test corpus error rate, we have to establish an absolute reference point. Although (Church, 1992) questions the concept of correct analysis, (Samuelsson and Voutilainen, 1997) establish that there exists a -statistically significant- absolute correct disambiguation, respect to which the error rates of either the tagger or the test corpus can be computed. W h a t we will focus on is how distorted is the tagger error rate by the use of a noisy test corpus as a reference. The cases we can find when evaluating the performance of a certain tagger are presented in table 1. OK/--aOK stand for a r i g h t / w r o n g tag (respect to the absolute correct disambiguation). When both the tagger and the test corpus have the correct tag, the tag is correctly evaluated as right."
padro-etal-2010-freeling,carreras-etal-2004-freeling,1,\N,Missing
padro-etal-2010-freeling,W02-2004,1,\N,Missing
padro-etal-2010-freeling,W07-1214,0,\N,Missing
padro-etal-2010-freeling,E09-1005,0,\N,Missing
padro-etal-2010-freeling,A00-1031,0,\N,Missing
padro-etal-2010-freeling,J01-4004,0,\N,Missing
padro-etal-2010-freeling,carreras-padro-2002-flexible,1,\N,Missing
padro-etal-2014-language,J93-2004,0,\N,Missing
padro-etal-2014-language,E06-1011,0,\N,Missing
padro-etal-2014-language,W07-1702,1,\N,Missing
padro-etal-2014-language,N03-1033,0,\N,Missing
padro-etal-2014-language,W03-1712,0,\N,Missing
padro-etal-2014-language,W08-2102,1,\N,Missing
padro-etal-2014-language,E09-1005,0,\N,Missing
padro-etal-2014-language,P05-1012,0,\N,Missing
padro-etal-2014-language,P08-1068,1,\N,Missing
padro-etal-2014-language,W09-1201,0,\N,Missing
padro-etal-2014-language,W14-0150,0,\N,Missing
padro-etal-2014-language,Q13-1018,1,\N,Missing
padro-etal-2014-language,taule-etal-2008-ancora,0,\N,Missing
padro-etal-2014-language,D07-1101,1,\N,Missing
padro-etal-2014-language,padro-stanilovsky-2012-freeling,1,\N,Missing
padro-etal-2014-language,P05-1045,0,\N,Missing
padro-stanilovsky-2012-freeling,W02-2004,1,\N,Missing
padro-stanilovsky-2012-freeling,E09-1005,0,\N,Missing
padro-stanilovsky-2012-freeling,A00-1031,0,\N,Missing
padro-stanilovsky-2012-freeling,J01-4004,0,\N,Missing
padro-stanilovsky-2012-freeling,W11-1501,1,\N,Missing
reese-etal-2010-wikicorpus,W07-0201,0,\N,Missing
reese-etal-2010-wikicorpus,agirre-de-lacalle-2004-publicly,0,\N,Missing
reese-etal-2010-wikicorpus,W08-2207,1,\N,Missing
reese-etal-2010-wikicorpus,widdows-ferraro-2008-semantic,0,\N,Missing
reese-etal-2010-wikicorpus,E09-1005,0,\N,Missing
reese-etal-2010-wikicorpus,S07-1015,1,\N,Missing
reese-etal-2010-wikicorpus,W01-0703,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2006-freeling,0,\N,Missing
reese-etal-2010-wikicorpus,zesch-etal-2008-extracting,0,\N,Missing
reese-etal-2010-wikicorpus,atserias-etal-2008-semantically,0,\N,Missing
S07-1095,atserias-etal-2006-freeling,0,0.0251929,"Missing"
S07-1095,W03-0421,1,0.890417,"Missing"
S07-1095,P02-1034,0,0.0541836,"Missing"
S07-1095,H05-1081,1,0.897279,"Missing"
S07-1095,S07-1008,1,0.816864,"Missing"
S07-1095,P05-1073,0,0.0449545,"to the features used, we focus only on global features that can be extracted independently of the local models. We show in Section 6 that this approach performs better on the small SemEval corpora than approaches that include features from the local models. We group the features into two sets: (a) features that extract information from the whole candidate set, and (b) features that model the structure of each candidate frame: Features from the whole candidate set: (1) Position of the current candidate in the whole set. Frame candidates are generated using the dynamic programming algorithm of Toutanova et al. (2005), and then sorted in descending order of the log probability of the whole frame (i.e., the sum of all argument log probabilities as reported by the local model). Hence, smaller positions indicate candidates that the local model considers better. (2) For each argument in the current frame, we store its number of repetitions in the whole candidate set. The intuition is that an argument that appears in many candidate frames is most likely correct. Features from each candidate frame: (3) The complete sequence of argument labels, extended with the predicate lemma and voice, similar to Toutanova et"
S10-1017,S10-1001,1,0.887615,"Missing"
S10-1017,C10-2125,1,0.260123,"Missing"
S10-1017,J01-4004,0,0.865624,"Missing"
W01-1013,2000.iwpt-1.7,0,0.155609,"Missing"
W01-1013,P00-1064,0,0.0912682,"Missing"
W01-1013,O98-4002,1,0.865745,"Missing"
W01-1013,W98-0705,0,0.144557,"Missing"
W01-1013,M95-1017,0,\N,Missing
W02-2004,W01-0726,1,\N,Missing
W03-0421,W01-0726,1,0.870152,"Missing"
W03-0421,W02-2004,1,0.561477,"Missing"
W03-0422,W02-1001,0,0.0206526,"ng strategy works online at sentence level. When visiting a sentence, the functions being learned are first used to recognize the NE phrases, and then updated according to the correctness of their solution. We analyze the dependencies among the involved perceptrons and a global solution in order to design a global update rule based on the recognition of namedentities, which reflects to each individual perceptron its committed errors from a global perspective. The learning approach presented here is closely related to –and inspired by– some recent works in the area of NLP and Machine Learning. Collins (2002) adapted the perceptron learning algorithm to tagging tasks, via sentence-based global feedback. Crammer and Singer (2003) presented an online topic-ranking algorithm involving several perceptrons and ranking-based update rules for training them. 2 Named-Entity Phrase Chunking In this section we describe our NERC approach as a phrase chunking problem. First we formalize the problem of NERC, then we propose a NE-Chunker. 2.1 Problem Formalization Let x be a sentence belonging to the sentence space X , formed by n words xi with i ranging from 0 to n − 1. Let K be the set of NE categories, which"
W03-1504,W02-2004,1,0.882393,"Missing"
W03-1504,E03-1038,1,0.880696,"Missing"
W03-1504,W99-0613,0,0.348162,"for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– independent NERC systems, which may be trained from small training sets by taking advantage of unlabelled examples (Collins and Singer, 1999; Abney, 2002), and which are easy to adapt to changing domains (being all these aspects closely related). This work focuses on exploring the construction of a low–cost Named Entity classification (NEC) module for Catalan without making use of large/expensive resources of the language. In doing so, the paper first explores the training of classification models by using only Catalan resources and then proposes a training scheme, in which a Catalan/Spanish bilingual classifier is trained directly from a training set including examples of the two languages. In both cases, the bootstrapping of the"
W03-1504,W03-0419,0,0.0206414,"Missing"
W03-1504,W02-2024,0,0.0121427,"ch may improve the performance of many applications, such as Information Extraction, Machine Translation, Question Answering, Topic Detection and Tracking, etc. Thus, interest on detecting and classifying those units in a text has kept on growing during the last years. Previous work in this topic is mainly framed in the Message Understanding Conferences (MUC), devoted to Information Extraction, which included a NERC competition task. More recent approaches can be found in the proceedings of the shared task at the 2002 and 2003 editions of the Conference on Natural Language Learning (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), where several machine–learning (ML) systems were compared at the NERC task for several languages. One remarkable aspect of most widely used ML algorithms is that they are supervised, that is, they require a set of labelled data to be trained on. This may cause a severe bottleneck when such data is not available or is expensive to obtain, which is usually the case for minority languages with few pre– existing linguistic resources and/or limited funding possibilities. This is one of the main causes for the recent growing interest on developing language– in"
W03-1504,P02-1046,0,\N,Missing
W11-1501,A00-1031,0,0.338641,"dictionary through an affixation module that checks whether they are derived forms, such as adverbs ending in -mente or clitic pronouns (-lo, -la) attached to verbs. This module has also been adapted, incorporating Old Spanish clitics (-gela, li) and other variants of derivation affixes (adverbs in -mientre or -mjentre). 5.3 Retraining the tagger FreeLing includes 2 different modules able to perform PoS tagging: a hybrid tagger (relax), integrating statistical and hand-coded grammatical rules, and a Hidden Markov Model tagger (hmm), which is a classical trigram markovian tagger, based on TnT (Brants, 2000). As mentioned in Section 4, the tagger for Standard Spanish has been used to pre-annotate the Gold Standard Corpus, which has subsequently been corrected to be able to carry out the retraining. The effort of correcting the corpus is much lower compared to annotating from scratch. In this paper we present the evaluation of the performance of the extended resource using the hmm tagger with the probabilities generated automatically from the trigrams in the Gold Standard Corpus. 6 Evaluation In this section we evaluate the dictionary (Section 6.1) and present the overall tagging results (Section"
W11-1501,padro-etal-2010-freeling,1,0.916541,"Missing"
W11-1501,W09-0214,0,0.059529,"Missing"
W11-1501,sanchez-marco-etal-2010-annotation,1,0.882435,"Missing"
W11-1903,P06-1005,0,0.252263,"d from 2 to 14 for pruning with a step of 2. Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and entity-based CEAF (Luo, 2005). 37 CoNLL shared task participation R ELAX C OR has participated in the CoNLL task in the Closed mode. All the knowledge required by the feature functions is obtained from the annotations of the corpora and no external resources have been used with the exception of WordNet (Miller, 1995), gender and number information (Bergsma and Lin, 2006) and sense inventories. All of them are allowed by the task organization and available in their website. There are many remarkable features that make this task different and more difficult but realistic than previous ones. About mention annotation, it is important to emphasize that singletons are not annotated, mentions must be detected by the system and the mapping between system and true mentions is limited to exact matching of boundaries. Moreover, some verbs have been annotated as corefering mentions. Regarding the evaluation, the scorer uses the modification of (Cai and Strube, 2010), unp"
W11-1903,W10-4305,0,0.0139991,"on (Bergsma and Lin, 2006) and sense inventories. All of them are allowed by the task organization and available in their website. There are many remarkable features that make this task different and more difficult but realistic than previous ones. About mention annotation, it is important to emphasize that singletons are not annotated, mentions must be detected by the system and the mapping between system and true mentions is limited to exact matching of boundaries. Moreover, some verbs have been annotated as corefering mentions. Regarding the evaluation, the scorer uses the modification of (Cai and Strube, 2010), unprecedented so far, and the corpora was published very recently and there are no published results yet to use as reference. Finally, all the preprocessed information is automatic for the test dataset, carring out some noisy errors which is a handicap from the point of view of machine learning. Following there is a description of the mention detection system developed for the task and an analysis of the obtained results in the development dataset. 3.1 The mention detection system extracts one mention for every NP found in the syntactic tree, one for every pronoun and one for every named ent"
W11-1903,H05-1004,0,0.228786,"mputational cost significantly and improves overall performance too. Optimizing this parameter depends on properties like document size and the quality of the information given by the constraints. The development process calculates a grid given the possible values of both parameters: from 0 to 1 for balance with a step of 0.05, and from 2 to 14 for pruning with a step of 2. Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and entity-based CEAF (Luo, 2005). 37 CoNLL shared task participation R ELAX C OR has participated in the CoNLL task in the Closed mode. All the knowledge required by the feature functions is obtained from the annotations of the corpora and no external resources have been used with the exception of WordNet (Miller, 1995), gender and number information (Bergsma and Lin, 2006) and sense inventories. All of them are allowed by the task organization and available in their website. There are many remarkable features that make this task different and more difficult but realistic than previous ones. About mention annotation, it is i"
W11-1903,W11-1901,0,0.343555,"on Coreference Resolution Emili Sapena, Llu´ıs Padr´o and Jordi Turmo∗ TALP Research Center Universitat Polit`ecnica de Catalunya Barcelona, Spain {esapena, padro, turmo}@lsi.upc.edu Abstract This paper describes the participation of R ELAX C OR in the CoNLL-2011 shared task: “Modeling Unrestricted Coreference in Ontonotes“. R ELAX C OR is a constraint-based graph partitioning approach to coreference resolution solved by relaxation labeling. The approach combines the strengths of groupwise classifiers and chain formation methods in one global method. 1 Introduction The CoNLL-2011 shared task (Pradhan et al., 2011) is concerned with intra-document coreference resolution in English, using Ontonotes corpora. The core of the task is to identify which expressions (usually NPs) in a text refer to the same discourse entity. This paper describes the participation of R ELAX C OR and is organized as follows. Section 2 describes R ELAX C OR, the system used in the task. Next, Section 3 describes the tuning needed by the system to adapt it to the task issues. The same section also analyzes the obtained results. Finally, Section 4 concludes the paper. 2 System description R ELAX C OR (Sapena et al., 2010a) is a cor"
W11-1903,W09-2411,1,0.892072,"Missing"
W11-1903,C10-2125,1,0.654005,"Missing"
W11-1903,S10-1017,1,0.750118,"Missing"
W11-1903,J01-4004,0,0.756034,"Missing"
W11-1903,M95-1005,0,0.591806,"ll be weight = precision − balance. the number of neighbors reduces the computational cost significantly and improves overall performance too. Optimizing this parameter depends on properties like document size and the quality of the information given by the constraints. The development process calculates a grid given the possible values of both parameters: from 0 to 1 for balance with a step of 0.05, and from 2 to 14 for pruning with a step of 2. Both parameters were empirically adjusted on the development set for the evaluation measure used in this shared task: the unweighted average of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and entity-based CEAF (Luo, 2005). 37 CoNLL shared task participation R ELAX C OR has participated in the CoNLL task in the Closed mode. All the knowledge required by the feature functions is obtained from the annotations of the corpora and no external resources have been used with the exception of WordNet (Miller, 1995), gender and number information (Bergsma and Lin, 2006) and sense inventories. All of them are allowed by the task organization and available in their website. There are many remarkable features that make this task different and more difficult but"
W11-1903,J03-4003,0,\N,Missing
W11-1903,S10-1001,1,\N,Missing
W15-2123,W06-2925,0,0.0415312,"Missing"
W15-2123,D07-1101,0,0.0299627,"the tree (Sleator and Temperley, 1991; J¨arvinen and Tapanainen, 1998; Lin, 1998). Concerning the languages this study is based on, some research on Spanish has been performed from the perspective of Constraint Grammar (Bick, 2006), Unification Grammar (Ferr´andez and Moreno, 2000), Head-Driven Phrase Structure Grammar (Marimon et al., 2014), and Dependency Grammar for statistical parsing, both supervised (Carreras et al., 2006) and semi-supervised (Calvo and Gelbukh, 2011). For Catalan, a rule-based parser based on Constraint Grammar (Alsina et al., 2002) and a statistical dependency parser (Carreras, 2007) are available. Despite the huge achievements in the area of parsing, argument/adjunct recognition is still a linguistic problem in which parsers still show low accuracy and in which there is still no generalized consensus in Theoretical Linguistics (Tesni`ere, 1959; Chomsky, 1965). This phenomenon refers to the subcategorization notion, which corresponds to the definition of the type and the number of arguments of a syntactic head. The acquisition of subcategorization frames from corpora is one of the strategies for integrating information about the argument structure into a parser. Depending"
W15-2123,W98-1114,0,0.597998,"of this paper is to show how subcategorization frames acquired from a syntactically annotated corpus and organized into fine-grained classes can improve the performance of two rulebased dependency grammars. 1 Introduction Statistical parsers and rule-based parsers have advanced over recent years. However, significant efforts are required to increase the performance of current parsers (Klein and Manning, 2003; Nivre et al., 2006; Ballesteros and Nivre, 2012; Marimon et al., 2014). One of the linguistic phenomena which parsers often fail to handle correctly is the argument/adjunct distinction (Carroll et al., 1998). For this reason, the main goal of this paper is to test empirically the accuracy of rule-based dependency grammars working exclusively with syntactic rules or adding subcategorization frames to the rules. A number of studies shows that subcategorization frames can contribute to improve parser performance (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). Particularly, these studies are mainly concerned with the integration of subcategorization information into statistical parsers. The list of studies about rule-based parsers integrating subcategorization information is also exte"
W15-2123,N13-1024,0,0.0927903,"nce of current parsers (Klein and Manning, 2003; Nivre et al., 2006; Ballesteros and Nivre, 2012; Marimon et al., 2014). One of the linguistic phenomena which parsers often fail to handle correctly is the argument/adjunct distinction (Carroll et al., 1998). For this reason, the main goal of this paper is to test empirically the accuracy of rule-based dependency grammars working exclusively with syntactic rules or adding subcategorization frames to the rules. A number of studies shows that subcategorization frames can contribute to improve parser performance (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). Particularly, these studies are mainly concerned with the integration of subcategorization information into statistical parsers. The list of studies about rule-based parsers integrating subcategorization information is also extensive (Lin, 1998; Alsina et al., 2002; Bick, 2006; Calvo and Gelbukh, 2011). However, they do not 2 Related Work There has been an extensive research on parser development, and most approaches can be classified as statistical or rule-based. In the former, a statistical model learnt from annotated or unannotated texts is applied to build the syntactic tree (Klein and M"
W15-2123,J05-1003,0,0.05177,", these studies are mainly concerned with the integration of subcategorization information into statistical parsers. The list of studies about rule-based parsers integrating subcategorization information is also extensive (Lin, 1998; Alsina et al., 2002; Bick, 2006; Calvo and Gelbukh, 2011). However, they do not 2 Related Work There has been an extensive research on parser development, and most approaches can be classified as statistical or rule-based. In the former, a statistical model learnt from annotated or unannotated texts is applied to build the syntactic tree (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012), whereas the latter uses hand-built grammars to guide the 201 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 201–210, Uppsala, Sweden, August 24–26 2015. formation to make a decision (Carroll et al., 1998). Depending on the characteristics of the parser, subcategorization assists in this task in a different way. Subcategorization information can be used to assign a probability to every possible syntactic tree and to rank them in parsers that perform the whole set of possible syntactic analysis"
W15-2123,W06-2933,0,0.0871768,"Missing"
W15-2123,padro-stanilovsky-2012-freeling,1,0.873006,"Missing"
W15-2123,W98-0501,0,0.28154,"Missing"
W15-2123,C00-2100,0,0.0287495,"he acquisition of subcategorization frames from corpora is one of the strategies for integrating information about the argument structure into a parser. Depending on the level of language analysis of the annotated corpus, two main strategies are used in automatic acquisition. If the acquisition is performed over a morphosyntactically annotated text, the subcategorization frames are inferred by applying statistical techniques on morphosyntactically annotated data (Brent, 1993; Manning, 1993; Korhonen et al., 2003). Alternatively, acquisition can be performed with syntactically annotated texts (Sarkar and Zeman, 2000; O’Donovan et al., 2005; Aparicio et al., 2008). Subcategorization acquisition can be performed straightforwardly because the information about the argument structure is available in the corpus. Therefore, this approach generally focuses on the methods for subcategorization frames classification. The final classification in a lexicon of frames is a computational resource for several NLP tools. In the framework which this research focuses on, the integration of the acquired subcategorization is orientated to the contribution towards building the syntactic tree when the parser has incomplete in"
W15-2123,P03-1054,0,0.0486029,"performance, such as the enhancement of the argument/adjunct recognition. There is evidence that verb subcategorization frames can contribute to parser accuracy, but a number of issues remain open. The main aim of this paper is to show how subcategorization frames acquired from a syntactically annotated corpus and organized into fine-grained classes can improve the performance of two rulebased dependency grammars. 1 Introduction Statistical parsers and rule-based parsers have advanced over recent years. However, significant efforts are required to increase the performance of current parsers (Klein and Manning, 2003; Nivre et al., 2006; Ballesteros and Nivre, 2012; Marimon et al., 2014). One of the linguistic phenomena which parsers often fail to handle correctly is the argument/adjunct distinction (Carroll et al., 1998). For this reason, the main goal of this paper is to test empirically the accuracy of rule-based dependency grammars working exclusively with syntactic rules or adding subcategorization frames to the rules. A number of studies shows that subcategorization frames can contribute to improve parser performance (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). Particularly, these"
W15-2123,P03-1009,0,0.0112446,"tion, which corresponds to the definition of the type and the number of arguments of a syntactic head. The acquisition of subcategorization frames from corpora is one of the strategies for integrating information about the argument structure into a parser. Depending on the level of language analysis of the annotated corpus, two main strategies are used in automatic acquisition. If the acquisition is performed over a morphosyntactically annotated text, the subcategorization frames are inferred by applying statistical techniques on morphosyntactically annotated data (Brent, 1993; Manning, 1993; Korhonen et al., 2003). Alternatively, acquisition can be performed with syntactically annotated texts (Sarkar and Zeman, 2000; O’Donovan et al., 2005; Aparicio et al., 2008). Subcategorization acquisition can be performed straightforwardly because the information about the argument structure is available in the corpus. Therefore, this approach generally focuses on the methods for subcategorization frames classification. The final classification in a lexicon of frames is a computational resource for several NLP tools. In the framework which this research focuses on, the integration of the acquired subcategorization"
W15-2123,C02-1118,0,0.244702,"the performance of current parsers (Klein and Manning, 2003; Nivre et al., 2006; Ballesteros and Nivre, 2012; Marimon et al., 2014). One of the linguistic phenomena which parsers often fail to handle correctly is the argument/adjunct distinction (Carroll et al., 1998). For this reason, the main goal of this paper is to test empirically the accuracy of rule-based dependency grammars working exclusively with syntactic rules or adding subcategorization frames to the rules. A number of studies shows that subcategorization frames can contribute to improve parser performance (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). Particularly, these studies are mainly concerned with the integration of subcategorization information into statistical parsers. The list of studies about rule-based parsers integrating subcategorization information is also extensive (Lin, 1998; Alsina et al., 2002; Bick, 2006; Calvo and Gelbukh, 2011). However, they do not 2 Related Work There has been an extensive research on parser development, and most approaches can be classified as statistical or rule-based. In the former, a statistical model learnt from annotated or unannotated texts is applied to build the"
W15-2123,lloberes-etal-2010-spanish,1,0.887736,"Missing"
W15-2123,J14-3001,1,0.881474,"Missing"
W15-2123,J93-2002,0,\N,Missing
W15-2123,J05-3003,0,\N,Missing
W15-2123,P93-1032,0,\N,Missing
W15-2123,D07-1096,0,\N,Missing
W15-2123,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
W15-2123,aparicio-etal-2008-ancora,0,\N,Missing
W15-2207,ballesteros-nivre-2012-maltoptimizer-system,0,0.0269919,"tures Domain HP general EAGLES specific TSNLP general Goal parsing NLP software Languages English grammar checkers English Annotation minimal minimal Content syntax taxonomy of errors English, German, French robust (extra-)linguistic Table 1: HP, EAGLES & TSNLP features Introduction added (Lloberes et al., 2010) using ParTes are discussed in Section 4. Finally, the main conclusions and future work are exposed (Section 5). Parsing has been a very active area, so that parsers have progressed significantly over the recent years (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012; Bohnet and Nivre, 2012; Ballesteros and Carreras, 2015). However, nowadays significant improvement in parser performance needs extra effort. A deeper and detailed analysis of the parsers performance can provide the keys to exceed the current accuracy. Tests suites are a linguistic resource which makes it possible this kind of analysis and which can contribute to highlight the key issues to improve decisively the Natural Language Processing (NLP) tools (Flickinger et al., 1987; EAGLES, 1994; Lehmann et al., 1996). This paper presents ParTes 15.02, a test suite of syntactic phenomena for parsi"
W15-2207,N13-1024,0,0.015681,"numeric id is assigned to every schema and a link to SenSem Corpus sentences with the same schema is created (idsensem). Every schema recorded is exemplified with a sentence for testing purposes (test). For every test 4 Evaluation task In order to test the usability of ParTes for parsing evaluation, it has been applied as a gold standard in an evaluation task of the FDGs. Particularly, the capabilities of the test suite have been tested for explaining the performance of FDG as regards the argument recognition since it still remains to be solved successfully (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). The FDGs are the core part of the rulebased FreeLing Dependency Parser (Padr´o and Stanilovsky, 2012). They provide a deep and complete syntactic analysis in the form of dependencies. The grammars are a set of manually-defined rules that comple the structure of the tree (linking rules) and assign a syntactic function to every link of the tree (labelling rules) by means of a system of priorities and a set of conditions. Two FDGs versions for both languages have been evaluated: a version without verb subcategorization classes (Bare) and a version with verb sub2 Tagset: adjt - adjunct; attr - a"
W15-2207,D12-1133,0,0.0338028,"S specific TSNLP general Goal parsing NLP software Languages English grammar checkers English Annotation minimal minimal Content syntax taxonomy of errors English, German, French robust (extra-)linguistic Table 1: HP, EAGLES & TSNLP features Introduction added (Lloberes et al., 2010) using ParTes are discussed in Section 4. Finally, the main conclusions and future work are exposed (Section 5). Parsing has been a very active area, so that parsers have progressed significantly over the recent years (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012; Bohnet and Nivre, 2012; Ballesteros and Carreras, 2015). However, nowadays significant improvement in parser performance needs extra effort. A deeper and detailed analysis of the parsers performance can provide the keys to exceed the current accuracy. Tests suites are a linguistic resource which makes it possible this kind of analysis and which can contribute to highlight the key issues to improve decisively the Natural Language Processing (NLP) tools (Flickinger et al., 1987; EAGLES, 1994; Lehmann et al., 1996). This paper presents ParTes 15.02, a test suite of syntactic phenomena for parsing evaluation. This reso"
W15-2207,W06-2933,0,0.099318,"Missing"
W15-2207,W98-1114,0,0.137652,"associated (freq). In addition, a numeric id is assigned to every schema and a link to SenSem Corpus sentences with the same schema is created (idsensem). Every schema recorded is exemplified with a sentence for testing purposes (test). For every test 4 Evaluation task In order to test the usability of ParTes for parsing evaluation, it has been applied as a gold standard in an evaluation task of the FDGs. Particularly, the capabilities of the test suite have been tested for explaining the performance of FDG as regards the argument recognition since it still remains to be solved successfully (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). The FDGs are the core part of the rulebased FreeLing Dependency Parser (Padr´o and Stanilovsky, 2012). They provide a deep and complete syntactic analysis in the form of dependencies. The grammars are a set of manually-defined rules that comple the structure of the tree (linking rules) and assign a syntactic function to every link of the tree (labelling rules) by means of a system of priorities and a set of conditions. Two FDGs versions for both languages have been evaluated: a version without verb subcategorization classes (Bare) and a version with v"
W15-2207,padro-stanilovsky-2012-freeling,1,0.881758,"Missing"
W15-2207,J05-1003,0,0.0604965,"ively improve the parser performance. 1 Features Domain HP general EAGLES specific TSNLP general Goal parsing NLP software Languages English grammar checkers English Annotation minimal minimal Content syntax taxonomy of errors English, German, French robust (extra-)linguistic Table 1: HP, EAGLES & TSNLP features Introduction added (Lloberes et al., 2010) using ParTes are discussed in Section 4. Finally, the main conclusions and future work are exposed (Section 5). Parsing has been a very active area, so that parsers have progressed significantly over the recent years (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012; Bohnet and Nivre, 2012; Ballesteros and Carreras, 2015). However, nowadays significant improvement in parser performance needs extra effort. A deeper and detailed analysis of the parsers performance can provide the keys to exceed the current accuracy. Tests suites are a linguistic resource which makes it possible this kind of analysis and which can contribute to highlight the key issues to improve decisively the Natural Language Processing (NLP) tools (Flickinger et al., 1987; EAGLES, 1994; Lehmann et al., 1996). This paper presents ParTes 15."
W15-2207,D09-1085,0,0.020109,"isticated the software became, the more complex the test suites evolved 61 Proceedings of the 14th International Conference on Parsing Technologies, pages 61–65, c Bilbao, Spain; July 22–24, 2015. 2015 Association for Computational Linguistics to be (Lehmann et al., 1996). From a collection of interesting examples, they transformed into deeply structured and richly annotated databases (Table 1), such as the HP test suite (Flickinger et al., 1987), the test suite developed by one of the groups of EAGLES (EAGLES, 1994), the TSNLP (Lehmann et al., 1996) and the corpus of unbounded depdendencies (Rimell et al., 2009). Concerning the languages of this study, a test suite for Spanish was developed by Marimon et al. (2007). The goal of this test suite is to assess the development of a Spanish Head-Driven Phrase Structure Grammar and it offers grammatical and agrammatical test cases. 3 &lt;level name=&quot;intrachunk&quot;&gt; &lt;constituent name=&quot;nounphrase&quot;&gt; &lt;hierarchy name=&quot;child&quot;&gt; &lt;realization id=&quot;0037&quot; name=&quot;prepositionalphrase&quot; class=&quot;noun&quot; subclass=&quot;prepobj&quot; link=&quot;n-s&quot; freq=&quot;0.084357&quot; parent_devel=&quot;recurso&quot; child_devel=&quot;para&quot; parent_test=&quot;libro&quot; child_test=&quot;para&quot; devel=&quot;Es un recurso para los alumnos&quot; test=&quot;Los alumnos"
W15-2207,C02-1118,0,0.0259033,"addition, a numeric id is assigned to every schema and a link to SenSem Corpus sentences with the same schema is created (idsensem). Every schema recorded is exemplified with a sentence for testing purposes (test). For every test 4 Evaluation task In order to test the usability of ParTes for parsing evaluation, it has been applied as a gold standard in an evaluation task of the FDGs. Particularly, the capabilities of the test suite have been tested for explaining the performance of FDG as regards the argument recognition since it still remains to be solved successfully (Carroll et al., 1998; Zeman, 2002; Mirroshandel et al., 2013). The FDGs are the core part of the rulebased FreeLing Dependency Parser (Padr´o and Stanilovsky, 2012). They provide a deep and complete syntactic analysis in the form of dependencies. The grammars are a set of manually-defined rules that comple the structure of the tree (linking rules) and assign a syntactic function to every link of the tree (labelling rules) by means of a system of priorities and a set of conditions. Two FDGs versions for both languages have been evaluated: a version without verb subcategorization classes (Bare) and a version with verb sub2 Tags"
W15-2207,P03-1054,0,0.0268502,"in factors that can decisively improve the parser performance. 1 Features Domain HP general EAGLES specific TSNLP general Goal parsing NLP software Languages English grammar checkers English Annotation minimal minimal Content syntax taxonomy of errors English, German, French robust (extra-)linguistic Table 1: HP, EAGLES & TSNLP features Introduction added (Lloberes et al., 2010) using ParTes are discussed in Section 4. Finally, the main conclusions and future work are exposed (Section 5). Parsing has been a very active area, so that parsers have progressed significantly over the recent years (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012; Bohnet and Nivre, 2012; Ballesteros and Carreras, 2015). However, nowadays significant improvement in parser performance needs extra effort. A deeper and detailed analysis of the parsers performance can provide the keys to exceed the current accuracy. Tests suites are a linguistic resource which makes it possible this kind of analysis and which can contribute to highlight the key issues to improve decisively the Natural Language Processing (NLP) tools (Flickinger et al., 1987; EAGLES, 1994; Lehmann et al., 1996). This pa"
W15-2207,C96-2120,0,0.414277,"years (Klein and Manning, 2003; Collins and Koo, 2005; Nivre et al., 2006; Ballesteros and Nivre, 2012; Bohnet and Nivre, 2012; Ballesteros and Carreras, 2015). However, nowadays significant improvement in parser performance needs extra effort. A deeper and detailed analysis of the parsers performance can provide the keys to exceed the current accuracy. Tests suites are a linguistic resource which makes it possible this kind of analysis and which can contribute to highlight the key issues to improve decisively the Natural Language Processing (NLP) tools (Flickinger et al., 1987; EAGLES, 1994; Lehmann et al., 1996). This paper presents ParTes 15.02, a test suite of syntactic phenomena for parsing evaluation. This resource contains an exhaustive and representative set of structure and word order phenomena for Spanish and Catalan languages (Lloberes et al., 2014). The new version adds a development data set and a test data set. The rest of the paper describes the main contributions in test suite development (Section 2). Section 3 shows the characteristics and the specifications of ParTes. The results of an evaluation task of the FreeLing Dependency Grammars (FDGs) with verb subcategorization information 2"
W15-2207,lloberes-etal-2010-spanish,1,0.873353,"Missing"
W15-2207,D07-1096,0,\N,Missing
W15-2207,taule-etal-2008-ancora,0,\N,Missing
W15-5404,Q13-1021,0,0.280423,"thus, in those languages the morphological segmentation may approximate morphological analysis well enough. Most unsupervised morphological segmentation systems 17 Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 17–23, c Hissar, Bulgaria, September 10, 2015. 2015 Association for Computational Linguistics of the Dravidian languages. We use the Adaptor Grammars framework (Johnson et al., 2007a) for implementing our segmentation models that has been shown to be effective for semi-supervised morphological segmentation learning (Sirts and Goldwater, 2013) and it provides a computational platform for building Bayesian non-parametric models and its inference procedure. We learn the segmentation patterns from transliterated input and convert the segmented transliterations into the orthographic representation by applying a set of regular expressions created from morphological and orthographic rules to deal with sandhi. We test our system on two major languages from the Dravidian family— Malayalam and Kannada. These languages, regardless of their large number of speakers, can be considered resource-scarce, for which not much annotated data availabl"
W15-5404,N07-1020,0,0.0277925,"t. Evaluation, Error analysis and Discussion The evaluation is based on how well the methods predicts the morpheme boundaries in orthographic form and calculates precision, recall and F- score. We used Python suite provided in the morphochallenge website4 for evaluation purposes We also train Morfessor baseline, Morfessor-CAP and Undivide, with 80K word types. We compare our results with several baselines that have been previously successfully used for agglutinated languages: Finnish and Turkish. For unsupervised baselines we use Morfessor CategoriesMAP (Creutz and Lagus, 2007b) and Undivide (Dasgupta and Ng, 2007). We train Morfessor Categories-MAP with the 80K most frequent word types and produce a model. Using this model the gold standard file is segmented and the results are compared with the manual segmentations. The same process is carried out in the case of Morfessor baseline. In the case of Undivide, we apply the system on the gold standard file and get the segmentation. We use Undivide software because it performed very well in the case of highly inflected Indian language Bengali. The results are evaluated by computing the segment boundary F1 score (F1) as is standard for morphological segmenta"
W15-5404,J11-2002,0,0.0260069,"n the case of two complex languages from the Dravidian family. The same morphological model and results are evaluated comparing to other state-of-the art unsupervised morphology learning systems. 1 Antoni Oliver Universitat Oberta de Catalunya aoliverg@uoc.edu Introduction Morphemes are the smallest individual units that form words. For example, the Malayalam word (മലക െട, malakaḷuṭe, related to mountains) consists of several morphemes ( stem mala, plural marker kal, and genitive case marker uṭe). Morphological segmentation is one of the most studied tasks in unsupervised morphology learning (Hammarström and Borin, 2011). In unsupervised morphology learning, the words are segmented into corresponding morphemes with any supervision, as for example morphological annotations. It provides the simplest form of morphological analysis for languages that lack supervised knowledge or annotation. In agglutinative languages, there is a close connection between suffixes and morphosyntactic functions and thus, in those languages the morphological segmentation may approximate morphological analysis well enough. Most unsupervised morphological segmentation systems 17 Proceedings of the Joint Workshop on Language Technology"
W15-5404,N09-1036,0,0.0206887,"ded to t before and Submorphs → Submorph Submorphs Submorphs → Chars Chars → Char 2. Expand as in PCFG considering the probability propositional to concentration parameter αA Chars → CharChars The Submorphs can be composed of single morph or Submorhs, which are combinations of Char. In our case Char is our internal representation for alpha-syllabic characters. The above grammar can generate various parse trees as a we put a Pitman-Yor prior on component. It is going to produce most probable morphological segmentation based on the prior probabilities. For more details of this procedure, refer (Johnson and Goldwater, 2009) Inference on this model can achieved using a sampling procedure . The formal definition of AGs can be found in (Johnson et al., 2007a), details of the inference procedures are described in (Johnson et al., 2007b) and (Johnson and Goldwater, 2009). 4 AGs for morphological segmentation for Dravidian languages Dravidian languages are highly agglutinative, which means that a stem can be attached a sequence of several suffixes and several words can be concatenated together to form compounds. The segmentation model has to take these language properties into account. We can define a grammar reflecti"
W15-5404,N07-1018,0,0.528737,"ges that lack supervised knowledge or annotation. In agglutinative languages, there is a close connection between suffixes and morphosyntactic functions and thus, in those languages the morphological segmentation may approximate morphological analysis well enough. Most unsupervised morphological segmentation systems 17 Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 17–23, c Hissar, Bulgaria, September 10, 2015. 2015 Association for Computational Linguistics of the Dravidian languages. We use the Adaptor Grammars framework (Johnson et al., 2007a) for implementing our segmentation models that has been shown to be effective for semi-supervised morphological segmentation learning (Sirts and Goldwater, 2013) and it provides a computational platform for building Bayesian non-parametric models and its inference procedure. We learn the segmentation patterns from transliterated input and convert the segmented transliterations into the orthographic representation by applying a set of regular expressions created from morphological and orthographic rules to deal with sandhi. We test our system on two major languages from the Dravidian family—"
W15-5404,N09-1024,0,0.0633921,"Missing"
