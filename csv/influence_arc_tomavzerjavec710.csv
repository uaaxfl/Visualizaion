2000.eamt-1.7,erjavec-etal-2000-concede,1,0.866771,"Missing"
2000.eamt-1.7,A00-2013,0,0.0519304,"Missing"
2020.lrec-1.409,hnatkova-etal-2014-syn,0,0.0735679,"Missing"
2020.lrec-1.409,L16-1242,1,0.826954,"into formats that facilitate digital processing (Bušta et al., 2017). 3. Corpus Annotation Linguistic annotation of the Gigafida 2.0 corpus was performed on the following levels: (1) tokenization and sentence segmentation, (2) annotation with morphosyntactic descriptions (MSDs) and (3) lemmatization. At the time this corpus was finalized, there were two major pipelines available for the Slovenian language: the first was developed within the SSJ project (Obeliks, Grcar et al., 2012, hereafter “the SSJ pipeline”), and the one developed within the CLARIN.SI research infrastructure (relditagger, Ljubešić and Erjavec, 2016, hereafter “CLARIN.SI pipeline”), each with their own strengths and weaknesses. While the SSJ pipeline includes explicit linguistic postprocessing rules, the CLARIN.SI pipeline excels in overall annotation accuracy. This was the reason why a &quot;metatagging&quot; approach was chosen to make use of the strong sides of each pipeline. For the first task of tokenization and sentence segmentation, the rule-based Obeliks4J tool was applied as it follows the segmentation rules applied both in the previous versions of the reference corpus, as well as the manually annotated training data. The remaining two an"
2020.lrec-1.409,R15-1049,1,0.825877,"egree possible for a number of reasons. Firstly, more reliable sources have since been compiled for the analysis of nonstandard Slovene, in particular the Janes Corpus of Internet Slovene (Fišer et al., 2018). Secondly, the number of nonstandard texts in Gigafida 1.0 was relatively small to begin with and thus not representative for the analysis of nonstandard linguistic features. Finally, corpus texts were not assigned any metadata on their degree of standardness, which made it impossible to exclude them from queries. Non-standard texts were identified automatically using a regression model (Ljubešić et al., 2015), where paragraphs were first assigned standardness values between 1 (standard) and 3 (highly non-standard). This provided a more precise overview of the distribution of nonstandardness within a text. Each text was thus assigned a vector of values, one for each paragraph. This distribution of values for each text was then compared with the distribution of values within the entire corpus by applying the Kolmogorov-Smirnov test. All texts with a statistically significant deviation from the distribution in the entire corpus and with a mean value of linguistic nonstandardness higher than the corpu"
2020.parlaclarin-1.6,J92-4003,0,0.327198,"Missing"
2020.parlaclarin-1.6,W17-1406,1,0.894148,"Missing"
2020.parlaclarin-1.6,W19-3704,0,0.0287743,"Missing"
2020.parlaclarin-1.6,K18-2016,0,0.0139923,"y debates is that they are essentially transcriptions of spoken language produced in controlled and regulated circumstances. For this reason, they are rich in invaluable (sociodemographic) meta-data. They are also easily available under various Freedom of Information Acts set in place to enable informed participation by the public and to improve effective functioning of democratic systems, making the datasets even more valuable for researchers with heterogeneous backgrounds. This has motivated a number of national as well as international initiatives (for an overview, see Fiˇser and Lenardiˇc (2018)) to compile, process and analyze parliamentary corpora. They are available for most European countries, with the UK’s Hansard Corpus being the largest (1.6 billion tokens) and spanning the longest time period (1803–2005) while corpora from other countries are significantly smaller (most comprise between 10 and 100 million tokens) and cover significantly shorter periods (mostly from the 1970s onward). 2. Corpus compilation and structure In the design of siParl corpus, we attempted to satisfy the following desiderata: 28 Level Legislative periods Sessions Days MPs Speakers Speeches Words Senten"
dzeroski-etal-2000-morphosyntactic,W96-0102,1,\N,Missing
dzeroski-etal-2000-morphosyntactic,A00-2013,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,P98-1050,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,C98-1049,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,A92-1018,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,H92-1022,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,A92-1021,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,P98-1080,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,C98-1077,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,J95-4004,0,\N,Missing
dzeroski-etal-2000-morphosyntactic,P98-1081,1,\N,Missing
dzeroski-etal-2000-morphosyntactic,C98-1078,1,\N,Missing
dzeroski-etal-2006-towards,cmejrek-etal-2004-prague,0,\N,Missing
dzeroski-etal-2006-towards,J93-2004,0,\N,Missing
dzeroski-etal-2006-towards,C00-2143,0,\N,Missing
dzeroski-etal-2006-towards,P04-1060,0,\N,Missing
dzeroski-etal-2006-towards,simov-etal-2002-building,0,\N,Missing
dzeroski-etal-2006-towards,erjavec-2004-multext,1,\N,Missing
dzeroski-etal-2006-towards,erjavec-2006-english,1,\N,Missing
erjavec-2004-multext,bauman-etal-2004-migrating,1,\N,Missing
erjavec-2004-multext,A00-2013,0,\N,Missing
erjavec-2004-multext,P98-1050,0,\N,Missing
erjavec-2004-multext,C98-1049,0,\N,Missing
erjavec-2004-multext,W02-0808,0,\N,Missing
erjavec-2004-multext,W03-2904,1,\N,Missing
erjavec-2006-english,A00-1031,0,\N,Missing
erjavec-2006-english,erjavec-fiser-2006-building,1,\N,Missing
erjavec-2006-english,dzeroski-etal-2006-towards,1,\N,Missing
erjavec-2006-english,erjavec-2004-multext,1,\N,Missing
erjavec-2010-multext,sharoff-etal-2008-designing,1,\N,Missing
erjavec-2010-multext,kemps-snijders-etal-2008-isocat,0,\N,Missing
erjavec-2010-multext,P98-1050,0,\N,Missing
erjavec-2010-multext,C98-1049,0,\N,Missing
erjavec-2010-multext,W02-0808,0,\N,Missing
erjavec-2010-multext,erjavec-2004-multext,1,\N,Missing
erjavec-2010-multext,erjavec-etal-2010-jos,1,\N,Missing
erjavec-2012-goo300k,sanchez-marco-etal-2010-annotation,0,\N,Missing
erjavec-2012-goo300k,W11-0415,0,\N,Missing
erjavec-2012-goo300k,W12-1001,1,\N,Missing
erjavec-2012-goo300k,erjavec-etal-2010-jos,1,\N,Missing
erjavec-2012-goo300k,W11-1505,1,\N,Missing
erjavec-etal-2000-concede,P98-1050,1,\N,Missing
erjavec-etal-2000-concede,C98-1049,1,\N,Missing
erjavec-etal-2004-making,E03-1043,0,\N,Missing
erjavec-etal-2004-making,popescu-belis-etal-2002-electronic,0,\N,Missing
erjavec-etal-2010-jos,erjavec-2010-multext,1,\N,Missing
erjavec-etal-2010-jos,dzeroski-etal-2006-towards,1,\N,Missing
erjavec-etal-2010-jos,erjavec-krek-2008-jos,1,\N,Missing
erjavec-fiser-2006-building,W02-0808,0,\N,Missing
erjavec-fiser-2006-building,erjavec-2004-multext,1,\N,Missing
erjavec-krek-2008-jos,sharoff-etal-2008-designing,1,\N,Missing
erjavec-krek-2008-jos,A00-1031,0,\N,Missing
erjavec-krek-2008-jos,erjavec-fiser-2006-building,1,\N,Missing
erjavec-krek-2008-jos,verdonik-etal-2004-creating,0,\N,Missing
erjavec-krek-2008-jos,erjavec-2004-multext,1,\N,Missing
fiser-etal-2014-slowcrowd,sagot-fiser-2012-cleaning,1,\N,Missing
javorsek-erjavec-2010-experimental,erjavec-2004-multext,1,\N,Missing
javorsek-erjavec-2010-experimental,erjavec-krek-2008-jos,1,\N,Missing
javorsek-erjavec-2010-experimental,tamburini-2004-building,0,\N,Missing
L16-1242,agic-ljubesic-2014-setimes,1,0.905996,"Missing"
L16-1242,W13-2408,1,0.919568,"Missing"
L16-1242,C12-1015,0,0.072104,"Missing"
L16-1242,P07-2053,0,0.366009,"Missing"
L16-1242,kobylinski-2014-polita,0,0.0458155,"Missing"
L16-1242,R15-1050,1,0.882971,"Missing"
L16-1242,W96-0213,0,0.536528,"However, we extend their approach by using a sequential tagger and inspecting many additional features. 3. The Dataset 4. Tagger Features and Evaluation In this section we describe the feature selection process for our tagger. For extracting the features we use our own Python code while we train our models using CRFsuite (Okazaki, 2007). We close this section with an evaluation of the final tagger. During the feature selection process we discriminate between two sets of features. The first, the core feature set, consists of features that were traditionally proven to work well for PoS tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). On the second, the experimental feature set, we ran a large number of experiments in the quest for a (near-to) optimal feature set for the given language (family). 4.1. The Core Feature Set The core feature set consists of the following features: For Slovene a number or resources are available as open datasets in the CLARIN.SI1 repository. For our experiments we used ssj500k 1.3 (Krek et al., 2013), a 500k word corpus manually annotated with context-disambiguated MSDs (and lemmas) and the Sloleks morphological lexicon 1.2 (Dobrovoljc et al., 2015) which contains abou"
L16-1242,E09-1087,0,0.0612501,"Missing"
L16-1242,P14-5003,0,0.0821697,"Missing"
L16-1242,N03-1033,0,0.00954978,"their approach by using a sequential tagger and inspecting many additional features. 3. The Dataset 4. Tagger Features and Evaluation In this section we describe the feature selection process for our tagger. For extracting the features we use our own Python code while we train our models using CRFsuite (Okazaki, 2007). We close this section with an evaluation of the final tagger. During the feature selection process we discriminate between two sets of features. The first, the core feature set, consists of features that were traditionally proven to work well for PoS tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). On the second, the experimental feature set, we ran a large number of experiments in the quest for a (near-to) optimal feature set for the given language (family). 4.1. The Core Feature Set The core feature set consists of the following features: For Slovene a number or resources are available as open datasets in the CLARIN.SI1 repository. For our experiments we used ssj500k 1.3 (Krek et al., 2013), a 500k word corpus manually annotated with context-disambiguated MSDs (and lemmas) and the Sloleks morphological lexicon 1.2 (Dobrovoljc et al., 2015) which contains about 100,000 lemmas with the"
L16-1242,C12-1170,0,0.0511636,"Missing"
L16-1573,W11-2123,0,0.0839554,"Missing"
L16-1573,W14-0405,1,0.889826,"Missing"
L16-1573,ljubesic-etal-2014-tweetcat,1,0.901883,"Missing"
L16-1573,R15-1049,1,0.901157,"Missing"
L16-1573,W02-2021,0,0.889319,"Missing"
L16-1573,D15-1275,0,0.0399206,"Missing"
L16-1573,W98-1504,0,0.311496,"Missing"
L16-1573,tufis-ceausu-2008-diac,0,0.0661917,"Missing"
L16-1573,P94-1013,0,0.196687,"Missing"
L18-1210,hinrichs-krauwer-2014-clarin,0,0.165251,"rn are newspaper, parliamentary, CMC (computer-mediated communication), and parallel corpora. We focus on their presentation within the infrastructure, their metadata in terms of size, temporal coverage, annotation, accessibility and license, and discuss current problems. Keywords: language resources, research infrastructure, open science, digital humanities and social sciences, CLARIN 1. Introduction CLARIN is a European Research Infrastructure that has been established to support the accessibility of language resources and technologies to researchers from the Humanities and Social Sciences (Krauwer and Hinrichs, 2014). CLARIN’s vision, mission and design are aimed at findability, accessibility, interoperability and re-usability of its resources, tools and services to support researchers in the Humanities and Social Sciences (SSH) (de Jong et al., 2018; De Smedt et al., 2018). At the time of writing, CLARIN has 20 member and 2 observer countries which provide numerous language resources and tools through certified data centres. Access to these resources is enhanced by the Virtual Language Observatory (VLO) portal which enables searching for resources and provides a uniform display of highly-granular Compone"
L18-1210,van-uytvanck-etal-2012-semantic,0,0.330742,"Missing"
lee-etal-2004-towards,callmeier-etal-2004-deepthought,0,\N,Missing
lee-etal-2004-towards,E95-1025,0,\N,Missing
lee-etal-2004-towards,W03-0802,0,\N,Missing
lee-etal-2004-towards,P03-1014,0,\N,Missing
lee-etal-2004-towards,C94-2144,0,\N,Missing
lee-etal-2004-towards,P03-2019,0,\N,Missing
lee-etal-2004-towards,kasper-etal-2004-integrated,0,\N,Missing
lee-etal-2004-towards,2002.jeptalnrecital-long.6,1,\N,Missing
ljubesic-etal-2014-tweetcat,W10-0513,0,\N,Missing
ljubesic-etal-2014-tweetcat,W14-0405,1,\N,Missing
ljubesic-etal-2014-tweetcat,baroni-bernardini-2004-bootcat,0,\N,Missing
R15-1049,N13-1037,0,0.0510739,"Missing"
R15-1049,I11-1100,0,0.0306528,"Missing"
R15-1049,P11-2008,0,0.320414,"Missing"
R15-1049,D12-1039,0,0.063609,"Missing"
R15-1049,1981.tc-1.7,0,0.844586,"Missing"
R15-1049,ljubesic-etal-2014-tweetcat,1,0.813486,"Missing"
R15-1049,I13-1041,0,0.0267248,"Missing"
sharoff-etal-2008-designing,J93-2004,0,\N,Missing
sharoff-etal-2008-designing,P98-1080,0,\N,Missing
sharoff-etal-2008-designing,C98-1077,0,\N,Missing
sharoff-etal-2008-designing,A00-1031,0,\N,Missing
sharoff-etal-2008-designing,gimenez-marquez-2004-svmtool,0,\N,Missing
sharoff-etal-2008-designing,feldman-etal-2006-cross,1,\N,Missing
sharoff-etal-2008-designing,erjavec-2004-multext,1,\N,Missing
sharoff-etal-2008-designing,W04-3229,1,\N,Missing
steinberger-etal-2006-jrc,J93-1004,0,\N,Missing
steinberger-etal-2006-jrc,moore-2002-fast,0,\N,Missing
steinberger-etal-2006-jrc,P06-2035,0,\N,Missing
steinberger-etal-2006-jrc,2005.mtsummit-papers.11,0,\N,Missing
steinberger-etal-2006-jrc,civera-juan-2006-bilingual,0,\N,Missing
steinberger-etal-2006-jrc,erjavec-2004-multext,1,\N,Missing
W01-1503,P98-1004,0,0.0150984,"corpus by comparing sentence lengths in characters by dynamic timewarping. The program assumes that hard boundaries are correctly aligned and performs alignment on soft boundaries. It is freely available with C source code distribution.  The Twente Word Aligner (Hiemstra, 1998) The program constructs a bilingual lexicon from a parallel sentence aligned corpus. The translations are ranked according to computed confidence. The system uses statistical measures and works for single words (tokens) only. It is available under the GNU General Public License and is written in C.  PLUG Word Aligner (Ahrenberg et al., 1998) The system integrates a set of modules for knowledge-lite approaches to word alignment, with various possibilities to change configuration and to adapt the system to other language pairs and text types. The system takes a parallel sentence aligned corpus as input and produces a list of word and phrase correspondences in the text (link instances) and additionally a bilingual lexicon from these instances (type links). It is available by a license agreement which is free of charge for non-commercial purposes. Distribution is available, in binary form only, for Linux and MS Windows. 5 Conclusions"
W01-1503,A00-1031,0,0.00931467,"rrently contains only a few sample entries, which, nevertheless, exemplify the kinds of software that are to be most relevant for inclusion into the catalogue:  tools that at least one TELRI partner has experience in using and that the partner is willing to support for new users  tools that are available free of cost, at least for academic purposes and, preferably, are open source  tools that are language independent or adapt easily to new languages  tools that are primarily meant for corpus processing At present, the catalogue lists the following tools:  The morpho-syntactic tagger TnT (Brants, 2000) A robust and very efficient statistical partof-speech tagger that is trainable on different languages and on virtually any tagset. It is available by a license agreement which is free of charge for non-commercial purposes. Distribution is available, in binaries only, for Linux and SunOS/Solaris. Figure 1: The TELRI Catalogue HTML form *name *task author affiliation street city country version language *description licres liccom restrict source url binary url platform os impl interface homepage doc url doc lang *helpline = Name of product = Task of product = Name(s) of author(s) = Name of comp"
W01-1503,declerck-etal-2000-new,0,0.0636943,"ble to the wider research and educational community. While the TRACTOR archives already offer a number of tools, the longer term objective is to offer a more substantial catalogue of corpus and lexicon processing software. Furthermore, the software itself is not necessarily available directly from TRACTOR, which would also have a more formalised structure and a well-defined process of updating and presenting its entries. A closely related initiative and model for this effort is the “The Natural Language Software Registry” of the ACL hosted at DFKI, a new edition of which was released in 2000 (Declerck et al., 2000). While the ACL registry offers a much larger array of tools, the TELRI catalogue should have the advantage that each entry also contains a pointer to the TELRI member who is able to offer advice on installing and using the tool in question. Other related catalogues on the Web are the CTI’s Guide to Digital Resources (http://info.ox.ac.uk/ctitext/resguide/) which has a section on Text Analysis Tools and Techniques. However, it does not seem to be maintained any longer. The Summer Institute of Linguistics (http://www.sil.org/computing/catalog/) also hosts a repository containing more than 60 pi"
W01-1503,P01-1040,0,0.0137607,"nd the stylesheet mechanism for display; Section 4 lists the current contents of the catalogue, while Section 5. gives some conclusions and outlines plans for its expansion and further maintenance. 2 Catalogue Format The overall encoding chosen for the catalogue was DocBook, an SGML/XML DTD primarily used for encoding computer manuals and other technical documentation. Choosing an SGML/XML framework follows a similar strand of research in annotating linguistic resources, as exemplified in the XML version of the Corpus Encoding Standard (Nancy et al., 2000) and in work on syntactic annotation (Nancy and Romary, 2001). An advantage of XML is the possibility of further standardisation by the use of related recommendations, i.e. the XML Stylesheet Language. DocBook has a large user base and is well documented: a reference book has been published and is available on-line (Walsh, 1999) for browsing or downloading. There is also an interesting public initiative utilising DocBook, namely the Linux Documentation Project, LDP (http://www.linuxdoc.org/), which is working on developing free, high quality documentation for the GNU/Linux operating system. Because DocBook is an application of SGML, and, more recently,"
W01-1503,ide-etal-2000-xces,0,0.0721899,".e. the Web form interface for input, editorial policy, and the stylesheet mechanism for display; Section 4 lists the current contents of the catalogue, while Section 5. gives some conclusions and outlines plans for its expansion and further maintenance. 2 Catalogue Format The overall encoding chosen for the catalogue was DocBook, an SGML/XML DTD primarily used for encoding computer manuals and other technical documentation. Choosing an SGML/XML framework follows a similar strand of research in annotating linguistic resources, as exemplified in the XML version of the Corpus Encoding Standard (Nancy et al., 2000) and in work on syntactic annotation (Nancy and Romary, 2001). An advantage of XML is the possibility of further standardisation by the use of related recommendations, i.e. the XML Stylesheet Language. DocBook has a large user base and is well documented: a reference book has been published and is available on-line (Walsh, 1999) for browsing or downloading. There is also an interesting public initiative utilising DocBook, namely the Linux Documentation Project, LDP (http://www.linuxdoc.org/), which is working on developing free, high quality documentation for the GNU/Linux operating system. Be"
W01-1503,C98-1004,0,\N,Missing
W03-2904,P98-1050,0,0.215895,"Missing"
W03-2904,dzeroski-etal-2000-morphosyntactic,1,0.832959,"allel corpus; lexical resources (Ide et al., 1998); and tool resources for the seven languages. One of the objectives of M ULTEXT -East has been to make its resources freely available for research purposes. In the scope of the TELRI concerted action the results of M ULTEXT -East have been extended with several new languages. This edition is now available via the TELRI Research Archive of Computational Tools and Resources, at http://www.tractor.de/. Following the TELRI release, the M ULTEXT East resources have been used in a number of studies and experiments, e.g., (Tufi¸s, 1999; Hajiˇc, 2000; Džeroski et al., 2000). In the course of such work, errors and inconsistencies were discovered in the M ULTEXT -East specifications and data, most of which were subsequently corrected. But because this work was done at different sites and in different manners, the encodings of the resources had begun to drift apart. The EU Copernicus project C ONCEDE , Consortium for Central European Dictionary Encoding, which ran from ’98 to ’00 and comprised most of the same partners as M ULTEXT -East, offered the possibility to bring the versions back on a common footing. Although C ONCEDE was primarily devoted to machine readab"
W03-2904,erjavec-etal-2000-concede,1,0.795265,"d inconsistencies were discovered in the M ULTEXT -East specifications and data, most of which were subsequently corrected. But because this work was done at different sites and in different manners, the encodings of the resources had begun to drift apart. The EU Copernicus project C ONCEDE , Consortium for Central European Dictionary Encoding, which ran from ’98 to ’00 and comprised most of the same partners as M ULTEXT -East, offered the possibility to bring the versions back on a common footing. Although C ONCEDE was primarily devoted to machine readable dictionaries and lexical databases (Erjavec et al., 2000), one of its workpackages did consider the integration of the dictionary data with the M ULTEXT -East corpus. In the scope of this workpackage, the corrected morphosyntactically annotated corpus was normalised and re-encoded. This release of the M ULTEXT East resources (Erjavec, 2001a; Erjavec, 2001b) contains the revised and expanded morphosyntactic specifications, the revised lexica, and the significantly corrected and re-encoded 1984 corpus. In Table 1, we give all these connected resources by language, type and release. The ones marked by T belong to the TELRI edition, and those with C to"
W03-2904,A00-2013,0,0.126704,"Missing"
W03-2904,C94-1097,0,0.436931,"h. 1 Duško Vitas Faculty of Mathematics University of Belgrade vitas@matf.bg.ac.yu Introduction The mid-nineties saw — to a large extent via EU projects — the rapid development of multilingual language resources and standards for human language technologies. However, while the development of resources, tools, and standards was well on its way for EU languages, there had been no comparable efforts for the languages of Central and Eastern Europe. The M ULTEXT -East project (Multilingual Text Tools and Corpora for Eastern and Central European Languages) was a spin-off of the EU M ULTEXT project (Ide and Véronis, 1994); it developed standardised language resources for six languages (Dimitrova et al., 1998): Bulgarian, Czech, Estonian, Hungarian, Romanian, and Slovene, as well as for English, the ’hub’ language of the project. The main results of the project were an annotated multilingual corpus (Erjavec and Ide, 1998), comprising a speech corpus, a comparable corpus and a parallel corpus; lexical resources (Ide et al., 1998); and tool resources for the seven languages. One of the objectives of M ULTEXT -East has been to make its resources freely available for research purposes. In the scope of the TELRI con"
W03-2904,tadic-2002-building,1,0.799841,"e tags for the auxiliary verbs and a hybrid POS tag referring to family names and adjectives derived from names; the pronoun adverbials were made more fine-grained etc. This tagset is being used for the annotation of the BulTreeBank Text Archive. The lexicon is encoded as a regular grammar within the CLaRK system (Simov et al., 2001). 3.2 Croatian The Croatian specifications were compiled soon after the M ULTEXT -East project ended in 1997, using the project’s Final report as the template. These specifications are used in the PoS-tagging and lemmatisation of the Croatian National Corpus (Tadi´c, 2002). It was also selected for the format of MSDs accompanying word-forms in Croatian Morphological Lexicon (Tadi´c, 2003) which is conformant with M ULTEXT -East lexica. 3.3 Czech The morphological specifications for Czech were developed exclusively for the M ULTEXT -East project but the authors had already had some experience with the first draft of morphological specifications for Czech which is now thoroughly described in (Hajiˇc, 2002). These specifications and the resulting tagset developed by Hajiˇc are nowadays used as a standard for morphological and morphosyntactic annotations of the maj"
W03-2904,W03-2906,1,0.829387,"ronoun adverbials were made more fine-grained etc. This tagset is being used for the annotation of the BulTreeBank Text Archive. The lexicon is encoded as a regular grammar within the CLaRK system (Simov et al., 2001). 3.2 Croatian The Croatian specifications were compiled soon after the M ULTEXT -East project ended in 1997, using the project’s Final report as the template. These specifications are used in the PoS-tagging and lemmatisation of the Croatian National Corpus (Tadi´c, 2002). It was also selected for the format of MSDs accompanying word-forms in Croatian Morphological Lexicon (Tadi´c, 2003) which is conformant with M ULTEXT -East lexica. 3.3 Czech The morphological specifications for Czech were developed exclusively for the M ULTEXT -East project but the authors had already had some experience with the first draft of morphological specifications for Czech which is now thoroughly described in (Hajiˇc, 2002). These specifications and the resulting tagset developed by Hajiˇc are nowadays used as a standard for morphological and morphosyntactic annotations of the majority of Czech corpora, especially the 100 million word corpus of synchronic Czech developed within the Czech National"
W03-2904,C98-1049,0,\N,Missing
W11-0402,I08-7013,0,0.0188419,"o enhance the consistency of linguistic annotations is to make use of crosslinguistic meta schemes or annotation standards, such as EAGLES (Leech and Wilson, 1996). The problem is that these enforce the use of the same categories across multiple languages, and this may be inappropriate for historically and geographically unrelated languages. For specific linguistic and historical regions, the application of standardization approaches has, however, been performed with great success, e.g., for Western (Leech and Wilson, 1996) and Eastern Europe (Erjavec et al., 2003) or the Indian subcontinent (Baskaran et al., 2008). 11 Proceedings of the Fifth Law Workshop (LAW V), pages 11–20, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics In this paper, we illustrate differences and commonalities of both approaches by creating an OWL/DL terminology repository from the MULTEXT-East (MTE) specifications (Erjavec et al., 2003; Erjavec, 2010), which define features for the morphosyntactic level of linguistic description, instantiate them for 16 languages and provide morphosyntactic tagsets for these languages. The specifications are a part of the MTE resources, which also include lexic"
W11-0402,brants-hansen-2002-developments,0,0.0373834,"notation mapping The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Zeman, 2008). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais ‘baroque-style palace’ in Neues Palais lit. ‘new palace’, a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants and Hansen, 2002). ISOcat (like other terminological repositories) does currently not provide the corresponding hybrid category, so that Palais is to be linked to both properNoun/DC-1371 and commonNoun/DC1256 if the information carried by the original annotation is to be preserved. Contractions pose similar problems: English gonna combines going (PTB tag VBG, Marcus et al., 1994) and to (TO). If whitespace tokenization is applied, both tags need to be assigned to the same token. A related problem is the representation of ambiguity: The SUSANNE (Sampson, 1995) tag ICSt applies to English after both as a preposi"
W11-0402,I08-1051,0,0.190073,"Missing"
W11-0402,buyko-etal-2008-ontology,1,0.689512,"n Slavic languages is expressed differently: For Czech, reduced adjectives are marked by Formation=nominal, but for Polish by Definiteness=short-art. In the ontology, such redundancies are resolved by owl:equivalentClass statements, marked by ≡ in Fig. 3. 5 Summary and Discussion We have described the semi-automatic creation of an ontological model of the MTE morphosyntactic specifications for 16 different languages. Such a model may be fruitfully applied in various ways, e.g., within an NLP pipeline that uses ontological specifications of annotations rather than their string representations (Buyko et al., 2008; Hellmann, 2010). The ontological modeling may serve also as a first step towards an ontology-based documentation of the annotations within a corpus query system (Rehm et al., 2007; Chiarcos et al., 2008), or even the ontological modeling of entire corpora (Burchardt et al., 2008; Hellmann et al., 2010) and lexicons (Martin et al., 2009). As an interesting side-effect of the OWL conversion of the entire body of MTE resources, they could be easily integrated with existing lexical-semantic resources as Linked Data, e.g., OWL/RDF versions of WordNet (Gangemi et al., 2003), which are currently be"
W11-0402,W03-2904,1,0.875673,"Missing"
W11-0402,erjavec-2010-multext,1,0.801985,"guistic and historical regions, the application of standardization approaches has, however, been performed with great success, e.g., for Western (Leech and Wilson, 1996) and Eastern Europe (Erjavec et al., 2003) or the Indian subcontinent (Baskaran et al., 2008). 11 Proceedings of the Fifth Law Workshop (LAW V), pages 11–20, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics In this paper, we illustrate differences and commonalities of both approaches by creating an OWL/DL terminology repository from the MULTEXT-East (MTE) specifications (Erjavec et al., 2003; Erjavec, 2010), which define features for the morphosyntactic level of linguistic description, instantiate them for 16 languages and provide morphosyntactic tagsets for these languages. The specifications are a part of the MTE resources, which also include lexicons and an annotated parallel corpus that use these morphosyntactic tagsets. The encoding of the MTE specifications follows the Text Encoding Initiative Guidelines, TEI P5 (TEI Consortium, 2007), and this paper concentrates on developing a semi-automatic procedure for converting them from TEI XML to OWL. While TEI is more appropriate for authoring th"
W11-0402,C94-1097,0,0.277953,"Missing"
W11-0402,zeman-2008-reusable,0,0.0903705,"As we argue below, different design decisions in the terminology repositories make it necessary to use a linking formalism that is capable of expressing both disjunctions and conjunctions of concepts. For this reason, we propose the application of OWL/DL. By representing the MTE specifications, the repositories, and the linking between them as separate OWL/DL models, we follow the architectural concept of the OLiA architecture (Chiarcos, 2008), see Sect. 5. 3.2 Annotation mapping The classic approach to link annotations with reference concepts is to specify rules that define a direct mapping (Zeman, 2008). It is, however, not always possible to find a 1:1 mapping. One problem is conceptual overlap: A common noun may occur as a part of a proper name, e.g., German Palais ‘baroque-style palace’ in Neues Palais lit. ‘new palace’, a Prussian royal palace in Potsdam/Germany. Palais is thus both a proper noun (in its function), and a common noun (in its form). Such conceptual overlap is sometimes represented with a specialized tag, e.g., in the TIGER scheme (Brants and Hansen, 2002). ISOcat (like other terminological repositories) does currently not provide the corresponding hybrid category, so that"
W11-0402,J93-2004,0,\N,Missing
W11-1505,erjavec-krek-2008-jos,1,0.824193,"mmatization has been performed. We currently use a set of about 100 transcription patterns, which were obtained by corpus inspection, using a dedicated concordancer. 4 Vaam also supports approximate matching based on edit distance, useful for identifying (and correcting) OCR errors; we have, however, not yet made use of this functionality. 2.5 Tagging For tagging words in the text with their context disambiguated morphosyntactic annotations we use TnT (Brants, 2000), a fast and robust tri-gram tagger. The tagger has been trained on jos1M, the 1 million word JOS corpus of contemporary Slovene (Erjavec and Krek, 2008), and is also given a large background lexicon extracted from the 600 million word FidaPLUS reference corpus of contemporary Slovene (Arhar and Gorjanc, 2007). 2.6 Lemmatisation Automatic lemmatisation is a core application for many language processing tasks. In inflectionally rich languages assigning the correct lemma (base form) to each word in a running text is not trivial, as, for instance, Slovene adjectives inflect for gender, number and case (3x3x6) with a complex configuration of endings and stem modifications. For our lemmatiser we use CLOG (Manandhar et al., 1998, Erjavec and Džerosk"
W11-1505,sanchez-marco-etal-2010-annotation,0,0.0575162,"the text and then attempts to transcribe the archaic words to their modern day equivalents. For here on, the text is tagged and lemmatised using the models for modern Slovene. Such an approach is not new, as it straightforwardly follows from a situation where good language models are available for contemporary language, but not for its historical variants. The focus of the research in such cases is on the mapping from historical words to modern ones, and such approaches have already been attempted for other languages, e.g. for English (Rayson et al. 2007), German (Pilz et al. 2008), Spanish (Sánchez-Marco et al. 2010) and Icelandic (Rögnvaldsson and Helgadóttir, 2008). These studies have mostly concentrated on mapping historical variants to modern words or evaluating PoS tagging accuracy and have dealt with Germanic and Romance languages. This paper discusses the complete annotation process, including lemmatisation, and treats a Slavic language, which has substantially different morphology; in Slovene, words belong to complex inflectional paradigms, which makes tagging and lemmatisation models quite complex, esp. for unknown words. The paper also discusses structural annotations supported by the tool, whic"
W12-1001,W10-1835,0,0.0445592,"Missing"
W12-1001,W11-1505,1,0.442983,"latter option is more interesting for our case, as TEI files can already be structurally and linguistically annotated. Zip files are also supported, which enables uploading large datasets with many separate files. The Slovene corpora are encoded in TEI, and each corpus file contains the transcription of a single page, together with the link to its facsimile image. The page is also annotated with paragraphs, line breaks, etc. Such annotation is imported into CoBaLT but not displayed or modified, and appears again only in the export. The texts in our project were first automatically annotated (Erjavec, 2011): each text was sentence segmented and tokenised into words. Punctuation symbols (periods, commas, etc.) and white-spaces were preserved in the annotation so the original text and layout can be reconstructed from the annotated text. Each word form was assigned its modern-day equivalent, its PoS tag and modern day lemma. 3 4 <entry> <form type=&quot;lemma&quot;> <orth type=&quot;hypothetical&quot;>glasnik</orth> <gramGrp> <gram type=&quot;msd&quot;>Ncm</gram> <gram type=&quot;PoS&quot;>Noun</gram> <gram type=&quot;Type&quot;>common</gram> <gram type=&quot;Gender&quot;>masculine</gram> </gramGrp> <gloss>samoglasnik</gloss> <bibl>kontekst, Pleteršnik</bib"
W12-1001,erjavec-2012-goo300k,1,0.832245,"ate lemmatisation, which is especially useful for highly inflecting languages as it abstracts away from the inflectional variants of words, thereby enabling better text searching. To develop such resources, a good editor is needed that caters to the peculiarities of historical texts. Preferably it would combine the production of annotated corpora and corpus-based lexica. This paper presents CoBaLT, a Web-based editor which has already been used for developing language resources for several languages. We describe it within the framework of developing a gold-standard annotated reference corpus (Erjavec, 2012) and a large lexicon of historical Slovene. This paper is structured as follows: in the next section we describe the implementation and functionality of CoBaLT. In Section 3 we present the input and output corpus and lexicon formats, in particular from the perspective of our project. In Section 4 we compare existing tools serving a similar purpose to CoBaLT and discuss the advantages and disadvantages of the CoBaLT environment. The last section summarizes and lists our conclusions. Abstract This paper describes a Web-based editor called CoBaLT (Corpus-Based Lexicon Tool), developed to construc"
W12-1001,rognvaldsson-etal-2012-icelandic,0,\N,Missing
W13-2409,J03-1002,0,0.0060212,"0 Pairs 3,199 3,638 10,033 16,029 Ident 493 1,708 8,281 9,834 Diff 2,706 1,930 1,752 6,195 4.1 Table 4: Size of Lf oo lexicon. SMT models consist of two main components: the translation model, which is trained on bilingual data, and the language model, which is trained on monolingual data of the target language. We use the word pairs from Lgoo to train the translation model, and the modern Slovene words from Lgoo to train the language model.7 As said above, we test the model on the word pairs of Lf oo . The experiments have been carried out with the tools of the standard SMT pipeline: GIZA++ (Och and Ney, 2003) for alignment, Moses (Koehn et al., 2007) for phrase extraction and decoding, and IRSTLM (Federico et al., 2008) for language modelling. After preliminary experimentation, we settled on the following parameter settings: mentioned, contains no hnform, mformi pairs already appearing in Lgoo . This setting simulates the task of an existing system receiving a new text to modernize. The lexicons used in the experiment contain entries with nform, mform, and the per-slice frequencies of the pair in the corpus from which the lexicon was derived, as illustrated in the example below: benetkah benetkah"
W13-2409,erjavec-2012-goo300k,1,0.847567,"con, Lf oo is derived from the foo3M corpus and, as 5 The corpora used in our experiments are slightly smaller than the originals: the text from two books and one newspaper issue has been removed, as the former contain highly idiosyncratic ways of spelling words, not seen elsewhere, and the latter contains a mixture of the Bohoriˇc and contemporary alphabet, causing problems for word form normalization. The texts older than 1750 have also been removed from goo300k, as such texts do not occur in foo3M, which is used for testing our approach. 6 A previous version of this corpus is described in (Erjavec, 2012). 2 The dataset used in this paper is available under the CC-BY-NC-SA license from http://nl.ijs.si/imp/ experiments/bsnlp-2013/. 3 Sloleks is encoded in LMF and available under the CCBY-NC-SA license from http://www.slovenscina. eu/. 4 The data for historical Slovene comes from the IMP resources, see http://nl.ijs.si/imp/. 59 Period 18B 19A 19B Σ Pairs 6,305 18,733 30,874 45,810 Ident 2,635 12,223 24,597 31,160 Diff 3,670 6,510 6,277 14,650 OOV 703 2,117 4,759 7,369 create three different models for the three time periods of old Slovene (18B, 19A, 19B). The first experiment follows a supervis"
W13-2409,W11-0415,0,0.142797,"dge. We present the relevant lexicons and two experiments. In one, we use a lexicon of historical word– contemporary word pairs and a list of contemporary words; in the other, we only use a list of historical words and one of contemporary ones. We show that both methods produce significantly better results than the baseline. 1 2 Related Work The most common approach to modernizing historical words uses (semi-) hand-constructed transcription rules, which are then applied to historical words, and the results filtered against a contemporary lexicon (Baron and Rayson, 2008; Scheible et al., 2010; Scheible et al., 2011); such rules are often encoded and used as (extended) finite state automata (Reffle, 2011). An alternative to such deductive approaches is the automatic induction of mappings. For example, Kestemont et al. (2010) use machine learning to convert 12th century Middle Dutch word forms to contemporary lemmas. Word modernization can be viewed as a special case of transforming cognate words from one language to a closely related one. This task has traditionally been performed with stochastic transducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character"
W13-2409,R11-1018,0,0.0365609,"Missing"
W13-2409,W02-0902,0,0.039742,"ion (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it enables better full-text search in cultural heritage digital libraries, makes old texts more understandable to today’s readers and significant"
W13-2409,2009.eamt-1.3,0,0.0391946,"utomata (Reffle, 2011). An alternative to such deductive approaches is the automatic induction of mappings. For example, Kestemont et al. (2010) use machine learning to convert 12th century Middle Dutch word forms to contemporary lemmas. Word modernization can be viewed as a special case of transforming cognate words from one language to a closely related one. This task has traditionally been performed with stochastic transducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character-based statistical machine translation (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fiš"
W13-2409,C04-1137,0,0.0389962,"ducers or HMMs trained on a set of cognate word pairs (Mann and Yarowsky, 2001). More recently, character-based statistical machine translation (C-SMT) (Vilar et al., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it ena"
W13-2409,N03-2016,0,0.0396247,"., 2007; Tiedemann, 2009) has been proposed as an alternative approach to translating words between closely related languages and has been shown to outperform stochastic transducers on the task of name transliteration (Tiedemann and Nabende, 2009). For the related task of matching cognate pairs in bilingual non-parallel corpora, various languageindependent similarity measures have been proposed on the basis of string edit distance (Kondrak and Dorr, 2004). Cognate word matching has been shown to facilitate the extraction of translation lexicons from comparable corpora (Koehn and Knight, 2002; Kondrak et al., 2003; Fišer and Ljubeši´c, 2011). Introduction A lot of recent work deals with detecting and matching cognate words in corpora of closely related language varieties. This approach is also useful for processing historical language (Piotrowski, 2012), where historical word forms are matched against contemporary forms, thus normalizing the varied and changing spelling of words over time. Such normalization has a number of applications: it enables better full-text search in cultural heritage digital libraries, makes old texts more understandable to today’s readers and significantly improves further te"
W13-2409,N01-1020,0,\N,Missing
W13-2409,W07-0705,0,\N,Missing
W13-2409,P07-2045,0,\N,Missing
W17-1406,L16-1248,1,0.831303,"s well as the features of tokens in the surrounding context. The conversion is performed in several iterations over tokens of a sentence, starting with the conversion of existing JOS-annotated constructions, and followed by different heuristics for annotation of previously un-annotated phenomena, including rules for root identification and punctuation attachment. In the last stage of the conversion, some mistakes and inconsistencies identified in the original ssj200k corpus are also corrected. 6 Some of these relations, however, do occur in the manually annotated Spoken Slovenian UD Treebank (Dobrovoljc and Nivre, 2016). punct parataxis ccomp punct advmod discourse advmod advmod cop obj punct punct nsubj advmod xcomp det ˇ vidimo , kajne , kako nam Kajn postaja bliˇzji , kako nismo zaman njegovi potomci . Ze Sb Atr Conj AdvO Atr Obj Obj Conj Atr Figure 2: The comparison of UD (above) and JOS (below) annotation schemes in terms of complexity of dependency trees. All unanalysed tokens in JOS have been annotated as direct dependents of the root element. acl case cop case nummod amod nmod V Ardenih je zablestel Aerts , ki mu je bila to sˇele cˇ etrta zmaga v 7 - letni karieri profesionalca . Atr Atr Atr Atr Atr"
W17-1406,dzeroski-etal-2006-towards,1,0.781343,"Missing"
W17-1406,erjavec-etal-2010-jos,1,0.945986,"Missing"
W17-1406,L16-1242,1,0.8021,"Missing"
W17-1406,de-marneffe-etal-2014-universal,0,0.0532727,"Missing"
W17-1406,L16-1262,0,0.0459901,"Missing"
W17-1406,petrov-etal-2012-universal,0,0.0609844,"tation of individual languages or specific language resources, that have prevented direct comparisons of annotated data and the performance of the resultant NLP tools. To overcome this heterogeneity inhibiting both theoretical and engineering advancements in the field, the Universal Dependencies1 annotation scheme provides a universal inventory of morphological and syntactic categories and guidelines for their application, while also allowing for language-specific extensions, when necessary (Nivre, 2015). The scheme is based on previous similar standardization projects (Marneffe et al., 2014; Petrov et al., 2012; Zeman, 2008), and has recently been substantially modified to its second version (UD v2), following five successive releases of treebanks pertaining to UD v1 (Nivre et al., 2016). In 1 http://universaldependencies.org/ 33 Dependency Treebanks for Slovenian The Slovenian UD Treebank represents the third generation of syntactically annotated corpora in Slovenian. The first was the Slovene Dependency Treebank (Dˇzeroski et al., 2006), based on the Prague Dependency Treebank (PDT) annotation scheme (Hajiˇcov´a et al., 1999) and consisting of approximately 30,000 tokens taken from the Slovenian c"
W17-1406,zeman-2008-reusable,0,0.0394415,"languages or specific language resources, that have prevented direct comparisons of annotated data and the performance of the resultant NLP tools. To overcome this heterogeneity inhibiting both theoretical and engineering advancements in the field, the Universal Dependencies1 annotation scheme provides a universal inventory of morphological and syntactic categories and guidelines for their application, while also allowing for language-specific extensions, when necessary (Nivre, 2015). The scheme is based on previous similar standardization projects (Marneffe et al., 2014; Petrov et al., 2012; Zeman, 2008), and has recently been substantially modified to its second version (UD v2), following five successive releases of treebanks pertaining to UD v1 (Nivre et al., 2016). In 1 http://universaldependencies.org/ 33 Dependency Treebanks for Slovenian The Slovenian UD Treebank represents the third generation of syntactically annotated corpora in Slovenian. The first was the Slovene Dependency Treebank (Dˇzeroski et al., 2006), based on the Prague Dependency Treebank (PDT) annotation scheme (Hajiˇcov´a et al., 1999) and consisting of approximately 30,000 tokens taken from the Slovenian component of th"
W17-1410,W13-2408,1,0.89407,"Missing"
W17-1410,W16-2606,0,0.030665,"Missing"
W17-1410,J92-4003,0,0.54702,"Missing"
W17-1410,P07-1033,0,0.262882,"Missing"
W17-1410,N13-1037,0,0.0793345,"cribes the tagging experiments we performed, Section 5 reports on the error analysis of the results and Section 6 gives some conclusions and directions for further research. 2 Related Work Early work on PoS tagging social media was, as usual, mostly focused on English (Gimpel et al., 2011; Owoputi et al., 2013). Recently there has been more work on other languages, primarily through the organization of shared tasks, such the EmpiriST on German (Beißwenger et al., 2016) and PoSTWITA on Italian.1 There are two main approaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Ze"
W17-1410,P11-2008,0,0.414792,"Missing"
W17-1410,P07-2053,0,0.238028,"Missing"
W17-1410,N15-1135,0,0.0120933,"roaches to processing non-standard data: normalization and domain adaptation (Eisenstein, 2013). Most approaches nowadays follow the domain adaptation path al1 http://corpora.ficlit.unibo.it/ PoSTWITA/ Proceedings of the 6th Workshop on Balto-Slavic Natural Language Processing, pages 60–68, c Valencia, Spain, 4 April 2017. 2017 Association for Computational Linguistics though the literature still lacks a detailed comparison of the two strategies on specific tasks. In domain adaptation there are, again, two main strategies (Horsmann and Zesch, 2015): adding more labeled data (Daum´e III, 2007; Hovy et al., 2015) and incorporating external knowledge (Owoputi et al., 2013). Horsmann and Zesch (2015) show that (1) adding manually annotated in-domain data is highly effective (but costly) and (2) adding out-of-domain training data or machine-tagged data is less effective than adding more external knowledge, especially word clustering information. The contribution of our paper is the following: First, we perform the first experiments in annotating Slavic non-standard texts with part-of-speech and morphosyntactic information, therefore dealing with several hundreds of tags. Next, we investigate the impact o"
W17-1410,R15-1049,1,0.891607,"Missing"
W17-1410,L16-1242,1,0.893411,"Missing"
W17-1410,L16-1676,1,0.915116,"Missing"
W17-1410,N13-1039,0,0.107593,"Missing"
W17-1410,C14-1168,0,0.045415,"Missing"
W17-1410,P10-1040,0,0.189388,"Missing"
W17-2901,W15-2913,0,0.0713611,"Missing"
W17-2901,D11-1120,0,0.101339,"arious languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, retweeted tweets). By following the three types, mean, med and var, we encode the following distributions in our feature space: retweet count, favorite count, posting hour, day of week the twe"
W17-2901,L16-1258,0,0.0841781,"average daily number of tweets, overall number of tweets, number of tweets the user has favorited, number of followers, number of friends, the ratio of follower to friend numbers, number of lists the user is on, whether the user has a background image defined, whether the user has the default profile image, whether the user has a profile description, whether the user has a location defined, and red, green and blue color component intensity (two-digit hexadecimal code from the RGB color definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use on"
W17-2901,P15-1073,0,0.0203148,"f-words model when training and testing on the same language, it regularly outperforms the bag-of-words model when applied to different languages, showing very stable results across various languages. Finally we perform a comparative analysis of feature effect sizes across the six languages and show that differences in our features correspond to cultural distances. 1 Introduction Gender prediction is a well-established task in author profiling, useful for a series of downstream analyses (Schler et al., 2006; Schwartz et al., 2013; Bamman et al., 2014) as well as predictive model improvements (Hovy, 2015). Most existing work on predicting gender focuses on exploiting the linguistic production of the users (Koppel et al., 2003; Schler et al., 2006; Kucukyilmaz et al., 2006; Burger et al., 2011; Miller et al., 2012; Rangel et al., 2016), just rarely using nonlinguistic information such as metadata (Plank 1 Proceedings of the Second Workshop on Natural Language Processing and Computational Social Science, pages 1–6, c Vancouver, Canada, August 3, 2017. 2017 Association for Computational Linguistics 2 The Dataset working hours, posting during weekends, truncated tweets, favorited tweets, quotes, r"
W17-2901,W17-1601,0,0.0138108,"definition) of the user’s text and background color. In our experiments we fully rely on the TwiSty corpus (Verhoeven et al., 2016) which was developed for research in author profiling. It contains personality (MBTI) and gender annotations for a total of 18,168 authors posting in German, Italian, Dutch, French, Portuguese or Spanish. The manual gender annotations in the TwiSty corpus are based on the user’s name, handle, description and profile picture and follow the performative view of gender, i.e., that gender is discriminated by performances that respond to societal norms or conventions (Larson, 2017). The corpus is distributed in the form of Twitter user IDs and specific tweet IDs of that user. In this work we use only the user IDs and their gender and language annotations to collect timelines of users through the Twitter API. For each user we collect up to 3,200 tweets (API restriction) and discard users with less than 100 tweets. By doing so we collected 45 million tweets for 16,156 users across the six languages. 3 4 The Features Experimental Setup In this section we outline the setup of our gender classification experiments, whose results we report in Section 5.1. We train models base"
W17-3007,N16-2013,0,0.0293422,"ew exceptions for Dutch (van Halteren and Oostdijk, 2013) and German (Ross et al., 2017). State-of-the-art approaches tackle this task through supervised machine learning (Sood et al., 2012; Dadvar et al., 2013). For this, of course, manually annotated datasets are needed. A major limitation of most existing work in this area is that it is based on an ad-hoc treatment of SUD classification in natural language processing and a lack of detailed guidelines that are necessary for reliable annotation (Ross et al., 2017). Annotated datasets have started to emerge only recently (Nobata et al., 2016; Waseem and Hovy, 2016), but nevertheless they lack precise documentation on data annotation and make use of only very basic In this paper we present the legal framework, dataset and annotation schema of socially unacceptable discourse practices on social networking platforms in Slovenia. On this basis we aim to train an automatic identification and classification system with which we wish contribute towards an improved methodology, understanding and treatment of such practices in the contemporary, increasingly multicultural information society. 1 Introduction In Slovenia, Socially Unacceptable Discourse (SUD) pract"
W18-5116,N16-2013,0,0.0486845,"r-generated content, there is increased pressure to manage inappropriate online content with (semi)automated methods. The research community is by now well aware of the multiple faces of inappropriateness in on-line communication, which preclude the use of simple vocabulary-based approaches, and are therefore turning to more robust machine learning methods (Pavlopoulos et al., 2017). These, however, require training data. Currently available datasets of inappropriate on-line communication are primarily datasets of English, such as a Twitter dataset annotated for racist and sexist hate speech (Waseem and Hovy, 2016)1 , the Wikimedia Toxicity Data Set (Wulczyn et al., 2017)2 , the Hate Speech Identifica3 https://data.world/crowdflower/ hate-speech-identification 4 https://github.com/sfu-discourse-lab/ SOCC 5 https://github.com/UCSM-DUE/IWG_ hatespeech_public 6 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/ gazzetta-comments-dataset.tar.gz 7 https://straintek.wediacloud.net/ static/gazzetta-comments-dataset/README. txt 1 https://github.com/ZeerakW/hatespeech https://figshare.com/projects/ Wikipedia_Talk/16731 2 124 Proceedings of the Second Workshop on Abusive Language Online (ALW2), p"
W18-5116,E17-2068,0,0.0753478,"Missing"
W18-5116,D17-1117,0,0.256497,"ja Fiˇser Faculty of Arts, University of Ljubljana Aˇskerˇceva cesta 2, 1000 Ljubljana, Slovenia darja.fiser@ff.uni-lj.si Abstract tion dataset containing tweets annotated as hate speech, offensive language, or neither (Davidson et al., 2017)3 , and the SFU Opinion and Comment Corpus consisting of online opinion articles and their comments annotated for toxicity4 . Datasets in other languages have recently also started to emerge, with a German Twitter dataset focused on the topic of refugees in Germany (Ross et al., 2017)5 and a Greek Sport News Comment dataset containing moderation metadata (Pavlopoulos et al., 2017)6 . In this paper we present two new and large datasets of news comments, one in Slovene, and one in Croatian. Apart from the texts, they also contain various metadata, the primary being whether the comment was removed by the site administrators. Given the sensitivity of the content, we publish the datasets in full-text form, but with user metadata semi-anonymised and the comment content encrypted via a simple character replacement method using a random, undisclosed bijective mapping, similar to the encryption method applied to the Gazzetta Greek Sport News Comments dataset7 introduced in Pavl"
W19-8004,W15-5301,1,0.851002,"uage resources, as well. This is especially true for morphological annotation (lemmatization, PoS tagging and morphological feature prediction), as many languages employ much larger morphology-annotated corpora than the costly (sub)corpora annotated for syntax, as well as morphological lexicons, essential for high-quality processing of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology an"
W19-8004,W17-1406,1,0.910535,"This is especially true for morphological annotation (lemmatization, PoS tagging and morphological feature prediction), as many languages employ much larger morphology-annotated corpora than the costly (sub)corpora annotated for syntax, as well as morphological lexicons, essential for high-quality processing of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology and syntax prediction. Usin"
W19-8004,erjavec-etal-2010-jos,1,0.779291,"development, the content and the availability of the extended UD resources for Slovenian and Croatian, namely the larger training sets for UD morphology (the ssj500k and hr500k corpora) and the large-scale UD-compliant lexicons of inflected forms (Sloleks and hrLex). Given the methodological differences in resource development for both languages due to divergent project frameworks and scopes, we present the resources by language rather than type. However, a brief quantitative overview and comparison is given at the end of the section. 2.1 Slovenian resources Both the ssj500k training corpus (Erjavec et al., 2010) and the Sloleks lexicon of inflected forms (Dobrovoljc et al., 2017b) adopt the JOS morphosyntactic annotation scheme (Erjavec and Krek, 2008), compatible with MULTEXT-East morphosyntactic specifications (Erjavec, 2012), which define the part-of-speech categories for Slovene, their morphological features (attributes) and values, and their mapping to morphosyntactic descriptions (MSDs).1 An automatic rule-based mapping from JOS to UD part-of-speech tags and features had already been developed as part of the original Slovenian UD Treebank conversion from the syntactically annotated subset of th"
W19-8004,erjavec-krek-2008-jos,1,0.705466,"D morphology (the ssj500k and hr500k corpora) and the large-scale UD-compliant lexicons of inflected forms (Sloleks and hrLex). Given the methodological differences in resource development for both languages due to divergent project frameworks and scopes, we present the resources by language rather than type. However, a brief quantitative overview and comparison is given at the end of the section. 2.1 Slovenian resources Both the ssj500k training corpus (Erjavec et al., 2010) and the Sloleks lexicon of inflected forms (Dobrovoljc et al., 2017b) adopt the JOS morphosyntactic annotation scheme (Erjavec and Krek, 2008), compatible with MULTEXT-East morphosyntactic specifications (Erjavec, 2012), which define the part-of-speech categories for Slovene, their morphological features (attributes) and values, and their mapping to morphosyntactic descriptions (MSDs).1 An automatic rule-based mapping from JOS to UD part-of-speech tags and features had already been developed as part of the original Slovenian UD Treebank conversion from the syntactically annotated subset of the ssj500k corpus (Dobrovoljc et al., 2017a), with the conversion scripts now publicly available at the CLARIN.SI GitHub repository.2 The large"
W19-8004,L16-1498,0,0.0717467,"Missing"
W19-8004,T87-1035,0,0.338695,"Missing"
W19-8004,W19-3704,1,0.743771,"Missing"
W19-8004,L16-1242,1,0.827209,"of languages with complex morphology. Examples of such cases are Croatian and Slovenian, two South Slavic languages with rich inflection. Their official UD releases include the conversions of the largest syntactically annotated corpora available for each language (Agić and Ljubešić, 2015; Dobrovoljc et al., 2017a), however, other manually created resources, such as the larger morphologically annotated corpora (Ljubešić et al., 2018b; Krek et al., 2019) and inflectional lexicons (Ljubešić, 2019; Dobrovoljc et al., 2019), have also been developed to support the development of related NLP tools (Ljubešić and Erjavec, 2016; Grčar et al., 2012) in the past. The aim of this paper is to present the conversion of these resources to the UD formalism and explore their potential contribution to the state-of-the-art in UD processing for both languages, from lemmatization to morphology and syntax prediction. Using the stanfordnlp tool, we investigate the impact of newly available data on all three tasks by (1) retraining the tagging and lemmatization models on larger training sets and (2) performing a simple lexicon lookup intervention in the lemmatization procedure. This paper is structured as follows. We first briefly"
W19-8004,L16-1676,1,0.863421,"les for pronouns and determiners, adverbs, numbers and the negated auxiliary.5 The only non-automatic part of the mapping was the resolution of the category of abbreviations from MULTEXT-East to the corresponding parts-of-speech. The resulting hr500k corpus was part of the initial release of hr500k (v1.0) and was published under CC BY-SA 4.0 (Ljubešić et al., 2018b). 2.2.2 hrLex inflectional lexicon The hrLex inflectional lexicon (Ljubešić, 2019) is currently the largest inflectional lexicon of Croatian. The process of semi-automatically building the hrLex inflectional lexicon is described in Ljubešić et al. (2016). 3 https://reldi.spur.uzh.ch 4 https://github.com/nljubesi/hr500k/blob/master/mte5-udv2.mapping 5 https://github.com/vukbatanovic/SETimes.SR/blob/master/msd_mapper.py The mapping of the MULTEXT-East tags that were initially present in the lexicon to the UPOS and FEATS layers was performed by applying the mapping that was used to map the hr500k training corpus to these layers, without the need for the manual mapping. The UD information became part of the hrLex lexicon with version 1.3 (Ljubešić, 2019), when the lexicon was published under the CC BY-SA 4.0 license. The lexicon is published as a"
W19-8004,petrov-etal-2012-universal,0,0.134172,"Missing"
W19-8004,K18-2016,0,0.0565309,"o explain the large difference in the ambiguity of the two lexicons, as possessive adjectives and proper nouns have a somewhat higher ambiguity than the remainder of the lexicon: possessive adjectives have an ambiguity of 4.68, and proper nouns of 3.78. 3 Experiment setup 3.1 Tool We perform experiments on morphosyntactic tagging, lemmatization and dependency parsing via the stanfordnlp tool, one of the best-performing systems in the CoNLL shared task in 2018 (Zeman et al., 2018) with code released and a vivid development community.6 The details on the implementation of the tool are given in (Qi et al., 2018). The tool assumes that morphosyntactic tagging is performed first, producing the UPOS and FEATS annotation layer. Next, lemmatization is performed by using the UPOS (but not FEATS) predictions. Finally, parsing is performed by exploiting all previously predicted layers (UPOS, FEATS and LEMMA). We investigate the impact of additional data on all three tasks by (1) retraining morphosyntactic tagging and lemmatization models with more data and (2) performing a simple intervention in the lemmatization procedure so that the lexicon lookup is not performed over the training data only, but the exter"
W19-8004,K18-2001,0,0.0500359,"in Figure 1. In any case, the different principles in the creation of the lexicons account for the larger size of hrLex lexicon and also explain the large difference in the ambiguity of the two lexicons, as possessive adjectives and proper nouns have a somewhat higher ambiguity than the remainder of the lexicon: possessive adjectives have an ambiguity of 4.68, and proper nouns of 3.78. 3 Experiment setup 3.1 Tool We perform experiments on morphosyntactic tagging, lemmatization and dependency parsing via the stanfordnlp tool, one of the best-performing systems in the CoNLL shared task in 2018 (Zeman et al., 2018) with code released and a vivid development community.6 The details on the implementation of the tool are given in (Qi et al., 2018). The tool assumes that morphosyntactic tagging is performed first, producing the UPOS and FEATS annotation layer. Next, lemmatization is performed by using the UPOS (but not FEATS) predictions. Finally, parsing is performed by exploiting all previously predicted layers (UPOS, FEATS and LEMMA). We investigate the impact of additional data on all three tasks by (1) retraining morphosyntactic tagging and lemmatization models with more data and (2) performing a simpl"
