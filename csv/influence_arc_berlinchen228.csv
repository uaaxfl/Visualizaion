2019.rocling-1.21,O16-3003,1,0.883244,"Missing"
2019.rocling-1.31,C10-2032,0,0.0225609,"Missing"
2019.rocling-1.31,P18-1063,0,0.0351622,"Missing"
2019.rocling-1.31,O18-1012,1,0.847784,"Missing"
2020.ijclclp-1.2,Q17-1010,0,0.0458031,"Missing"
2020.ijclclp-1.2,P16-1046,0,0.0770021,"Missing"
2020.ijclclp-1.2,J81-4005,0,0.677214,"Missing"
2020.ijclclp-1.2,D15-1180,0,0.0421221,"Missing"
2020.ijclclp-1.2,D15-1044,0,0.136385,"Missing"
2020.ijclclp-1.2,P17-1099,0,0.0728481,"Missing"
2020.ijclclp-1.2,P17-1108,0,0.0506162,"Missing"
2020.ijclclp-1.2,P17-1101,0,0.0328825,"Missing"
2020.rocling-1.17,N18-1202,0,0.0222119,"Missing"
2020.rocling-1.17,N19-1423,0,0.0578288,"Missing"
2020.rocling-1.17,D13-1170,0,0.00870726,"Missing"
2020.rocling-1.17,D16-1264,0,0.0334264,"Missing"
2020.rocling-1.17,P03-1021,0,0.0162266,"Missing"
2020.rocling-1.17,J05-1003,0,0.205019,"Missing"
2020.rocling-1.17,W19-2304,0,0.0466677,"Missing"
2020.rocling-1.17,2020.acl-main.240,0,0.0395042,"Missing"
2020.rocling-1.17,2020.acl-main.76,0,0.0206239,"Missing"
2020.rocling-1.24,O00-1007,0,0.248652,"Missing"
2020.rocling-1.24,N19-1423,0,0.0550531,"Missing"
2020.rocling-1.24,D19-1410,0,0.0468874,"Missing"
2020.rocling-1.24,N18-1202,0,0.104381,"Missing"
2020.rocling-1.24,C10-3014,0,0.0924427,"Missing"
2020.rocling-1.27,N16-1174,0,0.118658,"Missing"
2020.rocling-1.31,2020.acl-main.716,0,0.0299874,"Missing"
2020.rocling-1.31,2020.acl-main.348,0,0.0532743,"Missing"
2020.rocling-1.31,2018.ijclclp-2.2,1,0.751035,"Missing"
C16-1035,P07-1056,0,0.0477674,"7 0.819 0.852 0.835 0.833 0.852 0.862 Kitchen 0.824 0.858 0.860 0.857 0.884 0.860 0.871 0.884 0.890 Average 0.790 0.826 0.824 0.813 0.842 0.826 0.831 0.842 0.853 Table 1: Experimental results on sentiment analysis achieved by the proposed EV model and other baseline features, including unigrams, bigrams, PCA, and the combinations. 4 4.1 Experimental Setup & Results Experiments on the EV Model for Sentiment Analysis At the outset, we evaluate the proposed EV model on the sentiment polarity classification task. Four widely-used benchmark multi-domain sentiment datasets are used in this study 1 (Blitzer et al., 2007). They are product reviews taken from Amazon.com in four different domains: Books, DVD, Electronics, and Kitchen. Each of the reviews, ranging from Star-1 to Star-5, were rated by a customer. The reviews with Star-1 and Star-2 were labelled as Negative, and those with Star-4 and Star-5 were labeled as Positive. Each of the four datasets contains 1,000 positive reviews, 1,000 negative reviews, and a number of unlabeled reviews. Labeled reviews in each domain are randomly split up into ten folds (with nine folds serving as the training set and the remaining one as the test set). All of the follo"
C16-1035,P15-2136,0,0.026887,"r, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth ela"
C16-1035,W14-1504,0,0.0251256,"ph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph embedding methods infer"
C16-1035,N10-1134,0,0.0322079,"rpus/MATBN-corpus.htm https://catalog.ldc.upenn.edu/LDC2011T13 365 deficiency of the EV model in spoken document summarization; we thus believe that it is more suitable for use in spoken content processing. In the last set of experiments, we compare the results mentioned above with that of several wellpracticed, state-of-the-art unsupervised summarization methods, including the graph-based methods (i.e., the Markov random walk (MRW) method (Wan and Yang, 2008) and the LexRank method (Erkan and Radev, 2004)) and the combinatorial optimization methods (i.e., the submodularity-based (SM) method (Lin and Bilmes, 2010) and the integer linear programming (ILP) method (Riedhammer et al., 2010)). Among them, the ability of reducing redundant information has been aptly incorporated into the submodular-based method and the ILP method. Interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), and Nenkova and McKeown (2011) for comprehensive reviews and new insights into the major methods that have been developed and applied with good success to a wide range of spoken document summarization tasks. The results are also listed in Table 3. Several noteworthy observations can be drawn from the r"
C16-1035,P08-1054,0,0.142534,"on metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM) (Gong and Liu, 2001) and the LexRank (Erkan and Radev, 2004) method), the best peer systems (including Peer T, Peer 26, and Peer 65) participating DUC evaluations, and the recently elaborated DNN-based systems (including CNN and PriorSum) (Cao et al., 2015). Owing to the space limitation, we omit the detailed introduction to these summarization methods; interested readers may refer to Penn and Zhu (2008), Liu and Hakkani-Tur (2011), Nenkova and McKeown (2011), and Cao et al., (2015) for more in-depth elaboration. It is worthy to note that the proposed EV model, the two baseline systems, and the best peer systems are unsupervised methods, while the DNN-based systems are supervised ones. The experimental results are listed in Table 2. Several interesting observations can be concluded from the results. First, the proposed EV model outperforms VSM by a large margin in all cases, and performs comparably to other well-designed unsupervised summarization methods. Second, both LexRank and EV (with th"
C16-1035,P14-1146,0,0.0395017,"ent a given paragraph (or sentence and document) by simply taking an average over the word embeddings corresponding to the words occurring in the paragraph. By doing so, this thread of methods has recently This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 358 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 358–368, Osaka, Japan, December 11-17 2016. enjoyed substantial success in many NLP-related tasks (Collobert and Weston, 2008; Tang et al., 2014; Kageback et al., 2014). Although the empirical effectiveness of word embedding methods has been proven recently, the composite representation for a paragraph (or sentence and document) is a bit queer. Theoretically, paragraph-based representation learning is expected to be more suitable for such tasks as information retrieval, sentiment analysis and document summarization (Huang et al., 2013; Le and Mikolov, 2014; Palangi et al., 2015), to name but a few. However, to the best of our knowledge, unsupervised paragraph embedding has been largely under-explored on these tasks. Classic paragraph"
C16-1035,O05-3004,1,0.749163,"Missing"
C16-1035,N15-1136,0,0.0213232,"and were grouped into various thematic clusters. The summary length was limited to 100 words for both DUC 2001 and DUC 2002, and 665 bytes for DUC 2004. The general background information was inferred from the LDC Gigaword corpus 3 (including Associated Press Worldstream (AP), New York Times Newswire Service (NYT), and Xinhua News Agency (XIN)). The most common belief in the document summarization community is that relevance and redundancy are two key factors for generating a concise summary. In this paper, we leverage a density peaks clustering summarization method (Rodriguez and Laio, 2014; Zhang et al., 2015), which can take both relevance and redundancy information into account at the same time. That is, a concise summary for a given document set can be automatically generated through a one-pass process instead of an iterative process. Recently, the summarization method has proven its empirical effectiveness (Zhang et al., 2015). For evaluation, we adopt the widely-used automatic evaluation metric ROUGE (Lin, 2003), and take ROUGE-1 and ROUGE-2 (in F-scores) as the main measures following Cao et al., (2015). We compare the proposed EV model with two baseline systems (the vector space model (VSM)"
D14-1156,H91-1057,0,0.152474,"Missing"
D14-1156,P07-1085,0,0.0519061,"Missing"
D14-1156,P11-5003,0,0.0325849,"aneous (or in-domain) corpus. Finally, the enhanced query model (that is P(w|H) in speech recognition) can be estimated by RM, SMM, RSMM or QMM, and further combined with the background n-gram (e.g., trigram) language model to form an adaptive language model to guide the speech recognition process. 4.2 Speech Summarization On the other hand, extractive speech summarization aims at producing a concise summary by selecting salient sentences or paragraphs from the original spoken document according to a predefined target summarization ratio (Carbonell and Goldstein, 1998; Mani and Maybury, 1999; Nenkova and McKeown, 2011; Liu and Hakkani-Tur, 2011). Intuitively, this task could be framed as an ad-hoc IR problem, where the spoken document is treated as an information need and each sentence of the document is regarded as a candidate information unit to be retrieved according to its relevance to the information need. Therefore, KLM can be used to quantify how close the document D and one of its sentences S are: the closer the sentence model P(w|S) to the document model P(w|D), the more PRM ( w |Q )   D r DTop P( w |Dr )  DrDTop P(Q |Dr ) P( Dr )   L    P ( w |D r ) P ( Dr |Q )    wV  Dr D"
D14-1156,W01-0100,0,\N,Missing
H01-1050,1998.amta-tutorials.5,0,\N,Missing
H01-1050,W98-1005,0,\N,Missing
H01-1050,J95-4004,0,\N,Missing
H01-1050,A97-1029,0,\N,Missing
I11-1149,P02-1051,0,0.0232528,"ation was previously viewed as a translation problem. Virga and Khudanpur (2003) applied SMT models to translate English names into Chinese characters. Knight and Graehl (1997) proposed a generative transliteration model for Japanese and English using finite state transducers. Meng et al. (2001) developed an English-Chinese Named Entity transliteration technique using pronunciation lexicon and phonetic mapping rules. Li et al. (2004) proposed direct orthographic mapping with a joint source-channel model for proper name transliteration. There have also been other approaches to transliteration. Al-Onaizan and Knight (2002) used verification as a stepping stone to transliteration. More recently, the JHU Workshop (2008) reported on the importance of the similarity scoring method and conducted a comparative study on the various scoring methods for name transliterations. Data harvesting is another way of improving transliteration. Additional data source such as comparable corpora (Klementiev and Roth, 2006; Kuo et al., 2007; Sproat et al., 2006) and the web (Jiang et al., 2007) have also been explored to improve the performance. One of the vital building blocks in all of these approaches is a scoring component that"
I11-1149,J93-2003,0,0.0181926,"Missing"
I11-1149,H05-1012,0,0.0604787,"Missing"
I11-1149,2001.mtsummit-papers.68,0,0.0317017,"Missing"
I11-1149,P06-1010,0,0.0238169,"4) proposed direct orthographic mapping with a joint source-channel model for proper name transliteration. There have also been other approaches to transliteration. Al-Onaizan and Knight (2002) used verification as a stepping stone to transliteration. More recently, the JHU Workshop (2008) reported on the importance of the similarity scoring method and conducted a comparative study on the various scoring methods for name transliterations. Data harvesting is another way of improving transliteration. Additional data source such as comparable corpora (Klementiev and Roth, 2006; Kuo et al., 2007; Sproat et al., 2006) and the web (Jiang et al., 2007) have also been explored to improve the performance. One of the vital building blocks in all of these approaches is a scoring component that tests how likely a given pair of names in source and target languages is transliteration of each other. This is a key component and is the aspect we focus on in this work. We propose a method for transliteration verification that achieves the best EER compared to other approaches on the same dataset. Our work differentiates itself from the previous work in the following areas. We take the verification as a stand-alone prob"
I11-1149,W10-2403,0,\N,Missing
I11-1149,P04-1021,0,\N,Missing
I11-1149,W03-1508,0,\N,Missing
I11-1149,P02-1040,0,\N,Missing
I11-1149,P06-1103,0,\N,Missing
I11-1149,W10-2401,0,\N,Missing
I11-1149,N03-1017,0,\N,Missing
I11-1149,J03-1002,0,\N,Missing
I11-1149,J98-4003,0,\N,Missing
O04-1003,O05-3004,1,0.842674,"Missing"
O05-1025,1996.eamt-1.1,0,0.035684,"Missing"
O05-1025,H94-1097,0,0.0151821,"Missing"
O05-2001,H91-1057,0,0.165735,"Missing"
O06-4002,O05-2001,1,\N,Missing
O07-1006,O05-3004,1,0.84657,"Missing"
O08-5005,O05-2001,1,0.802658,"Missing"
O08-5005,O06-4002,1,0.57352,"Missing"
O09-1001,O05-3004,1,0.612342,"Missing"
O09-1014,J92-4003,0,0.29066,"Missing"
O09-1014,W97-0309,0,0.0819624,"Missing"
O10-1003,J05-1003,0,0.318778,"Missing"
O10-1003,W02-1001,0,0.203517,"Missing"
O10-1003,N03-1028,0,0.59055,"Missing"
O10-1003,P99-1069,0,0.114063,"Missing"
O10-1003,P03-1021,0,0.0166867,"Missing"
O10-1003,O05-3004,1,0.910458,"Missing"
O11-1001,J05-1003,0,0.0675241,"Missing"
O11-1001,O10-1003,1,0.552822,"Missing"
O11-1001,W02-1001,0,0.23192,"Missing"
O11-1001,N03-1028,0,0.168417,"Missing"
O11-1001,P03-1021,0,0.01456,"Missing"
O11-1001,O10-1005,1,0.807216,"Missing"
O11-1001,O05-3004,1,0.818006,"Missing"
O11-1001,N04-1023,0,0.0756256,"Missing"
O12-1001,O09-1019,1,0.651398,"Missing"
O12-1008,O05-3004,1,0.789837,"Missing"
O12-5005,O09-1019,1,0.8769,"Missing"
O13-1001,W97-0703,0,0.473494,"Missing"
O13-1001,P04-1085,0,0.0956261,"Missing"
O13-1001,P08-1054,0,0.0388536,"Missing"
O13-1001,N07-2054,0,0.0562749,"Missing"
O13-1014,O09-1004,0,0.0167471,"Missing"
O13-1014,O05-3004,1,0.27812,"Missing"
O14-1002,N10-1134,0,0.08846,"Missing"
O14-1002,P11-5003,0,0.0392463,"Missing"
O14-1002,P08-1054,0,0.0612,"Missing"
O14-1002,C10-1111,0,0.0751714,"Missing"
O14-1002,N07-2054,0,0.0711078,"Missing"
O14-5004,D14-1156,1,0.866523,"Missing"
O14-5004,C88-1071,0,0.587433,"Missing"
O14-5004,O05-3004,1,0.799651,"Missing"
O15-1001,O05-3004,1,0.74818,"Missing"
O15-1002,C88-1071,0,0.111829,"Missing"
O15-1002,O05-3004,1,0.783531,"Missing"
O15-1008,C10-2032,0,0.332579,"Missing"
O15-1008,Q13-1028,0,0.05379,"Missing"
O15-3004,mochizuki-okumura-2000-comparison,0,0.184053,"Missing"
O15-3004,W04-1013,0,0.0235877,"Missing"
O15-3004,W97-0707,0,0.294225,"Missing"
O15-3004,O05-3004,1,0.78245,"Missing"
O15-3005,O11-2001,1,0.521578,"Missing"
O15-3005,O09-1004,0,0.0128541,"Missing"
O16-1012,P11-5003,0,0.123639,"Missing"
O16-1012,D15-1229,0,0.0740231,"Missing"
O16-1012,D15-1044,0,0.0889799,"Missing"
O16-1012,K16-1028,0,0.0360188,"Missing"
O16-1012,D14-1179,0,0.0116194,"Missing"
O16-1012,D15-1166,0,0.0695141,"Missing"
O16-1024,C10-2032,0,0.295121,"Missing"
O16-1024,N04-1025,0,0.0686462,"Missing"
O16-1024,E09-1027,0,0.0550464,"Missing"
O16-1024,W08-0911,0,0.0683309,"Missing"
O16-1024,E09-2013,0,0.042636,"Missing"
O16-1024,W12-2207,0,0.468639,"Missing"
O16-1024,D14-1162,0,0.0807628,"Missing"
O16-3003,O09-1004,0,0.0757367,"Missing"
O16-3004,I08-2116,0,0.0837004,"Missing"
O16-3006,O05-3004,1,0.749315,"Missing"
O17-1011,C10-2032,0,0.0826494,"Missing"
O17-1011,E17-2068,0,0.063093,"Missing"
O17-2001,P04-1085,0,0.107934,"Missing"
O17-2001,N10-1134,0,0.105237,"Missing"
O17-3004,C10-2032,0,0.372694,"Missing"
O17-3004,W12-2207,0,0.0535841,"Missing"
O17-3004,D14-1181,0,0.00630887,"Missing"
O17-3004,P11-1031,0,0.0287908,"Missing"
O17-3004,O15-1008,1,0.893368,"Missing"
O18-1012,C10-2032,0,0.461477,"Missing"
O18-1012,O17-3004,1,0.869344,"Missing"
P10-1009,W04-1013,0,0.005397,"d. The difference value is defined as the difference between the minimum and maximum values of the spoken sentence, while the mean difference value is defined as the mean difference between a sentence and its previous sentence. Finally, the relevance feature (VSM score) is use to measure the degree of relevance for a sentence to the whole document (Gong and Liu, 2001). These features are outlined in Table 2, where each of them was further normalized to zero mean and unit variance. Performance evaluation For the assessment of summarization performance, we adopted the widely used ROUGE measure (Lin, 2004) because of its higher correlation with human judgments. It evaluates the quality of the summarization by counting the number of overlapping units, such as N-grams, longest common subsequences or skip-bigram, between the automatic summary and a set of reference summaries. Three variants of the ROGUE 6 6.1 Experimental results and discussions Baseline experiments In the first set of experiments, we evaluate the baseline performance of the LM and BC summarizers (cf. Sections 4.1 and 4.2), respectively. The corresponding results are detailed in Table 3, 84 BC LM Text Document (TD) ROGUE-1 ROUGE-2"
P10-1009,O05-3004,1,0.724519,"s S , respectively; the prior probability PS  and P S  are set to be equal in this research. To estimate PX j |S  and P X j |S  , several popular supervised classifiers (or summarizers), like BC or SVM, can be leveraged for this purpose. 4.3     Sim S i , S j  , (12) L Si , S j  1   SimS i , S '  1     S 'max Summ  5.1 (11) ~   Experimental setup Data The summarization dataset used in this research is a widely used broadcast news corpus collected by the Academia Sinica and the Public Television Service Foundation of Taiwan between November 2001 and April 2003 (Wang et al., 2005). Each story contains the speech of one studio anchor, as well as several field reporters and interviewees. A subset of 205 broadcast news docOnce the sentence generative model P D |S j , the sentence prior model PS j  and the loss function LSi , S j  have been properly estimated, the summary sentences can be selected iteratively by (8) according to a predefined target summarization ratio. However, as can be seen from (8), a new summary sentence is selected without considering the redundant information that is also 83 Kappa 0.400 measure were used to quantify the utility of the proposed me"
P10-1009,W06-1643,0,0.225508,"ing model that assigns a classification score (or a posterior probability) of being in the summary class to each sentence of a spoken document to be summarized; important sentences are subsequently ranked and selected according to these scores. To this end, several popular machine-learning methods could be utilized, like Bayesian classifier (BC) (Kupiec et al., 1999), Gaussian mixture model (GMM) (Fattah and Ren, 2009) , hidden Markov model (HMM) (Conroy and O'leary, 2001), support vector machine (SVM) (Kolcz et al., 2001), maximum entropy (ME) (Ferrier, 2001), conditional random field (CRF) (Galley, 2006; Shen et al., 2007), to name a few. Although such supervised summarizers are effective, most of them (except CRF) usually implicitly assume that sentences are independent of each other (the so-called “bag-of-sentences” assumption) and classify each sentence individually without leveraging the relationship among the sentences (Shen et al., 2007). Another major shortcoming of these summarizers is that a set of handcrafted document-reference summary exemplars are required for training the summarizers; however, such summarizers tend to limit their generalization capability and might not be readil"
P10-1009,N04-1022,0,0.0224243,"s, the performance of the unsupervised summarizers is usually worse than that of the supervised summarizers. Moreover, most of the unsupervised summarizers are constructed solely on the basis of the lexical information without considering other sources of information cues like discourse features, acoustic features, and so forth. 3 a*  arg min R ai |O . ai The notion of minimizing the Bayes risk has gained much attention and been applied with success to many natural language processing (NLP) tasks, such as automatic speech recognition (Goel and Byrne, 2000), statistical machine translation (Kumar and Byrne, 2004) and statistical information retrieval (Zhai and Lafferty, 2006). Following the same spirit, we formulate the extractive summarization task as a Bayes risk minimization problem. Without loss of generality, let us denote   Π as one of possible selection strategies (or state of nature) which comprises a set of indicators used to address the importance of each sentence Si in a document D to be summarized. A feasible selection strategy can be fairly arbitrary according to the underlying principle. For example, it could be a set of binary indicators denoting whether a sentence should be selected"
P10-1009,W04-3252,0,\N,Missing
P19-1439,E17-2026,0,0.0800118,"ng and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as Collobert et al. (2011). Luong et al. (2016) show promising results combining translation and parsing; Subramanian et al. (2018) benefit from multitask learning in sentence-to-vector encoding; and Bingel and Søgaard (2017) and Changpinyo et al. (2018) offer studies of when multitask learning is helpful for lower-level NLP tasks. 3 Task |Train| Task Type GLUE Tasks CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI 8.5K 67K 3.7K 364K 7K 393K 105K 2.5K 634 acceptability sentiment paraphrase detection paraphrase detection sentence similarity NLI QA (NLI) NLI coreference resolution (NLI) Outside Tasks Transfer Paradigms DisSent WT LM WT LM BWB MT En-De MT En-Ru Reddit SkipThought We consider two recent paradigms for transfer learning: pretraining and intermediate training. See Figure 1 for a graphical depiction. Pretraining"
P19-1439,S17-2001,0,0.0228104,"line is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train langua"
P19-1439,C18-1251,0,0.0459627,"Missing"
P19-1439,D14-1082,0,0.0210975,"Missing"
P19-1439,D17-1070,0,0.20309,"bservations suggest that while scaling up LM pretraining (as in Radford et al., 2019) is likely the most straightforward path to further gains, our current methods for multitask and transfer learning may be substantially limiting our results. 2 Related Work Work on reusable sentence encoders can be traced back at least as far as the multitask model of Collobert et al. (2011). Several works focused on learning reusable sentence-to-vector encodings, where the pretrained encoder produces a fixed-size representation for each input sentence (Dai and Le, 2015; Kiros et al., 2015; Hill et al., 2016; Conneau et al., 2017). More recent reusable sentence encoders such as CoVe (McCann et al., 2017) and GPT (Radford et al., 2018) instead represent sentences as sequences of vectors. These methods work well, but most use distinct pretraining objectives, and none offers a substantial investigation of the choice of objective like we conduct here. We build on two methods for pretraining sentence encoders on language modeling: ELMo and BERT. ELMo consists of a forward and backward LSTM (Hochreiter and Schmidhuber, 1997), the hidden states of which are used to produce a contextual vector representation for each token in"
P19-1439,N19-1423,0,0.616052,"ond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning"
P19-1439,I05-5002,0,0.0294281,"a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample"
P19-1439,N16-1162,0,0.0469202,"Missing"
P19-1439,J07-3004,0,0.0375747,"Missing"
P19-1439,P18-1031,0,0.0247575,"(left), and learn a target task model on top of the representations it produces (right). Middle (intermediate ELMo training): We train a BiLSTM on top of ELMo for an intermediate task (left). We then train a target task model on top of the intermediate task BiLSTM and ELMo (right). Bottom (intermediate BERT training): We fine-tune BERT on an intermediate task (left), and then fine-tune the resulting model again on a target task (right). limitation has prompted interest in pretraining for these encoders: The encoders are first trained on outside data, and then plugged into a target task model. Howard and Ruder (2018), Peters et al. (2018a), Radford et al. (2018), and Devlin et al. (2019) establish that encoders pretrained on variants of the language modeling task can be reused to yield strong performance on downstream NLP tasks. Subsequent work has homed in on language modeling (LM) pretraining, finding that such mod4465 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4465–4476 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics els can be productively fine-tuned on intermediate tasks like natural language inference bef"
P19-1439,P15-1162,0,0.0816432,"Missing"
P19-1439,N18-1038,0,0.0431129,"Missing"
P19-1439,P19-1441,0,0.176928,"sed to produce a contextual vector representation for each token in the inputted sequence. ELMo is adapted to target tasks by freezing the model weights and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically inves"
P19-1439,J93-2004,0,0.0711626,"Missing"
P19-1439,P19-1442,0,0.0239473,"n WMT14 English-German (Bojar et al., 2014) and WMT17 English-Russian (Bojar et al., 2017). We train SkipThought-style sequence-to-sequence (seq2seq) models to read a 1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 2 QNLI has been re-released with updated splits since the original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP"
P19-1439,N18-1202,0,0.787227,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,W19-4302,0,0.0321696,"sed to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced back at least as far as"
P19-1439,D18-1179,0,0.247572,"? Sentence-Level Pretraining Beyond Language Modeling Alex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2 R. Thomas McCoy,2 , Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6 Katherin Yu,5 Shuning Jin,7 Berlin Chen8 Benjamin Van Durme,2 Edouard Grave,5 Ellie Pavlick,3,4 and Samuel R. Bowman1 1 New York University, 2 Johns Hopkins University, 3 Brown University, 4 Google AI Language, 5 Facebook, 6 IBM, 7 University of Minnesota Duluth, 8 Swarthmore College Abstract Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo’s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target"
P19-1439,P18-2124,0,0.0384363,"Task Model BiLSTM Pretrained BiLSTM Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BiLSTM Intermediate Task-Trained BiLSTM ELMo Introduction State-of-the-art models in natural language processing (NLP) often incorporate encoder functions which generate a sequence of vectors intended to represent the in-context meaning of each word in an input text. These encoders have typically been trained directly on the target task at hand, which can be effective for data-rich tasks and yields human performance on some narrowlydefined benchmarks (Rajpurkar et al., 2018; Hassan et al., 2018), but is tenable only for the few tasks with millions of training data examples. This ∗ This paper supercedes “Looking for ELMo’s Friends: Sentence-Level Pretraining Beyond Language Modeling”, an earlier version of this work by the same authors. Correspondence to: alexwang@nyu.edu ❄ ❄ ❄ ELMo ❄ Input Text Input Text Intermediate Task Output Target Task Output Intermediate Task Model Target Task Model BERT Intermediate Task-Trained BERT Input Text Input Text Figure 1: Learning settings that we consider. Model components with frozen parameters are shown in gray and decorated"
P19-1439,P16-2022,0,0.0140103,"sion classifier. For seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset) we replace the classifier with a single-layer LSTM word-level decoder and initialize the hidden state with the [CLS] representation. For ELMo-style models, we use several model types: • Single-sentence classification tasks: We train a linear projection over the output states of the encoder, max-pool those projected states, and feed the result to an MLP. 4469 • Sentence-pair tasks: We perform the same steps on both sentences and use the heuristic feature vector [h1 ; h2 ; h1 · h2 ; h1 − h2 ] in the MLP, following Mou et al. (2016). When training target-task models on QQP, STS, MNLI, and QNLI, we use a cross-sentence attention mechanism similar to BiDAF (Seo et al., 2017). We do not use this mechanism in other cases as early results indicated it hurt transfer performance. • Seq2seq tasks (MT, SkipThought, pushshift.io Reddit dataset): We use a single-layer LSTM decoder where the hidden state is initialized with the pooled input representation. • Language modeling: We follow ELMo by concatenating forward and backward models and learning layer mixing weights. To use GLUE tasks for pretraining or intermediate training in a"
P19-1439,D13-1170,0,0.00874927,", 2019b). Accordingly, our pretraining and intermediate ELMo experiments include a baseline of a randomly initialized BiLSTM with no further training. This baseline is especially strong because our ELMo-style models use a skip connection from the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et"
P19-1439,W17-2625,0,0.0590097,"Missing"
P19-1439,P19-1452,1,0.863686,"lar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP is well studied, and again can be traced b"
P19-1439,I17-1100,1,0.867491,"Missing"
P19-1439,N18-1101,1,0.826868,"om the input of the encoder to the output, allowing the task-specific component to see the input representations, yielding a model similar to Iyyer et al. (2015). GLUE Tasks We use the nine tasks included with GLUE as pretraining and intermediate tasks: acceptability classification with CoLA (Warstadt et al., 2018); binary sentiment classification with SST (Socher et al., 2013); semantic similarity with the MSR Paraphrase Corpus (MRPC; Dolan and Brockett, 2005), Quora Question Pairs1 (QQP), and STS-Benchmark (STS; Cer et al., 2017); and textual entailment with the Multi-Genre NLI Corpus (MNLI Williams et al., 2018), RTE 1, 2, 3, and 5 (RTE; Dagan et al., 2006, et seq.), and data from SQuAD (QNLI;2 Rajpurkar et al., 2016) and the Winograd Schema Challenge (WNLI; Levesque et al., 2011) recast as entailment in the style of White et al. (2017). MNLI and QQP have previously been shown to be effective for pretraining in other settings (Conneau et al., 2017; Subramanian et al., 2018; Phang et al., 2018). Other tasks are included to represent a broad sample of labeling schemes commonly used in NLP. Outside Tasks We train language models on two datasets: WikiText-103 (WT; Merity et al., 2017) and Billion Word La"
P19-1439,1983.tc-1.13,0,0.144827,"Missing"
P19-1439,W18-3022,0,0.0192405,"he original release. We use the original splits. sentence from WT and predict the following sentence (Kiros et al., 2015; Tang et al., 2017). We train DisSent models to read two clauses from WT that are connected by a discourse marker such as and, but, or so and predict the the discourse marker (Jernite et al., 2017; Nie et al., 2019). Finally, we train seq2seq models to predict the response to a given comment from Reddit, using a previously existing dataset obtained by a third party (available on pushshift.io), comprised of 18M comment– response pairs from 2008-2011. This dataset was used by Yang et al. (2018) to train sentence encoders. Multitask Learning We consider three sets of these tasks for multitask pretraining and intermediate training: all GLUE tasks, all non-GLUE (outside) tasks, and all tasks. 5 Models and Experimental Details We implement our models using the jiant toolkit,3 which is in turn built on AllenNLP (Gardner et al., 2017) and on a public PyTorch implementation of BERT.4 Appendix A presents additional details. Encoder Architecture For both the pretraining and intermediate ELMo experiments, we process words using a pretrained character-level convolutional neural network (CNN) f"
P19-1439,W18-5448,1,0.914222,"s and only learning a set of task-specific scalar weights that are used to compute a linear combination of the LSTM layers. BERT consists of a pretrained Transformer (Vaswani et al., 2017), and is adapted to downstream tasks by fine-tuning the entire model. Follow-up work has explored parameterefficient fine-tuning (Stickland and Murray, 2019; Houlsby et al., 2019) and better target task adaptation via multitask fine-tuning (Phang et al., 2018; Liu et al., 2019), but work in this area is nascent. The successes of sentence encoder pretraining have sparked a line of work analyzing these models (Zhang and Bowman, 2018; Peters et al., 2018b; Tenney et al., 2019b; Peters et al., 2019; Tenney et al., 2019a; Liu et al., 2019, i.a.). Our work also attempts to better understand what is learned by pretrained encoders, but we study this question entirely through the lens of pretraining and fine-tuning tasks, rather than architectures or specific linguistic capabilities. Some of our experiments resemble those of Yogatama et al. (2019), who also empirically investigate transfer performance with limited amounts of data and find similar evidence of catastrophic forgetting. 4466 Multitask representation learning in NLP"
P19-1439,Q17-1027,1,0.847544,"Missing"
