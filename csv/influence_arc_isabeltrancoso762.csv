2010.iwslt-evaluation.9,N03-1017,0,0.0100386,"ents models than the IBM M4 alignment model, using the posterior distribution over alignments instead of the single best alignment. Moreover, we filtered phrase pairs from being extracted based on punctuation and phrase length differences. This paper is organized as follows: in Section 2 we will present the methods we used to improve the phrase extraction algorithm and Section 3 will describe the corpus used and data preparation. In Section 4, we will report the experimental results and, in Section 5, we will conclude the paper. 2. Phrase Extraction The most common phrase extraction algorithm [1] uses word alignment information to constraint the possible phrases that can be extracted. Given a word alignment, all phrase pairs 0 1 2 3 4 5 0 1 2 z p p p x p p p p p p p p p 3 p p z p p p p 4 p p p 5 p p p 6 p p p 7 p but p then p mr. z z z p p baldwin p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. consistent with that word alignment are extracted from the parallel sentence (a phrase pair is consistent with a word alignment if all words in one language contained in th"
2010.iwslt-evaluation.9,2006.amta-papers.11,0,0.0210516,"if there are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability and the lexical weight feature. The work by [2] conclude that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment, results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-evaluation.9,J10-3007,1,0.871261,"z 0 w 1 x y • 2 w a b c z Foreign x xy x xy xyz z yz z yz Source a a ab ab abc c c bc bc Points 0-0 0-0 0-0 0-0 0-0 2-2 2-2 2-2 2-2 2-2 Figure 1: Machine Translation phrase extraction from word alignments example. which leads that the word baldwin cannot be extracted without the incorrect surrounding context. This will make the pair baldwin, baldwin unavailable outside the given context. These led us to the work described in the following sections. 2.1. Constrained Alignments Rather than using the IBM M4 alignment model, we use the posterior regularization framework with symmetric constraints [3], which produces better overall results. This constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. We also made some tests with bijective constrains [3], which had better overall results compared to the IBM M4 model, but that were not as good as the symmetric constraints. In both of these alignments we use a threshold of 0.4 for accepting an alignment point and we train them using the conditions described in [3]. For each model, we initialize the translation ta"
2010.iwslt-evaluation.9,D09-1106,0,0.021721,"from the original paper for an explanation of the meaning of these parameters). 2.2. Weighted Alignment Matrixes We use information about the posterior distributions in the alignments, rather than the single best alignment to obtain better results. Given the posterior distribution for an alignment link, we use the soft union heuristic (the average of each link) to obtain a symmetrized alignment with link posteriors. Given these alignments links, we calculate the phrase translation probability and the link probability using the approach proposed for weighted alignment matrixes, as described in [4]. We only accept a phrase if its phrase posterior probability is above a particular threshold. For both the BTEC and DIALOG corpora we use a threshold of 0.1. We set the values based on the results of the original paper and leave the tuning of this particular threshold as future work, as lowering it does not always yields better results. 2.3. Phrase Pair Filtering For each phrase pair that is extracted from a sentence pair, we apply an acceptor to decide whether that phrase pair is accepted or not. We build special acceptors that deal with punctuation. The idea is that punctuation normally tra"
2010.iwslt-evaluation.9,W09-0439,0,0.0139838,"er than 4 are cut from the translation table, we could not perform such a translation. 4.1. Automatic Evaluation Results We observe from the preliminary automatic evaluation results that our main problem in this evaluation was the fact we only used BLEU as our tuning and evaluation metric, rather than a combination of metrics, which limited the quality of our results. In fact, in many instances our system yielded better BLEU scores in the evaluation, but the overall score was worse. It is also important notice that the tuning process we used does not use any stabilization methods described in [6], which states that there is a high variance between different runs of the tuning process for the same translation model. Thus, there is also the possibility that we obtained a bad set of weights for one of our translation models. In the DIALOG task, we performed specially well in the IWSLT09 testset, probably due to the fact that this set bears more similarity to the data set we used for tuning. In the BTEC task, we were ranked worse. We think that one of the reasons was, as stated above, that we obtained a worse set of weights during the tuning procedure. 5. Conclusions We have described the"
2010.iwslt-evaluation.9,P02-1040,0,\N,Missing
2010.iwslt-papers.14,N03-1017,0,0.631324,"vious approaches fit in this algorithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9],"
2010.iwslt-papers.14,N04-1035,0,0.0312417,"ithm, compare several of them and, in addition, we propose alternative heuristics, showing their impact on the final translation results. Considering two different test scenarios from the IWSLT 2010 competition (BTEC, Fr-En and DIALOG, Cn-En), we have obtained an improvement in the results of 2.4 and 2.8 BLEU points, respectively. 1. Introduction Modern statistical translation models depend crucially on the minimal translation units that are available during decoding. However, the evolution of statistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over t"
2010.iwslt-papers.14,P08-1112,1,0.927508,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,D07-1006,0,0.0135809,"tistical models – from Word-based (Wb) [1] to Phrase-based (Pb) [2, 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strat"
2010.iwslt-papers.14,P07-1003,0,0.0667469,", 3] or Syntaxbased (Sb) models [4, 5] – increased the difficulty of understanding what makes a good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation e"
2010.iwslt-papers.14,D09-1106,0,0.495182,"good translation unit, and increased as well their acquisition process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare severa"
2010.iwslt-papers.14,W05-0827,0,0.0165105,"ion process. Concerning this acquisition process, the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and"
2010.iwslt-papers.14,N07-2053,0,0.0158559,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,P08-1010,0,0.0128993,", the original and widely used approach [3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results."
2010.iwslt-papers.14,D07-1103,0,0.104809,"[3] to aquire minimal units for Pb models consists of a pipeline that starts with a set of word alignments and implements a series of heuristics to build the final phrase table. In recent years, several studies have tried to improve this pipeline. Thus, better alignment models were created [6, 7, 8], better combination of different alignments were tested [9], the posterior distribution over the alignments was used instead of a single best alignment [7, 10], different features were added to the phrase table [11], selective selection of phrases was tested [12, 13] and phrase tables were pruned [14], among others. In this paper we follow the Pb paradigm and present a general and extensible phrase extraction algorithm. We have highlighted several control points during the creation of phrase tables, that represent fundamental steps in this process. In each one of these points, different strategies/heuristics can be tested with minimal implementation efforts. In this paper, we also show how previous approaches fit in this algorithm and compare several of them. Moreover, we propose different heuristics and show their impact on the final translation results. Our experiments ran on two differe"
2010.iwslt-papers.14,W02-1018,0,0.0354354,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,W06-3105,0,0.0713912,"table, the translation process can be broken down into three steps: segment the source sentence into phrases, translate each source phrase into a target phrase, and reorder the target phrases. The quality of Pb systems depend directly on the quality of their phrase-table and therefore, phrase extraction is always a fundamental step in Statistical MT. Extracting all possible phrase-pairs is not a valid option since an exponential number of phrases would be extracted, most of which linguistically irrelevant. Learning a phrase table directly from a bilingual corpus has also been tried previously [15, 16], but these methods failed to compete with heuristic methods that we will describe briefly in the following section. In [16] some problems that result from learning a phrase table directly from data using the EM algorithm are identified. In general terms, phrase pairs with different segmentations (potentially all equally good) compete for probability mass (as opposed to learning bilingual lexicons where the competition is only based on bilingual word pairs). The most common phrase extraction algorithm [3] 313 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, D"
2010.iwslt-papers.14,2006.amta-papers.11,0,0.0529946,"re are too many incorrect alignment points forming a cluster, the correct phrases cannot be extracted without the spurious words, leading to missing words/phrases from the phrase table. In addition, unaligned words act as wild cards that can be aligned to every word in the neighborhood, thus increasing the size of the phrase table. Another undesirable effect of unaligned words is that they will only appear (in the phrase table) in the context of the surrounding words. Moreover, the spurious phrase pairs will change both the phrase probability as well as the lexical weight feature. The work by [17] concludes that the factor with most impact was the degradation of the translation probabilities due to noisy phrase pairs. Figure 1 shows the phrase tables extracted from two word alignments for the same sentence pair. These alignments only differ in one point: y-b. However, the nonexistence of this point in the second word alignment results in the removal of the phrase y-b in the second phrase table. Hence we would not be able to translate y as b except in the contexts shown in that table. Figure 2 shows an example of a word alignment where a rare source word is aligned to a group of target"
2010.iwslt-papers.14,2005.iwslt-1.8,0,0.0239954,"ery different alignments. For phrase extraction we 0 1 2 3 4 5 0 z p p p p p 1 p x p p p p 2 p p 3 p p 4 p p 5 p p 6 p p 7 p but p then z p p p p p mr. z z z p p baldwin p p p p p z p said p p p p p z: ce , m ba av en de´ : . pe ld ait su cla wi nd ite re´ n an t Figure 2: Example of a word alignment suffering from the garbage collector effect. are interested in a single alignment per sentence so the two directional alignments are combined to form a single alignment. Several approaches have been proposed to symmetrize these word alignments. The most commonly used is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbag"
2010.iwslt-papers.14,J10-3007,1,0.702323,"sed is called grow diagonal final [18]. It starts with the intersection of the sets of aligned points and adds points around the diagonal that are in the union of the two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality o"
2010.iwslt-papers.14,2006.iwslt-papers.7,0,0.0390249,"two sets of aligned points. The resulting alignment has high recall relative to the intersection and only slightly lower recall than the union. Other common approaches include the intersection and union of the directional alignments. However, the exact relationship between word alignment quality and machine translation quality is not straightforward. Despite recent work showing that better word alignments can result in better MT [6, 19], namely by reducing the garbage collector effect and hence increasing the phrase table coverage, there is evidence that the two are only indirectly connected [17, 20]. There are several reasons that explain these facts. First during the pipeline, one keeps committing to the best options available: best word alignment, best symmetrized alignment, etc. Second, the symmetrization heuristics tend to obfuscate the quality of the resulting alignment and the original ones by creating alignments close to the diagonal. Third, current phrase extraction weights each phrase independently of the quality of the underlying align314 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given t"
2010.iwslt-papers.14,2008.amta-papers.18,0,0.562826,"hop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 ments (all phrases are given the weight of 1). Several approaches have been proposed to mitigate this problem. Soft Union [9] uses the knowledge about the posterior distributions of each directional model. It includes a point in the final alignment if the average of the posteriors under the two models for that point is above a certain threshold. However, this approach still produces a single alignment to be used by the phrase extraction. An alternative is to not commit to any particular alignment, but either use n-best lists [21] or the posterior distribution over the alignments to extract phrases [7, 10]. Both of these approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for"
2010.iwslt-papers.14,W06-1607,0,0.0287577,"ese approaches increase the coverage of the phrase table. Moreover, by using the posterior weight of each phrase as its score instead of using 1 for all sentences, one can better estimate the phrase probabilities. The method computeGlobalPhraseStats calculates features, such as the phrase translation probability and the lexical probability based on the counts collected for each occurrence of each phrase pair. This is also the point where different types of smoothing for the phrase table can be applied (for instance, we can implement at this step the Knesser-Ney smoothing for bilingual phrases [22]). In this paper, we use as features both the phrase probability and lexical probability in the general phrase extraction [3] [10]. Algorithm 1 General Phrase Table Extraction Require: Bilingual Corpus Require: MaximumPhraseSize - max for each sentence pair (s, t) in Corpus do extractedPhrasePairs = extractPhrasePairs(s, t, max) for each phrase pair p in extractedPhrasePairs do phraseTable.add(p) end for end for computeGlobalPhraseStats pruneGlobalPhraseStats savePhraseTable Algorithm 2 Extract Phrase Pairs Require: Bilingual sentence s fl = s.foreignLen sl = s.sourceLen extractedPhrasePairs ="
2010.iwslt-papers.14,P02-1040,0,0.0797175,", “？”, “！”) with the respective latin punctuation. Furthermore, we leave the segmentation of Chinese characters as the one given in the corpus. At the end of the pipeline, we detokenize and recase the translation, so that the evaluation is performed according to the IWLST task. The recasing is done using a maximum entropy-based capitalization system [23]. For all experiments we use the Moses decoder (http://www.statmt. org/moses/), and before decoding the test set, we tune the weights of the phrase table using Minimum Error Rate Training (MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the"
2010.iwslt-papers.14,C96-2141,0,0.331127,"MERT). The results are evaluated using the BLEU metric [24]. We also follow the baseline described in the workshop above, which creates directional alignments produced by GIZA++ using the IBM M4 model (the train conditions are the same as the default moses training scripts: 5 iterations of IBM M1, 5 iterations of HMM and 5 iterations of IBM M4). Also, we combine these alignments using the grow-diagonalfinal heuristic, and use the default phrase extraction algorithm [3]. We will refer to the baseline as “Moses IBM M4”. Moreover, we also compare three different alignment models: the regular HMM [25] (referred to as “HMM”), the same model, but using the posterior regularization framework with bijective constraints (refered to as “BHMM”), and with symmetry constraints (refered to as “SHMM”) [19]. The latter constraint takes into account that if a certain unit a is aligned to unit b in the source to target alignment model, b should also be aligned to a in the target to source alignment model. In these alignments we use a threshold of 0.4 for accepting an alignment point. We trained these models using the conditions described in [19]. For each model, we initialize the translation tables with"
2010.iwslt-papers.14,E03-1009,0,0.0161308,"slation for the word “disait”. The phrases extracted with the default extraction method only contain the following right context “disait six heures” and, therefore, this context does not allow the translation of the sentence above. The words that are left unknown are due to the threshold being too high. We also add new features and new acceptors to address some observed problems. In the first experiment we add a part of speech feature that calculates the phrase probabilities and the lexical probabilities based on the part of speech of each word. We use the unsupervised POS system described by [26] (using the source code available at the authors website), and cluster the words into 50 different groups. We then tag each word with the attained tag. The intuition behind this feature is that a given word can be translated differently if it is being used as a noun or as a verb, and different POS sequences tend to generate different translations even if the words are the same. However, this approach produces worse results than the baseline. Two possible reasons are the use of an unsupervised system, whose accuracy is not very high. Furthermore, this system does not allows word ambiguity (the"
2010.iwslt-papers.14,2005.mtsummit-papers.36,0,0.0164424,"2) and 4 (length-diff4). Figure 3 shows the distribution of length difference of the phrase pairs in the phrase table, and Figure 4 shows the distributions of length difference of the phrase pairs actually used by the decoder. Finally, Table 6 shows the percentual reduction in the phrase table size using each heuristic. As expected, although there are a lot of phrase pairs with large length differences, the decoder only uses 1 sentence 317 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd and 3rd, 2010 70000 is similar to the approach proposed in [27] which only includes phrases of length larger than a given threshold if they cannot be realized by using smaller phrases. 60798 60000 50000 35945 40000 30000 28897 20000 15188 10000 5394 1725 453 5 6 0 0 1 2 3 4 Figure 3: Distribution of phrase-pairs in the phrase table by the difference between lenghts for source and target phrases for the BTEC corpus. with length difference of 6, as well as 4 phrase-pairs with a difference of 5. However, looking at Table 5 we see that both heuristics (length-diff2 and length-diff4) hurt the performance. Table 7 show the number of times phrase-pairs with a gi"
2010.iwslt-papers.14,J93-2003,0,\N,Missing
2010.iwslt-papers.14,P01-1067,0,\N,Missing
2010.iwslt-papers.14,J04-4002,0,\N,Missing
2010.iwslt-papers.14,J07-2003,0,\N,Missing
2010.iwslt-papers.14,P08-1000,0,\N,Missing
2011.eamt-1.19,J84-3009,0,0.408033,"Missing"
2011.eamt-1.19,almeida-etal-2010-bigorna,0,0.0592641,"Missing"
2011.eamt-1.19,D09-1030,0,0.0172107,"the EN→EP/BP/BP2EP models on the EN-EP test set 8 system, because it gave us several lexical / morphological entries used to resolve differences between BP to EP. We have used this to extract the lexical rules, where one entity in BP is written differently in EP. As the translation lexicon created automatically has a margin of error, it is necessary to manually filter spurious entries. If the language pair of varieties and/or dialects is well covered by the workers of crowdsourcing systems such as Amazon’s Mechanical turk (AMT), it is a viable option to avoid manually filtering these entries (Callison-Burch, 2009). Another possible improvement to extraction of multiword lexical contrastive pairs algorithm is the inclusion of the translational entropy to help to identify idiomatic multiword expressions (Moir´on and Tiedemann, 2006). We have also got encouraging results (about 5 BLEU points) when we evaluated the BP2EP output against manually created EP corpora. However, the system still needs some improvements to handle particular cases. The incomplete lexical pair coverage is one of the reasons. But there are exceptions to the rules that are hard to capture by rules. Also, the usage of handmade rules t"
2011.eamt-1.19,2001.mtsummit-papers.14,0,0.0386023,"Missing"
2011.eamt-1.19,A00-1002,0,0.726678,"Missing"
2011.eamt-1.19,N03-1017,0,0.0234167,"probability and scores, e.g.: “Eu achei” → “Eu pensei”, “Eu achei” → “Pensei”, “Eu achei” → “Pensava”. • Number based Removal: Numbers are generally translated one to one e.g.: “609 bilh˜oes em 2008” → “609 bili˜oes em 2008” • Identical Translation Removal : Many words in EP are translated equally to BP, which is done by default. Furthermore, if any word in the source phrase is contained in the target one or vice versa, the phrase pair is also removed, e.g.: “Eu”→ “E Eu”. • Confidence based Entries Filtering: Removes phrase pairs with low confidence based on their features, which are used in (Koehn et al., 2003). We remove entries having probabilities lower than 1 (to trim ambiguous entries) and the respective weights are lower than 0.5 (empiric threshold). We plan in future work to improve this filter by using a linear combination of the 4 parameters. • Lexicon based Filtering: Removes phrase pairs where the source or the target contain words that are not present in the lexicon of the respective language. • Number of Words Filtering: Removes phrase pairs where the number of words in the source is different from the number of words in the target. In our experiments, phrase pairs were limited to 2 wor"
2011.eamt-1.19,2010.iwslt-papers.14,1,0.883624,"Missing"
2011.eamt-1.19,W06-2405,0,0.0264081,"Missing"
2011.eamt-1.19,moore-2002-fast,0,0.0416943,"Our second experiment was formulated to analyze how the system can translate from BP to EP. It was used the manually parallel corpora of EP and BP. Our third experiment evaluated the usage of the BP2EP output in SMT. Our goal was to determine whether it is observed translation quality gains when adding the BP2EP output, created from the BP texts, to the EP models. The parallel corpora used in the SMT evaluation was created from TED talks. Since the audio transcriptions and translations available at the TED website are not aligned at the sentence level, we used the Bilingual Sentence Aligner (Moore, 2002) to accomplish this task. Table 5 shows some details about the EPEN, BP-EN, BP2EP-EN, PT-&-BP-EN and PT&BP2EP-EN. The BP2EP corpus corresponds to the output of the BP2EP system with the BP corpus as input. The EP-&-BP and EP-&-BP2EP corpus is the concatenation of the EP corpus with the BP and BP2EP corpus, respectively. 7.1 Evaluation of the extraction of Multiword Lexical Contrastive pairs from SMT Phrase-Tables We run the phrase table extraction algorithm for pairs of translations containing 1 and 2 words. The corpus was retrieved from the set of TED talks, that had both the Brazilian Portug"
2011.eamt-1.19,D09-1141,0,0.400395,"with identical approaches to translate from Czech to Slovak (Hajiˇc et al., 2000), from Spanish to Catalan (Canals-Marote et al., 2001)(Navarro et al., 2004), and from Irish to Gaelic Scottish (Scannell, 2006). Looking at Scannel’s system (2006) gives us a better understanding of the common system architecture. This architecture consists of a pipeline of components, such as a Part-of-Speech tagger (POS-tagger), a Na¨ıve Bayes word sense disambiguator, and a set of lexical and grammatical transfer rules, based on bilingual contrastive lexical and grammatical differences. On a different level, (Nakov and Ng, 2009) describes a way of building MT systems for lessresourced languages by exploring similarities with closely related and languages with much more resources. More than allowing translation for lessresourced languages, this work also aims at allowing translation from groups of similar languages to other groups of similar languages just like stated earlier. This method proposes the merging of bilingual texts and phrase-table combination in the training phase of the MT system. Merging bilingual texts from similar languages (on the source side), one with the less-resourced language and the other (muc"
2011.eamt-1.19,P02-1040,0,0.0834116,"arallel corpus on the EP translation, we made several experiments. Using the EP→EN and EN→EP models as baseline, we compared them with BP and BP2EP models, and also EP-&-BP and EP-&BP2EP. All experiments were performed using the Moses decoder 9 . Before decoding the test set (shown in Table 5), we tune the weights of the phrase table using Minimum Error Rate Training (MERT) using the devel corpus shown in Table 5. The devel and test set are in EP and EN and are the same among the several experiments. The language model was created only with EP texts. The results were evaluated using the BLEU (Papineni et al., 2002) and METEOR (Lavie and Denkowski, 2009) metrics. 9 http://www.statmt.org/moses/ Tables 6 and 7 shows the results for the EP/BP/BP2EP→EN and EN→EP/BP/BP2EP models, respectively. We observed that BP models generate better results than using only the EP ones. The larger amount of parallel data for BP explains these differences. The BP2EP results were systematically better than using BP models. This shows that our hypothesis of converting BP to EP using this approach led to consistent improvements to the translation between EN and EP. Data Train Devel Test Lang. Pairs EP EN BP EN BP2EP EN EP EN EP"
2011.iwslt-papers.3,E03-1035,0,0.30853,"Missing"
2011.iwslt-papers.3,J03-3002,0,0.159638,"Missing"
2011.iwslt-papers.3,N04-1036,0,0.0818659,"Missing"
2011.iwslt-papers.3,2010.iwslt-papers.14,1,0.832985,"Missing"
2011.iwslt-papers.3,P07-2045,0,0.0045147,"Missing"
2011.iwslt-papers.3,P06-1010,0,0.081669,"Missing"
2011.iwslt-papers.3,O08-2010,0,\N,Missing
2020.lrec-1.435,D19-1410,0,0.0784952,"loser to the true class decision boundary than unrelated ones, which allows for a better estimation when training a model to learn it. In regards to the information derived from the search results used in this work, we used exclusively information derived from the textual components of the video, and relay the study of acoustic and visual features for future work. In particular we used the transcription of the video, the title, the description, and the top 5 comments. In regards to the feature extraction process used in this work, we briefly describe the adopted process based on SentenceBERT (Reimers and Gurevych, 2019), that was used to encode the textual cues. Then, we establish an upper bound for the performance that can be achieved on the task of automatically labeling the WSM corpus using the chosen features, by training a fully supervised model on the dataset with the manually obtained labels. After this, we compare the performance of the fully supervised model to the performance obtained of a similar model but in a MIL scenario, where the instance labels are unavailable during training. We also study the contribution 3545 of each type of textual cues for the performance of the final model: the transcr"
barreiro-etal-2014-linguistic,C96-1030,0,\N,Missing
barreiro-etal-2014-linguistic,W07-0728,0,\N,Missing
barreiro-etal-2014-linguistic,P07-2045,0,\N,Missing
barreiro-etal-2014-linguistic,W07-0732,0,\N,Missing
barreiro-etal-2014-linguistic,2013.mtsummit-wmwumttt.5,1,\N,Missing
barreiro-etal-2014-linguistic,2007.mtsummit-papers.40,0,\N,Missing
barreiro-etal-2014-linguistic,N13-1140,0,\N,Missing
barreiro-etal-2014-linguistic,W11-2117,0,\N,Missing
barreiro-etal-2014-openlogos,W02-1204,0,\N,Missing
barreiro-etal-2014-openlogos,bel-etal-2000-simple,0,\N,Missing
barreiro-etal-2014-openlogos,P98-1013,0,\N,Missing
barreiro-etal-2014-openlogos,C98-1013,0,\N,Missing
barreiro-etal-2014-openlogos,P04-1048,0,\N,Missing
barreiro-etal-2014-openlogos,villegas-etal-2000-multilingual,0,\N,Missing
C12-2070,2005.iwslt-1.8,0,0.0165691,"eline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using"
C12-2070,N09-1025,0,0.0272792,"are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the best one-to-one phras"
C12-2070,P08-1010,0,0.0162618,"analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tun"
C12-2070,D07-1103,0,0.385725,"based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, relative entropy aims at pruning phrase pairs that can be reproduced using smaller constituents with a small or no loss in terms of the models predictions. On the other hand, significance pruning aims at re"
C12-2070,2005.mtsummit-papers.11,0,0.242631,"nd are originated from incorrect alignments at sentence or word level. This indicates that both methods can be combined to obtain better results. We propose a log-linear interpolation of the two metrics to achieve a better trade off between the number of phrase pairs and the translation quality. This paper is structured as follows: Section 2 includes a brief summary of relative entropy and significance pruning approaches in sub-sections 2.1 and 2.2. Sub-section 2.3 analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004;"
C12-2070,P07-2045,0,0.0067566,"ance score of 10. 3 Experimental Results 3.1 Data Sets Experiments were performed using the publicly available EUROPARL (Koehn, 2005) corpora for the English-French language pair. From this corpus, 1.2M sentence pairs were selected for training, 2000 for tuning and another 2000 for testing. 3.2 Baseline System The baseline translation system was trained using a conventional pipeline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we r"
C12-2070,N03-1017,0,0.0919428,"s structured as follows: Section 2 includes a brief summary of relative entropy and significance pruning approaches in sub-sections 2.1 and 2.2. Sub-section 2.3 analyses both algorithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, t"
C12-2070,D12-1088,1,0.819902,"al selection criteria. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, rela"
C12-2070,D09-1078,0,0.0220035,"s to more search errors due to the large search space. Furthermore, larger models are more expensive to store, which limits the portability of such models to smaller devices. Pruning is one approach to address this problem, where models are made more compact by discarding entries from the model, based on additional selection criteria. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by"
C12-2070,P03-1021,0,0.0139315,"ono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using MERT tuning (Och, 2003). We did not rerun tuning again after pruning to avoid adding noise to the results. Finally, we present the results evaluated with BLEU-4 (Papineni et al., 2002). After computing the negative log likelihood of both scores, we also rescale both score’s values by mean, so that scores will have similar values. This step is performed so the interpolation weights, in the results appear more intuitive. 718 3.3 Results We can see the results in table 2, where the first two rows, represent the BLEU scores for relative entropy pruning and significance pruning, respectively. Then, we have the scores obt"
C12-2070,N04-1021,0,0.0271312,"pus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the"
C12-2070,P02-1040,0,0.0848059,"of the relative entropy. As in (Zens et al., 2012), we removed all singleton phrase pairs from the phrase table. This will lower the effectiveness of significance pruning, since a large amount of least significant phrase pairs will be removed a priori. The filtered translation model contains, approximately 50 million phrase pairs. As language model, a 5-gram model with Kneser-ney smoothing was used. The baseline model was tuned using MERT tuning (Och, 2003). We did not rerun tuning again after pruning to avoid adding noise to the results. Finally, we present the results evaluated with BLEU-4 (Papineni et al., 2002). After computing the negative log likelihood of both scores, we also rescale both score’s values by mean, so that scores will have similar values. This step is performed so the interpolation weights, in the results appear more intuitive. 718 3.3 Results We can see the results in table 2, where the first two rows, represent the BLEU scores for relative entropy pruning and significance pruning, respectively. Then, we have the scores obtained using the scorer in 4 of these 2 scores, with α weights at intervals of 0.1. Finally, we have the scores using the scorer 5, also with the weight α set at"
C12-2070,2010.iwslt-evaluation.1,0,0.0164305,"d with the significance score of 10. 3 Experimental Results 3.1 Data Sets Experiments were performed using the publicly available EUROPARL (Koehn, 2005) corpora for the English-French language pair. From this corpus, 1.2M sentence pairs were selected for training, 2000 for tuning and another 2000 for testing. 3.2 Baseline System The baseline translation system was trained using a conventional pipeline similar to the one described in (Koehn et al., 2003). First, the word alignments were generated using IBM model 4. Then, the translation model was generate using the phrase extraction algorithm (Paul et al., 2010)(Koehn et al., 2007). The maximum size of the phrase pairs is set to 7, both for the source and the target language. The model uses as features: • • • • • Translation probability Reverse translation probability Lexical translation probability Reverse lexical translation probability Phrase penalty The reordering model is built using the lexicalized reordering model described in (Axelrod et al., 2005), with MSD (mono, swap and discontinuous) reordering features for orientations. All the translation and reordering features are considered during the calculation of the relative entropy. As in (Zens"
C12-2070,2008.amta-srw.6,0,0.0176384,"en their probabilities log P(t|s) . This value is then weighted by the empirical distribution P(s, t), so that phrase pairs that are more likely to be observed in the data are less likely to be pruned. The empirical distribution is given as: P(s, t) = C(s, t) (2) N Where C(s, t) denotes, the number of sentence pairs where s and t are observed, and N denotes the number of sentence pairs. Computing Pp (t|s) is the most computationally expensive operation in this model, since it involves finding all possible derivations of a phrase pair using smaller units, which involves a forced decoding step (Schwartz, 2008). While minimizing D(Pp ||P) would lead to optimal results, such optimization is computationally infeasible. Thus, an approximation is the find the local values for each phrase pair: RelEnt(s,t) = −P(s, t)l og Pp (t|s) P(t|s) (3) This score can be viewed as the relative entropy between Pp (t|s) and P(t|s), if only the phrase pair with source s and target t is pruned. The problem with this approximation is that, we might assume a given phrase pair A can be pruned, because it can be composed by phrase pairs B and C , only to discover later that B is also pruned. 2.2 Significance Pruning Signific"
C12-2070,2009.mtsummit-papers.17,1,0.786811,"sults, such optimization is computationally infeasible. Thus, an approximation is the find the local values for each phrase pair: RelEnt(s,t) = −P(s, t)l og Pp (t|s) P(t|s) (3) This score can be viewed as the relative entropy between Pp (t|s) and P(t|s), if only the phrase pair with source s and target t is pruned. The problem with this approximation is that, we might assume a given phrase pair A can be pruned, because it can be composed by phrase pairs B and C , only to discover later that B is also pruned. 2.2 Significance Pruning Significance pruning of phrase tables (Johnson et al., 2007; Tomeh et al., 2009) relies on a statistical test that assesses the strength of the association between the source and target phrases in a phrase pair. Such association can be represented using a two-by-two contingency table: 715 C(s, t) C(t) − C(s, t) C(s) − C(s, t) N − C(s) − C(t) + C(s, t) where N is the size of the training parallel corpus, C(s) is the count of the source phrase, C(t) is the count of the target phrase, and C(s, t) is the count of the co-occurences of s and t . The probability of this particular table is given by the the hypergeometric distribution: ph (C(s, t)) = C(s)  N −C(s)  C(s,t) C(t)−"
C12-2070,2011.iwslt-papers.10,1,0.758963,"combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an alr"
C12-2070,P03-1041,0,0.0349745,"rithms and preceeds our combination approach in sub-section 2.4. The results obtained with the EUROPARL corpus (Koehn, 2005) are shown in Section 3. Finally, we conclude and present directions for future research in Section 4. 2 Combining Relative Entropy and Significance Pruning In principle, any method of evaluation of phrase pairs can be used as the basis for pruning. This includes phrase counts and probabilities (Koehn et al., 2003), statistical significance tests (Johnson et al., 2007), and relative entropy scores (Ling et al., 2012; Zens et al., 2012) and many others (Deng et al., 2008; Venugopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Mo"
C12-2070,D12-1089,0,0.176872,"ia. The challenge in this task is to choose the entries that will least degenerate the quality of the task for which the model is used. For language models, an effective algorithm based on relative entropy is described in (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). In these approaches, a criteria based on the KL divergence is applied, so that higher order n-grams are only included in the model when they provide enough additional information to the model, given the lower order n-grams. Recently, this concept was applied for translation model pruning (Ling et al., 2012; Zens et al., 2012), and results indicate that this method yields a better phrase table size and translation quality ratio than previous methods, such as the well known method in (Johnson et al., 2007), which uses the Fisher’s exact test to calculate how well a phrase pair is supported by data. In this work, we attempt to improve the relative entropy model, by combining it with the significance based approach presented in (Johnson et al., 2007).The main motivation is that, as suggested in (Ling et al., 2012), relative entropy and significance based methods are complementary. On one hand, relative entropy aims at"
C12-2070,N07-2053,0,0.0172899,"gopal et al., 2003; Tomeh et al., 2011), in addition to the features typically found in phrase tables (Och et al., 2004; Chiang et al., 2009). Each method reflects some characteristics of phrase pairs that are not sought by the other, and hence trying to combine them is a tempting idea. (Deng et al., 2008) incorporate several features into a log-linear model parametrized with yk that are tuned, along with the extraction threshold, to maximize a translation quality, which makes the procedure extremely expensive. A similar model is used in (Venugopal et al., 2003) without any parameter tuning. (Zettlemoyer and Moore, 2007) use an already tuned model (using MERT) 714 in a competitive linking algorithm to keep the best one-to-one phrase matching in each training sentence. In our work we favor efficiency and we focus on relative entropy and significance pruning, which can be efficiently computed, without the need to external information. They also deliver good practical performance. 2.1 Relative Entropy Pruning Relative entropy pruning for translation models (Ling et al., 2012; Zens et al., 2012) has a solid foundation on information theory. The goal in these methods is to find a pruned model Pp (t|s) that yields"
cabarrao-etal-2014-revising,J96-2004,0,\N,Missing
D12-1088,J93-2003,0,0.0282878,"bilities. This model is then applied to phrase table pruning. Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality. In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding. 1 Introduction Phrase-based Machine Translation Models (Koehn et al., 2003) model n-to-m translations of n source words to m target words, which are encoded in phrase pairs and stored in the translation model. This approach has an advantage over Word-based Translation Models (Brown et al., 1993), since translating multiple source words allows the context for each source word to be considered during transHowever, not all words add the same amount of contextual information. Using the same example for “in”, if we add the context “(hid the key) in”, it is still not possible to accurately identify the best translation for the word “in”. The phrase extraction algorithm (Ling et al., 2010) does not discriminate which phrases pairs encode contextual information, and extracts all phrase pairs with consistent alignments. Hence, phrases that add no contextual information, such as, p(hid the key"
D12-1088,W06-1607,0,0.0192687,", the possible derivations are either using phrase p(s, t) or one element of its support set S1 , S2 or S3 . On the other hand, on the pruned model where p(s, t) does not exist, only S1 , S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability element in SP (p(s, t)), then both the models will choose that element, in which P (t|s) case, Pp(t|s) = 1. This can happen, if we define 966 features that penalize longer phrase pairs, such as lexical weighting, or if we apply smoothing (Foster et al., 2006). Secondly, if the probability of p(s, t) is equal to the most likely element in SP (p(s, t)), regardless of whether the unpruned model choses to use p(s, t) or that element, the probability emissions of the pruned and unpruned model will be identiP (t|s) cal. Thus, for this case Pp(t|s) = 1. Finally, if the probability of p(s, t) is higher than other possible derivations, the unpruned model will choose to emit the probability of p(s, t), while the pruned model will emit the most likely element in SP (p(s, t)). Hence, the probability loss between the 2 models, will be the ratio between the pro"
D12-1088,D07-1103,0,0.253111,"tion 5. Finally, we conclude in section 6. 2 Phrase Table Pruning Phrase table pruning algorithms are important in translation, since they efficiently reduce the size of the translation model, without having a large negative impact in the translation quality. This is especially relevant in environments where memory constraints are imposed, such as translation systems for small devices like cellphones, and also when time constraints for the translation are defined, such as online Speech-to-Speech systems. 963 2.1 Significance Pruning A relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their com"
D12-1088,N03-1017,0,0.0838211,"Missing"
D12-1088,P07-2045,0,0.00825301,"sentence. Using this distribution, the model is more biased in pruning phrase pairs with s, t pairs that do not occur frequently. 3.4 Computing Pp (t|s) P (t|s) P (t|s) The computation of Pp(t|s) depends on how the decoder adapts when a phrase pair is pruned from the model. In the case of back-off language models, this can be solved by calculating the difference of the logs between the n-gram estimate and the backoff estimate. However, a translation decoder generally functions differently. In our work, we will assume that the decoding will be performed using a Viterbi decoder, such as MOSES (Koehn et al., 2007), where the translation with the highest score is chosen. In the example above, where s=”John in Portugal” and t=”John em Portugal”, the decoder would choose the derivation with the highest probability from s to t. Using the unpruned model, the possible derivations are either using phrase p(s, t) or one element of its support set S1 , S2 or S3 . On the other hand, on the pruned model where p(s, t) does not exist, only S1 , S2 and S3 can be used. Thus, given a s, t pair one of three situations may occur. First, if the probability of the phrase pair p(s, t) is lower than the highest probability"
D12-1088,2005.mtsummit-papers.11,0,0.0197343,"del would eliminate p3 and keep p1 , yet the best decision could be to keep p3 and remove p1 , if p3 is also frequently used in derivations of other phrase pairs. Thus, we leave the problem of finding the best set of phrases to prune as future work. 5 Experiments We tested the performance of our system under two different environments. The first is the small scale DIALOG translation task for IWSLT 2010 evaluation (Paul et al., 2010) using a small corpora for the Chinese-English language pair (henceforth referred to as “IWSLT”). The second one is a large scale test using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric poster"
D12-1088,2007.mtsummit-papers.22,0,0.468875,"Missing"
D12-1088,D09-1078,0,0.198151,"nsiderable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training corpora and extracts the number of times each phrase pair is used in the 1-best translation hypothesis. Thus, phrase pairs that are rarely used during decoding are excluded first duri"
D12-1088,P03-1021,0,0.0751012,"Missing"
D12-1088,2010.iwslt-evaluation.1,0,0.0454513,"Missing"
D12-1088,2008.amta-srw.6,0,0.0613165,"hing in the space of possible translations, but in the space of possible derivations, which are sequences of phrase translations p1 (s1 , t1 ), ..., pn (sn , tn ) that can be applied to s to Qgenerate an output t with the score given by P (t) ni=1 P (si , ti ). Our algorithm to determine SP (p(s, t)) can be described as an adaptation to the decoding algorithm in Moses, where we restrict the search space to the subspace SP (p(s, t)), that is, our search space is only composed by derivations that output t, without using p itself. This can be done using the forced decoding algorithm proposed in (Schwartz, 2008). Secondly, the score of a given translation hypothesis does not depend on the language model probability P (t), since all derivations in this search space have the same t, thus we discard this probability from the score function. Finally, rather than using beam search, we exhaustively search all the search space, to reduce the hypothesis of incurring a search error at this stage. This is possible, since phrase pairs are generally smaller than text (less than 8 words), and because we are constraining the search space to t, which is an order of magnitude smaller than the reg967 ular search spac"
D12-1088,2009.mtsummit-papers.17,0,0.472482,"relevant reference in phrase table pruning is the work of (Johnson and Martin, 2007), where it is shown that a significant portion of the phrase table can be discarded without a considerable negative impact on translation quality, or even positive one. This work computes the probability, named p-value, that the joint occurrence event of the source phrase s and target phrase t occurring in same sentence pair happens by chance, and are actually statistically independent. Phrase pairs that have a high p-value, are more likely to be spurious and more prone to be pruned. This work is followed in (Tomeh et al., 2009), where phrase pairs are treated discriminately based on their complexity. Significance-based pruning has also been successfully applied in language modeling in (Moore and Quirk, 2009). Our work has a similar objective, but instead of trying to predict the independence between the source and target phrases in each phrase pair, we attempt to predict the independence between a phrase pair and other phrase pairs in the model. 2.2 Relevance Pruning Another proposed approach (Matthias Eck and Waibel, 2007) consists at collecting usage statistics for phrase pairs. This algorithm decodes the training"
D12-1088,J10-3007,1,0.896555,"Missing"
D12-1088,C96-2141,0,0.348977,"using the complete EUROPARL (Koehn, 2005) corpora for the Portuguese-English language pair, which we will denote by “EUROPARL”. 5.1 Corpus The IWSLT model was trained with 30K training sentences. The development corpus and test corpus were taken from the evaluation dataset in IWSLT 2006 (489 tuning and 500 test sentences with 7 references). The EUROPARL model was trained using the EUROPARL corpora with approximately 1.3M sentence pairs, leaving out 1K sentences for tuning and another 1K sentences for tests. 5.2 Setup In the IWSLT experiment, word alignments were generated using an HMM model (Vogel et al., 1996), with symmetric posterior constraints (V. Grac¸a et al., 2010), using the Geppetto toolkit2 . This setup was used in the official evaluation in (Ling et al., 2010). For the EUROPARL experiment the word alignments were generated using IBM model 4. In both experiments, the translation model was built using the phrase extraction algorithm (Paul et al., 2010), with commonly used features in Moses (Ex: probability, lexical weighting, lexicalized reordering model). The optimization of the translation model weights was done using MERT tuning (Och, 2003) and the results were evaluated using BLEU-4. 5"
D12-1088,2010.iwslt-papers.14,1,\N,Missing
D13-1008,P06-2005,0,0.119477,"Missing"
D13-1008,2005.iwslt-1.8,0,0.0618985,"Missing"
D13-1008,P05-1074,0,0.444565,"Missing"
D13-1008,P11-1131,0,0.0205445,"Missing"
D13-1008,J92-4003,0,0.0391747,"Missing"
D13-1008,J93-2003,0,0.0256659,"Missing"
D13-1008,W11-2107,0,0.0305025,"Missing"
D13-1008,P08-1115,0,0.0452721,"Missing"
D13-1008,N13-1073,1,0.862904,"Missing"
D13-1008,P11-1137,0,0.00905253,"Missing"
D13-1008,N13-1037,0,0.00849129,"Missing"
D13-1008,W11-2210,0,0.028087,"Missing"
D13-1008,P11-1038,0,0.106633,"Missing"
D13-1008,D12-1039,0,0.166445,"Missing"
D13-1008,N03-1017,0,0.0474429,"Missing"
D13-1008,P07-2045,1,0.0119874,"Missing"
D13-1008,P13-1018,1,0.283107,"Missing"
D13-1008,J03-1002,0,0.0062214,"Missing"
D13-1008,J04-4002,0,0.110137,"Missing"
D13-1008,P03-1021,0,0.0134582,"Missing"
D13-1008,P02-1040,0,0.105491,"Missing"
D13-1008,P11-1002,0,0.0235515,"Missing"
D13-1008,J03-3002,0,0.0406716,"Missing"
D13-1008,P12-1021,0,0.0203959,"Missing"
D13-1008,C96-2141,0,0.0951178,"Missing"
D13-1008,N13-1050,0,0.1235,"Missing"
D13-1008,W13-2515,0,0.541542,"Missing"
D13-1008,D13-1007,0,0.0725911,"Missing"
D13-1008,2010.iwslt-papers.14,1,\N,Missing
D15-1161,P14-2131,0,0.0497195,"uage Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning words representations tha"
D15-1161,D14-1082,0,0.00523589,"local context (e.g. function words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextu"
D15-1161,P14-1129,0,0.0182429,"pic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the le"
D15-1161,D14-1012,0,0.0194663,"data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depends on both the syntactic relation from nicely, which narrows down the list of candidates to verbs, and on the semant"
D15-1161,P12-1092,0,0.059041,"e a large drop on this semantically oriented task. Our attention-based model, on the other hand, out performs all other models on syntax-based tasks, while maintaining a competitive score on semantic tasks. This is an encouraging result that shows that it is possible to learn representations that can perform well on both semantic and syntactic tasks. 4 Related Work Many methods have been proposed for learning word representations. Earlier work learns embeddings using a recurrent language model (Collobert et al., 2011), while several simpler and more lightweight adaptations have been proposed (Huang et al., 2012; Mikolov et al., 2013). While most of the learnt vectors are semantically oriented, work has been done in order to extend the model to learn syntactically oriented embeddings (Ling et al., 2015a). Attention models are common in vision related tasks (Tang et al., 2014), where models learn to pay attention to certain parts of a image in order to make accurate predictions. This idea has been recently introduced in many NLP tasks, such as machine translation (Bahdanau et al., 2014). In the area of word representation learning, no prior work that uses attention models exists to our knowledge. 5 Co"
D15-1161,D13-1176,0,0.00900077,"g global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted wor"
D15-1161,D14-1108,1,0.740924,"ction words), while other words are more suited for determining global context, such as the topic of the document. Experiments performed on both semantically and syntactically oriented tasks show gains using our model over the existing bag of words model. Furthermore, compared to other more sophisticated models, our model scales better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently"
D15-1161,N15-1144,1,0.0648252,"ver, this model does not scale well as b increases as it requires V × dw more parameters for each new word in the window. Finally, setting a good value for b is difficult as larger values may introduce a degenerative behavior in the model, as more effort is spent predicting words that are conditioned on unrelated words, while smaller values of b may lead to cases where the window size is not large enough include words that are semantically related. For syntactic tasks, it has been shown that increasing the window size can adversely impact in the quality of the embeddings (Bansal et al., 2014; Lin et al., 2015). 2.2 CBOW with Attention We present a solution to these problems while maintaining the efficiency underlying the bag-ofwords model, and allowing it to consider contextual words within the window in a non-uniform way. We first rewrite the context window c as: X c= ai (wi )wi (2) i∈[−b,b]−{0} (1) where Oc corresponds to the projection of the context vector c onto the vocabulary V and v is a one-hot representation. For larger vocabularies it is inefficient to compute the normalizer P > v∈V exp v Oc. Solutions for problem are using the hierarchical softmax objective function (Mikolov et al., 2013"
D15-1161,N15-1142,1,0.386366,"isbon, Portugal Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA Instituto Superior T´ecnico, Lisbon, Portugal {lingwang,chuchenl,ytsvetko,cdyer,awb}@cs.cmu.edu {ramon.astudillo,samir,isabel.trancoso}@inesc-id.pt Abstract The continuous bag-of-words (Mikolov et al., 2013) is one of the many models that learns word representations from raw textual data. While these models are adequate for learning semantic features, one of the problems of this model is the lack of sensitivity for word order, which limits their ability of learn syntactically motivated embeddings (Ling et al., 2015a; Bansal et al., 2014). While models have been proposed to address this problem, the complexity of these models (“Structured skip-n-gram” and “CWindow”) grows linearly as size of the window of words considered increases, as a new set of parameters is created for each relative position. On the other hand, the continuous bag-of-words model requires no additional parameters as it builds the context representation by summing over the embeddings in the window and its performance is an order of magnitude higher than of other models. We introduce an extension to the bag-ofwords model for learning wo"
D15-1161,P14-1140,0,0.0283868,"Missing"
D15-1161,D07-1043,0,0.00495254,"embeddings in the domain of part-of-speech tagging in both supervised (Ling et al., 2015b) and unsupervised tasks (Lin et al., 2015). This later task is newly proposed, but we argue that success in it is a compelling demonstration of separation of words into syntactically coherent clusters. Part-of-speech induction. The work in (Lin et al., 2015) attempts to infer POS tags with a standard bigram hmm, which uses word embeddings to infer POS tags without supervision. We use the same dataset, obtained from the ConLL 2007 shared task (Nivre et al., 2007) Scoring is performed using the V-measure (Rosenberg and Hirschberg, 2007), which is used to predict syntactic classes at the word level. It has been shown in (Lin et al., 2015) that word embeddings learnt from structured skip-ngrams tend to work better at this task, mainly because it is less sensitive to larger window sizes. These results are consistent with our observations found in Table 1, in rows “Skip-ngram” and “SSkip-ngram”. We can observe that our attention based CBOW model (row “CBOW Attention”) improves over these results for both tasks and also the original CBOW model (row “CBOW”). 1369 CBOW Skip-ngram SSkip-ngram CBOW Attention POS Induction 50.40 33.86"
D15-1161,D13-1170,0,0.00265235,"sented in (Ling et al., 2015b) using the same hyper-parameters. Results on the POS accuracy on the test set are reported on Table 1. We can observe our model can obtain similar results compared to the structured skip-ngram model on this task, while training the model is significantly faster. The gap between the usage of different embeddings is not as large as in POS induction, as this is a supervised task, where pre-training generally leads to smaller improvements. 3.3 Semantic Evaluation To evaluate the quality of our vectors in terms of semantics, we use the sentiment analysis task (Senti) (Socher et al., 2013), which is a binary classification task for movie reviews. We simply use the mean of the word vectors of words in a sentence, and use them as features in an `2 regularized logistic regression classifier. We use the standard training/dev/test split and report accuracy on the test set in table 1. We can see that in this task, our models do not perform as well as the CBOW and Skipngram model, which hints that our model is learning embeddings that learn more towards syntax. This is expected as it is generally uncommon for embeddings to outperform existing models on both syntactic and semantic task"
D15-1161,P10-1040,0,0.138833,"better as we increase the size of the context of the model. 1 Introduction Learning word representations using raw text data have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These embeddings are generally learnt by defining an objective function, which predicts words conditioned on the context surrounding those words. Once trained, these can be used as features (Turian et al., 2010), as initializations of other neural networks (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). In this work, we propose an extension to the continuous bag-of-words model, which adds an attention model that considers contextual words differently depending on the word type and its relative position to the predicted word (distance to the left/right). The main intuition behind our model is that the prediction of a word is only dependent on certain words within the context. For instance, in the sentence We won the game! Nicely played!, the prediction of the word played, depen"
D15-1161,D15-1176,1,\N,Missing
D15-1161,D07-1096,0,\N,Missing
D15-1176,afonso-etal-2002-floresta,0,0.054909,"., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous representations and the same hyperparametrization as in language modeling (Section 4) is used. Additionally, we also compare to the convolutional model of Santos and Zadrozny (2014), which also requires the dimensionality for characters and the wor"
D15-1176,D15-1041,1,0.487282,"eling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic units to derive the embeddings for the word. On POS tagging, our models using characters alone can still achieve comparable or better results than state-of-the-art systems, without the need to manually engineer such lexical features. Although both language modeling and POS tagging both benefit strongly from morphological cues, the success of our models in languages with impover"
D15-1176,D13-1008,1,0.162387,"susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regul"
D15-1176,N15-1142,1,0.116424,"y are not found labelled in the training data, the model would be dependent on context to determine their tags, which could lead to errors in ambiguous contexts. Unsupervised training methods such as the Skip-n-gram model (Mikolov et al., 2013) can be used to pretrain the word representations on unannotated corpora. If such pretraining places cat, dog and snake near each other in vector space, and the supervised POS data contains evidence that cat and dog are nouns, our model will be likely to label snake with the same tag. We train embeddings using English wikipedia with the dataset used in (Ling et al., 2015), and the Structured Skip-n-gram model. Results using pre-trained word lookup tables and the C2W with the pre-trained word lookup tables as additional parameters are shown in rows “word(sskip)” and “C2W + word(sskip)”. We can observe that both systems can obtain improvements over their random initializations (rows “word” and (C2W)). Finally, we also found that when using the C2W model in conjunction pre-trained word embeddings, that adding a non-linearity to the representations extracted from the C2W model eC w improves the results over using a simple linear trans+feat no no yes yes yes yes no"
D15-1176,P14-1140,0,0.0196322,"tors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with"
D15-1176,W13-3512,0,0.437991,"are not independent. The wellknown “past tense debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have rel"
D15-1176,D14-1082,0,0.0447531,"Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly with other tasks with large amounts of data an"
D15-1176,J93-2004,0,0.0572265,"ls Experiments: Part-of-speech Tagging As a second illustration of the utility of our model, we turn to POS tagging. As morphology is a strong indicator for syntax in many languages, a much effort has been spent engineering features (Nakagawa et al., 2001; Mueller et al., 2013). We now show that some of these features can be learnt automatically using our model. eat embedings for words words1 .This 5 cats NNS VBP NN Figure 3: Illustration of our neural network for POS tagging. 5.2 Experiments Datasets For English, we conduct experiments on the Wall Street Journal of the Penn Treebank dataset (Marcus et al., 1993), using the standard splits (sections 1–18 for train, 19–21 for tuning and 22–24 for testing). We also perform tests on 4 other languages, which we obtained from the CoNLL shared tasks (Mart´ı et al., 2007; Brants et al., 2002; Afonso et al., 2002; Atalay et al., 2003). While the PTB dataset provides standard train, tuning and test splits, there are no tuning sets in the datasets in other languages, so we withdraw the last 100 sentences from the training dataset and use them for tuning. Setup The POS model requires two sets of hyperparameters. Firstly, words must be converted into continuous r"
D15-1176,D09-1078,0,0.0117625,"from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from b"
D15-1176,D13-1176,0,0.0372417,"onclude in Section 7. 2 Word Vectors and Wordless Word Vectors It is commonplace to represent words as vectors. In contrast to na¨ıve models in which all word types in a vocabulary V are equally different from each other, vector space models capture the intuition that words may be different or similar along a variety of dimensions. Learning vector representations of words by treating them as optimizable parameters in various kinds of language models has been found to be a remarkably effective means for generating vector representations that perform well in other tasks (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013; Liu et al., 2014; Chen and Manning, 2014). Formally, such models define a matrix P ∈ Rd×|V |, which contains d parameters for each word in the vocabulary V . For a given word type w ∈ V , a column is selected by right-multiplying P by a one-hot vector of length |V |, which we write 1w , that is zero in every dimension except for the element corresponding to w. Thus, P is often referred to as word lookup d table and we shall denote by eW w ∈ R the embedding obtained from a word lookup table for w as eW w = P · 1w . This allows tasks with low amounts of annotated data to be trained jointly wit"
D15-1176,kang-choi-2000-automatic,0,0.0359805,"tion according to the lexical model. Convolutional model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be r"
D15-1176,D15-1278,0,0.049515,"Missing"
D15-1176,D12-1088,1,0.613165,"Missing"
D15-1176,D13-1032,0,0.0412355,"Missing"
D15-1176,P11-2009,0,0.0797599,"Missing"
D15-1176,N15-1186,0,0.0289151,"se debate” between connectionists and proponents of symbolic accounts concerns disagreements about how humans represent knowledge of inflectional processes (e.g., the formation of the English past tense), not whether such knowledge exists (Marslen-Wilson and Tyler, 1998). 2.2 Solution: Compositional Models Our solution to these problems is to construct a vector representation of a word by composing smaller pieces into a representation of the larger form. This idea has been explored in prior work by composing morphemes into representations of words (Luong et al., 2013; Botha and Blunsom, 2014; Soricut and Och, 2015). Morphemes are an ideal primitive for such a model since they are— by definition—the minimal meaning-bearing (or syntax-bearing) units of language. The drawback to such approaches is they depend on a morphological analyzer. In contrast, we would like to compose representations of characters into representations of words. However, the relationship between words 1521 forms and their meanings is non-trivial (de Saussure, 1916). While some compositional relationships exist, e.g., morphological processes such as adding -ing or -ly to a stem have relatively regular effects, many words with lexical"
D15-1176,E09-1087,0,0.00853859,"Missing"
D15-1176,W13-2515,0,0.0187849,"al model are less susceptible to these problems as they combine windows of characters at each convolution, where the order within the window is preserved. However, the order between extracted windows is not, so the problem still persists for longer words, such as those found in agglutinative languages. Yet, these approaches work in conjunction with a word lookup table, as they compensate for this inability. Aside from neural approaches, characterbased models have been applied to address multiple lexically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence ou"
D15-1176,D12-1089,0,0.00506125,"xically oriented tasks, such as transliteration (Kang and Choi, 2000) and twitter normalization (Xu et al., 2013; Ling et al., 2013). Compacting models has been a focus of research in tasks, such as language modeling and machine translation, as extremely large models can be built with the large amounts of training data that are available in these tasks. In language modeling, it is frequent to prune higher order ngrams that do not encode any additional information (Seymore and Rosenfeld, 1996; Stolcke, 1998; Moore and Quirk, 2009). The same be applied in machine translation (Ling et al., 2012; Zens et al., 2012) by removing longer translation pairs that can be replicated using smaller ones. In essence our model learns regularities at the subword level that can be leveraged for building more compact word representations. Finally, our work has been applied to dependency parsing and found similar improvements over word models in morphologically rich languages (Ballesteros et al., 2015). 7 Conclusion We propose a C2W model that builds word embeddings for words without an explicit word lookup table. Thus, it benefits from being sensitive to lexical aspects within words, as it takes characters as atomic un"
D15-1176,D14-1179,0,\N,Missing
D15-1176,W03-2405,0,\N,Missing
I11-1006,2005.iwslt-1.8,0,0.117751,"are small, but per2 Lexicalized Reordering models In this section we will present the lexicalized reordering models approaches that are relevant for this work. 47 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 47–55, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP 2.1 Word-based Reordering prev word(s) The lexicalized reordering model is possibly the most used lexicalized reordering model and it calculates, as features, the reordering orientation for the previous and the next word, for each phrase pair. In the word-based reordering model (Axelrod et al., 2005), during the phrase extraction, given a source sentence S and a target sentence T , the alignment set A, where aji is an alignment from i to j, the phrase pair with words in positions between i and j in S, Sij , and n and m in T , Tnm , can be classified with one of three orientations with respect to the previous word. The orientation is a) source phrase next word(s) target phrase n−1 n−1 • Pc (p, S) = Wj+1 (1 − Wi−1 ) n−1 n−1 • Pc (p, D) = Wi−1 Wj+1 n−1 n−1 + (1 − Wi−1 )(1 − Wj+1 ) (1) 2.3 Phrase-based Reordering The problem with the word-based lexicalized reordering model is that it is assum"
I11-1006,P08-1115,0,0.0496024,"Missing"
I11-1006,D08-1089,0,0.366999,"gnment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). More recently, a more efficient representation of multiple alignments was proposed in (Liu et al., 2009) named weighted alignment matrices, which represents the alignment probability distribution over the words of each parallel sentence. The method for building a word-based lexicalized reordering model using these matrices is proposed in (Ling et al., 2011). However, phrase-based reordering models have been shown to perform better than word-based models for several language pairs (Tillmann, 2004; Su et al., 2010; Galley and Manning, 2008), such as Chinese-English and Arabic-English. Lexicalized reordering models play a central role in phrase-based statistical machine translation systems. Starting from the distance-based reordering model, improvements have been made by considering adjacent words in word-based models, adjacent phrases pairs in phrasebased models, and finally, all phrases pairs in a sentence pair in the reordering graphs. However, reordering graphs treat all phrase pairs equally and fail to weight the relationships between phrase pairs. In this work, we propose an extension to the reordering models, named weighte"
I11-1006,D09-1106,0,0.0357024,"Missing"
I11-1006,P08-1023,0,0.0619174,"Missing"
I11-1006,2010.iwslt-evaluation.1,0,0.0498423,"Missing"
I11-1006,P10-2003,0,0.193049,"Missing"
I11-1006,N04-4026,0,0.553008,"heses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). More recently, a more efficient representation of multiple alignments was proposed in (Liu et al., 2009) named weighted alignment matrices, which represents the alignment probability distribution over the words of each parallel sentence. The method for building a word-based lexicalized reordering model using these matrices is proposed in (Ling et al., 2011). However, phrase-based reordering models have been shown to perform better than word-based models for several language pairs (Tillmann, 2004; Su et al., 2010; Galley and Manning, 2008), such as Chinese-English and Arabic-English. Lexicalized reordering models play a central role in phrase-based statistical machine translation systems. Starting from the distance-based reordering model, improvements have been made by considering adjacent words in word-based models, adjacent phrases pairs in phrasebased models, and finally, all phrases pairs in a sentence pair in the reordering graphs. However, reordering graphs treat all phrase pairs equally and fail to weight the relationships between phrase pairs. In this work, we propose an exten"
I11-1006,J10-3007,1,0.878594,"Missing"
I11-1006,N03-1017,0,\N,Missing
I11-1006,P11-2079,1,\N,Missing
I11-1006,2010.iwslt-papers.14,1,\N,Missing
I11-1006,2008.amta-papers.18,0,\N,Missing
J16-2005,W10-0710,0,0.108835,"lations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, conte"
J16-2005,W12-2108,0,0.0274338,"weets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtained. To obtain this, we propose a simple crowdsourcing"
J16-2005,C10-2010,0,0.0214145,"the translation of xai . Model 1 naively assigns a uniform prior probability to all alignment configurations. Although this is an obviously flawed assumption, the posterior alignment probability under Model 1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. The"
J16-2005,J93-2003,0,0.116606,"Missing"
J16-2005,N13-1073,1,0.853048,"Missing"
J16-2005,2014.iwslt-papers.7,0,0.0308749,"also possible to focus the extraction in one particular type of phenomena. For example, the work on mining parenthetical translations (Lin et al. 2008), which attempts to find translations within the same document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quali"
J16-2005,W06-1008,0,0.0705046,"Missing"
J16-2005,P91-1023,0,0.541067,"n features - In informal domains, there are many terms that are not translated, such as hashtags (e.g., #twitter), at mentions (e.g., @NYC), numbers, and people’s names. The presence of such repeated terms in the same tweet can be a strong cue for detecting translations. Hence, we define features that trigger if a given word type occurs in a pair within a tweet. The word types considered are hashtags, at mentions, numbers, and words beginning with capital letters. Length feature - It has been known that the length differences between parallel sentences can be modeled by a normal distribution (Gale and Church 1991). Thus, we used parallel training data (used to train the alignment model) in the respective language pair to determine (µ˜ , σ˜ 2 ), which lets us calculate the likelihood of two hypothesized segments being parallel. For each language pair s, t, we train separate classifiers for each language pair on annotated parallel data Dgold (s, t). The method used to obtain the necessary annotations is described in Section 5. Intrinsic evaluation. The quality of the classifier can be determined in terms of precision and recall. We count one as a true positive (tp) if we correctly identify a parallel twe"
J16-2005,P11-2008,0,0.0478665,"Missing"
J16-2005,J11-4003,0,0.0223833,"te pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volumes of data involved, we can find a large amount of content. We must efficiently detect multilingual tweets, as monolingual tweets do not contain translations. Although language identification is a well-studied problem (Zissman 1996; Gottron and Lipka 2010; Greenhill 2011), even in the microblog domain (Bergsma et al. 2012), these assume that only one language is present within a document and cannot be directly applied to this problem. Furthermore, because of the magnitude of tweets that must be processed, many of the proposed solutions cannot be applied due to their computational complexity. In our work, we propose an efficient implementation for large-scale detection of multilingual documents. Crowdsourcing for parallel data extraction - To tune the parameters of our parallel data extractor and perform MT experiments, user-verified annotations must be obtaine"
J16-2005,P11-1038,0,0.0259193,"till with me or what?) and nonstandard abbreviations (idk! smh). Automated language processing tools (e.g., those that perform linguistic analysis or translation) face particular difficulty with this new kind of content. On one hand, these have been developed with the conventions of more edited genres in mind. For example, they often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmen"
J16-2005,W12-3153,0,0.0424014,"Missing"
J16-2005,D07-1103,0,0.0213949,"hus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated tweets is needed for different language pairs. Table 2 summarizes the annotated corpora for the two domains (column Source) and the different language pairs (column Language Pair). We also report the method used to obtain the annotati"
J16-2005,D14-1108,1,0.842008,"hey often make strong assumptions about orthographic and lexical uniformity (e.g., that there is just one way to spell you, and that cool, cooool, and cooooool represent completely unrelated lexical items). While modeling innovations are helping to relax these assumptions (Han and Baldwin 2011; Ritter et al. 2012; Owoputi et al. 2013; Ling et al. 2015), a second serious challenge is that many of the annotated text resources that our tools are learned from are drawn from edited genres, and poor generalization from edited to user-generated genres is a major source of errors (Gimpel et al. 2011; Kong et al. 2014). In this work, we present methods for finding naturally occurring parallel data on social media sites that is suitable for training machine translation (MT) systems. In MT, the domain mismatch problem is quite acute because most existing sources of parallel data are governmental, religious, or commercial, which are quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also"
J16-2005,I08-2120,0,0.190188,"he elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are"
J16-2005,P08-1113,0,0.0607684,"Missing"
J16-2005,D13-1008,1,0.90112,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,D15-1176,1,0.850166,"Missing"
J16-2005,W14-3356,1,0.854061,"parallel segment is correct. If so, the material is extracted, otherwise it is discarded. This research is an extension of the preliminary work described in Ling et al. (2013), in which we obtained over 1 million Chinese–English parallel segments from Sina Weibo, using only their public application program interface (API). This automatically extracted parallel data yielded substantial translation quality improvements in translating microblog text and modest improvements in translating edited news. Following this work, we developed a method for crowdsourcing judgments about parallel segments (Ling et al. 2014), which was then used to build gold standard data for other language pairs and for the Twitter domain. This article extends these two papers in several ways: r 310 Improved language pair detection - The previous work assumes that the language pair is formed by two languages with different unicode ranges, such as English–Chinese, and does not support the extraction of parallel data if the languages share the same unicode range (such as English–Portuguese). This issue is addressed in this article, where we present a novel approach for finding multilingual tweets. Ling et al. r r Mining Parallel"
J16-2005,P13-1018,1,0.731322,"e quite different from usergenerated content, both in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publ"
J16-2005,Q14-1003,0,0.0561496,"Missing"
J16-2005,J05-4003,0,0.318213,"the appropriately aligned parallel segments. Obviously, Ds,t will contain a considerable number of non-parallel segments, as multilingual messages in Tmult are not guaranteed to contain translated material. Furthermore, we must also consider errors from misalignments of the IDA model and misclassifications of the multilingual message detector. Thus, in order to identify messages that are actually parallel, a final identification step is necessary. 4.3 Identification Given a candidate sentence pair (s, t), many existing methods for detecting parallel data can be applied (Resnik and Smith 2003; Munteanu and Marcu 2005), as this problem becomes a regular unstructured bitext identification problem. In our initial work (Ling et al. 2013), we simply defined a threshold τ on the IDA model score, which was determined empirically. To obtain better results we train a logistic regression classifier for each language pair, similar to that presented in Munteanu and Marcu (2005), which detects whether two segments are parallel in a given language pair by looking at features of the candidate pair. Training is performed to maximize the classification decisions on annotated candidate pairs. 324 Ling et al. Mining Parallel"
J16-2005,N04-1034,0,0.0600954,"Missing"
J16-2005,P03-1021,0,0.0143381,"Missing"
J16-2005,N13-1039,1,0.773659,"regarding existing languages allows the detector to estimate the language probabilities more accurately. As we are using a character trigram model, a large amount of data is not required to saturate the model probabilities. Thus, for each language, we extract all data from Wikipedia up to a limit of 100K lines in order to keep the model compact. 7.1.3 Translation Lexicons. The IDA model uses translation lexicons to determine the translation score, as described in Section 3.1.1, which are estimated using parallel corpora. More specifically, we use the aligner described in Dyer, Chahuneau, and Smith (2013) to obtain the bidirectional alignments from the parallel sentences. Afterwards, we intersect the bidirectional alignments to obtain sure alignment points. Finally, we prune the lexicon using significance pruning (Johnson et al. 2007) with the threshold α +  (as defined in that work). The intersection and the pruning are performed to reduce the size of the lexicon to make the look-up faster. The breakdown of the different lexicons built is shown in Table 1. 7.2 Building Gold Standards To train and test the classifier described in Section 4.3, and perform MT experiments, a corpus of annotated"
J16-2005,P02-1040,0,0.0967467,"Missing"
J16-2005,P14-2110,0,0.0370301,"Missing"
J16-2005,W12-3152,0,0.0412989,"Missing"
J16-2005,J03-3002,0,0.467405,"s many challenges to current NLP and MT methods. As part of the elaboration of this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient"
J16-2005,N10-1063,0,0.0245615,"1 (i.e., PM1 (a |x, y)) is surprisingly informative. More robust models make less-naive prior assumptions and generally produce higher-quality alignments, but the uniform prior probability assumption simplifies the complexity of performing inference. Despite its simplicity, Model 1 has shown particularly good performance as a component in sentence alignment systems (Xu, Zens, and Ney 2005; Braune and Fraser 2010). Some work on parallel data extraction has also focused on extracting parallel segments from comparable corpora (Smith, Quirk, and Toutanova 2010; Munteanu, Fraser, and Marcu 2004). Smith et al. (2010) uses conditional random fields to identify parallel segments from comparable Wikipedia documents (since Wikipedia documents in multiple languages are not generally translations of each other, although they are about the same topics). The work of Jehl, Hieber, and Riezler (2012) uses cross-lingual information retrieval techniques to extract candidate English–Arabic translations from Twitter. These candidates are then refined using a more expressive model to identify translations (Xu, Weischedel, and Nguyen 2001). It is also possible to focus the extraction in one particular type of phenomena."
J16-2005,N12-1079,0,0.0918698,"following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but because of the volum"
J16-2005,C10-1124,0,0.3794,"this work, we made the following contributions to the field of NLP and MT. Figure 1 Examples of parallel posts in different language pairs and from different sources. Translated material is highlighted, non-translated material is not. DE = German; EN = English; ES = Spanish; PT = Portuguese; AR = Arabic; JA = Japanese; KO = Korean; RU = Russian; ZH = Chinese. 309 Computational Linguistics r r r Volume 42, Number 2 A method for extracting parallel data within documents - Existing methods for detecting parallel data (Resnik and Smith 2003; Fukushima, Taura, and Chikayama 2006; Li and Liu 2008; Uszkoreit et al. 2010; Ture and Lin 2012) assume that the source and target documents are separate. That is, these methods reason about the probability that documents A and B are translations. Previous work on extracting parallel data from Twitter (Jehl, Hieber, and Riezler 2012) retrieves candidate pairs of tweets and determines whether the tweets in the pair are translations. In our work, single tweets are considered, and the problem is to determine whether the tweet contains translations and, if so, which spans are parallel. Efficient multilingual document detection - Parallel tweets are a relative rarity, but"
J16-2005,D14-1122,0,0.0205903,"oth in language use and in topic. The extracted parallel data can then be used to create systems designed to translate user-generated content. Additionally, because microblogs host discussions of virtually limitless topics, they are also a potential source of information about how to translate names and words associated with breaking events, and, as such, may be useful for translation of texts from more traditional domains. Apart from machine translation, parallel data in this domain can improve and help create applications in other areas in NLP (Ling et al. 2013; Peng, Wang, and Dredze 2014; Wang et al. 2014). Our method is inspired by the (perhaps surprising) observation that a reasonable number of microblog users tweet “in parallel” in two or more languages. For instance, the American entertainer Snoop Dogg maintains an account on Sina Weibo that regularly posts English–Chinese parallel messages, for example, watup Kenny !!, where an English message and its Chinese Mayne!! - Kenny Mayne translation are in the same post, separated by a dash. It is not only celebrities (or 308 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter their publicists) who tweet in multiple languages; we see"
J16-2005,2005.eamt-1.37,0,0.0799727,"Missing"
J16-2005,P11-1122,0,0.136156,"document, has some similarities with our work, since parenthetical translations are within the same document. However, parenthetical translations are generally used to translate names or terms, which is more limited than our work targeting the extraction of whole sentence translations. More recently, a similar method for extracting parallel data from multilingual Facebook posts was proposed (Eck et al. 2014). 312 Ling et al. Mining Parallel Corpora from Sina Weibo and Twitter Finally, crowdsourcing techniques can be used to obtain translations of text from new domains (Ambati and Vogel 2010; Zaidan and Callison-Burch 2011; Ambati, Vogel, and Carbonell 2012; Post, Callison-Burch, and Osborne 2012). These approaches require compensating workers for their efforts, and the workers themselves must be generally proficient in two languages, making the technique quite expensive. Previous work has relied on employing workers to translate segments. Crowdsourcing methods must also address the need for quality control. Thus, in order to find good translations, subsequent post-editing and/or ranking is generally necessary. 3. The Intra-Document Alignment (IDA) Model As discussed above, content-based filtering is a method f"
J16-2005,J93-1004,0,\N,Missing
J16-2005,N03-1017,0,\N,Missing
L16-1615,A00-2029,0,0.130088,"Missing"
L16-1615,L16-1016,1,0.860727,"Missing"
L16-1615,schmitt-etal-2012-parameterized,0,0.023813,"Missing"
N15-1142,P14-2133,0,0.00813576,"ilt using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quali"
N15-1142,P14-2131,0,0.0361448,"e suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur with the word the, only nouns tend to occur exactly afterwords (e.g. the cat). This is supported by empirical evidence that suggests that order-insensitivity does indeed lead to substandard syntactic representations (Andreas and Klein, 2014; Bansal et al., 2014), where systems using pre-trained with Word2Vec models yield slight improvements while the computationally far more expensive which use word order information embeddings of Collobert et al. (2011) yielded much better results. 1299 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1299–1304, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics In this work, we describe two simple modifications to Word2Vec, one for the skip-gram model and one for the CBOW model, that improve the quality of the embeddings f"
N15-1142,D14-1082,0,0.416746,"o generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of"
N15-1142,P14-1129,0,0.00965647,"they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implement"
N15-1142,E14-1049,1,0.0586898,"isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing"
N15-1142,P11-2008,0,0.0164828,"Missing"
N15-1142,D14-1012,0,0.0227742,"g (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while semantics than “what words go together”. Obviously, in a model where word order is discarded, the many syntactic relations between words cannot be captured properly. For instance, while most words occur"
N15-1142,P12-1092,0,0.0570707,"yer,awb}@cs.cmu.edu isabel.trancoso@inesc-id.pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging"
N15-1142,D13-1176,0,0.00968706,"riginal models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et"
N15-1142,D14-1108,1,0.588959,"re suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely use"
N15-1142,P14-2050,0,0.0354569,".pt Abstract in particular the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models"
N15-1142,P14-1140,0,0.0252753,"n issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the"
N15-1142,N13-1039,1,0.692184,"Missing"
N15-1142,P10-1040,0,0.185776,"posed models. 1 Introduction Word representations learned from neural language models have been shown to improve many NLP tasks, such as part-of-speech tagging (Collobert et al., 2011), dependency parsing (Chen and Manning, 2014; Kong et al., 2014) and machine translation (Liu et al., 2014; Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). These low-dimensional representations are learned as parameters in a language model and trained to maximize the likelihood of a large corpus of raw text. They are then incorporated as features along side hand-engineered features (Turian et al., 2010), or used to initialize the parameters of neural networks targeting tasks for which substantially less training data is available (Hinton and Salakhutdinov, 2012; Erhan et al., 2010; Guo et al., 2014). One of the most widely used tools for building word vectors are the models described in (Mikolov et al., 2013), implemented in the Word2Vec tool, However, as these models are insensitive to word order, embeddings built using these models are suboptimal for tasks involving syntax, such as part-ofspeech tagging or dependency parsing. This is because syntax defines “what words go where?”, while sem"
N15-1142,N15-1069,0,0.0165027,"r the “skip-gram” and the “continuous bag-of-words” (CBOW) models. These two models make different independence and conditioning assumptions; however, both models discard word order information in how they account for context. Thus, embeddings built using these models have been shown to capture semantic information between words, and pre-training using these models has been shown to lead to major improvements in many tasks (Collobert et al., 2011). While more sophisticated approaches have been proposed (Dhillon et al., 2011; Huang et al., 2012; Faruqui and Dyer, 2014; Levy and Goldberg, 2014; Yang and Eisenstein, 2015), Word2Vec remains a popular choice due to their efficiency and simplicity. We present two simple modifications to the models in the popular Word2Vec tool, in order to generate embeddings more suited to tasks involving syntax. The main issue with the original models is the fact that they are insensitive to word order. While order independence is useful for inducing semantic representations, this leads to suboptimal results when they are used to solve syntax-based problems. We show improvements in part-ofspeech tagging and dependency parsing using our proposed models. 1 Introduction Word repres"
P08-2001,J96-1002,0,0.00481345,"Evaluation results may be influenced when taking such words into account (Kim and Woodland, 2004). The evaluation is performed using the metrics: Precision, Recall and SER (Slot Error Rate) (Makhoul et al., 1999). Only capitalized words (not lowercase) are considered as slots and used by these metrics. For example: Precision is calculated by dividing the number of correct capitalized words by the number of capitalized words in the testing data. The modeling approach here described is discriminative, and is based on maximum entropy (ME) models, firstly applied to natural language problems in (Berger et al., 1996). An ME model estimates the conditional probability of the events given the corresponding features. Therefore, all the information must be expressed in terms of features in a pre-processing step. Experiments here described only use features comprising word unigrams and bigrams: wi (current word), hwi−1 , wi i and hwi , wi+1 i (bigrams). Only words occurring more than once were included for training, thus reducing the number of misspelled words. All the experiments used the MegaM tool (Daumé III, 2004), which uses conjugate gradient and a limited memory optimization of logistic regression. The"
P08-2001,W04-3237,0,0.0169501,"gibility of texts is strongly influenced by this information. Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing and e-mail clients, perform automatic capitalization along with spell corrections and grammar check. The capitalization problem can be seen as a sequence tagging problem (Chelba and Acero, 2004; Lita et al., 2003; Kim and Woodland, 2004), where each lower-case word is associated to a tag that describes its capitalization form. (Chelba and Acero, 2004) study the impact of using increasing amounts of training data as well as a small amount of adaptation. This work uses a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with ca"
P08-2001,W99-0613,0,0.0130251,"oth from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance of a named tagger based on co-training (Collins and Singer, 1999) decreases. This paper studies and evaluates the effects of language dynamics in the capitalization of newspaper 1 Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 1–4, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics corpora. Section 2 describes the corpus and presents a short analysis on the lexicon variation. Section 3 presents experiments concerning the capitalization task, either using isolated training sets or by retraining with different training sets. Section 4 concludes and presents future plans. 2 Newspaper Corpus Experiments here des"
P08-2001,P03-1020,0,0.207582,"anguage variations and the way it affects the capitalization task over time. A discriminative approach, based on maximum entropy models, is proposed to perform capitalization, taking the language changes into consideration. The proposed method makes it possible to use large corpora for training. The evaluation is performed over newspaper corpora using different testing periods. The achieved results reveal a strong relation between the capitalization performance and the elapsed time between the training and testing data periods. 1 Introduction The capitalization task, also known as truecasing (Lita et al., 2003), consists of rewriting each word of an input text with its proper case information. The capitalization of a word sometimes depends on its current context, and the intelligibility of texts is strongly influenced by this information. Different practical applications benefit from automatic capitalization as a preprocessing step: when applied to speech recognition output, which usually consists of raw text, automatic capitalization provides relevant information for automatic content extraction, named entity recognition, and machine translation; many computer applications, such as word processing"
P08-2001,N06-1001,0,0.236925,"a Maximum Entropy Markov Model (MEMM) based approach, which allows to combine different features. A large written newspaper corpora is used for training and the test data consists of Broadcast News (BN) data. (Lita et al., 2003) builds a trigram language model (LM) with pairs (word, tag), estimated from a corpus with case information, and then uses dynamic programming to disambiguate over all possible tag assignments on a sentence. Other related work includes a bilingual capitalization model for capitalizing machine translation (MT) outputs, using conditional random fields (CRFs) reported by (Wang et al., 2006). This work exploits case information both from source and target sentences of the MT system, producing better performance than a baseline capitalizer using a trigram language model. A preparatory study on the capitalization of Portuguese BN has been performed by (Batista et al., 2007). One important aspect related with capitalization concerns the language dynamics: new words are introduced everyday in our vocabularies and the usage of some other words decays with time. Concerning this subject, (Mota, 2008) shows that, as the time gap between training and test data increases, the performance o"
P11-2079,P08-1115,0,0.061136,"IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propose two methods for generating th"
P11-2079,P08-1112,1,0.800281,"Missing"
P11-2079,P07-2045,0,0.00398059,"nd d) are classified as discontinuous. given by: P (p, mono) = C(mono) C(mono)+C(swap)+C(disc) (1) Where C(o) is the number of times a phrase is extracted with the orientation o in that group of phrase pairs. Moses also provides many options for this stage, such as types of smoothing. We use the default smoothing configuration which adds the fixed value of 0.5 to all C(o). 3 • The orientation is monotonous if only the previous word in the source is aligned with the previous word in the target, or, more formally, if n−1 an−1 / A. i−1 ∈ A ∧ aj+1 ∈ source phrase b) prev word(t) MSD models Moses (Koehn et al., 2007) allows many configurations for the reordering model to be used. In this work, we will only refer to the default configuration (msd-bidirectional-fe), which uses the MSD model, and calculates the reordering orientation for the previous and the next word, for each phrase pair. Other possible configurations are simpler than the default one. For instance, the monotonicity model only considers monotone and non-monotone orientation types, whereas the MSD model also considers the monotone orientation type, but distinguishes the non-monotone orientation type between swap and discontinuous. The approa"
P11-2079,D09-1106,0,0.150493,"Missing"
P11-2079,P08-1023,0,0.0774305,"matrices. Experiments on the IWSLT 2010 evaluation datasets for two language pairs with different alignment algorithms show that our methods produce more accurate reordering models, as can be shown by an increase over the regular MSD models of 0.4 BLEU points in the BTEC French to English test set, and of 1.5 BLEU points in the DIALOG Chinese to English test set. 1 Introduction The fact that spurious word alignments might occur leads to the use of alternative representations for word alignments that allow multiple alignment hypotheses, rather than the 1-best alignment (Venugopal et al., 2009; Mi et al., 2008; Christopher Dyer et al., 2008). While using n-best alignments yields improvements over using the 1-best alignment, these methods are computationally expensive. More recently, the method described in (Liu et al., 2009) produces improvements over the methods above, while reducing the computational cost by using weighted alignment matrices to represent the alignment distribution over each parallel sentence. However, their results were limited by the fact that they had no method for extracting a reordering model from these matrices, and used a simple distance-based model. In this paper, we propo"
P11-2079,2010.iwslt-evaluation.1,0,0.122315,"Missing"
P11-2079,J10-3007,1,0.84267,"Missing"
P11-2079,2006.iwslt-papers.7,0,0.0248528,"istical phrase-based systems (Koehn et al., 2003) is heavily dependent on the quality of the translation and reordering models generated during the phrase extraction algorithm (Ling et al., 2010). The basic phrase extraction algorithm uses word alignment information to constraint the possible phrases that can be extracted. It has been shown that better alignment quality generally leads to better results (Ganchev et al., 2008). However the relationship between the word alignThis paper is organized as follows: Section 2 dement quality and the results is not straightforward, and it was shown in (Vilar et al., 2006) that better scribes the MSD model; Section 3 presents our two alignments in terms of F-measure do not always lead algorithms; in Section 4 we report the results from the experiments conducted using these algorithms, to better translation quality. 450 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 450–454, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics and comment on the results; we conclude in Section 5. 2 prev word(s) a) • The orientation is swap, if only the next word in the source is aligned w"
P11-2079,N03-1017,0,\N,Missing
P11-2079,2010.iwslt-papers.14,1,\N,Missing
P11-2079,2008.amta-papers.18,0,\N,Missing
P13-1018,2005.iwslt-1.8,0,0.0958689,"Missing"
P13-1018,C10-2010,0,0.0480398,"Missing"
P13-1018,J93-2003,0,0.0313653,"Missing"
P13-1018,W06-1008,0,0.429388,"Missing"
P13-1018,P11-2008,0,0.0238097,"Missing"
P13-1018,W12-3153,0,0.264353,"Missing"
P13-1018,N03-1017,0,0.0156335,"Missing"
P13-1018,P08-1113,0,0.141099,"Missing"
P13-1018,P03-1021,0,0.0199252,"Missing"
P13-1018,P02-1040,0,0.105883,"Missing"
P13-1018,W12-3152,0,0.0549925,"Missing"
P13-1018,J03-3002,0,0.404661,"Missing"
P13-1018,N10-1063,0,0.0989051,"Missing"
P13-1018,N12-1079,0,0.229942,"Missing"
P13-1018,C10-1124,0,0.0203108,"Missing"
P13-1018,2005.mtsummit-papers.11,0,0.147745,"Missing"
P13-1018,C96-2141,0,0.295961,"Missing"
P13-1018,I08-2120,0,0.398573,"Missing"
P13-1018,2005.eamt-1.37,0,0.138002,"Missing"
P13-1018,N12-1006,0,0.0533017,"Missing"
P13-1018,N03-1031,0,\N,Missing
P13-1076,P11-1144,0,0.05362,"ive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The derived label distributions are regarded as prior knowledge to regulariz"
P13-1076,P11-1061,0,0.0350223,"ription Trigram + Context Trigram Left Context Right Context Center Word Trigram - Center Word Left Word + Right Context Right Word + Left Context Type of Trigram: number, punctuation, alphabetic letter and other In most graph-based label propagation tasks, the final effect depends heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned that taking individual characters as the vertices would result in various ambiguities, whereas the similarity measurement is still challenging if vertices corresponding to entire sentences. This study follows the intuitions of graph construction from Subramanya et al. (2010) in which vertices are represented by character trigrams occurring in labeled and unlabeled sentences. Formally, given a set of labeled sentences Dl , and unlabeled ones Du , where D , {Dl , Du }, the goal is to form an undirected weighted graph G = (V, E), where V is defined as the set of vertices which covers a"
P13-1076,N12-1086,0,0.657363,"o be a probability distribution, and fk (yij−1 , yij , xi , j). In this study, 771 the baseline feature templates of joint S&T are the ones used in (Ng and Low, 2004; Jiang et al., 2008), as shown in Table 1. Λ = {λ1 λ2 ...λK } ∈ RK are the weight parameters to be learned. In supervised training, the aim is to estimate the Λ that maximizes the conditional likelihood of the training data while regularizing model parameters: L(Λ) = l X i=1 log p(yi |xi ; Λ) − R(Λ) properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). (2) 4 The emphasis of this work is on building a joint S&T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a transductive graph-based label propagation"
P13-1076,P08-1102,0,0.01861,"n features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach p(yi |xi ; Λ) = N K XX 1 exp{ λk fk (yij−1 , yij , xi , j)} Z(xi ; Λ) j=1 k=1 (1) where Z(xi ; Λ) is the partition function that normalizes the exponential form to be a probability distribution, and fk (yij−1 , yij , xi , j). In this study, 771 the baseline feature templates of joint S&T are the ones used in (Ng and Low, 2004; Jiang et al., 2008), as shown in Table 1. Λ = {λ1 λ2 ...λK } ∈ RK are the weight parameters to be learned. In supervised training, the aim is to estimate the Λ that maximizes the conditional likelihood of the training data while regularizing model parameters: L(Λ) = l X i=1 log p(yi |xi ; Λ) − R(Λ) properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Induc"
P13-1076,P09-1059,0,0.0833383,"h Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. G"
P13-1076,P06-1027,0,0.0785357,"g interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011)."
P13-1076,P09-1058,0,0.158693,"Missing"
P13-1076,P10-1149,0,0.129322,"Missing"
P13-1076,D09-1134,0,0.290005,"04). The joint approaches of word segmentation and POS tagging (joint S&T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation 770 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770–779, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics labeled and unlabeled data to achieve the semisupervised learning. The approach performs the incorporation of the derived labeled distributions by manipulating a “virtual evidence” function as described in (Li, 2009). Experiments on the data from the Chinese tree bank (CTB-7) and Microsoft Research (MSR) show that the proposed model results in significant improvement over other comparative candidates in terms of F-score and out-of-vocabulary (OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The c"
P13-1076,I11-1035,0,0.0980357,"in, development and test set were assigned with the composite tags as described in Section 3.1. 3 775 5.3 based label propagation toolkit that provides several state-of-the-art algorithms. Data Train Develop Test #Sent 17,968 1,659 2,037 #Word 374,697 46,637 65,219 #Char 596,360 79,283 104,502 #OOV 0.074 0.089 Table 3: Training, development and testing data. 5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&T models. The corresponding settings of the above candidates are listed below: Main Results This experiment yielded a similarity graph that consists of 462,962 trigrams from labeled and unlabeled data. The majority (317,677 trigrams) occurred only in unlabeled data. Based on the development data, the hyperparameters of our model were tuned among the following settings: for the graph propagation, µ ∈ {0.2, 0.5, 0.8}"
P13-1076,N07-2028,0,0.361293,"ults in significant improvement over other comparative candidates in terms of F-score and out-of-vocabulary (OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The proposed approach regularizes the CRFs by the graph information. Subramanya et al. (2010) proposed a graph-based self-train style semi-supervised CRFs algorithm. In the proposed approach, an analogous way of graph construction intuition is applied. But overall, our approach differs in three important aspects: first, novel feature templates are defined for measuring the similarity between vertices. Second, the critical property, i.e., sparsity, is considered amon"
P13-1076,I08-4029,0,0.0551474,"Missing"
P13-1076,P07-2055,0,0.0139759,"Missing"
P13-1076,C08-1128,0,0.0497034,"Missing"
P13-1076,W04-3236,0,0.787263,"inese Word Segmentation and Part-of-Speech Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to"
P13-1076,P08-1101,0,0.0453526,"ation and Part-of-Speech Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervise"
P13-1076,D12-1046,0,0.291926,"Missing"
P13-1076,D10-1082,0,0.0618309,"eng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propaga"
P13-1076,Y06-1012,0,0.0152817,"Missing"
P13-1076,D10-1017,0,0.360523,", i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propag"
P13-1076,P11-1139,0,0.0830342,"Missing"
P13-1076,D11-1090,0,0.152421,"Missing"
P13-1076,D08-1061,0,0.239557,"he production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The de"
P13-2031,W02-1001,0,0.0100895,"representational power and consequently better recognition performance for in-ofvocabulary (IV) words. where Z(x; θc ) is a partition function that normalizes the exponential form to be a probability distribution, and f (x, y) are arbitrary feature functions. The aim of CRFs is to estimate the weight parameters θc that maximizes the conditional likelihood of the training data: θˆc = argmax φ(x, wi ) · θw where it maps the segmented sentence w to a global feature vector φ and denotes θw as its corresponding weight parameters. The parameters θw can be estimated by using the Perceptrons method (Collins, 2002) or other online learning algorithms, e.g., Passive Aggressive (Crammer et al., 2006). For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. Character-based CRFs Model pθc (y|x) = |w| X Semi-supervised Learning via Co-regularizing Both Models As mentioned earlier, the primary challenge of semi-supervised CWS concentrates on the unlabeled data. Obviously, the learning on unlabeled data does not come for “free”. Very often, it is necessary to discover certain gainful information, e.g., label constraints of unlabeled data, that is incorporated to guide the learner towar"
P13-2031,P12-1110,0,0.0343364,"Missing"
P13-2031,P08-1102,0,0.0227771,"e α value is tuned using the development data. 4 #Sentdev 350 2,079 10,136 Table 1: Statistics of CTB-5, CTB-6 and CTB-7 data. The Joint Score Function for Decoding Score(w) = α · log(pθc (y|x)) +(1 − α) · log(φ(x, w) · θw ) #Senttrain 18,089 23,420 31,131 Experiment Setting The experimental data is taken from the Chinese tree bank (CTB). In order to make a fair comparison with the state-of-the-art results, the versions of CTB-5, CTB-6, and CTB-7 are used for the evaluation. The training, development and testing sets are defined according to the previous works. For CTB-5, the data split from (Jiang et al., 2008) is employed. For CTB-6, the same data split as recommended in the CTB-6 official document is used. For CTB-7, the datasets are formed according to the way in (Wang et al., 2011). The corresponding statistic information on these data splits is reported in Table 1. The unlabeled data in 1 The “baseline” uses a different training configuration so that the α values in the decoding are also need to be tuned on the development sets. The tuned α values are {0.6, 0.6, 0.5} for CTB-5, CTB-6 and CTB-7. 174 bold scores indicate that our model does achieve significant gains over these two semi-supervised"
P13-2031,P09-1059,0,0.144133,"Missing"
P13-2031,P11-1139,0,0.0489767,"to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-based. This section briefly reviews t"
P13-2031,D11-1090,0,0.45216,"duction of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model"
P13-2031,Q13-1001,0,0.0846245,"Missing"
P13-2031,C10-1132,0,0.0370132,"Missing"
P13-2031,I11-1035,0,0.312676,"data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreemen"
P13-2031,C08-1128,0,0.110523,"´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract ta. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are use"
P13-2031,O03-4002,0,0.532995,"texts without label information. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: charact"
P13-2031,P07-1106,0,0.406907,"on. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-based. This section brief"
P13-2031,D10-1082,0,0.0377545,"Missing"
P13-2031,J11-1005,0,0.0141516,"each parameter CRFs with Constraints For the character-based model, this paper follows (T¨ackstr¨om et al., 2013) to incorporate the segmentation agreements into CRFs. The main 173 our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). update iteration k, each raw sentence xu is decoded with the current model into a segmentation zu . If the words in output zu do not match the agreements A(xu ) of the current sentence xu , the parameters are updated by adding the global feature vector of the current training example with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model 1: for k = 1...K, u = 1...m do P|w| k−1 2: calculate zu = argmax i=1 φ(xu , wi ) · θw Data CTB-5 CTB-6 CTB-7 w=GEN(x) 3: if zu 6= A(x"
P13-2031,Y06-1012,0,0.201841,"out label information. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-ba"
P13-2031,C10-2139,0,\N,Missing
P13-2031,P06-1027,0,\N,Missing
P13-4011,robinson-etal-2008-ask,0,0.0378918,"Missing"
P13-4011,2005.sigdial-1.26,0,0.0150458,"g and deploying kiosks. We will provide the hardware and software required to demonstrate E DGAR, both on a computer and on a tablet. This paper is organized as follows: in Section 2 we present E DGAR’s development platform Introduction Several initiatives have been taking place in the last years, targeting the concept of Edutainment, that is, education through entertainment. Following this strategy, virtual characters have animated several museums all over the world: the 3D animated Hans Christian Andersen is capable of establishing multimodal conversations about the writer’s life and tales (Bernsen and Dybkjr, 2005), Max is a virtual character employed as guide in the Heinz Nixdorf Museums Forum (Pfeiffer et al., 2011), and Sergeant Blackwell, installed in the Cooper-Hewitt National Design Museum in New York, is used by the U.S. Army Recruiting Command as a hi-tech attraction and information source (Robinson et al., 61 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 61–66, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Figure 2: E DGAR architecture and describe typical interactions, in Section 3 we show how we move from in"
P13-4011,W06-1303,0,0.0292703,"tions are declared in the knowledge sources of the agent. As shown in Figure 3, they are coordinated with viseme animations. This XML files have multilingual pairs constituted by different paraphrases of the same question and possible answers. The main reason to follow this approach (and contrary to other works where grammars are used), is to ease the process of creating/enriching the knowledge sources of the agent being developed, which is typically done by non experts in linguistics or computer science. Thus, we opted for following a similar approach of the work described, for instance, in (Leuski et al., 2006), where the agents knowledge sources are easy to create and maintain. An example of a questions/answers pair is: <questions&gt; <q en=&quot;How is everything?&quot; es=&quot;Todo bien?&quot;&gt; Tudo bem?</q&gt; </questions&gt; <answers&gt; <a en=&quot;I am ok, thank you.&quot; es=&quot;Estoy bien, gracias.&quot; emotion=&quot;smile_02&quot;&gt; Estou bem, obrigado.</a&gt; </answers&gt; Figure 3: The E DGAR character in a joyful state. 2.3 Interacting with E DGAR In a typical interaction, the user enters a question with a virtual keyboard or says it to the microphone while pressing a button (Figure 4), in the language chosen in the interface (as previously said, Por"
P14-1128,W05-0909,0,0.0182822,"This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q (t+1) = arg minRU (θ(t) , q (t) ) q M-step: θ(t+1) = arg maxL(θ) θ +δ u X X i=1 y∈Y 4 q (t+1) (y|xi ) log pθ (y|xi ) (7) Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over"
P14-1128,W08-0336,0,0.546288,"accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding"
P14-1128,D09-1075,0,0.140948,"nstraints over labelings to bias the model learning. The type-level word boundary extraction is formally described as follows. Given the ith sentence pair hxci , xfi , Ac→f i of the aligned bilini c↔f gual corpus D , the Chinese sentence xci consisting of m characters {xci,1 , xci,2 , ..., xci,m }, and the foreign language sentence xfi , consisting of The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in (Xu et al., 2004; Ma and Way, 2009; Chung and Gildea, 2009) to learn bilingual seg1 The distribution is on four word boundary labels indicating the character positions in a word, i.e., B (begin), M (middle), E (end) and S (single character). 2 A word boundary distribution corresponds to the center character of a type. In fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters (Altun et al., 2006). 1362 n words {xfi,1 , xfi,2 , ..., xfi,n }, Ac→f represents a i hCj , xfi,j i set of alignment pairs aj = that defines connections between a few Chinese characters Cj = {xci,j1 , xc"
P14-1128,P11-1061,0,0.149239,"nments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning"
P14-1128,N12-1086,0,0.0179428,"onstraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dlc and bitexts Duc . Each vertex Vi has a |T |-dimensional estimated measure vi = {vi,t ; t ∈ T } representing a probability distribution on word boundary tags. The induced typelevel word boundary distributions ri = {ri,t ; t ∈ T } are empirical measures for the corresponding M graph vertices. The edges E ∈ Vi × Vj connect all the vertices. Scores between pairs of graph vertices (types), wij , refer to the similarities of their syntactic environment, which are computed followin"
P14-1128,W13-3505,0,0.0555978,"xi ) Where Zθ (xi ) is a partition function that normalizes the exponential form to be a probability distribution, and f (yik−1 , yik , xi ) are arbitrary feature functions. In our setting, the CRFs model is required to learn from unlabeled data. This work employs the posterior regularization (PR) framework3 (Ganchev et al., 2010) to bias the CRFs model’s learning on unlabeled data, under a constraint encoded by the graph propagation expression. It is expected that similar types in the graph should have approximated expected taggings under the CRFs model. We follow the approach introduced by (He et al., 2013) to set up a penaltybased PR objective with GP: the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints: RU (θ, q) = KL(q||pθ ) + λP(v) (4) Rather than regularize CRFs model’s posteriors pθ (Y|xi ) directly, our model uses an auxiliary distribution q(Y|xi ) over the possible labelings 3 The readers are refered to the original paper of Ganchev et al. (2010). m X T X X 1(y b = t, y b−1 = c)q(y|xa ) c=1 y∈Y b=1; M(a,b)=Vi u X m X 1(M(a, b) = Vi ) a=1 b=1 (5) The final learning objective combines the CRFs likelihood with the PR regulari"
P14-1128,P07-2045,0,0.0083261,"o be tuned by using the development data (devMT ) among the following settings: for the graph propagation, µ ∈ {0.2, 0.5, 0.8} and ρ ∈ {0.1, 0.3, 0.5, 0.8}; for the PR learning, λ ∈ {0 ≤ λi ≤ 1} and σ ∈ {0 ≤ σi ≤ 1} where the step is 0.1. The best performed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9 and σ = 0.8, were used to measure the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that al"
P14-1128,E09-1063,0,0.697137,"ffects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually lin"
P14-1128,2005.iwslt-1.18,0,0.0267695,"odel induction is accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008)."
P14-1128,P08-1099,0,0.0270901,"wledge. This is accomplished by the posterior regularization (PR) framework (Ganchev et 1361 al., 2010). PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum (2008) and McCallum et al. (2007) proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations. 3 Methodology This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented Chinese sentences from treebank Dlc and parallel unsegmented sentences of Chinese and foreign language Duc and Duf . The first step is to conduct characterbased alignment over bitexts Duc and Duf , where every Chi"
P14-1128,J03-1002,0,0.0487003,"upervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmenta1360 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360–1369, c Baltimore"
P14-1128,P03-1021,0,0.0722543,"the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models: • Character Segmenter (CS): this model simply divides Chinese sentences into sequences of characters. • Supervised Monolingual Segmenter (SMS): this model is trained by CRFs on treebank training data (trainTB ). The same feature templates (Zhao et al"
P14-1128,P02-1040,0,0.0913939,"ameter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q (t+1) = arg minRU (θ(t) , q (t) ) q M-step: θ(t+1) = arg maxL(θ) θ +δ u X X i=1 y∈Y 4 q (t+1) (y|xi ) log pθ (y|xi ) (7) Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT t"
P14-1128,D10-1017,0,0.364816,"erging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bia"
P14-1128,tian-etal-2014-um,1,0.820676,"ated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM , crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT , and testing data, testMT , respectively. For the settings of our model, we adopted the standard feature temp"
P14-1128,P12-2056,0,0.15256,"the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. In recent years, a number of works (Xu et al., 2005; Chang et al., 2008; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled da"
P14-1128,P13-1076,1,0.900623,"rvisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning on unlabeled data,"
P14-1128,W03-1730,0,0.0468148,"• Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve two popular off-the-shelf segmentation models: • Stanford Segmenter: this model, trained by Chang et al. (2008), treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. This work also evaluated four variant models9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. • Self-training Segmenters (STS): two variant models were defined by the approach reported in (Subramanya et al., 2010) that uses the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examp"
P14-1128,W08-0335,0,0.178558,"results of the proposed model for a Chinese-to-English MT task. The conclusion is drawn in Section 5. 2 Related Work In the literature, many approaches have been proposed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. Chang et al. (2008) enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. (2008) produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum match"
P14-1128,W06-0127,0,0.249394,"2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM , crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT , and testing data, testMT , respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7 The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8 . Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2 w3 w4 ” surrounding contexts “w1 w2 w3 w4 w5 ”): unigram (w3 ), bigram (w1 w2 , w4 w5 , w2 w4 ), trigram (w2 w3 w4 , w2 w4 w5 , w1 w2 w4 ), trigram+context (w1 w2"
P14-1128,W04-1118,0,0.674584,"; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not o"
P15-1104,S15-2109,1,0.847734,"Missing"
P15-1104,P01-1005,0,0.117923,". All the word representations are adapted through a projection into this task-specific subspace, even if they do not occur on the labeled dataset. This approach was recently used in the SemEval 2015 Twitter sentiment analysis challenge, attaining state-of-the-art results. Here we show results improving those of the challenge, as well as additional experiments in a Twitter Part-Of-Speech tagging task. 1 Introduction The success of supervised systems largely depends on the amount and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that"
P15-1104,P14-2131,0,0.018946,"beddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on additional training data to fine tune the embeddings for a given supervised task. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced"
P15-1104,J92-4003,0,0.106276,"d Work NLP systems can benefit from a very large pool of unlabeled data. While raw documents are usually not annotated, they contain structure, which can be leveraged to learn word features. Context is one strong indicator for word similarity, as related words tend to occur in similar contexts (Firth, 1968). Approaches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are"
P15-1104,D14-1082,0,0.00356403,"this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representatio"
P15-1104,C14-1008,0,0.42499,"eatures. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in the predictions are backpropagated to update the embeddings, so that they better predict the supervised signal (Collobert et al., 2011; dos Santos and Gatti, 2014a). However, this strategy faces an additional challenge in noisy domains, such as social media. The lexical variation caused by the typos, use of slang and abbreviations leads to a great number of singletons and out-of-vocabulary words. For these words, the embeddings will be poorly reestimated. Even worse, words not present on the training set will never get their embeddings updated. In this paper, we describe a strategy to adapt unsupervised word embeddings when dealing with small and noisy labeled datasets. The intuition behind our approach is the following. For a given task, only a subset"
P15-1104,N15-1142,1,0.552511,"than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purpos"
P15-1104,P11-2008,0,0.0559975,"Missing"
P15-1104,P12-1092,0,0.037627,"and quality of the available training data, oftentimes, even more than the particular choice of learning algorithm (Banko and Brill, 2001). Labeled data is, however, expensive to obtain, while unlabeled data is widely available. In order to exploit this fact, semi-supervised learning methods can be used. In particular, it is possible to derive word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic,"
P15-1104,P13-2087,0,0.0224118,"ed data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn taskspecific representations from general pre-trained embeddings was presented. In this work, new features were estimated with a convex objective function that combined the log-likelihood of the training data, with regularization penalizing the Frobenius norm of the distortion matrix. That is, the matrix of the differences between the original and the new embeddings. Even though the adapted embeddings performed better than the purely unsupervised features, both were significantly outperformed by a simple bag-of-words baseline. Most other approaches, simply rely on add"
P15-1104,S14-2111,0,0.0573251,"he NLSE model, applied to sentiment polarity prediction. Development Tweets 2015 Tweets 2014 Tweets 2013 Positive 3230 1032 982 1572 Neutral 4109 983 669 1640 Negative 1265 364 202 601 Table 1: Number of examples per class in each SemEval dataset. The first row shows the training data; the other rows are sets used for testing. need to be enriched with additional hand-crafted features that try to capture more discriminative aspects of the content, most of which require external tools (e.g., part-of-speech taggers and parsers) or linguistic resources (e.g., dictionaries and sentiment lexicons) (Miura et al., 2014; Kiritchenko et al., 2014). With the embedding sub-space approach, however, we are able to attain state-ofthe-art performance while requiring only minimal processing of the data and few hyperparameters. To make our results comparable to other systems for this task, we adopted the guidelines from the benchmark. Our system was trained and tuned using only the development data. The evaluation was performed on the test sets, shown in Table 1, and we report the results in terms of the average F-measure for the positive and negative classes. Experimental Setup The first step of our approach require"
P15-1104,N13-1039,0,0.165772,"ches that are based on this concept include, Latent Semantic Analysis, where words are represented as rows in the lowrank approximation of a term co-occurrence matrix (Dumais et al., 1988), word clusters obtained with hierarchical clustering algorithms based on Hidden Markov Models (Brown et al., 1992), and continuous word vectors learned with neural language models (Bengio et al., 2003). The resulting clusters and vectors, can then be used as more generalizable features in supervised tasks, as they also provide representations for words not present in the labeled data (Bespalov et al., 2011; Owoputi et al., 2013; Chen and Manning, 2014). A great amount of work has been done on the problem of learning better word representations from unsupervised data. However, not many studies have discussed the best ways to use them in supervised tasks. Typically, in these cases, word representations are directly used as features or to initialize the parameters of more complex models. In some tasks, this approach is however prone to overfitting. The work presented here aims to provide a simple approach to overcome this last scenario. It is thus directly related to Labutov and Lipson (2013), where a method to learn t"
P15-1104,D14-1162,0,0.111819,"Missing"
P15-1104,petrov-etal-2012-universal,0,0.00763791,"Aside from embedding E and 1080 sub-space S matrices, the model is parametrized by the weights H ∈ Rh×ps and Y ∈ Rv×h as well as a bias b ∈ Rv×1 . Note that if S is set to the identity matrix, this would be equivalent to the original Collobert et al. (2011) model. Some ppl r juz unrealiable Word Embeddings Subspace Window Tanh Softmax over Tags Verb Figure 4: Illustration of the window model by (Collobert et al., 2011) using a sub-space layer. 6.2 Experiments Tests were performed in Gimpel et al. (2011) Twitter POS dataset, which uses the universal POS tag set composed by 25 different labels (Petrov et al., 2012). The dataset contains 1000 annotated tweets for training, 327 tweets for tuning and 500 tweets for testing. The number of word tokens in these sets are 15000, 5000 and 7000, respectively. There are 5000, 2000 and 3000 word types. Once again, we initialized the embeddings with unsupervised pre-training using the structured skip-gram approach. As for the hyperparameters of the model, we used embeddings with e = 50 dimensions, a hidden layer with h = 200 dimensions and a context of p = 2 as used in (Ling et al., 2015). Training employed mini-batch gradient descent, with mini batches of 100 sente"
P15-1104,S15-2078,0,0.00576043,"ain low dimensional embeddings fitting the complexity of the target task. On the other hand, we are able to learn new representations for all the words, even if they do not occur in the labeled dataset. To estimate the low dimensional sub-space, we propose a simple non-linear model equivalent to a neural network with one single hidden layer. The model is trained in supervised fashion on the labeled dataset, learning jointly the sub-space projection and a classifier for the target task. Using this model, we built a system to participate in the SemEval 2015 Twitter sentiment analysis benchmark (Rosenthal et al., 2015). Our submission attained state-of-the-art results without hand-coded features or linguistic resources (Astudillo et al., 2015). Here, we further investigate this approach and compare it against several state-of-the-art systems for Twitter sentiment classification. We also report on additional experiments to assess the adequacy of this strategy in other natural language problems. To this end, we apply the embedding sub-space layer to Ling et al. (2015) deep learning model for part-of-speech tagging. Even though the gains were not as significant as in the sentiment polarity prediction task, the"
P15-1104,S15-2079,0,0.0769896,"Missing"
P15-1104,S14-2033,0,0.0614964,"sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed"
P15-1104,P14-1146,0,0.147691,"sk. In Bansal et al. (2014), better word embeddings for dependency parsing were obtained by using a corpus created to capture dependency context. This technique requires, nevertheless, of a pre-existing dependency parser or, at least a parsed corpus. For some other tasks, it is possible to collect weakly labeled corpora by making strong assumptions about the data. In Go et al. (2009) a corpus for Twitter sentiment analysis was built by assuming that tweets with positive emoticons imply positive sentiment, whereas tweets with negative emoticons imply negative sentiment. Using a similar corpus, Tang et al. (2014b) induced sentiment specific word embeddings, for the Twitter domain. The embeddings 1075 were estimated with a neural network that minimized a linear combination of two loss functions, one penalized the errors made at predicting the center word within a sequence of words, while the other penalized mistakes made at deciding the sentiment label. Weakly labeled data has also been used to refine unsupervised embeddings, by retraining them to predict the noisy labels before using the actual task-specific supervised data (Severyn and Moschitti, 2015). 3 Unsupervised Structured Skip-Gram Word Embed"
P15-1104,P10-1040,0,0.0626179,"word representations by exploiting word co-occurrence patterns in large samples of unlabeled text. Based on this idea, several methods have been recently proposed to efficiently estimate word embeddings from raw text, leveraging neural language models (Huang et al., 2012; Mikolov et al., 2013; Pennington et al., 2014; Ling et al., 2015). These models work by maximizing the probability that words within a given window size are predicted correctly. The resulting embeddings are low-dimensional dense vectors that encode syntactic and semantic properties of words. Using these word representations, Turian et al. (2010) were able to improve near state-of-the-art systems for several tasks, by simply plugging in the learned word representations as additional features. However, because these features are estimated by minimizing the prediction errors made on a generic, unsupervised, task they might be suboptimal for the intended purposes. Ideally, word features should be adapted to the specific supervised task. One of the reasons for the success of deep learning models for language problems, is the use unsupervised word embeddings to initialize the word projection layer. Then, during training, the errors made in"
P15-1104,S13-2052,0,\N,Missing
P15-2105,P11-1038,0,0.0195409,"ical variants in Twitter (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three"
P15-2105,C10-2042,0,0.0355775,"selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including ne"
P15-2105,P12-1092,0,0.0163423,"11010, share the first three nodes in the hierarchically 110. Sharing more tree nodes tends to translate into better similarity between words within the clusters. Thus, a word a 11001 cluster is simultaneously in clusters 1, 11, 110, 1100 and 11001, and a feature can be extracted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported re"
P15-2105,P14-2111,0,0.0188732,"r (e.g., “cats vs. catz”). While variants tend to have the same meaning as their standardized form, the proposed model does not have this information and will not be able to generalize properly. For instance, if the term ”John” is labelled as keyword in the training set, the model would not be able to extract ”Jooohn” as keyword as it is in a different word form. One way to adThe corpus is submitted as supplementary material. 638 dress this would be using a normalization system either built using hand engineered rules (Gouws et al., 2011) or trained using labelled data (Han and Baldwin, 2011; Chrupała, 2014). However, these systems are generally limited as these need supervision and cannot scale to new data or data in other languages. Instead, we will used unsupervised methods that leverage large amounts of unannotated data. We used two popular methods for this purpose: Brown Clustering and Continuous Word Vectors. uments, such as a news article, contain approximately 3-5 keywords, so extracting 3 keywords per document is a reasonable option. However, this would not work in Twitter, since the number of keywords can be arbitrary small. In fact, many tweets contain less than three words, in which c"
P15-2105,W12-3153,0,0.0173412,"tugal {luis.marujo,wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We i"
P15-2105,P13-1018,1,0.272005,"wang.ling,isabel.trancoso,david.matos,joao.neto}@inesc-id.pt {cdyer,awb,anatoleg,jgc}@cs.cmu.edu, Abstract These messages tend to be shorter than web pages, especially on Twitter, where the content has to be limited to 140 characters. The language is also more casual with many messages containing orthographical errors, slang (e.g., cday), abbreviations among domain specific artifacts. In many applications, that existing datasets and models tend to perform significantly worse on these domains, namely in Part-of-Speech (POS) Tagging (Gimpel et al., 2011), Machine Translation (Jelh et al., 2012; Ling et al., 2013), Named Entity Recognition (Ritter et al., 2011; Liu et al., 2013), Information Retrieval (Efron, 2011) and Summarization (Duan et al., 2012; Chang et al., 2013). As automatic keyword extraction plays an important role in many NLP tasks, building an accurate extractor for the Twitter domain is a valuable asset in many of these applications. In this paper, we propose an automatic keyword extraction system for this end and our contributions are the following ones: In this paper, we build a corpus of tweets from Twitter annotated with keywords using crowdsourcing methods. We identify key differen"
P15-2105,N15-1142,1,0.664689,"racted for each cluster. In our experiments, we used the dataset with 1,000 Brown clusters made available by Owoputi et al. (Owoputi et al., 2013)2 . 4.1.2 Continuous Word Vectors Word representations learned from neural language models are another way to learn more generalizable features for words (Collobert et al., 2011; Huang et al., 2012). In these models, a hidden layer is defined that maps words into a continuous vector. The parameters of this hidden layer are estimated by maximizing a goal function, such as the likelihood of each word predicting surrounding words (Mikolov et al., 2013; Ling et al., 2015). In our work, we used the structured skip-ngram goal function proposed in (Ling et al., 2015) and for each word we extracted its respective word vector as features. 5 Experiments Experiments are performed on the annotated dataset using the train, development and test splits defined in Section 3. As baselines, we reported results using a TF-IDF, the default MAUI toolkit, and our own implementation of (Li et al., 2010) framework. In all cases the IDF component was computed over a collection of 52 million tweets. Results are reported on rows 1 and 2 in Table 1, respectively. The parameter k (col"
P15-2105,W08-1404,0,0.00839334,"e frequently used in many occasions as indicators of important information contained in documents. These can be used by human readers to search for their desired documents, but also in many Natural Language Processing (NLP) applications, such as Text Summarization (Pal et al., ¨ ur et al., 2005), 2013), Text Categorization (Ozg¨ Information Retrieval (Marujo et al., 2011a; Yang and Nyberg, 2015) and Question Answering (Liu and Nyberg, 2013). Many automatic frameworks for extracting keywords have been proposed (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Litvak and Last, 2008). These systems were built for more formal domains, such as news data or Web data, where the content is still produced in a controlled fashion. The emergence of social media environments, such as Twitter and Facebook, has created a framework for more casual data to be posted online. 1. Provide a annotated keyword annotated dataset consisting of 1827 tweets. These tweets are obtained from (Gimpel et al., 2011), and also contain POS annotations. 2. Improve a state-of-the-art keyword extraction system (Marujo et al., 2011b; Marujo et al., 2013) for this domain by learning additional features in a"
P15-2105,N13-1039,1,0.474503,"Missing"
P15-2105,D10-1036,0,0.0132353,"l et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised key"
P15-2105,D11-1141,0,0.0220653,"Missing"
P15-2105,marujo-etal-2012-supervised,1,0.940917,"ihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and filtering of the phrases selected before. MAUI toolkit-indexer (Medelyan et al., 2010), an improved version of the KEA (Witten et al., 1999) toolkit including new set of features and more robust classifier, remains the state-of-the-art system in the news domain (Marujo et al., 2012). To the best of our knowledge, only (Li et al., 2010) used a supervised keyword extraction framework (based on KEA) with additional features, such as POS tags to performed keyword extraction on Facebook posts. However, at that time Facebook status updates or posts did not contained either hashtags or user mentions. The size of Facebook posts is frequently longer than tweets and has less abbreviations since it is not limited by number of character as in tweets. 3 4 There are many methods that have been proposed for keyword extraction. TF-IDF is one of the simplest approaches for this end (Salt"
P15-2105,N10-1101,0,0.0485415,"ly 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work formation (e.g., retweet). The annotations of each annotator are combined by selecting keywords that are chosen by at least three annotators. We also divided the 1827 tweets into 1000 training samples, 327 development samples and 500 test samples, using the splits as in (Gimpel et al., 2011). Both supervised and unsupervised approaches have been explored to perform key word extraction. Most of the automatic keyword/keyphrase extraction methods proposed for social media data, such as tweets, are unsupervised methods (Wu et al., 2010; Zhao et al., 2011; Bellaachia and Al-Dhelaan, 2012). However, the TF-IDF across different methods remains a strong unsupervised baseline (Hasan and Ng, 2010). These methods include adaptations to the PageRank method (Brin and Page, 1998) including TextRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and Topic PageRank (Liu et al., 2010). Supervised keyword extraction methods formalize this problem as a binary classification problem of two steps (Riloff and Lehnert, 1994; Witten et al., 1999; Turney, 2000; Medelyan et al., 2010; Wang and Li, 2011): candidate generation and fi"
P15-2105,W04-3252,0,\N,Missing
P15-2105,W11-2210,0,\N,Missing
P15-2105,P11-1039,0,\N,Missing
P15-2105,P11-2008,0,\N,Missing
P15-2105,C12-1047,0,\N,Missing
R13-1094,D11-1033,0,0.274559,"al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them were segmented 4 (Zhang,"
R13-1094,J93-2003,0,0.0499157,"Missing"
R13-1094,W07-0722,0,0.0228688,"tion A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corp"
R13-1094,P11-2071,0,0.0320572,"Missing"
R13-1094,P12-2023,0,0.0140684,"ne translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG"
R13-1094,W07-0717,0,0.025379,"em of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each"
R13-1094,2005.eamt-1.19,0,0.85304,"ct model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG )  Sim(sG , R) (1) which means if we could find a better function to measure the similarity between sG and R, G could be replaced by a new sub-corpus Gsub for training a domain-specific SMT system. We focus on two data selection criteria that have been explored for domain adaptation. One comes from the realm of information retrieval (IR), which is defined as the cosine of the angle between two vectors based on term frequencyinverse document frequency (TF-IDF). Hildebrand et al. (2005) showed that it is possible to apply this standard IR technique for both TM adaptation and LM adaptation. It is also similar to the offline data optimization approach proposed by Lüet al. (2007), who re-sample and re727 Proceedings of Recent Advances in Natural Language Processing, pages 727–732, Hissar, Bulgaria, 7-13 September 2013. weight sentences in general corpus, achieving an improvement of about 1 BLEU point over the baseline system. This simple co-occurrence based matching only considers keywords overlap, which may result in weakness in filtering irrelevant data. Thus, it needs a larg"
R13-1094,C12-1096,0,0.0304151,"Missing"
R13-1094,2005.mtsummit-papers.11,0,0.0269798,"Missing"
R13-1094,P07-2045,0,0.00757585,"her penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits"
R13-1094,2010.jec-1.4,0,0.0192472,"onal in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementations of the edit-distance-based retrieval model. We used the normalized Levenshtein similarity score (fuzzy matching score, FMS) proposed by Koehn and Senellart (2010): FMS  1  LEDword (sG , sR ) Max( sG , sR ) (7) in which LEDword is a distance function and |s |is the number of tokens of sentence s. In this study, we employed a word-based Levenshtein edit distance function instead of additionally using a letter-based one. If the score of a sentence exceeds a threshold, we will further penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie sub"
R13-1094,D07-1036,0,0.0841494,"that are more similar to the target domain but different to others in general corpus. The third one is to sum the cross-entropy difference over both source and target side of the corpus:  H I src (s)  HGsrc (s)   H I tar (s)  H G tar (s) Data Set Sentences Tokens Ave. Len. Test Set 3,500 34,382 9.60 In-domain 17,975 151,797 9.45 Training Set 5,211,281 53,650,998 12.93 (6) Table 1: Corpora statistics. The third variant has been is proven to achieve the best result among the three cross-entropy variants (Axelrod et al., 2011). 2.3 In practice, we followed the experiments conducted by Lü et al. (2007) and Hildebrand et al. (2005), where the test set was used to select indomain data from general corpus. The only difference is that an additional in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementat"
R13-1094,2012.iwslt-papers.7,0,0.0135626,"computationally-limited environment. 1 Introduction A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a s"
R13-1094,P10-2041,0,0.277716,". As in (Hildebrand et al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them w"
R13-1094,J03-1002,0,0.00747202,"nglish-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits and settings as described above. The baseline BLEU is 29.34 points. This low value is occurred by the fact that he general corpus does not consist of enough sentences on the travel domain and has a lo"
R13-1094,P02-1040,0,0.0855658,"Missing"
R13-1094,W03-1730,0,0.0144502,"Missing"
S15-2102,P97-1023,0,0.0589137,"nyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Tw"
S15-2102,kamps-etal-2004-using,0,0.0579992,"Missing"
S15-2102,N06-1026,0,0.0117873,". We found this approach to be effective for the proposed problem. The rest of the paper proceeds as follows: we review the work related to lexicon expansion in Section 2 and describe the methods used to derive word embeddings in Section 3. Our approach and the experimental results are presented in Sections 5 and 6, respectively. We conclude in Section 7. 2 Related Work Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from c"
S15-2102,N15-1142,1,0.600546,"trix, transforming the one-hot sparse representation into a compact real valued space of size e; Cp ∈ Rv×e is a matrix mapping the realvalued representation to a vector with the size of the vocabulary v. A distribution over all possible words is then attained by exponentiating and normalizing over the v possible options. In practice, due to the large value of v, various techniques are used to avoid having to normalize over the whole vocabulary (Mikolov et al., 2013a). In the particular case of the structured skip-gram model, the matrix Cp depends only of the relative position between words p (Ling et al., 2015). After training, the low dimensional embedding E· wi ∈ Re×1 encapsulates the information about each word and its surrounding contexts. CBOW The CBOW model defines a different objective function, that predicts a word at position i given the window of context i − d, where d is the size of the context window. The probability of the word wi is (a) Phrases as the sum of embeddings (b) Phrases as the mean of embeddings Figure 1: Performance of the different embeddings and phrase representations, as function of vector size. defined as: p(wi |wi−d , ..., wi+d ; C, E) ∝ exp(C · Si+d i−d ) (2) where Si"
S15-2102,N13-1039,0,0.0819375,"Missing"
S15-2102,D14-1162,0,0.0901746,", 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014). Moreover, a notion of semantic similarity, as well as other linguistic regularities seem to be encoded in the embedding space (Mikolov et al., 2013b). In word2vec, Mikolov et al. (2013a) induce word vectors with two simple neural network architectures, CBOW and skip-gram. These models estimate the optimal word embeddings by maximizing the probability that, words within a given window size are predicted correctly. Skip-gram and Structured Skip-gram Central to the skip-gram is a log-linear model of word prediction. Given the i-th word from a sentence wi , the skip-gram estimates the probabilit"
S15-2102,E09-1077,0,0.0159245,"Most of the literature on automatic lexicon expansion consists of dictionary-based or corpora-based approaches. In the former, the main idea is to use a dictionary, such as WordNet, to extract semantic relations between words. Kim and Hovy (2006) simply assign the same polarity to synonyms and the opposite polarity to antonyms, of known words. Others, create a graph from the semantic relationships, to find new sentiment words and their polarity. Using the seed words, new terms are classified using a distance measure (Kamps et al., 2004), or propagating the labels along the edges of the graph (Rao and Ravichandran, 2009). However, given that dictionaries mostly describe conventional language, these methods are unsuited for social media. Corpora based approaches follow the assumption that the polarity of new words can be inferred from co-occurrence patterns with known words. Hatzivassiloglou and McKeown (1997) discovered new polar adjectives by looking at conjunctions found in a corpus. The adjectives connected with and got the same polarity, whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing posi"
S15-2102,C14-1018,0,0.0977943,"whereas adjectives connected with but were assigned opposing polarities. Turney and Littman (2003) created two small sets of prototypical polar words, one containing positive and another containing negative examples. The polarity of a new term was computed using the point-wise mutual information between that word and each of the prototypical sets (Lin, 1998). The same method was used by Kiritchenko et al. (2014), to create large scale sentiment lexicons for Twitter. A recently proposed alternative is to learn word embeddings specific for Twitter sentiment analysis, using distant supervision (Tang et al., 2014). The resulting features are then used in a supervised classifier to predict the polarity of phrases. This work is the most related to our approach, but it differs in the sense that we use general word embeddings, learned from unlabeled data, and predict both polarity and intensity of sentiment. 3 Unsupervised Word Embeddings In recent years, several models have been proposed, to derive word embeddings from large corpora. These are essentially, dense vector representations that implicitly capture syntactic and semantic properties of words (Collobert et al., 2011; Mikolov et al., 2013a; Penning"
S15-2109,P14-2131,0,0.0326399,"large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to"
S15-2109,J92-4003,0,0.0675756,"mbeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given po"
S15-2109,D14-1082,0,0.056327,"embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the"
S15-2109,N15-1142,1,0.824245,"llobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extraction include brown clustering (Brown et al., 1992) and LDA (Blei et al., 2003), One popular objective function for embeddings is to maximize the prediction of contextual words. In the work described in (Mikolov et al., 2013), commonly referred as word2vec, the models defined estimate the optimal word embeddings by maximizing the probability that the words within a given window size are predicted correctly. In the work presented here, a structured skip-gram (Ling et al., 2015) was used to generate the embeddings. Central to the skip-gram (Mikolov et al., 2013) is a log linear model of word prediction. Let w = i denote that a word at a given position of a sentence is the i-th word on a vocabulary of size v. Let wp = j denote that the word p positions further in the sentence is the j-th word on the vocabulary. The skip-gram models the following probability:  p(wp = j|w = i; C, E) ∝ exp Cj · E · wi . (1) Here, wi ∈ {1, 0}v×1 is a one-hot representation of w = i. That is, a vector of zeros of the size of the vocabulary v with a 1 on the i-th entry of the vector. The s"
S15-2109,S14-2089,0,0.111909,"need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containi"
S15-2109,S13-2052,0,0.0670063,"sed training is thus reduced to finding the optimal projection which can be carried out efficiently despite the little data available. We analyze in detail the proposed approach and show that a competitive system can be attained with only a few configuration parameters. 1 Introduction Web-based social networks are a rich data source for both businesses and academia. However, the sheer volume, diversity and rate of creation of social media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expre"
S15-2109,N13-1039,0,0.152652,"Missing"
S15-2109,S15-2078,0,0.0925042,"rch has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous word representations containing contextual information. Instead of using these word embeddings directly with, for instance, a logistic regression classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we j"
S15-2109,P10-1040,0,0.0908234,"classifier, we estimate a sentiment subspace of the embeddings. The idea is to find a projection of the embedding space that is meaningful for the supervised task. In the proposed model, we jointly learn the sentiment subspace projection and the classifier using the SemEval training data. The resulting system attains state-of-theart performance without hand-coded features or linguistic resources and only a few configuration parameters. 2 Unsupervised Learning of Word Embeddings Unsupervised word embeddings trained from large amounts of unlabeled data have been shown to improve many NLP tasks (Turian et al., 2010; Collobert et al., 2011). Embeddings capture generic regularities about the data and can be trained with virtually an infinite amount of data in unsupervised 652 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 652–656, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics fashion. Once trained, they can be used as features for supervised tasks or to initialize more complex models (Collobert et al., 2011; Chen and Manning, 2014; Bansal et al., 2014). Other unsupervised approaches that can also be used for feature extractio"
S15-2109,S14-2077,0,0.0321089,"l media, imposes the need for automated analysis tools. The growing interest in this problem motivated the creation of a shared task for Twitter Sentiment Analysis (Nakov et al., 2013). The Message Polarity Classification task consists in classifying a message as positive, negative, or neutral in sentiment. A great deal of research has been done on methods for sentiment analysis on user generated content. However, state-of-the-art systems still largely depend on linguistic resources, extensive feature engineering and tuning. Indeed, if we look at the best performing systems from SemEval 2014 (Zhu et al., 2014), (Malandrakis et al., 2014), both make extensive use of these resources, including hundreds of thousands of features, special treatment for negation, multi-word expressions or special strings like emoticons. In this paper we present the INESC-ID system for the 2015 SemEval message polarity classification task (Rosenthal et al., 2015). The system is able to learn good message representations for message polarity classification directly from raw text with a simple tokenization scheme. Our approach is based on using large amounts of unlabeled data to induce word embeddings, that is, continuous w"
S16-1036,P15-1104,1,0.90835,"e-trained word embeddings provide a simple means to attain semi-supervised learning in Natural Language Processing (NLP) tasks (Collobert et al., 2011). They can be trained with large amounts of unsupervised data and be fine tuned as the initial building block of a semi-supervised system. However, in domains with a significant number of typos, use of slang and abbreviations, such as social media, the high number of singletons leads to a poor fine tuning of the embeddings. In previous work, 2 NLSE Model Overview In this section, we briefly review the approach introduced in the 2015 evaluation (Astudillo et al., 2015a). For a particular regression or classification task, only a subset of all the latent aspects captured by the word embeddings will be useful. Therefore, instead of updating the embeddings directly with the available labeled data, we estimate a projection of these embeddings into a low dimensional sub-space. This simple method brings two fundamental advan238 Proceedings of SemEval-2016, pages 238–242, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics tages. Firstly, the lower dimensional embeddings require fewer parameters fitting the complexity of the"
S16-1036,S15-2109,1,0.891997,"Missing"
S16-1036,N15-1142,1,0.890643,"Missing"
S16-1036,S16-1001,0,0.0188104,"Comparision of strategies to address the problem of OOEV 5 System https://github.com/wlin12/wang2vec 241 examples after each training epoch and performed model selection by early stopping after 12 iterations. The candidate for submission was manually selected by observing the performance across 2013, 2014 and 2015 datasets. Priority was given to models that presented a consistent high performance in all the datasets. In retrospect, this was most probably a suboptimal decision judging from the evaluation results. Table 4 displays the performance for the top 5 systems in SemEval 2016 task 4-B (Nakov et al., 2016). The NLSE system (labeled INESC-ID) ranks forth with a stable performance across all years. The results are particularly strong for 2013 with a difference of 0.017 points over the next best performing system on the top five. This is consistent with the divide noticed during system selection between performance in 2013 and 2015. High-performing systems in 2014, and particularly in 2013, do not appear to be equally performing in recent years. 6 Conclusions We presented the INESC-ID system for the SemEval 2016 task 4-A, built on top of the successful NonLinear Subspace Embedding model. We found"
S16-1036,N13-1039,0,0.0316451,"Missing"
S16-1036,S15-2079,0,0.0408819,"Missing"
silva-etal-2012-dealing,N03-2016,0,\N,Missing
silva-etal-2012-dealing,N01-1020,0,\N,Missing
silva-etal-2012-dealing,W02-0902,0,\N,Missing
silva-etal-2012-dealing,D07-1092,0,\N,Missing
silva-etal-2012-dealing,mulloni-pekar-2006-automatic,0,\N,Missing
trancoso-etal-2008-lectra,W01-1819,0,\N,Missing
W14-3356,W10-0710,0,0.132945,"Missing"
W14-3356,2005.iwslt-1.8,0,0.0947835,"Missing"
W14-3356,P05-1074,0,0.156435,"Missing"
W14-3356,P11-1061,0,0.0567147,"Missing"
W14-3356,P91-1023,0,0.724078,"Missing"
W14-3356,1992.tmi-1.9,0,0.36141,"Missing"
W14-3356,W12-3134,0,0.0268465,"Missing"
W14-3356,2012.amta-papers.7,0,0.0943115,"Missing"
W14-3356,W11-2123,0,0.0200324,"Missing"
W14-3356,W12-3153,0,0.0696685,"Missing"
W14-3356,P07-2045,1,0.0120714,"Missing"
W14-3356,P13-1018,1,0.81692,"Missing"
W14-3356,I11-1125,0,0.0609802,"Missing"
W14-3356,J05-4003,0,0.186489,"Missing"
W14-3356,P03-1058,0,0.0964326,"Missing"
W14-3356,P03-1021,0,0.0602179,"Missing"
W14-3356,P02-1040,0,0.108513,"Missing"
W14-3356,W12-3152,0,0.0723752,"Missing"
W14-3356,J03-3002,0,0.372304,"Missing"
W14-3356,N10-1113,0,0.0352934,"Missing"
W14-3356,N10-1063,0,0.0896412,"Missing"
W14-3356,P11-1122,0,0.0870956,"Missing"
W14-3356,J93-1004,0,\N,Missing
W14-3356,W13-2201,0,\N,Missing
