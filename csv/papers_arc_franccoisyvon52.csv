2021.tacl-1.2,Revisiting Multi-Domain Machine Translation,2021,-1,-1,3,1,835,minhquang pham,Transactions of the Association for Computational Linguistics,0,"When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises."
2021.mtsummit-research.21,Optimizing Word Alignments with Better Subword Tokenization,2021,-1,-1,2,0,5064,anh ho,Proceedings of Machine Translation Summit XVIII: Research Track,0,Word alignment identify translational correspondences between words in a parallel sentence pair and are used and for example and to train statistical machine translation and learn bilingual dictionaries or to perform quality estimation. Subword tokenization has become a standard preprocessing step for a large number of applications and notably for state-of-the-art open vocabulary machine translation systems. In this paper and we thoroughly study how this preprocessing step interacts with the word alignment task and propose several tokenization strategies to obtain well-segmented parallel corpora. Using these new techniques and we were able to improve baseline word-based alignment models for six language pairs.
2021.jeptalnrecital-taln.2,Biais de genre dans un syst{\\`e}me de traduction automatiqueneuronale : une {\\'e}tude pr{\\'e}liminaire (Gender Bias in Neural Translation : a preliminary study ),2021,-1,-1,4,0,168,guillaume wisniewski,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Cet article pr{\'e}sente les premiers r{\'e}sultats d{'}une {\'e}tude en cours sur les biais de genre dans les corpus d{'}entra{\^\i}nements et dans les syst{\`e}mes de traduction neuronale. Nous {\'e}tudions en particulier un corpus minimal et contr{\^o}l{\'e} pour mesurer l{'}intensit{\'e} de ces biais dans les deux directions anglais-fran{\c{c}}ais et fran{\c{c}}ais-anglais ; ce cadre contr{\^o}l{\'e} nous permet {\'e}galement d{'}analyser les repr{\'e}sentations internes manipul{\'e}es par le syst{\`e}me pour r{\'e}aliser ses pr{\'e}dictions lexicales, ainsi que de formuler des hypoth{\`e}ses sur la mani{\`e}re dont ce biais se distribue dans les repr{\'e}sentations du syst{\`e}me."
2021.jeptalnrecital-taln.8,Vers la production automatique de sous-titres adapt{\\'e}s {\\`a} l{'}affichage (Towards automatic adapted monolingual captioning),2021,-1,-1,2,0,5599,franccois buet,Actes de la 28e Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 : conf{\\'e}rence principale,0,"Une fa{\c{c}}on de r{\'e}aliser un sous-titrage automatique monolingue est d{'}associer un syst{\`e}me de reconnaissance de parole avec un mod{\`e}le de traduction de la transcription vers les sous-titres. La t{\^a}che de Â« traduction Â» est d{\'e}licate dans la mesure o{\`u} elle doit op{\'e}rer une simplification et une compression du texte, respecter des normes li{\'e}es {\`a} l{'}affichage, tout en composant avec les erreurs issues de la reconnaissance vocale. Une difficult{\'e} suppl{\'e}mentaire est la relative raret{\'e} des corpus mettant en parall{\`e}le transcription automatique et sous-titres sont relativement rares. Nous d{\'e}crivons ici un nouveau corpus en cours de constitution et nous exp{\'e}rimentons l{'}utilisation de m{\'e}thodes de contr{\^o}le plus ou moins direct de la longueur des phrases engendr{\'e}es, afin d{'}am{\'e}liorer leur qualit{\'e} du point de vue linguistique et normatif."
2021.emnlp-main.665,Graph Algorithms for Multiparallel Word Alignment,2021,-1,-1,5,0,9978,ayyoob imanigooghari,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"With the advent of end-to-end deep learning approaches in machine translation, interest in word alignments initially decreased; however, they have again become a focus of research more recently. Alignments are useful for typological research, transferring formatting like markup to translated texts, and can be used in the decoding of machine translation systems. At the same time, massively multilingual processing is becoming an important NLP scenario, and pretrained language and machine translation models that are truly multilingual are proposed. However, most alignment algorithms rely on bitexts only and do not leverage the fact that many parallel corpora are multiparallel. In this work, we exploit the multiparallelity of corpora by representing an initial set of bilingual alignments as a graph and then predicting additional edges in the graph. We present two graph algorithms for edge prediction: one inspired by recommender systems and one based on network link prediction. Our experimental results show absolute improvements in F1 of up to 28{\%} over the baseline bilingual word aligner in different datasets."
2021.emnlp-main.671,"One Source, Two Targets: {C}hallenges and Rewards of Dual Decoding",2021,-1,-1,2,1,9989,jitao xu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Machine translation is generally understood as generating one target text from an input source document. In this paper, we consider a stronger requirement: to jointly generate two texts so that each output side effectively depends on the other. As we discuss, such a device serves several practical purposes, from multi-target machine translation to the generation of controlled variations of the target text. We present an analysis of possible implementations of dual decoding, and experiment with four applications. Viewing the problem from multiple angles allows us to better highlight the challenges of dual decoding and to also thoroughly analyze the benefits of generating matched, rather than independent, translations."
2021.calcs-1.11,Can You Traducir This? Machine Translation for Code-Switched Input,2021,-1,-1,2,1,9989,jitao xu,Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching,0,"Code-Switching (CSW) is a common phenomenon that occurs in multilingual geographic or social contexts, which raises challenging problems for natural language processing tools. We focus here on Machine Translation (MT) of CSW texts, where we aim to simultaneously disentangle and translate the two mixed languages. Due to the lack of actual translated CSW data, we generate artificial training data from regular parallel texts. Experiments show this training strategy yields MT systems that surpass multilingual systems for code-switched texts. These results are confirmed in an alternative task aimed at providing contextual translations for a L2 writing assistant."
2021.blackboxnlp-1.24,Screening Gender Transfer in Neural Machine Translation,2021,-1,-1,4,0,168,guillaume wisniewski,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"This paper aims at identifying the information flow in state-of-the-art machine translation systems, taking as example the transfer of gender when translating from French into English. Using a controlled set of examples, we experiment several ways to investigate how gender information circulates in a encoder-decoder architecture considering both probing techniques as well as interventions on the internal representations used in the MT system. Our results show that gender information can be found in all token representations built by the encoder and the decoder and lead us to conclude that there are multiple pathways for gender transfer."
2020.wmt-1.63,Priming Neural Machine Translation,2020,0,28,4,0,5485,minh pham,Proceedings of the Fifth Conference on Machine Translation,0,"Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources."
2020.wmt-1.72,A Study of Residual Adapters for Multi-Domain Neural Machine Translation,2020,-1,-1,3,0,5485,minh pham,Proceedings of the Fifth Conference on Machine Translation,0,"Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors."
2020.wmt-1.86,{LIMSI} @ {WMT} 2020,2020,-1,-1,4,0,2848,sadaf rauf,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes LIMSI{'}s submissions to the translation shared tasks at WMT{'}20. This year we have focused our efforts on the biomedical translation task, developing a resource-heavy system for the translation of medical abstracts from English into French, using back-translated texts, terminological resources as well as multiple pre-processing pipelines, including pre-trained representations. Systems were also prepared for the robustness task for translating from English into German; for this large-scale task we developed multi-domain, noise-robust, translation systems aim to handle the two test conditions: zero-shot and few-shot domain adaptation."
2020.lrec-1.407,The {E}uropean Language Technology Landscape in 2020: Language-Centric and Human-Centric {AI} for Cross-Cultural Communication in Multilingual {E}urope,2020,4,1,47,0,60,georg rehm,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Multilingualism is a cultural cornerstone of Europe and firmly anchored in the European treaties including full language equality. However, language barriers impacting business, cross-lingual and cross-cultural communication are still omnipresent. Language Technologies (LTs) are a powerful means to break down these barriers. While the last decade has seen various initiatives that created a multitude of approaches and technologies tailored to Europe{'}s specific needs, there is still an immense level of fragmentation. At the same time, AI has become an increasingly important concept in the European Information and Communication Technology area. For a few years now, AI {--} including many opportunities, synergies but also misconceptions {--} has been overshadowing every other topic. We present an overview of the European LT landscape, describing funding programmes, activities, actions and challenges in the different countries with regard to LT, including the current state of play in industry and the LT market. We present a brief overview of the main LT-related activities on the EU level in the last ten years and develop strategic guidance with regard to four key dimensions."
2020.findings-emnlp.147,{S}im{A}lign: High Quality Word Alignments Without Parallel Training Data Using Static and Contextualized Embeddings,2020,58,2,3,0.454545,9979,masoud sabet,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Word alignments are useful for tasks like statistical and neural machine translation (NMT) and cross-lingual annotation projection. Statistical word aligners perform well, as do methods that extract alignments jointly with translations in NMT. However, most approaches require parallel training data and quality decreases as less training data is available. We propose word alignment methods that require no parallel data. The key idea is to leverage multilingual word embeddings {--} both static and contextualized {--} for word alignment. Our multilingual embeddings are created from monolingual data only without relying on any parallel data or dictionaries. We find that alignments created from embeddings are superior for four and comparable for two language pairs compared to those produced by traditional statistical aligners {--} even with abundant parallel data; e.g., contextualized embeddings achieve a word alignment F1 for English-German that is 5 percentage points higher than eflomal, a high-quality statistical aligner, trained on 100k parallel sentences."
2020.amta-research.6,Generative latent neural models for automatic word alignment,2020,-1,-1,2,0,5064,anh ho,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),0,None
W19-4443,Measuring text readability with machine comprehension: a pilot study,2019,0,0,2,0,24211,marc benzahra,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"This article studies the relationship between text readability indice and automatic machine understanding systems. Our hypothesis is that the simpler a text is, the better it should be understood by a machine. We thus expect to a strong correlation between readability levels on the one hand, and performance of automatic reading systems on the other hand. We test this hypothesis with several understanding systems based on language models of varying strengths, measuring this correlation on two corpora of journalistic texts. Our results suggest that this correlation is rather small that existing comprehension systems are far to reproduce the gradual improvement of their performance on texts of decreasing complexity."
N19-1019,{H}ow {B}ad are {P}o{S} {T}agger in {C}ross-{C}orpora {S}ettings? {E}valuating {A}nnotation {D}ivergence in the {UD} {P}roject.,2019,0,0,2,0.871796,168,guillaume wisniewski,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The performance of Part-of-Speech tagging varies significantly across the treebanks of the Universal Dependencies project. This work points out that these variations may result from divergences between the annotation of train and test sets. We show how the annotation variation principle, introduced by Dickinson and Meurers (2003) to automatically detect errors in gold standard, can be used to identify inconsistencies between annotations; we also evaluate their impact on prediction performance."
W18-6433,"The {WMT}{'}18 Morpheval test suites for {E}nglish-{C}zech, {E}nglish-{G}erman, {E}nglish-{F}innish and {T}urkish-{E}nglish",2018,-1,-1,8,1,23863,franck burlot,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"Progress in the quality of machine translation output calls for new automatic evaluation procedures and metrics. In this paper, we extend the Morpheval protocol introduced by Burlot and Yvon (2017) for the English-to-Czech and English-to-Latvian translation directions to three additional language pairs, and report its use to analyze the results of WMT 2018{'}s participants for these language pairs. Considering additional, typologically varied source and target languages also enables us to draw some generalizations regarding this morphology-oriented evaluation procedure."
W18-6315,Using Monolingual Data in Neural Machine Translation: a Systematic Study,2018,0,14,2,1,23863,franck burlot,Proceedings of the Third Conference on Machine Translation: Research Papers,0,"Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement."
W18-5804,{A}daptor {G}rammars for the Linguist: Word Segmentation Experiments for Very Low-Resource Languages,2018,0,1,3,0,27865,pierre godard,"Proceedings of the Fifteenth Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Computational Language Documentation attempts to make the most recent research in speech and language technologies available to linguists working on language preservation and documentation. In this paper, we pursue two main goals along these lines. The first is to improve upon a strong baseline for the unsupervised word discovery task on two very low-resource Bantu languages, taking advantage of the expertise of linguists on these particular languages. The second consists in exploring the Adaptor Grammar framework as a decision and prediction tool for linguists studying a new language. We experiment 162 grammar configurations for each language and show that using Adaptor Grammars for word segmentation enables us to test hypotheses about a language. Specializing a generic grammar with language specific knowledge leads to great improvements for the word discovery task, ultimately achieving a leap of about 30{\%} token F-score from the results of a strong baseline."
N18-2064,Automatically Selecting the Best Dependency Annotation Design with Dynamic Oracles,2018,0,0,3,0.955851,168,guillaume wisniewski,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,This work introduces a new strategy to compare the numerous conventions that have been proposed over the years for expressing dependency structures and discover the one for which a parser will achieve the highest parsing performance. Instead of associating each sentence in the training set with a single gold reference we propose to consider a set of references encoding alternative syntactic representations. Training a parser with a dynamic oracle will then automatically select among all alternatives the reference that will be predicted with the highest accuracy. Experiments on the UD corpora show the validity of this approach.
N18-2066,Exploiting Dynamic Oracles to Train Projective Dependency Parsers on Non-Projective Trees,2018,0,0,3,1,29360,lauriane aufrant,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Because the most common transition systems are projective, training a transition-based dependency parser often implies to either ignore or rewrite the non-projective training examples, which has an adverse impact on accuracy. In this work, we propose a simple modification of dynamic oracles, which enables the use of non-projective data when training projective parsers. Evaluation on 73 treebanks shows that our method achieves significant gains (+2 to +7 UAS for the most non-projective languages) and consistently outperforms traditional projectivization and pseudo-projectivization approaches."
L18-1531,A Very Low Resource Language Speech Corpus for Computational Language Documentation Experiments,2018,0,4,13,0,27865,pierre godard,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Most speech and language technologies are trained with massive amounts of speech and text information. However, most of the world languages do not have such resources and some even lack a stable orthography. Building systems under these almost zero resource conditions is not only promising for speech technology but also for computational language documentation. The goal of computational language documentation is to help field linguists to (semi-)automatically analyze and annotate audio recordings of endangered, unwritten languages. Example tasks are automatic phoneme discovery or lexicon discovery from the speech signal. This paper presents a speech corpus collected during a realistic language documentation process. It is made up of 5k speech utterances in Mboshi (Bantu C25) aligned to French text translations. Speech transcriptions are also made available: they correspond to a non-standard graphemic form close to the language phonology. We detail how the data was collected, cleaned and processed and we illustrate its use through a zero-resource task: spoken term discovery. The dataset is made available to the community for reproducible computational language documentation experiments and their evaluation."
D18-1328,Fixing Translation Divergences in Parallel Corpora for Neural {MT},2018,0,2,4,1,835,minhquang pham,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance."
C18-1270,Quantifying training challenges of dependency parsers,2018,0,0,3,1,29360,lauriane aufrant,Proceedings of the 27th International Conference on Computational Linguistics,0,"Not all dependencies are equal when training a dependency parser: some are straightforward enough to be learned with only a sample of data, others embed more complexity. This work introduces a series of metrics to quantify those differences, and thereby to expose the shortcomings of various parsing algorithms and strategies. Apart from a more thorough comparison of parsing systems, these new tools also prove useful for characterizing the information conveyed by cross-lingual parsers, in a quantitative but still interpretable way."
2018.jeptalnrecital-long.5,{\\'E}valuation morphologique pour la traduction automatique : adaptation au fran{\\c{c}}ais (Morphological Evaluation for Machine Translation : Adaptation to {F}rench),2018,-1,-1,2,1,23863,franck burlot,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Le nouvel {\'e}tat de l{'}art en traduction automatique (TA) s{'}appuie sur des m{\'e}thodes neuronales, qui diff{\'e}rent profond{\'e}ment des m{\'e}thodes utilis{\'e}es ant{\'e}rieurement. Les m{\'e}triques automatiques classiques sont mal adapt{\'e}es pour rendre compte de la nature du saut qualitatif observ{\'e}. Cet article propose un protocole d{'}{\'e}valuation pour la traduction de l{'}anglais vers le fran{\c{c}}ais sp{\'e}cifiquement focalis{\'e} sur la comp{\'e}tence morphologique des syst{\`e}mes de TA, en {\'e}tudiant leurs performances sur diff{\'e}rents ph{\'e}nom{\`e}nes grammaticaux."
2018.jeptalnrecital-court.41,Divergences entre annotations dans le projet {U}niversal {D}ependencies et leur impact sur l{'}{\\'e}valuation des performance d{'}{\\'e}tiquetage morpho-syntaxique (Evaluating Annotation Divergences in the {UD} Project),2018,-1,-1,2,0.955851,168,guillaume wisniewski,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Ce travail montre que la d{\'e}gradation des performances souvent observ{\'e}e lors de l{'}application d{'}un analyseur morpho-syntaxique {\`a} des donn{\'e}es hors domaine r{\'e}sulte souvent d{'}incoh{\'e}rences entre les annotations des ensembles de test et d{'}apprentissage. Nous montrons comment le principe de variation des annotations, introduit par Dickinson {\&} Meurers (2003) pour identifier automatiquement les erreurs d{'}annotation, peut {\^e}tre utilis{\'e} pour identifier ces incoh{\'e}rences et {\'e}valuer leur impact sur les performances des analyseurs morpho-syntaxiques."
W17-4703,Word Representations in Factored Neural Machine Translation,2017,0,4,5,1,23863,franck burlot,Proceedings of the Second Conference on Machine Translation,0,"Translation into a morphologically rich languagen  requires a large output vocabulary to model variousn  morphological phenomena, which is a challenge forn  neural machine translation architectures. To address this issue,n  the present paper investigates the impact of havingn  two output factors with a system able to generaten  separately two distinct representations of the targetn  words. Within this framework, we investigaten  several word representations that correspond ton  different distributions of morpho-syntactic informationn  across both factors. We report experiments for translationn  from English into two morphologically rich languages,n  Czech and Latvian, and show the importance of explicitlyn  modeling target morphology."
W17-4705,Evaluating the morphological competence of Machine Translation Systems,2017,24,13,2,1,23863,franck burlot,Proceedings of the Second Conference on Machine Translation,0,"While recent changes in Machine Translation state-of-the-art brought translation quality a step further, it is regularly acknowledged that the standard automatic metrics do not provide enough insights to fully measure the impact of neural models. This paper proposes a new type of evaluation focused specifically on the morphological competence of a system with respect to various grammatical phenomena. Our approach uses automatically generated pairs of source sentences, where each pair tests one morphological contrast. This methodology is used to compare several systems submitted at WMT'17 for English into Czech and Latvian."
W17-4721,{LIMSI}@{WMT}{'}17,2017,-1,-1,5,1,23863,franck burlot,Proceedings of the Second Conference on Machine Translation,0,None
W17-4734,The {QT}21 Combined Machine Translation System for {E}nglish to {L}atvian,2017,0,0,8,0.512486,30412,janthorsten peter,Proceedings of the Second Conference on Machine Translation,0,None
K17-3017,{LIMSI}@{C}o{NLL}{'}17: {UD} Shared Task,2017,-1,-1,3,1,29360,lauriane aufrant,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"This paper describes LIMSI{'}s submission to the CoNLL 2017 UD Shared Task, which is focused on small treebanks, and how to improve low-resourced parsing only by ad hoc combination of multiple views and resources. We present our approach for low-resourced parsing, together with a detailed analysis of the results for each test treebank. We also report extensive analysis experiments on model selection for the PUD treebanks, and on annotation consistency among UD treebanks."
E17-2051,Don{'}t Stop Me Now! Using Global Dynamic Oracles to Correct Training Biases of Transition-Based Dependency Parsers,2017,16,0,3,1,29360,lauriane aufrant,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"This paper formalizes a sound extension of dynamic oracles to global training, in the frame of transition-based dependency parsers. By dispensing with the pre-computation of references, this extension widens the training strategies that can be entertained for such parsers; we show this by revisiting two standard training procedures, early-update and max-violation, to correct some of their search space sampling biases. Experimentally, on the SPMRL treebanks, this improvement increases the similarity between the train and test distributions and yields performance improvements up to 0.7 UAS, without any computation overhead."
D17-1044,Learning the Structure of Variable-Order {CRF}s: a finite-state perspective,2017,18,1,2,0.77029,8590,thomas lavergne,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"The computational complexity of linear-chain Conditional Random Fields (CRFs) makes it difficult to deal with very large label sets and long range dependencies. Such situations are not rare and arise when dealing with morphologically rich languages or joint labelling tasks. We extend here recent proposals to consider variable order CRFs. Using an effective finite-state representation of variable-length dependencies, we propose new ways to perform feature selection at large scale and report experimental results where we outperform strong baselines on a tagging task."
2017.jeptalnrecital-long.2,Normalisation automatique du vocabulaire source pour traduire depuis une langue {\\`a} morphologie riche (Learning Morphological Normalization for Translation from Morphologically Rich Languages),2017,-1,-1,2,1,23863,franck burlot,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Lorsqu{'}ils sont traduits depuis une langue {\`a} morphologie riche vers l{'}anglais, les mots-formes sources contiennent des marques d{'}informations grammaticales pouvant {\^e}tre jug{\'e}es redondantes par rapport {\`a} l{'}anglais, causant une variabilit{\'e} formelle qui nuit {\`a} l{'}estimation des mod{\`e}les probabilistes. Un moyen bien document{\'e} pour att{\'e}nuer ce probl{\`e}me consiste {\`a} supprimer l{'}information non pertinente de la source en la normalisant. Ce pr{\'e}-traitement est g{\'e}n{\'e}ralement effectu{\'e} de mani{\`e}re d{\'e}terministe, {\`a} l{'}aide de r{\`e}gles produites manuellement. Une telle normalisation est, par essence, sous-optimale et doit {\^e}tre adapt{\'e}e pour chaque paire de langues. Nous pr{\'e}sentons, dans cet article, une m{\'e}thode simple pour rechercher automatiquement une normalisation optimale de la morphologie source par rapport {\`a} la langue cible et montrons que celle-ci peut am{\'e}liorer la traduction automatique."
2017.jeptalnrecital-court.17,Adaptation au domaine pour l{'}analyse morpho-syntaxique (Domain Adaptation for {P}o{S} tagging),2017,-1,-1,6,0,33245,eleonor bartenlian,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,Ce travail cherche {\`a} comprendre pourquoi les performances d{'}un analyseur morpho-syntaxiques chutent fortement lorsque celui-ci est utilis{\'e} sur des donn{\'e}es hors domaine. Nous montrons {\`a} l{'}aide d{'}une exp{\'e}rience jouet que ce comportement peut {\^e}tre d{\^u} {\`a} un ph{\'e}nom{\`e}ne de masquage des caract{\'e}ristiques lexicalis{\'e}es par les caract{\'e}ristiques non lexicalis{\'e}es. Nous proposons plusieurs mod{\`e}les essayant de r{\'e}duire cet effet.
W16-2304,{LIMSI}@{WMT}{'}16: Machine Translation of News,2016,-1,-1,8,1,5598,alexandre allauzen,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,None
W16-2320,The {QT}21/{H}im{L} Combined Machine Translation System,2016,5,6,20,0.512486,30412,janthorsten peter,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the joint submission of the QT21 and HimL projects for the Englishxe2x86x92Romanian translation task of the ACL 2016 First Conference on Machine Translation (WMT 2016). The submission is a system combination which combines twelve different statistical machine translation systems provided by the different groups (RWTH Aachen University, LMU Munich, Charles University in Prague, University of Edinburgh, University of Sheffield, Karlsruhe Institute of Technology, LIMSI, University of Amsterdam, Tilde). The systems are combined using RWTHxe2x80x99s system combination approach. The final submission shows an improvement of 1.0 BLEU compared to the best single system on newstest2016."
W16-2337,{LIMSI}{'}s Contribution to the {WMT}{'}16 Biomedical Translation Task,2016,30,3,3,0,10027,julia ive,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"The article describes LIMSIxe2x80x99s submission to the first WMTxe2x80x9916 shared biomedical translation task, focusing on the sole English-French translation direction. Our main submission is the output of a MOSES-based statistical machine translation (SMT) system, rescored with Structured OUtput Layer (SOUL) neural network models. We also present an attempt to circumvent syntactic complexity: our proposal combines the outputs of PBSMT systems trained either to translate entire source sentences or specific syntactic constructs extracted from those sentences. The approach is implemented using Confusion Network (CN) decoding. The quality of the combined output is comparable to the quality of our main system."
W16-1203,Cross-lingual Dependency Transfer : What Matters? Assessing the Impact of Pre- and Post-processing,2016,-1,-1,3,0.94357,2749,ophelie lacroix,Proceedings of the Workshop on Multilingual and Cross-lingual Methods in {NLP},0,None
W16-1205,Cross-lingual alignment transfer: a chicken-and-egg story?,2016,25,0,3,1,29360,lauriane aufrant,Proceedings of the Workshop on Multilingual and Cross-lingual Methods in {NLP},0,None
N16-3006,{T}rans{R}ead: Designing a Bilingual Reading Experience with Machine Translation Technologies,2016,13,0,1,1,837,franccois yvon,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"In this paper, we use multilingual Natural Language Processing (NLP) tools to improve the reading experience of parallel texts on mobile devices. Such enterprise poses multiple challenging issues both from the NLP and from the Human Computer Interaction (HCI) perspectives. We discuss these problems, and report on our own solutions, now implemented in a full-fledged bilingual reading device."
N16-1121,Frustratingly Easy Cross-Lingual Transfer for Transition-Based Dependency Parsing,2016,20,7,4,0.94357,2749,ophelie lacroix,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper, we present a straightforward strategy for transferring dependency parsers across languages. The proposed method learns a parser from partially annotated data obtained through the projection of annotations across unambiguous word alignments. It does not rely on any modeling of the reliability of dependency and/or alignment links and is therefore easy to implement and parameter free. Experiments on six languages show that our method is at par with recent algorithmically demanding methods, at a much cheaper computational cost. It can thus serve as a fair baseline for transferring dependencies across languages with the use of parallel corpora."
L16-1099,Novel elicitation and annotation schemes for sentential and sub-sentential alignments of bitexts,2016,34,1,2,1,34613,yong xu,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Resources for evaluating sentence-level and word-level alignment algorithms are unsatisfactory. Regarding sentence alignments, the existing data is too scarce, especially when it comes to difficult bitexts, containing instances of non-literal translations. Regarding word-level alignments, most available hand-aligned data provide a complete annotation at the level of words that is difficult to exploit, for lack of a clear semantics for alignment links. In this study, we propose new methodologies for collecting human judgements on alignment links, which have been used to annotate 4 new data sets, at the sentence and at the word level. These will be released online, with the hope that they will prove useful to evaluate alignment software and quality estimation tools for automatic alignment. Keywords: Parallel corpora, Sentence Alignments, Word Alignments, Confidence Estimation"
L16-1241,Cross-lingual and Supervised Models for Morphosyntactic Annotation: a Comparison on {R}omanian,2016,0,1,3,1,29360,lauriane aufrant,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Because of the small size of Romanian corpora, the performance of a PoS tagger or a dependency parser trained with the standard supervised methods fall far short from the performance achieved in most languages. That is why, we apply state-of-the-art methods for cross-lingual transfer on Romanian tagging and parsing, from English and several Romance languages. We compare the performance with monolingual systems trained with sets of different sizes and establish that training on a few sentences in target language yields better results than transferring from large datasets in other languages."
C16-1012,Zero-resource Dependency Parsing: Boosting Delexicalized Cross-lingual Transfer with Linguistic Knowledge,2016,15,9,3,1,29360,lauriane aufrant,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper studies cross-lingual transfer for dependency parsing, focusing on very low-resource settings where delexicalized transfer is the only fully automatic option. We show how to boost parsing performance by rewriting the source sentences so as to better match the linguistic regularities of the target language. We contrast a data-driven approach with an approach relying on linguistically motivated rules automatically extracted from the World Atlas of Language Structures. Our findings are backed up by experiments involving 40 languages. They show that both approaches greatly outperform the baseline, the knowledge-driven method yielding the best accuracies, with average improvements of +2.9 UAS, and up to +90 UAS (absolute) on some frequent PoS configurations."
C16-1142,Parallel Sentence Compression,2016,23,2,2,0,10027,julia ive,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Sentence compression is a way to perform text simplification and is usually handled in a monolingual setting. In this paper, we study ways to extend sentence compression in a bilingual context, where the goal is to obtain parallel compressions of parallel sentences. This can be beneficial for a series of multilingual natural language processing (NLP) tasks. We compare two ways to take bilingual information into account when compressing parallel sentences. Their efficiency is contrasted on a parallel corpus of News articles."
2016.jeptalnrecital-long.1,Apprentissage d{'}analyseur en d{\\'e}pendances cross-lingue par projection partielle de d{\\'e}pendances (Cross-lingual learning of dependency parsers from partially projected dependencies ),2016,-1,-1,4,0.94357,2749,ophelie lacroix,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Cet article pr{\'e}sente une m{\'e}thode simple de transfert cross-lingue de d{\'e}pendances. Nous montrons tout d{'}abord qu{'}il est possible d{'}apprendre un analyseur en d{\'e}pendances par transition {\`a} partir de donn{\'e}es partiellement annot{\'e}es. Nous proposons ensuite de construire de grands ensembles de donn{\'e}es partiellement annot{\'e}s pour plusieurs langues cibles en projetant les d{\'e}pendances via les liens d{'}alignement les plus s{\^u}rs. En apprenant des analyseurs pour les langues cibles {\`a} partir de ces donn{\'e}es partielles, nous montrons que cette m{\'e}thode simple obtient des performances qui rivalisent avec celles de m{\'e}thodes {\'e}tat-de-l{'}art r{\'e}centes, tout en ayant un co{\^u}t algorithmique moindre."
2016.jeptalnrecital-long.19,Ne nous arr{\\^e}tons pas en si bon chemin : am{\\'e}liorations de l{'}apprentissage global d{'}analyseurs en d{\\'e}pendances par transition (Don{'}t Stop Me Now ! Improved Update Strategies for Global Training of Transition-Based),2016,-1,-1,3,1,29360,lauriane aufrant,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Dans cet article, nous proposons trois am{\'e}liorations simples pour l{'}apprentissage global d{'}analyseurs en d{\'e}pendances par transition de type A RC E AGER : un oracle non d{\'e}terministe, la reprise sur le m{\^e}me exemple apr{\`e}s une mise {\`a} jour et l{'}entra{\^\i}nement en configurations sous-optimales. Leur combinaison apporte un gain moyen de 0,2 UAS sur le corpus SPMRL. Nous introduisons {\'e}galement un cadre g{\'e}n{\'e}ral permettant la comparaison syst{\'e}matique de ces strat{\'e}gies et de la plupart des variantes connues. Nous montrons que la litt{\'e}rature n{'}a {\'e}tudi{\'e} que quelques strat{\'e}gies parmi les nombreuses variations possibles, n{\'e}gligeant ainsi plusieurs pistes d{'}am{\'e}liorations potentielles."
2016.jeptalnrecital-demo.12,Lecture bilingue augment{\\'e}e par des alignements multi-niveaux (Augmenting bilingual reading with alignment information),2016,-1,-1,1,1,837,franccois yvon,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 5 : D{\\'e}monstrations,0,"Le travail qui a conduit {\`a} cette d{\'e}monstration combine des outils de traitement des langues multilingues, en particulier l{'}alignement automatique, avec des techniques de visualisation et d{'}interaction. Il vise {\`a} proposer des pistes pour le d{\'e}veloppement d{'}outils permettant de lire simultan{\'e}ment les diff{\'e}rentes versions d{'}un texte disponible en plusieurs langues, avec des applications en lecture de loisir ou en lecture professionnelle."
W15-3012,The {KIT}-{LIMSI} Translation System for {WMT} 2015,2015,30,1,6,0,5767,thanhle ha,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper presented the joined submission of KIT and LIMSI to the English to German translation task of WMT 2015. In this year submission, we integrated a neural network-based translation model into a phrase-based translation model by rescoring the n-best lists. Since the computation complexity is one of the main issues for continuous space models, we compared two techniques to reduce the computation cost. We investigated models using a structured output layer as well as models trained with noise contrastive estimation. Furthermore, we evaluated a new method to obtain the best log-linear combination in the rescoring phase. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points."
W15-3016,{LIMSI}@{WMT}{'}15 : Translation Task,2015,16,5,11,0,8610,benjamin marie,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9915 translation task. We report results for French-English, Russian-English in both directions, as well as for Finnish-into-English. Our submissions use NCODE and MOSES along with continuous space translation models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: for Russian-English, we investigate a tailored normalization of Russian to translate into English, and a two-step process to translate first into simplified Russian, followed by a conversion into inflected Russian. For French-English, the challenge is domain adaptation, for which only monolingual corpora are available. Finally, for the Finnish-to-English task, we explore unsupervised morphological segmentation to reduce the sparsity of data induced by the rich morphology on the Finnish side."
W15-3027,Why Predicting Post-Edition is so Hard? Failure Analysis of {LIMSI} Submission to the {APE} Shared Task,2015,5,4,3,1,168,guillaume wisniewski,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"This paper describes the two systems submitted by LIMSI to the WMTxe2x80x9915 Shared Task on Automatic Post-Editing. The first one relies on a reformulation of the APE task as a Machine Translation task; the second implements a simple rule-based approach. Neither of these two systems manage to improve the automatic translation. We show, by carefully analyzing the failure of our systems that this counterperformance mainly results from the inconsistency in the annotations."
D15-1121,A Discriminative Training Procedure for Continuous Translation Models,2015,33,2,3,1,36853,quockhanh do,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Continuous-space translation models have recently emerged as extremely powerful ways to boost the performance of existing translation systems. A simple, yet effective way to integrate such models in inference is to use them in an N -best rescoring step. In this paper, we focus on this scenario and show that the performance gains in rescoring can be greatly increased when the neural network is trained jointly with all the other model parameters, using an appropriate objective function. Our approach is validated on two domains, where it outperforms strong baselines."
2015.lilt-12.6,Sentence alignment for literary texts: The state-of-the-art and beyond,2015,-1,-1,3,1,34613,yong xu,"Linguistic Issues in Language Technology, Volume 12, 2015 - Literature Lifts up Computational Linguistics",0,"Literary works are becoming increasingly available in electronic formats, thus quickly transforming editorial processes and reading habits. In the context of the global enthusiasm for multilingualism, the rapid spread of e-book readers, such as Amazon Kindle R or Kobo Touch R , fosters the development of a new generation of reading tools for bilingual books. In particular, literary works, when available in several languages, offer an attractive perspective for self-development or everyday leisure reading, but also for activities such as language learning, translation or literary studies. An important issue in the automatic processing of multilingual e-books is the alignment between textual units. Alignment could help identify corresponding text units in different languages, which would be particularly beneficial to bilingual readers and translation professionals. Computing automatic alignments for literary works, however, is a task more challenging than in the case of better behaved corpora such as parliamentary proceedings or technical manuals. In this paper, we revisit the problem of computing high-quality. alignment for literary works. We first perform a large-scale evaluation of automatic alignment for literary texts, which provides a fair assessment of the actual difficulty of this task. We then introduce a two-pass approach, based on a maximum entropy model. Experimental results for novels available in English and French or in English and Spanish demonstrate the effectiveness of our method."
2015.jeptalnrecital-long.1,Apprentissage par imitation pour l{'}{\\'e}tiquetage de s{\\'e}quences : vers une formalisation des m{\\'e}thodes d{'}{\\'e}tiquetage easy-first,2015,-1,-1,3,0.952381,14735,elena knyazeva,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"De nombreuses m{\'e}thodes ont {\'e}t{\'e} propos{\'e}es pour acc{\'e}l{\'e}rer la pr{\'e}diction d{'}objets structur{\'e}s (tels que les arbres ou les s{\'e}quences), ou pour permettre la prise en compte de d{\'e}pendances plus riches afin d{'}am{\'e}liorer les performances de la pr{\'e}diction. Ces m{\'e}thodes reposent g{\'e}n{\'e}ralement sur des techniques d{'}inf{\'e}rence approch{\'e}e et ne b{\'e}n{\'e}ficient d{'}aucune garantie th{\'e}orique aussi bien du point de vue de la qualit{\'e} de la solution trouv{\'e}e que du point de vue de leur crit{\`e}re d{'}apprentissage. Dans ce travail, nous {\'e}tudions une nouvelle formulation de l{'}apprentissage structur{\'e} qui consiste {\`a} voir celui-ci comme un processus incr{\'e}mental au cours duquel la sortie est construite de fa{\c{c}}on progressive. Ce cadre permet de formaliser plusieurs approches de pr{\'e}diction structur{\'e}e existantes. Gr{\^a}ce au lien que nous faisons entre apprentissage structur{\'e} et apprentissage par renforcement, nous sommes en mesure de proposer une m{\'e}thode th{\'e}oriquement bien justifi{\'e}e pour apprendre des m{\'e}thodes d{'}inf{\'e}rence approch{\'e}e. Les exp{\'e}riences que nous r{\'e}alisons sur quatre t{\^a}ches de TAL valident l{'}approche propos{\'e}e."
2015.jeptalnrecital-long.4,"Oublier ce qu{'}on sait, pour mieux apprendre ce qu{'}on ne sait pas : une {\\'e}tude sur les contraintes de type dans les mod{\\`e}les {CRF}",2015,-1,-1,5,1,36855,nicolas pecheux,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Quand on dispose de connaissances a priori sur les sorties possibles d{'}un probl{\`e}me d{'}{\'e}tiquetage, il semble souhaitable d{'}inclure cette information lors de l{'}apprentissage pour simplifier la t{\^a}che de mod{\'e}lisation et acc{\'e}l{\'e}rer les traitements. Pourtant, m{\^e}me lorsque ces contraintes sont correctes et utiles au d{\'e}codage, leur utilisation lors de l{'}apprentissage peut d{\'e}grader s{\'e}v{\`e}rement les performances. Dans cet article, nous {\'e}tudions ce paradoxe et montrons que le manque de contraste induit par les connaissances entra{\^\i}ne une forme de sous-apprentissage qu{'}il est cependant possible de limiter."
2015.jeptalnrecital-long.23,Apprentissage discriminant des mod{\\`e}les continus de traduction,2015,-1,-1,3,1,36853,quockhanh do,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Alors que les r{\'e}seaux neuronaux occupent une place de plus en plus importante dans le traitement automatique des langues, les m{\'e}thodes d{'}apprentissage actuelles utilisent pour la plupart des crit{\`e}res qui sont d{\'e}corr{\'e}l{\'e}s de l{'}application. Cet article propose un nouveau cadre d{'}apprentissage discriminant pour l{'}estimation des mod{\`e}les continus de traduction. Ce cadre s{'}appuie sur la d{\'e}finition d{'}un crit{\`e}re d{'}optimisation permettant de prendre en compte d{'}une part la m{\'e}trique utilis{\'e}e pour l{'}{\'e}valuation de la traduction et d{'}autre part l{'}int{\'e}gration de ces mod{\`e}les au sein des syst{\`e}mes de traduction automatique. De plus, cette m{\'e}thode d{'}apprentissage est compar{\'e}e aux crit{\`e}res existants d{'}estimation que sont le maximum de vraisemblance et l{'}estimation contrastive bruit{\'e}e. Les exp{\'e}riences men{\'e}es sur la t{\^a}ches de traduction des s{\'e}minaires TED Talks de l{'}anglais vers le fran{\c{c}}ais montrent la pertinence d{'}un cadre discriminant d{'}apprentissage, dont les performances restent toutefois tr{\`e}s d{\'e}pendantes du choix d{'}une strat{\'e}gie d{'}initialisation idoine. Nous montrons qu{'}avec une initialisation judicieuse des gains significatifs en termes de scores BLEU peuvent {\^e}tre obtenus."
2015.iwslt-papers.10,Morphology-aware alignments for translation to and from a synthetic language,2015,23,2,2,1,23863,franck burlot,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,"Most statistical translation models rely on the unsupervized computation of word-based alignments, which both serve to identify elementary translation units and to uncover hidden translation derivations. It is widely acknowledged that such alignments can only be reliably established for languages that share a sufficiently close notion of a word. When this is not the case, the usual method is to pre-process the data so as to balance the number of tokens on both sides of the corpus. In this paper, we propose a factored alignment model specifically designed to handle alignments involving a synthetic language (using the case of the Czech:English language pair). We show that this model can greatly reduce the number of non-aligned words on the English side, yielding more compact translation models, with little impact on the translation quality in our testing conditions."
W14-3307,The {KIT}-{LIMSI} Translation System for {WMT} 2014,2014,27,1,5,0.833333,14186,quoc do,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score."
W14-3330,{LIMSI} @ {WMT}{'}14 Medical Translation Task,2014,-1,-1,10,1,36855,nicolas pecheux,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
W14-3344,{LIMSI} Submission for {WMT}{'}14 {QE} Task,2014,16,8,4,1,168,guillaume wisniewski,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI participation to the WMTxe2x80x9914 Shared Task on Quality Estimation; we took part to the wordlevel quality estimation task for English to Spanish translations. Our system relies on a random forest classifier, an ensemble method that has been shown to be very competitive for this kind of task, when only a few dense and continuous features are used. Notably, only 16 features are used in our experiments. These features describe, on the one hand, the quality of the association between the source sentence and each target word and, on the other hand, the fluency of the hypothesis. Since the evaluation criterion is the f1 measure, a specific tuning strategy is proposed to select the optimal values for the hyper-parameters. Overall, our system achieves a 0.67 f1 score on a randomly extracted test set."
wisniewski-etal-2014-corpus,A Corpus of Machine Translation Errors Extracted from Translation Students Exercises,2014,8,3,3,1,168,guillaume wisniewski,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present a freely available corpus of automatic translations accompanied with post-edited versions, annotated with labels identifying the different kinds of errors made by the MT system. These data have been extracted from translation students exercises that have been corrected by a senior professor. This corpus can be useful for training quality estimation tools and for analyzing the types of errors made MT system."
pecheux-etal-2014-rule,Rule-based Reordering Space in Statistical Machine Translation,2014,32,0,3,1,36855,nicolas pecheux,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In Statistical Machine Translation (SMT), the constraints on word reorderings have a great impact on the set of potential translations that are explored. Notwithstanding computationnal issues, the reordering space of a SMT system needs to be designed with great care: if a larger search space is likely to yield better translations, it may also lead to more decoding errors, because of the added ambiguity and the interaction with the pruning strategy. In this paper, we study this trade-off using a state-of-the art translation system, where all reorderings are represented in a word lattice prior to decoding. This allows us to directly explore and compare different reordering spaces. We study in detail a rule-based preordering system, varying the length or number of rules, the tagset used, as well as contrasting with oracle settings and purely combinatorial subsets of permutations. We focus on two language pairs: English-French, a close language pair and English-German, known to be a more challenging reordering pair."
F14-3011,(Much) Faster Construction of {SMT} Phrase Tables from Large-scale Parallel Corpora (Construction (tr{\\`e}s) rapide de tables de traduction {\\`a} partir de grands bi-textes) [in {F}rench],2014,0,0,3,1,26548,li gong,Proceedings of TALN 2014 (Volume 3: System Demonstrations),0,None
F14-2002,Towards a More Efficient Development of Statistical Machine Translation Systems (Vers un d{\\'e}veloppement plus efficace des syst{\\`e}mes de traduction statistique : un peu de vert dans un monde de {BLEU}) [in {F}rench],2014,0,0,3,1,26548,li gong,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F14-1016,Cross-Lingual {POS} Tagging through Ambiguous Learning: First Experiments (Apprentissage partiellement supervis{\\'e} d{'}un {\\'e}tiqueteur morpho-syntaxique par transfert cross-lingue) [in {F}rench],2014,-1,-1,5,1,168,guillaume wisniewski,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
F14-1023,Comparison of scheduling methods for the learning rate of neural network language models (Mod{\\`e}les de langue neuronaux: une comparaison de plusieurs strat{\\'e}gies d{'}apprentissage) [in {F}rench],2014,0,0,3,1,36853,quockhanh do,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
F14-1025,Topic Adaptation for the Automatic Translation of News Articles (Adaptation th{\\'e}matique pour la traduction automatique de d{\\'e}p{\\^e}ches de presse) [in {F}rench],2014,-1,-1,3,1,40027,souhir gahbichebraham,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
D14-1187,Cross-Lingual Part-of-Speech Tagging through Ambiguous Learning,2014,21,16,4,1,168,guillaume wisniewski,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"When Part-of-Speech annotated data is scarce, e.g. for under-resourced languages, one can turn to cross-lingual transfer and crawled dictionaries to collect partially supervised data. We cast this problem in the framework of ambiguous learning and show how to learn an accurate history-based model. Experiments on ten languages show significant improvements over prior state of the art performance."
2014.iwslt-papers.6,Discriminative adaptation of continuous space translation models,2014,5,1,3,1,36853,quockhanh do,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"In this paper we explore various adaptation techniques for continuous space translation models (CSTMs). We consider the following practical situation: given a large scale, state-of-the-art SMT system containing a CSTM, the task is to adapt the CSTM to a new domain using a (relatively) small in-domain parallel corpus. Our method relies on the definition of a new discriminative loss function for the CSTM that borrows from both the max-margin and pair-wise ranking approaches. In our experiments, the baseline out-of-domain SMT system is initially trained for the WMT News translation task, and the CSTM is to be adapted to the lecture translation task as defined by IWSLT evaluation campaign. Experimental results show that an improvement of 1.5 BLEU points can be achieved with the proposed adaptation method."
2014.iwslt-papers.9,Incremental development of statistical machine translation systems,2014,-1,-1,3,1,26548,li gong,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Statistical Machine Translation produces results that make it a competitive option in most machine-assisted translation scenarios. However, these good results often come at a very high computational cost and correspond to training regimes which are unfit to many practical contexts, where the ability to adapt to users and domains and to continuously integrate new data (eg. in post-edition contexts) are of primary importance. In this article, we show how these requirements can be met using a strategy for on-demand word alignment and model estimation. Most remarkably, our incremental system development framework is shown to deliver top quality translation performance even in the absence of tuning, and to surpass a strong baseline when performing online tuning. All these results obtained with great computational savings as compared to conventional systems."
2014.iwslt-evaluation.15,{LIMSI} {E}nglish-{F}rench speech translation system,2014,-1,-1,7,0,31615,natalia segal,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper documents the systems developed by LIMSI for the IWSLT 2014 speech translation task (EnglishâFrench). The main objective of this participation was twofold: adapting different components of the ASR baseline system to the peculiarities of TED talks and improving the machine translation quality on the automatic speech recognition output data. For the latter task, various techniques have been considered: punctuation and number normalization, adaptation to ASR errors, as well as the use of structured output layer neural network models for speech data."
2014.amta-researchers.17,Combining techniques from different {NN}-based language models for machine translation,2014,23,1,3,0,5714,jan niehues,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas: MT Researchers Track,0,"This paper presents two improvements of language models based on Restricted Boltzmann Machine (RBM) for large machine translation tasks. In contrast to other continuous space approach, RBM based models can easily be integrated into the decoder and are able to directly learn a hidden representation of the n-gram. Previous work on RBM-based language models do not use a shared word representation and therefore, they might suffer of a lack of generalization for larger contexts. Moreover, since the training step is very time consuming, they are only used for quite small copora. In this work we add a shared word representation for the RBM-based language model by factorizing the weight matrix. In addition, we propose an efficient and tailored sampling algorithm that allows us to drastically speed up the training process. Experiments are carried out on two German to English translation tasks and the results show that the training time could be reduced by a factor of 10 without any drop in performance. Furthermore, the RBM-based model can also be trained on large size corpora."
W13-2204,{LIMSI} @ {WMT}13,2013,27,0,8,1,38567,alexander allauzen,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9913 translation task. We report results for French-English, German-English and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated."
W13-2250,{LIMSI} Submission for the {WMT}{`}13 Quality Estimation Task: an Experiment with N-Gram Posteriors,2013,18,2,3,0,13740,anil singh,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task. Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output.
F13-2028,A corpus of post-edited translations (Un corpus d{'}erreurs de traduction) [in {F}rench],2013,-1,-1,4,1,168,guillaume wisniewski,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
F13-1033,A fully discriminative training framework for Statistical Machine Translation (Un cadre d{'}apprentissage int{\\'e}gralement discriminant pour la traduction statistique) [in {F}rench],2013,0,1,3,1,8590,thomas lavergne,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
2013.mtsummit-papers.15,"Design and Analysis of a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition",2013,-1,-1,4,1,168,guillaume wisniewski,Proceedings of Machine Translation Summit XIV: Papers,0,None
2013.iwslt-papers.7,Improving bilingual sub-sentential alignment by sampling-based transpotting,2013,-1,-1,3,1,26548,li gong,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"In this article, we present a sampling-based approach to improve bilingual sub-sentential alignment in parallel corpora. This approach can be used to align parallel sentences on an as needed basis, and is able to accurately align newly available sentences. We evaluate the resulting alignments on several Machine Translation tasks. Results show that for the tasks considered here, our approach performs on par with the state-of-the-art statistical alignment pipeline giza++/Moses, and obtains superior results in a number of configurations, notably when aligning additional parallel sentence pairs carefully selected to match the test input."
W12-4201,{WSD} for n-best reranking and local language modeling in {SMT},2012,22,6,5,0,2673,marianna apidianaki,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We integrate semantic information at two stages of the translation process of a state-of-the-art SMT system. A Word Sense Disambiguation (WSD) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways. First, the probabilities serve to rerank a list of n-best translations produced by the system. Second, the WSD predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this specific sentential context. Both approaches lead to significant improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT."
W12-3120,Non-Linear Models for Confidence Estimation,2012,14,4,3,0,42247,yong zhuang,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes our work with the data distributed for the WMT'12 Confidence Estimation shared task. Our contribution is twofold: i) we first present an analysis of the data which highlights the difficulty of the task and motivates our approach; ii) we show that using non-linear models, namely random forests, with a simple and limited feature set, succeeds in modeling the complex decisions required to assess translation quality and achieves results that are on a par with the second best results of the shared task."
W12-3141,{LIMSI} @ {WMT}12,2012,23,1,9,1,40944,haison le,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model."
W12-2701,Measuring the Influence of Long Range Dependencies with Neural Network Language Models,2012,31,17,3,1,42291,hai le,Proceedings of the {NAACL}-{HLT} 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for {HLT},0,"In spite of their well known limitations, most notably their use of very local contexts, n-gram language models remain an essential component of many Natural Language Processing applications, such as Automatic Speech Recognition or Statistical Machine Translation. This paper investigates the potential of language models using larger context windows comprising up to the 9 previous words. This study is made possible by the development of several novel Neural Network Language Model architectures, which can easily fare with such large context windows. We experimentally observed that extending the context size yields clear gains in terms of perplexity and that the n-gram assumption is statistically reasonable as long as n is sufficiently high, and that efforts should be focused on improving the estimation procedures for such large models."
W12-2505,Aligning Bilingual Literary Works: a Pilot Study,2012,16,1,3,0,20569,qian yu,Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature,0,"Electronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common. It is often the case that literary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation. To be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difficult task. In this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems. Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach significantly outperforms two open source tools."
N12-1005,Continuous Space Translation Models with Neural Networks,2012,33,103,3,1,42291,hai le,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models. For lack of sufficient training data, most models only consider a small amount of context. As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations. In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture. In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance."
gahbiche-braham-etal-2012-joint,Joint Segmentation and {POS} Tagging for {A}rabic Using a {CRF}-based Classifier,2012,18,11,4,1,40027,souhir gahbichebraham,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Arabic is a morphologically rich language, and Arabic texts abound of complex word forms built by concatenation of multiple subparts, corresponding for instance to prepositions, articles, roots prefixes, or suffixes. The development of Arabic Natural Language Processing applications, such as Machine Translation (MT) tools, thus requires some kind of morphological analysis. In this paper, we compare various strategies for performing such preprocessing, using generic machine learning techniques. The resulting tool is compared with two open domain alternatives in the context of a statistical MT task and is shown to be faster than its competitors, with no significant difference in MT quality."
F12-2009,Alignement sous-phrastique hi{\\'e}rarchique avec Anymalign (Hierarchical Sub-Sentential Alignment with Anymalign) [in {F}rench],2012,-1,-1,2,1,13815,adrien lardilleux,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2044,Rep{\\'e}rage des entit{\\'e}s nomm{\\'e}es pour l{'}arabe : adaptation non-supervis{\\'e}e et combinaison de syst{\\`e}mes (Named Entity Recognition for {A}rabic : Unsupervised adaptation and Systems combination) [in {F}rench],2012,0,0,4,1,40027,souhir gahbichebraham,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
E12-1013,Computing Lattice {BLEU} Oracle Scores for Machine Translation,2012,19,9,3,1,7089,artem sokolov,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The search space of Phrase-Based Statistical Machine Translation (PBSMT) systems can be represented under the form of a directed acyclic graph (lattice). The quality of this search space can thus be evaluated by computing the best achievable hypothesis in the lattice, the so-called oracle hypothesis. For common SMT metrics, this problem is however NP-hard and can only be solved using heuristics. In this work, we present two new methods for efficiently computing BLEU oracles on lattices: the first one is based on a linear approximation of the corpus BLEU score and is solved using the FST formalism; the second one relies on integer linear programming formulation and is solved directly and using the Lagrangian relaxation framework. These new decoders are positively evaluated and compared with several alternatives from the literature for three language pairs, using lattices produced by two PBSMT systems."
2012.iwslt-papers.20,Towards contextual adaptation for any-text translation,2012,22,3,3,1,26548,li gong,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"Adaptation for Machine Translation has been studied in a variety of ways, using an ideal scenario where the training data can be split into {''}out-of-domain{''} and {''}in-domain{''} corpora, on which the adaptation is based. In this paper, we consider a more realistic setting which does not assume the availability of any kind of {''}in-domain{''} data, hence the name {''}any-text translation{''}. In this context, we present a new approach to contextually adapt a translation model onthe-fly, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists."
2012.eamt-1.62,Hierarchical Sub-sentential Alignment with Anymalign,2012,34,11,2,1,13815,adrien lardilleux,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"We present a sub-sentential alignment algorithm that relies on association scores between words or phrases. This algorithm is inspired by previous work on alignment by recursive binary segmentation and on document clustering. We evaluate the resulting alignments on machine translation tasks and show that we can obtain state-of-the-art results, with gains up to more than 4 BLEU points compared to previous work, with a method that is simple, independent of the size of the corpus to be aligned, and directly computes symmetric alignments. This work also provides new insights regarding the use of heuristic alignment scores in statistical machine translation."
2012.amta-papers.17,Non-linear n-best List Reranking with Few Features,2012,-1,-1,3,1,7089,artem sokolov,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In Machine Translation, it is customary to compute the model score of a predicted hypothesis as a linear combination of multiple features, where each feature assesses a particular facet of the hypothesis. The choice of a linear combination is usually justified by the possibility of efficient inference (decoding); yet, the appropriateness of this simple combination scheme to the task at hand is rarely questioned. In this paper, we propose an approach that replaces the linear scoring function with a non-linear scoring function. To investigate the applicability of this approach, we rescore n-best lists generated with a conventional machine translation engine (using a linear scoring function for generating its hypotheses) with a non-linear scoring function learned using the learning-to-rank framework. Moderate, though consistent, gains in BLEU are demonstrated on the WMT{'}10, WMT{'}11 and WMT{'}12 test sets."
W11-4415,Measuring the Confusability of Pronunciations in Speech Recognition,2011,16,4,2,0,44031,panagiota karanasou,Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing,0,"In this work, we define a measure aimed at assessing how well a pronunciation model will function when used as a component of a speech recognition system. This measure, pronunciation entropy, fuses information from both the pronunciation model and the language model. We show how to compute this score by effectively composing the output of a phoneme recognizer with a pronunciation dictionary and a language model, and investigate its role as predictor of pronunciation model performance. We present results of this measure for different dictionaries with and without pronunciation variants and counts."
W11-2135,{LIMSI} @ {WMT}11,2011,19,17,6,1,5598,alexandre allauzen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the French-English and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up."
W11-2168,From n-gram-based to {CRF}-based Translation Models,2011,46,12,4,1,8590,thomas lavergne,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"A major weakness of extant statistical machine translation (SMT) systems is their lack of a proper training procedure. Phrase extraction and scoring processes rely on a chain of crude heuristics, a situation judged problematic by many. In this paper, we recast the machine translation problem in the familiar terms of a sequence labeling task, thereby enabling the use of enriched feature sets and exact training and inference procedures. The tractability of the whole enterprise is achieved through an efficient implementation of the conditional random fields (CRFs) model using a weighted finite-state transducers library. This approach is experimentally contrasted with several conventional phrase-based systems."
W11-1207,Two Ways to Use a Noisy Parallel News Corpus for Improving Statistical Machine Translation,2011,24,12,3,1,40027,souhir gahbichebraham,Proceedings of the 4th Workshop on Building and Using Comparable Corpora: Comparable Corpora and the Web,0,"In this paper, we present two methods to use a noisy parallel news corpus to improve statistical machine translation (SMT) systems. Taking full advantage of the characteristics of our corpus and of existing resources, we use a bootstrapping strategy, whereby an existing SMT engine is used both to detect parallel sentences in comparable data and to provide an adaptation corpus for translation models. MT experiments demonstrate the benefits of various combinations of these strategies."
2011.jeptalnrecital-long.36,G{\\'e}n{\\'e}ralisation de l{'}alignement sous-phrastique par {\\'e}chantillonnage (Generalization of sub-sentential alignment by sampling),2011,-1,-1,2,1,13815,adrien lardilleux,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}alignement sous-phrastique consiste {\`a} extraire des traductions d{'}unit{\'e}s textuelles de grain inf{\'e}rieur {\`a} la phrase {\`a} partir de textes multilingues parall{\`e}les align{\'e}s au niveau de la phrase. Un tel alignement est n{\'e}cessaire, par exemple, pour entra{\^\i}ner des syst{\`e}mes de traduction statistique. L{'}approche standard pour r{\'e}aliser cette t{\^a}che implique l{'}estimation successive de plusieurs mod{\`e}les probabilistes de complexit{\'e} croissante et l{'}utilisation d{'}heuristiques qui permettent d{'}aligner des mots isol{\'e}s, puis, par extension, des groupes de mots. Dans cet article, nous consid{\'e}rons une approche alternative, initialement propos{\'e}e dans (Lardilleux {\&} Lepage, 2008), qui repose sur un principe beaucoup plus simple, {\`a} savoir la comparaison des profils d{'}occurrences dans des souscorpus obtenus par {\'e}chantillonnage. Apr{\`e}s avoir analys{\'e} les forces et faiblesses de cette approche, nous montrons comment am{\'e}liorer la d{\'e}tection d{'}unit{\'e}s de traduction longues, et {\'e}valuons ces am{\'e}liorations sur des t{\^a}ches de traduction automatique."
2011.jeptalnrecital-long.37,Estimation d{'}un mod{\\`e}le de traduction {\\`a} partir d{'}alignements mot-{\\`a}-mot non-d{\\'e}terministes (Estimating a translation model from non-deterministic word-to-word alignments),2011,-1,-1,3,1,21324,nadi tomeh,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans les syst{\`e}mes de traduction statistique {\`a} base de segments, le mod{\`e}le de traduction est estim{\'e} {\`a} partir d{'}alignements mot-{\`a}-mot gr{\^a}ce {\`a} des heuristiques d{'}extraction et de valuation. Bien que ces alignements mot-{\`a}-mot soient construits par des mod{\`e}les probabilistes, les processus d{'}extraction et de valuation utilisent ces mod{\`e}les en faisant l{'}hypoth{\`e}se que ces alignements sont d{\'e}terministes. Dans cet article, nous proposons de lever cette hypoth{\`e}se en consid{\'e}rant l{'}ensemble de la matrice d{'}alignement, d{'}une paire de phrases, chaque association {\'e}tant valu{\'e}e par sa probabilit{\'e}. En comparaison avec les travaux ant{\'e}rieurs, nous montrons qu{'}en utilisant un mod{\`e}le exponentiel pour estimer de mani{\`e}re discriminante ces probabilit{\'e}s, il est possible d{'}obtenir des am{\'e}liorations significatives des performances de traduction. Ces am{\'e}liorations sont mesur{\'e}es {\`a} l{'}aide de la m{\'e}trique BLEU sur la t{\^a}che de traduction de l{'}arabe vers l{'}anglais de l{'}{\'e}valuation NIST MT{'}09, en consid{\'e}rant deux types de conditions selon la taille du corpus de donn{\'e}es parall{\`e}les utilis{\'e}es."
2011.iwslt-papers.10,How good are your phrases? Assessing phrase quality with single class classification,2011,0,8,5,1,21324,nadi tomeh,Proceedings of the 8th International Workshop on Spoken Language Translation: Papers,0,"We present a novel translation quality informed procedure for both extraction and scoring of phrase pairs in PBSMT systems. We reformulate the extraction problem in the supervised learning framework. Our goal is twofold. First, We attempt to take the translation quality into account; and second we incorporating arbitrary features in order to circumvent alignment errors. One-Class SVMs and the Mapping Convergence algorithm permit training a single-class classifier to discriminate between useful and useless phrase pairs. Such classifier can be learned from a training corpus that comprises only useful instances. The confidence score, produced by the classifier for each phrase pairs, is employed as a selection criteria. The smoothness of these scores allow a fine control over the size of the resulting translation model. Finally, confidence scores provide a new accuracy-based feature to score phrase pairs. Experimental evaluation of the method shows accurate assessments of phrase pairs quality even for regions in the space of possible phrase pairs that are ignored by other approaches. This enhanced evaluation of phrase pairs leads to improvements in the translation performance as measured by BLEU."
2011.iwslt-evaluation.7,{LIMSI}{'}s experiments in domain adaptation for {IWSLT}11,2011,20,13,4,1,8590,thomas lavergne,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"LIMSI took part in the IWSLT 2011 TED task in the MT track for English to French using the in-house n-code system, which implements the n-gram based approach to Machine Translation. This framework not only allows to achieve state-of-the-art results for this language pair, but is also appealing due to its conceptual simplicity and its use of well understood statistical language models. Using this approach, we compare several ways to adapt our existing systems and resources to the TED task with mixture of language models and try to provide an analysis of the modest gains obtained by training a log linear combination of inand out-of-domain models."
2011.iwslt-evaluation.15,Advances on spoken language translation in the Quaero program,2011,25,2,15,0,43221,karim boudahmane,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"The Quaero program is an international project promoting research and industrial innovation on technologies for automatic analysis and classification of multimedia and multilingual documents. Within the program framework, research organizations and industrial partners collaborate to develop prototypes of innovating applications and services for access and usage of multimedia data. One of the topics addressed is the translation of spoken language. Each year, a project-internal evaluation is conducted by DGA to monitor the technological advances. This work describes the design and results of the 2011 evaluation campaign. The participating partners were RWTH, KIT, LIMSI and SYSTRAN. Their approaches are compared on both ASR output and reference transcripts of speech data for the translation between French and German. The results show that the developed techniques further the state of the art and improve translation quality."
2011.eamt-1.33,Minimum Error Rate Training Semiring,2011,20,4,2,1,7089,artem sokolov,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"Modern Statistical Machine Translation (SMT) systems make their decisions based on multiple information sources, which assess various aspects of the match between a source sentence and its possible translation(s). Tuning a SMT system consists in finding the right balance between these sources so as to produce the best possible output, and is usually achieved through Minimum Error Rate Training (MERT) (Och, 2003). In this paper, we recast the operations implied in MERT in the terms of operations over a specific semiring, which, in particular, enables us to derive a simple and generic implementation of MERT over word lattices."
2011.eamt-1.41,Discriminative Weighted Alignment Matrices For Statistical Machine Translation,2011,22,0,3,1,21324,nadi tomeh,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"In extant phrase-based statistical machine translation (SMT) systems, the translation model relies on word-to-word alignments, which serve as constraints for the subsequent heuristic extraction and scoring processes. Word alignments are usually inferred in a probabilistic framework; yet, only one single best alignment is retained, as if alignments were deterministically produced. In this paper, we explore ways to take into account the entire alignment matrix, where each alignment link is scored by its probability. By comparison with previous attempts, we use an exponential model to compute these probabilities, which enables us to achieve significant improvements on the NIST MTxe2x80x9909 Arabic-English translation task."
W10-1704,{LIMSI}{'}s Statistical Translation Systems for {WMT}{'}10,2010,19,15,4,1,5598,alexandre allauzen,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes our Statistical Machine Translation systems for the WMT10 evaluation, where LIMSI participated for two language pairs (French-English and German-English, in both directions). For German-English, we concentrated on normalizing the German side through a proper preprocessing, aimed at reducing the lexical redundancy and at splitting complex compounds. For French-English, we studied two extensions of our in-house N-code decoder: firstly, the effect of integrating a new bilingual reordering model; second, the use of adaptation techniques for the translation model. For both set of experiments, we report the improvements obtained on the development and test data."
P10-1052,Practical Very Large Scale {CRF}s,2010,31,253,3,1,8590,thomas lavergne,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Conditional Random Fields (CRFs) are a widely-used approach for supervised sequence labelling, notably due to their ability to handle large description spaces and to integrate structural dependency between labels. Even for the simple linear-chain model, taking structure into account implies a number of parameters and a computational effort that grows quadratically with the cardinality of the label set. In this paper, we address the issue of training very large CRFs, containing up to hundreds output labels and several billion features. Efficiency stems here from the sparsity induced by the use of a l penalty term. Based on our own implementation, we compare three recent proposals for implementing this regularization strategy. Our experiments demonstrate that very large CRFs can be trained efficiently and that very large models are able to improve the accuracy, while delivering compact parameter sets."
max-etal-2010-contrastive,Contrastive Lexical Evaluation of Machine Translation,2010,15,10,3,0.345092,28247,aurelien max,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper advocates a complementary measure of translation performance that focuses on the constrastive ability of two or more systems or system versions to adequately translate source words. This is motivated by three main reasons : 1) existing automatic metrics sometimes do not show significant differences that can be revealed by fine-grained focussed human evaluation, 2) these metrics are based on direct comparisons between system hypotheses with the corresponding reference translations, thus ignoring the input words that were actually translated, and 3) as these metrics do not take input hypotheses from several systems at once, fine-grained contrastive evaluation can only be done indirectly. This proposal is illustrated on a multi-source Machine Translation scenario where multiple translations of a source text are available. Significant gains (up to +1.3 BLEU point) are achieved on these experiments, and contrastive lexical evaluation is shown to provide new information that can help to better analyse a system's performance."
D10-1076,Training Continuous Space Language Models: Some Practical Issues,2010,23,31,4,1,42291,hai le,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms. The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues. A new initialization scheme and new training techniques are then introduced. These methods are shown to greatly reduce the training time and to significantly improve performance, both in terms of perplexity and on a large-scale translation task."
D10-1091,Assessing Phrase-Based Translation Models with Oracle Decoding,2010,30,13,3,1,168,guillaume wisniewski,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"Extant Statistical Machine Translation (SMT) systems are very complex softwares, which embed multiple layers of heuristics and embark very large numbers of numerical parameters. As a result, it is difficult to analyze output translations and there is a real need for tools that could help developers to better understand the various causes of errors.n n In this study, we make a step in that direction and present an attempt to evaluate the quality of the phrase-based translation model. In order to identify those translation errors that stem from deficiencies in the phrase table (PT), we propose to compute the oracle BLEU-4 score, that is the best score that a system based on this PT can achieve on a reference corpus. By casting the computation of the oracle BLEU-1 as an Integer Linear Programming (ILP) problem, we show that it is possible to efficiently compute accurate lower-bounds of this score, and report measures performed on several standard benchmarks. Various other applications of these oracle decoding techniques are also reported and discussed."
C10-2023,Improving Reordering with Linguistically Informed Bilingual n-grams,2010,18,16,2,0.778773,836,josep crego,Coling 2010: Posters,0,"We present a new reordering model estimated as a standard n-gram language model with units built from morpho-syntactic information of the source and target languages. It can be seen as a model that translates the morpho-syntactic structure of the input sentence, in contrast to standard translation models which take care of the surface word forms. We take advantage from the fact that such units are less sparse than standard translation units to increase the size of bilingual context that is considered during the translation process, thus effectively accounting for mid-range reorderings. Empirical results on French-English and German-English translation tasks show that our model achieves higher translation accuracy levels than those obtained with the widely used lexicalized reordering model."
C10-1027,Local lexical adaptation in Machine Translation through triangulation: {SMT} helping {SMT},2010,23,11,3,0.778773,836,josep crego,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We present a framework where auxiliary MT systems are used to provide lexical predictions to a main SMT system. In this work, predictions are obtained by means of pivoting via auxiliary languages, and introduced into the main SMT system in the form of a low order language model, which is estimated on a sentence-by-sentence basis. The linear combination of models implemented by the decoder is thus extended with this additional language model. Experiments are carried out over three different translation tasks using the European Parliament corpus. For each task, nine additional languages are used as auxiliary languages to obtain the triangulated predictions. Translation accuracy results show that improvements in translation quality are obtained, even for large data conditions."
2010.jeptalnrecital-long.13,Recueil et analyse d{'}un corpus {\\'e}cologique de corrections orthographiques extrait des r{\\'e}visions de Wikip{\\'e}dia,2010,-1,-1,3,1,168,guillaume wisniewski,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous introduisons une m{\'e}thode {\`a} base de r{\`e}gles permettant d{'}extraire automatiquement de l{'}historique des {\'e}ditions de l{'}encyclop{\'e}die collaborative Wikip{\'e}dia des corrections orthographiques. Cette m{\'e}thode nous a permis de construire un corpus d{'}erreurs compos{\'e} de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n{'}existe pas, {\`a} notre connaissance, de plus gros corpus d{'}erreurs {\'e}cologiques librement disponible. En outre, les techniques mises en oeuvre peuvent {\^e}tre facilement transpos{\'e}es {\`a} de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l{'}{\'e}tude des erreurs fr{\'e}quentes ainsi que l{'}apprentissage et l{'}{\'e}valuation des correcteurs orthographiques automatiques. Plusieurs exp{\'e}riences illustrant son int{\'e}r{\^e}t sont propos{\'e}es."
2010.iwslt-evaluation.13,{LIMSI} @ {IWSLT} 2010,2010,30,0,6,1,5598,alexandre allauzen,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes LIMSI{'}s Statistical Machine Translation systems (SMT) for the IWSLT evaluation, where we participated in two tasks (Talk for English to French and BTEC for Turkish to English). For the Talk task, we studied an extension of our in-house n-code SMT system (the integration of a bilingual reordering model over generalized translation units), as well as the use of training data extracted from Wikipedia in order to adapt the target language model. For the BTEC task, we concentrated on pre-processing schemes on the Turkish side in order to reduce the morphological discrepancies with the English side. We also evaluated the use of two different continuous space language models for such a small size of training data."
2010.amta-papers.18,Refining Word Alignment with Discriminative Training,2010,-1,-1,3,1,21324,nadi tomeh,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"The quality of statistical machine translation systems depends on the quality of the word alignments that are computed during the translation model training phase. IBM alignment models, as implemented in the GIZA++ toolkit, constitute the de facto standard for performing these computations. The resulting alignments and translation models are however very noisy, and several authors have tried to improve them. In this work, we propose a simple and effective approach, which considers alignment as a series of independent binary classification problems in the alignment matrix. Through extensive feature engineering and the use of stacking techniques, we were able to obtain alignments much closer to manually defined references than those obtained by the IBM models. These alignments also yield better translation models, delivering improved performance in a large scale Arabic to English translation task."
W09-0417,{LIMSI}{`}s Statistical Translation Systems for {WMT}{`}09,2009,18,3,4,1,5598,alexandre allauzen,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus."
E09-1056,Improvements in Analogical Learning: Application to Translating Multi-Terms of the Medical Domain,2009,17,19,2,0.019654,7084,philippe langlais,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"Handling terminology is an important matter in a translation workflow. However, current Machine Translation (MT) systems do not yet propose anything proactive upon tools which assist in managing terminological databases. In this work, we investigate several enhancements to analogical learning and test our implementation on translating medical terms. We show that the analogical engine works equally well when translating from and into a morphologically rich language, or when dealing with language pairs written in different scripts. Combining it with a phrase-based statistical engine leads to significant improvements."
2009.jeptalnrecital-court.28,Plusieurs langues (bien choisies) valent mieux qu{'}une : traduction statistique multi-source par renforcement lexical,2009,17,3,3,0.778773,836,josep crego,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les syst{\`e}mes de traduction statistiques int{\`e}grent diff{\'e}rents types de mod{\`e}les dont les pr{\'e}dictions sont combin{\'e}es, lors du d{\'e}codage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polys{\'e}miques, comme, par exemple, le mot avocat du fran{\c{c}}ais vers l{'}anglais (lawyer ou avocado), requiert l{'}utilisation de mod{\`e}les suppl{\'e}mentaires, dont l{'}estimation et l{'}int{\'e}gration s{'}av{\`e}rent complexes. Une alternative consiste {\`a} tirer parti de l{'}observation selon laquelle les ambigu{\""\i}t{\'e}s li{\'e}es {\`a} la polys{\'e}mie ne sont pas les m{\^e}mes selon les langues source consid{\'e}r{\'e}es. Si l{'}on dispose, par exemple, d{'}une traduction vers l{'}espagnol dans laquelle avocat a {\'e}t{\'e} traduit par aguacate, alors la traduction de ce mot vers l{'}anglais n{'}est plus ambigu{\""e}. Ainsi, la connaissance d{'}une traduction fran{\c{c}}ais!espagnol permet de renforcer la s{\'e}lection de la traduction avocado pour le syst{\`e}me fran{\c{c}}ais!anglais. Dans cet article, nous proposons d{'}utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectu{\'e}s par un syst{\`e}me de traduction automatique. En particulier, nous montrons une am{\'e}lioration des performances sur plusieurs m{\'e}triques lorsque les traductions auxiliaires utilis{\'e}es sont obtenues manuellement."
2009.eamt-1.10,Gappy Translation Units under Left-to-Right {SMT} Decoding,2009,16,12,2,0.778773,836,josep crego,Proceedings of the 13th Annual conference of the European Association for Machine Translation,0,"This paper presents an extension for a bilingual n-gram statistical machine translation (SMT) system based on allowing translation units with gaps. Our gappy translation units can be seen as a first step towards introducing hierarchical units similar to those employed in hierarchical MT systems. Our goal is double. On the one hand we aim at capturing the benefits of the higher generalization power shown by hierarchical systems. On the other hand, we want to avoid the computational burden of decoding based on parsing techniques, which among other drawbacks, make dicult the introduction of the required target language model costs. Our experiments show slight but consistent improvements for Chinese-toEnglish machine translation. Accuracy results are competitive with those achieved by a state-of-the-art phrasebased system."
W08-2106,Using {LDA} to detect semantically incoherent documents,2008,21,36,3,0,47687,hemant misra,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"Detecting the semantic coherence of a document is a challenging task and has several applications such as in text segmentation and categorization. This paper is an attempt to distinguish between a 'semantically coherent' true document and a 'randomly generated' false document through topic detection in the framework of latent Dirichlet analysis. Based on the premise that a true document contains only a few topics and a false document is made up of many topics, it is asserted that the entropy of the topic distribution will be lower for a true document than that for a false document. This hypothesis is tested on several false document sets generated by various methods and is found to be useful for fake content detection applications."
W08-0310,Limsi{'}s Statistical Translation Systems for {WMT}{`}08,2008,10,19,8,0,47820,daniel dechelotte,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper describes our statistical machine translation systems based on the Moses toolkit for the WMT08 shared task. We address the Europarl and News conditions for the following language pairs: English with French, German and Spanish. For Europarl, n-best rescoring is performed using an enhanced n-gram or a neuronal language model; for the News condition, language models incorporate extra training data. We also report unconvincing results of experiments with factored models."
C08-2013,Scaling up Analogical Learning,2008,6,20,2,0.019654,7084,philippe langlais,Coling 2008: Companion volume: Posters,0,"Recent years have witnessed a growing interest in analogical learning for NLP applications. If the principle of analogical learning is quite simple, it does involve complex steps that seriously limit its applicability, the most computationally demanding one being the identification of analogies in the input space. In this study, we investigate different strategies for efficiently solving this problem and study their scalability."
C08-1056,Normalizing {SMS}: are Two Metaphors Better than One ?,2008,17,124,2,0,27334,catherine kobus,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Electronic written texts used in computermediated interactions (e-mails, blogs, chats, etc) present major deviations from the norm of the language. This paper presents an comparative study of systems aiming at normalizing the orthography of French SMS messages: after discussing the linguistic peculiarities of these messages, and possible approaches to their automatic normalization, we present, evaluate and contrast two systems, one drawing inspiration from the Machine Translation task; the other using techniques that are commonly used in automatic speech recognition devices. Combining both approaches, our best normalization system achieves about 11% Word Error Rate on a test set of about 3000 unseen messages."
C08-1075,Robust Similarity Measures for Named Entities Matching,2008,10,53,2,0,19792,erwan moreau,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Matching coreferent named entities without prior knowledge requires good similarity measures. Soft-TFIDF is a fine-grained measure which performs well in this task. We propose to enhance this kind of metrics, through a generic model in which measures may be mixed, and show experimentally the relevance of this approach."
2008.jeptalnrecital-long.13,Transcrire les {SMS} comme on reconna{\\^\\i}t la parole,2008,-1,-1,2,0,27334,catherine kobus,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une architecture inspir{\'e}e des syst{\`e}mes de reconnaissance vocale pour effectuer une normalisation orthographique de messages en Â« langage SMS Â». Nous d{\'e}crivons notre syst{\`e}me de base, ainsi que diverses {\'e}volutions de ce syst{\`e}me, qui permettent d{'}am{\'e}liorer sensiblement la qualit{\'e} des normalisations produites."
2008.jeptalnrecital-court.19,Appariement d{'}entit{\\'e}s nomm{\\'e}es cor{\\'e}f{\\'e}rentes : combinaisons de mesures de similarit{\\'e} par apprentissage supervis{\\'e},2008,-1,-1,2,0,19792,erwan moreau,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"L{'}appariement d{'}entit{\'e}s nomm{\'e}es consiste {\`a} regrouper les diff{\'e}rentes formes sous lesquelles appara{\^\i}t une entit{\'e}. Pour cela, des mesures de similarit{\'e} textuelle sont g{\'e}n{\'e}ralement utilis{\'e}es. Nous proposons de combiner plusieurs mesures afin d{'}am{\'e}liorer les performances de la t{\^a}che d{'}appariement. {\`A} l{'}aide d{'}exp{\'e}riences men{\'e}es sur deux corpus, nous montrons la pertinence de l{'}apprentissage supervis{\'e} dans ce but, particuli{\`e}rement avec l{'}algorithme C4.5."
2006.jeptalnrecital-long.14,Productivit{\\'e} quantitative des suffixations par -it{\\'e} et -Able dans un corpus journalistique moderne,2006,-1,-1,9,0,5649,natalia grabar,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans ce travail, nous {\'e}tudions en corpus la productivit{\'e} quantitative des suffixations par -Able et par -it{\'e} du fran{\c{c}}ais, d{'}abord ind{\'e}pendamment l{'}une de l{'}autre, puis lorsqu{'}elles s{'}encha{\^\i}nent d{\'e}rivationnellement (la suffixation en -it{\'e} s{'}applique {\`a} des bases en -Able dans environ 15 {\%} des cas). Nous estimons la productivit{\'e} de ces suffixations au moyen de mesures statistiques dont nous suivons l{'}{\'e}volution par rapport {\`a} la taille du corpus. Ces deux suffixations sont productives en fran{\c{c}}ais moderne : elles forment de nouveaux lex{\`e}mes tout au long des corpus {\'e}tudi{\'e}s sans qu{'}on n{'}observe de saturation, leurs indices de productivit{\'e} montrent une {\'e}volution stable bien qu{'}{\'e}tant d{\'e}pendante des calculs qui leur sont appliqu{\'e}s. On note cependant que, de fa{\c{c}}on g{\'e}n{\'e}rale, de ces deux suffixations, c{'}est la suffixation par -it{\'e} qui est la plus fr{\'e}quente en corpus journalistique, sauf pr{\'e}cis{\'e}ment quand -it{\'e} s{'}applique {\`a} un adjectif en -Able. {\'E}tant entendu qu{'}un adjectif en -Able et le nom en -it{\'e} correspondant expriment la m{\^e}me propri{\'e}t{\'e}, ce r{\'e}sultat indique que la complexit{\'e} de la base est un param{\`e}tre {\`a} prendre en consid{\'e}ration dans la formation du lexique possible."
W05-0616,An Analogical Learner for Morphological Analysis,2005,16,74,2,0.833333,49191,nicolas stroppa,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"Analogical learning is based on a two-step inference process: (i) computation of a structural mapping between a new and a memorized situation; (ii) transfer of knowledge from the known to the unknown situation. This approach requires the ability to search for and exploit such mappings, hence the need to properly define analogical relationships, and to efficiently implement their computation.n n In this paper, we propose a unified definition for the notion of (formal) analogical proportion, which applies to a wide range of algebraic structures. We show that this definition is suitable for learning in domains involving large databases of structured data, as is especially the case in Natural Language Processing (NLP). We then present experimental results obtained on two morphological analysis tasks which demonstrate the flexibility and accuracy of this approach."
2004.jeptalnrecital-poster.22,Analogies dans les s{\\'e}quences : un solveur {\\`a} {\\'e}tats finis,2004,-1,-1,2,0.833333,49191,nicolas stroppa,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"L{'}apprentissage par analogie se fonde sur un principe inf{\'e}rentiel potentiellement pertinent pour le traitement des langues naturelles. L{'}utilisation de ce principe pour des t{\^a}ches d{'}analyse linguistique pr{\'e}suppose toutefois une d{\'e}finition formelle de l{'}analogie entre s{\'e}quences. Dans cet article, nous proposons une telle d{\'e}finition et montrons qu{'}elle donne lieu {\`a} l{'}implantation efficace d{'}un solveur d{'}{\'e}quations analogiques sous la forme d{'}un transducteur fini. Munis de ces r{\'e}sultats, nous caract{\'e}risons empiriquement l{'}extension analogique de divers langages finis, correspondant {\`a} des dictionnaires de quatre langues."
2003.jeptalnrecital-long.10,Apprentissage Automatique de Paraphrases pour l{'}Am{\\'e}lioration d{'}un Syst{\\`e}me de Questions-R{\\'e}ponses,2003,-1,-1,3,1,51263,florence duclaye,Actes de la 10{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous pr{\'e}sentons une m{\'e}thodologie d{'}apprentissage faiblement supervis{\'e} pour l{'}extraction automatique de paraphrases {\`a} partir du Web. {\`A} partir d{'}un seule exemple de paire (pr{\'e}dicat, arguments), un corpus est progressivement accumul{\'e} par sondage duWeb. Les phases de sondage alternent avec des phases de filtrage, durant lesquelles les paraphrases les moins plausibles sont {\'e}limin{\'e}es {\`a} l{'}aide d{'}une proc{\'e}dure de clustering non supervis{\'e}e. Ce m{\'e}canisme d{'}apprentissage s{'}appuie sur un syst{\`e}me de Questions-R{\'e}ponses existant et les paraphrases apprises seront utilis{\'e}es pour en am{\'e}liorer le rappel. Nous nous concentrons ici sur le m{\'e}canisme d{'}apprentissage de ce syst{\`e}me et en pr{\'e}sentons les premiers r{\'e}sultats."
duclaye-etal-2002-using,Using the Web as a Linguistic Resource for Learning Reformulations Automatically,2002,12,16,2,1,51263,florence duclaye,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The use of paraphrases as a potential way to improve question answering, machine translation or automatic text summarization systems has long attracted the interest of researchers in natural language processing. However, manually entering reformulations into a system is a tedious and time-consuming process, if not an endless one. In this paper, we introduce a learning machinery aimed at acquiring reformulations automatically. Our system uses the Web as a linguistic resource and takes advantage of the results of an existing question answering system. Starting with one single prototypical argument tuple of a given semantic relation, our system first searches for potential alternative formulations of the relation, then finds new potential argument tuples, and iterates this process to progressively validate the candidate formulations. This learning process combines an acquisition stage, whose goal is to retrieve new evidences from Web pages, and a validation stage, whose role is to filter out noise and discard invalid paraphrases. After justifying the use of the Web as a linguistic resource, we describe our system, and report on primary results on a series of test semantic relations."
de-mareuil-etal-2000-french,A {F}rench Phonetic Lexicon with Variants for Speech and Language Processing,2000,8,14,3,0,14683,philippe mareuil,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"This paper reports on a project aiming at the semi-automatic development of a large orthographic-phonetic lexicon for French, based on the Multext dictionary. It details the various stages of the project, with an emphasis on the methodological and design aspects. Information regarding the lexiconxe2x80x99s content is also given, together with a description of interface tools which should facilitate its exploitation."
P97-1055,Paradigmatic Cascades: A Linguistically Sound Model of Pronunciation by Analogy,1997,13,23,1,1,837,franccois yvon,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"We present and experimentally evaluate a new model of prounciation by analogy: the paradigmatic cascades model. Given a pronunciation lexicon, this algorithm first extracts the most productive paradigmatic mappings in the graphemic domain, and pairs them statistically with their correlate(s) in the phonemic domain. These mappings are used to search and retrieve in the lexical database the most promising analog of unseen words. We finally apply to the analogs pronunciation the correlated series of mappings in the phonemic domain to get the desired pronunciation."
