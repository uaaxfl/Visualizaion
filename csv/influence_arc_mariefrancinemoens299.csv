2020.aacl-main.50,D17-1209,0,0.0484683,"Missing"
2020.aacl-main.50,W04-1013,0,0.0504992,"NN, i.e. no Faster R-CNN features. † Results reported in the authors’ original papers. compute scores over several dimensions (object, relation, attribute, colour, count, and size). We use the MSCOCO karpathy split (Lin et al., 2014; Karpathy and Fei-Fei, 2015) which has 5k images each in validation and test sets, and we use the remaining 113k images for training. We build a vocabulary based on all words in the train split that occur at least 5 times. We use MSCOCO evaluation scripts (Lin et al., 2014) and report BLEU4 (B4; Papineni et al., 2002), CIDEr (C; Vedantam et al., 2015), ROUGE-L (R; Lin, 2004), and SPICE (S; Anderson et al., 2016). See Appendix A for extra information on our implementation and training procedures. 5.1 Image Captioning without Relational Features Our re-implementation of the BUTD baseline scores slightly worse compared to the results reported by Anderson et al. (2018). This difference can be attributed to the Faster R-CNN features used, i.e. we always use 36 objects per image whereas Anderson et al. (2018) use a variable number of objects per image (i.e. 10 to 100), and there are other smaller differences in their training procedure. Image Captioning with relational"
2020.aacl-main.50,K17-1041,0,0.025888,"ps using detected object labels (Lu et al., 2018); (3) and the BUTD model (Anderson et al., 2018) described in Section 4.1. We also compare with the following baselines that use scene graphs: (1) The “Know more, say less” model KMSL extracts features for objects and relations based on the scene graph, which are passed through two attention heads and finally combined using a flat attention head (Li and Jiang, 2019); and (2) the Cascade model (Wang et al., 2019) which is similar to our hierarchical attention model with a GAT layer, but that instead uses a relational graph convolutional network (Marcheggiani et al., 2017). We do not discuss model variants/results that are trained with an additional reinforcement learning step (Rennie et al., 2017; Yang et al., 2019) and only compare single model results, since training and performing inference with such models is very costly and orthogonal to our research questions. Our proposed models are: flat attention (FA), hierarchical attention with scene graph first (HA-SG) following Equation 5, hierarchical attention with objects detected first (HA-IM) following Equation 6, HA-SG with graph attention network (HASG+GAT), and HA-SG with conditional graph attention (HA-SG"
2020.aacl-main.50,P02-1040,0,0.115135,"scene graph). ∗ Model uses features from last convolutional layer in CNN, i.e. no Faster R-CNN features. † Results reported in the authors’ original papers. compute scores over several dimensions (object, relation, attribute, colour, count, and size). We use the MSCOCO karpathy split (Lin et al., 2014; Karpathy and Fei-Fei, 2015) which has 5k images each in validation and test sets, and we use the remaining 113k images for training. We build a vocabulary based on all words in the train split that occur at least 5 times. We use MSCOCO evaluation scripts (Lin et al., 2014) and report BLEU4 (B4; Papineni et al., 2002), CIDEr (C; Vedantam et al., 2015), ROUGE-L (R; Lin, 2004), and SPICE (S; Anderson et al., 2016). See Appendix A for extra information on our implementation and training procedures. 5.1 Image Captioning without Relational Features Our re-implementation of the BUTD baseline scores slightly worse compared to the results reported by Anderson et al. (2018). This difference can be attributed to the Faster R-CNN features used, i.e. we always use 36 objects per image whereas Anderson et al. (2018) use a variable number of objects per image (i.e. 10 to 100), and there are other smaller differences in"
2020.aacl-main.50,D19-6405,0,0.207364,"Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 504–515 c December 4 - 7, 2020. 2020 Association for Computational Linguistics inference time, etc. Scene graph generation (SGG) is the task where given an image a model predicts a graph with its objects and their relations. We use a pretrained SGG model (Xu et al., 2017) to obtain and inject explicit relation information into image captioning, and investigate different image captioning model architectures that incorporate object and relation features, similarly to Li and Jiang (2019); Wang et al. (2019). We propose an extension to graph attention networks (Veliˇckovi´c et al., 2018) which we call conditional graph attention (C-GAT), where we condition the updates of the scene graph features on the current image captioning decoder state. Finally, we conduct an in-depth analysis of the captions produced by different models and determine if scene graphs actually improve the content of the captions. Our approach is illustrated in Figure 1. Our main contributions are: • We investigate different graph-based architectures to fuse object and relation information derived from scene graph generation m"
2020.coling-main.610,L18-1433,0,0.39784,"n terms of precision, training time and inference efficiency. 1 Introduction The task of multi-hop explanation generation has recently received interest as it could be a stepping-stone towards general multi-hop inference over language. Multi-hop reasoning requires algorithms to combine multiple sources of evidence. This becomes increasingly hard when the number of required facts for an inference grows, because of the exploding number of combinations and phenomena such as semantic drift (Fried et al., 2015; Jansen, 2018). The WorldTree dataset was designed specifically for (&gt;2)-fact inference (Jansen et al., 2018; Xie et al., 2020): it consists of elementary science exam questions that can be explained by an average of 6 facts from a complementary dataset of textual facts. The explanation regeneration task as in the TextGraphs Shared Tasks (Jansen and Ustalov, 2019; Jansen and Ustalov, 2020) asks participants to retrieve and rank relevant facts (given one of these natural language questions and its answer as input1 ) such that the top-ranked facts explain the answer to the question. An example is shown in the upper left part of fig. 1 (and more in appendix A). As each question-answer pair potentially"
2020.coling-main.610,W18-1703,0,0.0742641,"n used with a pre-trained transformer model, outperforms the previous state-of-the-art in terms of precision, training time and inference efficiency. 1 Introduction The task of multi-hop explanation generation has recently received interest as it could be a stepping-stone towards general multi-hop inference over language. Multi-hop reasoning requires algorithms to combine multiple sources of evidence. This becomes increasingly hard when the number of required facts for an inference grows, because of the exploding number of combinations and phenomena such as semantic drift (Fried et al., 2015; Jansen, 2018). The WorldTree dataset was designed specifically for (&gt;2)-fact inference (Jansen et al., 2018; Xie et al., 2020): it consists of elementary science exam questions that can be explained by an average of 6 facts from a complementary dataset of textual facts. The explanation regeneration task as in the TextGraphs Shared Tasks (Jansen and Ustalov, 2019; Jansen and Ustalov, 2020) asks participants to retrieve and rank relevant facts (given one of these natural language questions and its answer as input1 ) such that the top-ranked facts explain the answer to the question. An example is shown in the"
2020.coling-main.610,D18-1405,0,0.0275866,"e x individually and trains to correctly classify the candidate f c as relevant or irrelevant. We propose to use the pairwise RankNet loss (Burges et al., 2005): L(xp , xn ; θ) = − log(σ(fθ (xp )) − σ(fθ (xn ))) , (2) fc Where σ is the logistic sigmoid function, and xp and xn are samples in which is a positive and negative fact, respectively. This loss is shown by Chen et al. (2009) to maximize a lower bound on the MAP. To further amplify between-fact interactions in the gradient, we also use the conditional ranking variant of Noise-Contrastive Estimation (NCE), which covers &gt;2 facts at once (Ma and Collins, 2018; Gutmann and Hyv¨arinen, 2010): exp(fθ (x1 ) − log Pn (f c )) L(x1,...,B ; θ) = − log PB , c )) exp(f (x ) − log P (f j n θ j=1 (3) Where x1 is positive and x&gt;1 are negative, B is the batch size, and Pn is a negative sampling distribution over candidates: a uniform distribution over visk (p). This loss has been used for training word embeddings as a more efficient approximation to the negative log-likelihood loss (Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). When training with NCE and RankNet, samples in one batch share a common prefix p and only differ in their candidate fact f c , so"
2020.coling-main.610,D19-1410,0,0.0605765,"Missing"
2020.coling-main.610,2020.lrec-1.671,0,0.382102,"training time and inference efficiency. 1 Introduction The task of multi-hop explanation generation has recently received interest as it could be a stepping-stone towards general multi-hop inference over language. Multi-hop reasoning requires algorithms to combine multiple sources of evidence. This becomes increasingly hard when the number of required facts for an inference grows, because of the exploding number of combinations and phenomena such as semantic drift (Fried et al., 2015; Jansen, 2018). The WorldTree dataset was designed specifically for (&gt;2)-fact inference (Jansen et al., 2018; Xie et al., 2020): it consists of elementary science exam questions that can be explained by an average of 6 facts from a complementary dataset of textual facts. The explanation regeneration task as in the TextGraphs Shared Tasks (Jansen and Ustalov, 2019; Jansen and Ustalov, 2020) asks participants to retrieve and rank relevant facts (given one of these natural language questions and its answer as input1 ) such that the top-ranked facts explain the answer to the question. An example is shown in the upper left part of fig. 1 (and more in appendix A). As each question-answer pair potentially has many supporting"
2020.emnlp-tutorials.5,D19-1215,1,0.883324,"Missing"
2020.emnlp-tutorials.5,kordjamshidi-etal-2010-spatial,1,0.743733,"Missing"
2020.emnlp-tutorials.5,P19-1655,0,0.0284336,"ng the question of reasoning, we (a) point out the role of qualitative and quantitative formal representations in helping spatial reasoning based on natural language and the possibility of learning such representations from data to support compositionality and inference (Hudson and Manning, 2018; Hu et al., 2017); and (b) examine how continuous representations contribute to supporting reasoning and alternative hypothesis formation in learning (Krishnaswamy et al., 2019). We point to the cutting edge research that shows the influence of explicit representation of spatial entities and concepts (Hu et al., 2019; Liu et al., 2019). • Spatial Reasoning – Overview on natural language and visual reasoning tasks and data – Modeling compositionality and spatial reasoning in (Deep) learning models • Downstream tasks – Spatial concepts in dialogue systems – Spatial reasoning for QA and VQA – HRI, navigation and way-finding instructions – Corpus-based GIS systems 3 Prerequisites and reading list Familiarity with machine learning and natural language processing will be helpful for this tutorial. Our selected reading list is as follows. 29 • Qualitative spatial representation and reasoning. Anthony G. Cohn, an"
2020.emnlp-tutorials.5,P06-1131,0,0.0866842,"hidi and Moens, 2015) and extraction that is driven by various target tasks and applications. We discuss machine learning models including structured output prediction models, deep learning architectures and probabilistic graphical models that have been used in the related work. 2 Outline The tutorial will cover the following syllabus: • Spatial Representations Finally, we overview the usage of spatial semantics by various downstream tasks and killer applications including language grounding, navigation, self-driving cars, robotics (Tellex et al., 2011; Kollar et al., 2010), dialogue systems (Kelleher and Kruijff, 2006) and human machine interaction, and geographical information systems and knowledge graphs (Stock et al., 2013; Mai et al., 2020). Spatial semantics is very closely connected and relevant to visualization of natural language and grounding language into perception, central to dealing with configurations in the physical world and motivating a combination of vision and language for richer spatial understanding. The related tasks include: text-to-scene conversion; image captioning; spatial and visual question answering; and spatial understanding in multimodal settings (Rahgooy et al., 2018) for rob"
2020.emnlp-tutorials.5,N18-2124,1,0.831025,"(Kelleher and Kruijff, 2006) and human machine interaction, and geographical information systems and knowledge graphs (Stock et al., 2013; Mai et al., 2020). Spatial semantics is very closely connected and relevant to visualization of natural language and grounding language into perception, central to dealing with configurations in the physical world and motivating a combination of vision and language for richer spatial understanding. The related tasks include: text-to-scene conversion; image captioning; spatial and visual question answering; and spatial understanding in multimodal settings (Rahgooy et al., 2018) for robotics and navigation tasks and language grounding (Thomason et al., 2018). – Linguistic corpora and semantic annotations – Spatial knowledge representation and spatial calculi models – Distributed representations • Spatial Information Extraction – Spatial entity and relation extraction – Spatial ontology population – Considering domain knowledge and pragmatics in spatial extractions • Spatial Semantic Grounding – Combining vision and language (symbolic and multimodal embeddings) – Capturing spatial common sense – Grounding language in 2D and 3D physical worlds – Generating referring ex"
2020.emnlp-tutorials.5,P17-2034,0,0.0288127,"semantics (Cohn et al., 1997). Spatial language meaning representation includes research related to cognitive and linguistically motivated spatial semantic representations, spatial knowledge representation and spatial ontologies, qualitative and quantitative representation models used for formal meaning representation, and various spatial annotation schema and efforts for creating specialized corpora. We discuss various datasets that either focus on spatial annotations or downstream tasks that need spatial language learning and reasoning. Particularly, natural language visual reasoning data (Suhr et al., 2017, 2018). Moreover, continuous meaning representations for spatial concepts is another aspect to be highlighted in the tutorial, e.g., (Collell Talleda and Moens, 2018; Collell Talleda et al., 2018; DeruytThis tutorial provides an overview over the cutting edge research on spatial language understanding. However, we cover some background material from various perspectives given that ACL community has not paid enough attention, in the last two decades, to this topic. There are a few emerging research work very recently looking back into the importance of spatial language in various NLP tasks. On"
2020.findings-emnlp.408,D14-1217,0,0.142962,"s intuitive for humans, while arranging objects in a multidimensional space is tedious. Therefore, building models that exhibit spatial understanding is a key step towards automation – aiding humans in the repetitive time-consuming tasks. To date, multiple methods that explicitly investigate spatial reasoning in a multidimensional space have been proposed. However, the main limitations are: (i) scene environments with strong priors on (relative) object placements (e.g., indoor home environments); (Chang et al., 2017; Fisher et al., 2012; Choi et al., 2013; Xu et al., 2013; Jiang et al., 2012; Chang et al., 2014; Kermani et al., 2016); (ii) modelling only pairwise relationships (i.e., two objects and a single relationship) (Dan et al., 2020a; Collell et al., 2018); (iii) not using natural language descriptions of scenes, but only structured language (Collell et al., 2018; Dan et al., 2020b). In this paper we address the three limitations above by introducing a model that analyzes all available textual and visual data jointly. We formally frame our research problem as: “Given a set of discretely encoded clip-arts (people, objects, etc.), and a textual description of a scene, what 4549 Findings of the"
2020.findings-emnlp.408,W19-8668,0,0.0264721,"Zitnick and Parikh (2013) and generate realistically-looking scenes, given a language description of the scene’s spatial arrangements. Despite showcasing that our method is superior to theirs, it can also complete partial scenes and is more extensively evaluated on the Abstract Scenes dataset. Dan et al. (2020a) predict the relationship word given the image, a bounding box, and the subject and object words by using a spatial model to filter the predictions of a finetuned B ERT model. Their model does not decode language to 2D spatial arrangements while reasoning about their position. Finally, Ghanimifard and Dobnik (2019) generate spatial image descriptions to investigate what kind of spatial bottom-up knowledge, benefits the top-down methods the most. 6 Conclusion In this paper, we address the problem of spatial understanding by predicting spatial arrangements of scenes given their natural language descriptions. This work advances towards general spatial understanding of visual scenes and language by addressing the limitations of prior work: (i) modelling only pairwise relationships; (ii) using scene environments with strong priors on (relative) object placements (e.g., indoor home environments); (iii) use of"
2020.findings-emnlp.408,2020.lrec-1.288,0,0.435076,"al understanding is a key step towards automation – aiding humans in the repetitive time-consuming tasks. To date, multiple methods that explicitly investigate spatial reasoning in a multidimensional space have been proposed. However, the main limitations are: (i) scene environments with strong priors on (relative) object placements (e.g., indoor home environments); (Chang et al., 2017; Fisher et al., 2012; Choi et al., 2013; Xu et al., 2013; Jiang et al., 2012; Chang et al., 2014; Kermani et al., 2016); (ii) modelling only pairwise relationships (i.e., two objects and a single relationship) (Dan et al., 2020a; Collell et al., 2018); (iii) not using natural language descriptions of scenes, but only structured language (Collell et al., 2018; Dan et al., 2020b). In this paper we address the three limitations above by introducing a model that analyzes all available textual and visual data jointly. We formally frame our research problem as: “Given a set of discretely encoded clip-arts (people, objects, etc.), and a textual description of a scene, what 4549 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4549–4560 c November 16 - 20, 2020. 2020 Association for Computational"
2020.findings-emnlp.408,2020.lrec-1.717,0,0.383582,"al understanding is a key step towards automation – aiding humans in the repetitive time-consuming tasks. To date, multiple methods that explicitly investigate spatial reasoning in a multidimensional space have been proposed. However, the main limitations are: (i) scene environments with strong priors on (relative) object placements (e.g., indoor home environments); (Chang et al., 2017; Fisher et al., 2012; Choi et al., 2013; Xu et al., 2013; Jiang et al., 2012; Chang et al., 2014; Kermani et al., 2016); (ii) modelling only pairwise relationships (i.e., two objects and a single relationship) (Dan et al., 2020a; Collell et al., 2018); (iii) not using natural language descriptions of scenes, but only structured language (Collell et al., 2018; Dan et al., 2020b). In this paper we address the three limitations above by introducing a model that analyzes all available textual and visual data jointly. We formally frame our research problem as: “Given a set of discretely encoded clip-arts (people, objects, etc.), and a textual description of a scene, what 4549 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4549–4560 c November 16 - 20, 2020. 2020 Association for Computational"
2020.findings-emnlp.408,D19-1633,0,0.0616752,"November 16 - 20, 2020. 2020 Association for Computational Linguistics is the best positioning of the clip-arts that corresponds to the spatial relations implied by the text?”. Our approach is based on a large pre-trained language model – B ERT (Devlin et al., 2018), adapted to jointly process multi-modal data with distinct positional encoding. We introduce SR-B ERT, a model that explicitly focuses on the spatial relations – decoding the language cues to 2D spatial arrangements, achieved by masking the information related to the spatial arrangements during training. We build on the methods of Ghazvininejad et al. (2019a); Kasai et al. (2020); Wang and Cho (2019); Lee et al. (2018), initially proposed for nonautoregressive text decoding, in our case specifically adapted to iteratively mask-out and predict the spatial arrangements of the objects of interest. Inspired by Lawrence et al. (2019) we develop distinct ways of imposing a decoding order, tailored for generating spatial arrangements from language. We perform ad-hoc experiments to gain insights in three main research questions: (RQ 1) Can we decode a set of language spatial relations to the 2D space without imposing constraints on the number and type o"
2020.findings-emnlp.408,kordjamshidi-etal-2010-spatial,1,0.812283,"Missing"
2020.findings-emnlp.408,D19-1001,0,0.0254495,"intly process multi-modal data with distinct positional encoding. We introduce SR-B ERT, a model that explicitly focuses on the spatial relations – decoding the language cues to 2D spatial arrangements, achieved by masking the information related to the spatial arrangements during training. We build on the methods of Ghazvininejad et al. (2019a); Kasai et al. (2020); Wang and Cho (2019); Lee et al. (2018), initially proposed for nonautoregressive text decoding, in our case specifically adapted to iteratively mask-out and predict the spatial arrangements of the objects of interest. Inspired by Lawrence et al. (2019) we develop distinct ways of imposing a decoding order, tailored for generating spatial arrangements from language. We perform ad-hoc experiments to gain insights in three main research questions: (RQ 1) Can we decode a set of language spatial relations to the 2D space without imposing constraints on the number and type of objects and relationships? (RQ 2) Does the model merely exploit dataset bias to generate arrangements or does it acquire understanding of the language and the spatial domain? (RQ 3) Is the model able to interpret only explicit spatial relationships (e.g., on, above) or can i"
2020.findings-emnlp.408,D18-1149,0,0.0225725,"s the best positioning of the clip-arts that corresponds to the spatial relations implied by the text?”. Our approach is based on a large pre-trained language model – B ERT (Devlin et al., 2018), adapted to jointly process multi-modal data with distinct positional encoding. We introduce SR-B ERT, a model that explicitly focuses on the spatial relations – decoding the language cues to 2D spatial arrangements, achieved by masking the information related to the spatial arrangements during training. We build on the methods of Ghazvininejad et al. (2019a); Kasai et al. (2020); Wang and Cho (2019); Lee et al. (2018), initially proposed for nonautoregressive text decoding, in our case specifically adapted to iteratively mask-out and predict the spatial arrangements of the objects of interest. Inspired by Lawrence et al. (2019) we develop distinct ways of imposing a decoding order, tailored for generating spatial arrangements from language. We perform ad-hoc experiments to gain insights in three main research questions: (RQ 1) Can we decode a set of language spatial relations to the 2D space without imposing constraints on the number and type of objects and relationships? (RQ 2) Does the model merely explo"
2020.findings-emnlp.408,D19-1514,0,0.025896,"split to retain as much information as possible within the train-validation-test splits. We also include an experiment with a random split in appendix D. validation set respectively, and 7989 scenes in the training set. The maximum number of clip-arts in a scene is 17 while the minimum and median are 6. The total number of unique clip-arts in the dataset is 126. 3 Methods The main building block for all our models is B ERT (Devlin et al., 2018). In particular, we present SR-B ERT, a B ERT variant based on a pre-trained B ERT BASE . Compared with existing B ERT architectures (Sun et al., 2019; Tan and Bansal, 2019; Chen et al., 2019; Su et al., 2019; Lu et al., 2019; Li et al., 2019b), with SR-B ERT our contributions are twofold: (i) We alter the input-embedding module to process two discrete modalities with a different positional encoding — sequential and spatial. (ii) We design a novel training method – Masked Position Modelling, where we iteratively mask and predict the positional encoding of the input tokens. 3.1 B ERT revisited In B ERT, the input sequence is tokenized using WordPiece tokenization (Wu et al., 2016) and encoded in token indices {w1 , ..., wN } with a [CLS] token index prepended at"
2020.findings-emnlp.408,W19-2304,0,0.0273799,"ational Linguistics is the best positioning of the clip-arts that corresponds to the spatial relations implied by the text?”. Our approach is based on a large pre-trained language model – B ERT (Devlin et al., 2018), adapted to jointly process multi-modal data with distinct positional encoding. We introduce SR-B ERT, a model that explicitly focuses on the spatial relations – decoding the language cues to 2D spatial arrangements, achieved by masking the information related to the spatial arrangements during training. We build on the methods of Ghazvininejad et al. (2019a); Kasai et al. (2020); Wang and Cho (2019); Lee et al. (2018), initially proposed for nonautoregressive text decoding, in our case specifically adapted to iteratively mask-out and predict the spatial arrangements of the objects of interest. Inspired by Lawrence et al. (2019) we develop distinct ways of imposing a decoding order, tailored for generating spatial arrangements from language. We perform ad-hoc experiments to gain insights in three main research questions: (RQ 1) Can we decode a set of language spatial relations to the 2D space without imposing constraints on the number and type of objects and relationships? (RQ 2) Does the"
2020.semeval-1.274,2020.lrec-1.758,0,0.0872588,"The ‘CLS’ token representation is fed to the classification layer as the sentence representation. 4 Experiments 4.1 Datasets In this section, we present an overview of the datasets used in this article for training our models for the OffensEval-2020 competition. For the English language, we use the large unlabeled dataset provided by the organizers (Rosenthal et al., 2020) to create the weakly labeled dataset. Also, we use the OLID (Zampieri et al., 2019a) dataset for training the English model. For other languages, we utilize the provided labeled datasets by the organizers for Turkish (C¸o¨ ltekin, 2020), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Arabic (Mubarak et al., 2020) languages. The detailed statistics of the datasets are summarized in Table 1. 2 https://translate.google.com/ 2075 Language English Danish Turkish Arabic Greek OFF 300k 307 4837 1371 1989 Train NOT Total 300k 600k 2061 2368 20184 25021 5468 6839 5005 6994 OFF 1080 41 716 402 242 Test NOT 2807 288 2812 1598 1302 Total 3887 329 3528 2000 1544 Table 1: Datasets statistics 4.2 Experimental Settings For the English language, an extensive preprocessing is conducted including emoji to text p"
2020.semeval-1.274,P18-1031,0,0.0258456,"s, word clusters, sentiment analysis outcomes, lexical and linguistic features, knowledge-based features, and multimodal information (Mehdad and Tetreault, 2016; Warner and Hirschberg, 2012; Gitari et al., 2015; Dinakar et al., 2012; Hosseinmardi et al., 2015). The extracted features were used to train machine learning methods like a support vector machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 competition (Zampieri et al., 2019b), among the top-10 teams participated in Subtask A, seven used BERT with variations in the parameter settings and the preprocessing steps (Liu et al., 2019; Nikolov and Radivchev, 2019; Pelicon et al., 2019). 2.2 Multilingual Methods There is a substantial body of work that investigates how to leverage multilingual data to improve the performance of a monolingual model or even to enable zero-shot classification. The XLM model ("
2020.semeval-1.274,W18-4401,0,0.0356075,"e multilingual BERT to obtain sentence representations. LIIR achieved rank 14/38, 18/47, 24/86, 24/54, and 25/40 in Greek, Turkish, English, Arabic, and Danish languages, respectively. 1 Introduction Nowadays, with an exponential increase in the use of social media platforms such as Facebook and Twitter by people from different educational and cultural backgrounds, the need for automatic methods for recognizing and filtering offensive languages is necessary (Chen et al., 2012; Nobata et al., 2016). Different types of offensive content like hate speech (Malmasi and Zampieri, 2018), aggression (Kumar et al., 2018) and cyberbullying (Dinakar et al., 2011) can be very harmful to the user’s mental health, especially to children and youth (Xu and Zhu, 2010). The OffensEval 2019 competition (Zampieri et al., 2019b) was an attempt to build systems capable of recognizing offensive content in social networks for the English language. The OffensEval 2019 organizers defined three Subtasks: whether a message is offensive or not (Subtask A), what is the type of the offensive message (Subtask B), and who is the target of the offensive message (Subtask C). This year, they have extended the competition to several lan"
2020.semeval-1.274,S19-2011,0,0.0318024,"ke a support vector machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 competition (Zampieri et al., 2019b), among the top-10 teams participated in Subtask A, seven used BERT with variations in the parameter settings and the preprocessing steps (Liu et al., 2019; Nikolov and Radivchev, 2019; Pelicon et al., 2019). 2.2 Multilingual Methods There is a substantial body of work that investigates how to leverage multilingual data to improve the performance of a monolingual model or even to enable zero-shot classification. The XLM model (Lample and Conneau, 2019) extended BERT to a cross-lingual setting in which instead of monolingual text, it used concatenated parallel sentences in the pretraining procedure. This method achieved strong results in machine translation, language modeling, and cross-lingual Natural Language Inference. XLDA (Singh et al., 2019"
2020.semeval-1.274,W16-3638,0,0.019864,"ws related works. Section 3 describes the methodology of our proposed models. We will discuss experiments in Section 4 and the results are presented in Section 5. Finally, the last section contains the conclusion of our work. 2 2.1 Related Work Offensive Language Identification Earlier works for addressing Offensive Language Identification relied on manually extracting the different types of features (Schmidt and Wiegand, 2017) such as token and character n-grams, word clusters, sentiment analysis outcomes, lexical and linguistic features, knowledge-based features, and multimodal information (Mehdad and Tetreault, 2016; Warner and Hirschberg, 2012; Gitari et al., 2015; Dinakar et al., 2012; Hosseinmardi et al., 2015). The extracted features were used to train machine learning methods like a support vector machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 comp"
2020.semeval-1.274,S19-2123,0,0.0129478,"r machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 competition (Zampieri et al., 2019b), among the top-10 teams participated in Subtask A, seven used BERT with variations in the parameter settings and the preprocessing steps (Liu et al., 2019; Nikolov and Radivchev, 2019; Pelicon et al., 2019). 2.2 Multilingual Methods There is a substantial body of work that investigates how to leverage multilingual data to improve the performance of a monolingual model or even to enable zero-shot classification. The XLM model (Lample and Conneau, 2019) extended BERT to a cross-lingual setting in which instead of monolingual text, it used concatenated parallel sentences in the pretraining procedure. This method achieved strong results in machine translation, language modeling, and cross-lingual Natural Language Inference. XLDA (Singh et al., 2019) is a cross-lingual data aug"
2020.semeval-1.274,S19-2108,0,0.0151587,"logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 competition (Zampieri et al., 2019b), among the top-10 teams participated in Subtask A, seven used BERT with variations in the parameter settings and the preprocessing steps (Liu et al., 2019; Nikolov and Radivchev, 2019; Pelicon et al., 2019). 2.2 Multilingual Methods There is a substantial body of work that investigates how to leverage multilingual data to improve the performance of a monolingual model or even to enable zero-shot classification. The XLM model (Lample and Conneau, 2019) extended BERT to a cross-lingual setting in which instead of monolingual text, it used concatenated parallel sentences in the pretraining procedure. This method achieved strong results in machine translation, language modeling, and cross-lingual Natural Language Inference. XLDA (Singh et al., 2019) is a cross-lingual data augmentation method that s"
2020.semeval-1.274,2020.lrec-1.629,0,0.0522748,"as the sentence representation. 4 Experiments 4.1 Datasets In this section, we present an overview of the datasets used in this article for training our models for the OffensEval-2020 competition. For the English language, we use the large unlabeled dataset provided by the organizers (Rosenthal et al., 2020) to create the weakly labeled dataset. Also, we use the OLID (Zampieri et al., 2019a) dataset for training the English model. For other languages, we utilize the provided labeled datasets by the organizers for Turkish (C¸o¨ ltekin, 2020), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Arabic (Mubarak et al., 2020) languages. The detailed statistics of the datasets are summarized in Table 1. 2 https://translate.google.com/ 2075 Language English Danish Turkish Arabic Greek OFF 300k 307 4837 1371 1989 Train NOT Total 300k 600k 2061 2368 20184 25021 5468 6839 5005 6994 OFF 1080 41 716 402 242 Test NOT 2807 288 2812 1598 1302 Total 3887 329 3528 2000 1544 Table 1: Datasets statistics 4.2 Experimental Settings For the English language, an extensive preprocessing is conducted including emoji to text projection3 , hashtag segmentation4 , replacing slang and abbreviations (Eff"
2020.semeval-1.274,W17-1101,0,0.0143547,"English track. Also, empirical results show that our cross-lingual augmentation approach is effective in improving results. The rest of the article is organized as follows. The next section reviews related works. Section 3 describes the methodology of our proposed models. We will discuss experiments in Section 4 and the results are presented in Section 5. Finally, the last section contains the conclusion of our work. 2 2.1 Related Work Offensive Language Identification Earlier works for addressing Offensive Language Identification relied on manually extracting the different types of features (Schmidt and Wiegand, 2017) such as token and character n-grams, word clusters, sentiment analysis outcomes, lexical and linguistic features, knowledge-based features, and multimodal information (Mehdad and Tetreault, 2016; Warner and Hirschberg, 2012; Gitari et al., 2015; Dinakar et al., 2012; Hosseinmardi et al., 2015). The extracted features were used to train machine learning methods like a support vector machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT ("
2020.semeval-1.274,2020.lrec-1.430,0,0.0708134,"entation is fed to the classification layer as the sentence representation. 4 Experiments 4.1 Datasets In this section, we present an overview of the datasets used in this article for training our models for the OffensEval-2020 competition. For the English language, we use the large unlabeled dataset provided by the organizers (Rosenthal et al., 2020) to create the weakly labeled dataset. Also, we use the OLID (Zampieri et al., 2019a) dataset for training the English model. For other languages, we utilize the provided labeled datasets by the organizers for Turkish (C¸o¨ ltekin, 2020), Danish (Sigurbergsson and Derczynski, 2020), Greek (Pitenis et al., 2020), and Arabic (Mubarak et al., 2020) languages. The detailed statistics of the datasets are summarized in Table 1. 2 https://translate.google.com/ 2075 Language English Danish Turkish Arabic Greek OFF 300k 307 4837 1371 1989 Train NOT Total 300k 600k 2061 2368 20184 25021 5468 6839 5005 6994 OFF 1080 41 716 402 242 Test NOT 2807 288 2812 1598 1302 Total 3887 329 3528 2000 1544 Table 1: Datasets statistics 4.2 Experimental Settings For the English language, an extensive preprocessing is conducted including emoji to text projection3 , hashtag segmentation4 , replacin"
2020.semeval-1.274,W12-2103,0,0.0506223,"describes the methodology of our proposed models. We will discuss experiments in Section 4 and the results are presented in Section 5. Finally, the last section contains the conclusion of our work. 2 2.1 Related Work Offensive Language Identification Earlier works for addressing Offensive Language Identification relied on manually extracting the different types of features (Schmidt and Wiegand, 2017) such as token and character n-grams, word clusters, sentiment analysis outcomes, lexical and linguistic features, knowledge-based features, and multimodal information (Mehdad and Tetreault, 2016; Warner and Hirschberg, 2012; Gitari et al., 2015; Dinakar et al., 2012; Hosseinmardi et al., 2015). The extracted features were used to train machine learning methods like a support vector machine (SVM), naive Bayes, logistic regression, random forest classifier, or a neural network. With the success of transfer learning enabled by pre-trained language models such as BERT (Devlin et al., 2018), GPT (Radford et al., 2018), and ULMFiT (Howard and Ruder, 2018), researchers have resorted to using these methods for addressing the Offensive Language Identification task. In the OffensEval 2019 competition (Zampieri et al., 201"
2020.semeval-1.274,N19-1144,0,0.234849,"ion Nowadays, with an exponential increase in the use of social media platforms such as Facebook and Twitter by people from different educational and cultural backgrounds, the need for automatic methods for recognizing and filtering offensive languages is necessary (Chen et al., 2012; Nobata et al., 2016). Different types of offensive content like hate speech (Malmasi and Zampieri, 2018), aggression (Kumar et al., 2018) and cyberbullying (Dinakar et al., 2011) can be very harmful to the user’s mental health, especially to children and youth (Xu and Zhu, 2010). The OffensEval 2019 competition (Zampieri et al., 2019b) was an attempt to build systems capable of recognizing offensive content in social networks for the English language. The OffensEval 2019 organizers defined three Subtasks: whether a message is offensive or not (Subtask A), what is the type of the offensive message (Subtask B), and who is the target of the offensive message (Subtask C). This year, they have extended the competition to several languages while the Subtasks remain the same as in OffensEval 2019. OffensEval 2020 (Zampieri et al., 2020) features a multilingual dataset with five languages including English, Danish, Turkish, Greek"
2020.semeval-1.274,S19-2010,0,0.0567676,"Missing"
2021.conll-1.27,D18-1307,0,0.346969,"h vastly outperforms the baseline classifier. A tSNE (Van der Maaten and Hinton, 2008) analysis illustrates that meaningful relation-related clusters can be identified in the learned embedding space. This provides a second strong indication that structure can be effectively imposed on LM embeddings using our proposed framework. Even if the main focus of this work is not solving the IE problem directly, to further explore the capabilities of the relation-aware representation space, we train a simple KNN classifier for RE that is competitive with state-of-the-art performance. Strict evaluation (Bekoulis et al., 2018b; Taillé et al., 2020) of the RE task presupposes correct detection of the boundaries and the entity type of each argument in the relation. Hence, we apply the CL paradigm to learn a distinct embedding space for the entities and use a KNN classifier to solve the NER task. Finally, we perform a strict evaluation of the complete entity-relation extraction task. This transparent, computationally inexpensive and intuitively simple approach has comparable results to the state-of-the-art models. This achievement illustrates how informative and meaningful the learned embedding spaces are. In summary"
2021.conll-1.27,P06-1060,0,0.0151082,"ere relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text. However, their ability to encapsulate ple baseline neural network classifier for RE, us337 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 337–348 November 10–11, 2021. ©"
2021.conll-1.27,D10-1033,0,0.0298654,"Missing"
2021.conll-1.27,N07-1015,0,0.0913009,"BERT text encoder. Different graph formulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach,"
2021.conll-1.27,2021.ccl-1.108,0,0.0383346,"Missing"
2021.conll-1.27,P16-1105,0,0.0554233,"Missing"
2021.conll-1.27,N18-1202,0,0.0493942,"e entities. Adverse effects (AEs) cover a range of signs, symptoms, diseases, disorders, abnormalities, organ damage and even death caused by that drug. The corpus is annotated at the sentence-level, so non-local relations (between entities of different sentences) do not exist. 2.1 Data Preprocessing The input of the main CL framework consists of the encoded padded sentence and the relation graph, which is extracted from the sentence. The graphs are used only in the training setup. To prepare the input for CharacterBERT, tokenization is applied to each sentence using the character-CNN module (Peters et al., 2018). The BERT tokenizer handles out-of-vocabulary (OOV) words by splitting these words into word pieces. However, the existence of word pieces can be an obstacle in creating and testing the CL experiments of this study from the implementation point of view. Additionally, word pieces may add biases to the model (El Boukkouri et al., 2020), especially in biomedical text where most of the drugs and many adverse effects are OOV words. Hence, CharacterBERT is chosen instead of BERT. For each sentence, a knowledge graph is ob• We propose a novel CL framework for impostained to model the relations betwe"
2021.conll-1.27,P13-1147,0,0.021336,"ulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text. However, their ability to e"
2021.conll-1.27,W09-1119,0,0.0155515,"information in the graph effectively (Kipf and Welling, 2016) and is described by the following equations: Ahat = A + I, (1) Anorm = D−0.5 ∗ Ahat ∗ D−0.5 , (2) where A is the initial adjacency matrix, I is the identity matrix and D is the degree matrix. Initially, the whole corpus is stored in one text file. Hence, the data should be transformed and stored using a different more flexible format. For each sentence of the dataset, a distinct JSON file is created and contains a list with the tokens 1 , a list with named entity (NE) tags adopting the BIO encoding scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009), a list with token index pairs that are members of an existing relation, the padded encoded version of the sentence, the attention mask vector of the sentence, a list with the embeddings of each node of the graph and the normalized adjacency matrix. 2.2 Dataset Statistics The ADE dataset is not officially split into training, validation, and test sets. Hence, we evaluate our models using 10-fold cross-validation similar to Li et al. (2017). We use the same splits as Eberts and 1 The sentence tokenization is performed using the SpaCy library. Ulges (2020). As Taillé et al. (2020) stresses, man"
2021.conll-1.27,E99-1023,0,0.501756,"ating and propagating the information in the graph effectively (Kipf and Welling, 2016) and is described by the following equations: Ahat = A + I, (1) Anorm = D−0.5 ∗ Ahat ∗ D−0.5 , (2) where A is the initial adjacency matrix, I is the identity matrix and D is the degree matrix. Initially, the whole corpus is stored in one text file. Hence, the data should be transformed and stored using a different more flexible format. For each sentence of the dataset, a distinct JSON file is created and contains a list with the tokens 1 , a list with named entity (NE) tags adopting the BIO encoding scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009), a list with token index pairs that are members of an existing relation, the padded encoded version of the sentence, the attention mask vector of the sentence, a list with the embeddings of each node of the graph and the normalized adjacency matrix. 2.2 Dataset Statistics The ADE dataset is not officially split into training, validation, and test sets. Hence, we evaluate our models using 10-fold cross-validation similar to Li et al. (2017). We use the same splits as Eberts and 1 The sentence tokenization is performed using the SpaCy library. Ulges (2020). As Taillé et"
2021.conll-1.27,P11-1053,0,0.0384971,"fferent graph formulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text."
2021.conll-1.27,2020.emnlp-main.133,0,0.0865003,"xtract very good representations for the NER task. Figure 8: tSNE plot - Entity representation space obtained with CLNER model 6 Entity-Relation task considered correct if its type and the two entities (boundaries and type) involved in the relation are correctly predicted. We measure precision, recall and F1 score. Following previous work on IE, we report the macro-averaged F1 score, and as 10-fold cross-validation is adopted, we average the scores over the folds. Model Li et al., 2016 Li et al., 2017 Bekoulis et al., 2018b Bekoulis et al., 2018a Tran and Kavuluru, 2019 Eberts and Ulges, 2020 Wang and Lu, 2020 Zhao et al., 2020 Ours NER 79.5 84.6 86.4 86.73 87.11 89.25 89.7 89.4 88.3 RE 63.4 71.4 74.58 75.52 77.29 79.24 80.1 81.14 79.97 RE86.5 Table 4: Test set results: macro-averaged F1 score The insights of the tSNE analysis, with the wellTable 4 presents the best performing models, defined clusters in the embedding spaces, lead us to evaluated on the ADE (Gurulingappa et al., 2012) approach the entity-relation task using intuitively dataset. These studies address the IE problem as simple and transparent KNN classifiers. For the a joint task, solving NER and RE tasks jointly. Li RE task, we utili"
2021.eacl-main.290,N19-1423,0,0.0195499,"ani et al., 2017) encoder as our baseline model. The computation within a single layer transformer encoder is presented in appendix A.1, and the details of the input and training objective functions are illustrated in below subsections. The main idea of applying the transformer architecture to the visual dialog task is to use the multi-head self-attention mechanism to implicitly learn the intra and inter interactions within the single modality and between the different modalities (in this case language and vision), respectively. Linguistic Representation. Following the monolingual BERT model (Devlin et al., 2019) and the multi-modal BERT model (Lu et al., 2019; Li et al., 2019; Su et al., 2020), we use the WordPiece (Wu et al., 2016) tokenization tool to tokenize each input sequence into word pieces sequence. Then the sum of the word piece embedding, position embedding and segment embedding, where the segment embedding is used to differentiate questions from answers and to delimit boundaries of questionanswer pairs, are taken as the language sequence input of the model. Image Representation. Following the multimodal BERT model (Anderson et al., 2018a; Lu et al., 2019; Li et al., 2019; Su et al., 2020)"
2021.eacl-main.290,D15-1271,1,0.840828,"text of the discourse. The task has been dominated by machine learning approaches since the first learning based coreference resolution system was proposed by Connolly et al. (1997). Before Lee et al. (2017) proposed the first end-to-end neural network based coreference resolution system, most of the learningbased systems have been built with hand engineered linguistic features. Durrett and Klein (2013) use surface linguistic features, such as mention type, the semantic head of a mention, etc., and their combinations to build a classifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constraints in the inference step in order to adapt an existing coreference system trained on the newswire domain to short narrative stories without any retraining. Recently, Joshi et al. (2019) apply a BERT model to coreference resolution and achieve promising results on the OntoNotes corpus (Pradhan et al., 2012) and the GAP dataset (Webster et al., 2018). Different from all coreference systems mentioned above,"
2021.eacl-main.290,D13-1203,0,0.0342778,"al. (2017) use adversarial learning and Yang et al. (2019) apply reinforcement learning. Coreference Resolution. Coreference resolution aims at detecting linguistic expressions referring to the same entities in the context of the discourse. The task has been dominated by machine learning approaches since the first learning based coreference resolution system was proposed by Connolly et al. (1997). Before Lee et al. (2017) proposed the first end-to-end neural network based coreference resolution system, most of the learningbased systems have been built with hand engineered linguistic features. Durrett and Klein (2013) use surface linguistic features, such as mention type, the semantic head of a mention, etc., and their combinations to build a classifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constraints in the inference step in order to adapt an existing coreference system trained on the newswire domain to short narrative stories without any retraining. Recently, Joshi et al. (2019) apply a BER"
2021.eacl-main.290,P19-1648,0,0.0412004,"Missing"
2021.eacl-main.290,D19-1588,0,0.012865,"tures. Durrett and Klein (2013) use surface linguistic features, such as mention type, the semantic head of a mention, etc., and their combinations to build a classifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constraints in the inference step in order to adapt an existing coreference system trained on the newswire domain to short narrative stories without any retraining. Recently, Joshi et al. (2019) apply a BERT model to coreference resolution and achieve promising results on the OntoNotes corpus (Pradhan et al., 2012) and the GAP dataset (Webster et al., 2018). Different from all coreference systems mentioned above, which rely on supervised learning and on a dataset annotated with coreference links, our work focuses on applying soft linguistic constraints to improve the model’s ability of resolving coreferents in an implicit and unsupervised way. Similar to the work of Venkitasubramanian et al. (2017) that operates on language and vision information, our model uses attention to jointly"
2021.eacl-main.290,D19-1209,0,0.0163949,"to its antecedent. 2 Related Work Visual Dialog. The Visual Dialog task is proposed by Das et al. (2017), where a dialog agent has to answer questions grounded in an image based on its understanding of the dialog history and the image. Most of previous work focuses on using an attention mechanism to learn interactions between image, dialog history and question. Gan et al. (2019) use an attention network to conduct multi-step reasoning in order to answer a question. Niu et al. (2019) propose a recursive attention network, which selects relevant information from the dialog history recursively. Kang et al. (2019) apply a multi-head attention mechanism (Vaswani et al., 2017) to learn mutimodal representations. Schwartz et al. (2019) fuse information from all entities including question, answer, dialog history, caption and image using a factor graph. Murahari et al. (2019) propose two-stage training. They first pretrain their transformer based two-stream attention network on other visual-language datasets, then finetune it on the visual dialog dataset. Other approaches consider different learning methodologies to model the visual dialog task, for example, Lu et al. (2017) use adversarial learning and Ya"
2021.eacl-main.290,D17-1018,0,0.020505,"n other visual-language datasets, then finetune it on the visual dialog dataset. Other approaches consider different learning methodologies to model the visual dialog task, for example, Lu et al. (2017) use adversarial learning and Yang et al. (2019) apply reinforcement learning. Coreference Resolution. Coreference resolution aims at detecting linguistic expressions referring to the same entities in the context of the discourse. The task has been dominated by machine learning approaches since the first learning based coreference resolution system was proposed by Connolly et al. (1997). Before Lee et al. (2017) proposed the first end-to-end neural network based coreference resolution system, most of the learningbased systems have been built with hand engineered linguistic features. Durrett and Klein (2013) use surface linguistic features, such as mention type, the semantic head of a mention, etc., and their combinations to build a classifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constra"
2021.eacl-main.290,P14-5010,0,0.00523362,"Missing"
2021.eacl-main.290,J93-2004,0,0.0740704,"(8) i=0 2i LP OS = −E(w,I)∼D (Pnon−pronoun + Ppronoun ) Pnon−pronoun = −logP (P OS(w)|wm , I) Ppronoun = logP (N N |wm , I)) (5) where wm denote all unmasked words, and D is the dataset. This soft constraint will make pronouns focus more on nouns instead of other words such as verb, adverb or adjective, etc. As it does not violate any linguistic rules, it will not introduce a bias to the language model. Nearest Preference Constraint. Our second soft constraint is inspired by the observation that in human dialog a pronoun is more likely to refer 2 We use the POS tags used in Penn Treebank (Marcus et al., 1993). where wi = 1/(M +  d ), and ∆pos denotes the distance between two positions.  is a parameter, which makes the wavelength of the sinusoidal function in each dimension to form a geometrical progression.3 Since ∆pos ∈ [−M, M ], wi ∆pos ∈ [−1, 1], it follows that cos(wi ∆pos) == cos(wi |∆pos|) which is monotonically decreasing in the region of [0, 1].The details of the derivation of equation 8 are presented in appendix A.2. The closer two sentences are, the larger of the product of their sentence position embedding, resulting in stronger local interactions between nearby sentences in the dialo"
2021.eacl-main.290,Q18-1042,0,0.0160665,"sifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constraints in the inference step in order to adapt an existing coreference system trained on the newswire domain to short narrative stories without any retraining. Recently, Joshi et al. (2019) apply a BERT model to coreference resolution and achieve promising results on the OntoNotes corpus (Pradhan et al., 2012) and the GAP dataset (Webster et al., 2018). Different from all coreference systems mentioned above, which rely on supervised learning and on a dataset annotated with coreference links, our work focuses on applying soft linguistic constraints to improve the model’s ability of resolving coreferents in an implicit and unsupervised way. Similar to the work of Venkitasubramanian et al. (2017) that operates on language and vision information, our model uses attention to jointly learn multi-modal representations. 3 Methodology In this section, we formally describe the visual dialog task (Das et al., 2017) and the approaches we propose. In vi"
2021.eacl-main.290,W12-4501,0,0.041696,"tc., and their combinations to build a classifier to determine if two mentions refer to the same entity. Do et al. (2015) adopt integer linear programming (ILP) to introduce coreference constraints including centering theory constraints, direct speech constraints and definite noun phrase and exact match constraints in the inference step in order to adapt an existing coreference system trained on the newswire domain to short narrative stories without any retraining. Recently, Joshi et al. (2019) apply a BERT model to coreference resolution and achieve promising results on the OntoNotes corpus (Pradhan et al., 2012) and the GAP dataset (Webster et al., 2018). Different from all coreference systems mentioned above, which rely on supervised learning and on a dataset annotated with coreference links, our work focuses on applying soft linguistic constraints to improve the model’s ability of resolving coreferents in an implicit and unsupervised way. Similar to the work of Venkitasubramanian et al. (2017) that operates on language and vision information, our model uses attention to jointly learn multi-modal representations. 3 Methodology In this section, we formally describe the visual dialog task (Das et al.,"
2021.emnlp-main.240,2021.findings-acl.263,1,0.722619,"periments with PredALBERT-B as the Default model. alent to modifying equation 1 by gar (ztl , clt−1 ). We found that this ablation severely affects the performance of the Default model performance by ∼2.89 in DiscoEval and ∼4.43 in SciDTB-DE. Our findings indicate that top-down pathway is beneficial for improving discourse representations of BERT-type models. However, it is not clear in which layers it is crucial to have the PC mechanism. We hypothesize that this is related to the fact that the BERT-style models encode syntactic and semantic features in different layers (Jawahar et al., 2019; Aspillaga et al., 2021), so a specialized PC mechanism for syntax or semantics would be desirable. We left this study for future work. 7 What Does The Model Learn? Because our model excels at detecting discourse relations, in this section, we explore whether the resulting vectors actually represent the role of a sentence in its discursive context. To illustrate what PredBERT learns, we follow the methodology proposed by Lee et al. (2020). We use labeled sentences with discourse relations as queries to retrieve the top 3 most similar sentences from an unlabeled corpus using cosine similarity. We obtained the queries"
2021.emnlp-main.240,D19-1060,0,0.049599,"Missing"
2021.emnlp-main.240,D14-1179,0,0.0128408,"Missing"
2021.emnlp-main.240,L18-1269,0,0.0298347,"2020) Base and Large, which are directly comparable to our model. For a fair and consistent comparison, we rerun all baseline evaluations. We use the pre-trained Huggingface models (Wolf et al., 2020) for BERT and ALBERT. In the case of CONPONO, we use a version pre-trained to predict 2 next surrounding sentences1 . Evaluation: In the case of DiscoEval, we use the original code provided by Chen et al. (2019). We observe that this configuration leads to CONPONO model results that differ from the reported on the original paper. On the other hand, following Huber et al. (2020), we use SentEval (Conneau and Kiela, 2018) toolkit for SciDTB-DE evaluation. In both cases, the process involves loading a pre-trained model with frozen weights and training a logistic regression on top of the sentence embeddings. To train, we use the average of sentence representations ([CLS]) from all the layers. B by ∼3.92, PredALBERT-L by ∼7.43, and PredBERT-B by ∼1.46 points on average. The Stuart–Maxwell tests demonstrated a significant difference between our best model PredALBERT-L and ALBERT-L (p = 0.009) or CONPONO (p = 0.05). We also highlight that PredALBERT-B/L achieves competitive performance with fewer parameters than BE"
2021.emnlp-main.240,N19-1423,0,0.194188,"model achieves competitive performance compared to baselines, especially in tasks that require to discover discourse-level relations. Pre-trained language models are among the leading 2 Related Work methods to learn useful representations for textual data. Several pre-training objectives have been pro- 2.1 BERT for Sentence Representation posed in recent years, such as causal language mod- Pre-trained self-supervised language models have become popular in recent years. BERT (Devlin eling (Radford et al., 2018, 2019), masked language et al., 2019) adopts a transformer encoder using a modeling (Devlin et al., 2019), and permutation language modeling (Yang et al., 2019). However, masked language modeling (MLM) objective for these approaches do not produce suitable represen- word representation. It also proposes an additional loss called next-sentence prediction (NSP) to train tations at the discourse level (Huber et al., 2020). Simultaneously, neuroscience studies have sug- a model that understands sentence relationships. gested that predictive coding (PC) plays an es- On the other hand, ALBERT (Lan et al., 2020) proposes a loss based primarily on coherence called sential role in language development in"
2021.emnlp-main.240,2020.findings-emnlp.196,0,0.0704048,"Missing"
2021.emnlp-main.240,2020.codi-1.9,0,0.33281,"type models with recursive bottom-up and top-down computation based on PC theory. Specifically, we incorporate top-down connections that, according to PC, convey predictions from upper to lower layers, which are contrasted with bottom-up representations to generate an error signal that is used to guide the optimization of the model. Using this approach, we attempt to build feature representations that capture discourse-level relationships by continually predicting future sentences in a latent space. We evaluate our approach on DiscoEval (Chen et al., 2019) and SciDTB for discourse evaluation (Huber et al., 2020) to assess whether the embeddings produced by our model capture discourse properties of sentences without finetuning. Our model achieves competitive performance compared to baselines, especially in tasks that require to discover discourse-level relations. Pre-trained language models are among the leading 2 Related Work methods to learn useful representations for textual data. Several pre-training objectives have been pro- 2.1 BERT for Sentence Representation posed in recent years, such as causal language mod- Pre-trained self-supervised language models have become popular in recent years. BERT"
2021.emnlp-main.240,2020.acl-main.439,0,0.0798848,"g local predictions at each level using top-down connections. CPC (Oord et al., 2018) is an unsupervised learning approach to extract useful representations by predicting text in a latent space. Our method takes inspiration from these models, considering top-down connections and predictive processing in a latent space. T[CLS] ..... tence pair tasks, so they are not comparable with our which intended to be general-purpose. More recently, SLM (Lee et al., 2020) proposes a sentence unshuffling approach for a fine understanding of the relations among the sentences at the discourse level. CONPONO (Iter et al., 2020) considers a discourse-level objective to predict the surrounding sentences given an anchor text. This work is related to our approach; the key difference is that our model predicts future sentences sequentially using a top-down pathway. We consider CONPONO as our main baseline. (2) In the spirit of Seq2Seq (Sutskever et al., 2014), representations are predicted sequentially, which differs from the CONPONO model that predicts k future sentences with a unique context vector. 3.2 Loss Function We rely on the InfoNCE loss proposed for the CPC model (Oord et al., 2018). This constructs a binary ta"
2021.emnlp-main.240,P19-1356,0,0.0189486,"Results of ablation experiments with PredALBERT-B as the Default model. alent to modifying equation 1 by gar (ztl , clt−1 ). We found that this ablation severely affects the performance of the Default model performance by ∼2.89 in DiscoEval and ∼4.43 in SciDTB-DE. Our findings indicate that top-down pathway is beneficial for improving discourse representations of BERT-type models. However, it is not clear in which layers it is crucial to have the PC mechanism. We hypothesize that this is related to the fact that the BERT-style models encode syntactic and semantic features in different layers (Jawahar et al., 2019; Aspillaga et al., 2021), so a specialized PC mechanism for syntax or semantics would be desirable. We left this study for future work. 7 What Does The Model Learn? Because our model excels at detecting discourse relations, in this section, we explore whether the resulting vectors actually represent the role of a sentence in its discursive context. To illustrate what PredBERT learns, we follow the methodology proposed by Lee et al. (2020). We use labeled sentences with discourse relations as queries to retrieve the top 3 most similar sentences from an unlabeled corpus using cosine similarity."
2021.emnlp-main.240,E14-3011,0,0.0194183,"ork. 7 What Does The Model Learn? Because our model excels at detecting discourse relations, in this section, we explore whether the resulting vectors actually represent the role of a sentence in its discursive context. To illustrate what PredBERT learns, we follow the methodology proposed by Lee et al. (2020). We use labeled sentences with discourse relations as queries to retrieve the top 3 most similar sentences from an unlabeled corpus using cosine similarity. We obtained the queries from the MIT Discourse Relations Annotation Guide2 and the unlabeled sentences from the Gutenberg dataset (Lahiri, 2014). We compute the representations as mentioned in Section 4.2. This process allowed us to verify that similar vectors share the same or equivalent discourse relations. Temporal relation: Query = He knows a tasty meal when he eats one. 1. The last five words took Tuppence’s fancy mightily, especially after a meagre breakfast and a supper of buns the night before. 2. I know a disinterested man when I see him. 3. He had about ten pounds when I found him. Sentence 1 has a succession relation due to the use of the word after. Sentence 3 shows a synchrony relation because it uses when as the query. S"
2021.emnlp-main.240,2020.emnlp-main.120,0,0.114644,"uang et al., 2020) image classification. PredNet (Lotter et al., 2017) proposes a network capable of predicting future frames in a video sequence by making local predictions at each level using top-down connections. CPC (Oord et al., 2018) is an unsupervised learning approach to extract useful representations by predicting text in a latent space. Our method takes inspiration from these models, considering top-down connections and predictive processing in a latent space. T[CLS] ..... tence pair tasks, so they are not comparable with our which intended to be general-purpose. More recently, SLM (Lee et al., 2020) proposes a sentence unshuffling approach for a fine understanding of the relations among the sentences at the discourse level. CONPONO (Iter et al., 2020) considers a discourse-level objective to predict the surrounding sentences given an anchor text. This work is related to our approach; the key difference is that our model predicts future sentences sequentially using a top-down pathway. We consider CONPONO as our main baseline. (2) In the spirit of Seq2Seq (Sutskever et al., 2014), representations are predicted sequentially, which differs from the CONPONO model that predicts k future senten"
2021.emnlp-main.240,D19-1410,0,0.0236211,"able represen- word representation. It also proposes an additional loss called next-sentence prediction (NSP) to train tations at the discourse level (Huber et al., 2020). Simultaneously, neuroscience studies have sug- a model that understands sentence relationships. gested that predictive coding (PC) plays an es- On the other hand, ALBERT (Lan et al., 2020) proposes a loss based primarily on coherence called sential role in language development in humans (Ylinen et al., 2016; Zettersten, 2019). PC postu- sentence-order prediction (SOP). lates that the brain is continually making predicSBERT (Reimers and Gurevych, 2019) uses a tions of incoming sensory stimuli (Rao and Ballard, siamese structure to obtain semantically meaning1999; Friston, 2005; Clark, 2013; Hohwy, 2013), ful sentence embeddings, focusing on textual simiwith word prediction being the main mechanism larity tasks. ConveRT (Henderson et al., 2020) uses (Berkum et al., 2005; Kuperberg and Jaeger, 2015). a dual-encoder to improve sentence embeddings for However, recent studies speculate that the predic- response selection tasks. These models focus on tive process could occur within and across utter- obtaining better representations for specialize"
2021.semeval-1.139,P18-1058,0,0.0137939,"Back-Translation If Trump is not Hitler, Then I'm an idiot CLIP Text Encoder Chained Classifier Estimated Probabilities CLIP Image Encoder Figure 1: The overall architecture of our proposed model. For each example, use Back-Translation to derive augmentations of the text, and we compute persuasion techniques probabilities separately. Then, we average the estimated probabilities from augmented and original examples. including offensive language detection (Pradhan et al., 2020; Ghadery and Moens, 2020) emotion analysis (Dolan, 2002), computational study of persuasiveness (Guerini et al., 2008; Carlile et al., 2018) and argumentation (Palau and Moens, 2009; Habernal and Gurevych, 2016). 3 Methodology In this section, we introduce the design of our proposed method. The overall architecture of our method is depicted in figure 1. Our model consists of several components: a data augmentation component (Back-translation), a feature extraction component(CLIP), and a chained classifier. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challenges in this subtask is the low number of training data where the organizers have provided just 200 training samples."
2021.semeval-1.139,2021.semeval-1.7,0,0.0473369,"Missing"
2021.semeval-1.139,2020.semeval-1.274,1,0.708009,"nd (online), August 5–6, 2021. ©2021 Association for Computational Linguistics If Trump Isn't Hitler, Then I'm a Moron Back-Translation If Trump is not Hitler, Then I'm an idiot CLIP Text Encoder Chained Classifier Estimated Probabilities CLIP Image Encoder Figure 1: The overall architecture of our proposed model. For each example, use Back-Translation to derive augmentations of the text, and we compute persuasion techniques probabilities separately. Then, we average the estimated probabilities from augmented and original examples. including offensive language detection (Pradhan et al., 2020; Ghadery and Moens, 2020) emotion analysis (Dolan, 2002), computational study of persuasiveness (Guerini et al., 2008; Carlile et al., 2018) and argumentation (Palau and Moens, 2009; Habernal and Gurevych, 2016). 3 Methodology In this section, we introduce the design of our proposed method. The overall architecture of our method is depicted in figure 1. Our model consists of several components: a data augmentation component (Back-translation), a feature extraction component(CLIP), and a chained classifier. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challeng"
2021.semeval-1.139,guerini-etal-2008-resources,0,0.0566981,"tler, Then I'm a Moron Back-Translation If Trump is not Hitler, Then I'm an idiot CLIP Text Encoder Chained Classifier Estimated Probabilities CLIP Image Encoder Figure 1: The overall architecture of our proposed model. For each example, use Back-Translation to derive augmentations of the text, and we compute persuasion techniques probabilities separately. Then, we average the estimated probabilities from augmented and original examples. including offensive language detection (Pradhan et al., 2020; Ghadery and Moens, 2020) emotion analysis (Dolan, 2002), computational study of persuasiveness (Guerini et al., 2008; Carlile et al., 2018) and argumentation (Palau and Moens, 2009; Habernal and Gurevych, 2016). 3 Methodology In this section, we introduce the design of our proposed method. The overall architecture of our method is depicted in figure 1. Our model consists of several components: a data augmentation component (Back-translation), a feature extraction component(CLIP), and a chained classifier. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challenges in this subtask is the low number of training data where the organizers have provided jus"
2021.semeval-1.139,D16-1129,0,0.0246951,"Text Encoder Chained Classifier Estimated Probabilities CLIP Image Encoder Figure 1: The overall architecture of our proposed model. For each example, use Back-Translation to derive augmentations of the text, and we compute persuasion techniques probabilities separately. Then, we average the estimated probabilities from augmented and original examples. including offensive language detection (Pradhan et al., 2020; Ghadery and Moens, 2020) emotion analysis (Dolan, 2002), computational study of persuasiveness (Guerini et al., 2008; Carlile et al., 2018) and argumentation (Palau and Moens, 2009; Habernal and Gurevych, 2016). 3 Methodology In this section, we introduce the design of our proposed method. The overall architecture of our method is depicted in figure 1. Our model consists of several components: a data augmentation component (Back-translation), a feature extraction component(CLIP), and a chained classifier. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challenges in this subtask is the low number of training data where the organizers have provided just 200 training samples. To enrich the training set we propose to use the back-translation tech"
2021.semeval-1.139,2020.semeval-1.186,0,0.0916355,"Missing"
2021.semeval-1.139,P16-1009,0,0.0309923,"hodology In this section, we introduce the design of our proposed method. The overall architecture of our method is depicted in figure 1. Our model consists of several components: a data augmentation component (Back-translation), a feature extraction component(CLIP), and a chained classifier. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challenges in this subtask is the low number of training data where the organizers have provided just 200 training samples. To enrich the training set we propose to use the back-translation technique (Sennrich et al., 2016) for paraphrasing a given sentence by translating it to a specific target language and translating back to the original language. To this end, we use four translation models, English-to-German, German-to-English, Englishto-Russian, and Russian-to-English provided by (Ng et al., 2019). Therefore, for each training sentence, we obtain two paraphrased version of it. In the test time, we average the probability distributions over the original and paraphrased sentenceimage pairs. 3.2 Feature Extraction Our system isProbabilities of a combination of pretrained visuolinguistic and linguistic models."
2021.semeval-1.139,S19-1004,1,0.889706,"Missing"
2021.semeval-1.139,W19-5333,0,0.0280587,"er. Details of each component are described in the following subsections. 3.1 Augmentation Method One of the challenges in this subtask is the low number of training data where the organizers have provided just 200 training samples. To enrich the training set we propose to use the back-translation technique (Sennrich et al., 2016) for paraphrasing a given sentence by translating it to a specific target language and translating back to the original language. To this end, we use four translation models, English-to-German, German-to-English, Englishto-Russian, and Russian-to-English provided by (Ng et al., 2019). Therefore, for each training sentence, we obtain two paraphrased version of it. In the test time, we average the probability distributions over the original and paraphrased sentenceimage pairs. 3.2 Feature Extraction Our system isProbabilities of a combination of pretrained visuolinguistic and linguistic models. We use CLIP (Radford et al., 2021) as a pretrained visuolinguistic model. CLIP provides an image encoder fi and a text encoder ft . They were pretrained on a prediction of matching image/text pairs. The training objective incentivizes high values of fi (I).ft (T ) if I and T are matc"
bethard-etal-2012-annotating,S07-1014,0,\N,Missing
bethard-etal-2012-annotating,P12-1010,1,\N,Missing
bethard-etal-2012-annotating,S10-1010,0,\N,Missing
bethard-etal-2012-annotating,J11-4004,0,\N,Missing
bethard-etal-2012-annotating,P09-1025,0,\N,Missing
bethard-etal-2012-annotating,W11-0419,0,\N,Missing
C02-2011,P00-1065,0,0.0154427,"here is that to build case frames one needs prior knowledge on which information exactly one wants to extract. In recent years, different solutions have been offered to automatically generate those frames from annotated examples (e.g. Riloff & Schmelzenbach 1998, Soderland 1999) or by using added knowledge (e.g. Harabagiu & Maiorano 2000). Many of those approaches were very successful but most of them have a tendency to blend syntactic and semantic concepts and they still have to be trained on individual domains. Some very interesting research on case frame detection has been done by Gildea (Gildea 2000, Gildea 2001). He uses statistical methods to learn case frames from parsed examples from FrameNet (Johnson et al. 2001). Conclusion There is a definite need for case role analysis in IE and in natural language processing in general. In this article, we have tried to argue that generic case role detection is possible by using shallow text analysis methods. We outlined our functional framework and presented a model that considers case role pattern extraction to be a standard classification task. Our main focus for the near future will be on automating as many aspects of the annotation process"
C02-2011,harabagiu-maiorano-2000-acquisition,0,0.028152,"this two-step approach might appear cumbersome, but it will enable us to easily expand the pattern base while reusing the hardwon patterns. 5 Related research Historically, case role detection has its roots in frame-based approaches to IE (e.g. Schank & Abelson 1977). The main problem here is that to build case frames one needs prior knowledge on which information exactly one wants to extract. In recent years, different solutions have been offered to automatically generate those frames from annotated examples (e.g. Riloff & Schmelzenbach 1998, Soderland 1999) or by using added knowledge (e.g. Harabagiu & Maiorano 2000). Many of those approaches were very successful but most of them have a tendency to blend syntactic and semantic concepts and they still have to be trained on individual domains. Some very interesting research on case frame detection has been done by Gildea (Gildea 2000, Gildea 2001). He uses statistical methods to learn case frames from parsed examples from FrameNet (Johnson et al. 2001). Conclusion There is a definite need for case role analysis in IE and in natural language processing in general. In this article, we have tried to argue that generic case role detection is possible by using s"
C02-2011,W98-1106,0,0.0497371,"Missing"
C12-1166,J93-2003,0,0.0929656,"al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009),"
C12-1166,J93-1001,0,0.254266,"pora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead of data augmentation. The method is directed towards extraction of only highly reliable translation pairs from parallel data of limited size. It is based on the idea of sub-corpora sampling from the original corpus. For inst"
C12-1166,W93-0301,0,0.15569,"or analysis, we have detected that both IBM Model 1 and LLR provide a wrong translation of the Dutch word beschouwen (consider), since both models retrieve the English word as as the first translation candidate (due to a very high frequency of the collocation consider as). Other examples of the same type include the Dutch word integreren (integrate) which is translated as into, betwijfelen (doubt) which is translated as whether, or an Italian example of the verb entrare (enter) which is translated as into. Our BLE model, on the other hand, provides correct translations for all these examples. Dagan et al. (1993) noted that collocates often tend to cause confusion among algorithms for bilingual lexicon extraction. More examples include the Dutch word opinie (opinion), translated as public by IBM Model 1 and LLR (due to a high frequency of the collocation public opinion), the Dutch word cirkels (circles), translated as concentric, or the Italian word pensionabile (pensionable), translated as age. All these examples are again correctly translated by our model for lexicon extraction. In order to test the hypothesis that our lexicon extraction model does not suffer from the problem of learning indirect as"
C12-1166,J93-1003,0,0.45469,"hms built upon the same idea. 2.3.3 Properties of the Algorithm Reducing corpora size provides several benefits. First, establishing associations between translation candidates is much easier when we deal with low-frequency words - we reduce our problem to a binary decision problem. According to the specified criteria for extraction, two words are simply considered to be a translation pair, or they are not. By employing the criteria that rely on raw frequency counts as distributional evidences, we remove the need of an association measure based on hypothesis testing such as the G 2 statistic (Dunning, 1993; Agresti, 2002) or a similarity-based measure such as the Dice coefficient (Dice, 1945), which are often unreliable when dealing with low-frequency words (Manning and Schütze, 1999). The SampLEX algorithm is symmetric and non-directional. The final output of the algorithm provides translation pairs along with their counts obtained after training. We can easily transform them into word translation probabilities to build word translation tables similar to those of IBM Model 1. Since the algorithm is symmetric, we can obtain both source-to-target and target-to-source word translation probabiliti"
C12-1166,W04-3208,0,0.0270937,"statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons"
C12-1166,P98-1069,0,0.125863,"ieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almo"
C12-1166,P08-1088,0,0.031167,"ey, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexico"
C12-1166,J93-1006,0,0.0943796,"it is easier to establish translational equivalence for low-frequency words. 2.2 Criteria for Extraction of Translation Pairs Given is a source language S, a target language T , and a corpus C of N aligned item pairs C = {(I1S , I1T ), (I2S , I2T ), . . . , (I NS , I NT )}, where, depending on the corpus type, item pairs may be sentences, paragraphs, chunks, documents, etc. For parallel corpora, the item pairs are pairs of sentences. The goal is to extract potential translation candidates from the item-aligned set using only internal distributional evidences. Internal evidences, according to Kay and Röscheisen (1993), represent information derived only from the given corpora themselves. Our criteria for establishing translational equivalence between words are derived from this trivial case: Imagine the scenario where a source word w1S occurs only once on the source side of the corpus C , in a source item I Sj . There is a target word w2T occurring in a target item I jT (which is aligned to I Sj ) and the word w2T also occurs only once on the target side of the corpus C . Additionally, there does not exist another source word w aS such that it occurs only once on the source side of the corpus and, at the s"
C12-1166,2005.mtsummit-papers.11,0,0.012472,"nce. Also, by building sub-corpora of smaller sizes from the original large corpus, we perform an implicit disambiguation - a word occurring only once or twice in a small sub-corpus cannot bear more meanings in that sub-corpus, although it might have more meanings in the large superset corpus. 3 Experimental Setup In this section, we present datasets used for training, training setup of the SampLEX method and state-of-the-art models for bilingual lexicon extraction from parallel data often used in real-life applications. 2727 3.1 Training 3.1.1 Training Collections We work with Europarl data (Koehn, 2005) for Dutch-English and Italian-English language pairs, retrieved from the website2 of the OPUS project (Tiedemann, 2009). We use subsets of the corpora, comprising the first 300, 000 sentence pairs. For Dutch-English, there are 76, 762 unique Dutch words, and 37, 138 unique English words. For Italian-English, there are 68, 710 unique Italian words and 37, 391 unique English words. The unbalance between the number of unique vocabulary words is mostly due to a richer morphological system in Italian and the noun compounding phenomenon in Dutch. Since we also want to test and evaluate the behavior"
C12-1166,C10-1070,0,0.0197607,"construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from"
C12-1166,E09-1057,0,0.0981246,"ls (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead"
C12-1166,J00-2004,0,0.0642524,"ient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Ki"
C12-1166,W03-0301,0,0.0997297,"Missing"
C12-1166,W01-1411,0,0.0390315,"ey (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Kilgarriff, 2001; Padó and Lapata, 2007), which is a four-cell matrix for each pair of words (w1S , w2T ) (see Table 1). w2T ¬w2T w1S k m ¬w1S l n Table 1: The contigency table for a pair of words (w1S , w2T ). The contingency table records that source word w1S and target word w2T co-occur in k aligned item/sentences pairs, and w1S occurs in m aligned pairs in which w2T is not present. Similarly, w2T occurs in l aligned pairs in which w1S is not present, and n is the number of aligned pairs that"
C12-1166,moore-2002-fast,0,0.043971,"t to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize st"
C12-1166,P04-1066,0,0.288762,"tool for bilingual lexicon extraction from parallel data (e.g., Venugopal et al. (2003), Munteanu and Marcu (2005), Munteanu and Marcu (2006), Lefever et al. (2009)). We use standard GIZA++ settings and train IBM Model 1 with 5 iterations 2 http://opus.lingfil.uu.se/Europarl3.php We have also tried to use word translation probabilities from the higher order IBM Models, but we have not detected any major difference in results on the task of bilingual word lexicon extraction. 3 2728 (IBM1-i5) and 20 iterations (IBM1-i20) of the EM algorithm, as often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-"
C12-1166,W04-3243,0,0.425853,"tool for bilingual lexicon extraction from parallel data (e.g., Venugopal et al. (2003), Munteanu and Marcu (2005), Munteanu and Marcu (2006), Lefever et al. (2009)). We use standard GIZA++ settings and train IBM Model 1 with 5 iterations 2 http://opus.lingfil.uu.se/Europarl3.php We have also tried to use word translation probabilities from the higher order IBM Models, but we have not detected any major difference in results on the task of bilingual word lexicon extraction. 3 2728 (IBM1-i5) and 20 iterations (IBM1-i20) of the EM algorithm, as often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-"
C12-1166,P07-1084,0,0.0208928,"anslation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from paral"
C12-1166,J05-4003,0,0.373784,"and-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of"
C12-1166,P06-1011,0,0.491432,"lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains"
C12-1166,J03-1002,0,0.233415,"cnih rjeˇcnika, empirijsko prevo¯ denje rijeˇci, uzorkovanje potkorpusa, smanjivanje koliˇcine podataka, niskofrekventne rijeˇci. Proceedings of COLING 2012: Technical Papers, pages 2721–2738, COLING 2012, Mumbai, December 2012. 2721 1 Introduction Bilingual word lexicons serve as an invaluable and indispensable source of knowledge for both end users (as an aid for translators or other language specialists) and many natural language processing tasks, such as dictionary-based cross-language information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi"
C12-1166,J07-2002,0,0.0139445,"ure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and Marcu, 2006). Its definition is easily explained on the basis of a contigency table (Kilgarriff, 2001; Padó and Lapata, 2007), which is a four-cell matrix for each pair of words (w1S , w2T ) (see Table 1). w2T ¬w2T w1S k m ¬w1S l n Table 1: The contigency table for a pair of words (w1S , w2T ). The contingency table records that source word w1S and target word w2T co-occur in k aligned item/sentences pairs, and w1S occurs in m aligned pairs in which w2T is not present. Similarly, w2T occurs in l aligned pairs in which w1S is not present, and n is the number of aligned pairs that involve neither w1S nor w2T . The final formula for the log-likelihood ratio is then defined as: L LR(w1S , w2T ) = G 2 (k, l, m, n) = 2(k"
C12-1166,P11-1133,0,0.223311,"ns manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003"
C12-1166,P95-1050,0,0.147782,"rmation retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or"
C12-1166,P99-1067,0,0.113752,"al., 1997; Levow et al., 2005) and statistical machine translation (Och and Ney, 2003). In order to construct high quality bilingual lexicons for various domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusive"
C12-1166,J03-3002,0,0.0301959,"word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues, we propose a novel approach built upon the idea of data reduction instead of data augmentation. The method is directed towards extraction of only highly reliable translation pairs from parallel data of limited size. It is based on the idea of sub-corpora sampling from the original corpus. For instance, given an initial corpus C of 4 data items {I1 , I2 , I3 , I4 }, the construction of, say, a sub-corpus SC = {I2 , I4 } may be observed as: (1) sampling items I2 , I4 ∈ C for SC (hence the term sub-corpora sampling) or (2) removing data items I1 , I3 from the or"
C12-1166,P10-1011,0,0.0246365,"ous domains, it is necessary to build such lexicons manually by hand or extract them automatically from parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignme"
C12-1166,D12-1003,0,0.0125501,"rom parallel corpora. Compiling such lexicons manually is often a labor-intensive and time-consuming task, whereas parallel corpora either do not exist or are of limited size for most language pairs. Therefore the focus of the researchers has turned towards bilingual lexicon extraction (BLE) from comparable corpora (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Laroche and Langlais, 2010; Andrade et al., 2010; Shezaf and Rappoport, 2010; Vuli´c et al., 2011; Prochasson and Fung, 2011; Vuli´c and Moens, 2012; Tamura et al., 2012). However, such lexicons contain a great deal of noise and, moreover, the methods for BLE from comparable corpora typically rely on seed lexicons which are again hand-built or extracted from parallel corpora. With respect to that observation, numerous systems for various applications trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as t"
C12-1166,E03-1026,0,0.163137,"often found in the literature (Och and Ney, 2003; Moore, 2004a). 3.2.2 The Dice Coefficient Another baseline model is a similarity-based model relying on the Dice coefficient (DICE): DI C E(w1S , w2T ) = 2 · C(w1S , w2T ) C(w1S ) + C(w2T ) (2) where C(w1S , w2T ) denotes the co-occurrence count of words w1S and w2T in the aligned items from the corpus. C(w1S ) and C(w2T ) denote the count of w1S on the source side of the corpus, and the count of w2T on the target side of the corpus, respectively. The Dice coefficient was used as an associative method for word alignment by Och and Ney (2003), Tiedemann (2003) used it as one associative clue for his clue-based word alignment, and Melamed (2000) used it to measure the strength of translational equivalence. 3.2.3 Log-Likelihood Ratio Another associative model that we use is based on the log-likelihood-ratio (LLR), that is derived from the G 2 statistic (Dunning, 1993). LLR is a more appropriate hypothesis testing method for detecting word associations from limited data than the χ 2 test (Manning and Schütze, 1999) and was previously used as an effective tool for automatically constructing bilingual lexicons (Melamed, 2000; Moore, 2001; Munteanu and M"
C12-1166,J07-1003,0,0.0136774,"sually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, these systems are typically faced with only limited parallel data for many language pairs and domains (Resnik and Smith, 2003). In order to tackle these issues"
C12-1166,P03-1041,0,0.225583,"tions trained on parallel or comparable data almost exclusively rely on knowledge from bilingual lexicons extracted from parallel texts. These lexicons are usually acquired from word translation probabilities of the IBM alignment models (Brown et al., 1993; Och and Ney, 2003) or obtained by associative methods such as the log-likelihood score or the Dice coefficient. They are then used in systems for extracting parallel sentences from non-parallel corpora (Fung and Cheung, 2004; Munteanu and Marcu, 2005), bilingual sentence alignment (Moore, 2002), estimating phrase translation probabilities (Venugopal et al., 2003), extracting parallel sub-sentential fragments from nonparallel corpora (Munteanu and Marcu, 2006), word-level confidence estimation (Ueffing and Ney, 2007), sub-sentential alignment for terminology extraction (Lefever et al., 2009), cross-lingual text classification and plagiarism detection (Pinto et al., 2009) and others. High accuracy of automatically constructed bilingual word lexicons is the top priority for these systems. Church and Mercer (1993) advocate a simple solution of collecting more data in order to utilize statistical and stochastic methods in a more effective way. However, the"
C12-1166,P11-2084,1,0.895272,"Missing"
C12-1166,E12-1046,1,0.886432,"Missing"
C12-1166,C98-1066,0,\N,Missing
C12-1166,C10-1003,0,\N,Missing
C12-2025,C10-1017,0,0.0173566,"-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized. Several researchers have worked on generating a globally optimized clustering, but these suffer from a very large search space, and need to resort to heuristics to find an approximate solution. E.g. (Luo et al., 2004) uses a Bell tree representation to construct the space of all possible clusterings, although a complete search in it is intractable and partial and heuristic search strategies have to be employed. Other approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al."
C12-2025,W11-1904,0,0.155805,"to resort to heuristics to find an approximate solution. E.g. (Luo et al., 2004) uses a Bell tree representation to construct the space of all possible clusterings, although a complete search in it is intractable and partial and heuristic search strategies have to be employed. Other approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010). We also use Integer Linear Programming (ILP) to formulate the clustering problem, and to solve this exactly. Although previous approaches decide for every pair of mentions if they are in the same cluster, we instead decide on which clusters a"
C12-2025,H05-1013,0,0.0562893,"Missing"
C12-2025,P08-2012,0,0.0194583,"rge search space, and need to resort to heuristics to find an approximate solution. E.g. (Luo et al., 2004) uses a Bell tree representation to construct the space of all possible clusterings, although a complete search in it is intractable and partial and heuristic search strategies have to be employed. Other approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010). We also use Integer Linear Programming (ILP) to formulate the clustering problem, and to solve this exactly. Although previous approaches decide for every pair of mentions if they are in the same cluster, we instead decid"
C12-2025,P07-1107,0,0.019664,"he classifier outputs a probability that reflects the degree to which the two mentions are coreferent. Second, the coreferent mentions need to be clustered to form coreference chains. Transitivity is an important aspect, since two coreferent pairs (m1 , m2 ) and (m2 , m3 ) entail that m1 and m3 are coreferent as well. In the beginning of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we focus on the clustering aspect of coreference resolution. Previous work has solved this using heuristic approaches, most notable (Soon et al., 2001), who use the link-first decision, which links a mention to its closest candidate referent. (Ng a"
C12-2025,D09-1120,0,0.0201764,"g of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we focus on the clustering aspect of coreference resolution. Previous work has solved this using heuristic approaches, most notable (Soon et al., 2001), who use the link-first decision, which links a mention to its closest candidate referent. (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized. Several researchers have worked on generating a globally optimized clustering, but these suffer from a very large search space, and need to resort to heuristic"
C12-2025,P04-1018,0,0.0349282,"g aspect of coreference resolution. Previous work has solved this using heuristic approaches, most notable (Soon et al., 2001), who use the link-first decision, which links a mention to its closest candidate referent. (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized. Several researchers have worked on generating a globally optimized clustering, but these suffer from a very large search space, and need to resort to heuristics to find an approximate solution. E.g. (Luo et al., 2004) uses a Bell tree representation to construct the space of all possible clusterings, although a complete search in it is intractable and partial and heuristic search strategies have to be employed. Other approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but w"
C12-2025,P09-1039,0,0.0356054,"d Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010). We also use Integer Linear Programming (ILP) to formulate the clustering problem, and to solve this exactly. Although previous approaches decide for every pair of mentions if they are in the same cluster, we instead decide on which clusters are in the optimal clustering. This leads to an ILP problem with an exponential amount of variables (i.e. one for every possible cluster of mentions), but few constraints. However, by using column generation, and exploiting the special structure of the clustering problem, we can efficiently find a solution. We show that we obtain a dra"
C12-2025,P02-1014,0,0.0768079,"irst, there is the identification of which mentions in a document are likely to be coreferent. For each two mentions a decision is made by a local pairwise classifier whether or not they are compatible. More generally, the classifier outputs a probability that reflects the degree to which the two mentions are coreferent. Second, the coreferent mentions need to be clustered to form coreference chains. Transitivity is an important aspect, since two coreferent pairs (m1 , m2 ) and (m2 , m3 ) entail that m1 and m3 are coreferent as well. In the beginning of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we focus on the clusteri"
C12-2025,W06-1633,0,0.0182951,"links a mention to its most confident candidate referent. Both these clustering decisions are locally optimized. Several researchers have worked on generating a globally optimized clustering, but these suffer from a very large search space, and need to resort to heuristics to find an approximate solution. E.g. (Luo et al., 2004) uses a Bell tree representation to construct the space of all possible clusterings, although a complete search in it is intractable and partial and heuristic search strategies have to be employed. Other approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010)."
C12-2025,D08-1068,0,0.0224544,"obability that reflects the degree to which the two mentions are coreferent. Second, the coreferent mentions need to be clustered to form coreference chains. Transitivity is an important aspect, since two coreferent pairs (m1 , m2 ) and (m2 , m3 ) entail that m1 and m3 are coreferent as well. In the beginning of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we focus on the clustering aspect of coreference resolution. Previous work has solved this using heuristic approaches, most notable (Soon et al., 2001), who use the link-first decision, which links a mention to its closest candidate referent. (Ng and Cardie, 2002) consider"
C12-2025,D10-1048,0,0.0249531,"nce chains. Transitivity is an important aspect, since two coreferent pairs (m1 , m2 ) and (m2 , m3 ) entail that m1 and m3 are coreferent as well. In the beginning of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we focus on the clustering aspect of coreference resolution. Previous work has solved this using heuristic approaches, most notable (Soon et al., 2001), who use the link-first decision, which links a mention to its closest candidate referent. (Ng and Cardie, 2002) consider instead the link-best decision, which links a mention to its most confident candidate referent. Both these clustering decisions are locally optimiz"
C12-2025,W04-2401,0,0.0289239,"ther approaches are based on graph partitioning (Cai and Strube, 2010; Nicolae and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010). We also use Integer Linear Programming (ILP) to formulate the clustering problem, and to solve this exactly. Although previous approaches decide for every pair of mentions if they are in the same cluster, we instead decide on which clusters are in the optimal clustering. This leads to an ILP problem with an exponential amount of variables (i.e. one for every possible cluster of mentions), but few constraints. However, by using column generation, and exploiting the special structure of the clustering problem, we can e"
C12-2025,D10-1001,0,0.0299628,"e and Nicolae, 2006), to divide the fully-connected pairwise graph into smaller graphs that represent entities. Few have attempted to calculate an exact solution to the clustering problem. (Denis and Baldridge, 2009; Finkel and Manning, 2008; Chang et al., 2011) solve this with an Integer Linear Programming approach, but when enforcing transitivity on the pairwise decisions, they are faced with a cubic number of constraints, and solving large instances takes too long. Linear Programming techniques have many benefits (Roth and Yih, 2004), but efficiency is still an issue (Martins et al., 2009; Rush et al., 2010). We also use Integer Linear Programming (ILP) to formulate the clustering problem, and to solve this exactly. Although previous approaches decide for every pair of mentions if they are in the same cluster, we instead decide on which clusters are in the optimal clustering. This leads to an ILP problem with an exponential amount of variables (i.e. one for every possible cluster of mentions), but few constraints. However, by using column generation, and exploiting the special structure of the clustering problem, we can efficiently find a solution. We show that we obtain a drastic decrease in tim"
C12-2025,J01-4004,0,0.0958904,"rence resolution. First, there is the identification of which mentions in a document are likely to be coreferent. For each two mentions a decision is made by a local pairwise classifier whether or not they are compatible. More generally, the classifier outputs a probability that reflects the degree to which the two mentions are coreferent. Second, the coreferent mentions need to be clustered to form coreference chains. Transitivity is an important aspect, since two coreferent pairs (m1 , m2 ) and (m2 , m3 ) entail that m1 and m3 are coreferent as well. In the beginning of the previous decade (Soon et al., 2001; Ng and Cardie, 2002), these two steps were done separately, and the latter rather naively. Later, more advanced Machine Learning approaches were proposed to solve the two tasks simultaneously (Daume III and Marcu, 2005; Haghighi and Klein, 2007; Poon and Domingos, 2008). Recently there has been a movement towards more conservative models, that employ very rich and accurate feature spaces (Raghunathan et al., 2010), but still the clustering method is understudied, and taking the transitive closure of the individual pairwise decision is still common (Haghighi and Klein, 2009). In this paper we"
C12-2025,P10-2029,0,0.0132067,"several calls to the LP solver are made. The overhead associated with keeping track of the current basis is negligible. We group the documents by the number of mentions they contain, and put these in bins of 10 wide. So we have a set of documents with 11 to 20 mentions, a set with 21 to 30 mentions, etc. We take the average runtime of each bin. In the experiments we used the CoNLL 2011 data, which contains documents with over one hundred mentions. We trained the pairwise classifier on the training set, and evaluated on the development set. In our implementation we use the RECONCILE framework (Stoyanov et al., 2010) to learn a pairwise classifier, using 76 state of the art features. We use default values for the classifier and training sample generation, and train a model to obtain pairwise similarity measures in the [0, 1] range, and subtract 0.5. This is then used as the pairwise similarity. 6 Results The results are in figure 1. The graphs shows the average runtime of the two methods in function of the number of mentions in the document. The baseline method, indicated with all-link, appears to have a cubic complexity. The method proposed in this paper, named all-link-colgen, appears to have a lower co"
C16-1121,P98-1013,0,0.229651,"its arguments in a given sentence. Intuitively, it aims at answering the questions of “Who did What to Whom, and How, When and Where?” in text. For example, the processing of the sentence “He bought tons of roses yesterday” should result in the identification of a “buying” event corresponding to the predicate “bought” with three arguments including “he” as the Agent (A0), “tons of roses” as the Thing being bought (A1), and “yesterday” as the Time (AM-TMP) arguments. Traditional SRL systems have concentrated on supervised learning from several manually-built semantic corpora, (e.g., FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005)). One important limitation of supervised approaches is that they depend heavily on the accuracy, coverage and labeling scheme of the labeled corpus. When the training and the testing data are in different domains, the linguistic patterns and their distributions in the testing domain are different from the ones observed in the training data, resulting in a considerable performance drop. Developing more manually-built semantic corpora is expensive and requires huge human efforts. Thus, exploiting large unlabeled datasets by semi-supervised or unsupervised appr"
C16-1121,C10-3009,0,0.0456503,"Missing"
C16-1121,D15-1112,0,0.0236176,"Missing"
C16-1121,J12-1005,0,0.0294812,"Missing"
C16-1121,N15-1121,0,0.0288347,"Missing"
C16-1121,W04-2405,0,0.103629,"Missing"
C16-1121,J05-1004,0,0.0289786,". Intuitively, it aims at answering the questions of “Who did What to Whom, and How, When and Where?” in text. For example, the processing of the sentence “He bought tons of roses yesterday” should result in the identification of a “buying” event corresponding to the predicate “bought” with three arguments including “he” as the Agent (A0), “tons of roses” as the Thing being bought (A1), and “yesterday” as the Time (AM-TMP) arguments. Traditional SRL systems have concentrated on supervised learning from several manually-built semantic corpora, (e.g., FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005)). One important limitation of supervised approaches is that they depend heavily on the accuracy, coverage and labeling scheme of the labeled corpus. When the training and the testing data are in different domains, the linguistic patterns and their distributions in the testing domain are different from the ones observed in the training data, resulting in a considerable performance drop. Developing more manually-built semantic corpora is expensive and requires huge human efforts. Thus, exploiting large unlabeled datasets by semi-supervised or unsupervised approaches is a promising solution. Our"
C16-1121,J08-2005,0,0.0188295,"step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to reduce human annotation efforts. He and Gildea (2006) investigate the possibility of a weakly supervised approach by using self-training and co-training for unseen frames of SRL. They separate the headword and path as the two views for co-training, but could not show a clear performance improvement. The sources of the problem appeared to be the big gap in performance between the headword and path feature sets and the complexity of the task. Some other works show"
C16-1121,P16-1113,0,0.0724166,"eriment is presented in Section 5 and Section 6 and finally we conclude in Section 7. 2 Related Work In traditional supervised approaches, SRL is modeled as a pipeline of predicate identification, predicate disambiguation, argument identification, and argument classification steps. Hand-engineered linguisticallymotivated feature templates represent the semantic structure employed to train classifiers for each step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to reduce human annotation efforts. He and Gildea (2006)"
C16-1121,D14-1045,0,0.222562,"essing. Such representations are typically learned from a large corpus using neural networks (e.g., Weston et al. (2008)), probabilistic graphical models (e.g., Deschacht et al. (2012)) or term-cooccurrence statistics (e.g., Turney and Pantel (2010)) by capturing the contexts in which the words appear. Often words from the vocabulary or phrases are mapped to vectors of real numbers in a low dimensional continuous space resulting in so-called word embeddings. Deschacht et al. (2012) employ distributed representations for each argument candidate as extra features when training a supervised SRL. Roth and Woodsend (2014) propose to use the compositional representations such as interaction of predicate and argument, dependency path and the full argument span to improve a state-of-the-art SRL system. 3 A Semantic Role Labeling System for Semi-Supervised Approaches In this section, we introduce a semantic role labeling system designed for semi-supervised settings. The system has a simple training strategy with local classifiers for different steps in SRL pipeline. Instead of training a global reranker on top of the local classifiers to improve performance as in other common pipeline-based state-of-the-art system"
C16-1121,W11-3906,0,0.0176956,"ed to reduce human annotation efforts. He and Gildea (2006) investigate the possibility of a weakly supervised approach by using self-training and co-training for unseen frames of SRL. They separate the headword and path as the two views for co-training, but could not show a clear performance improvement. The sources of the problem appeared to be the big gap in performance between the headword and path feature sets and the complexity of the task. Some other works show slight improvements of using co-training for SRL when there is a limited number of labeled data (Lee et al., 2007; Samad Zadeh Kaljahi and Baba, 2011). F¨urstenau and Lapata (2012) find novel instances for classifier training based on their similarity to manually labeled seed instances. This strategy is formalized via a graph alignment problem. Recently, there has been interest in distributional word representations for natural language processing. Such representations are typically learned from a large corpus using neural networks (e.g., Weston et al. (2008)), probabilistic graphical models (e.g., Deschacht et al. (2012)) or term-cooccurrence statistics (e.g., Turney and Pantel (2010)) by capturing the contexts in which the words appear. O"
C16-1121,D11-1012,0,0.0932483,"s work, each word wi is assigned probabilities P AC (p, wi , Lj ) to receive Lj ∈ L as semantic label. We employ the features proposed by Bj¨orkelund et al. (2010) as the basic feature set. All of the local classifiers are trained using L2-regularized logistic regression. For multiclass problems, we use the one-vs-rest strategy. At inference time, the local classifier predictions are merged using integer linear programming (ILP). In most of the prior work, ILP was only used for AC inference. However, this approach limits the interaction of AI and AC when making decisions. In another approach, Srikumar and Roth (2011) introduce a simple approach to joint inference over AI and AC allowing the two argument sub-tasks to support each other. Their local AC classifier has an empty label which indicates that the candidate is, in fact, not an argument. This forces AC module to learn also the argument identification and is in contrast with our approach in which the tasks of AI and AC classifiers are completely separated leading to a simpler AC learning. Their inference is formularized as an ILP problem that mazimizes the sum of local prediction scores over AI and AC. The authors then enforce consistency constraints"
C16-1121,Q15-1003,0,0.022731,"Missing"
C16-1121,P05-1073,0,0.153986,"y in Section 3 and Section 4 respectively. Our experiment is presented in Section 5 and Section 6 and finally we conclude in Section 7. 2 Related Work In traditional supervised approaches, SRL is modeled as a pipeline of predicate identification, predicate disambiguation, argument identification, and argument classification steps. Hand-engineered linguisticallymotivated feature templates represent the semantic structure employed to train classifiers for each step. It is common among the state-of-the-art systems to train a global reranker on top of the local classifiers to improve performance (Toutanova et al., 2005; Bj¨orkelund et al., 2010; Roth and Lapata, 2016). SRL models have also been trained using graphical models (T¨ackstr¨om et al., 2015) and neural networks (Collobert et al., 2011; FitzGerald et al., 2015). Some systems have applied a set of structural constraints to the argument classification sub-task, such as avoiding overlapping arguments and repeated core roles, and enforced these constraints with integer linear programming (ILP) (Punyakanok et al., 2008) or a dynamic program (T¨ackstr¨om et al., 2015). Regarding leveraging unlabeled data, semi-supervised methods have been proposed to red"
C16-1121,C00-2137,0,0.0586598,"Missing"
C16-1121,C98-1013,0,\N,Missing
C16-1264,P14-1023,0,0.0403471,"perception, in which the learning occurs at different levels of abstraction—or layers of a network. On the language side, distributional models (DMs) have been employed for learning semantic representations a long time ago. These are based on the distributional hypothesis: Words which are similar in meaning occur in similar contexts (Rubenstein and Goodenough, 1965). Recently, neural-based distributional models or word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have achieved great success, rapidly replacing the old DMs (Turney et al., 2010) based on word co-occurrence counts (Baroni et al., 2014). Instead of counting words, neural-based DMs capture words co-occurrences by trying to predict the context given a word (skip-gram) or by trying to predict a word given its context (CBOW). Alternative approaches such as generative probabilistic models that learn the probability distribution of a vocabulary word in a context window as a latent variable have also been proposed (Deschacht et al., 2012; Deschacht and Moens, 2009). 2.2 Multimodal Representations There exist certain properties of perceptible objects that are poorly captured by language. For example, everyone can easily tell from an"
C16-1264,P12-1015,0,0.0157055,"tributive properties (Baroni and Lenci, 2008). While Rubinstein et al. (2015) investigated whether DMs are equally good at capturing each type of attribute, our research questions are different. First, we want to answer whether there are differences between textual and visual representations in the type of attributes that they encode; and second, where these differences are. In other words, we present an inter-modality analysis while Rubinstein et al. (2015) performed only intra-modality comparisons. In addition to the survey of Rubinstein et al. (2015), the closest work to ours is a study by Bruni et al. (2012) who showed that a very particular type of attribute, namely color, is better captured by visual representations than by DMs. Here, we go one step further and compare performance between visual and text embeddings for a larger number of visual attributes, as well as for other non-visual attributes such as taxonomic, functional or encyclopedic. 3 Approach and Experimental Settings In this section we describe the procedure that we follow in order answer our research questions. An explanatory diagram is shown in Fig. 1. 3.1 Visual Representations We use ImageNet (Russakovsky et al., 2015) as our"
C16-1264,D09-1003,1,0.585234,"embeddings (Mikolov et al., 2013; Pennington et al., 2014) have achieved great success, rapidly replacing the old DMs (Turney et al., 2010) based on word co-occurrence counts (Baroni et al., 2014). Instead of counting words, neural-based DMs capture words co-occurrences by trying to predict the context given a word (skip-gram) or by trying to predict a word given its context (CBOW). Alternative approaches such as generative probabilistic models that learn the probability distribution of a vocabulary word in a context window as a latent variable have also been proposed (Deschacht et al., 2012; Deschacht and Moens, 2009). 2.2 Multimodal Representations There exist certain properties of perceptible objects that are poorly captured by language. For example, everyone can easily tell from an image whether a face is attractive, yet if one has to describe with language what properties make a face attractive will certainly struggle. Recently, CNN-based computer vision models have achieved reasonable success in the task of recognizing attractiveness (Rothe et al., 2015). Furthermore, psychological research evidences that human concept formation is strongly grounded in visual perception (Barsalou, 2008). All this sugg"
C16-1264,D14-1005,0,0.559848,"reakthroughs in learning unimodal representations (a.k.a. embeddings) in both, computer vision (CV) and natural language processing (NLP) (LeCun et al., 2015). However, the automatic integration of visual and linguistic modalities is still a challenging—and usually task-dependent—problem that has gained increasing popularity within the NLP and CV communities. Lately, several studies have achieved reasonable success in integrating visual and linguistic representations, showing improvement over the unimodal baselines in simple linguistic tasks such as concept similarity (Lazaridou et al., 2015; Kiela and Bottou, 2014; Silberer and Lapata, 2014)—which is only possible if vision and language encode complementary knowledge. In this paper we do not tackle the problem of how to integrate both modalities, but instead, we systematically study what type of fine-grain semantic knowledge is encoded in each modality, shedding light on the potential benefit of combining vision and language. By fine-grain semantics we refer to the recognition of different types of attributes or properties (e.g., shape, function, sound, etc.) that concrete nouns might exhibit. A recent study by Rubinstein et al. (2015) evidenced that s"
C16-1264,D14-1162,0,0.0838066,"rapidly become the state-of-the-art approach in computer vision (Krizhevsky et al., 2012). To some extent, CNN algorithms emulate human visual perception, in which the learning occurs at different levels of abstraction—or layers of a network. On the language side, distributional models (DMs) have been employed for learning semantic representations a long time ago. These are based on the distributional hypothesis: Words which are similar in meaning occur in similar contexts (Rubenstein and Goodenough, 1965). Recently, neural-based distributional models or word embeddings (Mikolov et al., 2013; Pennington et al., 2014) have achieved great success, rapidly replacing the old DMs (Turney et al., 2010) based on word co-occurrence counts (Baroni et al., 2014). Instead of counting words, neural-based DMs capture words co-occurrences by trying to predict the context given a word (skip-gram) or by trying to predict a word given its context (CBOW). Alternative approaches such as generative probabilistic models that learn the probability distribution of a vocabulary word in a context window as a latent variable have also been proposed (Deschacht et al., 2012; Deschacht and Moens, 2009). 2.2 Multimodal Representations"
C16-1264,P15-2119,0,0.517953,"idou et al., 2015; Kiela and Bottou, 2014; Silberer and Lapata, 2014)—which is only possible if vision and language encode complementary knowledge. In this paper we do not tackle the problem of how to integrate both modalities, but instead, we systematically study what type of fine-grain semantic knowledge is encoded in each modality, shedding light on the potential benefit of combining vision and language. By fine-grain semantics we refer to the recognition of different types of attributes or properties (e.g., shape, function, sound, etc.) that concrete nouns might exhibit. A recent study by Rubinstein et al. (2015) evidenced that state-of-the-art linguisticonly representations do not succeed at capturing all types of attributes equally well. Here, we extend their work into the multimodal domain by comparing the performance between visual and linguistic representations at encoding different types of attributes. In contrast with Rubinstein et al. (2015)’s unimodal research, here we aim to answer two different research questions. First, whether either vision or This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http://"
C16-1264,P14-1068,0,0.105497,"unimodal representations (a.k.a. embeddings) in both, computer vision (CV) and natural language processing (NLP) (LeCun et al., 2015). However, the automatic integration of visual and linguistic modalities is still a challenging—and usually task-dependent—problem that has gained increasing popularity within the NLP and CV communities. Lately, several studies have achieved reasonable success in integrating visual and linguistic representations, showing improvement over the unimodal baselines in simple linguistic tasks such as concept similarity (Lazaridou et al., 2015; Kiela and Bottou, 2014; Silberer and Lapata, 2014)—which is only possible if vision and language encode complementary knowledge. In this paper we do not tackle the problem of how to integrate both modalities, but instead, we systematically study what type of fine-grain semantic knowledge is encoded in each modality, shedding light on the potential benefit of combining vision and language. By fine-grain semantics we refer to the recognition of different types of attributes or properties (e.g., shape, function, sound, etc.) that concrete nouns might exhibit. A recent study by Rubinstein et al. (2015) evidenced that state-of-the-art linguisticon"
C18-1291,S17-2093,0,0.0246375,"of the art for temporal relation extraction on the THYME dataset even without dedicated clinical preprocessing. 2 Related Work The model we present draws inspiration from prior research on (temporal) relation classification and neural multi-task learning. 2.1 Clinical Temporal Relation Extraction Temporal relation extraction from clinical texts is a widely studied area in NLP and has been explored through various shared tasks, such as the i2b2 shared task on clinical temporal information (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural mode"
C18-1291,D15-1085,0,0.0212278,"raction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jiang, 2016), sentence level name prediction (Cheng et al., 2015), semantic role labeling (Collobert and Weston, 2008), and many more. For example, Collobert and Weston (2008) used an auxiliary unsupervised objective for semantic role labeling (SRL). They alternately trained embeddings in a language model and a SRL model. In contrast to their work, we learn both tasks truly jointly, and optimize a single semi-supervised objective. Typically in neural MTL, one or more layers of the network are shared among different models. Two issues in MTL are (1) how to determine if the tasks are related enough to benefit from each other, and (2) what layers to share amon"
C18-1291,E17-2118,0,0.21456,"uistics, pages 3436–3447 Santa Fe, New Mexico, USA, August 20-26, 2018. in three iterations of the Clinical TempEval Shared Task (Bethard et al., 2016). Still there is a gap of more than 0.20 in F-measure between the state-of-the-art CR extraction systems and the inter-adjunctator agreement (indicating an upper bound for performance). This shows that this task is very challenging. Following the current trend in NLP, the recent state-of-the-art models for extraction of CR are neural network models. These models all use pre-trained word embeddings as word representations (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2017). Pre-training of the embeddings is done with an auxiliary task (a task where one is not interested in the final predictions, but in the trained model components), like the skip-gram task (Mikolov et al., 2013). When used for classification tasks in NLP, these pre-trained word representations are often either used as fixed inputs for the classification model, or as initialization for the word representations of the classification model (sometimes called fine-tuned embeddings). A problem with pre-trained representations in classification models is that solving the main task o"
C18-1291,P15-1166,0,0.024516,"and employ only a general news domain POS tagger (Toutanova et al., 2003), providing important temporal relation extraction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jiang, 2016), sentence level name prediction (Cheng et al., 2015), semantic role labeling (Collobert and Weston, 2008), and many more. For example, Collobert and Weston (2008) used an auxiliary unsupervised objective for semantic role labeling (SRL). They alternately trained embeddings in a language model and a SRL model. In contrast to their work, we learn both tasks truly jointly, and optimize a single semi-supervised objective. Typically in neural MTL, one or more layers of the network are shared among different models. Two issues in MTL a"
C18-1291,D17-1206,0,0.0272068,"o issues in MTL are (1) how to determine if the tasks are related enough to benefit from each other, and (2) what layers to share among the models. Baxter and others (2000) theoretically argue that tasks are related when they share an inductive bias. In our model, we expect that the skip-gram task (Mikolov et al., 2013) can act as a reasonable word-level inductive bias for our task, as it has already shown its effectiveness in SRL (Collobert and Weston, 2008) and sentiment analysis (Peng and Dredze, 2015) in MTL, and for many NLP classification tasks when using them as pre-trained embeddings. Hashimoto et al. (2017) showed that even when combining many tasks, considering the task hierarchy (simpler tasks lower in the network) allows them to benefit from each other. In most work on MTL the auxiliary tasks are supervised and specifically chosen for their relatedness to the main task (Ruder, 2017), whereas in our model we chose the unsupervised auxiliary skip-gram task, and share weights of the word embedding layer. This results in a new joint relation classification objective that is semi-supervised on the word-level and provides better generalization for the final classification model. 3 The Model Our mod"
C18-1291,S16-1201,0,0.0268973,"draws inspiration from prior research on (temporal) relation classification and neural multi-task learning. 2.1 Clinical Temporal Relation Extraction Temporal relation extraction from clinical texts is a widely studied area in NLP and has been explored through various shared tasks, such as the i2b2 shared task on clinical temporal information (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural models with dedicated clinical NLP tools for preprocessing the clinical texts, often using the English cTAKES system (Savova et al., 2010), which"
C18-1291,E17-1108,1,0.928759,"from prior research on (temporal) relation classification and neural multi-task learning. 2.1 Clinical Temporal Relation Extraction Temporal relation extraction from clinical texts is a widely studied area in NLP and has been explored through various shared tasks, such as the i2b2 shared task on clinical temporal information (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural models with dedicated clinical NLP tools for preprocessing the clinical texts, often using the English cTAKES system (Savova et al., 2010), which contains tools for clinical PO"
C18-1291,W16-2914,0,0.299696,"+ SGLR variant. Additionally, it should be noticed that parameters are not returned on each dataset size, but obtained from tuning on the full Dev set. Still the model ranking is consistent. 5.3 Evaluation on Subsets of Relations To get a more detailed insight in what each model learns relative to the others, we evaluated our models on different subsets of the data. First, we split the containment relations based on their argument types and separately evaluated the 3.3k EE relations and the 2.7k TE relations. EE relations are generally found more difficult than TE relations (Lin et al., 2015; Lin et al., 2016; Dligach et al., 2017; Lin et al., 2017). In Table 1 we can see that also for our model, EE relations are harder to recognize than the TE relations, as all models achieve higher scores for TE compared to EE relations. What is interesting to see is that when training with the combined loss (SG or SGLR) we obtain a clear improvement on the more difficult EE relations, and perform slightly worse on TE relations compared to using pre-trained embeddings (the three upper settings). The reason could be that EE relations are more diverse in vocabulary, and are consequently more influenced by the qual"
C18-1291,W17-2341,0,0.265307,"47 Santa Fe, New Mexico, USA, August 20-26, 2018. in three iterations of the Clinical TempEval Shared Task (Bethard et al., 2016). Still there is a gap of more than 0.20 in F-measure between the state-of-the-art CR extraction systems and the inter-adjunctator agreement (indicating an upper bound for performance). This shows that this task is very challenging. Following the current trend in NLP, the recent state-of-the-art models for extraction of CR are neural network models. These models all use pre-trained word embeddings as word representations (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2017). Pre-training of the embeddings is done with an auxiliary task (a task where one is not interested in the final predictions, but in the trained model components), like the skip-gram task (Mikolov et al., 2013). When used for classification tasks in NLP, these pre-trained word representations are often either used as fixed inputs for the classification model, or as initialization for the word representations of the classification model (sometimes called fine-tuned embeddings). A problem with pre-trained representations in classification models is that solving the main task often requires diffe"
C18-1291,N15-1142,0,0.0371,"g Lsg (θ ) = − X yi log pˆsg (wi ) (4) i=1 1 We also experimented with bidirectional LSTMs (Zhang et al., 2015) and adding attention (Zhou et al., 2016). In our experiments, this did not result in significant improvements. 3439 3.2.1 Separate Left & Right Context (SGLR) The skip-gram model is quite rough in its context description and does not take into account word order very well. However, for temporal relations we expect word order to be relevant. For this reason, we also experimented with a variation on the skip-gram model, separating the left and right context, following the intuition of Ling et al. (2015). The context separation is achieved by extending the context words by a ‘left’ or ‘right’ prefix depending on their location relative to the sampled word. 3.3 Combination (RC + SG) We train our proposed model on a combination of both loss functions, each with their own dataset Drc , and Dsg respectively. The combined loss, shown in Eq. 5, is a weighted sum of their cross-entropy losses, where λsg determines the importance of the SG loss. Lrc+sg (θ) = Lrc (θrc ) + λsg Lsg (θsg ) (5) A crucial part of our model is that although both models sample different types of inputs (the RC: sequences, th"
C18-1291,W15-1506,0,0.0230967,"ation (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural models with dedicated clinical NLP tools for preprocessing the clinical texts, often using the English cTAKES system (Savova et al., 2010), which contains tools for clinical POS tagging, named entity recognition, and a dependency parser all trained on clinical data. The main reason for using these dedicated clinical tools is that parsers trained on non-clinical texts perform significantly worse on clinical data (Jiang et al., 2015). 3437 Dedicated clinical NLP tools are not available for mos"
C18-1291,D15-1064,0,0.0238449,"S tagger (Toutanova et al., 2003), providing important temporal relation extraction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jiang, 2016), sentence level name prediction (Cheng et al., 2015), semantic role labeling (Collobert and Weston, 2008), and many more. For example, Collobert and Weston (2008) used an auxiliary unsupervised objective for semantic role labeling (SRL). They alternately trained embeddings in a language model and a SRL model. In contrast to their work, we learn both tasks truly jointly, and optimize a single semi-supervised objective. Typically in neural MTL, one or more layers of the network are shared among different models. Two issues in MTL are (1) how to determine if the tasks are re"
C18-1291,petrov-etal-2012-universal,0,0.0230915,"2015). 3437 Dedicated clinical NLP tools are not available for most languages though, and retraining NLP tools on clinical data is quite resource intensive, because it requires extra annotation effort. Additionally, clinical data is often difficult to obtain or share publicly for patient privacy reasons. Hence, we keep resource intensive preprocessing to a minimum and employ only a general news domain POS tagger (Toutanova et al., 2003), providing important temporal relation extraction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jiang, 2016), sentence level name prediction (Cheng et al., 2015), semantic role labeling (Collobert and Weston, 2008), and many more. For example, Collobert and Weston (2008) used an"
C18-1291,P16-2038,0,0.0359551,"rmines the importance of the SG loss. Lrc+sg (θ) = Lrc (θrc ) + λsg Lsg (θsg ) (5) A crucial part of our model is that although both models sample different types of inputs (the RC: sequences, the SG: single words) from different datasets, and have different classification weights, the token (θ rc ∩ θ sg = W token ), also illustrated in Fig. 2. So only the word embeddings are shared, i.e. Wem em word embeddings are directly influenced by both losses. All other weights (from RC or SG) are only influenced indirectly, through the word embedding weights, as both models are trained simultaneously. Søgaard and Goldberg (2016) showed that, for NLP, sharing representations at the lower levels of the network is most effective: when lower level features are shared, there is room for the model to learn task specific abstractions in higher layers. For this reason we choose our model to share only the word embedding layer, as schematically illustrated in Fig. 5. Figure 5: Schematic representation of how the SG model component extends the RC model when using the combined loss on input word wj ∈ Dsg , and word wt at time step t from input sequence xi ∈ Drc . The gray layer indicates the shared word embedding parameters. Th"
C18-1291,P17-2035,0,0.138237,"e on Computational Linguistics, pages 3436–3447 Santa Fe, New Mexico, USA, August 20-26, 2018. in three iterations of the Clinical TempEval Shared Task (Bethard et al., 2016). Still there is a gap of more than 0.20 in F-measure between the state-of-the-art CR extraction systems and the inter-adjunctator agreement (indicating an upper bound for performance). This shows that this task is very challenging. Following the current trend in NLP, the recent state-of-the-art models for extraction of CR are neural network models. These models all use pre-trained word embeddings as word representations (Tourille et al., 2017; Dligach et al., 2017; Lin et al., 2017). Pre-training of the embeddings is done with an auxiliary task (a task where one is not interested in the final predictions, but in the trained model components), like the skip-gram task (Mikolov et al., 2013). When used for classification tasks in NLP, these pre-trained word representations are often either used as fixed inputs for the classification model, or as initialization for the word representations of the classification model (sometimes called fine-tuned embeddings). A problem with pre-trained representations in classification models is that s"
C18-1291,N03-1033,0,0.0452982,"d on clinical data. The main reason for using these dedicated clinical tools is that parsers trained on non-clinical texts perform significantly worse on clinical data (Jiang et al., 2015). 3437 Dedicated clinical NLP tools are not available for most languages though, and retraining NLP tools on clinical data is quite resource intensive, because it requires extra annotation effort. Additionally, clinical data is often difficult to obtain or share publicly for patient privacy reasons. Hence, we keep resource intensive preprocessing to a minimum and employ only a general news domain POS tagger (Toutanova et al., 2003), providing important temporal relation extraction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jia"
C18-1291,S13-2001,0,0.0932141,"Missing"
C18-1291,D16-1023,0,0.0176645,"al., 2003), providing important temporal relation extraction cues, such as tense shifts (Derczynski, 2017), and for which training data are available for many languages (Petrov et al., 2012). 2.2 Multi-task Learning Our proposed model training can be seen as multi-task learning (MTL), where the aim is to improve model generalization by leveraging the information from training signals of different related tasks (Caruana, 1998). In earlier work, MTL has shown to be quite effective for different NLP tasks such as machine translation (Dong et al., 2015), sentiment analysis (Peng and Dredze, 2015; Yu and Jiang, 2016), sentence level name prediction (Cheng et al., 2015), semantic role labeling (Collobert and Weston, 2008), and many more. For example, Collobert and Weston (2008) used an auxiliary unsupervised objective for semantic role labeling (SRL). They alternately trained embeddings in a language model and a SRL model. In contrast to their work, we learn both tasks truly jointly, and optimize a single semi-supervised objective. Typically in neural MTL, one or more layers of the network are shared among different models. Two issues in MTL are (1) how to determine if the tasks are related enough to benef"
C18-1291,C14-1220,0,0.274724,"ks, such as the i2b2 shared task on clinical temporal information (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural models with dedicated clinical NLP tools for preprocessing the clinical texts, often using the English cTAKES system (Savova et al., 2010), which contains tools for clinical POS tagging, named entity recognition, and a dependency parser all trained on clinical data. The main reason for using these dedicated clinical tools is that parsers trained on non-clinical texts perform significantly worse on clinical data (Jiang et al"
C18-1291,Y15-1009,0,0.0172511,"ed to a word embedding, from which the probability distribution y over its surrounding context words wj−c , ..., wj−1 , wj+1 , ..., wj+c is predicted, given a context window size c. The full model is given by Eq. 3. token pˆsg (wj ) = sof tmax(Wpsg (wj · Wem ) + bpsg ) (3) Like the RC model, we use cross-entropy loss for our SG model, as shown in Eq. 4. Dsg indicates the unsupervised dataset, consisting of words and their contexts. θsg is the collection of all trainable parameters of our model. |Dsg | sg Lsg (θ ) = − X yi log pˆsg (wi ) (4) i=1 1 We also experimented with bidirectional LSTMs (Zhang et al., 2015) and adding attention (Zhou et al., 2016). In our experiments, this did not result in significant improvements. 3439 3.2.1 Separate Left & Right Context (SGLR) The skip-gram model is quite rough in its context description and does not take into account word order very well. However, for temporal relations we expect word order to be relevant. For this reason, we also experimented with a variation on the skip-gram model, separating the left and right context, following the intuition of Ling et al. (2015). The context separation is achieved by extending the context words by a ‘left’ or ‘right’ pr"
C18-1291,P16-2034,0,0.0999466,"cal temporal information (Sun et al., 2013), and three iterations of Clinical TempEval (Bethard et al., 2015; Bethard et al., 2016; Bethard et al., 2017). Until recently, most of the top performing systems employed manually constructed linguistic feature sets (Lin et al., 2015; Lee et al., 2016; Leeuwenberg and Moens, 2017). In the last few years, there has been a shift towards using neural models, using LSTM (Tourille et al., 2017) and CNN models (Dligach et al., 2017; Lin et al., 2017) inspired by the work on relation classification in other domains (Zeng et al., 2014; Zhang and Wang, 2015; Zhou et al., 2016; Nguyen and Grishman, 2015). The top results in clinical temporal relation extraction are still achieved when enhancing the neural models with dedicated clinical NLP tools for preprocessing the clinical texts, often using the English cTAKES system (Savova et al., 2010), which contains tools for clinical POS tagging, named entity recognition, and a dependency parser all trained on clinical data. The main reason for using these dedicated clinical tools is that parsers trained on non-clinical texts perform significantly worse on clinical data (Jiang et al., 2015). 3437 Dedicated clinical NLP too"
C18-2035,W09-1206,0,0.106269,"Missing"
C18-2035,P17-1044,0,0.197368,"n has received a central interest in the natural language processing (NLP) community. Semantic role labeling, which is a sentence-level semantic task aimed at identifying “Who did What to Whom, and How, When and Where?” (Palmer et al., 2010), has strengthened this focus. Recently, several neural mechanisms have been used to train end-to-end SRL models that do not require task-specific feature engineering as the traditional SRL models do. Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, Tan et al. (2018) replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, language-specific characteristics and the available amount of training data highly influence the optimal model structure. DAMESRL facilitates exploration and fai"
C18-2035,P15-1109,0,0.179967,"pache 2.0 license. 1 Introduction During the first decade of the 21st century, mapping from the syntactic analysis of a sentence to its semantic representation has received a central interest in the natural language processing (NLP) community. Semantic role labeling, which is a sentence-level semantic task aimed at identifying “Who did What to Whom, and How, When and Where?” (Palmer et al., 2010), has strengthened this focus. Recently, several neural mechanisms have been used to train end-to-end SRL models that do not require task-specific feature engineering as the traditional SRL models do. Zhou and Xu (2015) introduced the first deep end-to-end model for SRL using a stacked Bi-LSTM network with a conditional random field (CRF) as the top layer. He et al. (2017) simplified their architecture using a highway Bi-LSTM network. More recently, Tan et al. (2018) replaced the common recurrent architecture with a self-attention network, directly capturing relationships between tokens regardless of their distance, resulting in better results and faster training. The work in deep end-to-end SRL has focused heavily on applying deep learning advances without considering the multilingual aspect. However, langu"
D09-1003,W04-3213,0,\N,Missing
D09-1003,nivre-etal-2006-maltparser,0,\N,Missing
D09-1003,W96-0213,0,\N,Missing
D09-1003,E09-1026,0,\N,Missing
D09-1003,W04-3212,0,\N,Missing
D09-1003,J96-1002,0,\N,Missing
D09-1003,P98-1013,0,\N,Missing
D09-1003,C98-1013,0,\N,Missing
D09-1003,J92-4003,0,\N,Missing
D09-1003,P97-1009,0,\N,Missing
D09-1003,P08-1068,0,\N,Missing
D09-1003,P06-4020,0,\N,Missing
D09-1003,W08-2123,0,\N,Missing
D09-1003,W06-1606,0,\N,Missing
D09-1003,P03-1002,0,\N,Missing
D09-1003,J02-3001,0,\N,Missing
D09-1003,J05-1004,0,\N,Missing
D09-1003,P98-2127,0,\N,Missing
D09-1003,C98-2122,0,\N,Missing
D09-1003,W04-2419,0,\N,Missing
D09-1003,N04-1030,0,\N,Missing
D09-1003,M98-1001,0,\N,Missing
D13-1168,J10-4006,0,0.0293563,"le 2: IT-EN: Results with different sizes of the seed lexicon. The number in the parentheses denotes the number of dimensions in the bilingual space after the bootstrapping procedure converges. The seeding method is SEED-RB. are therefore not always able to push the real crosslingual synonyms higher in the ranked list of semantically similar words, while the window-based bootstrapping approach is better tailored to model the relation of cross-lingual synonymy, i.e., to extract one-to-one translation pairs (as reflected in Acc1 scores). A similar conclusion for monolingual settings is drawn by Baroni and Lenci (2010). (iv) Since our bootstrapping approach utilizes ResponseBC or TopicBC as a preprocessing step, it is obvious that the approach leads to an increased complexity. On top of the initial complexity of ResponseBC and TopicBC, the bootstrapping method requires |V S ||V T |comparisons at each iteration, but given the fact that each wiS ∈ V S may be processed independently of any other wjS ∈ V S in each iteration, the bootstrapping method is trivially parallelizable. That makes the method computationally feasible even for vocabularies larger than the ones reported in the paper. 1621 4.2 Is Confidence"
D13-1168,P11-1061,0,0.0250393,"retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context feat"
D13-1168,P11-2071,0,0.0295884,"Missing"
D13-1168,D12-1001,0,0.0292423,"le source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional v"
D13-1168,R11-1018,0,0.0377545,"Missing"
D13-1168,W04-3208,0,0.0640861,"er to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a sm"
D13-1168,P98-1069,0,0.0380592,"ch as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context features. The context features are typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck"
D13-1168,P04-1067,0,0.252236,"Missing"
D13-1168,P08-1088,0,0.793174,"ational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon. The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn and Knight, 2002; Peirsman and Pad´o, 2010), and it spans the initial bilingual vector space. The space is then gradually enriched with new dimensions/axes during the bootstrapping procedure. The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as S"
D13-1168,E12-1029,0,0.0130607,"constraint should ensure a relative reliability of translation pairs. In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs. We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg. 1), and choose only the best B candidates from the pool in each iteration (step 4) as done in (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Huang and Riloff, 2012). By picking only a subset of the B most confident candidates in each iteration, we hope to further prevent a possibility of semantic drift, i.e., “poisoning” the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space. In this paper, we investigate 3 different confidence estimation functions:3 (1) Absolute similarity score. Confidence of a translation pair CF (wiS , T C(wiS )) is simply the absolute similarity value sim(wiS , T C(wiS )) (2) M-Best confidence function. It contrasts the score of the translation candidate with the average scor"
D13-1168,C10-2055,0,0.0142854,"representations in the N -dimensional bilingual vector space. The cross-lingual similarity is computed following the standard procedure (Gaussier et al., 2004): (1) For each source word wiS ∈ V S , build its N dimensional context vector cv(wiS ) that consists of association scores scSk (cSk ), that is, we compute the strength of association with the “source” part of each dimension ck that constitutes the N -dimensional bilingual space. The association is dependent on the co-occurrence of wiS and cSk in a predefined context. Various functions such as the log-likelihood ratio (LLR) (Rapp, 1999; Ismail and Manandhar, 2010), TF-IDF (Fung and Yee, 1998), or pointwise mutual information (PMI) (Bullinaria and Levy, 2007; Shezaf and Rappoport, 2010) are typically used as weighting functions to quantify the strength of the association. (2) Repeat step (1) for each target word wjT ∈ V T and build context vectors cv(wjT ) that consist of scores scTk (cTk ). (3) Since cSk and cTk address the same dimension ck in the bilingual vector space for each k = 1, . . . , N , we are able to compute the similarity between cv(wiS ) and cv(wjT ) using any similarity measure such as the Jaccard index, the Kullback-Leibler or the Jens"
D13-1168,P10-1026,0,0.0200566,"Missing"
D13-1168,W02-0902,0,0.221947,"sume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore, the focus of the researchers has turned to designing BLE methods that do not rely on any external translation resources such as machine-readable bilingual lexicons and parallel corpora (Haghighi et al., 2008; Vuli´c et al., 2011). In order to circumvent this issue, one line of recent work aims to bootstrap high-quality bilingual vector spaces from a small initial seed lexicon. The seed lexicon is constructed by harvesting identical or similarly spelled words across languages (Koehn and Knight, 2002; Peirsman and Pad´o, 2010), and it spans the initial bilingual vector space. The space is then gradually enriched with new dimensions/axes during the bootstrapping procedure. The bootstrapping process has already proven its validity in inducing bilingual lexicons for closely similar languages such as Spanish-Portuguese or Croatian-Slovene (Fiˇser and Ljubeˇsi´c, 2011), but it still lacks further generalization to more distant language pairs. The main goal of this paper is to shed new light on the bootstrapping approaches to bilingual lexicon extraction, and to construct a language pair agnost"
D13-1168,N10-1087,0,0.0157337,"at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones. For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter. 2.2 Initializing Bilingual Vector Spaces frequent symmetric pairs as seeds. Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure. However, Peirsman and Pad´o (2011) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces. Here, we present two different strategies of initializing the bilingual vector space. Identical words and cognates. Previous work relies exclusively on identical and similarly spelled words to build the initial set of dimensions Z0 (Koehn and Knight, 2002; Peirsman and Pad´o, 2010; Fiˇser and Ljubeˇsi´c, 2011). This strategy yields promising results for closely similar lang"
D13-1168,C10-1070,0,0.0141025,"ck-Leibler, Jensen-Shannon) (Lee, 1999; Turney and Pantel, 2010). In this paper, we present results obtained by positive pointwise mutual information (PPMI) (Niwa and Nitta, 1994) as a weighting function, which is a standard choice in vector space semantics (Turney and Pantel, 2010), and (combined with cosine) yields the best results over a group of semantic tasks according to (Bullinaria and Levy, 2007). We use a smoothed version of PPMI as presented in (Pantel and Lin, 2002; Turney and Pantel, 2010). Again, based on the results reported in the relevant literature (Bullinaria and Levy, 2007; Laroche and Langlais, 2010; Turney and Pantel, 2010), we opt for the cosine similarity as a standard choice for SF . We have also experimented with different window sizes ranging from 3 to 15 in both directions around the pivot word, but we have not detected any major qualitative difference in the results and their interpretation. Therefore, all results reported in the paper are obtained by setting the window size to 6. 4 4.1 Results and Discussion Are Seeds Important? In recent work, Peirsman and Pad´o (2010; 2011) report that “the size and quality of the (seed) lex1618 icon are not of primary importance given that th"
D13-1168,P99-1004,0,0.0935525,"typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors (Lee, 1999). When operating with 2 languages, the context features cannot be compared directly. Therefore, in order to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. T"
D13-1168,P09-1045,0,0.0606458,"y of the space to increase at each stage of the bootstrapping process, and newer translation pairs should be more confident than the older ones. For instance, if 2 out of N dimensions of a Spanish-English bilingual space are pairs (piedra,wall) and (tapia,stone), but then if during the bootstrapping process we extract a new candidate pair (piedra,stone), we will delete the former two dimensions and add the latter. 2.2 Initializing Bilingual Vector Spaces frequent symmetric pairs as seeds. Seeding or initializing a bootstrapping procedure is often a critical step regardless of the actual task (McIntosh and Curran, 2009; Kozareva and Hovy, 2010), and it decides whether the complete process will end as a success or a failure. However, Peirsman and Pad´o (2011) argue that the initialization step is not crucial when dealing with bootstrapping bilingual vector spaces. Here, we present two different strategies of initializing the bilingual vector space. Identical words and cognates. Previous work relies exclusively on identical and similarly spelled words to build the initial set of dimensions Z0 (Koehn and Knight, 2002; Peirsman and Pad´o, 2010; Fiˇser and Ljubeˇsi´c, 2011). This strategy yields promising result"
D13-1168,J00-2004,0,0.0133663,"mework We assume that we are solely in possession of a (non-parallel) bilingual corpus C that is composed of a sub-corpus CS given in the source language LS , and a sub-corpus CT in the target language LT . All word types that occur in CS constitute a set V S . All word types in CT constitute a set V T . The goal is to build a bilingual vector space using only corpus C. Assumption 1. Dimensions of the bilingual vector space are one-to-one word translation pairs. For instance, dimensions of a Spanish-English space are pairs like (perro, dog), (ciencia, science), etc. The one-to-one constraint (Melamed, 2000), although not valid in general, simplifies the construction of the bootstrapping procedure. Z denotes the set of translation pairs that are the dimensions of the space. Computing cross-lingual word similarity in a bilingual vector space. Now, assume that our bilingual vector space consists of N one-to-one word translation pairs ck = (cSk , cTk ), k = 1, . . . , N . For each word wiS ∈ V S , we compute the similarity of that word with each word wjT ∈ V T by computing the similarity between their context vectors cv(wiS ) and cv(wjT ), which are actually their representations in the N -dimension"
D13-1168,D09-1092,0,0.173224,"of limited use for other language pairs. High-frequency seeds. Another problem with using only identical words and cognates as seeds lies in the fact that many of them might be infrequent in the corpus, and as a consequence the expressiveness of a bilingual vector space might be limited. On the other hand, high-frequency words offer a lot of evidence in the corpus that could be exploited in the bootstrapping approach. In order to induce initial translation pairs, we rely on the framework of multilingual probabilistic topic modeling (MuPTM) (BoydGraber and Blei, 2009; De Smet and Moens, 2009; Mimno et al., 2009; Zhang et al., 2010), that does not require a bilingual lexicon, it operates with nonparallel data, and is able to produce highly confident translation pairs for high-frequency words (Mimno et al., 2009; Vuli´c and Moens, 2013).2 Therefore, we can construct the initial seed lexicon as follows: (1) Train a multilingual topic model on the corpus. (2) Obtain one-to-one translation pairs using any of the MuPTM-based models of cross-lingual similarity, e.g., (Vuli´c et al., 2011; Vuli´c and Moens, 2013). (3) Retain only symmetric translation pairs. This step ensures that only highly confident pair"
D13-1168,C94-1049,0,0.138401,"ails about the methods in the relevant literature. These two models also serve as our baseline models, and our goal is to test whether we are able to obtain bilingual lexicons of higher quality using bootstrapping that starts from the output of these models. Weighting and similarity functions. We have experimented with different families of weighting (e.g., PMI, LLR, TF-IDF, chi-square) and similarity functions (e.g., cosine, Dice, Kullback-Leibler, Jensen-Shannon) (Lee, 1999; Turney and Pantel, 2010). In this paper, we present results obtained by positive pointwise mutual information (PPMI) (Niwa and Nitta, 1994) as a weighting function, which is a standard choice in vector space semantics (Turney and Pantel, 2010), and (combined with cosine) yields the best results over a group of semantic tasks according to (Bullinaria and Levy, 2007). We use a smoothed version of PPMI as presented in (Pantel and Lin, 2002; Turney and Pantel, 2010). Again, based on the results reported in the relevant literature (Bullinaria and Levy, 2007; Laroche and Langlais, 2010; Turney and Pantel, 2010), we opt for the cosine similarity as a standard choice for SF . We have also experimented with different window sizes ranging"
D13-1168,J03-1002,0,0.0115179,"ping approach in the task of bilingual lexicon extraction (BLE) for two language pairs. Results reveal that, contrary to conclusions from prior work, the seeding of the bootstrapping process has a heavy impact on the quality of the learned lexicons. We also show that our approach outperforms the best performing fully corpus-based BLE methods on these test sets. 1 Introduction Bilingual lexicons serve as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction"
D13-1168,N10-1135,0,0.221083,"Missing"
D13-1168,P11-1133,0,0.0215868,"idence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki-ES-EN), (ii) 18, 898 Italian-English Wikipedia article pairs (Wiki-IT-EN).4 Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we use TreeTagger (Schmid, 1994) for POS-tagging and lemmatization of the corpora, and then retain only nouns that occur at least 5 times in the corpus. We record the lemmatized form when available, and the original form otherwise. Our final vocabularies consist of 9, 439 Spanish nouns and 4 Vuli´c and Moens (2013) also worked with Dutch-English (NL-EN), but we have decided to leave out the results obtained for that language pair due to space constraints, high similarity between the two languages, and the fact that the results obtained for that language pair are qual"
D13-1168,P99-1067,0,0.108932,"abeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vector are its context features. The context features are typically words co-occurring with the word in a predefined context.1 The similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then computed as sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )). cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] is a context vector for w1S with N context features ck , where scS1"
D13-1168,P10-1011,0,0.0331304,"d procedure (Gaussier et al., 2004): (1) For each source word wiS ∈ V S , build its N dimensional context vector cv(wiS ) that consists of association scores scSk (cSk ), that is, we compute the strength of association with the “source” part of each dimension ck that constitutes the N -dimensional bilingual space. The association is dependent on the co-occurrence of wiS and cSk in a predefined context. Various functions such as the log-likelihood ratio (LLR) (Rapp, 1999; Ismail and Manandhar, 2010), TF-IDF (Fung and Yee, 1998), or pointwise mutual information (PMI) (Bullinaria and Levy, 2007; Shezaf and Rappoport, 2010) are typically used as weighting functions to quantify the strength of the association. (2) Repeat step (1) for each target word wjT ∈ V T and build context vectors cv(wjT ) that consist of scores scTk (cTk ). (3) Since cSk and cTk address the same dimension ck in the bilingual vector space for each k = 1, . . . , N , we are able to compute the similarity between cv(wiS ) and cv(wjT ) using any similarity measure such as the Jaccard index, the Kullback-Leibler or the Jensen-Shannon divergence, the cosine measure, or others (Lee, 1999; Cha, 2007). The similarity score for two words wiS and wjT"
D13-1168,D07-1070,0,0.0610707,"r words in the ranked list. The larger the difference, the more confidence we have in the translation candidate. Given a word wiS ∈ V S and a ranked list RLM (wiS ), the 3 A symmetrized version of the confidence functions is computed as the geometric mean of source-to-target and target-tosource confidence scores. average score of the best M words is computed as: simM (wiS ) = 1 M X sim(wiS , wjT ) wjT ∈RLM (wiS ) The final confidence score is then: CF (wiS , T C(wiS )) = sim(wiS , T C(wiS )) − simM (wiS ) (3) Entropy-based confidence function. We adapt the well-known entropy-based confidence (Smith and Eisner, 2007; Tu and Honavar, 2012) to this particular task. First, we need to define a distribution: S p(wjT |wiS ) =P T esim(wi ,wj ) S wlT ∈V T T) esim(wi ,wl The confidence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipe"
D13-1168,Q13-1001,0,0.0493112,"Missing"
D13-1168,N13-1126,0,0.00893387,"Missing"
D13-1168,D12-1003,0,0.289249,"features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler divergence, the Jaccard index) operating on the context vectors (Lee, 1999). When operating with 2 languages, the context features cannot be compared directly. Therefore, in order to compare the feature vectors cv(w1S ) and cv(w2T ), the context features need to span a shared 1 The context may be a document, a paragraph, a window of predefined size around each occurrence of wiS in CS , etc. For an overview, see, e.g., (Tamura et al., 2012). 1613 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1613–1624, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics bilingual vector space. The standard way of building a bilingual vector space is to use bilingual lexicon entries (Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004) as dimensions of the space. However, there seems to be an apparent flaw in logic, since the methods assume that there exist readily available bilingual lexicons that are then used to induce bilingual lexicons! Therefore,"
D13-1168,W02-1028,0,0.0291222,"T C(wiS ) = wjT and T C(wjT ) = wiS . This symmetry constraint should ensure a relative reliability of translation pairs. In each iteration of the bootstrapping process, we may add all symmetric pairs from the pool of candidates as new dimensions, or we could impose additional selection criteria that quantify the degree of confidence in translation pairs. We are then able to rank the symmetric candidate translation pairs in the pool of candidates according to their confidence scores (step 3 of alg. 1), and choose only the best B candidates from the pool in each iteration (step 4) as done in (Thelen and Riloff, 2002; McIntosh and Curran, 2009; Huang and Riloff, 2012). By picking only a subset of the B most confident candidates in each iteration, we hope to further prevent a possibility of semantic drift, i.e., “poisoning” the bootstrapping process that might happen if we include incorrect translation pairs as dimensions of the space. In this paper, we investigate 3 different confidence estimation functions:3 (1) Absolute similarity score. Confidence of a translation pair CF (wiS , T C(wiS )) is simply the absolute similarity value sim(wiS , T C(wiS )) (2) M-Best confidence function. It contrasts the scor"
D13-1168,D12-1121,0,0.014041,"st. The larger the difference, the more confidence we have in the translation candidate. Given a word wiS ∈ V S and a ranked list RLM (wiS ), the 3 A symmetrized version of the confidence functions is computed as the geometric mean of source-to-target and target-tosource confidence scores. average score of the best M words is computed as: simM (wiS ) = 1 M X sim(wiS , wjT ) wjT ∈RLM (wiS ) The final confidence score is then: CF (wiS , T C(wiS )) = sim(wiS , T C(wiS )) − simM (wiS ) (3) Entropy-based confidence function. We adapt the well-known entropy-based confidence (Smith and Eisner, 2007; Tu and Honavar, 2012) to this particular task. First, we need to define a distribution: S p(wjT |wiS ) =P T esim(wi ,wj ) S wlT ∈V T T) esim(wi ,wl The confidence function is then minus the entropy of the probability distribution p: CF (wiS , T C(wiS )) = X p(wlT |wiS ) log p(wlT |wiS ) wlT ∈V T 3 Experimental Setup Data collections. We investigate our bootstrapping approach on the BLE task for 2 language pairs: Spanish-English (ES-EN) and Italian-English (ITEN), and work with the following corpora previously used by Vuli´c and Moens (2013): (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki"
D13-1168,P11-2052,0,0.0339269,"Missing"
D13-1168,N13-1011,1,0.838398,"Missing"
D13-1168,P11-2084,1,0.789296,"Missing"
D13-1168,N01-1026,0,0.0748191,"cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word by a high-dimensional vector in a feature vector space, where the dimensions of the vecto"
D13-1168,P10-1115,0,0.0488981,"Missing"
D13-1168,P09-1007,0,0.010773,"e as an indispensable source of knowledge for various cross-lingual tasks such as cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005) or statistical machine translation (Och and Ney, 2003). Additionally, they are a crucial component in cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a), etc. Techniques for automatic bilingual lexicon extraction (BLE) from parallel corpora on the basis of word alignment models are well established (Och and Ney, 2003). However, due to a relative scarceness of parallel data for many language pairs and domains, alternative approaches that rely on comparable corpora have also gained much interest (e.g., Fung and Yee (1998); Rapp (1999)). The models that rely on non-parallel data typically represent each word b"
D13-1168,C98-1066,0,\N,Missing
D14-1040,J12-1002,0,0.0134765,"rm way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context"
D14-1040,P11-1061,0,0.0131515,"nd Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these mode"
D14-1040,W11-2203,0,0.0217513,"n the Spanish sentence ”She was unable to find a match in her pocket to light up a cigarette.”, it is clear that the strength of semantic similarity should change in context as only cerilla exhibits a strong semantic similarity to match within this particular sentential context. Following this intuition, in this paper we investigate models of cross-lingual semantic similarity in context. The context-sensitive models of similarity target to re-rank the lists of semantically similar words based on the co-occurring contexts of words. Unlike prior work (e.g., (Ng et al., 2003; Prior et al., 2011; Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cross-lingual concepts (Haghighi et al.,"
D14-1040,P11-2071,0,0.0540797,"Missing"
D14-1040,D10-1115,0,0.0326918,"hen compute the similarity between words and sets of words given in the same latent semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk"
D14-1040,D12-1050,0,0.0166891,"semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to p"
D14-1040,D10-1113,0,0.0535594,", and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic space spanned by latent cross-lingual concepts in both monolingual (Dinu and Lapata, 2010a; Dinu and Lapata, 2010b) and multilingual settings (Vuli´c et al., 2011) typically proceed in the following manner. Latent language-independent concepts (e.g., cross-lingual topics or latent word senses) are estimated on a large corpus. The K-dimensional vector representation of the word w1S ∈ V S is: vec(w1S ) = [P (z1 |w1S ), . . . , P (zK |w1S )] By using all words occurring with w1S in a context set (e.g., a sentence) to build the set Con(w1S ), we do not make any distinction between “informative and “uninformative” context words. However, some context words bear more contextual informat"
D14-1040,C10-2029,0,0.19433,", and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic space spanned by latent cross-lingual concepts in both monolingual (Dinu and Lapata, 2010a; Dinu and Lapata, 2010b) and multilingual settings (Vuli´c et al., 2011) typically proceed in the following manner. Latent language-independent concepts (e.g., cross-lingual topics or latent word senses) are estimated on a large corpus. The K-dimensional vector representation of the word w1S ∈ V S is: vec(w1S ) = [P (z1 |w1S ), . . . , P (zK |w1S )] By using all words occurring with w1S in a context set (e.g., a sentence) to build the set Con(w1S ), we do not make any distinction between “informative and “uninformative” context words. However, some context words bear more contextual informat"
D14-1040,J93-2003,0,0.070668,"oven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in the target language in isolation or invariably, that is, they do not explicitly idenWe propose the first probabilistic approach to modeling cross-lingual semantic similarity (CLSS) in context which requires only comparable data. The approach relies on an idea of projecting words an"
D14-1040,D12-1001,0,0.0145737,"S models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingua"
D14-1040,P14-1006,0,0.0280069,"e properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent semantic space (and again assume the uniform topic prior P"
D14-1040,D13-1205,0,0.0129757,"models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ran"
D14-1040,P12-1092,0,0.0246423,"istic topic models output probability scores P (wiS |zk ) and P (wjT |zk ) for each wiS ∈ V S and wjT ∈ V T and each zk ∈ P S Z, and it holds wiS ∈V S P (wi |zk ) = 1 and P T wT ∈V T P (wj |zk ) = 1. The scores are then Defining Context. Given an occurrence of a word w1S , we build its context set Con(w1S ) = {cw1S , . . . , cwrS } that comprises r words from V S that co-occur with w1S in a defined contextual scope or granularity. In this work we do not investigate the influence of the context scope (e.g., document-based, paragraph-based, window-based contexts). Following the recent work from Huang et al. (2012) in the monolingual setting, we limit the contextual scope to the sentential context. However, we emphasize that the proposed models are designed to be fully functional regardless of the actual chosen context granularity. e.g., when operating in the sentential context, Con(w1S ) consists of words occurring in the same sentence with the particular instance of w1S . Following Mitchell and Lapata (2008), for the sake of simplicity, we impose the bag-of-words assumption, and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Inves"
D14-1040,P04-1067,0,0.161912,"Missing"
D14-1040,J00-4006,0,0.0197136,"to their respective similarity scores and the best scoring candidate may be selected as the best translation of an occurrence of the word w1S given its local context. Since the contextual knowledge is integrated directly into the estimation of probability P (zk |w1S , Con(w1S )), we name this context-aware CLSS model the Direct-Fusion model. Model II: Smoothed-Fusion. The next model follows the modeling paradigm established within the framework of language modeling (LM), where the idea is to “back off” to a lower order Ngram in case we do not possess any evidence about a higher-order N-gram (Jurafsky and Martin, 2000). The idea now is to smooth the representation of a word in the latent semantic space induced only by the words in its local context with the out-of-context type-based representation of that word induced directly from a large training corpus. In other words, the modulated probability score P 0 (zk |w1S ) from eq. (5) is calculated as: vec(w1S , Con(w1S )) = [P 0 (z1 |w1S ), . . . , P 0 (zK |w1S )] (5) where P 0 (zK |w1S ) denotes the recalculated (or modulated) probability score for the conditional concept/topic distribution of w1S after observing its context Con(w1S ). For an illustration of"
D14-1040,P10-1026,0,0.0441824,"Missing"
D14-1040,D11-1129,0,0.0492216,"Missing"
D14-1040,P12-1073,0,0.0127399,"ne language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in the target language in isolation or invariably, that is, they do not explicitly idenWe propose the first"
D14-1040,C12-1089,0,0.0226263,"ence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent semantic space (and again assume the uniform topic prior P (zk )), we finally obtain the following formula: text units beyond words is similar to (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), but unlike them, we do not need high-quality sentence-aligned parallel data to induce bilingual text representations. Moreover, this work on compositionality in multilingual settings is only preliminary (e.g., we treat phrases and sentences as bags-of-words), and in future work we will aim to include syntactic information in the composition models as already done in monolingual settings (Socher et al., 2012; Hermann and Blunsom, 2013). Intuition behind the Approach. Going back to our novel CLSS models in context, these models rely on the re"
D14-1040,P08-1088,0,0.27697,"Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cross-lingual concepts (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011; Vuli´c and Moens, 2013) induced from non-parallel data. While all the previous models in this framework are context-insensitive models of semantic similarity, we demonstrate how to build context-aware models of semantic similarity within the same probabilistic framework which relies on the same shared set of latent concepts. The main contributions of this paper are: 2 Towards Cross-Lingual Semantic Similarity in Context Latent Cross-Lingual Concepts. Latent crosslingual concepts/senses may be interpreted as language-independent semantic"
D14-1040,W02-0902,0,0.0220362,"sh, Italian and Dutch from our test set accompanied by the sets of their respective possible senses/translations in English. All corpora are theme-aligned comparable corpora, i.e, the aligned document pairs discuss similar themes, but are in general not direct translations (except for Europarl). By training on Wiki+EP-NL-EN we want to test how the training corpus of higher quality affects the estimation of latent cross-lingual concepts that span the shared latent semantic space and, consequently, the overall results in the task of suggesting word translations in context. Following prior work (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we retain only nouns that occur at least 5 times in the corpus. We record lemmatized word forms when available, and original forms otherwise. We use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in English (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible translations) in each of the"
D14-1040,D09-1124,0,0.0285564,"similar to the model from O and Korhonen (2011) in the monolingual setting, one may try to introduce dependency-based contexts (Pad´o and Lapata, 2007) and incorporate the syntax-based knowledge in the context-aware CLSS modeling. It is also worth studying other models that induce latent semantic concepts from multilingual data (see sect. 2) within this framework of context-sensitive CLSS modeling. One may also investigate a similar approach to contextsensitive CLSS modeling that could operate with explicitly defined concept categories (Gabrilovich and Markovitch, 2007; Cimiano et al., 2009; Hassan and Mihalcea, 2009; Hassan and Mihalcea, 2011; McCrae et al., 2013). Acc1 0.75 0.7 0.65 ES-EN IT-EN NL-EN (Wiki) 0.6 NL-EN (Wiki+EP) 0.55 1 2 3 4 5 6 7 8 9 10 11 All Size of the ranked context Figure 2: The influence of the size of sorted context on the accuracy of word translation in context. The model is Cue+Smoothed-Fusion. also investigate the utility of context sorting and pruning, and its influence on the overall results in our evaluation task. Therefore, we have conducted experiments with sorted context sets that were pruned at different positions, ranging from 1 (only the most similar word to w1S in a s"
D14-1040,2005.mtsummit-papers.11,0,0.0517389,"st is then the suggested correct translation for that particular occurrence of w1S after observing its local context Con(w1S ). Training Data. We use the following corpora for inducing latent cross-lingual concepts/topics, i.e., for training our multilingual topic model: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (Wiki-ES-EN), (ii) a collection of 18, 898 Italian-English Wikipedia article pairs, (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (Wiki-NL-EN), and (iv) the Wiki-NLEN corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005) (Wiki+EP-NL-EN). The corpora were previously used in (Vuli´c and Moens, 2013). No explicit use is made of sentence-level alignments in Europarl. (10) where λ2 is the interpolation parameter. Since this model computes the similarity with each target word separately for the source word in isolation and its local context, and combines the ob354 Sentence in Italian 1. I primi calci furono prodotti in legno ma recentemente... 2. In caso di osteoporosi si verifica un eccesso di rilascio di calcio dallo scheletro... 3. La crescita del calcio femminile professionistico ha visto il lancio di competizi"
D14-1040,P14-2037,0,0.0837968,"Missing"
D14-1040,P13-1088,0,0.0147807,"uniform topic prior P (zk )), we finally obtain the following formula: text units beyond words is similar to (Klementiev et al., 2012; Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), but unlike them, we do not need high-quality sentence-aligned parallel data to induce bilingual text representations. Moreover, this work on compositionality in multilingual settings is only preliminary (e.g., we treat phrases and sentences as bags-of-words), and in future work we will aim to include syntactic information in the composition models as already done in monolingual settings (Socher et al., 2012; Hermann and Blunsom, 2013). Intuition behind the Approach. Going back to our novel CLSS models in context, these models rely on the representations of words and their contexts in the same latent semantic space spanned by latent cross-lingual concepts/topics. The models differ in the way the contextual knowledge is fused with the out-of-context word representations. The key idea behind these models is to represent a word w1S in the latent semantic space as a distribution over the latent cross-lingual concepts, but now with an additional modulation of the representation after taking its local context into account. The mo"
D14-1040,D11-1097,0,0.0397802,"Missing"
D14-1040,J03-1002,0,0.0118243,"pts Induced from Comparable Data Ivan Vuli´c and Marie-Francine Moens Department of Computer Science KU Leuven, Belgium {ivan.vulic|marie-francine.moens}@cs.kuleuven.be Abstract output of the CLSS models is a key resource in the models of dictionary-based cross-lingual information retrieval (Ballesteros and Croft, 1997; Lavrenko et al., 2002; Levow et al., 2005; Wang and Oard, 2006) or may be utilized in query expansion in cross-lingual IR models (Adriani and van Rijsbergen, 1999; Vuli´c et al., 2013). These CLSS models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petr"
D14-1040,J07-2002,0,0.0161936,"Missing"
D14-1040,P99-1004,0,0.0211846,"modulation of the representation after taking its local context into account. The modulated word representation in the semantic space spanned by K latent cross-lingual concepts is then: Q P (w1S |zk ) rj=1 P (cwjS |zk ) P 0 (zk |w1S ) ≈ PK Qr S S j=1 P (cwj |zl ) l=1 P (w1 |zl ) The ranking of all words w2T ∈ V T according to their similarity to w1S may be computed by detecting the similarity score between their representation in the K-dimensional latent semantic space and the modulated source word representation as given by eq. (5) and eq. (7) using any of the existing similarity functions (Lee, 1999; Cha, 2007). The similarity score Sim(w1S , w2T , Con(w1S )) between some w2T ∈ V T represented by its vector vec(w2T ) and the observed word w1S given its context Con(w1S ) is computed as: sim(w1S , w2T , Con(w1S ))    = SF vec w1S , Con(w1S ) , vec w2T (8) where SF denotes a similarity function. Words are then ranked according to their respective similarity scores and the best scoring candidate may be selected as the best translation of an occurrence of the word w1S given its local context. Since the contextual knowledge is integrated directly into the estimation of probability P (zk |w"
D14-1040,N10-1135,0,0.184797,"Missing"
D14-1040,D13-1179,0,0.0224558,"e monolingual setting, one may try to introduce dependency-based contexts (Pad´o and Lapata, 2007) and incorporate the syntax-based knowledge in the context-aware CLSS modeling. It is also worth studying other models that induce latent semantic concepts from multilingual data (see sect. 2) within this framework of context-sensitive CLSS modeling. One may also investigate a similar approach to contextsensitive CLSS modeling that could operate with explicitly defined concept categories (Gabrilovich and Markovitch, 2007; Cimiano et al., 2009; Hassan and Mihalcea, 2009; Hassan and Mihalcea, 2011; McCrae et al., 2013). Acc1 0.75 0.7 0.65 ES-EN IT-EN NL-EN (Wiki) 0.6 NL-EN (Wiki+EP) 0.55 1 2 3 4 5 6 7 8 9 10 11 All Size of the ranked context Figure 2: The influence of the size of sorted context on the accuracy of word translation in context. The model is Cue+Smoothed-Fusion. also investigate the utility of context sorting and pruning, and its influence on the overall results in our evaluation task. Therefore, we have conducted experiments with sorted context sets that were pruned at different positions, ranging from 1 (only the most similar word to w1S in a sentence is included in the context set Con(w1S ))"
D14-1040,P02-1027,0,0.0207223,"ingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention recently. All these models from parallel and comparable corpora provide ranked lists of semantically similar words in"
D14-1040,P11-1133,0,0.0202546,"nied by the sets of their respective possible senses/translations in English. All corpora are theme-aligned comparable corpora, i.e, the aligned document pairs discuss similar themes, but are in general not direct translations (except for Europarl). By training on Wiki+EP-NL-EN we want to test how the training corpus of higher quality affects the estimation of latent cross-lingual concepts that span the shared latent semantic space and, consequently, the overall results in the task of suggesting word translations in context. Following prior work (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013), we retain only nouns that occur at least 5 times in the corpus. We record lemmatized word forms when available, and original forms otherwise. We use TreeTagger (Schmid, 1994) for POS tagging and lemmatization. Test Data. We have constructed test datasets in Spanish (ES), Italian (IT) and Dutch (NL), where the aim is to find their correct translation in English (EN) given the sentential context. We have selected 15 polysemous nouns (see tab. 2 for the list of nouns along with their possible translations) in each of the 3 languages, and have manually extracted 24 sente"
D14-1040,D09-1092,0,0.527947,"Missing"
D14-1040,D10-1114,0,0.0419217,"Missing"
D14-1040,P08-1028,0,0.0688078,"fined contextual scope or granularity. In this work we do not investigate the influence of the context scope (e.g., document-based, paragraph-based, window-based contexts). Following the recent work from Huang et al. (2012) in the monolingual setting, we limit the contextual scope to the sentential context. However, we emphasize that the proposed models are designed to be fully functional regardless of the actual chosen context granularity. e.g., when operating in the sentential context, Con(w1S ) consists of words occurring in the same sentence with the particular instance of w1S . Following Mitchell and Lapata (2008), for the sake of simplicity, we impose the bag-of-words assumption, and do not take into account the order of words in the context set as well as context words’ dependency relations to w1S . Investigating different context types (e.g., dependency-based) is a subject of future work. j used to compute scores P (zk |wiS ) and P (zk |wjT ) in order to represent words from the two different languages in the same latent semantic space in a uniform way. Context-Insensitive Models of Similarity. Without observing any context, the standard models of semantic word similarity that rely on the semantic s"
D14-1040,P10-1093,0,0.0173152,", P (zK |Con(w1S ))] We can then compute the similarity between words and sets of words given in the same latent semantic space in a uniform way, irrespective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Sin"
D14-1040,P03-1058,0,0.0111603,"atch when observed in isolation, given the Spanish sentence ”She was unable to find a match in her pocket to light up a cigarette.”, it is clear that the strength of semantic similarity should change in context as only cerilla exhibits a strong semantic similarity to match within this particular sentential context. Following this intuition, in this paper we investigate models of cross-lingual semantic similarity in context. The context-sensitive models of similarity target to re-rank the lists of semantically similar words based on the co-occurring contexts of words. Unlike prior work (e.g., (Ng et al., 2003; Prior et al., 2011; Apidianaki, 2011)), we explore these models in a particularly difficult and minimalist setting that builds only on co-occurrence counts and latent cross-lingual semantic concepts induced directly from comparable corpora, and which does not rely on any other resource (e.g., machine-readable dictionaries, parallel corpora, explicit ontology and category knowledge). In that respect, the work reported in this paper extends the current research on purely statistical data-driven distributional models of cross-lingual semantic similarity that are built upon the idea of latent cr"
D14-1040,D12-1110,0,0.0822891,"ective of their actual language. We use all these properties when building our context-sensitive CLSS models. One remark: As a by-product of our modeling approach, by this procedure for computing representations for sets of words, we have in fact paved the way towards compositional cross-lingual models of similarity which rely on latent cross-lingual concepts. Similar to compositional models in monolingual settings (Mitchell and Lapata, 2010; Rudolph and Giesbrecht, 2010; Baroni and Zamparelli, 2010; Socher et al., 2011; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012; Clarke, 2012; Socher et al., 2012) and multilingual settings (Hermann and Blunsom, 2014; Koˇcisk´y et al., 2014), the representation of a set of words (e.g., a phrase or a sentence) is exactly the same as the representation of a single word; it is simply a K-dimensional real-valued vector. Our work on inducing structured representations of words and P (Con(w1S )|zk )P (zk ) P (Con(w1S )) P (cw1S , . . . , cwrS |zk )P (zk ) = PK S S l=1 P (cw1 , . . . , cwr |zl )P (zl ) (3) (2) 352 Since P (zk , w1S ) = P (w1S |zk )P (zk ), if we closely follow the derivation from eq. (3) which shows how to project context into the latent seman"
D14-1040,C08-1125,0,0.0541665,"Missing"
D14-1040,N01-1026,0,0.0133772,"dge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the models of cross-lingual similarity from comparable corpora have gained much attention re"
D14-1040,Q13-1001,0,0.025178,"Missing"
D14-1040,N13-1126,0,0.0221236,"Missing"
D14-1040,P10-1115,0,0.386221,"ng set and do not take into account any contextual information. They provide only out-of-context word representations and are therefore able to deliver only context-insensitive models of similarity. factorization (Lee and Seung, 1999; Gaussier and Goutte, 2005; Ding et al., 2008) on concatenated documents in aligned document pairs. Other more recent models include matching canonical correlation analysis (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011) and multilingual probabilistic topic models (Ni et al., 2009; De Smet and Moens, 2009; Mimno et al., 2009; Boyd-Graber and Blei, 2009; Zhang et al., 2010; Fukumasu et al., 2012). Due to its inherent language pair independent nature and state-of-the-art performance in the tasks such as bilingual lexicon extraction (Vuli´c et al., 2011) and cross-lingual information retrieval (Vuli´c et al., 2013), the description in this paper relies on the multilingual probabilistic topic modeling (MuPTM) framework. We draw a direct parallel between latent cross-lingual concepts and latent cross-lingual topics, and we present the framework from the MuPTM perspective, but the proposed framework is generic and allows the usage of all other models that are able t"
D14-1040,D12-1003,0,0.0126294,"suggesting word translations in context. Evaluation Procedure. Our task is to present the system a list of possible translations and let the system decide a single most likely translation given the word and its sentential context. Ground truth thus contains one word, that is, one correct translation for each sentence from the evaluation dataset. We have manually annotated the correct translation for the ground truth1 by inspecting the discourse in Wikipedia articles and the interlingual Wikipedia links. We measure the performance of all models as Top 1 accuracy (Acc1 ) (Gaussier et al., 2004; Tamura et al., 2012). It denotes the number of word instances from the evaluation dataset whose top proposed candidate in the ranked list of translation candidates from T is exactly the correct translation for that word instance as given by ground truth over the total number of test word instances (360 in each test dataset). Parameters. We have tuned λ1 and λ2 on the development sets. We set λ1 = λ2 = 0.9 for all language pairs. We use sorted context sets (see sect. 2) and perform a cut-off at M = 3 most descriptive context words in the sorted context sets for all models. In the following section we discuss the u"
D14-1040,P09-1007,0,0.013475,"., 2013). These CLSS models may also be utilized as an additional source of knowledge in SMT systems (Och and Ney, 2003; Wu et al., 2008). Additionally, the models are a crucial component in the crosslingual tasks involving a sort of cross-lingual knowledge transfer, where the knowledge about utterances in one language may be transferred to another. The utility of the transfer or annotation projection by means of bilingual lexicons obtained from the CLSS models has already been proven in various tasks such as semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), parsing (Zhao et al., 2009; Durrett et al., 2012; T¨ackstr¨om et al., 2013b), POS tagging (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013a; Ganchev and Das, 2013), verb classification (Merlo et al., 2002), inducing selectional preferences (Peirsman and Pad´o, 2010), named entity recognition (Kim et al., 2012), named entity segmentation (Ganchev and Das, 2013), etc. The models of cross-lingual semantic similarity from parallel corpora rely on word alignment models (Brown et al., 1993; Och and Ney, 2003), but due to a relative scarceness of parallel texts for many language pairs and domains, the"
D14-1040,P11-2052,0,0.0201485,"Missing"
D14-1040,N13-1011,1,0.854231,"Missing"
D14-1040,P11-2084,1,0.813579,"Missing"
D15-1271,N06-1046,0,0.0146148,"ee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original model. 4 Centering theory const"
D15-1271,D08-1031,0,0.0360294,"atives adopted as soft constraints. When testing on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 h"
D15-1271,W09-1206,0,0.10973,"Missing"
D15-1271,W10-2608,0,0.0583813,"Missing"
D15-1271,N07-1030,0,0.0191877,"tion and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without retraining the original mod"
D15-1271,D13-1203,0,0.511163,", 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by using the simplest possible set of features. Its advantage is that the system can both implicitly model important linguistic effects and capture other patterns in the data that are not easily teased out by hand. With a simple set of features including head/first/last words, preceding/following words, length, exact string match, head match, sentence/menti"
D15-1271,P08-2012,0,0.0147353,"string match between a mention and its antecedent” (Lee et al., 2011). If we want to encourage such matches, for each pair j &lt; i where the two nominal mentions mi and mj have an exact string match, we would introduce a constraint indicator variable cexact,i,j and add the constraint vij + cexact,i,j = 1 to the ILP model. The result would be that when the exact match constraint is violated and some vij = 0, ILP would force the corresponding cexact,i,j = 1 and the objective function would be reduced by ρexact . ILP has been used previously to enforce global consistency in coreference resolution (Finkel and Manning, 2008; Denis and Baldridge, 2007; Barzilay and Lapata, 2006). These models were designed for an all-pairs classification approach to 2263 coreference resolution, and are not directly applicable to the back pointer approach of (Durrett and Klein, 2013). But the back pointer approach allows features to be expressed more naturally using local context, rather than requiring, for example, judgments of whether two pronouns separated by many paragraphs are coreferent. Moreover, our ILP formulation is the only one to consider the problem of adapting to another domain and incorporating new features without"
D15-1271,finlayson-etal-2014-n2,0,0.0598125,"Missing"
D15-1271,P14-5010,1,0.0237744,"Missing"
D15-1271,P10-2029,0,0.0269714,"Missing"
D15-1271,J95-2003,0,0.766789,"uge effect on information flow across sentences. Since they are almost void of meaning (only signal gender and number of the antecedent), the discourse referent to be picked up must be particularly salient, so that it can be readily identified by the reader (Stede, 2011). The discourse center hypothesis (HudsonD’Zmura, 1988) states that at any point in discourse understanding, there is one single entity that is the most salient discourse referent at that point. This referent is called the center. Centering theory is a key element of the discourse center hypothesis used in anaphora resolution (Grosz et al., 1995). Beaver (2004) reformulates the centering theory in terms of Optimality Theory (Prince and Smolensky, 2004). Six ranked constraints – Agree, Disjoint, ProTop, FamDef, Cohere and Align – are used to make anaphora decisions. We adopt four of these constraints in our ILP model as follows: Disjoint “Co-arguments of a predicate4 are disjoint.” For each j &lt; i such that mi and mj are subject and object arguments of a non-reflexive predicate, we introduce a constraint indicator variable cdisjoint,i,j , and add the ILP constraint vij − cdisjoint,i,j = 0. ProTop “The topic of a sentence which is the en"
D15-1271,D09-1120,0,0.0229707,"stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by usi"
D15-1271,N10-1061,0,0.0133744,"on the UMIREC1 and N22 corpora with the-stateof-the-art Berkeley coreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle t"
D15-1271,W11-1902,0,0.539948,"oreference resolution system trained on OntoNotes3 , our inference substantially outperforms the original inference on the CoNLL 2011 metric. 1 Introduction Coreference resolution is the task of partitioning the set of mentions of discourse referents in a text into classes (or ‘chains’) corresponding to those referents (Stede, 2011). To solve the problem, contextual and grammatical clues, as well as semantic information and world knowledge are necessary for either learning-based (Bengtson and Roth, 2008; Stoyanov et al., 2010; Haghighi and Klein, 2010) or rule-based (Haghighi and Klein, 2009; Lee et al., 2011) coreference systems. These systems draw on diverse information sources and complex heuristics to resolve pronouns, model discourse, determine anaphoricity, and identify semantically compatible mentions. However, this leads to systems with many hetorogenous parts that can be difficult to interpret or modify. Durrett and Klein (2013) propose a learningbased, mention-synchronous coreference system to 1 http://dspace.mit.edu/handle/1721.1/57507 http://dspace.mit.edu/handle/1721.1/85893 3 https://catalog.ldc.upenn.edu/LDC2011T03 2 tackle the various aspects of coreference by using the simplest pos"
D18-1155,S13-2002,0,0.0399055,"extraction of TimeML-style temporal information from text using machine learning was first explored by Mani et al. (2006). They proposed a multinomial logistic regression classifier to predict the TLinks between entities. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are m"
D18-1155,S17-2093,0,0.120773,"Missing"
D18-1155,P14-2082,0,0.797968,"Missing"
D18-1155,Q14-1022,0,0.407296,"ed with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and in"
D18-1155,D08-1073,0,0.396961,"Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition. 2.2 Temporal Reasoning Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event"
D18-1155,P17-2001,0,0.132288,"logistic regression classifier to predict the TLinks between entities. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the c"
D18-1155,E17-2118,0,0.0936919,"ties. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky"
D18-1155,D12-1062,0,0.832551,"approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition. 2.2 Temporal Reasoning Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z"
D18-1155,P82-1020,0,0.759072,"Missing"
D18-1155,E17-1108,1,0.786423,"arning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition. 2.2 Temporal Reasoning Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990). Or (2), by first mapping the temporal interval expressions to expressions about interval end-points (start and endings of"
D18-1155,P06-1095,0,0.350224,"ssions such as dates, times or duration expressions (e.g. 10-05-2010 or yesterday). Temporal information is also captured in text implicitly, through background knowledge about, for example, duration of events mentioned in the text (e.g. even without context, walks are usually shorter than journeys). Most temporal corpora are annotated with TimeML-style annotations, of which an example is shown in Fig 1, indicating temporal entities, their attributes, and the TLinks among them. The automatic extraction of TimeML-style temporal information from text using machine learning was first explored by Mani et al. (2006). They proposed a multinomial logistic regression classifier to predict the TLinks between entities. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks ("
D18-1155,C16-1007,0,0.0305059,"reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al.,"
D18-1155,N03-1033,0,0.0661298,"Missing"
D18-1155,P11-2061,0,0.208002,"e temporal interval expressions to expressions about interval end-points (start and endings of entities) (Vilain et al., 1990). An example of such mapping is that If event X occurs before Y then the end of X should be before the start of Y. Then reasoning can be done with end-points in a point algebra, which has only three point-wise relations (=, &lt;, &gt;), making reasoning much more efficient compared to reasoning with Allen’s thirteen interval relations. Mapping interval relations to point-wise expressions has been exploited for model inference by Denis and Muller (2011), and for evaluation by UzZaman and Allen (2011). In this work, we ex1238 Input: Text with Temporal Entities (n=3) t1 TLink Extraction: O(n) TimeML Annotations t2 e1 Output: Relative Timeline DCT includes t1 S-TLM and C-TLM: O(n) Last week, John jogged for many hours. before TL2RTL e1: jogged simultaneous e1 DCT t2: many hours t2 t1: last week Last week, John jogged for many hours. Figure 1: An overview of two paradigms: (1) The indirect approach (dashed arrows), where first TLinks are predicted from which we can build a relative time-line using TL2RTL. And (2), the direct approach (solid arrow), where a relative time-line is predicted di"
D18-1155,S13-2001,0,0.344583,"this paper, we propose a new time-line construction paradigm that evades phase 2, the relation extraction phase, because in the classical paradigm temporal relation extraction comes with many difficulties in training and prediction that arise from the fact that for a text with n temporal entities (events or temporal expressions) there are n2 possible entity pairs, which makes it likely for annotators to miss relations, and makes inference slow as n2 pairs need to be considered. Temporal relation extraction models consistently give lower performance than those in the entity recognition phase (UzZaman et al., 2013; Bethard et al., 2016, 2017), introducing errors in the time-line construction pipe-line. The ultimate goal of our proposed paradigm is to predict from a text in which entities are already detected, for each entity: (1) a probability distribution on the entity’s starting point, and (2) another distribution on the entity’s duration. The probabilistic aspect is crucial for time-line based decision making. Constructed time-lines allow for further quantitative reasoning with the temporal information, if this would be needed for certain applications. As a first approach towards this goal, in this"
D18-1155,P09-1046,0,0.224205,"nd Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition. 2.2 Temporal Reasoning Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990). Or (2), by first mapping the temporal interval e"
D18-1155,D17-1108,0,0.728958,"ased and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reasoning can be used at the cost of computation time, during inference (Chambers and Jurafsky, 2008; Denis and Muller, 2011; Do et al., 2012), or during both training and inference (Yoshikawa et al., 2009; Laokulrat et al., 2015; Ning et al., 2017; Leeuwenberg and Moens, 2017). In this work, we circumvent these issues, as we predict time-lines - in linear time complexity - that are temporally consistent by definition. 2.2 Temporal Reasoning Temporal reasoning plays a central role in temporal information extraction, and there are roughly two approaches: (1) Reasoning directly with Allen’s interval relations (shown in Table 1), by constructing rules like: If event X occurs before Y, and event Y before Z then X should happen before Z (Allen, 1990). Or (2), by first mapping the temporal interval expressions to expressions about interval en"
D18-1155,P18-1122,0,0.395888,"starting points. We observe that events that generally have more events included are assigned longer duration and vice versa. And, events with low start values are in the past tense and events with high start values are generally in the present (or future) tense. 7 Discussion A characteristic of our model is that it assumes that all events can be placed on a single timeline, and that it does not assume that unlabeled pairs are temporally unrelated. This has big advantages: it results in fast prediction, and missed annotation do not act as noise to the training, as they do for pairwise models. Ning et al. (2018) argue that actual, negated, hypothesized, expected or opinionated events should possibly be annotated 1244 on separate time-axis. We believe such multi-axis representations can be inferred from the generated single time-lines if hedging information is recognized. 8 This work leads to the following three main contributions14 : (1) Three new loss functions that connect the interval-based TimeML-annotations to points on a time-line, (2) A new method, TL2RTL, to predict relative time-lines from a set of predicted temporal relations. And (3), most importantly, two new models, S-TLM and C-TLM, that"
D18-1155,P16-1207,0,0.0927395,"Missing"
D18-1155,Q18-1006,0,0.0235247,"Missing"
D18-1155,P17-2035,0,0.113422,"proposed a multinomial logistic regression classifier to predict the TLinks between entities. They also noted the problem of missed TLinks by annotators, and experimented with using temporal reasoning (temporal closure) to expand their training data. Since then, much research focused on further improving the pairwise classification models, by exploring different types of classifiers and features, such as (among others) logistic regression and support vector machines (Bethard, 2013; Lin et al., 2015), and different types of neural network models, such as long short-term memory networks (LSTM) (Tourille et al., 2017; Cheng and Miyao, 2017), and convolutional neural networks (CNN) (Dligach et al., 2017). Moreover, different sievebased approaches were proposed (Chambers et al., 2014; Mirza and Tonelli, 2016), facilitating mixing of rule-based and machine learning components. Two major issues shared by these existing approaches are: (1) models classify TLinks in a pairwise fashion, often resulting in an inference complexity of O(n2 ), and (2) the pair-wise predictions are made independently, possibly resulting in prediction of temporally inconsistent graphs. To address the second, additional temporal reason"
D19-1215,N18-2123,0,0.0241006,"Missing"
D19-1215,D17-1090,0,0.0204885,"e the agent in its decision process. Third, some situations request feedback. For example, a passenger might indicate that they want to park in the shade during a sunny day. Finally, the problem of urban scene understanding is one of practical relevance that has been well studied (Cordts et al., 2016; Geiger et al., 2013). We believe all of this makes it an interesting setting to assess the performance of grounding natural language commands into the visual space. Introduction Researchers have studied the problem of understanding actions communicated through natural language in both simulated (Das et al., 2017; Gordon et al., 2017; Hermann et al., 2017) and real environments (Loghmani et al., 2018; de Vries et al., 2018; Anderson et al., 2017). This paper focuses on the latter. More concretely, we consider the problem in an autonomous driving setting, where a passenger can control the actions of an Autonomous Vehicle (AV) by giving natuTo perform the requested action, an agent is required to take two steps. First, the agent needs to interpret the command and ground it into the physical visual space. Secondly, the agent has to devise a plan to execute the given command. This paper focuses on this fo"
D19-1215,D16-1044,0,0.0251722,"ther with the original command. If the command is ambiguous and more than one caption indicates the referring expression, the system will ask a clarifying question in order to be able to pick the right object. Due to its computational complexity during prediction, we did not select the last model in our evaluations. Visual Question Answering The goal of VQA is to ask any type of question about an image for which the system should return the correct answer. This requires the system to have a good understanding of the image and the question. Early work (Kafle and Kanan, 2016; Zhou et al., 2015; Fukui et al., 2016) tried to solve the task by fusing image features extracted by a convolutional neural network (CNN), together with an encoding of the question. (Johnson et al., 2017; Suarez et al., 2018) experimented with modular networks for this task. Hudson and Manning (2018) proposed the use of a network made of recurrent Memory, Attention and Composition (MAC) cells. Similar to modular networks, the MAC model also uses multiple reasoning steps, making it a suitable model in our evaluation (section 5). Object Referral Datasets Over the years, various object referral datasets based on both real world and c"
D19-1215,D15-1162,0,0.0141586,"t set. This model randomly samples one region from the proposals and uses it as prediction for the referred object. This is done 100 times and results are averaged. Biggest Overlapping Bounding Box (BOBB) From the heatmap in Fig. 2 (a) we can see that there is some bias of the referred objects on the left side. This model tries to exploit this information by searching a 2D bounding box that optimizes the overlap with all the bounding boxes in the training set. The algorithm is explained in Section A of the supplementary material. Random Noun Matching (RNM) In the test set a dependency parser (Honnibal and Johnson, 2015) is used to extract the set of nouns from a given command. We keep the nouns which are substrings of the category names. Then, we randomly sample an object from the region proposals of the corresponding image. If the set of category names is empty, we randomly sample a region from all region proposals. We re-use the RPN explained in OSM for the region proposals. This method is evaluated 100 times before averaging the results. 5.4 Results and Discussion Overall Results We evaluated all seven models on the object referral task, using both the test split from subsection 3.3 as well as multiple in"
D19-1215,D14-1086,0,0.0363941,"ures extracted by a convolutional neural network (CNN), together with an encoding of the question. (Johnson et al., 2017; Suarez et al., 2018) experimented with modular networks for this task. Hudson and Manning (2018) proposed the use of a network made of recurrent Memory, Attention and Composition (MAC) cells. Similar to modular networks, the MAC model also uses multiple reasoning steps, making it a suitable model in our evaluation (section 5). Object Referral Datasets Over the years, various object referral datasets based on both real world and computer generated images have been proposed. Kazemzadeh et al. (2014) introduced the first real-world large-scale object referral dataset named ReferIt. Yu et al. (2016) constructed RefCOCO and RefCOCO+ and as the names suggest, these two datasets are based on the MSCOCO dataset (Lin et al., 2014). A third dataset also based on MSCOCO, named RefCOCOg (Mao et al., 2016), contains longer language expressions than the previous two datasets. This dataset has an average expression length of 8.43 words per expression compared to 3.61 for RefCOCO and 3.53 for RefCOCO+. The dataset closest to ours is the dataset by Vasudevan et al. (2018), as it augments Cityscapes (Co"
D19-1215,D14-1162,0,0.0825471,"Missing"
D19-1215,N18-1202,0,0.0871076,"Missing"
E12-1046,C02-2020,0,0.0586144,"one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between la"
E12-1046,C02-1166,0,0.387985,"Missing"
E12-1046,W04-3208,0,0.0259042,"ild such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled"
E12-1046,P98-1069,0,0.0950968,"In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in comm"
E12-1046,P04-1067,0,0.334945,"Missing"
E12-1046,P08-1088,0,0.156009,"exicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corp"
E12-1046,W02-0902,0,0.544091,"ds have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach that improves precision of the existing methods for bilingual lexicon extraction, based on improving the comparability of the corpus under consideration, prior to extracting actual bilingual lexicons. Other methods such as (Koehn and Knight, 2002) try to design a bootstrapping algorithm based on an initial seed lexicon of translations and various lexical evidences. However, the quality of their initial seed lexicon is disputable, 449 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 449–459, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics since the construction of their lexicon is languagepair biased and cannot be completely employed on distant languages. It solely relies on unsatisfactory language-pair independent cross-language clue"
E12-1046,C10-1070,0,0.162042,"suming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (2011) have proposed an approach"
E12-1046,J00-2011,0,0.733077,"duced for word alignments in SMT (Och and Ney, 2003), where the intersection heuristics is employed for a precision-oriented algorithm. In our setting, it basically means that we keep a translation pair (wiS , wjT ) if and only if, after the symmetrization process, the top translation candidate for the source word wiS is the target word wiT and vice versa. The one-to-one constraint aims at matching the most confident candidates during the early stages of the algorithm, and then excluding them from further search. The utility of the constraint for parallel corpora has already been evaluated by Melamed (2000). The remainder of the paper is structured as follows. Section 2 gives a brief overview of the methods, relying on per-topic word distributions, which serve as the tool for computing crosslanguage similarity between words. In Section 3, we motivate the main assumptions of the algorithm and describe the full algorithm. Section 4 justifies the underlying assumptions of the algorithm by providing comparisons with a current-state-of-the-art system for Italian-English and Dutch-English language pairs. It also contains another set of experiments which investigates the potential of the algorithm in b"
E12-1046,D09-1092,0,0.0376677,"Missing"
E12-1046,P07-1084,0,0.0223644,"nd. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obta"
E12-1046,J05-4003,0,0.0148281,"For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method i"
E12-1046,J03-1002,0,0.021325,"metrization process and the one-to-one constraint. We report our results for Italian-English and Dutch-English language pairs that outperform the current state-of-the-art results by a significant margin. In addition, we show how to use the algorithm for the construction of high-quality initial seed lexicons of translations. 1 Introduction Bilingual lexicons serve as an invaluable resource of knowledge in various natural language processing tasks, such as dictionary-based crosslanguage information retrieval (Carbonell et al., 1997; Levow et al., 2005) and statistical machine translation (SMT) (Och and Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995"
E12-1046,P95-1050,0,0.449495,"Ney, 2003). In order to construct high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but t"
E12-1046,P99-1067,0,0.318419,"t high quality bilingual lexicons for different domains, one usually needs to possess parallel corpora or build such lexicons by hand. Compiling such lexicons manually is often an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need fo"
E12-1046,P10-1011,0,0.0672013,"en an expensive and time-consuming task, whereas the methods for mining the lexicons from parallel corpora are not applicable for language pairs and domains where such corpora is unavailable or missing. Therefore the focus of researchers turned to comparable corpora, which consist of documents Over the years, several approaches for mining translations from non-parallel corpora have emerged (Rapp, 1995; Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000; D´ejean et al., 2002; Chiao and Zweigenbaum, 2002; Gaussier et al., 2004; Fung and Cheung, 2004; Morin et al., 2007; Haghighi et al., 2008; Shezaf and Rappoport, 2010; Laroche and Langlais, 2010), all sharing the same Firthian assumption, often called the distributionial hypothesis (Harris, 1954), which states that words with a similar meaning are likely to appear in similar contexts across languages. All these methods have examined different representations of word contexts and different methods for matching words across languages, but they all have in common a need for a seed lexicon of translations to efficiently bridge the gap between languages. That seed lexicon is usually crawled from the Web or obtained from parallel corpora. Recently, Li et al. (20"
E12-1046,P03-1010,0,0.0188721,"and 9, 116 English nouns. For Dutch-English language pair, we use 7, 602 Wikipedia article pairs, and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for t"
E12-1046,E09-1096,0,0.024643,"and 6, 206 Europarl document pairs, and combine them for training.4 Our final vocabularies consist of 15, 284 Dutch nouns and 12, 715 English nouns. Unlike, for instance, Wikipedia articles, where document alignment is established via interlingual links, in some cases it is necessary to perform document alignment as the initial step. Since our work focuses on Wikipedia data, we will not get into detail with algorithms for document alignment. An IR-based method for document alignment is given in (Utiyama and Isahara, 2003; Munteanu and Marcu, 2005), and a feature-based method can be found in (Vu et al., 2009). 4.2 Experimental Setup All our experiments rely on BiLDA training with comparable data. Corpora and software for 4 In case of Europarl, we use only the evidence of document alignment during the training and do not benefit from the parallelness of the sentences in the corpus. BiLDA training are obtained from Vuli´c et al. (2011). We train the BiLDA model with 2000 topics using Gibbs sampling, since that number of topics displays the best performance in their paper. The linear interpolation parameter for the combined TI+Cue method is set to λ = 0.1. The parameters of the algorithm, adjusted on"
E12-1046,P11-2084,1,0.463208,"Missing"
E12-1046,C98-1066,0,\N,Missing
E12-1046,J00-2004,0,\N,Missing
E12-1046,P11-2083,0,\N,Missing
E17-1102,D15-1131,0,0.0503498,"d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual space: sim(w, v) = SF (w, ~ ~v ) = cos(w, ~ ~v ). A plethora of variant BWE models were proposed, differing mostly in the strength of bilingual supervision used in training (e.g., word, sentence, document alignments, translation pairs) (Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014; Chandar et al., 2014; Søgaard et al., 2015; Gouws et al., 2015; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia). Although the BLI evaluation of the BWE models was typically performed on Indo-European languages, none of the works attempted to learn character-level representations to enhance the BLI performance. In this work, we experiment with two BWE models that have demonstrated a strong BLI performance using only a small seed set of word translation pairs (Mikolov et al., 2013b), or document alignments (Vuli´c and Moens, 2016) for bilingual supervision. It is also important to note that other word-level 1086 translation evidence was investigated in the literature."
E17-1102,D16-1136,0,0.109307,"ngual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as dense real-valued vectors. The shared space enables words from"
E17-1102,D12-1001,0,0.038715,"including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., wor"
E17-1102,E14-1049,0,0.176486,"oth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as"
E17-1102,P16-1190,0,0.0432286,"Missing"
E17-1102,P08-1088,0,0.427392,"established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to improve on the BLI task. Moreover, while prior work typically relies on simple orthographic distance measures such as edit distance (Navarro, 2001), we show that such character-level representations can be induced from the data. Second, Irvine and Callison-Burch (2013; 2016) demonstrated that bilingual lexicon induction may be framed as a classification task where multiple heterogeneous translation clues/features may be easily combined. Yet, all current BLI models still"
E17-1102,P14-1006,0,0.0532448,"f td ], where f tk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual space: sim(w, v) = SF (w, ~ ~v ) = cos(w, ~ ~v ). A plethora of variant BWE models were proposed, differing mostly in the strength of bilingual supervision used in training (e.g., word, sentence, document alignments, translation pairs) (Zou et al., 2013; Mikolov et al., 2013b; Hermann and Blunsom, 2014; Chandar et al., 2014; Søgaard et al., 2015; Gouws et al., 2015; Coulmance et al., 2015; Vuli´c and Moens, 2016, inter alia). Although the BLI evaluation of the BWE models was typically performed on Indo-European languages, none of the works attempted to learn character-level representations to enhance the BLI performance. In this work, we experiment with two BWE models that have demonstrated a strong BLI performance using only a small seed set of word translation pairs (Mikolov et al., 2013b), or document alignments (Vuli´c and Moens, 2016) for bilingual supervision. It is also important to"
E17-1102,P82-1020,0,0.828624,"Missing"
E17-1102,N13-1056,0,0.11565,"he 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to improve on the BLI task. Moreover, while prior work typically relies on simple orthographic distance measures such as edit distance (Navarro, 2001), we show that such character-level representations can be induced from the data. Second, Irvine and Callison-Burch (2013; 2016) demonstrated that bilingual lexicon induction may be framed as a classification task where multiple heterogeneous translation clues/features may be easily combined. Yet, all current BLI models still rely on straightforward similarity computations in the shared bilingual word-level semantic space (see Sect. 2). Motivated by these insights, we propose a novel bilingual lexicon induction (BLI) model that combines automatically extracted word-level and character-level representations in a classification framework. As the seminal bilingual representation model of Mikolov et al. (2013b), our"
E17-1102,W02-0902,0,0.717637,"a similarity function operating in the space (cosine similarity is typically used). A target language word v with the highest similarity score arg maxv SF (w, ~ ~v ) is then taken as the correct translation of a source language word w. In this work, we detect two major gaps in current representation learning for BLI. First, the standard embedding-based approach to BLI learns representations solely on the basis of word-level information. While early BLI works already established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the data and apply them to impr"
E17-1102,D14-1177,0,0.0128698,"er-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embedding (BWE) models. Given source and target language vo"
E17-1102,C10-1070,0,0.0293514,"is possible to effectively combine word- and character-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embeddi"
E17-1102,P15-1027,0,0.127058,"Missing"
E17-1102,W13-3523,0,0.0132662,"l context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of-the-art BLI results were obtained by a suite of bilingual word embedding (BWE) models. Given source and target language vocabularies V S and V T , all BWE models learn a representation of each word w ∈ V S tV T as a realvalued vector: w ~ = [f t1 , . . . , f td ], where f tk ∈ R denotes the value for the k-th cross-lingual feature for w within a d-dimensional shared bilingual embedding space. Semantic similarity sim(w, v) between two words w, v ∈ V S t V T is then computed by applying a similarity function (SF), e.g. cosine (cos) on their representations in the bilingual s"
E17-1102,N01-1020,0,0.11925,"atures, and regularities (e.g., ideal:ideaal, apparition:aparici´on). Orthographic translation clues are even more important in certain domains such as medicine, where words with the same roots (from Greek and Latin), and abbreviations are frequently encountered (e.g., D-dimer:D-dimeer, meiosis:meiose). When present, such orthographic clues are typically strong indicators of translation pairs (Haghighi et al., 2008). This observation was exploited in BLI, applying simple string distance metrics such as Longest Common Subsequence Ratio (Melamed, 1995; Koehn and Knight, 2002), or edit distance (Mann and Yarowsky, 2001; Haghighi et al., 2008). Irvine and Callison-Burch (2016) showed that these metrics may be used with languages with different scripts: they transliterate all words to the Latin script before calculating normalized edit distance. BLI as a Classification Task Irvine and Callison-Burch (2016) demonstrate that BLI can be observed as a classification problem. They train a linear classifier to combine similarity scores from different signals (e.g., temporal word variation, normalized edit distance, word burstiness) using a set of training translation pairs. The approach outperforms an unsupervised"
E17-1102,W95-0115,0,0.341095,"and SF denotes a similarity function operating in the space (cosine similarity is typically used). A target language word v with the highest similarity score arg maxv SF (w, ~ ~v ) is then taken as the correct translation of a source language word w. In this work, we detect two major gaps in current representation learning for BLI. First, the standard embedding-based approach to BLI learns representations solely on the basis of word-level information. While early BLI works already established that character-level orthographic features may serve as useful evidence for identifying translations (Melamed, 1995; Koehn and Knight, 2002; 1 See recent comparative studies on cross-lingual word embedding learning (Upadhyay et al., 2016; Vuli´c and Korhonen, 2016) for an in-depth discussion of the differences in modeling and bilingual supervision. 1085 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1085–1095, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics Haghighi et al., 2008), there has been no attempt to learn character-level bilingual representations automatically from the dat"
E17-1102,C10-2174,0,0.0380102,"Missing"
E17-1102,P95-1050,0,0.367971,"e orthographic similarity. (C3) We finally show that it is possible to effectively combine word- and character-level signals using a deep feed-forward neural network. The combined model outperforms “single” word-level and character-level BLI models which rely on only one set of features. 2 Background Word-Level Information for BLI Bilingual lexicon induction is traditionally based on word-level features, aiming at quantifying cross-lingual word similarity on the basis of either (1) context vectors, or (2) automatically induced bilingual word representations. A typical context-vector approach (Rapp, 1995; Fung and Yee, 1998; Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c and Moens, 2013b; Kontonatsios et al., 2014, inter alia) constructs context vectors in two languages using weighted co-occurrence patterns with other words, and a bilingual seed dictionary is then used to translate the vectors. Second-order BLI approaches which represent a word by its monolingual semantic similarity with other words were also proposed, e.g., (Koehn and Knight, 2002; Vuli´c and Moens, 2013a), as well as models relying on latent topic models (Vuli´c et al., 2011; Liu et al., 2013). Recently, state-of"
E17-1102,P15-1165,0,0.0877573,"Missing"
E17-1102,Q13-1001,0,0.0321998,"Missing"
E17-1102,D12-1003,0,0.0193191,"ownstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document alignments, translation pairs from a seed lexicon),1 they all induce word translations in the same manner. (1) They learn a shared bilingual semantic space in which all source language and target language words are represented as dense real-valued vectors. The shared space enables words from both languages to be represented in a uniform languageindependent manner such that similar words (regardless of the actual"
E17-1102,N16-1072,0,0.0271672,"iting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui an"
E17-1102,P16-1157,0,0.179112,"l information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used in training (e.g., word, sentence, document al"
E17-1102,P11-2052,0,0.058083,"Missing"
E17-1102,P16-1024,1,0.860214,"Missing"
E17-1102,N13-1011,1,0.861539,"Missing"
E17-1102,D13-1168,1,0.894328,"Missing"
E17-1102,J03-1002,0,0.00622353,"-of-the-art results for BLI, and the best results are obtained by exploiting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are ob"
E17-1102,P11-1133,0,0.0658225,"Missing"
E17-1102,P11-2084,1,0.889928,"Missing"
E17-1102,N01-1026,0,0.0325692,"nduced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ"
E17-1102,N16-1156,0,0.042383,"s in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from paralle"
E17-1102,P09-1007,0,0.0326295,"nguage processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lingual word embeddings (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016; Duong et al., 2016, inter alia). They significantly outperform traditional count-based baselines (Gaussier et al., 2004; Tamura et al., 2012). Although cross-lingual word embedding models differ on the basis of a bilingual signal from parallel, comparable or monolingual data used"
E17-1102,D13-1141,0,0.305567,"for BLI, and the best results are obtained by exploiting the synergy between these wordand character-level representations in the classification model. 1 Introduction Bilingual lexicon induction (BLI) is the task of finding words that share a common meaning across different languages. Automatically induced bilingual lexicons support a variety of tasks in information retrieval and natural language processing, including cross-lingual information retrieval (Lavrenko et al., 2002; Levow et al., 2005; Vuli´c and Moens, 2015; Mitra et al., 2016), statistical machine translation (Och and Ney, 2003; Zou et al., 2013), or cross-lingual entity linking (Tsai and Roth, 2016). In addition, they serve as a natural bridge for cross-lingual annotation and model transfer from resource-rich to resource-impoverished languages, finding their application in downstream tasks such as cross-lingual POS tagging (Yarowsky and Ngai, 2001; T¨ackstr¨om et al., 2013; Zhang et al., 2016), dependency parsing (Zhao et al., 2009; Durrett et al., 2012; Upadhyay et al., 2016), semantic role labeling (Pad´o and Lapata, 2009; van der Plas et al., 2011), to name only a few. Current state-of-the-art BLI results are obtained by cross-lin"
E17-1102,P98-1069,0,\N,Missing
E17-1102,C98-1066,0,\N,Missing
E17-1102,P04-1067,0,\N,Missing
E17-1102,J17-2001,0,\N,Missing
E17-1108,D08-1073,0,0.576311,"cies. Not modeling these may result in inconsistent output labels, that do not result in a consistent time-line. An example of inconsistent labeling is given in Figure 2. The example is inconsistent when assigning the AFTER label for the relation between lesion and the document-time. It is inconsistent because we can also infer that lesion occurs BEFORE the document-time, as the colonoscopy event occurs before the document-time, and the lesion is contained by the colonoscopy. Temporal inference, in particular temporal closure, is frequently used to expand the training data (Mani et al., 2006; Chambers and Jurafsky, 2008; Lee et al., 2016; Lin et al., 2016b), most of the times resulting in an increase in performance, and is also taken into account when evaluating the predicted labels (Bethard et al., 2014; UzZaman and Allen, 2011). Only very limited research regards the modeling of temporal dependencies into the machine learning model. (Chambers and Juraf1151 contains contains (event) (timex3) (event) A colonoscopy on September 27, 2008 revealed a circumferential lesion . BEFORE AFTER Figure 2: Example of inconsistent output labeling. Containment is indicated by directed edges, and the relation to the documen"
E17-1108,P07-2044,0,0.203503,"ediction time respectively. 5 Conclusions In this work, we proposed a structured perceptron model for learning temporal relations between events and the document-creation time (DCTR), and between temporal entities in the text (TLINKS) in clinical records. Our model efficiently learns and predicts at a document level, exploiting loss-augmented negative subsampling, and uses global features allowing it to exploit relations between local output labels. For construction of a consistent output labeling, needed for time-line construction, we formulated a number of constraints, including those from (Chambers et al., 2007; Do et al., 2012), and assessed them during inference. Our best system outperforms the state-of-the-art of both the CONTAINS TLINK task, and the DCTR task. Our code for this work is available at https://github.com/tuur/SPTempRels. Acknowledgment The authors would like to thank the reviewers for their constructive comments which helped us to improve the paper. Also, we would like to thank the Mayo Clinic for permission to use the THYME corpus. This work was funded by the KU Leuven C22/15/16 project ”MAchine Reading of patient recordS (MARS)”, and by the IWTSBO 150056 project ”ACquiring CrUcial"
E17-1108,D12-1062,0,0.749499,"es resulting in an increase in performance, and is also taken into account when evaluating the predicted labels (Bethard et al., 2014; UzZaman and Allen, 2011). Only very limited research regards the modeling of temporal dependencies into the machine learning model. (Chambers and Juraf1151 contains contains (event) (timex3) (event) A colonoscopy on September 27, 2008 revealed a circumferential lesion . BEFORE AFTER Figure 2: Example of inconsistent output labeling. Containment is indicated by directed edges, and the relation to the document-time by small caps below the events. sky, 2008) and (Do et al., 2012) modeled label dependencies when predicting TimeBank TLINKS (Pustejovsky et al., 2003). They trained local classifiers and used a set of global temporal label constraints. Integer linear programming was employed to maximize the score from the local classifiers, while satisfying the global label constraints at prediction time. For both, this gave a significant increase in performance, and resulted in consistent output labels. (Yoshikawa et al., 2009) modeled the label dependencies between TLINKS and DCTR with Markov Logic Networks (MLN), allowing for soft label constraints during training and p"
E17-1108,S16-1195,0,0.229877,"closure (UzZaman and Allen, 2011). Section Documents TLINKS EVENTS 440 151 17.109 8.903 38.872 18.989 Train Test Table 4: Dataset statistics for the THYME sections we used in our experiments. 4.2 Baselines Our first baseline is a perceptron algorithm, trained for each local task using the same local features as used to compose the joint feature function Φjoint of our structured perceptron. We have two competitive state-of-the-art baselines, one for the DCTR sub-task, and one for the TLINK subtask. The first baseline is the best performing system of the Clinical TempEval 2016 on the DCTR task (Khalifa et al., 2016). They experiment with a feature rich SVM and a sequential conditional random field (CRF) for the prediction of DCTR and report the – to our knowledge – highest performance on the DCTR task. The competitive TLINK baseline is the latest version of the cTAKES Temporal system (Lin et al., 2016b; Lin et al., 2016a). They employ two SVMS to predict TLINKS, one for TLINKS between events, and one for TLINKS between events and temporal expressions and recently improved their system by generating extra training data using extracted UMLS concepts. They report the – to our knowledge – highest performance"
E17-1108,S16-1201,0,0.250746,"Missing"
E17-1108,W16-2914,0,0.554556,"tent output labels, that do not result in a consistent time-line. An example of inconsistent labeling is given in Figure 2. The example is inconsistent when assigning the AFTER label for the relation between lesion and the document-time. It is inconsistent because we can also infer that lesion occurs BEFORE the document-time, as the colonoscopy event occurs before the document-time, and the lesion is contained by the colonoscopy. Temporal inference, in particular temporal closure, is frequently used to expand the training data (Mani et al., 2006; Chambers and Jurafsky, 2008; Lee et al., 2016; Lin et al., 2016b), most of the times resulting in an increase in performance, and is also taken into account when evaluating the predicted labels (Bethard et al., 2014; UzZaman and Allen, 2011). Only very limited research regards the modeling of temporal dependencies into the machine learning model. (Chambers and Juraf1151 contains contains (event) (timex3) (event) A colonoscopy on September 27, 2008 revealed a circumferential lesion . BEFORE AFTER Figure 2: Example of inconsistent output labeling. Containment is indicated by directed edges, and the relation to the document-time by small caps below the event"
E17-1108,P06-1095,0,0.816739,"Missing"
E17-1108,P11-2061,0,0.412006,"AFTER label for the relation between lesion and the document-time. It is inconsistent because we can also infer that lesion occurs BEFORE the document-time, as the colonoscopy event occurs before the document-time, and the lesion is contained by the colonoscopy. Temporal inference, in particular temporal closure, is frequently used to expand the training data (Mani et al., 2006; Chambers and Jurafsky, 2008; Lee et al., 2016; Lin et al., 2016b), most of the times resulting in an increase in performance, and is also taken into account when evaluating the predicted labels (Bethard et al., 2014; UzZaman and Allen, 2011). Only very limited research regards the modeling of temporal dependencies into the machine learning model. (Chambers and Juraf1151 contains contains (event) (timex3) (event) A colonoscopy on September 27, 2008 revealed a circumferential lesion . BEFORE AFTER Figure 2: Example of inconsistent output labeling. Containment is indicated by directed edges, and the relation to the document-time by small caps below the events. sky, 2008) and (Do et al., 2012) modeled label dependencies when predicting TimeBank TLINKS (Pustejovsky et al., 2003). They trained local classifiers and used a set of global"
E17-1108,P15-1032,0,0.0178806,"ive , ypositive ). This cutting plane optimization gives preference to negative training examples that are more likely to be classified wrongly, and thus can be learned from (in an online manner), and it provides only one negative training example for each positive training example, balancing the TLINK classes. 3.5.2 Local Initialization To reduce training time, we don’t initialize λ with ones, but we train a perceptron for both local subtasks, based on the same local features mentioned in Table 1, and use the trained weights to initialize λ for those features. A similar approach was used by (Weiss et al., 2015) for dependency parsing. Details on the training parameters of the perceptron are given in Section 4.3. 4 Experiments We use our experiments to look at the effects of four modeling settings. 1154 Abbrev. Label Dependencies CCtrans CBtrans CCBB CCAA CBBB CBAA CONTAINSi,j BEFORE i,j Constraints ∧ CONTAINSj,k → ∧ BEFOREj,k → BEFORE i,k CONTAINSi,j ∧ BEFOREi,d → CONTAINSi,j ∧ AFTERi,d → BEFORE i,j ∧ BEFOREj,d → BEFORE i,j ∧ AFTERi,d → CONTAINSi,k BEFORE j,d AFTER j,d BEFORE i,d AFTER j,d contains − w contains − w contains ≥ −1 ∀i,j,k : wi,k i,j j,k bef ore bef ore bef ore ∀i,j,k : wi,k − wi,j − wj"
E17-1108,P09-1046,0,0.317309,"istent output labeling. Containment is indicated by directed edges, and the relation to the document-time by small caps below the events. sky, 2008) and (Do et al., 2012) modeled label dependencies when predicting TimeBank TLINKS (Pustejovsky et al., 2003). They trained local classifiers and used a set of global temporal label constraints. Integer linear programming was employed to maximize the score from the local classifiers, while satisfying the global label constraints at prediction time. For both, this gave a significant increase in performance, and resulted in consistent output labels. (Yoshikawa et al., 2009) modeled the label dependencies between TLINKS and DCTR with Markov Logic Networks (MLN), allowing for soft label constraints during training and prediction. However, MLN can sometimes be sub-optimal for text mining tasks w.r.t. time efficiency (Mojica and Ng, 2016). Quite recently, for a similar problem, spatial relation extraction, (Kordjamshidi et al., 2015) used an efficient combination of a structured perceptron or structured support vector machine with integer linear programming. In their experiments, they compare a local learning model (LO), a local learning model with global inference"
E17-1108,L16-1695,0,0.0270552,"d local classifiers and used a set of global temporal label constraints. Integer linear programming was employed to maximize the score from the local classifiers, while satisfying the global label constraints at prediction time. For both, this gave a significant increase in performance, and resulted in consistent output labels. (Yoshikawa et al., 2009) modeled the label dependencies between TLINKS and DCTR with Markov Logic Networks (MLN), allowing for soft label constraints during training and prediction. However, MLN can sometimes be sub-optimal for text mining tasks w.r.t. time efficiency (Mojica and Ng, 2016). Quite recently, for a similar problem, spatial relation extraction, (Kordjamshidi et al., 2015) used an efficient combination of a structured perceptron or structured support vector machine with integer linear programming. In their experiments, they compare a local learning model (LO), a local learning model with global inference at prediction time (L+I), and a structured learning model with and without inference during training (IBT+I, and IBT-I respectively). In their experiments L+I gave better results than LO, but a more significant improvement was made when using structured learning in"
E17-1108,W11-0419,0,0.0576347,"Missing"
E17-1108,S15-2136,0,\N,Missing
E17-1108,Q14-1012,0,\N,Missing
E17-1108,S16-1199,1,\N,Missing
E17-1108,P02-1062,0,\N,Missing
glavas-etal-2014-hieve,S10-1062,0,\N,Missing
glavas-etal-2014-hieve,W04-1017,0,\N,Missing
glavas-etal-2014-hieve,E12-1034,0,\N,Missing
glavas-etal-2014-hieve,chambers-jurafsky-2010-database,0,\N,Missing
glavas-etal-2014-hieve,S07-1014,0,\N,Missing
glavas-etal-2014-hieve,W03-0502,0,\N,Missing
glavas-etal-2014-hieve,J05-2005,0,\N,Missing
glavas-etal-2014-hieve,S10-1010,0,\N,Missing
glavas-etal-2014-hieve,P08-1090,0,\N,Missing
glavas-etal-2014-hieve,S13-2001,0,\N,Missing
glavas-etal-2014-hieve,P09-1068,0,\N,Missing
glavas-etal-2014-hieve,W13-0119,0,\N,Missing
glavas-etal-2014-hieve,S13-2002,0,\N,Missing
glavas-etal-2014-hieve,mani-etal-2008-spatialml,0,\N,Missing
glavas-etal-2014-hieve,P11-2061,0,\N,Missing
glavas-etal-2014-hieve,kordjamshidi-etal-2010-spatial,1,\N,Missing
I17-1010,J81-4005,0,0.750902,"Missing"
I17-1010,N09-1017,0,0.0215323,"proach improves state-of-the-art performance on implicit semantic role labeling with less reliance than prior work on manually constructed language resources. 1 Introduction Semantic role labeling (SRL) has traditionally focused on semantic frames consisting of verbal or nominal predicates and explicit arguments that occur within the clause or sentence that contains the predicate. However, many predicates, especially nominal ones, may bear arguments that are left implicit because they regard common sense knowledge or because they are mentioned earlier in a discourse (Ruppenhofer et al., 2010; Gerber et al., 2009). These arguments, called implicit arguments, are resolved by another semantic task, implicit semantic role labeling (iSRL). Consider a NomBank (Meyers et al., 2004) annotation example: [A0 The network] had been expected to have [NP losses] [A1 of $20 million] . . . Those [NP losses] may widen because of the short Series. 90 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 90–99, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP their implicit semantic roles. Our PRNSFM-based iSRL model improves state-of-the-art performance, outperformin"
I17-1010,P10-1160,0,0.420457,"he second sentence has no associated arguments. However, for a good reader, a reasonable interpretation of the second loss should be that it receives the same A0 and A1 as the first instance. These arguments are implicit to the second loss. As an emerging task, implicit semantic role labeling faces a lack of resources. First, hand-crafted implicit role annotations for use as training data are seriously limited: SemEval 2010 Task 10 (Baker et al., 1998) provided FrameNet-style (Baker et al., 1998) annotations for a fairly large number of predicates but with few annotations per predicate, while Gerber and Chai (2010) provided PropBank-style (Palmer et al., 2005) data with many more annotations per predicate but covering just 10 predicates. Second, most existing iSRL systems depend on other systems (explicit semantic role labelers, named entity taggers, lexical resources, etc.), and as a result not only need iSRL annotations to train the iSRL system, but annotations or manually built resources for all of their sub-systems as well. We propose an iSRL approach that addresses these challenges, requiring no manually annotated iSRL data and only a single sub-system, an explicit semantic role labeler. We introdu"
I17-1010,J12-4003,0,0.0627672,"Discussion Experimental Setup In the baseline mode, instead of using the PRNSFM, we only use the deterministic prediction by the explicit SRL system. We refer to this mode as Baseline in Table 1. In the main mode, the joint embedding LSTM model (Model 1) and the separate embedding LSTM model (Model 2) were trained on the same dataset which is a combination of the automatic SRL annotations and the gold standard CoNLL small values reported in the article achieved similar results with faster processing times. 2 Following Schenk and Chiarcos (2016), we do not perform the alternative evaluation of Gerber and Chai (2012) that evaluates systems on the iSRL training set, since the iSRL training set overlaps with the CoNLL 2009 explicit semantic role training set on which MATE is trained. 3 For iSRL, one implicit role may receive more than one annotated filler across a coreference chain in the discourse. 96 NER system WordNet SRL system PRNSFM training data iSRL data Method Gerber and Chai (2010) Laparra and Rigau (2013) Schenk and Chiarcos (2016) Baseline Skip-gram Model 1: Joint Embedding Model 2: Separate Embedding Model 1: Joint Embedding Model 2: Separate Embedding P R F1 X X X 44.5 40.4 42.3 X X X 47.9 43."
I17-1010,P13-1116,0,0.310098,"presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce proto"
I17-1010,W04-2705,0,0.300191,"Missing"
I17-1010,C10-3009,0,0.0855675,"Missing"
I17-1010,J05-1004,0,0.0495999,"However, for a good reader, a reasonable interpretation of the second loss should be that it receives the same A0 and A1 as the first instance. These arguments are implicit to the second loss. As an emerging task, implicit semantic role labeling faces a lack of resources. First, hand-crafted implicit role annotations for use as training data are seriously limited: SemEval 2010 Task 10 (Baker et al., 1998) provided FrameNet-style (Baker et al., 1998) annotations for a fairly large number of predicates but with few annotations per predicate, while Gerber and Chai (2010) provided PropBank-style (Palmer et al., 2005) data with many more annotations per predicate but covering just 10 predicates. Second, most existing iSRL systems depend on other systems (explicit semantic role labelers, named entity taggers, lexical resources, etc.), and as a result not only need iSRL annotations to train the iSRL system, but annotations or manually built resources for all of their sub-systems as well. We propose an iSRL approach that addresses these challenges, requiring no manually annotated iSRL data and only a single sub-system, an explicit semantic role labeler. We introduce a predictive recurrent neural semantic fram"
I17-1010,P16-1028,0,0.0256194,"model (PRNSFM) from these explicit frames and roles. Our PRNSFM views semantic frames as a sequence: a predicate, followed by the arguments in their textual order, and terminated by a special EOS symbol. We draw predicates from PropBank Language Modeling Language models, from ngram models to continuous space language models (Mikolov et al., 2013; Pennington et al., 2014), provide probability distributions over sequences of words and have shown their usefulness in many natural language processing tasks. However, to our knowledge, they have not yet been used to model semantic frames. Recently, Peng and Roth (2016) developed two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities, but these models focus on discourse 91 verbal semantic frames, and represent arguments with their nominal/pronominal heads. For example, Michael Phelps swam at the Olympics is represented as [swam:PRED, Phelps:A0, Olympics:AMLOC, EOS], where the predicate is labeled PRED and the arguments Phelps and Olympics are labeled A0 and AM-LOC, respectively. Our PRNSFM’s task is thus to take a predicate and zero or more arguments, and predic"
I17-1010,D14-1162,0,0.0796109,"Missing"
I17-1010,J15-4003,0,0.0365329,"ts are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce prototypical roles using distributed word representations, which required only an explicit SRL system and a large unannotated corpus, but their model performance was almost 10 points lower than the state-of-the-art of Laparra and Rigau (2013). Similar to Schenk and Chiarcos (2016), our model requires only an explicit SRL system and a large unannotated corpus, but we tak"
I17-1010,W09-2417,0,0.313807,"Missing"
I17-1010,N16-1173,0,0.510939,"f resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit SRL system, WordNet super-senses, a named entity tagger, and a manual categorization of SuperSenseTagger semantic classes. Roth and Frank (2015) generated additional training data for iSRL through comparable texts, but the resulting model performed below the previous state-of-the-art of Laparra and Rigau (2013). Schenk and Chiarcos (2016) proposed an approach to induce prototypical roles using distributed word representations, which required only an explicit SRL system and a large unannotated corpus, but their model performance was almost 10 points lower than the state-of-the-art of Laparra and Rigau (2013). Similar to Schenk and Chiarcos (2016), our model requires only an explicit SRL system and a large unannotated corpus, but we take a very different approach to leveraging these, and as a result improve state-of-the-art performance. 3 Related work Predictive Recurrent Neural Semantic Frame Model Our goal is to use unlabeled"
I17-1010,N15-1001,0,0.0248588,"es the related work. Second, section 3 proposes the predictive recurrent neural semantic frame model including the formal definition, architecture, and an algorithm to extract selectional preferences from the trained model. Third, in section 4, we introduce the application of our PRNSFM in implicit semantic role labeling. Fourth, the experimental results and discussions are presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constructed resources: an explicit S"
I17-1010,D15-1295,0,0.0211042,"llows: First, section 2 describes the related work. Second, section 3 proposes the predictive recurrent neural semantic frame model including the formal definition, architecture, and an algorithm to extract selectional preferences from the trained model. Third, in section 4, we introduce the application of our PRNSFM in implicit semantic role labeling. Fourth, the experimental results and discussions are presented in section 5. Finally, we conclude our work and suggest some future work in section 6. 2 processing tasks, not semantic frame processing. Semantic Role Labeling In unsupervised SRL, Woodsend and Lapata (2015) and Titov and Khoddam (2015) induce embeddings to represent a predicate and its arguments from unannotated texts, but in their approaches, the arguments are words only, not the semantic role labels, while in our models, both are considered. Low-resource Implicit Semantic Role Labeling Several approaches have attempted to address the lack of resources for training iSRL systems. Laparra and Rigau (2013) proposed an approach based on exploiting argument coherence over different instances of a predicate, which did not require any manual iSRL annotations but did require many other manually-constru"
kordjamshidi-etal-2010-spatial,C08-2024,0,\N,Missing
kordjamshidi-etal-2010-spatial,J08-2001,0,\N,Missing
kordjamshidi-etal-2010-spatial,W09-2814,0,\N,Missing
kordjamshidi-etal-2010-spatial,kuroda-etal-2006-getting,0,\N,Missing
kordjamshidi-etal-2010-spatial,mani-etal-2008-spatialml,0,\N,Missing
L16-1222,N06-2015,0,0.026459,"gnment Procedure The alignment task of predicates and their semantic roles is challenging because the transcribed speech does not contain sentence boundaries. First, we align the predicates between the gold standard data and the speech data after that we align their corresponding semantic roles. 3.1. Predicate Alignment Before aligning the predicates between two corpora, we assign a token id for each token in both corpora so that the predicate also has token id in both corpora. For each predData Set We use the OntoNotes release 3.0 dataset which covers English broadcast and conversation news (Hovy et al., 2006). 1398 Table 2: Predicate statistics in the gold standard dataset. # Total number of files # Total number of predicates # Maximum number of predicates in a file # Minimum number of predicates in a file # Average number of predicates in a file Frequency 722 52181 2105 2 72 Figure 2: Sentences from OntoNotes data (“ \” represents the end of line). Figure 3: Automatic segmented sentences from ASR data (“ \” represents the end of line). icate from the gold standard, we look for the matching token in the speech data within its three left and three right context words. If a predicate in the gold s"
L16-1222,D08-1008,0,0.0151124,"predicate-argument structures in language utterances by identifying predicates and their related semantic roles. SRL reveals more information about the content than a syntactic analysis in the field of natural language processing (NLP) in order to better understand “who” did “what” to “whom”, and “how”, “when” and “where”. SRL has many key applications in NLP, such as question answering, machine translation, and dialogue systems. Many effective SRL systems have been developed to work with written data. However, when applying popular SRL systems such as ASSERT (Pradhan et al., 2005), Lund SRL (Johansson and Nugues, 2008), SWIRL (Surdeanu and Turmo, 2005), and Illinois SRL (Punyakanok et al., 2008) on transcribed speech, which was generated by an automatic speech recognizer (ASR), many errors are made due to the specific nature of the ASR transcribed data. SRL on written data performs well due to the availability of annotated corpora like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) etc., which help to train the SRL system and also the written data is clean and wellformed. Most of the SRL systems on written data are evaluated at the sentence level. On the other hand, ASR data is noisy and not"
L16-1222,J05-1004,0,0.00902457,", such as question answering, machine translation, and dialogue systems. Many effective SRL systems have been developed to work with written data. However, when applying popular SRL systems such as ASSERT (Pradhan et al., 2005), Lund SRL (Johansson and Nugues, 2008), SWIRL (Surdeanu and Turmo, 2005), and Illinois SRL (Punyakanok et al., 2008) on transcribed speech, which was generated by an automatic speech recognizer (ASR), many errors are made due to the specific nature of the ASR transcribed data. SRL on written data performs well due to the availability of annotated corpora like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) etc., which help to train the SRL system and also the written data is clean and wellformed. Most of the SRL systems on written data are evaluated at the sentence level. On the other hand, ASR data is noisy and not well-formed, it does not contain sentence boundaries and it exhibits many errors like insertion, deletion and misspelling of words. Because of the lack of sentence boundaries in speech data and the problem of transcribed speech, it is very hard to align speech data and OntoNotes data on the sentence level. Keeping these complexities in mind, we align p"
L16-1222,W05-0634,0,0.0352309,") is a process of predicting the predicate-argument structures in language utterances by identifying predicates and their related semantic roles. SRL reveals more information about the content than a syntactic analysis in the field of natural language processing (NLP) in order to better understand “who” did “what” to “whom”, and “how”, “when” and “where”. SRL has many key applications in NLP, such as question answering, machine translation, and dialogue systems. Many effective SRL systems have been developed to work with written data. However, when applying popular SRL systems such as ASSERT (Pradhan et al., 2005), Lund SRL (Johansson and Nugues, 2008), SWIRL (Surdeanu and Turmo, 2005), and Illinois SRL (Punyakanok et al., 2008) on transcribed speech, which was generated by an automatic speech recognizer (ASR), many errors are made due to the specific nature of the ASR transcribed data. SRL on written data performs well due to the availability of annotated corpora like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) etc., which help to train the SRL system and also the written data is clean and wellformed. Most of the SRL systems on written data are evaluated at the sentence level. On the"
L16-1222,J08-2005,0,0.0160974,"d their related semantic roles. SRL reveals more information about the content than a syntactic analysis in the field of natural language processing (NLP) in order to better understand “who” did “what” to “whom”, and “how”, “when” and “where”. SRL has many key applications in NLP, such as question answering, machine translation, and dialogue systems. Many effective SRL systems have been developed to work with written data. However, when applying popular SRL systems such as ASSERT (Pradhan et al., 2005), Lund SRL (Johansson and Nugues, 2008), SWIRL (Surdeanu and Turmo, 2005), and Illinois SRL (Punyakanok et al., 2008) on transcribed speech, which was generated by an automatic speech recognizer (ASR), many errors are made due to the specific nature of the ASR transcribed data. SRL on written data performs well due to the availability of annotated corpora like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) etc., which help to train the SRL system and also the written data is clean and wellformed. Most of the SRL systems on written data are evaluated at the sentence level. On the other hand, ASR data is noisy and not well-formed, it does not contain sentence boundaries and it exhibits many erro"
L16-1222,W05-0635,0,0.0458217,"guage utterances by identifying predicates and their related semantic roles. SRL reveals more information about the content than a syntactic analysis in the field of natural language processing (NLP) in order to better understand “who” did “what” to “whom”, and “how”, “when” and “where”. SRL has many key applications in NLP, such as question answering, machine translation, and dialogue systems. Many effective SRL systems have been developed to work with written data. However, when applying popular SRL systems such as ASSERT (Pradhan et al., 2005), Lund SRL (Johansson and Nugues, 2008), SWIRL (Surdeanu and Turmo, 2005), and Illinois SRL (Punyakanok et al., 2008) on transcribed speech, which was generated by an automatic speech recognizer (ASR), many errors are made due to the specific nature of the ASR transcribed data. SRL on written data performs well due to the availability of annotated corpora like PropBank (Palmer et al., 2005), FrameNet (Baker et al., 1998) etc., which help to train the SRL system and also the written data is clean and wellformed. Most of the SRL systems on written data are evaluated at the sentence level. On the other hand, ASR data is noisy and not well-formed, it does not contain s"
L16-1222,P98-1013,0,\N,Missing
L16-1222,C98-1013,0,\N,Missing
N13-1011,N09-1003,0,0.00723768,"t translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as"
N13-1011,C10-1003,0,0.0122019,"cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of fre"
N13-1011,J93-2003,0,0.0420768,"Missing"
N13-1011,P11-2071,0,0.0219132,"Missing"
N13-1011,C02-1166,0,0.0175334,"Missing"
N13-1011,C10-2029,0,0.0545471,"s regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual 111 (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (≈ 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and β = 0.01. We are aware that different hyper-parameter settings (Asuncion et al., 2009; Lu et al., 2011), might have influence on the quality of learned cross-lingual topics, but that analysis is out of the scope of this paper. 4.3 Compared Method"
N13-1011,W04-3208,0,0.0783774,"a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al."
N13-1011,P98-1069,0,0.0359152,"and results on the BLE task for a variety of language pairs. 2 Related Work When dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce re"
N13-1011,P04-1067,0,0.124411,"Missing"
N13-1011,P08-1088,0,0.198932,". (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V"
N13-1011,D09-1124,0,0.14225,"machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target la"
N13-1011,P10-1026,0,0.0216003,"Missing"
N13-1011,W02-0902,0,0.345399,"d over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additio"
N13-1011,2005.mtsummit-papers.11,0,0.0059277,"ed by the response-based method. That property may be exploited to identify one-to-one translations across languages and build a bilingual lexicon (see Table 1). 4 4.1 Experimental Setup Data Collections We work with the following corpora: • IT-EN-W: A collection of 18, 898 ItalianEnglish Wikipedia article pairs previously used by Vuli´c et al. (2011). • ES-EN-W: A collection of 13, 696 SpanishEnglish Wikipedia article pairs. • NL-EN-W: A collection of 7, 612 DutchEnglish Wikipedia article pairs. • NL-EN-W+EP: The NL-EN-W corpus augmented with 6,206 Dutch-English document pairs from Europarl (Koehn, 2005). Although Europarl is a parallel corpus, no explicit use is made of sentence-level alignments. All corpora are theme-aligned, that is, the aligned document pairs discuss similar subjects, but are in general not direct translations (except the Europarl document pairs). NL-EN-W+EP serves to test whether better semantic responses could be learned from data of higher quality, and to measure how it affects the response-based similarity method and the quality of induced lexicons. Following (Koehn and Knight, 2002; Haghighi et al., 2008; Prochasson and Fung, 2011), we consider only noun word types."
N13-1011,C10-1070,0,0.027199,"is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word r"
N13-1011,P99-1004,0,0.0201023,"action (BLE). Such lexicons and semantically similar words serve as important resources Sim(w1S , w2T ) = SF (cv(w1S ), cv(w2T )) (1) cv(w1S ) = [scS1 (c1 ), . . . , scS1 (cN )] denotes a context vector for w1S with N context features ck , where scS1 (ck ) denotes the score for w1S associated with context feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al.,"
N13-1011,D09-1092,0,0.670374,"guage pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007) proposed models of free word association and semantic word similarity in the monolingual settings based on per-topic word distributions from probabilistic topic models such as pLSA (Hofmann, 1999) and LDA (Blei et al., 2003). Additionally, Vuli´c et al. (2011) constructed several models that utilize a shared cross-lingual topical space obtained by a multilingual topic model (Mimno et al., 2009; De Smet and Moens, 2009; Boyd-Graber and Blei, 2009; Ni et al., 2009; Jagarlamudi and Daum´e III, 2010; Zhang et al., 2010) to identify potential translation candidates in the cross-lingual settings without any background knowledge. In this paper, we show that a transition from their semantic space spanned by cross-lingual topics to a semantic space spanned by all vocabulary words yields more robust models of cross-lingual semantic word similarity. 3 Modeling Word Similarity as the Similarity of Semantic Word Responses This section contains a detailed description of our semantic word similar"
N13-1011,P07-1084,0,0.0104373,"en dealing with the cross-lingual semantic word similarity, the focus of the researchers is typically on BLE, since usually the most similar words across languages are direct translations of each other. Numerous approaches emerged over the years that try to induce bilingual word lexicons on the basis of distributional information. Especially challenging is the task of mining semantically similar words from comparable data without any external knowledge source such as machine-readable seed bilingual lexicons used in (Fung and Yee, 1998; Rapp, 1999; Fung and Cheung, 2004; Gaussier et al., 2004; Morin et al., 2007; Andrade et al., 2010; Tamura et al., 2012), predefined explicit ontology or category knowledge used in (D´ejean et al., 2002; Hassan and Mihalcea, 2009; Agirre et al., 2009), or orthographic clues as used in (Koehn and Knight, 2002; Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011). This work addresses that particularly difficult setting which does not assume any language pair dependent background knowledge. It makes methods developed in such a setting applicable even on distant language pairs with scarce resources. Recently, Griffiths et al. (2007), and Steyvers and Griffiths (2007)"
N13-1011,J03-1002,0,0.0253116,"tic Similarity of Words as the Similarity of Their Semantic Word Responses Ivan Vuli´c and Marie-Francine Moens Department of Computer Science KU Leuven Celestijnenlaan 200A Leuven, Belgium {ivan.vulic,marie-francine.moens}@cs.kuleuven.be Abstract in cross-lingual knowledge induction (e.g., Zhao et al. (2009)), statistical machine translation (Och and Ney, 2003) and cross-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are like"
N13-1011,D10-1025,0,0.0173453,"we use is a straightforward multilingual extension of the standard Blei et al.’s LDA model (Blei et al., 2003) called bilingual LDA (Mimno et al., 2009; Ni et al., 2009; De Smet and Moens, 2009). For the details regarding the modeling assumptions, generative story, training and inference procedure of the bilingual LDA model, we refer the interested reader to the aforementioned relevant literature. The potential of the model in the task of bilingual lexicon extraction was investigated before (Mimno et al., 2009; Vuli´c et al., 2011), and it was also utilized in other cross-lingual tasks (e.g., Platt et al. (2010); Ni et al. (2011)). We use Gibbs sampling for training. In a typical setting for mining semantically similar words using latent topic models in both monolingual 111 (Griffiths et al., 2007; Dinu and Lapata, 2010) and cross-lingual setting (Vuli´c et al., 2011), the best results are obtained with the number of topics set to a few thousands (≈ 2000). Therefore, our bilingual LDA model on all corpora is trained with the number of topics K = 2000. Other parameters of the model are set to the standard values according to Steyvers and Griffiths (2007): α = 50/K and β = 0.01. We are aware that diffe"
N13-1011,P11-1133,0,0.10381,"s-lingual information retrieval (Ballesteros and Croft, 1997; Levow et al., 2005). From parallel corpora, semantically similar words and bilingual lexicons are induced on the basis of word alignment models (Brown et al., 1993; Och and Ney, 2003). However, due to a relative scarceness of parallel texts for many language pairs and domains, there has been a recent growing interest in mining semantically similar words across languages on the basis of comparable data readily available on the Web (e.g., Wikipedia, news stories) (Haghighi et al., 2008; Hassan and Mihalcea, 2009; Vuli´c et al., 2011; Prochasson and Fung, 2011). Approaches to detecting semantic word similarity from comparable corpora are most commonly based on an idea known as the distributional hypothesis (Harris, 1954), which states that words with similar meanings are likely to appear in similar contexts. Each word is typically represented by a highdimensional vector in a feature vector space or a socalled semantic space, where the dimensions of the vector are its context features. The semantic similarity of two words, w1S given in the source language LS with vocabulary V S and w2T in the target language LT with vocabulary V T is then: We propose"
N13-1011,P99-1067,0,0.131993,"feature ck (similar for w2T ). SF is a similarity function (e.g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic"
N13-1011,D10-1114,0,0.0178367,"for Response-BC. Additionally, since P (zk |wi ) > 0 and P (wk |wi ) > 0 for each zk ∈ Z and each wk ∈ V S ∪ V T , a lot of probability mass is assigned to topics and semantic responses that are completely irrelevant to the given word. Reducing the dimensionality of the semantic representation a posteriori to only a smaller number of most important semantic axes in the semantic spaces should decrease the effects of that statistical noise, and even more firmly emphasize the latent correlation among words. The utility of such semantic space truncating or feature pruning in monolingual settings (Reisinger and Mooney, 2010) was also detected previously for LSA and LDA-based models (Landauer and Dumais, 1997; Griffiths et al., 2007). Therefore, unless noted otherwise, we perform all our calculations over the best scoring 200 crosslingual topics and the best scoring 2000 semantic word responses.3 4.4 Evaluation Ground truth translation pairs.4 Since our task is bilingual lexicon extraction, we designed a set of ground truth one-to-one translation pairs for all 3 language pairs as follows. For Dutch-English and Spanish-English, we randomly sampled a set of Dutch (Spanish) nouns from our Wikipedia corpora. Following"
N13-1011,D12-1003,0,0.535767,"g., cosine, the Kullback-Leibler 106 Proceedings of NAACL-HLT 2013, pages 106–116, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics divergence, the Jaccard index) operating on the context vectors (Lee, 1999; Cha, 2007). In order to compute cross-lingual semantic word similarity, one needs to design the context features of words given in two different languages that span a shared cross-lingual semantic space. Such crosslingual semantic spaces are typically spanned by: (1) bilingual lexicon entries (Rapp, 1999; Gaussier et al., 2004; Laroche and Langlais, 2010; Tamura et al., 2012), or (2) latent language-independent semantic concepts/axes (e.g., latent cross-lingual topics) induced by an algebraic model (Dumais et al., 1996), or more recently by a generative probabilistic model (Haghighi et al., 2008; Daum´e III and Jagarlamudi, 2011; Vuli´c et al., 2011). Context vectors cv(w1S ) and cv(w2T ) for both source and target words are then compared in the semantic space independently of their respective languages. In this work, we propose a new approach to constructing the shared cross-lingual semantic space that relies on a paradigm of semantic word responding or free word"
N13-1011,P11-2084,1,0.796005,"Missing"
N13-1011,P10-1115,0,0.0615287,"Missing"
N13-1011,P09-1007,0,0.0744512,"Missing"
N13-1011,C98-1066,0,\N,Missing
N18-5014,P10-1040,0,0.0517434,"uch as image caption generation, machine translation, simplification of text, and text summarization— especially when dealing with noisy texts. Another impediment is the nature of clinical data, which is often unstructured and not wellformed, yet commonly has a high and important information density. Textual reports often don’t follow regular syntax rules and contain very specific medical terminology. Moreover, the amount of training data is often limited and each physician has a personal writing style. Simply reusing pretrained continuous representations, such as vectorbased word embeddings (Turian et al., 2010), is therefore not always feasible for medical datasets. The approach to text generation has mainly been dominated by Long Short-Term Memory networks (LSTMs). While LSTMs are successful in creating realistic samples, no actionable smooth representation is created of the text and thus there are limited possibilities to manipulate or employ the representations in additional applications that require continuous inputs. While the creation of continuous representations of text usually involves an autoencoder, the results mostly lack enough semantic information to be particularly useful in an altern"
N19-1188,P79-1000,0,0.311237,"Missing"
N19-1188,D18-1399,0,0.0678852,"inducing cross-lingual word embeddings for two or more languages in the same vector space. Embeddings of translations and words with similar meaning are geometrically close in the shared cross-lingual vector space. This property makes them effective features for cross-lingual NLP tasks such as cross-lingual document classification (Klementiev et al., 2012), cross-lingual information retrieval (Vuli´c and Moens, 2015), bilingual lexicon induction (Mikolov et al., 2013b; Gouws et al., 2015; Heyman et al., 2017), and (unsupervised) machine translation (Artetxe et al., 2017b; Lample et al., 2018; Artetxe et al., 2018c). Most prior work has focused on methods for constructing bilingual word embeddings (BWEs), yielding word representations for exactly two languages. For problems such as multilingual document classification, however, it is highly-desirable to represent words in a multilingual space. A favourable property is that it enables fitting a single classifier on the union of training datasets in many languages, which results in 1) knowledge transfer across languages that may lead to better classification performance, and 2) a setup that is easier to maintain as it is no longer required to train many"
N19-1188,E17-1084,0,0.442389,"v et al., 2013a) on a union of monolingual corpora where they replace words with their cluster id such that words in the same cluster get the same representation. MultiCCA is the multilingual extension of the method of Faruqui and Dyer (2014): Using canonical correlation analysis (CCA) and dictionaries with English as the target language, monolingual embeddings are projected to the English vector space. MultiSkip is a straightforward extension of the BiSkip method (Luong et al., 2015) which generalizes the monolingual SG objective to account for word alignments in parallel corpora. Similarly, Duong et al. (2017), extend CBOW to multiple languages. All these methods learn multilingual embeddings using bilingual dictionaries of parallel corpora: This limits their applicability for many languages. More recently, Conneau et al. (2018); Artetxe et al. (2018a) showed that BWEs can be effectively induced without any cross-lingual supervision. The approaches are based on the assumption that monolingual embedding spaces are approximately isomorphic.1 Improving on earlier attempts (Cao et al., 2016; Zhang et al., 2017), Conneau et al. (2018) propose a two-step framework to map two monolingual spaces to the sha"
N19-1188,J82-2005,0,0.63485,"Missing"
N19-1188,C16-1171,0,0.0205537,"which generalizes the monolingual SG objective to account for word alignments in parallel corpora. Similarly, Duong et al. (2017), extend CBOW to multiple languages. All these methods learn multilingual embeddings using bilingual dictionaries of parallel corpora: This limits their applicability for many languages. More recently, Conneau et al. (2018); Artetxe et al. (2018a) showed that BWEs can be effectively induced without any cross-lingual supervision. The approaches are based on the assumption that monolingual embedding spaces are approximately isomorphic.1 Improving on earlier attempts (Cao et al., 2016; Zhang et al., 2017), Conneau et al. (2018) propose a two-step framework to map two monolingual spaces to the shared space. First, they use an adversarial objective to get an initial bilingual space in which the discriminator can no longer distinguish to which language a given word embedding belongs. They then fine-tune the initial solution. An important limitation is that the adversarial objective is prone to converge to degenerate solutions. Furthermore, Søgaard et al. (2018) empirically prove that the method typically fails for distant language pairs such as English-Finnish. In parallel, A"
N19-1188,D18-1024,0,0.473412,"ra (Gouws et al., 2015), or subjectaligned document pairs (Vuli´c and Moens, 2016). In such paradigms, modeling dependencies between all languages is impractical as it requires supervision for all language pair combinations. Recent research has shown that BWEs can also be learned without cross-lingual supervision and can even outperform supervised BWE variants 1890 Proceedings of NAACL-HLT 2019, pages 1890–1902 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics on bilingual lexicon induction benchmarks (Conneau et al., 2018; Artetxe et al., 2018a). Chen and Cardie (2018) took a first step towards learning multilingual spaces without supervision while incorporating dependencies between all languages but their approach extends the work of Conneau et al. (2018), which has known limitations concerning optimization stability with distant language pairs (Søgaard et al., 2018). In this work, we investigate robust methods to induce MWEs without any cross-lingual supervision. The robustness of our approach is illustrated in good performance for distant languages such as Finnish and Bulgarian. This paper makes the following contributions. First, based on a reformulatio"
N19-1188,P15-1033,0,0.0215414,"ssifier is the average perceptron used by Klementiev et al. (2012). MLPARSING is a multilingual dependency parsing dataset sampled from the Universal Dependencies 1.1 corpus (Agi´c et al., 2015)12 . It contains 12 languages: English, German, French, Spanish, Italian, Bulgarian, Czech, Danish, Swedish, Greek, Finnish, and Hungarian. The respective training and test set contain 6,748 and 1,200 sentences. The test set contains 100 sentences for each language, while for the training set the number of sentences for a language ranges between 98 and 6,694. The parser used is the stack-LSTM parser by Dyer et al. (2015). The parser is not allowed to use any partof-speech and morphology features, and keeps the input word embeddings fixed to isolate the effect of the evaluated embeddings on the parsing performance (Ammar et al., 2016). The reported scores are UAS scores averaged across languages. For comparison with related work, we train 512dimensional monolingual embeddings on the text collections used by Ammar et al. (2016) and Duong et al. (2017). The monolingual embeddings are again trained using fastText. Training Setup. In all experiments, we set the following hyper-parameters to values that were used i"
N19-1188,E14-1049,0,0.0565925,"o not limit our evaluation to the intrinsic BLI task only. Consequently, we investigate if embedding reweighting, a recently proposed best practice for BWEs, is useful for extrinsic tasks such as document classification and dependency parsing in multilingual settings. 2 Related Work Cross-lingual word embeddings have received a lot of attention in recent years. Most methods construct a space shared between two languages using cross-lingual supervision in the form of bilingual lexicons (Mikolov et al., 2013a; Artetxe et al., 2016; Smith et al., 2017), parallel corpora (Klementiev et al., 2012; Faruqui and Dyer, 2014; Gouws et al., 2015; Luong et al., 2015) or subject-aligned document pairs (Vuli´c and Moens, 2016). See Ruder et al. (2018) for a full overview of BWE model typology in relation to the required supervision. To enable knowledge transfer across an arbitrary number of languages, multilingual methods have been introduced. Huang et al. (2015), propose decomposing a matrix with multilingual cooccurrence counts weighted by probabilistic dictionaries. Ammar et al. (2016) compare this method to three other MWE models: MultiCluster, MultiCCA, and MultiSkip. MultiCluster uses bilingual dictionaries to"
N19-1188,E17-1102,1,0.725248,"Missing"
P07-1126,W06-0505,1,0.834245,"Missing"
P07-1126,J97-3003,0,0.0266051,"Missing"
P07-1126,W06-3804,1,0.609625,"e for each noun entity er in the discourse its salience (Sal1) in the discourse tree, which is proportional with the depth of the entity in the discourse tree -hereby assuming that deeper in this tree more detailed topics of a text are describedand normalize this value to be between zero and one. When an entity occurs in different subtrees, its maximum score is chosen. 4.2 Refinement with sentence parse information Because not all entities of the text are captured in the discourse tree, we implement an additional refinement of the computation of the salience of an entity which is inspired by (Moens et al., 2006). The segmentation module already determines the main topic of a sentence. Since the syntactic structure is often indicative of the information distribution in a sentence, we can determine the relative importance of the other entities in a sentence by relying on the relationships between entities as signaled by the parse tree. When determining the salience of an entity, we take into account the level of the entity mention in the parse tree (Sal2), and the number of children for the entity in this structure (Sal3), where the normalized score is respectively inversely proportional with the depth"
P07-1126,N04-3012,0,0.0384842,"ss of a given synset. We first define a similarity measure between synsets in the WordNet database. Then we select a set of seed synsets, i.e. synsets with a predefined visualness, and use the similarity of a given synset to the seed synsets to determine the visualness. 6.3 and S2 . First it finds the most specific (lowest in the tree) synset Sp that is a parent of both S1 and S2 . Then it computes the similarity of S1 and S2 as where C(Si ) is the number of occurrences of Si , N is the total number of synsets in WordNet and K is the number of children of Si . The WordNet::Similarity package (Pedersen et al., 2004) implements this distance measure and was used by the authors. 6.4 Seed synsets We have manually selected 25 seed synsets in WordNet, where we tried to cover the wide range of topics we were likely to encounter in the test corpus. We have set the visualness of these seed synsets to either 1 (visual) or 0 (not visual). We determine the visualness of all other synsets using these seed synsets. A synset that is close to a visual seed synset gets a high visualness and vice versa. We choose a linear weighting: vis(s) = X vis(si ) i sim(s, si ) C(s) where vis(s) returns a number between 0 and 1 deno"
P11-2047,N07-1053,0,0.0340212,"Missing"
P11-2047,W99-0613,0,0.0271612,"(LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding tokens, parts-of-speech, syntactic chunks etc.) and t"
P11-2047,W09-2207,0,0.0228935,"Missing"
P11-2047,D09-1003,1,0.935715,"zer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in the newswire training data. We therefore introduce a semisupervised approach for expanding the training data, where we take words from temporal expressions in the data, substitute these words with likely synonyms, and add the generated examples to the training set. We select synonyms both via WordNet, and via predictions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and doc"
P11-2047,S10-1071,0,0.0136579,"be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are ava"
P11-2047,C96-1079,0,0.0125111,"Reuters corpus, and smaller improvements on the Wikipedia corpus. We find that WordNet alone never improves performance, though intersecting the examples from the LWLM and WordNet provides more stable results for Wikipedia. 1 Introduction The recognition of time expressions such as April 2011, mid-September and early next week is a crucial first step for applications like question answering that must be able to handle temporally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the s"
P11-2047,W06-2207,0,0.036447,"Missing"
P11-2047,S10-1072,1,0.85663,"training corpus for learning a supervised model rather than for selecting high precision seeds, we generate additional training examples using synonyms rather than bootstrapping based on patterns, and we evaluate on Reuters and Wikipedia data that differ from the domain on which our model was trained. 3 Method The proposed method implements a supervised machine learning approach that classifies each chunk-phrase candidate top-down starting at the parse tree root provided by the OpenNLP parser. Time expressions are identified as phrasal chunks with spans derived from the parse as described in (Kolomiyets and Moens, 2010). 3.1 Basic TempEval Model We implemented a logistic regression model with the following features for each phrase-candidate: • The head word of the phrase • The part-of-speech tag of the head word • All tokens and part-of-speech tags in the phrase as a bag of words 272 • • • 3.2 The word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30 The condensed word-shape representation for the head word and the entire phrase, e.g. X(x) (9) for the expression April 30 The concatenated string of the syntactic types of the children of the phrase in the p"
P11-2047,S10-1063,0,0.0182554,"ally anchored queries. This need has inspired a variety of shared tasks for identifying time expressions, including the Message Understanding Conference named entity task (Grishman and Sundheim, 1996), the Automatic Content Extraction time normalization task (http://fofoca.mitre.org/tern.html) and the TempEval 2010 time expression task (Verhagen et al., 2010). Many researchers competed in these tasks, applying both rule-based and machine-learning approaches (Mani and Wilson, 2000; Negri and Marseglia, 2004; Hacioglu et al., 2005; Ahn et al., 2007; Poveda et al., 2007; Strötgen and Gertz 2010; Llorens et al., 2010), and achieving F1 measures as high as 0.86 for recognizing temporal expressions. Yet in most of these recent evaluations, models are both trained and evaluated on text from the same domain, typically newswire. Thus we know little about how well time expression recognition systems generalize to other sorts of text. We therefore take a state-of-the-art time recognizer and evaluate it both on TempEval 2010 and on two new test sets drawn from Reuters and Wikipedia. At the same time, we are interested in helping the model recognize more types of time expressions than are available explicitly in th"
P11-2047,P95-1026,0,0.194589,"tions from the Latent Words Language Model (LWLM) (Deschacht and Moens, 2009). We then evaluate the semi-supervised model on the TempEval, Reuters and Wikipedia test sets and observe how well the model has expanded its temporal vocabulary. 271 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 271–276, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work • Semi-supervised approaches have been applied to a wide variety of natural language processing tasks, including word sense disambiguation (Yarowsky, 1995), named entity recognition (Collins and Singer, 1999), and document classification (Surdeanu et al., 2006). The most relevant research to our work here is that of (Poveda et al., 2009), which investigated a semi-supervised approach to time expression recognition. They begin by selecting 100 time expressions as seeds, selecting only expressions that are almost always annotated as times in the training half of the Automatic Content Extraction corpus. Then they begin an iterative process where they search an unlabeled corpus for patterns given their seeds (with patterns consisting of surrounding"
P11-2047,P00-1010,0,\N,Missing
P11-2047,S10-1010,0,\N,Missing
P11-2084,P04-1067,0,0.922188,"Missing"
P11-2084,W02-0902,0,0.411123,"rk and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic models for the task of finding translation correspondences. (Ni et al., 2009) have design"
P11-2084,D09-1092,0,0.375897,"Missing"
P11-2084,P95-1050,0,0.257456,"BiLDA model used in the experiments, presents all main ideas behind our work and gives an overview and a theoretical background of the methods. Section 4 evaluates and discusses initial results. Finally, section 5 proposes several extensions and gives a summary of the current work. 479 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479–484, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Related Work The idea to acquire translation candidates based on comparable and unrelated corpora comes from (Rapp, 1995). Similar approaches are described in (Diab and Finch, 2000), (Koehn and Knight, 2002) and (Gaussier et al., 2004). These methods need an initial lexicon of translations, cognates or similar words which are then used to acquire additional translations of the context words. In contrast, our method does not bootstrap on language pairs that share morphology, cognates or similar words. Some attempts of obtaining translations using cross-lingual topic models have been made in the last few years, but they are model-dependent and do not provide a general environment to adapt and apply other topic mod"
P12-1010,S07-1025,1,0.760854,"Missing"
P12-1010,P12-1010,1,0.106103,"Missing"
P12-1010,W06-1623,0,0.541437,"Missing"
P12-1010,D08-1073,0,0.555514,"Missing"
P12-1010,S07-1052,0,0.459892,"Missing"
P12-1010,P09-2093,0,0.0167344,"Missing"
P12-1010,W11-0116,1,0.804338,"Missing"
P12-1010,S10-1063,0,0.187638,"Missing"
P12-1010,P09-1025,0,0.0231566,"Missing"
P12-1010,P06-1050,0,0.222722,"Missing"
P12-1010,W11-0419,0,0.0439569,"Missing"
P12-1010,D11-1036,0,0.0573,"Missing"
P12-1010,S10-1062,0,0.388708,"Missing"
P12-1010,W03-3023,0,\N,Missing
P12-1010,S07-1014,0,\N,Missing
P12-1010,J08-4003,0,\N,Missing
P12-1010,H05-1066,0,\N,Missing
P12-1010,S10-1010,0,\N,Missing
P12-1010,P09-1046,0,\N,Missing
P12-1010,bethard-etal-2012-annotating,1,\N,Missing
P15-2118,W15-1521,0,0.49074,"Missing"
P15-2118,P14-1023,0,0.0408388,"., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has oc719 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 719–725, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Figure 1: The architecture of our BWE Skip-Gram model for learning bilingual word embeddings from document-aligned comparable data. Source language words and documents are drawn as gray boxes, while target langu"
P15-2118,D09-1092,0,0.00826124,"om shuffling procedure and show that the model is fairly robust to different 720 art BWEs from (Gouws et al., 2014; Chandar et al., 2014). Moreover, in order to test the effect of window size on final results, we have varied the maximum window size cs from 4 to 60 in steps of 4.3 Since cosine is used for all similarity computations in the BLI task, we call our new BLI model BWESG+cos. Baseline BLI Models We compare BWESG+cos to a series of state-of-the-art BLI models from document-aligned comparable data: (1) BiLDA-BLI - A BLI model that relies on the induction of latent cross-lingual topics (Mimno et al., 2009) by the bilingual LDA model and represents words as probability distributions over these topics (Vuli´c et al., 2011). (2) Assoc-BLI - A BLI model that represents words as vectors of association norms (Roller and Schulte im Walde, 2013) over both vocabularies, where these norms are computed using a multilingual topic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and"
P15-2118,P04-1067,0,0.0241651,"Missing"
P15-2118,D14-1162,0,0.0944734,"based on our novel BWEs significantly outperforms a series of strong baselines that reported previous best scores on these datasets in the same learning setting, as well as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al.,"
P15-2118,P11-1133,0,0.0504185,"use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10 random corpora shuffles for"
P15-2118,P08-1088,0,0.0675096,"Setup Training Data We use comparable Wikipedia data introduced in (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b) available in three language pairs to induce bilingual word embeddings: (i) a collection of 13, 696 Spanish-English Wikipedia article pairs (ES-EN), (ii) a collection of 18, 898 ItalianEnglish Wikipedia article pairs (IT-EN), and (iii) a collection of 7, 612 Dutch-English Wikipedia article pairs (NL-EN). All corpora are theme-aligned comparable corpora, that is, the aligned document pairs discuss similar themes, but are in general not direct translations. Following prior work (Haghighi et al., 2008; Prochasson and Fung, 2011; Vuli´c and Moens, 2013b), we retain only nouns that occur at least 5 times in the corpus. Lemmatized word forms are recorded when available, and original forms otherwise. TreeTagger (Schmid, 1994) is used for POS tagging and lemmatization. After the preprocessing vocabularies comprise between 7,000 and 13,000 noun types for each language in each language pair. Exactly the same training data and vocabularies are used to induce bilingual lexicons with all other BLI models in comparison. BWESG Training Setup We have trained the BWESG model with random shuffling on 10"
P15-2118,D13-1115,0,0.0292935,"Missing"
P15-2118,P14-1006,0,0.0376606,"Missing"
P15-2118,W14-1503,0,0.0209037,"opic model (Vuli´c and Moens, 2013a). (3) PPMI+cos - A standard distributional model for BLI relying on positive pointwise mutual information and cosine similarity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflecte"
P15-2118,C12-1089,0,0.155244,"Missing"
P15-2118,D12-1003,0,0.0342663,"larity (Bullinaria and Levy, 2007). The seed lexicon is bootstrapped using the method from (Peirsman and Pad´o, 2011; Vuli´c and Moens, 2013b). All parameters of the baseline BLI models (i.e., topic models and their settings, the number of dimensions K, feature pruning values, window size) are set to their optimal values according to suggestions in prior work (Steyvers and Griffiths, 2007; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b; Kiela and Clark, 2014). Due to space constraints, for (much) more details about the baselines we point to the relevant literature (Peirsman and Pad´o, 2011; Tamura et al., 2012; Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Test Data For each language pair, we evaluate on standard 1,000 ground truth one-to-one translation pairs built for the three language pairs (ES/IT/NLEN) (Vuli´c and Moens, 2013a; Vuli´c and Moens, 2013b). Translation direction is ES/IT/NL → EN. Evaluation Metrics Since we can build a oneto-one bilingual lexicon by harvesting one-to-one translation pairs, the lexicon qualiy is best reflected in the Acc1 score, that is, the number of source language (ES/IT/NL) words wiS from ground truth translation pairs for which the top ranked word cross-l"
P15-2118,Q15-1016,0,0.0146499,"as other BLI models based on recently proposed BWE induction models (Gouws et al., 2014; Chandar et al., 2014). The focus of the work is on learning lexicons from documentaligned comparable corpora (e.g., Wikipedia articles aligned through inter-wiki links). Dense real-valued vectors known as distributed representations of words or word embeddings (WEs) (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014) have been introduced recently as part of neural network architectures for statistical language modeling. Recent studies (Levy and Goldberg, 2014; Levy et al., 2015) have showcased a direct link and comparable performance to “more traditional” distributional models (Turney and Pantel, 2010), but the skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013c) is still established as the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015). A natural extension of interest from monolingual to multilingual word embeddings has oc719 Proceedings of the 53rd Annual Meeting of the Association for Compu"
P15-2118,N13-1011,1,0.774482,"Missing"
P15-2118,D13-1168,1,0.560609,"Missing"
P15-2118,P11-2084,1,0.607412,"Missing"
P15-2118,D13-1141,0,0.312134,"Missing"
P16-2031,P14-1006,0,0.112167,"that share a common meaning across different languages. It plays an important role in a variety of fundamental tasks in IR and NLP, e.g. cross-lingual information retrieval and statistical machine translation. The majority of current BLL models aim to learn lexicons from comparable data. These approaches work by (1) mapping language pairs to a shared crosslingual vector space (SCLVS) such that words are close when they have similar meanings; and (2) extracting close lexical items from the induced SCLVS. Bilingual word embedding (BWE) induced models currently hold the state-of-the-art on BLL (Hermann and Blunsom, 2014; Gouws et al., 2015; Vuli´c and Moens, 2016). Although methods for learning SCLVSs are predominantly text-based, this space need not be linguistic in nature: Bergsma and van Durme (2011) and Kiela et al. (2015) used labeled images from 2 2.1 Methodology Linguistic Representations We use three representative linguistic BWE models. Given a source and target vocabulary V S and V T , BWE models learn a representation of each word w ∈ V S ∪ V T as a real-valued vec188 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7"
P16-2031,D14-1005,1,0.413644,"eneral performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set for (linguistic) BLL models from comparable data (Vuli´c and Moens, 2016)5 . It comprises 1, 000 nouns in ES, IT, and NL, along with their oneto-one ground-truth word translations in EN compiled semi-automatically. Translation direction is ES/IT /N L → EN . Multi-Modal Representations We experiment with two ways of fusing information stemming from the linguistic and visual modalities. Following recent work in multi-modal semantics (Bruni et al., 2014; Kiela and Bottou, 2014), we construct representations by concatenating the centered and L2 -normalized linguistic and visual feature vectors: wmm = α × wling ||(1 − α) × wvis Experimental Setup Training Data and Setup We used standard training data and suggested settings to learn M/G/V-EMB model representations. M-EMB and G-EMB were trained on the full cleaned and tokenized Wikipedias from the Polyglot website (AlRfou et al., 2013). V-EMB was trained on the full tokenized document-aligned Wikipedias from (1) where ||denotes concatenation and α is a parameter governing the contributions of each unimodal representatio"
P16-2031,P14-2135,1,0.557318,"setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from Kiela et al. (2015). We also propose a weighting technique based on image dispersion (Kiela et al., 2014) that governs the influence of visual information in fused representations, and show that this technique leads to robust multi-modal models which do not require fine tuning of the fusion parameter. Recent work has revealed the potential of using visual representations for bilingual lexicon learning (BLL). Such image-based BLL methods, however, still fall short of linguistic approaches. In this paper, we propose a simple yet effective multimodal approach that learns bilingual semantic representations that fuse linguistic and visual input. These new bilingual multi-modal embeddings display signi"
P16-2031,D15-1015,1,0.471003,"Missing"
P16-2031,J99-4009,0,0.810588,"image dispersion (ID) (Kiela et al., 2014). ID is defined as the average pairwise cosine distance between all the image representations/vectors {i1 . . . in } in the set of images for a given word w: id(w) = X 2 ij · ik 1− n(n − 1) |ij ||ik | (2) j<k≤n 5 Intuitively, more concrete words display more coherent visual representations and consequently lower ID scores (see Footnote 9 again). The lowest improvements on V ULIC 1000 are reported for the IT-EN language pair, which is incidentally the most abstract test set. There is some evidence that abstract concepts are also perceptually grounded (Lakoff and Johnson, 1999), albeit in a more complex way, since abstract concepts will relate more varied situations (Barsalou and Wiemer-Hastings, 2005). Consequently, uni-modal visual representations are not powerful enough to capture all the semantic intricacies of such abstract concepts, and the linguistic components are more beneficial in such cases. This explains an improved performance with α = 0.7, but also calls for a more intelligent decision mechanism on how much perceptual information to include in the multi-modal models. The decision should be closely related to the degree of a concept’s concreteness, e.g."
P16-2031,P15-1027,0,0.289784,"feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is learned through an L2 -regularized least-squa"
P16-2031,W16-3210,0,0.0149091,"Missing"
P16-2031,E14-1049,0,0.0830889,"l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate regression problem: it implies learning a function that maps the source language vectors to their corresponding target language vectors. A standard approach (Mikolov et al., 2013; Dinu et al., 2015) is to assume a linear map W ∈ RdS ×dT , which is le"
P16-2031,W15-1521,0,0.168909,"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 188–194, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics tor: wling = [f1ling , . . . , fdling ], where fkling ∈ l R is the value of the k-th cross-lingual feature for w. Similarity between w, v ∈ V S ∪ V T is computed through a similarity function (SF), simling (w, v) = SF (wling , vling ), e.g., cosine. BOWA model from Gouws et al. (2015) as the representative model to be included in the comparisons, due to its solid performance and robustness in the BLL task (Luong et al., 2015), its reduced complexity reflected in fast computations on massive datasets and its public availability.2 Type 1: M-EMB This type of BWE induction model assumes the following setup for learning the SCLVS (Mikolov et al., 2013; Faruqui and Dyer, 2014; Dinu et al., 2015; Lazaridou et al., 2015a): First, two monolingual spaces, RdS and RdT , are induced separately in each language using a standard monolingual embedding model. The bilingual signal is provided in the form of word translation pairs (xi , yi ), where xi ∈ V S , yi ∈ V T , and xi ∈ RdS , yi ∈ RdT . Training is cast as a multivariate r"
P16-2031,P04-1067,0,0.0295097,"SCLVS: simvis (w, v) = SF (wvis , vvis ), e.g. cosine. (2) CNN-AVG M AX: An alternative strategy, introduced by Bergsma and van Durme (2011), is to consider the similarities between individual images from the two sets and take the average of the maximum similarity scores as the final similarity simvis (w, v). 2.3 3 Task: Bilingual Lexicon Learning Given a source language word ws , the task is to find a target language word wt closest to ws in the SCLVS, and the resulting pair (ws , wt ) is a bilingual lexicon entry. Performance is measured using the BLL standard Top 1 accuracy (Acc1 ) metric (Gaussier et al., 2004; Gouws et al., 2015). Test Sets We work with three language pairs: English-Spanish/Dutch/Italian (EN-ES/NL/IT), and two benchmarking BLL test sets: (1) B ERGSMA 500: consisting of a set of 500 ground truth noun pairs for the three language pairs, it is considered a benchmarking test set in prior work on BLL using vision (Bergsma and van Durme, 2011)4 . Translation direction in our tests is EN → ES/IT /N L. (2) V ULIC 1000: constructed to measure the general performance of linguistic BLL models from comparable Wikipedia data (Vuli´c and Moens, 2013), this is considered a benchmarking test set"
P16-2031,N16-1021,0,0.0159524,"le. As future work, we plan to analyse the ability of multi-view representation learning algorithms to yield fused multi-modal representations in bilingual settings (Lazaridou et al., 2015b; Rastogi et al., 2015; Wang et al., 2015), as well as to apply multi-modal bilingual spaces in other tasks such as zero-short learning (Frome et al., 2013) or cross-lingual MM information search and retrieval following paradigms from monolingual settings (Pereira et al., 2014; Vuli´c and Moens, 2015). The inclusion of perceptual data, as this paper reveals, seems especially promising in bilingual settings (Rajendran et al., 2016; Elliott et al., 2016), since the perceptual information demonstrates the ability to transcend linguistic borders. Image Dispersion Weighting The intuition that the inclusion of visual information may lead to negative effects in MM modeling has been exploited by Kiela et al. (2014) in their work on image-dispersion filtering: Although the filtering method displays some clear benefits, its shortcoming lies in the fact that it performs a binary decision which can potentially discard valuable perceptual information for less concrete concepts. Here, we introduce a weighting scheme where the perce"
P16-2031,N15-1058,0,0.0380203,"Missing"
P16-2031,P14-1068,0,0.159578,"ven.be Abstract the Web to learn bilingual lexicons based on visual features, with features derived from deep convolutional neural networks (CNNs) leading to the best results (Kiela et al., 2015). However, vision-based BLL does not yet perform at the same level as state-of-the-art linguistic models. Here, we unify the strengths of both approaches into one single multi-modal vision-language SCLVS. It has been found in multi-modal semantics that linguistic and visual representations are often complementary in terms of the information they encode (Deselaers and Ferrari, 2011; Bruni et al., 2014; Silberer and Lapata, 2014). This is the first work to test the effectiveness of the multi-modal approach in a BLL setting. Our contributions are: We introduce bilingual multi-modal semantic spaces that merge linguistic and visual components to obtain semantically-enriched bilingual multi-modal word representations. These representations display significant improvements for three language pairs on two benchmarking BLL test sets in comparison to three different bilingual linguistic representations (Mikolov et al., 2013; Gouws et al., 2015; Vuli´c and Moens, 2016), as well as over the uni-modal visual representations from"
P16-2031,tiedemann-2012-parallel,0,0.0181405,"ting from the BNC word frequency list (Kilgarriff, 1997), the 6, 318 most frequent EN words were translated to the three other languages using Google Translate. The lists were subsequently cleaned, removing all pairs that contain IT/ES/NL words occurring in the test sets and least frequent pairs, to build the final 3×5K training pairs. We trained two monolingual SGNS models, using SGD with a global learning rate of 0.025. For G-EMB, as in the original work (Gouws et al., 2015), the bilingual signal for the cross-lingual regularization was provided in the first 500K sentences from Europarl.v7 (Tiedemann, 2012). We used SGD with a global learning rate 0.15. For V-EMB, monolingual SGNS was trained on pseudo-bilingual documents using SGD with a global learning rate 0.025. All BWEs were trained with d = 300.7 Other parameters are: 15 epochs, 15 negatives, subsampling rate 1e − 4. We report results with two α standard values: 0.5 and 0.7 (more weight assigned to the linguistic part). 4 sions8 . There is a marked difference in performance on B ERGSMA 500 and V ULIC 1000: visual-only BLL models on V ULIC 1000 perform two times worse than linguistic-only BLL models. This is easily explained by the increase"
P16-2031,D13-1168,1,0.367109,"Missing"
P16-2031,W13-3520,0,\N,Missing
P18-2074,P14-1132,0,0.0286388,"use of semantic-based criteria to evaluate the quality of predicted vectors such as the neighborhood-based measure proposed here, instead of purely geometric measures such as mean squared error (MSE). 2 Related Work and Motivation Neural network and linear mappings are popular tools to bridge modalities in cross-modal retrieval systems. Lazaridou et al. (2015b) leverage a text-to-image linear mapping to retrieve images given text queries. Weston et al. (2011) map label and image features into a shared space with a linear mapping to perform image annotation. Alternatively, Frome et al. (2013), Lazaridou et al. (2014) and Socher et al. (2013) perform zero-shot image classification with an image-to-text neural network mapping. Instead of mapping to latent features, Collell et al. (2018) use a 2-layer feedforward network to map word embeddings directly to image pixels in order to visualize spatial arrangements of objects. Neural networks are also popular in other cross-space applications such as cross-lingual tasks. Lazaridou et al. (2015a) learn a linear map from language A to language B and then translate new words by returning the nearest neighbor of the mapped vector in the B space. In the context of zer"
P18-2074,P15-1027,0,0.534599,"predicted (mapped) vectors. For instance, in zero-shot image classification, image features are mapped to the text space and the label of the nearest neighbor word is assigned. Thus, the success of such systems relies entirely on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure.1 However, whether neural nets achieve this goal in general has not been investigated yet. In fact, recent work evidences that considerable information about the input modality propagates into the predicted modality (Collell et al., 2017; Lazaridou et al., 2015b; Frome et al., 2013). To shed light on these questions, we first introduce the (to the best of our knowledge) first existing measure to quantify similarity between the neighborhood structures of two sets of vectors. Second, we perform extensive experiments in three benchmarks where we learn image-to-text and text-to-image neural net mappings using a rich variety of state-of-the-art text and image features and loss functions. Our results reveal that, contrary to expectation, the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the tar"
P18-2074,C16-1264,1,0.848499,"he input, such as connectedness, are preserved (Armstrong, 2013). Furthermore, continuity in a topology induced by a metric also ensures that points that are close together are mapped close together. As a toy example, Fig. 1 illustrates the distortion of a manifold after being mapped by a neural net.2 In a noiseless world with fully statistically dependent modalities, the vectors of one modality could be perfectly predicted from those of the other. However, in real-world problems this is unrealistic given the noise of the features and the fact that modalities encode complementary information (Collell and Moens, 2016). Such unpredictability combined with continuity and topology-preserving properties of neural nets propel the phenomenon identified, namely mapped vectors resembling more the input than the target vectors, in nearest neighbors terms. M f(M ) Figure 1: Effect of applying a mapping f to a (disconnected) manifold M with three hypothetical classes (, N and •). ignored phenomenon relevant to a wide range of cross-modal / cross-space applications such as retrieval, zero-shot learning or image annotation. Ultimately, this paper aims at: (1) Encouraging the development of better architectures to brid"
P18-2074,D14-1162,0,0.0935911,"Each input vector xi is paired to the output vector yi of the same index (i = 1, · · · , N ). Let us henceforth denote the mapped input vectors by f (X) ∈ RN ×dy . In order to explore the similarity between f (X) and X, and between f (X) and Y , we propose two ad hoc settings below. 3.1 N 1 X NNOK (vi , zi ) (1) KN 3.2 Mapping with Untrained Networks (Experiment 2) To complement the setting above (Sect. 3.1), it is instructive to consider the limit case of an untrained network. Concept similarity tasks provide a suitable setting to study the semantic structure of distributed representations (Pennington et al., 2014). That is, semantically similar concepts should ideally be close together. In particular, our interest is in comparing X with its projection f (X) through a mapping with random parameters, to understand the extent to which the mapping may disrupt or preserve the semantic structure of X. 4 4.1 Experimental Setup Experiment 1 4.1.1 Datasets To test the generality of our claims, we select a rich diversity of cross-modal tasks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki). ImageNet (Russakovsky et al., 2015). Consists of ∼14M images,"
P18-2074,D16-1235,0,0.0442571,"Missing"
P18-2074,P14-1068,0,0.151442,"ngs). Following Collell et al. (2017), we take the most relevant word for each synset and keep only synsets with more than 50 images. This yields 9,251 different words (or instances). IAPR TC-12 (Grubinger et al., 2006). Contains 20K images (18K train / 2K test) annotated with 255 labels. Each image is accompanied with a short description of one to three sentences. Wikipedia (Pereira et al., 2014). Has 2,866 samples (2,173 train / 693 test). Each sample is a section of a Wikipedia article paired with one image. 2014) and WordSim-353 (Finkelstein et al., 2001); (iii) Visual similarity: VisSim (Silberer and Lapata, 2014) which includes the same word pairs as SemSim, rated for visual similarity instead of semantic. All six test sets contain human ratings of similarity for word pairs, e.g., (‘cat’,‘dog’). 4.2.2 The parameters in W0 , W1 are drawn from a random uniform distribution [−1, 1] and b0 , b1 are set to zero. We use a tanh activation σ().6 The output dimension dy is set to 2,048 for all embeddings. 4.1.2 Hyperparameters and Implementation See the Supplement (Sect. 1) for details. 4.2.3 Image and Text Features Textual and visual features are the same as described in Sect. 4.1.3 for the ImageNet dataset."
Q18-1010,N09-1003,0,0.115665,"Missing"
Q18-1010,P14-2131,0,0.0357468,"wledge: Leveraging Language and Vision Marie-Francine Moens Department of Computer Science KU Leuven 3001 Heverlee, Belgium sien.moens@cs.kuleuven.be Guillem Collell Department of Computer Science KU Leuven 3001 Heverlee, Belgium gcollell@kuleuven.be Abstract robot understanding of natural language commands (Guadarrama et al., 2013; Moratz and Tenbrink, 2006) or a number of robot navigation tasks. Despite recent advances in building specialized representations in domains such as sentiment analysis (Tang et al., 2014), semantic similarity/relatedness (Kiela et al., 2015) or dependency parsing (Bansal et al., 2014), little progress has been made towards building distributed representations (a.k.a. embeddings) specialized in spatial knowledge. Spatial understanding is crucial in many realworld problems, yet little progress has been made towards building representations that capture spatial knowledge. Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given objectrelationship-object instances (e.g., “cat under chair”) and a simple neural network model that learns the task from annotated"
Q18-1010,D13-1128,0,0.0413678,"rrences and relative positions of objects—which they mine from text and the web—in order to rank possible object detections. In a similar fashion, Lin and Parikh (2015) leverage common sense visual knowledge (e.g., object locations and co-occurrences) in two tasks: fill-in-the-blank and visual paraphrasing. They compute the likelihood of a scene to identify the most likely answer to multiple-choice textual scene descriptions. In contrast, we focus solely on spatial information rather than semantic plausibility. Moreover, our primary target is to build (spatial) representations. Alternatively, Elliott and Keller (2013) annotate geometric relationships between objects in images (e.g., they add an “on” link between “man” and “bike” in an image of a “man” “riding” a “bike”) to better infer the action present in the image. For instance, if the “man” is next to the bike one can infer that the action “repairing” is more likely than “riding” in this image. Accounting for this extra spatial structure allows them to outperform bag-offeatures methods in an image captioning task. In contrast with those who restrict to a small domain of 10 actions (e.g., “taking a photo”, “riding”, etc.), our goal is to generalize to a"
Q18-1010,D15-1242,0,0.0190872,"Representations Specialized in Spatial Knowledge: Leveraging Language and Vision Marie-Francine Moens Department of Computer Science KU Leuven 3001 Heverlee, Belgium sien.moens@cs.kuleuven.be Guillem Collell Department of Computer Science KU Leuven 3001 Heverlee, Belgium gcollell@kuleuven.be Abstract robot understanding of natural language commands (Guadarrama et al., 2013; Moratz and Tenbrink, 2006) or a number of robot navigation tasks. Despite recent advances in building specialized representations in domains such as sentiment analysis (Tang et al., 2014), semantic similarity/relatedness (Kiela et al., 2015) or dependency parsing (Bansal et al., 2014), little progress has been made towards building distributed representations (a.k.a. embeddings) specialized in spatial knowledge. Spatial understanding is crucial in many realworld problems, yet little progress has been made towards building representations that capture spatial knowledge. Here, we move one step forward in this direction and learn such representations by leveraging a task consisting in predicting continuous 2D spatial arrangements of objects given objectrelationship-object instances (e.g., “cat under chair”) and a simple neural netwo"
Q18-1010,D14-1162,0,0.0935899,"Missing"
Q18-1010,P14-1068,0,0.0402575,"Missing"
Q18-1010,P14-1146,0,0.117547,"Missing"
S10-1072,N07-1053,0,0.255648,"Missing"
S10-1072,D09-1003,1,0.809151,"find a way how to find other temporal expressions outside the available data, which can be used for training. On the other hand, we want to avoid a naïve selection of words as, for example, from a gazetteer with temporal triggers, which may contradict with grammatical rules and the lexical context of a timex in text, e.g.: on Tuesday said.... But grammatically wrong by naïve replacement from a gazetteer: … on week said*… … on day said*… … on month said* … In order to find these words, which are legitimate at a certain position in a certain context we use the latent word language model (LWLM) (Deschacht & Moens, 2009) with a Hidden Markov Model approach for estimating the latent word parameters. Complementary, we use WordNet (Miller, 1995) as a source that can provide a most complete set of words similar to the given one. One should note that the use of WordNet is not straight-forward. Due to the polysemy, the word sense disambiguation (WSD) problem has to be solved. Our system uses latent words obtained by the LWLM and chooses the synset with the high326 est overlap between WordNet synonyms and coordinate terms, and the latent words. The overlap value is calculated as the sum of LWLM probabilities for mat"
S10-1072,W09-2418,0,0.0120901,"usions are provided in Section 4. 2 System Architecture The system is implemented in Java and follows a pipelined method for information processing. Regarding the problems it solves, it can be split in two sub-systems: recognition and normalization. Recognition of temporal expressions1 is a task of proper identification of phrases with temporal semantics in running text. After several evaluation campaigns targeted at temporal processing of text, such as MUC, ACE TERN and TempEval-1 (Verhagen et al., 2007), the recognition and normalization task has been again newly reintroduced in TempEval-2 (Pustejovsky & Verhagen, 2009). The task is defined as follows: determine the extent of the time expressions; in addition, determine the value of the features TYPE for the type of the temporal expression and its temporal value VAL. In this paper we describe the KUL system that has participated in this task. This sub-system is employed for finding temporal expressions in the text. It takes a sentence as input and looks for temporal expressions in it. Pre-processing: At this step the input text undergoes syntactic analysis. Sentence detection, tokenization, part-of-speech tagging and parsing are applied2. Candidate selection"
S10-1072,S07-1014,0,0.0228473,"with single modules and describes theirs functions. Section 3 presents the results and error analysis; the conclusions are provided in Section 4. 2 System Architecture The system is implemented in Java and follows a pipelined method for information processing. Regarding the problems it solves, it can be split in two sub-systems: recognition and normalization. Recognition of temporal expressions1 is a task of proper identification of phrases with temporal semantics in running text. After several evaluation campaigns targeted at temporal processing of text, such as MUC, ACE TERN and TempEval-1 (Verhagen et al., 2007), the recognition and normalization task has been again newly reintroduced in TempEval-2 (Pustejovsky & Verhagen, 2009). The task is defined as follows: determine the extent of the time expressions; in addition, determine the value of the features TYPE for the type of the temporal expression and its temporal value VAL. In this paper we describe the KUL system that has participated in this task. This sub-system is employed for finding temporal expressions in the text. It takes a sentence as input and looks for temporal expressions in it. Pre-processing: At this step the input text undergoes syn"
S12-1048,J96-2004,0,0.00843001,"les are assigned both to phrases and their headwords, but only the headwords are evaluated for this task. The spatial relations indicate a triplet of these roles. The general-type is assigned to each triplet of spatial indicator, trajector and landmark. At the starting point two annotators including one task-organizer and another non-expert annotator, annotated 325 sentences for the spatial roles and relations. The purpose was to realize the disagreement points and prepare a set of instructions in a way to achieve highest-possible agreement. From the first effort an inter-annotator agreement (Carletta, 1996) of 0.89 for Cohen’s kappa was obtained. We continued with the a third annotator for the remaining 888 sentences. The annotator had an explanatory session and received a set of instructions and annotated examples to decrease the ambiguity in the annotations. To avoid complexity only the relations that are directly expressed in the sentence are annotated and spatial reasoning was avoided during the annotations. Sometimes the trajectors and landmarks or both are implicit, meaning that there is no word in the sentence to represent them. For example in the sentence Come over here, the trajector yo"
S12-1048,kordjamshidi-etal-2010-spatial,1,0.615597,"Missing"
S12-1048,mani-etal-2008-spatialml,0,0.167952,"such as ACE, GUM, GML, KML, TRML which are briefly described and compared to SpatialML scheme in (MITRE Corporation, 2010). But to our knowledge, the main obstacles for employing machine learning in this context and the very limited usage of this effective approach have been (a) the lack of an agreement on a unique semantic model for spatial information; (b) the diversity of formal spatial relations; and consequently (c) the lack of annotated data on which machine learning can be employed to learn and extract the spatial relations. The most systematic work in this area includes the SpatialML (Mani et al., 2008) scheme which focuses on geographical information, and the work of (Pustejovsky and Moszkowicz, 2009) in which the pivot of the spatial information is the spatial verb. The most recent and active work is the ISO-Space scheme (Pustejovsky et al., 2011) which is based on the above two schemes. The ideas behind ISOSpace are closely related to our annotation scheme in (Kordjamshidi et al., 2010b), however it considers more detailed and fine-grained spatial and linguistic elements which makes the preparation of the data for machine learning more difficult. Spatial information is directly related to"
S12-1048,S12-1056,0,0.111317,"s a true prediction requires all the three elements are correctly predicted at the same time. The last evaluation is on how well the systems are able to retrieve the relations and their general type i.e {region, direction, distance} at the same time. To evaluate the GENERAL - TYPE similarly the RELA TION tag is checked. For a true prediction, an exact match between the ground-truth and all the elements of the predicted RELATION tag including TR, LM,SP and GENERAL - TYPE is required. 7 Systems and results One system with two runs was submitted from the University of Texas Dallas. The two runs (Roberts and Harabagiu, 2012), UTDS P RL- SUPERVISED 1 and UTDS P RL- SUPERVISED 2 are based on the joint classification of the spatial triplets in a binary classification setting. To produce the candidate (indicator, trajector, landmark) triples, in the first stage heuristic rules targeting a high recall are used. Then a binary support vector machine classifier is employed to predict whether a triple is a spatial relation or not. Both runs start with a large number of manually engineered features, and use floating forward feature selection to select the most important ones. The difference between the two runs of UTDS P R"
S12-1048,C08-2024,0,\N,Missing
S13-2014,W06-1618,0,0.0345603,"ns denoting the same date and vagueness in language, rule-based approaches are usually employed for the normalization task, and our implementation is a rule-based system. The normalization procedure is the same as described in (Kolomiyets and Moens, 2010), which participated in TempEval-2. 2.2 Event Processing The proposed method to event recognition implements a supervised machine learning approach that classifies every single token in the input sentence as an event instance of a specific semantic type. We implemented a logistic regression model with features largely derived from the work of Bethard and Martin (2006): • the token, its lemma, coarse and fine-grained POS tags, token’s suffixes and affixes; • token’s hypernyms and derivations in WordNet; • the grammatical class of the chunk, in which the token occurs; • the lemma of the governing verb of the token; • phrasal chunks in the contextual window; 84 • the light verb feature for the governing verb; • the polarity of the token’s context; • the determiner of the token and the sentence’s subject; In addition, we classify the tense attribute for the detected event by applying a set of thirteen handcrafted rules. 2.3 Temporal Relation Processing Tempora"
S13-2014,S10-1072,1,0.856272,"ags are determiners, adjectives, and cardinals. Another set of rules specifies unsuitable timexes, such as single cardinals with values outside predefined ranges of day-of-month, monthof-year and year numbers. Normalization of temporal expressions is a process of estimating standard temporal values and types for temporal expressions. Due to a large variance of expressions denoting the same date and vagueness in language, rule-based approaches are usually employed for the normalization task, and our implementation is a rule-based system. The normalization procedure is the same as described in (Kolomiyets and Moens, 2010), which participated in TempEval-2. 2.2 Event Processing The proposed method to event recognition implements a supervised machine learning approach that classifies every single token in the input sentence as an event instance of a specific semantic type. We implemented a logistic regression model with features largely derived from the work of Bethard and Martin (2006): • the token, its lemma, coarse and fine-grained POS tags, token’s suffixes and affixes; • token’s hypernyms and derivations in WordNet; • the grammatical class of the chunk, in which the token occurs; • the lemma of the governin"
S13-2014,P11-2047,1,0.854367,"to the proposed pair. Moreover, TempEval-3 proposes the task of end-to-end temporal processing in which Timex Processing Timex Recognition and Normalization The proposed method for timex recognition implements a supervised machine learning approach that processes each chunk-phrase derived from the parse tree. Time expressions are detected by the model as phrasal chunks in the parse with their corresponding spans. In addition, the model is bootstrapped by substitutions of temporal triggers with their synonyms learned by the Latent Words Language Model (Deschacht et al., 2012) as described in (Kolomiyets et al., 2011). We implemented a logistic regression model that makes use of the following features: • the head word of the phrase and its POS tag; • all tokens and POS tags in the phrase as a bag of words; • the word-shape representation of the head word and the entire phrase, e.g. Xxxxx 99 for the expression April 30; 83 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 83–87, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics • the condensed word-shape representatio"
S13-2014,P12-1010,1,0.904022,"verb; • the polarity of the token’s context; • the determiner of the token and the sentence’s subject; In addition, we classify the tense attribute for the detected event by applying a set of thirteen handcrafted rules. 2.3 Temporal Relation Processing Temporal relation recognition is the most difficult task of temporal information processing, as it requires recognitions of argument pairs, and subsequent classifications of relation types. Our approach employs a shift-reduce parsing technique, which treats each document as a dependency structure of annotations labeled with temporal relations (Kolomiyets et al., 2012). On the one hand, the advantage of the model is that the relation arguments and the relation between them are extracted as a single decision of a statistical classification model. On the other hand, such a decision is local and might not lead to the optimal global solution1 . The following features for deterministic shift-reduce temporal parsing are employed: • the token, its lemma, suffixes, coarse and finegrained POS tags; • the governing verb, its POS tag and suffixes; • the sentence’s root verb, its lemma and POS tag; • features for a prepositional phrase occurrence, and domination by an"
S13-2014,S07-1014,0,0.0769532,"system employs a number of machine learning classifiers to perform the core tasks of: identification of time expressions and events, recognition of their attributes, and estimation of temporal links between recognized events and times. The central feature of the proposed system is temporal parsing – an approach which identifies temporal relation arguments (eventevent and event-timex pairs) and the semantic label of the relation as a single decision. 2 Our Approach 2.1 2.1.1 1 Introduction Temporal Evaluations 2013 (TempEval-3) is the third iteration of temporal evaluations (after TempEval-1 (Verhagen et al., 2007) and TempEval2 (Verhagen et al., 2010)) which addresses the task of temporal information processing of text. In contrast to the previous evaluation campaigns where the temporal relation recognition task was simplified by restricting grammatical context (events in adjacent sentences, events and times in the same sentences) and proposed relation pairs, TempEval-3 does not set any context in which temporal relations have to be identified. Thus, for temporal relation recognition the challenges consist of: first, detecting a pair of events, or an event and a time that constitutes a temporal relatio"
S13-2014,S10-1010,0,\N,Missing
S13-2044,S13-2096,0,0.37199,"nces is required for a positive match under condition that the roles 259 tp tp + f n (1) (2) where tp is the number of true positives (the number of instances that are correctly found), f p is the number of false positives (number of instances that are predicted by the system but not a true instance), and f n is the number of false negatives (missing results). • Task E: Semantic classification of spatial relations identified in Task D. 5 tp tp + f p P recision · Recall P recision + Recall (3) System Description and Evaluation Results UNITOR. The UNITOR-HMM-TK system addressed Tasks A,B and C (Bastianelli et al., 2013). In Tasks A and C, roles are labeled by a sequencebased classifier: each word in a sentence is classified with respect to the possible spatial roles. An approach based on the SVM-HMM learning algorithm, formulated in (Tsochantaridis et al., 2006), was used. It is in line with other methods based on sequence-based classifier for Spatial Role Labeling, such as Conditional Random Fields (Kordjamshidi et al., 2011), and the same SVM-HMM learning algorithm (Kordjamshidi et al., 2012b). UNITOR’s labeling approach has been inspired by the work in (Croce et al., 2012), where an SVMHMM learning algori"
S13-2044,D11-1096,0,0.0486304,"Missing"
S13-2044,kordjamshidi-etal-2010-spatial,1,0.885302,"Missing"
S13-2044,S12-1048,1,0.784942,"Building upon the previous work, we used the notions of trajectors, landmarks and spatial indicators as introduced by Kordjamshidi et al. (2010). In addition, we further expanded the set of spatial roles labels with motion indicators, paths, directions and distances to capture fine-grained spatial semantics of static spatial relations (as the ones which do not involve motions), and to accommodate dynamic spatial relations (the ones which do involve motions). Introduction Spatial Role Labeling at SemEval-2013 is the second iteration of the task, which was initially introduced at SemEval-2012 (Kordjamshidi et al., 2012a). The second iteration extends the previous work with an additional training corpus, which contains besides “static” spatial relations, annotated motions. Motion detection is a novel task for annotating trajectors (objects, which are moving), landmarks (spatial context in which the motion is performed), motion indicators (lexical triggers which signals trajector’s motion), paths (a path along which the motion is performed), directions (absolute or relative directions of trajector’s motion) and distances (a distance as a product of motion). For annotating motions the existing annotation schem"
S13-2044,S12-1056,0,0.287434,"mine the proper conjunction of all roles, a Smoothed Partial Tree Kernel (SPTK) within the classifier that enhances both syntactic and lexical information of the examples was applied (Croce et al., 260 2011). This is a convolution kernel that measures the similarity between syntactic structures, which are partially similar and whose nodes can be different, but are, nevertheless, semantically related. Each example is represented as a tree-structure which is directly derived from the sentence dependency parse, and thus allows for avoiding manual feature engineering as in contrast to the work of Roberts and Harabagiu (2012). In the end, the similarity score between lexical nodes is measured by the Word Space model. UNITOR submitted two runs for the IAPR TC12 Image benchmark corpus (we refer to them as to UNITOR.Run1.1 and UNITOR.Run1.2) and one run for the Confluence Project corpus (UNITOR.Run2.1), based on the models individually trained on the different corpora. The difference between UNITOR.Run1.1 and UNITOR.Run1.2 is that for UNITOR.Run1.1 the results are obtained for all spatial roles (also the ones that have no spatial relation), and UNITOR.Run1.2 only provided the roles for which also spatial relations we"
S13-2044,W11-0416,0,0.0444525,"Format One important change to the data was made in SpRL-2013. In contrast to SpRL-2012, where spatial roles were annotated over “head words” whose indexes were part of unique identifiers, in SpRL2013 we switched to span-based annotations. Moreover, in order to provide a single data format for the task, we transformed SpRL-2012 data into spanbased annotations, in course of which, we identified a number of annotation errors and made further improvements for about 50 annotations. For annotating the Confluence Project corpus we used a freely available annotation tool MAE created by Amber Stubbs (Stubbs, 2011). The resulting data format uses the same annotation tags as in SpRL2012, but each role annotation refers to a character offset in the original text2 . Spatial relations are composed of references to annotations by their unique identifiers. Similarly to SpRL-2012, we allowed annotators to provide non-consuming annotations, where entity mentions, for which spatial roles can be identified, are omitted in text but necessary for a spatial relation triggered by either a spatial indicator or a motion indicator. Two spatial roles are eligible for non-consuming annotations: trajectors and landmarks. 4"
S15-2012,S12-1051,0,0.0391829,"r a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011)"
S15-2012,S14-2010,0,0.030874,"Missing"
S15-2012,I13-1041,0,0.0128648,"dditionally provide an analysis of the dataset and point to some peculiarities of the evaluation setup. 1 Introduction Recognizing tweets that convey the same meaning (paraphrases) or similar meaning is useful in applications such as event detection (Petrovi´c et al., 2012), tweet summarization (Yang et al., 2011), and tweet retrieval (Naveed et al., 2011). Paraphrase detection in tweets is a more challenging task than paraphrase detection in other domains such as news (Xu et al., 2013). Besides brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in st"
S15-2012,P09-1053,0,0.172779,"ammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluatio"
S15-2012,P12-1091,0,0.0512082,"g the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset prov"
S15-2012,J10-3003,0,0.109662,"es brevity (max. 140 characters), tweets exhibit all the irregularities typical of social media text (Baldwin et al., 2013), such as informality, ungrammaticality, disfluency, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 201"
S15-2012,N12-1019,0,0.0843001,"ncy, and excessive use of jargon. In this paper we present the TweetingJay system for detecting paraphrases in tweets, with which we participated in Task 1 of SemEval 2015 evaluation exercise (Xu et al., 2015). Our system builds on findings from a large body of work on semantic texˇ c et al., 2012; Sultan et al., tual similarity (STS) (Sari´ 2 Related Work There is a large body of work on automated paraphrase detection; see (Madnani and Dorr, 2010) for a comprehensive overview. The majority of research efforts focus on detecting paraphrases in standard texts such as news (Das and Smith, 2009; Madnani et al., 2012) or artificially generated text (Madnani et al., 2012). State-of-the-art approaches typically combine several measures of semantic similarity between text fragments. For instance, Madnani et al. (2012) achieve state-of-the-art performance by combining eight different machine translation metrics in a supervised fashion. A task closely related to paraphrase detection is semantic textual similarity (STS), introduced at SemEval 2012 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages"
S15-2012,D14-1162,0,0.0807917,"tokens in tweets, and the other taking into account only content words. 2 https://github.com/ma-sultan/ monolingual-word-aligner 72 Anchor count (ANC). We re-implemented the M ULTI P model of Xu et al. (2014).3 As anchor candidates we consider all pairs of content words from the two tweets. We use a minimalistic set of features including (1) Levenshtein distance between candidate words, (2) several binary features indicating relatedness of words (e.g., lowercased tokens match, POStags match), and (3) semantic similarity obtained as the cosine of word embeddings, obtained with the GloVe model (Pennington et al., 2014) trained on Twitter data.4 To account for feature interactions, following (Xu et al., 2014), we also use conjunction features. We use the number of anchors identified by this method for a pair of tweets as a feature for our SVM model. 4 Evaluation Each team was allowed to submit two runs on the test set provided by the task organizers (Xu et al., 2015). Participants were provided with a training set (13,063 pairs) and a development set (4,727 pairs). We used the train and development set to optimize the hyperparameters C and γ of our SVM model with the RBF kernel. For the final evaluation, the"
S15-2012,N12-1034,0,0.314377,"Missing"
S15-2012,S12-1060,1,0.880991,"Missing"
S15-2012,S14-2039,0,0.0567314,"nces. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2012), and (2) word alignment features, Sari´ based on (a) the output of the word alignment model by Sultan et al. (2014) and (b) a re-implementation of the M ULTI P model by Xu et al. (2014). In the dataset provided by the organizers, each tweet is associated with a topic, with 10 to 100 tweet pairs per topic. An important preprocessing step is to remove tokens that can be found in the name of a topic. For example, for the topic “Roberto Mancini”, we trim the tweets “Roberto Mancini gets the boot from the Man City” and “City sacked Mancini” to “gets the boot from the Man City” and “City sacked”, respectively, and then compute the features on the trimmed tweets. The rationale is that, given a topic, there is an"
S15-2012,W13-2515,0,0.245099,"Missing"
S15-2012,Q14-1034,0,0.531452,"Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number of features is relatively small, we use SVM with a non-linear (RBF) kernel. Our features can be divided into (1) semantic overlap features, most of which are adaptations of STS features proposed by ˇ c et al. (2"
S15-2012,D11-1061,0,0.158719,"2 (Agirre et al., 2012). There is now a 1 http://takelab.fer.hr/tweetingjay 70 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 70–74, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics significant amount of work on this task. The best performing STS systems employ various methods for aligning semantically corresponding words or otherwise quantifying the amount of semantically congruent content between two sentences (Sultan et ˇ c et al., 2012). al., 2014; Sari´ In contrast, STS research on Twitter data has been scarce. Zanzotto et al. (2011) detect content redundancy between tweets, where redundant means paraphrased or entailed content. They achieve reasonable performance with SVM using vector-comparison and syntactic tree kernels. Xu et al. (2014) propose M UL TI P, a latent variable model for joint inference of correspondence of words and sentences. An unsupervised model based on representing sentences in latent space is presented by Guo and Diab (2012). 3 TweetingJay TweetingJay is essentially a supervised machine learning model, which employs a number of semantic similarity features (18 features in total). Because the number"
S15-2012,S15-2001,0,\N,Missing
S15-2149,W14-0150,0,0.0161128,"HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants of motion events and their attributes. 3 UTD submitted three runs, however, after evaluating all the data, all three runs achieved similar scores; the results reported here are for their third and final submitted run. 4 These baseline classifiers were developed at Brandeis University by Aaron Levine and Zachary Yocum. Cf. Section 5.1 for full description. 5 This system was developed at Brandeis Un"
S15-2149,J14-1004,0,0.00559209,"d rapid training and model inspection. The hypotheses were written out to XML in accordance to the task DTD. We used a small set of 9 core features, augmented with bigram contexts, resulting in a total of 27 features. These features consist of lexical, syntactic, and semantic information, many of which have 6 We experimented with additional features for attribute classification, such as counting tags and their types in the local context of the trigger, however additional features all resulted in performance decreases. been applied successfully in a variety of information extraction tasks (Fei Huang et al., 2014), such as named entity recognition (Vilain et al., 2009b) or coreference resolution (Fernandes et al., 2014). The complete set of features are outlined in Table 3. Type Lexical Syntactic Semantic Sparser Id word[-1,0,1] isupper[-1,0,1] wordlen[-1,0,1] pos[-1,0,1] ner[-1,0,1] CATEGORY[-1,0,1] FORM[-1,0,1] LCATEGORY[-1,0,1] LFORM[-1,0,1] Value string binary ternary7 POS tag NER tag Sparser category Sparser form Sparser category Sparser form Table 4: Top Ten Positive Feature Weights Table 3: BRANDEIS-CRF Features For part-of-speech (POS) and named entity (NE) tags, we used the Stanford Log-linear"
S15-2149,P05-1045,0,0.0275864,"and F1. c. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (P"
S15-2149,S13-2044,1,0.317352,"that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the links between these roles which form spatial relation"
S15-2149,kordjamshidi-etal-2010-spatial,1,0.728017,"Missing"
S15-2149,S12-1048,1,0.515402,"tion, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths. SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information. In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task. 1 Introduction SpaceEval builds on the Spatial Role Labeling (SpRL) task introduced in SemEval 2012 (Kordjamshidi et al., 2012) and used in SemEval 2013 (Kolomiyets et al., 2013). The base annotation scheme of the previous tasks was introduced in (Kordjamshidi et al., 2010), with empirical practices in (Kordjamshidi et al., 2011; Kordjamshidi and Moens, 2015). While those previous tasks are similar in their goal, SpacEval adopts the annotation specification from ISOspace (Pustejovsky et al., 2011a; Moszkowicz and Pustejovsky, 2010; ISO/TC 37/SC 4/WG 2, 2014), a new standard for capturing spatial information. The SpRL in SemEval 2012 had a focus on the main roles of trajectors, landmarks, spatial indicators, and the li"
S15-2149,P14-5010,0,0.010339,"a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial rol"
S15-2149,D14-1162,0,0.119259,"and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellbaum, 1998), PropBank (Palmer et al., 2003) and the Predicate Matrix (de la Calle et al., 2014). UTD A suite of 13 classifiers for classifying spatial roles and relations including classifiers for stationary spatial relations and their participants in addition to classification of participants o"
S15-2149,W13-0503,1,0.756586,"ial relations between spatial signals and spatial elements (connected, unconnected, part-of, etc.). b. Identify their attributes. Spatial Orientation Identification (OLink) a. Identify orientational relations between spatial signals and spatial elements (above, under, in front of, etc.). b. Identify their attributes. 3 The SpaceBank Corpus The data for this task are comprised of annotated textual descriptions of spatial entities, places, paths, motions, localized non-motion events, and spatial relations. The data set selected for this task, a subset of the SpaceBank corpus first described in (Pustejovsky and Yocum, 2013), consists of submissions retrieved from the Degree Confluence Project (DCP) (Jarrett, 2013), Berlitz Travel Guides retrieved from the American National Corpus (ANC) (Reppen et al., 2005), and entries retrieved from a travel weblog, Ride for Climate (RFC) (Kroosma, 2012). The DCP documents are the same set as those annotated with Spatial Role Labeling (SpRL) for SemEval2013 Task 3 (Kolomiyets et al., 2013), however, for this task, the DCP texts were re-annotated according to ISO-Space. 3.1 Annotation Schema The annotation of spatial information in text involves at least the following: a PLACE"
S15-2149,W11-0416,0,0.0287912,"1577 61 3 148 34 69 15 16 39 17 19 14 14 Table 1: Corpus Statistics Phase 2 Extent tag adjudication. Phase 3 Link tag argument and attribute annotation. Phase 4 Link tag adjudication. Phases 2 and 4 produced gold standards from annotations in the preceding annotation phases. This annotation strategy ensured that the intermediate gold standard extent tag set was adjudicated before any link tag annotations were performed. The annotation and adjudication effort was conducted at Brandeis University using Multidocument Annotation Environment (MAE) and Multi-annotator Adjudication Interface (MAI) (Stubbs, 2011). We used MAE to perform each phase of the annotation procedure and MAI to adjudicate and produce gold standard standoff annotations in XML format. In addition to the ISO-Space annotation tags and attributes, as a post-process, we also provided sentence and lexical tokenization as a separate standoff annotation layer in the XML data for the training and test sets. Each document was covered by a minimum of three annotators for each annotation phase (though not necessarily the same annotators per phase). As such, we report inter-annotator agreement (IAA) as a mean Fleiss’s κ coefficient for all"
S15-2149,N03-1033,0,0.0146901,"ink.a precision, recall, and F1. c. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. a. MoveLink.a, QSLink.a, OLink.a precision, recall, and F1. b. MoveLink.b, QSLink.b, OLink.b precision, recall, and F1 for each attribute, and an overall precision, recall, and F1. Submissions and Results In this section we evaluate results from runs of five systems. Three systems were submitted by outside 888 BRANDEIS-CRF A system using a conditional random field (CRF) model (Okazaki, 2007) with features including Stanford POS and NER tags (Toutanova et al., 2003) (Finkel et al., 2005) in combination with Sparser (McDonald, 1996) tags.5 HRIJP-CRF-VW A system using a CRF model using CoreNLP, (Manning et al., 2014), CRFSuite (Okazaki, 2007) and Vowpal Wabbit (Langford et al., 2007) with lemmatization, POS, NER, GloVe word vector (Pennington et al., 2014) and dependency parse features. IXA X-Space: A system using a binary support vector machine model from SVM-light (Joachims, 1999) and a pipeline architecture using ClearNLP (Choi and Adviser-Palmer, 2012), OpenNLP (OpenNLP, 2014), and leveraging computational linguistic resources including WordNet (Fellba"
S15-2149,W09-1124,0,0.0506481,"Missing"
S15-2149,R09-1083,0,0.0331692,"Missing"
S15-2149,J05-1004,0,\N,Missing
S15-2149,J14-4004,0,\N,Missing
S16-1199,S16-1165,0,0.0380841,"of Computer Science KU Leuven, Belgium {tuur.leeuwenberg, sien.moens}@cs.kuleuven.be Abstract In this paper, we describe the KULeuvenLIIR system at the Clinical TempEval 2016 Shared Task for the narrative container relation sub-task (CR). Our approach is based on the cTAKES Temporal system (Lin et al., 2015). We explored extending this system with different features. Moreover, we provide an error analysis of the submitted system, and report on some additional experiments done after submission. 1 Introduction We describe the KULeuven-LIIR submissions for the Clinical TempEval 2016 shared task (Bethard et al., 2016). Our motivation for this first participation is to gain insight into the task, and the data, as a basis for future work. We participated in the narrative container relation (CR) task. In the CR task narrative containment relations between events, and events and temporal expressions are to be extracted. Two examples of such relations are given in Sentence 1. (1) A colonoscopy on 27 September 2008 revealed a circumferential lesion. The relations that are to be extracted are • CONTAINS(27 September 2008, colonoscopy) • CONTAINS(colonoscopy, lesion) In the shared task, the clinical records on col"
S16-1199,S15-2136,0,\N,Missing
S17-2181,N03-1033,0,0.0245436,"Missing"
S17-2181,S17-2093,0,0.0251837,"ystem of the KULeuven-LIIR submission for Clinical TempEval 2017. We participated in all six subtasks, using a combination of Support Vector Machines (SVM) for event and temporal expression detection, and a structured perceptron for extracting temporal relations. Moreover, we present and analyze the results from our submissions, and verify the effectiveness of several system components. Our system performed above average for all subtasks in both phases. 1 Introduction In this paper, we describe the system used for the KULeuven-LIIR submissions at SemEval task 12, named Clinical TempEval 2017 (Bethard et al., 2017), which is concerned with temporal information extraction from clinical records. In Clinical TempEval extraction of temporal information is split into six subtasks. Our system participated in all tasks: This year, a new aspect of Clinical TempEval is that systems will be evaluated across domains, which involves two phases: Firstly, unsupervised domain adaptation (Phase I), where the training data is in the colon cancer domain, and the test data in the brain cancer domain. And secondly, supervised domain adaptation (Phase II), where the vast majority of the training data are colon cancer report"
S17-2181,P07-1034,0,0.0339544,"nada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Figure 1: Schematic overview of our system. Components we expect to help domain adaptation are dashed. the brain cancer domain. Some statistics about the dataset can be found in Table 1. Table 1: Dataset statistics for the THYME sections used in our experiments. Section Documents Training Colon Cancer Training Brain Cancer Test Brain Cancer 591 30 148 Our first simple method for adapting to a new domain, when given target-domain training data (Phase II), is to assign more weight to the targetdomain data at training time (Jiang and Zhai, 2007). In our submissions we assigned a 10 times higher weight to the target-domain training data compared to the colon cancer training data. In all experiments, we preprocess the text by using a very straightforward tokenization procedure considering punctuation1 or newline tokens as individual tokens, and splitting on spaces. We also employ lowercasing, and conflate all digits to a single representation. An example would be: October 20, 1991 ⇒ october 55 , 5555 For our part-of-speech features, we rely on the Stanford POS Tagger (Toutanova et al., 2003), with the English bidirectional tagger model"
S17-2181,E17-1108,1,0.919667,"irstly, unsupervised domain adaptation (Phase I), where the training data is in the colon cancer domain, and the test data in the brain cancer domain. And secondly, supervised domain adaptation (Phase II), where the vast majority of the training data are colon cancer reports, and a small number of brain cancer reports is made available for training as well. The test data is again in the brain cancer domain. Our system consist of a combination of linear Support Vector Machines (SVM) for entity span and attribute recognition (tasks ES, EA, TS and TA), and a document-level structured perceptron (Leeuwenberg and Moens, 2017) for relation extraction tasks (tasks DR and CR). We used three system components for the domain adaptation: (1) assigning more weight to target-domain training data, (2) introduction of a UNK (unknown) token to model out-of-vocabulary words, and (3) exploitation of relational properties of temporality during prediction. In Section 2, we provide a detailed description of our full system, and in Section 3 we discuss the results from our submissions. 1. Detection of event spans (ES) 2 2. Identification of event attributes (EA) Our system consist of three main components (1) preprocessing, (2) en"
S17-2181,P06-1095,0,0.139465,"Missing"
S17-2181,Q14-1012,0,0.0472766,"Missing"
W06-0505,C96-1005,0,0.0533787,"Missing"
W06-0505,P98-1013,0,0.00451334,"rbs. This can become problematic as WordNet has no hypernym/hyponym relation (or equivalent) for the synsets of adjectives and adverbs. WordNet has an equivalent relation for verbs (hypernym/troponym), but this structures the verb synsets in a big number of loosely structured trees, which is less suitable for the described method. VerbNet (Kipper et al., 2000) seems a more promising resource to use when classifying verbs, and we will also investigate the use of other lexical databases, such as ThoughtTreasure (Mueller, 1998), Cyc (Lenat, 1995), Openmind Commonsense (Stork, 1999) and FrameNet (Baker et al., 1998). 8 Future work In this section we’ll discuss some of the work we plan to do in the future. First of all we wish to evaluate our algorithm on standard test sets, such as the data of the Senseval conference5 , which tests performance on word sense disambiguation, and the data of the CoNLL 2003 shared task6 , on named entity recognition. An important weakness of our algorithm is the fact that, to label a sentence, we have to traverse the hierarchy tree and choose the correct synsets at every level. An error at a certain level can not be recovered. Therefor, we would like to perform Acknowledgmen"
W06-0505,W04-0837,0,0.0526685,"Missing"
W06-0505,P99-1020,0,0.030248,"Missing"
W06-0505,N04-1042,0,0.015437,"likelihood of the conditional distribution (McCallum, 2003). We are confronted with the problem of efficiently calculating the expectation of each feature function with respect to the CRF model distribution for every observation sequence x in the training data. Formally, we are given a set x(i) , y(i) (i) i=1 4 Parameter estimation n (i) λk fk (yt−1 , yt , x(i) ) log Z(x(i) ) − Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). of training examples D = T P K N P P l(θ) = 5 Reducing complexity In this section we’ll see how we create groups of features for every label that enable an important reduction in complexity of both labeling and training. We’ll first discuss how these groups of features are created (section 5.1) and then how both labeling (section 5.2) and training (section 5.3) are performed using these groups. (3) i=1 After substituting the CRF model (2) in the like35 collection of features"
W06-0505,W95-0107,0,0.0172243,"Missing"
W06-0505,N03-1028,0,0.029653,"ng the expectation of each feature function with respect to the CRF model distribution for every observation sequence x in the training data. Formally, we are given a set x(i) , y(i) (i) i=1 4 Parameter estimation n (i) λk fk (yt−1 , yt , x(i) ) log Z(x(i) ) − Lafferty et al. (Lafferty et al., 2001) have shown that CRFs outperform both MEMM and HMM on synthetic data and on a part-of-speech tagging task. Furthermore, CRFs have been used successfully in information extraction (Peng and McCallum, 2004), named entity recognition (Li and McCallum, 2003; McCallum and Li, 2003) and sentence parsing (Sha and Pereira, 2003). of training examples D = T P K N P P l(θ) = 5 Reducing complexity In this section we’ll see how we create groups of features for every label that enable an important reduction in complexity of both labeling and training. We’ll first discuss how these groups of features are created (section 5.1) and then how both labeling (section 5.2) and training (section 5.3) are performed using these groups. (3) i=1 After substituting the CRF model (2) in the like35 collection of features fk (yt−1 , yt , x) for which it is possible to find a node vt−1 and input x for which fk (vt−1 , v, x) 6= 0. If v is a"
W06-0505,P98-2228,0,0.0191981,"Missing"
W06-0505,P95-1026,0,0.0164833,"Missing"
W06-0505,C98-2223,0,\N,Missing
W06-0505,C98-1013,0,\N,Missing
W06-0505,W03-0430,0,\N,Missing
W06-3804,W99-0611,0,0.0347607,"d at the University of Edinburgh and the Charniak parser developed at Brown University. 2.1 Noun Phrase Coreference Resolution Coreference resolution focuses on detecting “identity&apos;&apos; relationships between noun phrases (i.e. not on is-a or whole/part links). It is natural to view coreferencing as a partitioning or clustering of the set of entities. The idea is to group coreferents into the same cluster, which is accomplished in two steps: 1) detection of the entities and extraction of their features set; 2) clustering of the entities. For the first subtask we use the same set of features as in Cardie and Wagstaff (1999). For the second step we used the progressive fuzzy clustering algorithm described in Angheluta et al. (2004). 2.2 Learning Biographical Terms We learn a term’s biographical value as the correlation of the term with texts of biographical nature. There are different ways of learning associations present in corpora (e.g., use of the mutual information statistic, use of the chi-square statistic). We use the likelihood ratio for a binomial distribution (Dunning 1993), which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographic"
W06-3804,J93-1003,0,0.0634814,"ction of their features set; 2) clustering of the entities. For the first subtask we use the same set of features as in Cardie and Wagstaff (1999). For the second step we used the progressive fuzzy clustering algorithm described in Angheluta et al. (2004). 2.2 Learning Biographical Terms We learn a term’s biographical value as the correlation of the term with texts of biographical nature. There are different ways of learning associations present in corpora (e.g., use of the mutual information statistic, use of the chi-square statistic). We use the likelihood ratio for a binomial distribution (Dunning 1993), which tests the hypothesis whether the term occurs independently in texts of biographical nature given a large corpus of biographical and non-biographical texts. For considering a term as biography-related, we set a likelihood ratio threshold such that the hypothesis can be rejected with a certain significance level. 2.3 Reference Detection between Entities We assume that the syntactic relationships between entities (proper or common nouns) in a text give us information on their semantic reference status. In our simple experiment, we consider reference relationships found within a single sen"
W06-3804,W04-3252,0,\N,Missing
W09-2408,N07-1053,0,0.0267878,"Missing"
W09-2408,P00-1010,0,0.134861,"Missing"
W12-2031,W10-4236,0,0.0691016,"Missing"
W12-2031,W11-2838,0,0.120535,"ion process are further explained in (Dale et al., 2012). 264 β2 precision · recall · precision + recall (3) where β is used as a weight factor regulating the trade-off between recall and precision. We use the balanced F -score, i.e. β = 1, such that recall and precision are equally weighted. Combined We provide results on prepositions and determiners combined, and for each of these two subcategories separately. We also report on each of the different error types separately. 2 See http://www.correcttext.org/hoo2012. 3 Related Work HOO-2012 follows on from the HOO-2011 Shared Task Pilot Round (Dale and Kilgarriff, 2011). That task targeted a broader range of error types, and used a much smaller dataset. Most work on models for determiner and preposition generation has been developed in the context of machine translation output (e.g. (Knight and Chander, 1994), (Minnen et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most"
W12-2031,dale-narroway-2012-framework,0,0.0122216,"versity Press Error Coding System, fully described in (Nicholls, 2003). 263 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 263–271, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics • Replace preposition (RT): In the other hand. . . → On the other hand. . . 2.3 Evaluation Criteria and Metrics • Missing preposition (MT): She woke up 6 o’clock. → She woke up at 6 o’clock. For evaluation in the HOO framework, a distinction is made between scores and measures. The complete evaluation mechanism is described in detail in (Dale and Narroway, 2012) and on the HOO-2012 website.2 • Unnecessary preposition (UT): He must go to home. → He must go home. Scores Three different scores are used: 2.2 Data The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al., 2011). This corpus contains texts written by students who attended the Cambridge ESOL First Certificate in English examination in 2000 and 2001. The entire development dataset comprises 374680 words, with an average of 375 words per file. The test data consists of a further 100 files provided by Cambridge University Press ("
W12-2031,W12-2006,0,0.358007,"false positives (the number of instances that are incorrectly found), and f n the number of false negatives (missing results). Fβ = (1 + β 2 ) Table 1: Data error statistics. Counts of the different error types are provided in Table 1. The table shows counts for the development dataset (‘Dev’) and two versions of the gold standard test data: the original version as derived from the CUP-provided dataset (‘Test A’), and a revised version (‘Test B’) which was compiled in response to requests for corrections from participating teams. The datasets and the revision process are further explained in (Dale et al., 2012). 264 β2 precision · recall · precision + recall (3) where β is used as a weight factor regulating the trade-off between recall and precision. We use the balanced F -score, i.e. β = 1, such that recall and precision are equally weighted. Combined We provide results on prepositions and determiners combined, and for each of these two subcategories separately. We also report on each of the different error types separately. 2 See http://www.correcttext.org/hoo2012. 3 Related Work HOO-2012 follows on from the HOO-2011 Shared Task Pilot Round (Dale and Kilgarriff, 2011). That task targeted a broader"
W12-2031,W07-1607,0,0.0675979,"Missing"
W12-2031,C08-1022,0,0.173929,"ine translation output (e.g. (Knight and Chander, 1994), (Minnen et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). Yi et al. (2008) used web counts to determine correct article usage, while Han et al. (2010) trained a classifier solely on a large error-tagged learner corpus for preposition error correction. 4 System Description 4.1 Global System Workflow 3. Correction validation: Once a correction has been proposed, it is validated by a language model derived from a large corpus of highquality English text. 4.1.1 In HOO-2012, texts submitted for automated corrections are written by learners of English. Besides the error types that are addressed in HOO-2012, misspellings are another type of highly-frequent errors. For exam"
W12-2031,I08-1059,0,0.053427,"Missing"
W12-2031,han-etal-2004-detecting,0,0.0250719,"for determiner and preposition generation has been developed in the context of machine translation output (e.g. (Knight and Chander, 1994), (Minnen et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). Yi et al. (2008) used web counts to determine correct article usage, while Han et al. (2010) trained a classifier solely on a large error-tagged learner corpus for preposition error correction. 4 System Description 4.1 Global System Workflow 3. Correction validation: Once a correction has been proposed, it is validated by a language model derived from a large corpus of highquality English text. 4.1.1 In HOO-2012, texts submitted for automated corrections are written by learners of English. Besides the error types that are addressed in HO"
W12-2031,han-etal-2010-using,0,0.055603,"et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). Yi et al. (2008) used web counts to determine correct article usage, while Han et al. (2010) trained a classifier solely on a large error-tagged learner corpus for preposition error correction. 4 System Description 4.1 Global System Workflow 3. Correction validation: Once a correction has been proposed, it is validated by a language model derived from a large corpus of highquality English text. 4.1.1 In HOO-2012, texts submitted for automated corrections are written by learners of English. Besides the error types that are addressed in HOO-2012, misspellings are another type of highly-frequent errors. For example, one student writes the following: In my point of vue, Internet is the m"
W12-2031,W00-0708,0,0.126618,"Missing"
W12-2031,N07-1007,0,0.0271508,"ositions and determiners combined, and for each of these two subcategories separately. We also report on each of the different error types separately. 2 See http://www.correcttext.org/hoo2012. 3 Related Work HOO-2012 follows on from the HOO-2011 Shared Task Pilot Round (Dale and Kilgarriff, 2011). That task targeted a broader range of error types, and used a much smaller dataset. Most work on models for determiner and preposition generation has been developed in the context of machine translation output (e.g. (Knight and Chander, 1994), (Minnen et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). Yi et al. (2008) used web counts to determine correct article usage, while Han et al. (2010) trained a classifier solely on a large error-tagged learne"
W12-2031,P11-1019,0,0.0628464,"tion (RT): In the other hand. . . → On the other hand. . . 2.3 Evaluation Criteria and Metrics • Missing preposition (MT): She woke up 6 o’clock. → She woke up at 6 o’clock. For evaluation in the HOO framework, a distinction is made between scores and measures. The complete evaluation mechanism is described in detail in (Dale and Narroway, 2012) and on the HOO-2012 website.2 • Unnecessary preposition (UT): He must go to home. → He must go home. Scores Three different scores are used: 2.2 Data The HOO development dataset consists of 1000 exam scripts drawn from a subset of the CLC FCE Dataset (Yannakoudakis et al., 2011). This corpus contains texts written by students who attended the Cambridge ESOL First Certificate in English examination in 2000 and 2001. The entire development dataset comprises 374680 words, with an average of 375 words per file. The test data consists of a further 100 files provided by Cambridge University Press (CUP), with 18013 words, and an average of 180 words per file. Type # Dev # Test A # Test B RD MD UD 609 2230 1048 38 125 53 37 131 62 Det 3887 217 230 RT MT UT 2618 1104 822 136 57 43 148 56 39 Prep 4545 236 243 Total 8432 453 473 Words/Error 44.18 39.77 38.08 1. Detection: does"
W12-2031,I08-2082,0,0.242025,"xt of machine translation output (e.g. (Knight and Chander, 1994), (Minnen et al., 2000), (De Felice and Pulman, 2007) and (Toutanova and Suzuki, 2007)). Some of these methods depend on full parsing of text, which is not reliable in the context of noisy non-native English texts. Only more recently, models for automated error detection and correction of non-native texts have been explicitly developed and studied. Most of these methods use large corpora of well-formed native English text to train statistical models, e.g. (Han et al., 2004), (Gamon et al., 2008) and (De Felice and Pulman, 2008). Yi et al. (2008) used web counts to determine correct article usage, while Han et al. (2010) trained a classifier solely on a large error-tagged learner corpus for preposition error correction. 4 System Description 4.1 Global System Workflow 3. Correction validation: Once a correction has been proposed, it is validated by a language model derived from a large corpus of highquality English text. 4.1.1 In HOO-2012, texts submitted for automated corrections are written by learners of English. Besides the error types that are addressed in HOO-2012, misspellings are another type of highly-frequent errors. For exam"
W13-2020,W13-2026,0,0.153931,"ameters (C, and the RBF kernel parameter σ) identical across the different entries of the table. While in theory a separate parameter optimisation on each model could affect the comparison, this showed to be of little qualitative influence on the results. 4 We optimised α to be 0.4, by tuning on a 25-fold crossvalidation, only using training and validation set. 137 Final results on test data On submission of the output from the test data, our system achieved a Slot Error Rate (SER) of 0.830 (precision: 0.500, recall: 0.227, F1: 0.313), coming in second place after the University of Ljubljana (Zitnik et al., 2013) who scored a SER of 0.727 (precision: 0.682, recall: 0.341, F1: 0.455). Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural Language Processing with Python. OReilly Media Inc. Robert Bossy, Philippe Bessi`res, and Claire N´edellec. 2013. BioNLP shared task 2013 - an overview of the genic regulation network task. In Proceedings of BioNLP Shared Task 2013 Workshop, Sofia, Bulgaria, August. Association for Computational Linguistics. Exploring structure One of the main issues of interest for future research is the inherent hierarchical structure in the interactions under consideration. These"
W13-2020,W13-2023,0,\N,Missing
W13-2020,W11-1810,0,\N,Missing
W14-5407,P05-1045,0,0.00394994,"in the brides and bridegrooms (which are also the most important persons when indexing the video) we use a gazetteer with their names for recognizing the names in the texts. 3.3 Detecting the faces of persons In the video key frames are extracted at the rate of 1 frame per second using (ffmpeg, 2012), which ensures that no faces appearing in the video are omitted. To detect the faces in the video, a face detector tool from (Bradski, 2000) is used. Next, we extract the features from the faces detected in the video. Although there are several dedicated facial feature extraction methods such as (Finkel et al., 2005),(Strehl and Ghosh, 2003), in this implementation, we use a simple bag-of-visual-words model (Csurka et al., 2004). Once feature vectors are built, clustering of the bounding boxes of the detected faces is performed. Each object is, then, compared to the cluster centers obtained and is replaced with the closest center. The clustering is done using Elkan’s k-means algorithm (Jain and Obermayer, 2010) which produces the same results as the regular k-means algorithm, but is computationally more efficient. This accelerated algorithm eliminates some distance calculations by applying the triangle in"
W14-5407,J02-1002,0,0.0098054,") ∪ GT (b) = [min(xa , xb ), max(ya , yb )] (1) 4.2 Evaluation Metrics Let F L be the final list of name and face alignment retrieved by our system for all the faces detected in all frames, and GL the complete ground truth list. To evaluate the name and face alignment task, we use standard precision (P ), recall (R) and F1 scores for evaluation: P = |F L ∩ GL| |F L| R= |F L ∩ GL| |GL| F1 = 2 · P ·R P +R To evaluate correctness of event segment boundaries, precision and recall are too strict since they penalize boundaries placed very close to the ground truth boundaries. We use the WindowDiff (Pevzner and Hearst, 2002) metric that measures the difference between the ground truth segment GT and the segment SE found by the machine originally designed for text segmentation. For our scenario, this metric is defined as follows: W D(GT, SE) = M −k X 1 (|b(GTi , GTi+k ) − b(SEi , SEi+k ) |> 0) M −k (2) i=1 where M = 7102, is the number of frames extracted, k = 1, is the window size and b(i, j) represents the number of boundaries between frame indices i and j. 5 Results Figure 4: Events learned from the Wikipedia data and their identification in the subtitles and ASR by the system 51 5.1 Evaluation of the extractio"
W16-6009,W09-1206,0,0.0606362,"Missing"
W16-6009,D15-1271,1,0.837037,"virtual-world translation problem based on a probabilistic graphical model that maps text and its semantic annotations (generated by more traditional NLP modules, like semantic role labelers or coreference resolvers) to the knowledge representation of the graphical engine, which is defined in predicate logic. In the process, we discovered several failings of traditional NLP systems when faced with this task: To address this, we introduced a technique based on recurrent neural networks for automatically generating additional training data that was similar to the target domain (Do et al., 2014; Do et al., 2015b). For each selected word (predicate, argument head word) from the source domain, a list of replacement words from the target domain which we believe can occur at the same position as the selected word, are generated by using a recurrent neural network (RNN) language model (Mikolov et al., 2010). In addition, linguistic resources such as part of speech tags, WordNet (Miller, 1995), and VerbNet (Schuler, 2005), are used as filters to select the best replacement words. We primarily targeted improving the results of the four circumstance roles AM-LOC, AMTMP, AM-MNR and AM-DIR, which are importan"
W16-6009,D13-1203,0,0.060961,"Missing"
W17-2003,D14-1086,0,0.0944985,"heir mentions and coreferences in the subtitles2 . It is possible that the frame has some or all or none of the animals in Ni . Corresponding to every name nl ∈ Ni , we have a binary label yl indicating the presence or absence of nl . Our objective is to find the most likely value of yl corresponding to name nl ∈ Ni for every frame fi . of animals may not be found on ImageNet. Recently, there has been considerable interest in sentence/caption generation from images as well as natural language based object detection, e.g. (Karpathy and Fei-Fei, 2014; Fang et al., 2014; Guadarrama et al., 2013; Kazemzadeh et al., 2014). These approaches typically rely on text snippets that accurately describe the content of the images or videos. However, in our context, the subtitles and the visuals are not parallel, but complementary. For example, often a few animals are mentioned in the text, while the connected frame only shows one of them. The connection between the vision and the text is therefore much weaker. Additionally, in our setup, we have too few data to train similar models. As a result, these approaches are not directly applicable to our setting. In this paper, we explore weakly-supervised models that can deal"
W18-5405,W15-2813,0,0.0213332,"h as semantic and syntactic similarity, and extrinsic, such as noun phrase chunking and sentiment classification. For the extrinsic tasks, they found that different representations performed best for different tasks, suggesting that perhaps there isn’t one optimal representation for all tasks. Such studies suggest that better methodologies and more research is needed into methods that accurately assess the value of different continuous representations. This paper addresses this by focusing on the evaluation of the information content of the representation rather than any task-oriented metric. Lazaridou et al. (2015) also worked towards a visualization method for text representations by averaging images of the nearest neighbors vectors after a cross-modal mapping. Contrary to this work, their approach did not inmin W (G) = G min max Ex∼Pr [f (x)] − Ex¯∼Pg [f (¯ x)] (1) G f where G is the generator, f is the critic, W is the Wasserstein distance, and Pr and Pg are the real and generated data distributions respectively. To ensure that the approximation to the earth mover distance is valid, the critic f should be enforced to be 1-Lipschitz continuous. (Arjovsky et al., 2017) achieve this by clipping the crit"
W18-5405,W16-2503,0,0.0271,"Missing"
W18-5405,W16-2506,0,0.0170168,"for example the Wasserstein GAN (WGAN) (Arjovsky et al., 2017). In this formulation the discriminator is replaced by a critic, f , that is trained to approximate the Earth-Mover distance (EM). The EM is an estimate of the minimum amount of effort that is necessary to displace one distribution to another (Arjovsky et al., 2017). The loss function to train a GAN with the Wasserstein Distance is presented in Equation 1. The quality of distributed vectors can be assessed with similarity tasks that give a rough measure of semantic and syntactic information (Mikolov et al., 2013a,c) but studies by Faruqui et al. (2016) and Linzen (2016) indeed suggest that the use of word similarity tasks for the evaluation of word vectors is problematic and may lead to incorrect inferences. Schnabel et al. (2015) have evaluated embeddings with a range of methods, both intrinsic, such as semantic and syntactic similarity, and extrinsic, such as noun phrase chunking and sentiment classification. For the extrinsic tasks, they found that different representations performed best for different tasks, suggesting that perhaps there isn’t one optimal representation for all tasks. Such studies suggest that better methodologies and m"
W18-5405,N13-1090,0,0.391414,"distributed representation space which serves as a proxy for generalized, semantic information storage. Word embeddings can be built with unsupervised training, for example by leveraging positional information of texts in a corpus; with weakly supervised training, for example in an adversarial setting; or with supervision of output labels. While this paper focuses on unsupervised and weakly supervised methods only, the methods that are described here are applicable to supervised representations as well. Well-known methods of creating word embeddings are the word2vec algorithms, introduced by Mikolov et al. (2013a). Word embeddings are usually constructed with neural networks that predict the context of a word in a text document. They are able to scale to large training corpora, thus representing large amounts of information and features in a relatively small amount of dimensions. While word2vec word embeddings solely operate on the word level, extensions have been made that include information at the level of characters (e.g. char-CNN-RNN (Kim et al., 2016)), or at higher • The formulation of a methodology to visualize and evaluate the information and quality of different textual representations. • T"
W18-5405,D15-1036,0,0.0236978,"stance (EM). The EM is an estimate of the minimum amount of effort that is necessary to displace one distribution to another (Arjovsky et al., 2017). The loss function to train a GAN with the Wasserstein Distance is presented in Equation 1. The quality of distributed vectors can be assessed with similarity tasks that give a rough measure of semantic and syntactic information (Mikolov et al., 2013a,c) but studies by Faruqui et al. (2016) and Linzen (2016) indeed suggest that the use of word similarity tasks for the evaluation of word vectors is problematic and may lead to incorrect inferences. Schnabel et al. (2015) have evaluated embeddings with a range of methods, both intrinsic, such as semantic and syntactic similarity, and extrinsic, such as noun phrase chunking and sentiment classification. For the extrinsic tasks, they found that different representations performed best for different tasks, suggesting that perhaps there isn’t one optimal representation for all tasks. Such studies suggest that better methodologies and more research is needed into methods that accurately assess the value of different continuous representations. This paper addresses this by focusing on the evaluation of the informati"
W18-5405,N18-5014,1,0.922296,"on the autoencoder approach is an Adversarially Regularized Autoencoder (ARAE) (Kim et al., 2017). Here, the representation is built explicitly from an encoder that is trained as part of an autoencoder as well as a conventional Generative Adversarial Network (GAN). Such representations contain semantic information about the sentence but also discriminative information that allows the adversarial network to distinguish real samples from fake ones. As a result, a smoother semantic transition is apparent while traversing the representation space when compared to an autoencoder. Spinks and Moens (2018) have applied this technique to create textual representations of X-Ray captions and generate textual output with low perplexity. 2.2 Generative models Recent text-to-image models rely on advances in generative models, which are probabilistic models that estimate a distribution given a certain input. Such generative systems have shown impressive progress in the creation of realistic data, most notably with Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). In the original formulation, GANs are trained by alternately improving a generator network, G, which aims to create realisti"
W18-5405,P10-1040,0,0.0392226,"arbitrary in the sense that two texts that are near each other in the code space don’t necessarily share a similar meaning or syntax. More efficient methods assign particular handengineered or automatically extracted features to a lower-dimensional vector. One feature can be stored in exactly one dimension or it could be shared over many. In this paper we will focus on the latter, also referred to as distributed representations or word embeddings, which is the traditional method to represent sentences in recent neural network related research. They are dense, low-dimensional and real-valued (Turian et al., 2010). Texts that contain similar concepts or meaning for a typical task end up near each other in such a distributed representation space which serves as a proxy for generalized, semantic information storage. Word embeddings can be built with unsupervised training, for example by leveraging positional information of texts in a corpus; with weakly supervised training, for example in an adversarial setting; or with supervision of output labels. While this paper focuses on unsupervised and weakly supervised methods only, the methods that are described here are applicable to supervised representations"
