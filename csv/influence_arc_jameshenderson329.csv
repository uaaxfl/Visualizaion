2010.iwslt-papers.8,W05-1506,0,\N,Missing
2010.iwslt-papers.8,P10-1017,0,\N,Missing
2010.iwslt-papers.8,D09-1007,0,\N,Missing
2010.iwslt-papers.8,D07-1104,0,\N,Missing
2010.iwslt-papers.8,P05-1033,0,\N,Missing
2010.iwslt-papers.8,P10-4002,0,\N,Missing
2010.iwslt-papers.8,J07-2003,0,\N,Missing
2010.iwslt-papers.8,J99-4004,0,\N,Missing
2020.acl-main.561,P16-1231,0,0.0529813,"Missing"
2020.acl-main.561,D14-1082,0,0.0125439,"esigned to follow locality in this derivation structure, thereby giving the neural network a linguistically appropriate inductive bias. More recently, Dyer et al. (2015) provide a more direct relationship between the derivation structure and the model structure with their StackLSTM parsing model. In all these models, the use of recurrent neural networks allows arbitrarily large parse structures to be modelled without making any hard independence assumptions, in contrast to non-neural statistical models. Feed-forward neural networks have also been applied to modelling the derivation structure (Chen and Manning, 2014), but the accuracy is worse than using recurrent models (see Table 1), presumably because such models suffer from the need to make hard independence assumptions. Representing the parse tree as a derivation sequence, rather than a derivation structure, makes it possible to define syntactic parsing as a sequenceto-sequence problem, mapping the sentence to its parse sequence. If a neural network architecture for modelling sequences (called seq2seq models) can perform well at this task, then maybe the structured linguistic representations of natural language are not necessary (contrary to Fodor an"
2020.acl-main.561,P97-1003,0,0.25062,"son, 1994, 2000). But these arguments were largely theoretical, and it was not clear how they could be incorporated in learning-based architectures. 2.3 Statistical Models Although researchers in computational linguistics did not want to abandon their representations, they did recognise the importance of learning from data. The first successes in this direction came from learning rules with statistical methods, such as part-of-speech tagging with hidden Markov models. For syntactic parsing, the development of the Penn Treebank led to many statistical models which learned the rules of grammar (Collins, 1997, 1999; Charniak, 1997; Ratnaparkhi, 1999). These statistical models were very successful at learning from the distributions of linguistic representations which had been annotated in the corpus they were trained on. But they still required linguistically-motivated designs to work well. In particular, feature engineering is necessary to make sure that these statistical machine-learning method can search a space of rules which is sufficiently broad to include good models but sufficiently narrow to allow learning from limited data. 3 Inducing Features of Entities Early work on neural networks for"
2020.acl-main.561,D19-1223,0,0.0408647,"Missing"
2020.acl-main.561,N19-1423,0,0.0202891,"position structure of the underlying sequence. In fact, explicitly inputting relative position relations as embeddings into the attention functions works even better (Shaw et al., 2018) (discussed further below). Whether input as properties or as relations, these inputs are just features, not hard-coded model structure. The attention weight functions can then learn to use these features to induce their own structure. The appropriateness and generality for natural language of the Transformer architecture became even more apparent with the development of pretrained Transformer models like BERT (Devlin et al., 2019). BERT models are large Transformer models trained mostly on a masked language model objective, as well as a next-sentence prediction objective. After training on a very large amount of unlabelled text, the resulting pretrained model can be fine tuned for various tasks, with very impressive improvements in accuracy across a wide variety of tasks. The success of BERT has led to various analyses of what it has learned, including the structural relations learned by the attention functions. Although there is no exact mapping from these structures to the structures posited by linguistics, there are"
2020.acl-main.561,P15-1033,0,0.364537,"r in non-neural statistical models (e.g. (Deerwester et al., 1990; Sch¨utze, 1993; Burgess, 1998; Pad´o and Lapata, 2007; Erk, 2010)). This work showed that similarity in the 6296 PTB Constituents model Costa et al. (2001) PoS Henderson (2003) PoS Henderson (2003) Henderson (2004) Vinyals et al. (2015) seq2seq Vinyals et al. (2015) attn Vinyals et al. (2015) seq2seq semisup LP 57.8 83.3 88.8 89.8 CoNLL09 Dependencies model (transition-based) Titov and Henderson (2007a)* Chen and Manning (2014)* Yazdani and Henderson (2015) Stanford Dependencies model (transition-based) Chen and Manning (2014) Dyer et al. (2015) Andor et al. (2016) Kiperwasser and Goldberg (2016) Mohammadshahi and Henderson (2019) BERT LR 64.9 84.3 89.5 90.4 F1 61.1 83.8 89.1 90.1 <70 88.3 90.5 UAS 91.44 89.17 90.75 LAS 88.65 86.49 88.14 UAS 91.80 93.10 94.61 93.9 95.63 LAS 89.60 90.90 92.79 91.9 93.81 Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy. * results reported in (Yazdani and Henderson, 2015). resulting vector space is correlated with semantic similarity. Learning vector-space representations of word"
2020.acl-main.561,D13-1005,0,0.0316391,"t these are (to the best of our knowledge) not learned jointly with a downstream deep learning model. Common examples include BPE (Sennrich et al., 2016) and unigram language model (Kudo, 2018), which use statistics of character n-grams to decide how to split words into subwords. The resulting subwords then become the entities for a deep learning model, such as Transformer (e.g. BERT), but they do not explicitly optimise the performance of this downstream model. In a more linguisticallyinformed approach to the same problem, statistical models have been proposed for morphology induction (e.g. (Elsner et al., 2013)). Also, Semi-Markov CRF models (Sarawagi and Cohen, 2005) can learn segmentations of an input string, which have been used in the output layers of neural models (e.g. (Kong et al., 2015)). The success of these models in finding useful segmentations of characters into subwords suggests that learning the set of entities can be integrated into a deep learning model. But this task is complicated by the inherently discrete nature of the segmentation into entities. It remains to find effective neural architectures for learning the set of entities jointly with the rest of the neural model, and for g"
2020.acl-main.561,N03-1014,1,0.583691,"endent embeddings of words. We will refer to such contextdependent embeddings as token embeddings. For example, Peters et al. (2018) train a stacked BiLSTM language model, and these token embeddings have proved effective in many tasks. More such models will be discussed below. For syntactic parsing, early connectionist approaches (Jain, 1991; Miikkulainen, 1993; Ho and Chan, 1999; Costa et al., 2001) had limited success. The first neural network models to achieve empirical success used a recurrent neural network to model the derivation structure of a traditional syntactic constituency parser (Henderson, 2003, 2004). The recurrent neural network learns to model the sequence of parser actions, estimating the probability of the next parser action given the history of previous parser actions. This allows the decoding algorithm from the traditional parsing model to be used to efficiently search the space of possible parses. These models have also been applied to syntactic dependency parsing (Titov and Henderson, 2007b; Yazdani and Henderson, 2015) and joint syntactic-semantic dependency parsing (Henderson et al., 2013). Crucially, these neural networks do not model the sequence of parser decisions as"
2020.acl-main.561,Q16-1023,0,0.0280547,"by the attention functions, as with the use of position embeddings to compute relative position relations. For the model to induce its own structure, lower levels must learn to embed its relations in pairs of token embeddings, which higher levels of attention then extract. That Transformer learns to embed relations in pairs of token embeddings is apparent from recent work on dependency parsing (Kondratyuk and Straka, 2019; Mohammadshahi and Henderson, 2019, 2020). Earlier models of dependency parsing successfully use BiLSTMs to embed syntactic dependencies in pairs of token embeddings (e.g. (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016)), which are then extracted to predict the dependency tree. Mohammadshahi and Henderson (2019, 2020) use their proposed Graphto-Graph Transformer to encode dependencies in pairs of token embeddings, for transition-based and graph-based dependency parsing respectively. Graph-to-Graph Transformer also inputs previously predicted dependency relations into its attention functions (like relative position encoding (Shaw et al., 2018)). These parsers achieve state of the art accuracies, indicating that Transformer finds it easy to input and predict syntactic dependency relat"
2020.acl-main.561,D19-1279,0,0.0172339,"d of generalisation ability required to exhibit systematicity, as in (Fodor and Pylyshyn, 1988). Interestingly, the relations are not stored explicitly. Instead they are extracted from pairs of vectors by the attention functions, as with the use of position embeddings to compute relative position relations. For the model to induce its own structure, lower levels must learn to embed its relations in pairs of token embeddings, which higher levels of attention then extract. That Transformer learns to embed relations in pairs of token embeddings is apparent from recent work on dependency parsing (Kondratyuk and Straka, 2019; Mohammadshahi and Henderson, 2019, 2020). Earlier models of dependency parsing successfully use BiLSTMs to embed syntactic dependencies in pairs of token embeddings (e.g. (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016)), which are then extracted to predict the dependency tree. Mohammadshahi and Henderson (2019, 2020) use their proposed Graphto-Graph Transformer to encode dependencies in pairs of token embeddings, for transition-based and graph-based dependency parsing respectively. Graph-to-Graph Transformer also inputs previously predicted dependency relations into its attention f"
2020.acl-main.561,P04-1013,1,0.584976,"Missing"
2020.acl-main.561,P18-1007,0,0.0162307,"the number of token embeddings they use in an unsupervised way.4 Given that current models hard-code different token definitions for different tasks (e.g. character embeddings versus word embeddings versus sentence embeddings), it is natural to ask whether a specification of the set of entities at a given level of representation can be learned. There are models which induce the set of entities in an input text, but these are (to the best of our knowledge) not learned jointly with a downstream deep learning model. Common examples include BPE (Sennrich et al., 2016) and unigram language model (Kudo, 2018), which use statistics of character n-grams to decide how to split words into subwords. The resulting subwords then become the entities for a deep learning model, such as Transformer (e.g. BERT), but they do not explicitly optimise the performance of this downstream model. In a more linguisticallyinformed approach to the same problem, statistical models have been proposed for morphology induction (e.g. (Elsner et al., 2013)). Also, Semi-Markov CRF models (Sarawagi and Cohen, 2005) can learn segmentations of an input string, which have been used in the output layers of neural models (e.g. (Kong"
2020.acl-main.561,J13-4006,1,0.922161,"l network to model the derivation structure of a traditional syntactic constituency parser (Henderson, 2003, 2004). The recurrent neural network learns to model the sequence of parser actions, estimating the probability of the next parser action given the history of previous parser actions. This allows the decoding algorithm from the traditional parsing model to be used to efficiently search the space of possible parses. These models have also been applied to syntactic dependency parsing (Titov and Henderson, 2007b; Yazdani and Henderson, 2015) and joint syntactic-semantic dependency parsing (Henderson et al., 2013). Crucially, these neural networks do not model the sequence of parser decisions as a flat sequence, but instead model the derivation structure it specifies. A derivation structure includes relationships for the inter-dependencies between nodes in the parse tree. The pattern of interconnections between hidden layers of the recurrent neural network (henceforth referred to as the model structure) is designed to follow locality in this derivation structure, thereby giving the neural network a linguistically appropriate inductive bias. More recently, Dyer et al. (2015) provide a more direct relati"
2020.acl-main.561,Q15-1016,0,0.0342855,"61.1 83.8 89.1 90.1 <70 88.3 90.5 UAS 91.44 89.17 90.75 LAS 88.65 86.49 88.14 UAS 91.80 93.10 94.61 93.9 95.63 LAS 89.60 90.90 92.79 91.9 93.81 Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy. * results reported in (Yazdani and Henderson, 2015). resulting vector space is correlated with semantic similarity. Learning vector-space representations of words with neural networks (rather than SVD) have showed similar effects (e.g. (Turian et al., 2010; Mikolov et al., 2013; Levy et al., 2015; Pennington et al., 2014)), resulting in impressive improvements for many NLP tasks. More recent work has used neural network language models to learn context-dependent embeddings of words. We will refer to such contextdependent embeddings as token embeddings. For example, Peters et al. (2018) train a stacked BiLSTM language model, and these token embeddings have proved effective in many tasks. More such models will be discussed below. For syntactic parsing, early connectionist approaches (Jain, 1991; Miikkulainen, 1993; Ho and Chan, 1999; Costa et al., 2001) had limited success. The first ne"
2020.acl-main.561,P82-1020,0,0.784303,"Missing"
2020.acl-main.561,N18-2078,0,0.0211028,"yer et al. (2015) showed improvements by adding these representations to a model of the derivation structure. Socher et al. (2013a) only modelled the linguistic structure, making it difficult to do decoding efficiently. But the resulting induced constituent embeddings have a clear linguistic interpretation, making it easier to use them within other tasks, such as sentiment analysis (Socher et al., 2013b). Similarly, models based on Graph Convolutional Networks have induced embeddings with clear linguistic interpretations within pre-defined model structures (e.g. (Marcheggiani and Titov, 2017; Marcheggiani et al., 2018)). All these results demonstrate the incredible effectiveness of inducing vector-space representations with neural networks, relieving us from the need to do feature engineering. But neural networks do not relieve us of the need to understand the nature of language when designing our models. Instead of feature engineering, these results show that the best accuracy is achieved by engineering the inductive bias of deep learning models through their model structure. By designing a hand-coded model structure which reflects the linguistic structure, locality in the model structure can reflect local"
2020.acl-main.561,D17-1159,0,0.020067,"ctic constituents bottom-up. Dyer et al. (2015) showed improvements by adding these representations to a model of the derivation structure. Socher et al. (2013a) only modelled the linguistic structure, making it difficult to do decoding efficiently. But the resulting induced constituent embeddings have a clear linguistic interpretation, making it easier to use them within other tasks, such as sentiment analysis (Socher et al., 2013b). Similarly, models based on Graph Convolutional Networks have induced embeddings with clear linguistic interpretations within pre-defined model structures (e.g. (Marcheggiani and Titov, 2017; Marcheggiani et al., 2018)). All these results demonstrate the incredible effectiveness of inducing vector-space representations with neural networks, relieving us from the need to do feature engineering. But neural networks do not relieve us of the need to understand the nature of language when designing our models. Instead of feature engineering, these results show that the best accuracy is achieved by engineering the inductive bias of deep learning models through their model structure. By designing a hand-coded model structure which reflects the linguistic structure, locality in the model"
2020.acl-main.561,J07-2002,0,0.253384,"Missing"
2020.acl-main.561,D14-1162,0,0.0813851,"<70 88.3 90.5 UAS 91.44 89.17 90.75 LAS 88.65 86.49 88.14 UAS 91.80 93.10 94.61 93.9 95.63 LAS 89.60 90.90 92.79 91.9 93.81 Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy. * results reported in (Yazdani and Henderson, 2015). resulting vector space is correlated with semantic similarity. Learning vector-space representations of words with neural networks (rather than SVD) have showed similar effects (e.g. (Turian et al., 2010; Mikolov et al., 2013; Levy et al., 2015; Pennington et al., 2014)), resulting in impressive improvements for many NLP tasks. More recent work has used neural network language models to learn context-dependent embeddings of words. We will refer to such contextdependent embeddings as token embeddings. For example, Peters et al. (2018) train a stacked BiLSTM language model, and these token embeddings have proved effective in many tasks. More such models will be discussed below. For syntactic parsing, early connectionist approaches (Jain, 1991; Miikkulainen, 1993; Ho and Chan, 1999; Costa et al., 2001) had limited success. The first neural network models to ach"
2020.acl-main.561,N18-1202,0,0.388659,"belled dependency accuracy. * results reported in (Yazdani and Henderson, 2015). resulting vector space is correlated with semantic similarity. Learning vector-space representations of words with neural networks (rather than SVD) have showed similar effects (e.g. (Turian et al., 2010; Mikolov et al., 2013; Levy et al., 2015; Pennington et al., 2014)), resulting in impressive improvements for many NLP tasks. More recent work has used neural network language models to learn context-dependent embeddings of words. We will refer to such contextdependent embeddings as token embeddings. For example, Peters et al. (2018) train a stacked BiLSTM language model, and these token embeddings have proved effective in many tasks. More such models will be discussed below. For syntactic parsing, early connectionist approaches (Jain, 1991; Miikkulainen, 1993; Ho and Chan, 1999; Costa et al., 2001) had limited success. The first neural network models to achieve empirical success used a recurrent neural network to model the derivation structure of a traditional syntactic constituency parser (Henderson, 2003, 2004). The recurrent neural network learns to model the sequence of parser actions, estimating the probability of t"
2020.acl-main.561,P16-1162,0,0.0243558,"ample. These Transformer models never try to induce the number of token embeddings they use in an unsupervised way.4 Given that current models hard-code different token definitions for different tasks (e.g. character embeddings versus word embeddings versus sentence embeddings), it is natural to ask whether a specification of the set of entities at a given level of representation can be learned. There are models which induce the set of entities in an input text, but these are (to the best of our knowledge) not learned jointly with a downstream deep learning model. Common examples include BPE (Sennrich et al., 2016) and unigram language model (Kudo, 2018), which use statistics of character n-grams to decide how to split words into subwords. The resulting subwords then become the entities for a deep learning model, such as Transformer (e.g. BERT), but they do not explicitly optimise the performance of this downstream model. In a more linguisticallyinformed approach to the same problem, statistical models have been proposed for morphology induction (e.g. (Elsner et al., 2013)). Also, Semi-Markov CRF models (Sarawagi and Cohen, 2005) can learn segmentations of an input string, which have been used in the ou"
2020.acl-main.561,N18-2074,0,0.124259,"the sequential structure is input in the form of position embeddings. In our formulation, position embeddings are just properties of individual entities (typically words or subwords). As such, these inputs facilitate learning about absolute positions. But they are also designed to allow the model to easily calculate relative position between entities. This allows the model’s attention functions to learn to discover the relative position structure of the underlying sequence. In fact, explicitly inputting relative position relations as embeddings into the attention functions works even better (Shaw et al., 2018) (discussed further below). Whether input as properties or as relations, these inputs are just features, not hard-coded model structure. The attention weight functions can then learn to use these features to induce their own structure. The appropriateness and generality for natural language of the Transformer architecture became even more apparent with the development of pretrained Transformer models like BERT (Devlin et al., 2019). BERT models are large Transformer models trained mostly on a masked language model objective, as well as a next-sentence prediction objective. After training on a"
2020.acl-main.561,D13-1170,0,0.0116589,"e (Collobert and Weston, 2008; Collobert et al., 2011) for an earlier related line of work. 6297 In contrast to seq2seq models, there have also been neural network models of parsing which directly represent linguistic structure, rather than just derivation structure, giving them induced vector representations which map one-to-one with the entities in the linguistic representation. Typically, a recursive neural network is used to compute embeddings of syntactic constituents bottom-up. Dyer et al. (2015) showed improvements by adding these representations to a model of the derivation structure. Socher et al. (2013a) only modelled the linguistic structure, making it difficult to do decoding efficiently. But the resulting induced constituent embeddings have a clear linguistic interpretation, making it easier to use them within other tasks, such as sentiment analysis (Socher et al., 2013b). Similarly, models based on Graph Convolutional Networks have induced embeddings with clear linguistic interpretations within pre-defined model structures (e.g. (Marcheggiani and Titov, 2017; Marcheggiani et al., 2018)). All these results demonstrate the incredible effectiveness of inducing vector-space representations"
2020.acl-main.561,K16-1019,0,0.0255135,"Missing"
2020.acl-main.561,P19-1580,0,0.0265114,", as well as a next-sentence prediction objective. After training on a very large amount of unlabelled text, the resulting pretrained model can be fine tuned for various tasks, with very impressive improvements in accuracy across a wide variety of tasks. The success of BERT has led to various analyses of what it has learned, including the structural relations learned by the attention functions. Although there is no exact mapping from these structures to the structures posited by linguistics, there are clear indications that the attention functions are learning to extract linguistic relations (Voita et al., 2019; Tenney et al., 2019; Reif et al., 2019). With variable binding for the properties of entities and attention functions for relations between entities, Transformer can represent the kinds of structured representations argued for above. With parameters shared across entities and sensitive to these properties and relations, learned rules are parameterised in terms of these structures. Thus Transformer is a deep learning architecture with the kind of generalisation ability required to exhibit systematicity, as in (Fodor and Pylyshyn, 1988). Interestingly, the relations are not stored explicitly."
2020.acl-main.561,K15-1015,1,0.854049,"irst neural network models to achieve empirical success used a recurrent neural network to model the derivation structure of a traditional syntactic constituency parser (Henderson, 2003, 2004). The recurrent neural network learns to model the sequence of parser actions, estimating the probability of the next parser action given the history of previous parser actions. This allows the decoding algorithm from the traditional parsing model to be used to efficiently search the space of possible parses. These models have also been applied to syntactic dependency parsing (Titov and Henderson, 2007b; Yazdani and Henderson, 2015) and joint syntactic-semantic dependency parsing (Henderson et al., 2013). Crucially, these neural networks do not model the sequence of parser decisions as a flat sequence, but instead model the derivation structure it specifies. A derivation structure includes relationships for the inter-dependencies between nodes in the parse tree. The pattern of interconnections between hidden layers of the recurrent neural network (henceforth referred to as the model structure) is designed to follow locality in this derivation structure, thereby giving the neural network a linguistically appropriate induc"
2020.acl-main.561,P19-1452,0,0.0980884,"sentence prediction objective. After training on a very large amount of unlabelled text, the resulting pretrained model can be fine tuned for various tasks, with very impressive improvements in accuracy across a wide variety of tasks. The success of BERT has led to various analyses of what it has learned, including the structural relations learned by the attention functions. Although there is no exact mapping from these structures to the structures posited by linguistics, there are clear indications that the attention functions are learning to extract linguistic relations (Voita et al., 2019; Tenney et al., 2019; Reif et al., 2019). With variable binding for the properties of entities and attention functions for relations between entities, Transformer can represent the kinds of structured representations argued for above. With parameters shared across entities and sensitive to these properties and relations, learned rules are parameterised in terms of these structures. Thus Transformer is a deep learning architecture with the kind of generalisation ability required to exhibit systematicity, as in (Fodor and Pylyshyn, 1988). Interestingly, the relations are not stored explicitly. Instead they are extr"
2020.acl-main.561,W07-2218,1,0.835977,") had limited success. The first neural network models to achieve empirical success used a recurrent neural network to model the derivation structure of a traditional syntactic constituency parser (Henderson, 2003, 2004). The recurrent neural network learns to model the sequence of parser actions, estimating the probability of the next parser action given the history of previous parser actions. This allows the decoding algorithm from the traditional parsing model to be used to efficiently search the space of possible parses. These models have also been applied to syntactic dependency parsing (Titov and Henderson, 2007b; Yazdani and Henderson, 2015) and joint syntactic-semantic dependency parsing (Henderson et al., 2013). Crucially, these neural networks do not model the sequence of parser decisions as a flat sequence, but instead model the derivation structure it specifies. A derivation structure includes relationships for the inter-dependencies between nodes in the parse tree. The pattern of interconnections between hidden layers of the recurrent neural network (henceforth referred to as the model structure) is designed to follow locality in this derivation structure, thereby giving the neural network a l"
2020.acl-main.561,P10-1040,0,0.0369546,"rson (2019) BERT LR 64.9 84.3 89.5 90.4 F1 61.1 83.8 89.1 90.1 <70 88.3 90.5 UAS 91.44 89.17 90.75 LAS 88.65 86.49 88.14 UAS 91.80 93.10 94.61 93.9 95.63 LAS 89.60 90.90 92.79 91.9 93.81 Table 1: Some neural network parsing results on Penn Treebank WSJ. LP/LR/F1: labelled constituent precision/recall/F-measure. UAS/LAS: unlabelled/labelled dependency accuracy. * results reported in (Yazdani and Henderson, 2015). resulting vector space is correlated with semantic similarity. Learning vector-space representations of words with neural networks (rather than SVD) have showed similar effects (e.g. (Turian et al., 2010; Mikolov et al., 2013; Levy et al., 2015; Pennington et al., 2014)), resulting in impressive improvements for many NLP tasks. More recent work has used neural network language models to learn context-dependent embeddings of words. We will refer to such contextdependent embeddings as token embeddings. For example, Peters et al. (2018) train a stacked BiLSTM language model, and these token embeddings have proved effective in many tasks. More such models will be discussed below. For syntactic parsing, early connectionist approaches (Jain, 1991; Miikkulainen, 1993; Ho and Chan, 1999; Costa et al."
2020.acl-main.561,J03-4003,0,\N,Missing
2020.acl-main.561,W10-2803,0,\N,Missing
2020.acl-main.769,Q15-1034,0,0.0880252,"Missing"
2020.acl-main.769,D19-1341,0,0.122801,".com/rabeehk/robust-nli. 1 Introduction Recent neural models (Devlin et al., 2019; Radford et al., 2018; Chen et al., 2017) have achieved high and even near human-performance on several largescale natural language understanding benchmarks. However, it has been demonstrated that neural models tend to rely on existing idiosyncratic biases in the datasets, and leverage superficial correlations between the label and existing shortcuts in the training dataset to perform surprisingly well,1 without learning the underlying task (Kaushik and Lipton, 2018; Gururangan et al., 2018; Poliak et al., 2018; Schuster et al., 2019; 1 We use biases, heuristics or shortcuts interchangeably. McCoy et al., 2019b). For instance, natural language inference (NLI) is supposed to test the ability of a model to determine whether a hypothesis sentence (There is no teacher in the room) can be inferred from a premise sentence (Kids work at computers with a teacher’s help) (Dagan et al., 2006).2 However, recent work has demonstrated that large-scale NLI benchmarks contain annotation artifacts; certain words in the hypothesis that are highly indicative of inference class and allow models that do not consider the premise to perform un"
2020.acl-main.769,P18-2119,0,0.0286132,"ated with the contradiction label. As a result of the existence of such biases, models exploiting statistical shortcuts during training often perform poorly on out-of-domain datasets, especially if the datasets are carefully designed to limit the spurious cues. To allow proper evaluation, recent studies have tried to create new evaluation datasets that do not contain such biases (Gururangan et al., 2018; Schuster et al., 2019; McCoy et al., 2019b). Unfortunately, it is hard to avoid spurious statistical cues in the construction of large-scale benchmarks, and collecting new datasets is costly (Sharma et al., 2018). It is, therefore, crucial to develop techniques to reduce the reliance on biases during the training of the neural models. We propose two end-to-end debiasing techniques that can be used when the existing bias patterns are identified. These methods work by adjusting the crossentropy loss to reduce the biases learned from the training dataset, down-weighting the biased examples so that the model focuses on learning the hard examples. Figure 1 illustrates an example of applying our strategy to prevent an NLI model from predicting the labels using existing biases in the hypotheses, where the bi"
2020.acl-main.769,P16-2022,0,0.0608664,"Missing"
2020.acl-main.769,P16-1204,0,0.0804035,"Missing"
2020.acl-main.769,I17-1100,0,0.0955643,"Missing"
2020.acl-main.769,N18-1101,0,0.0784115,"urces of bias by training multiple bias-only models. Our approaches are simple and highly effective. They require training only a simple model on top of the base model. They are model agnostic and general enough to be applicable for addressing common biases seen in many datasets in different domains. We evaluate our models on challenging benchmarks in textual entailment and fact verification, including HANS (Heuristic Analysis for NLI Systems) (McCoy et al., 2019b), hard NLI sets (Gururangan et al., 2018) of Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and MultiNLI (MNLI) (Williams et al., 2018), and FEVER Symmetric test set (Schuster et al., 2019). The selected datasets are highly challenging and have been carefully designed to be unbiased to allow proper evaluation of the out-of-domain performance of the models. We additionally construct hard MNLI datasets from MNLI development sets to facilitate the out-of-domain evaluation on this dataset.3 We show that including our strategies on training baseline models, including BERT (Devlin et al., 2019), provides a substantial gain on out-of-domain performance in all the experiments. In summary, we make the following contributions: 1) Propo"
2020.crac-1.10,N18-1204,0,0.021694,"approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of ent"
2020.crac-1.10,P19-1064,0,0.019721,"ning signal is coming indirectly from the coreference annotation. Zhang et al. (2018) used a similar approach but introducing a direct learning signal for the mention detection, which is done by adding a loss for mention detection with a scaling factor as hyperparameter. This allows a faster convergence at training time. Lee et al. (2018) proposed a high-order coreference resolution where the mention representation are inferred over several iterations of the model. However, the mention detection part is same as in (Lee et al., 2017). The following studies proposed improvements over this work (Fei et al., 2019; Joshi et al., 2019; Joshi et al., 2020) but maintaining the same method for mention detection. Name entity recognition has been largely studied in the community. However, many of these models ignored the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neural network that includes extra connections to handle nested mention detection. Ju et al. (2018) uses stack layers to model the nested mentions, and (Wang et al., 2018) use an stack recurrent network. Lin et al. (2019) proposed a sequence-to-nuggets architecture for nested men"
2020.crac-1.10,N04-1001,0,0.160417,"However, it is common to have partially annotated data for this problem. Here, we investigate two approaches to deal with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and t"
2020.crac-1.10,D19-1588,0,0.0110666,"ing indirectly from the coreference annotation. Zhang et al. (2018) used a similar approach but introducing a direct learning signal for the mention detection, which is done by adding a loss for mention detection with a scaling factor as hyperparameter. This allows a faster convergence at training time. Lee et al. (2018) proposed a high-order coreference resolution where the mention representation are inferred over several iterations of the model. However, the mention detection part is same as in (Lee et al., 2017). The following studies proposed improvements over this work (Fei et al., 2019; Joshi et al., 2019; Joshi et al., 2020) but maintaining the same method for mention detection. Name entity recognition has been largely studied in the community. However, many of these models ignored the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neural network that includes extra connections to handle nested mention detection. Ju et al. (2018) uses stack layers to model the nested mentions, and (Wang et al., 2018) use an stack recurrent network. Lin et al. (2019) proposed a sequence-to-nuggets architecture for nested mention detection. Li e"
2020.crac-1.10,2020.tacl-1.5,0,0.0157407,"the coreference annotation. Zhang et al. (2018) used a similar approach but introducing a direct learning signal for the mention detection, which is done by adding a loss for mention detection with a scaling factor as hyperparameter. This allows a faster convergence at training time. Lee et al. (2018) proposed a high-order coreference resolution where the mention representation are inferred over several iterations of the model. However, the mention detection part is same as in (Lee et al., 2017). The following studies proposed improvements over this work (Fei et al., 2019; Joshi et al., 2019; Joshi et al., 2020) but maintaining the same method for mention detection. Name entity recognition has been largely studied in the community. However, many of these models ignored the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neural network that includes extra connections to handle nested mention detection. Ju et al. (2018) uses stack layers to model the nested mentions, and (Wang et al., 2018) use an stack recurrent network. Lin et al. (2019) proposed a sequence-to-nuggets architecture for nested mention detection. Li et al. (2019) uses poi"
2020.crac-1.10,N18-1131,0,0.386572,"for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and machine translation. Data from coreference resolution is suitable for our task, but the annotation is partial in that it contains only mention"
2020.crac-1.10,N18-1079,0,0.246101,"ecall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and machine translation. Data from coreference resolution is suitable for our task, but the annotation is partial in that it cont"
2020.crac-1.10,N16-1030,0,0.0507823,"the two mention detection approaches we use in our experiments. Section 4 presents the proposed methods to deal with partially annotated mentions. We use coreference resolution as a proxy task for testing our methods which is described in Section 5. Section 6 contains the experimental setting and the analysis of results. Section 7 contains related work to this study. Finally, the final conclusion is drawn Section 8. 2 Sequence tagging model Several studies have tackled mention detection and named entity recognition as a tagging problem. Some of them use one-to-one sequence tagging techniques (Lample et al., 2016; Xu et al., 2017), while others use more elaborate techniques to include nested mentions (Katiyar and Cardie, 2018; Wang et al., 2018). Here, we propose a simpler yet effective tagging approach that can manage nested mentions. We use a sequence-to-sequence model, which allows us to tag each word with multiple labels. The words are first encoded and contextualized using a recurrent neural network, and then a sequential decoder predicts the output tag sequence. During decoding, the model keeps a pointer into the encoder, indicating the word’s position, which is being tagged at each time step. T"
2020.crac-1.10,D17-1018,0,0.191546,"tated in sample 1 but not in 2. Thus, we approach mention detection as a partially supervised problem and investigate two simple techniques to compensate for the fact that some negative examples are true mentions: weighted loss functions and soft-target classification. By doing this, the model is encouraged to predict more false-positive samples, so it can detect potential mentions which were not annotated. We implement two neural mention detection methods: a sequence tagging approach, and an exhaustive search approach. The first method is novel, whereas the other is similar to previous work (Lee et al., 2017). We evaluate both techniques for coreference resolution by implementing a multitask learning system. We show that the proposed techniques help the model increase recall significantly with a minimal decrease in precision. In consequence, the F1 score of the mention detection and coreference resolution improves for both methods, and the exhaustive search approach yields a significant improvement over the baseline coreference resolver. Our contributions are: i We investigate two techniques to deal with partially annotated data. This work is licensed under a Creative Commons Attribution 4.0 Inter"
2020.crac-1.10,N18-2108,0,0.0178617,"2 59.9 64.1 61.2 67.0 67.1 67.6 Table 2: Coreference resolution evaluation (CoNLL 2012) Figure 3: Recall of the mention scoring function with respect to the detection threshold τ . Values for the sequence tagging are referential document as the candidate mentions, and the learning signal is coming indirectly from the coreference annotation. Zhang et al. (2018) used a similar approach but introducing a direct learning signal for the mention detection, which is done by adding a loss for mention detection with a scaling factor as hyperparameter. This allows a faster convergence at training time. Lee et al. (2018) proposed a high-order coreference resolution where the mention representation are inferred over several iterations of the model. However, the mention detection part is same as in (Lee et al., 2017). The following studies proposed improvements over this work (Fei et al., 2019; Joshi et al., 2019; Joshi et al., 2020) but maintaining the same method for mention detection. Name entity recognition has been largely studied in the community. However, many of these models ignored the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neur"
2020.crac-1.10,P19-1511,0,0.022399,"Missing"
2020.crac-1.10,D18-1325,1,0.833997,"evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and"
2020.crac-1.10,P09-1113,0,0.11135,"ted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic pa"
2020.crac-1.10,W12-4501,0,0.269378,"in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and machine translation. Data from coreference resolution is suitable for our task, but the annotation is partial in that it contains only mentions that belong to a coreference chain, not singletons. Nevertheless, the missing mentions have approximately the same distribution as the annotated ones, so we can still learn this distribution from the data. Figure 1 shows an example from Ontonotes V.5 dataset (Pradhan et al., 2012) where “the taxi driver” is annotated in sample 1 but not in 2. Thus, we approach mention detection as a partially supervised problem and investigate two simple techniques to compensate for the fact that some negative examples are true mentions: weighted loss functions and soft-target classification. By doing this, the model is encouraged to predict more false-positive samples, so it can detect potential mentions which were not annotated. We implement two neural mention detection methods: a sequence tagging approach, and an exhaustive search approach. The first method is novel, whereas the oth"
2020.crac-1.10,2020.tacl-1.39,0,0.0217053,"tion detection. Name entity recognition has been largely studied in the community. However, many of these models ignored the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neural network that includes extra connections to handle nested mention detection. Ju et al. (2018) uses stack layers to model the nested mentions, and (Wang et al., 2018) use an stack recurrent network. Lin et al. (2019) proposed a sequence-to-nuggets architecture for nested mention detection. Li et al. (2019) uses pointer networks and adversarial learning. Shibuya and Hovy (2020) uses CRF with a iterative decoder that detect nested mentions from the outer to the inner tags. Yu et al. (2020) use a bi-affine model with a similar method as in (Lee et al., 2017). 8 Conclusion We investigate two simple techniques to deal with partially annotated data for mention detection and propose two methods to approach it: a Weighted loss function and a soft-target classification. We evaluate them on coreference resolution and mention detection with a multitask learning approach. We show that the techniques effectively increase the recall of mentions and coreference links with a small"
2020.crac-1.10,J01-4004,0,0.525564,"al with partial annotation of mentions: weighted loss and soft-target classification. We also propose two neural mention detection approaches: a sequence tagging, and an exhaustive search. We evaluate our methods with coreference resolution as a downstream task, using multitask learning. The results show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred t"
2020.crac-1.10,D18-1124,0,0.167961,"1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and machine translation. Data from coreference resolution is suitable for our task, but the annotation is partial in that it contains only mentions that belong to a c"
2020.crac-1.10,P17-1114,0,0.134344,"s show that the recall and F1 score improve for all methods. 1 Introduction Mention detection is the task of identifying text spans referring to an entity: named, nominal or pronominal (Florian et al., 2004). It is a fundamental component for several downstream tasks, such as coreference resolution (Soon et al., 2001), and relation extraction (Mintz et al., 2009); and it can help to maintain coherence in large text generation (Clark et al., 2018), and contextualized machine translation (Miculicich et al., 2018). Previous studies tackled mention detection jointly with named entity recognition (Xu et al., 2017; Katiyar and Cardie, 2018; Ju et al., 2018; Wang et al., 2018). There, only certain types of entities are considered (e.g., person, location), and the goal is to recognize mention spans and their types. In this study, we are interested in discovering entity mentions, which can potentially be referred to in the text, without the use of syntactic parsing information. Our long term objective is to have a model that keeps track of entities in a document for word disambiguating language modeling and machine translation. Data from coreference resolution is suitable for our task, but the annotation"
2020.crac-1.10,2020.acl-main.577,0,0.0102196,"the nested entity names. Katiyar and Cardie (2018) presents a nested named entity recognition model using a recurrent neural network that includes extra connections to handle nested mention detection. Ju et al. (2018) uses stack layers to model the nested mentions, and (Wang et al., 2018) use an stack recurrent network. Lin et al. (2019) proposed a sequence-to-nuggets architecture for nested mention detection. Li et al. (2019) uses pointer networks and adversarial learning. Shibuya and Hovy (2020) uses CRF with a iterative decoder that detect nested mentions from the outer to the inner tags. Yu et al. (2020) use a bi-affine model with a similar method as in (Lee et al., 2017). 8 Conclusion We investigate two simple techniques to deal with partially annotated data for mention detection and propose two methods to approach it: a Weighted loss function and a soft-target classification. We evaluate them on coreference resolution and mention detection with a multitask learning approach. We show that the techniques effectively increase the recall of mentions and coreference links with a small decrease in precision, thus, improving the F1 score. In the future, we plan to use these methods to maintain coh"
2020.crac-1.10,P18-2017,0,0.0175715,"e et al. (2017) Sequence tagging + wt. loss w=0.01 + soft-target ρ=0.1 Span scoring + wt. loss w=0.3 + soft-target ρ=0.1 Rec. – 73.1 77.3 74.3 75.3 76.3 78.4 Mention Prec. – 84.9 83.2 84.0 88.3 88.1 87.9 F1 – 78.6 80.1 78.8 81.3 81.8 82.9 Coref. Avg. F1 67.2 59.9 64.1 61.2 67.0 67.1 67.6 Table 2: Coreference resolution evaluation (CoNLL 2012) Figure 3: Recall of the mention scoring function with respect to the detection threshold τ . Values for the sequence tagging are referential document as the candidate mentions, and the learning signal is coming indirectly from the coreference annotation. Zhang et al. (2018) used a similar approach but introducing a direct learning signal for the mention detection, which is done by adding a loss for mention detection with a scaling factor as hyperparameter. This allows a faster convergence at training time. Lee et al. (2018) proposed a high-order coreference resolution where the mention representation are inferred over several iterations of the model. However, the mention detection part is same as in (Lee et al., 2017). The following studies proposed improvements over this work (Fei et al., 2019; Joshi et al., 2019; Joshi et al., 2020) but maintaining the same me"
2020.emnlp-main.491,2020.acl-main.23,0,0.341107,"Missing"
2020.emnlp-main.491,K18-1040,0,0.0230713,"uccess, these models are costly to train and require a large amount of parallel data. Yet parallel data is scarce for conditional text generation problems, necessitating unsupervised 1 We use this term to refer to text generation conditioned on textual input. solutions. Text autoencoders (Bowman et al., 2016) have proven useful for a particular subclass of unsupervised problems that can be broadly defined as style transfer, i.e., changing the style of a text in such a way that the content of the input is preserved. Examples include sentiment transfer (Shen et al., 2017), sentence compression (Fevry and Phang, 2018), and neural machine translation (Artetxe et al., 2018). Most existing methods specialize autoencoders to the task by conditioning the decoder on the style attribute of interest (Lample et al., 2019; Logeswaran et al., 2018), assuming the presence of labels during training of the autoencoder. The main drawback of this approach is that it cannot leverage pretraining on unlabeled data, which is probably the most important factor for widespread progress in supervised NLP models in recent years in text analysis (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019) and generation tasks ("
2020.emnlp-main.491,K16-1028,0,0.0613516,"Missing"
2020.emnlp-main.491,D19-1659,0,0.0262443,"2018; Logeswaran et al., 2018; Yang et al., 2018; Li et al., 2019), which hinders their employment in a plug and play fashion. Most methods either rely on adversarial objectives (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018), retrieval (Li et al., 2018), or backtranslation (Lample et al., 2019; Logeswaran et al., 2018) to make the latent codes independent of the style attribute. Notable exceptions are Transformer-based (Dai et al., 2019; Sudhakar et al., 2019), use reinforcement learning for backtranslating through the discrete space (Liu and Liu, 2019), build pseudo-parallel corpora (Kruengkrai, 2019; Jin et al., 2019), or modify the latent-variable at inference time by following the gradient of a style classifier (Wang et al., 2019; Liu et al., 2020). Similar to our motivation, Li et al. (2019) aim at improving in-domain performance by incorporating out-of-domain data into training. However, because their model again conditions on the target data, they have to train the autoencoder jointly with the target corpus, defeating the purpose of large-scale pretraining. In contrast to previous methods, Emb2Emb can be combined with any pretrained autoencoder even if it was not trained with target"
2020.emnlp-main.491,D19-1325,0,0.0199346,"t be conditioned on input text, and are thus not applicable to style transfer. Figure 6: Sentiment transfer results for different model scenarios. Up and right is better. 4 Related Work Text Style Transfer The most common approach to text style transfer is to learn a disentangled shared latent space that is agnostic to the style of the input. Style transfer is then achieved by training the decoder conditioned on the desired style attribute, (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018; Lample et al., 2019; Li et al., 2018; Logeswaran et al., 2018; Yang et al., 2018; Li et al., 2019), which hinders their employment in a plug and play fashion. Most methods either rely on adversarial objectives (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018), retrieval (Li et al., 2018), or backtranslation (Lample et al., 2019; Logeswaran et al., 2018) to make the latent codes independent of the style attribute. Notable exceptions are Transformer-based (Dai et al., 2019; Sudhakar et al., 2019), use reinforcement learning for backtranslating through the discrete space (Liu and Liu, 2019), build pseudo-parallel corpora (Kruengkrai, 2019; Jin et al., 2019), or modify the latent-variable"
2020.emnlp-main.491,N18-1202,0,0.0832923,"Missing"
2020.emnlp-main.491,W19-4302,1,0.874578,"Missing"
2020.emnlp-main.491,P19-1153,0,0.0239885,"etrain-and-plugin variational autoencoders (Duan Textual Autoencoders Autoencoders are a very active field of research, leading to constant progress through denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and, more recently, regularized (Ghosh et al., 2020) autoencoders, to name a few. Ever since Bowman et al. (2016) adopted variational autoencoders for sentences by employing a recurrent sequence-to-sequence model, improving both the architecture (Semeniuta et al., 2017; Prato et al., 2019; Liu and Liu, 2019; Gagnon-Marchand et al., 2019) and the training objective (Zhao et al., 2018; Shen et al., 2020) have received considerable attention. The goal is typically to improve both the reconstruction and generation performance (C´ıfka et al., 2018). Our framework is completely agnostic to the type of autoencoder that is used, as long as it is trained to reconstruct the input. Hence, our framework directly benefits from any kind of modelling advancement in autoencoder research. 5 Conclusion In this paper, we present Emb2Emb, a framework that reduces conditional text generation tasks"
2020.emnlp-main.491,D15-1044,0,0.047061,"rms better than or comparable to strong baselines while being up to four times faster. 1 Figure 1: The manifold of a text autoencoder is the lowdimensional region of the high-dimensional embedding space where texts are actually embedded. The example shows the mapping of a source sequence x with embedding zx to zy , which is the embedding of target sequence y such that it reflects the target manifold. Introduction Conditional text generation1 encompasses a large number of natural language processing tasks such as text simplification (Nisioi et al., 2017; Zhang and Lapata, 2017), summarization (Rush et al., 2015; Nallapati et al., 2016), machine translation (Bahdanau et al., 2015; Kumar and Tsvetkov, 2019) and style transfer (Shen et al., 2017; Fu et al., 2018). When training data is available, the state of the art includes encoder-decoder models with an attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) which are both extensions of the original sequence-to-sequence framework with a fixed bottleneck introduced by Sutskever et al. (2014). Despite their success, these models are costly to train and require a large amount of parallel data. Yet parallel data is scarce for conditional text"
2020.emnlp-main.491,D17-1066,0,0.011888,"It is also similar to pretrain-and-plugin variational autoencoders (Duan Textual Autoencoders Autoencoders are a very active field of research, leading to constant progress through denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and, more recently, regularized (Ghosh et al., 2020) autoencoders, to name a few. Ever since Bowman et al. (2016) adopted variational autoencoders for sentences by employing a recurrent sequence-to-sequence model, improving both the architecture (Semeniuta et al., 2017; Prato et al., 2019; Liu and Liu, 2019; Gagnon-Marchand et al., 2019) and the training objective (Zhao et al., 2018; Shen et al., 2020) have received considerable attention. The goal is typically to improve both the reconstruction and generation performance (C´ıfka et al., 2018). Our framework is completely agnostic to the type of autoencoder that is used, as long as it is trained to reconstruct the input. Hence, our framework directly benefits from any kind of modelling advancement in autoencoder research. 5 Conclusion In this paper, we present Emb2Emb, a framework that reduces conditional t"
2020.findings-emnlp.294,P16-1231,0,0.0534975,"Missing"
2020.findings-emnlp.294,D16-1211,0,0.145507,"Missing"
2020.findings-emnlp.294,P16-2006,0,0.0542393,"s follows: ht , ct = LSTM((ht−1 ,ct−1 ),at +lt ) (8) where at and lt are the previous transition and its associated dependency label, and ht−1 and ct−1 are the previous output vector and cell state of the history model. The output of the history model is input directly to the parser action classifiers in (5). 4.2 Sentence Transformer We propose another attention-based architecture, called Sentence Transformer (SentTr), to compute a representation for the parser state. This model first uses a Transformer to compute context-dependent embeddings for the tokens in the input sentence. Similarly to Cross and Huang (2016), a separate stack and buffer data structure is used to keep track of the parser state, as shown in Figure 1b, and the context-dependent embeddings of the tokens that are involved in the next parser action are used to predict the next transition. More specifically, the input sentence tokens are computed with the BERT tokeniser (Devlin et al., 2018) and the next transition is predicted from the embeddings of the first sub-words of the top two elements of the stack and the front element of the buffer.5 In the baseline version of this model, the Transformer which computes the token embeddings 4 P"
2020.findings-emnlp.294,N19-1423,0,0.0601741,"Missing"
2020.findings-emnlp.294,P15-1033,0,0.093113,"unction. This mechanism can be generalised to input arbitrary graphs of relations. We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as continuous embeddings, instead of discrete model structure (e.g. (Henderson, 2003; Henderson et al., 2013; Dyer et al., 2015)) or predefined discrete attention heads (e.g. (Ji et al., 2019; Strubell et al., 2018)). An explicit representation of binary relations is supported by inputting these relation embeddings to the attention functions, which are applied to every pair of tokens. In this way, each attention head can easily learn to attend only to tokens in a given relation, but it can also learn other structures in combination with other inputs. This gives a bias towards attention weights which respect locality in the input graph but does not hard-code any specific attention weights. We focus our investigation on"
2020.findings-emnlp.294,N19-1076,0,0.0275722,"Missing"
2020.findings-emnlp.294,N03-1014,1,0.609606,"position is input to the self-attention function. This mechanism can be generalised to input arbitrary graphs of relations. We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as continuous embeddings, instead of discrete model structure (e.g. (Henderson, 2003; Henderson et al., 2013; Dyer et al., 2015)) or predefined discrete attention heads (e.g. (Ji et al., 2019; Strubell et al., 2018)). An explicit representation of binary relations is supported by inputting these relation embeddings to the attention functions, which are applied to every pair of tokens. In this way, each attention head can easily learn to attend only to tokens in a given relation, but it can also learn other structures in combination with other inputs. This gives a bias towards attention weights which respect locality in the input graph but does not hard-code any specific atten"
2020.findings-emnlp.294,J13-4006,1,0.8124,"to the self-attention function. This mechanism can be generalised to input arbitrary graphs of relations. We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as continuous embeddings, instead of discrete model structure (e.g. (Henderson, 2003; Henderson et al., 2013; Dyer et al., 2015)) or predefined discrete attention heads (e.g. (Ji et al., 2019; Strubell et al., 2018)). An explicit representation of binary relations is supported by inputting these relation embeddings to the attention functions, which are applied to every pair of tokens. In this way, each attention head can easily learn to attend only to tokens in a given relation, but it can also learn other structures in combination with other inputs. This gives a bias towards attention weights which respect locality in the input graph but does not hard-code any specific attention weights. We focus o"
2020.findings-emnlp.294,P13-1088,0,0.0184085,"re-trained word vectors of the BERT model. During training and evaluation, we use the pre-trained embedding of first sub-word as the token representation of each word and discard embeddings of non-first sub-words due to training Figure 2: An Example of Composition model. efficiency.2 parameters. The PoS embeddings are trained Composition Model: As an alternative to our proposed graph input method, previous work has shown that complex phrases can be input to a neural network by using recursive neural networks to recursively compose the embeddings of subphrases (Socher et al., 2011, 2014, 2013; Hermann and Blunsom, 2013; Tai et al., 2015). We extend the proposed composition model of Dyer et al. (2015) by applying a one-layer feed-forward neural network as a composition model and adding skip connections to each recursive step.3 Since a syntactic head may contain an arbitrary number of dependents, we compute new token embeddings of head-dependent pairs one at a time as they are specified by the parser, as shown in Figure 2. At each parser step t, we compute each new token embedding Cit of token i by inputting to the composition model, its previous token embedding Cjt−1 and the embedding of the most recent depe"
2020.findings-emnlp.294,P19-1237,0,0.0143688,"hs of relations. We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as continuous embeddings, instead of discrete model structure (e.g. (Henderson, 2003; Henderson et al., 2013; Dyer et al., 2015)) or predefined discrete attention heads (e.g. (Ji et al., 2019; Strubell et al., 2018)). An explicit representation of binary relations is supported by inputting these relation embeddings to the attention functions, which are applied to every pair of tokens. In this way, each attention head can easily learn to attend only to tokens in a given relation, but it can also learn other structures in combination with other inputs. This gives a bias towards attention weights which respect locality in the input graph but does not hard-code any specific attention weights. We focus our investigation on this novel graph input method and therefore limit our investiga"
2020.findings-emnlp.294,Q16-1023,0,0.0436511,"Missing"
2020.findings-emnlp.294,P19-1340,0,0.106419,"its previous token embedding Cjt−1 and the embedding of the most recent dependent with its associated dependency label, where j is the position of token i in the previous parser state. At t = 0, Ci0 is set to the initial token embedding Twi . More mathematical and implementation details are given in Appendix B. Position and Segment Embeddings: To distinguish the different positions and roles of words in 2 Using embeddings of first sub-word for each word results in better performance than using the last one or averaging all of them as also shown in previous works (Kondratyuk and Straka, 2019; Kitaev et al., 2019). 3 These skip connections help address the vanishing gradient problem, and preliminary experiments indicated that they were necessary to integrate pre-trained BERT (Devlin et al., 2018) parameters with the model (discussed in Section 4.4 and Appendix A.A). 3281 the parser state, we add their embeddings to the token embeddings. Position embeddings βi encode the token’s position in the whole sequence.4 Segment embeddings γi encode that the input sequence contains distinct segments (e.g. stack and buffer). Total Input Embeddings: Finally, at each step t, we sum the outputs of the composition mod"
2020.findings-emnlp.294,D19-1279,0,0.146104,"ing to the composition model, its previous token embedding Cjt−1 and the embedding of the most recent dependent with its associated dependency label, where j is the position of token i in the previous parser state. At t = 0, Ci0 is set to the initial token embedding Twi . More mathematical and implementation details are given in Appendix B. Position and Segment Embeddings: To distinguish the different positions and roles of words in 2 Using embeddings of first sub-word for each word results in better performance than using the last one or averaging all of them as also shown in previous works (Kondratyuk and Straka, 2019; Kitaev et al., 2019). 3 These skip connections help address the vanishing gradient problem, and preliminary experiments indicated that they were necessary to integrate pre-trained BERT (Devlin et al., 2018) parameters with the model (discussed in Section 4.4 and Appendix A.A). 3281 the parser state, we add their embeddings to the token embeddings. Position embeddings βi encode the token’s position in the whole sequence.4 Segment embeddings γi encode that the input sequence contains distinct segments (e.g. stack and buffer). Total Input Embeddings: Finally, at each step t, we sum the outputs"
2020.findings-emnlp.294,D19-1277,0,0.0257815,"Missing"
2020.findings-emnlp.294,C18-1271,0,0.0359955,"Missing"
2020.findings-emnlp.294,P18-1130,0,0.106378,". Details of Implementation All hyper-parameter details are given in Appendix F. Unless specified otherwise, all models have 6 self-attention layers. We use the AdamW optimiser provided by Wolf et al. (2019) to fine-tune model parameters. All our models use greedy decoding, meaning that at each step only the highest scoring parser action is considered for continuation. This was done for simplicity, although beam search could also be used. The pseudo-code for computing the elements of the graph input matrix (pij ) for each baseline is provided in Appendix G. 8 We do not consider the models of (Ma et al., 2018; Fern´andez-Gonz´alez and G´omez-Rodr´ıguez, 2019) to be comparable to traditional transition-based models like ours because they make decoding decisions between O(n) alternatives. In this sense, they are in between the O(1) alternatives for transition-based models and the O(n2 ) alternatives for graph-based models. Future work will investigate applying Graph2Graph Transformer to these types of parsers as well. 9 The number of parameters and average running times for each model are provided in Appendix E. 3283 Dev Set Test Set UAS LAS UAS LAS Transition-based: Dyer et al. (2015) Weiss et al."
2020.findings-emnlp.294,J93-2004,0,0.0720998,"Missing"
2020.findings-emnlp.294,J11-1007,0,0.0210025,"loss of information, resulting in lower results for our BERT StateTr+G2GTr model than the baseline. This problem is resolved by our SentTr parser design because all sub-words are input. The BERT SentTr+G2GTr model performs substantially better than the baseline on all languages, which confirms the effectiveness of our Graph2Graph Transformer architecture to capture a diversity of types of structure from a variety of corpus sizes. 6.3 Error Analysis To analyse the effectiveness of the proposed graph input and output mechanisms in variations of our StateTr model pre-trained with BERT, we follow McDonald and Nivre (2011) and measure their accuracy as a function of dependency length, distance to root, sentence length, and dependency type, as shown in Figure 3 and Table 3.12 . These results demonstrate that most of the improvement of the StateTr+G2GTr model over other variations comes from the hard cases which require a more global view of the sentence. Dependency Length: The leftmost plot shows labelled F-scores on dependencies binned by dependency lengths. The integrated G2GTr models outperform other models on the longer (more difficult) dependencies, which demonstrates the benefit of adding the partial depen"
2020.findings-emnlp.294,nilsson-nivre-2008-malteval,0,0.0197321,"pare to using the output embedding of the CLS token as the input to the transition classifiers for both the baseline model (StateCLSTr) and its combined version (StateTr+G2CLSTr). For Sentence Transformer, we evaluate the SentTr and SentTr+G2GTr models with BERT initialisation. We also evaluate the best variations of each baseline on the UD Treebanks.9 5.3 We evaluate our models on two types of datasets, WSJ Penn Treebank, and Universal Dependency (UD) Treebanks. Following Kulmizev et al. (2019), for evaluation, we include punctuation for UD treebanks and exclude it for the WSJ Penn Treebank (Nilsson and Nivre, 2008).7 7 et al. (2019). This set of languages contains different scripts, various morphological complexity and character set sizes, different training sizes, and non-projectivity ratios. Details of Implementation All hyper-parameter details are given in Appendix F. Unless specified otherwise, all models have 6 self-attention layers. We use the AdamW optimiser provided by Wolf et al. (2019) to fine-tune model parameters. All our models use greedy decoding, meaning that at each step only the highest scoring parser action is considered for continuation. This was done for simplicity, although beam sea"
2020.findings-emnlp.294,W04-0308,0,0.0262201,"o other NLP tasks that have any graph as the input and need to predict a graph over the same set of nodes as output. In summary, our contributions are: • We propose Graph2Graph Transformer for conditioning on and predicting graphs. • We propose two novel Transformer models of transition-based dependency parsing. • We successfully integrate pre-trained BERT initialisation in Graph2Graph Transformer. • We improve state-of-the-art accuracies for traditional transition-based dependency parsing.1 2 Transition-based Dependency Parsing Our transition-based parser uses arc-standard parsing sequences (Nivre, 2004), which makes parsing decisions in bottom-up order. The main data structures for representing the state of an arc-standard parser are a buffer of words and a stack of partially constructed syntactic sub-trees. At each step, the parser chooses between adding a leftward or rightward labelled arc between the top two words on the stack (LEFT-ARC(l) or RIGHT-ARC(l), where l is a dependency label) or shifting a word from the buffer onto the stack (SHIFT). To handle 1 Our implementation is available at: //github.com/alirezamshi/G2GTr non-projective dependency trees, we allow the SWAP action proposed"
2020.findings-emnlp.294,P09-1040,0,0.0222992,"hich makes parsing decisions in bottom-up order. The main data structures for representing the state of an arc-standard parser are a buffer of words and a stack of partially constructed syntactic sub-trees. At each step, the parser chooses between adding a leftward or rightward labelled arc between the top two words on the stack (LEFT-ARC(l) or RIGHT-ARC(l), where l is a dependency label) or shifting a word from the buffer onto the stack (SHIFT). To handle 1 Our implementation is available at: //github.com/alirezamshi/G2GTr non-projective dependency trees, we allow the SWAP action proposed in Nivre (2009), which shifts the second-from-top element of the stack to the front of the buffer, resulting in the reordering of the top two elements of the stack. 3 Graph2Graph Transformer We propose a version of the Transformer which is designed for both conditioning on graphs and predicting graphs, which we call Graph2Graph Transformer (G2GTr), and show how it can be applied to transition-based dependency parsing. G2GTr supports arbitrary input graphs and arbitrary edges in the output graph. But since the nodes of both these graphs are the input tokens, the nodes of the output graph are limited to the se"
2020.findings-emnlp.294,N18-1202,0,0.0385443,"Missing"
2020.findings-emnlp.294,N18-2074,0,0.212441,"ted with many previous structured prediction methods, making it easy to apply to a wide range of NLP tasks. 1 Introduction In recent years, there has been a huge amount of research on applying self-attention models to NLP tasks. Transformer (Vaswani et al., 2017) is the most common architecture, which can capture long-range dependencies by using a self-attention mechanism over a set of vectors. To encode the sequential structure of sentences, typically absolute position embeddings are input to each vector in the set, but recently a mechanism has been proposed for inputting relative positions (Shaw et al., 2018). For each pair of vectors, an embedding for their relative position is input to the self-attention function. This mechanism can be generalised to input arbitrary graphs of relations. We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as contin"
2020.findings-emnlp.294,Q14-1017,0,0.127593,"Missing"
2020.findings-emnlp.294,D13-1170,0,0.0205857,"Missing"
2020.findings-emnlp.294,D18-1548,0,0.0144317,"We propose a version of the Transformer architecture which combines this attention-based mechanism for conditioning on graphs with an attention-like mechanism for predicting graphs and demonstrate its effectiveness on syntactic dependency parsing. We call this architecture Graph2Graph Transformer. This mechanism for conditioning on graphs differs from previous proposals in that it inputs graph relations as continuous embeddings, instead of discrete model structure (e.g. (Henderson, 2003; Henderson et al., 2013; Dyer et al., 2015)) or predefined discrete attention heads (e.g. (Ji et al., 2019; Strubell et al., 2018)). An explicit representation of binary relations is supported by inputting these relation embeddings to the attention functions, which are applied to every pair of tokens. In this way, each attention head can easily learn to attend only to tokens in a given relation, but it can also learn other structures in combination with other inputs. This gives a bias towards attention weights which respect locality in the input graph but does not hard-code any specific attention weights. We focus our investigation on this novel graph input method and therefore limit our investigation to models which pre"
2020.findings-emnlp.294,P15-1150,0,0.0551226,"the BERT model. During training and evaluation, we use the pre-trained embedding of first sub-word as the token representation of each word and discard embeddings of non-first sub-words due to training Figure 2: An Example of Composition model. efficiency.2 parameters. The PoS embeddings are trained Composition Model: As an alternative to our proposed graph input method, previous work has shown that complex phrases can be input to a neural network by using recursive neural networks to recursively compose the embeddings of subphrases (Socher et al., 2011, 2014, 2013; Hermann and Blunsom, 2013; Tai et al., 2015). We extend the proposed composition model of Dyer et al. (2015) by applying a one-layer feed-forward neural network as a composition model and adding skip connections to each recursive step.3 Since a syntactic head may contain an arbitrary number of dependents, we compute new token embeddings of head-dependent pairs one at a time as they are specified by the parser, as shown in Figure 2. At each parser step t, we compute each new token embedding Cit of token i by inputting to the composition model, its previous token embedding Cjt−1 and the embedding of the most recent dependent with its asso"
2020.findings-emnlp.294,N03-1033,0,0.0391624,"Missing"
2020.findings-emnlp.294,P15-1032,0,0.215371,"Missing"
2020.findings-emnlp.294,D17-1175,0,0.394645,"Missing"
2021.acl-long.47,N19-1131,0,0.0249616,"80.74±7.6 86.67±5.0 60.74±16.66 76.29±4.45 81.48±6.2 87.41±2.96 89.63±4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the ada"
2021.acl-long.47,2020.coling-main.448,0,0.0333384,"ltiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019). Liu et al. (2019a) proposed Multi-Task Deep Neural Network (MTDNN) for learning from multiple NLU tasks. Although MTDNN obtains impressive results on GLUE, it applies multi-task learning as a form of pretraining followed by task-specific fine-tuning. Concurrently 572 Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020). Meta-learning approaches are notoriously slow to train. In addition, generating softmax parameters requires a substantially higher number of parameters, leaves the method unable to adapt the lower layers of the model, and restricts their application to classification tasks. ¨ un et al. (2020) In contemporaneous work, Ust¨ proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings. Their study is limited to m"
2021.acl-long.47,D19-1165,0,0.0354995,"generating separate adapter layers for each task. For each new task, our model only requires learning an additional task embedding, reducing the number of trained parameters. We use the encoder-decoder T5 model (Raffel et al., 2020) as the underlying model for our experiments and evaluate on the standard GLUE benchmark (Wang et al., 2019b). We achieve strong gains over both the T5BASE model as well as adapters (Houlsby et al., 2019). To our knowledge, this is the first time that adapters have been successfully integrated into a stateof-the-art encoder-decoder model beyond machine translation (Bapna and Firat, 2019), demonstrating that our method effectively balances sharing information across tasks while minimizing negative transfer. In summary, we make the following contributions: (1) We propose a parameter-efficient method for multitask fine-tuning based on hypernetworks and adapter layers. (2) We demonstrate that our method scales more efficiently than prior work. (3) We provide empirical results on GLUE demonstrating the effectiveness of the proposed method on multi-task learning. (4) We perform extensive few-shot domain transfer experiments, which reveal that the captured shared knowledge can posit"
2021.acl-long.47,2020.acl-main.625,0,0.0335057,"pter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights of the target model. Our method is substantially more efficient as we do not generate all the weights of the target model but a very small number of parameters for adapter modules to allow the model to adapt to each individual task efficiently. Similarly, Jin et al. (2020) generate the full model from task-specific descriptions in different domains whereas we efficiently generate only small adapter modules for each task. 5 Related Work Multi-task learning: Multi-task learning, i.e., learning a unified model to perform well on multiple different tasks, is a challenging problem in NLP. It requires addressing multiple challenges such as catastrophic forgetting, and handling disproportionate task sizes resulting in a model overfitting in lowresource tasks while underfitting in high-resource ones (Arivazhagan et al., 2019). Liu et al. (2019a) proposed Multi-Task Dee"
2021.acl-long.47,C02-1150,0,0.0401945,"4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the adapter and the task embedding respectively trained on the most simi"
2021.acl-long.47,P19-1441,0,0.0584278,"s (βτ and γτ ). During training, we only update layer normalizations in T5, hypernetworks, and task embeddings. The compact HYPERFORMER++ shares the same hypernetworks across all layers and tasks and computes the task embedding based on task, layer id, and position of the adapter module (§2.4). model. 2) Fine-tuning the model across multiple tasks allows sharing information between the different tasks and positive transfer to other related tasks. Specifically, when target datasets have limited training data, multi-task learning improves the performance compared to individually trained models (Liu et al., 2019a; Ratner et al., 2018). However, multi-task fine-tuning can result in models underperforming on high-resource tasks due to constrained capacity (Arivazhagan et al., 2019; McCann et al., 2018). An additional issue with multi-task fine-tuning is the potential for task interference or negative transfer, 565 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 565–576 August 1–6, 2021. ©2021 Association for Computational Linguistics where achieving good performance on one task can"
2021.acl-long.47,N19-1300,0,0.0187174,"ifferent from other tasks and is not grouped with any of the observed task embeddings. In addition, the task embeddings for 1) all the sentiment analysis datasets namely SST-2, Yelp polarity, and IMDB; 2) the two large-scale NLI datasets namely MNLI and SciTail; 3) question answering datasets, i.e. BoolQ and QNLI; and 4) paraphrase datasets namely QQP and PAWS are each grouped together. with us, Tay et al. (2021) propose a multi-task learning method by training task-conditioned hyper networks; however, their method is 43x less parameter efficient compared to ours. In another line of research, Clark et al. (2019b) proposed to learn multi-task models with knowledge distillation. Houlsby et al. (2019) trained adapters for each task separately, keeping the model fixed. Stickland and Murray (2019) share the model parameters across tasks and introduce task-specific adapter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights"
2021.acl-long.47,P11-1015,0,0.0361431,"3.40±0.2 4 16 32 100 250 57.78±10.9 77.04±7.2 80.0±7.6 85.93±5.4 85.19±4.7 51.11±9.2 74.81±5.4 74.81±5.9 80.74±7.6 86.67±5.0 60.74±16.66 76.29±4.45 81.48±6.2 87.41±2.96 89.63±4.32 Question Classification TREC 4 16 32 100 500 1000 2000 28.11±5.9 40.08±12.6 62.49±6.2 87.79±0.7 93.57±1.3 95.5±0.9 96.87±1.3 23.61±7.7 43.45±14.0 59.6±7.0 78.07±3.8 93.65±1.7 96.06±0.4 97.03±0.7 28.85±6.9 49.40±9.5 68.94±7.5 88.42±1.7 94.78±1.4 96.72±1.3 96.92±0.9 Question Answering BoolQ 3.3 Low-resource Fine-tuning Average scores on GLUE Sa m pl es Dataset # et al., 2019a); 3) the sentiment analysis datasets IMDB (Maas et al., 2011) and Yelp Polarity (Zhang et al., 2015); and 4) the paraphrase detection dataset PAWS (Baldridge et al., 2019); 5) the question classification dataset TREC (Li and Roth, 2002). For CB and BoolQ, since test sets are not available, we divide the validation sets in half, using one half for validation and the other for testing. For Yelp polarity, TREC, and IMDB, since validation sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test"
2021.acl-long.47,P19-1595,0,0.0171301,"ifferent from other tasks and is not grouped with any of the observed task embeddings. In addition, the task embeddings for 1) all the sentiment analysis datasets namely SST-2, Yelp polarity, and IMDB; 2) the two large-scale NLI datasets namely MNLI and SciTail; 3) question answering datasets, i.e. BoolQ and QNLI; and 4) paraphrase datasets namely QQP and PAWS are each grouped together. with us, Tay et al. (2021) propose a multi-task learning method by training task-conditioned hyper networks; however, their method is 43x less parameter efficient compared to ours. In another line of research, Clark et al. (2019b) proposed to learn multi-task models with knowledge distillation. Houlsby et al. (2019) trained adapters for each task separately, keeping the model fixed. Stickland and Murray (2019) share the model parameters across tasks and introduce task-specific adapter parameters, which is more parameter-inefficient than our method. Hypernetworks and contextual parameter generation: Our work is closely related to hypernetworks (Ha et al., 2017). In a continual learning setup, where tasks are learned sequentially, Oswald et al. (2020) proposed a task-conditioned hypernetwork to generate all the weights"
2021.acl-long.47,L18-1269,0,0.0206226,"n sets are not available, we similarly divide the test sets to form validation sets. For the rest, we report on the original test sets. We consider the models trained on GLUE reported in Table 1 and evaluate them on the test set after the few-shot fine-tuning on each target training data. For Adapters† and our method, we use the adapter and the task embedding respectively trained on the most similar GLUE task for initialization, i.e. MNLI for NLI, QNLI for QA, SST-2 for sentiment analysis, and QQP for paraphrase detection. Following prior evidence of positive transfer from NLI to other tasks (Conneau and Kiela, 2018; Yin et al., 2020; Phang et al., 2018), we initialize the out-of-domain TREC from MNLI. We show the results of full fine-tuning of all model’s parameters, Adapters†, and HYPERFORMER++4 in Table 2. Our method significantly surpasses the baselines on the majority of settings. 4 16 32 100 500 1000 2000 85 50.49±11.1 56.50±7.1 58.43±4.9 60.10±2.4 66.49±1.2 69.01±1.1 71.58±0.8 53.48±2.8 51.37±6.5 54.52±5.1 58.60±1.6 66.72±0.7 70.21±1.3 73.60±0.8 48.03±4.8 50.21±7.9 58.37±3.7 62.03±2.0 70.04±1.4 72.35±1.7 74.94±0.6 Sentiment Analysis 80 75 IMDB 70 65 T5BASE HyperFormer++BASE 60 0 1000 2000 3000 # S"
2021.acl-long.47,W19-4302,1,0.820575,"aneously on multiple tasks, obtaining state-of-the-art performance across a diverse set of tasks. We use the T5 framework as it enables training a universal model that interfaces with many language tasks. Our model has three main components: 1) task conditional adapter layers; 2) task conditional layer normalizations; and 3) hypernetworks that generate task-specific parameters. We next describe these components. 2.1 Task Conditional Adapter Layers Prior work has shown that fine-tuning all parameters of the model can result in a sub-optimal solution, particularly for resource-limited datasets (Peters et al., 2019). As an alternative to fine-tuning all the model’s parameters, prior work (Houlsby et al., 2019; Rebuffi et al., 2018; Stickland and Murray, 2019) inserted small modules called adapter layers within layers of a pretrained model, as shown in Figure 1. Adapters introduce no change to the structure or parameters of the original model. In this work, we propose conditional adapter modules, in which we generate the adapters weights based on input task embeddings using shared hypernetworks (Ha et al., 2017), which capture information across tasks that can be used to positively transfer to other relev"
2021.acl-long.47,D18-1039,0,0.0204,"572 Prior work also proposed meta-learning or Bayesian approaches to generate softmax layer parameters for new settings (Bansal et al., 2020; Ponti et al., 2020). Meta-learning approaches are notoriously slow to train. In addition, generating softmax parameters requires a substantially higher number of parameters, leaves the method unable to adapt the lower layers of the model, and restricts their application to classification tasks. ¨ un et al. (2020) In contemporaneous work, Ust¨ proposed a multilingual dependency parsing method based on adapters and contextual parameter generator networks (Platanios et al., 2018) where they generate adapter parameters conditioned on trained input language embeddings. Their study is limited to multilingual dependency parsing, while our work studies multi-task learning and applies to several tasks thanks to the general sequence-to-sequence nature of our model. Moreover, their number of trainable parameters is 2.88× larger than their base model since they employ a contextual parameter generator in each layer. In contrast, we use a single compact hypernetwork allowing us to efficiently condition on multiple tasks and layers of a transformer model. Marie-Catherine De Marne"
2021.acl-long.47,Q19-1040,0,0.019891,"sk projector network hI (.), which is a multi-layer perceptron consisting of two feed-forward layers and a ReLU non-linearity: Iτ =hI (zτ ), (5) t0 where zτ ∈ R can be a learnable parameter or any pretrained task features (Vu et al., 2020), and the task projector network hI (.) learns a suitable compressed task embedding from input task features. In this work, we consider a parametric zτ to allow end-to-end training which is convenient in practice.1 Removing task prefixes: The T5 model prepends task-specific prefixes to the input sequence for conditioning. For instance, when training on CoLA (Warstadt et al., 2019), cola sentence: is prepended to each sample. Instead, we remove task prefixes and use task embeddings for conditioning. Task conditioned hypernetworks: We consider simple linear layers as hypernetworks that are functions of input task embeddings Iτ . We introduce these hypernetworks in each layer of the transformer. We define hypernetwork hlA(.) that generates task conditional adapter weights (Uτl , Dτl ): Conventional layer normalization (Ba et al., 2016) is defined as: 1 We ran some pilot experiments with pretrained task embeddings (Vu et al., 2020), but did not observe extra benefits. 567"
2021.acl-long.47,2020.emnlp-main.180,0,0.128417,"Missing"
2021.conll-1.27,D18-1307,0,0.346969,"h vastly outperforms the baseline classifier. A tSNE (Van der Maaten and Hinton, 2008) analysis illustrates that meaningful relation-related clusters can be identified in the learned embedding space. This provides a second strong indication that structure can be effectively imposed on LM embeddings using our proposed framework. Even if the main focus of this work is not solving the IE problem directly, to further explore the capabilities of the relation-aware representation space, we train a simple KNN classifier for RE that is competitive with state-of-the-art performance. Strict evaluation (Bekoulis et al., 2018b; Taillé et al., 2020) of the RE task presupposes correct detection of the boundaries and the entity type of each argument in the relation. Hence, we apply the CL paradigm to learn a distinct embedding space for the entities and use a KNN classifier to solve the NER task. Finally, we perform a strict evaluation of the complete entity-relation extraction task. This transparent, computationally inexpensive and intuitively simple approach has comparable results to the state-of-the-art models. This achievement illustrates how informative and meaningful the learned embedding spaces are. In summary"
2021.conll-1.27,P06-1060,0,0.0151082,"ere relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text. However, their ability to encapsulate ple baseline neural network classifier for RE, us337 Proceedings of the 25th Conference on Computational Natural Language Learning (CoNLL), pages 337–348 November 10–11, 2021. ©"
2021.conll-1.27,D10-1033,0,0.0298654,"Missing"
2021.conll-1.27,N07-1015,0,0.0913009,"BERT text encoder. Different graph formulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach,"
2021.conll-1.27,2021.ccl-1.108,0,0.0383346,"Missing"
2021.conll-1.27,P16-1105,0,0.0554233,"Missing"
2021.conll-1.27,N18-1202,0,0.0493942,"e entities. Adverse effects (AEs) cover a range of signs, symptoms, diseases, disorders, abnormalities, organ damage and even death caused by that drug. The corpus is annotated at the sentence-level, so non-local relations (between entities of different sentences) do not exist. 2.1 Data Preprocessing The input of the main CL framework consists of the encoded padded sentence and the relation graph, which is extracted from the sentence. The graphs are used only in the training setup. To prepare the input for CharacterBERT, tokenization is applied to each sentence using the character-CNN module (Peters et al., 2018). The BERT tokenizer handles out-of-vocabulary (OOV) words by splitting these words into word pieces. However, the existence of word pieces can be an obstacle in creating and testing the CL experiments of this study from the implementation point of view. Additionally, word pieces may add biases to the model (El Boukkouri et al., 2020), especially in biomedical text where most of the drugs and many adverse effects are OOV words. Hence, CharacterBERT is chosen instead of BERT. For each sentence, a knowledge graph is ob• We propose a novel CL framework for impostained to model the relations betwe"
2021.conll-1.27,P13-1147,0,0.021336,"ulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text. However, their ability to e"
2021.conll-1.27,W09-1119,0,0.0155515,"information in the graph effectively (Kipf and Welling, 2016) and is described by the following equations: Ahat = A + I, (1) Anorm = D−0.5 ∗ Ahat ∗ D−0.5 , (2) where A is the initial adjacency matrix, I is the identity matrix and D is the degree matrix. Initially, the whole corpus is stored in one text file. Hence, the data should be transformed and stored using a different more flexible format. For each sentence of the dataset, a distinct JSON file is created and contains a list with the tokens 1 , a list with named entity (NE) tags adopting the BIO encoding scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009), a list with token index pairs that are members of an existing relation, the padded encoded version of the sentence, the attention mask vector of the sentence, a list with the embeddings of each node of the graph and the normalized adjacency matrix. 2.2 Dataset Statistics The ADE dataset is not officially split into training, validation, and test sets. Hence, we evaluate our models using 10-fold cross-validation similar to Li et al. (2017). We use the same splits as Eberts and 1 The sentence tokenization is performed using the SpaCy library. Ulges (2020). As Taillé et al. (2020) stresses, man"
2021.conll-1.27,E99-1023,0,0.501756,"ating and propagating the information in the graph effectively (Kipf and Welling, 2016) and is described by the following equations: Ahat = A + I, (1) Anorm = D−0.5 ∗ Ahat ∗ D−0.5 , (2) where A is the initial adjacency matrix, I is the identity matrix and D is the degree matrix. Initially, the whole corpus is stored in one text file. Hence, the data should be transformed and stored using a different more flexible format. For each sentence of the dataset, a distinct JSON file is created and contains a list with the tokens 1 , a list with named entity (NE) tags adopting the BIO encoding scheme (Sang and Veenstra, 1999; Ratinov and Roth, 2009), a list with token index pairs that are members of an existing relation, the padded encoded version of the sentence, the attention mask vector of the sentence, a list with the embeddings of each node of the graph and the normalized adjacency matrix. 2.2 Dataset Statistics The ADE dataset is not officially split into training, validation, and test sets. Hence, we evaluate our models using 10-fold cross-validation similar to Li et al. (2017). We use the same splits as Eberts and 1 The sentence tokenization is performed using the SpaCy library. Ulges (2020). As Taillé et"
2021.conll-1.27,P11-1053,0,0.0384971,"fferent graph formulations how to successfully combine both representathat represent the text relations are explored. The tion spaces in an entity-relation task. main goal is to create a common embedding space where relations can be easily detected. To evaluate 1 Introduction progress towards this goal, we use the ADE dataset (Gurulingappa et al., 2012), which is widely used Pretrained language models (LMs), such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) for relation extraction (RE) (Zhao and Grishman, and GPT-3 (Brown et al., 2020), capture contex- 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) and named entity recognitualized information effectively and are used in a wide variety of natural language processing (NLP) tion (NER) tasks (Curran and Clark, 2003; Florian et al., 2006; Nadeau and Sekine, 2007; Florian tasks. They have revolutionized NLP research. The et al., 2010) in the challenging field of information main mechanism of these models is multi-head extraction (IE) from biomedical text. self-attention (Vaswani et al., 2017), which enables capturing patterns of semantic and syntactic interTo evaluate the efficacy of our approach, a simest in text."
2021.conll-1.27,2020.emnlp-main.133,0,0.0865003,"xtract very good representations for the NER task. Figure 8: tSNE plot - Entity representation space obtained with CLNER model 6 Entity-Relation task considered correct if its type and the two entities (boundaries and type) involved in the relation are correctly predicted. We measure precision, recall and F1 score. Following previous work on IE, we report the macro-averaged F1 score, and as 10-fold cross-validation is adopted, we average the scores over the folds. Model Li et al., 2016 Li et al., 2017 Bekoulis et al., 2018b Bekoulis et al., 2018a Tran and Kavuluru, 2019 Eberts and Ulges, 2020 Wang and Lu, 2020 Zhao et al., 2020 Ours NER 79.5 84.6 86.4 86.73 87.11 89.25 89.7 89.4 88.3 RE 63.4 71.4 74.58 75.52 77.29 79.24 80.1 81.14 79.97 RE86.5 Table 4: Test set results: macro-averaged F1 score The insights of the tSNE analysis, with the wellTable 4 presents the best performing models, defined clusters in the embedding spaces, lead us to evaluated on the ADE (Gurulingappa et al., 2012) approach the entity-relation task using intuitively dataset. These studies address the IE problem as simple and transparent KNN classifiers. For the a joint task, solving NER and RE tasks jointly. Li RE task, we utili"
2021.iwpt-1.22,2020.iwpt-1.24,1,0.88458,"deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-process"
2021.iwpt-1.22,2020.iwpt-1.16,0,0.101583,"a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and lend themselves more easily to the 3. A post-processor ensuring that every graph is a rooted graph where all nodes are reachable from the notional root token. Our official system placed 6th out of 9 teams with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. In a number of unofficial postevaluation experiments, we make four incremental changes to our pipeline approach: 1. We replace the Stanza pre-processing pipeline with Tranki"
2021.iwpt-1.22,2021.iwpt-1.15,0,0.0243899,"dline experiments which include using Trankit for pre-processing, XLMRoBERTaLARGE , treebank concatenation, and multitask learning between a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and lend themselves more easily to the 3. A post-processor ensuring that every graph is a rooted graph where all nodes are reachable from the notional root token. Our official system placed 6th out of 9 teams with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. In a number of unofficial postevalu"
2021.iwpt-1.22,2020.acl-main.747,0,0.100941,"Missing"
2021.iwpt-1.22,2020.iwpt-1.20,0,0.0944655,"Missing"
2021.iwpt-1.22,N19-1423,0,0.0123963,"or English. They use a rule-based system that converts basic UD trees to enhanced UD graphs based on dependency structures identified to require enhancement. Nivre 2 https://github.com/jbrry/ IWPT-2021-shared-task The IWPT 2020 Shared Task on Parsing Enhanced Universal Dependencies The first shared task on parsing Enhanced Universal Dependencies (Bouma et al., 2020) brought renewed attention to the problem of predicting enhanced UD graphs. Ten teams submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers t"
2021.iwpt-1.22,P81-1022,0,0.526309,"Missing"
2021.iwpt-1.22,P18-2077,0,0.0164803,"check for unreachable nodes and the number of unreachable nodes that can be reached from them. We choose the candidate which maximises this number (in the case there are ties, we choose the first node in surface order) and makes it a child of the notional ROOT, i.e. this node becomes an additional root node. System outputs are then validated at level 2 by the UD validator 6 to catch bugs prior to submission. 4 Loss Function For edge prediction, sigmoid cross-entropy loss is used, and for label prediction, as we want to select the label for each chosen edge, softmax cross-entropy loss is used (Dozat and Manning, 2018). We interpolate between the loss given by the edge classifier and the loss given by the label classifier (Dozat and Manning, 2018; Wang et al., 2020) with a constant λ: L = λL(label) + (1 − λ)L(edge) Size 768 1024 300 300 0.35 0.35 0.5 0.10 Experiments In this section, we discuss our official results and then describe post-deadline experiments that improved our submission’s score. Model hyperparameters are listed in Table 1. The choice of XLM-R encoder (Base or Large) determines the hyperparameters of the encoder part of our model. In our official submission, we use XLM-RBase . A dropout valu"
2021.iwpt-1.22,P15-1033,0,0.014271,"ions, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-processing which involves altering enhanced dependency labels with lemma information. Input Sentence Pre-Processing 3.2 Enhanced UD Parsing XLM-Roberta CLS S1 S2 S3 S4 ... Z1 Z2 S5 SEP Z6 Z7 ... Z3 EUD Decoder"
2021.iwpt-1.22,2020.iwpt-1.23,0,0.156782,"g the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based system to generate EUD graphs from the predicted trees. 3 Official System Overview This section describes our official system, which is the system we submitted prior to the competition deadline. The architecture of our system is 205 for completeness and to enable possible additional post-processing which involves altering enhanced dependency labels with lemma information. Input Sentence Pre-Processing 3.2 Enhanced UD Parsing XLM-Roberta CLS S1 S2 S3 S4 ... Z1 Z2 S5 SEP Z6 Z7 ... Z3 EUD Decoder Z4 Z5 Basic Decoder EUD"
2021.iwpt-1.22,2020.iwpt-1.26,0,0.0477423,"Missing"
2021.iwpt-1.22,2020.iwpt-1.19,0,0.0327498,"veloped a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are used as frozen embeddings (Hershcovich et al., 2020). The only transition-based system among the participating teams (Hershcovich et al., 2020) used a stack-LSTM architecture (Dyer et al., 2015). Ek and Bernardy (2020) and Dehouck et al. (2020) combine basic dependency parsers and a rule-based s"
2021.iwpt-1.22,P80-1024,0,0.530287,"Missing"
2021.iwpt-1.22,2020.iwpt-1.17,0,0.0453022,"Missing"
2021.iwpt-1.22,Q16-1023,0,0.173874,"res apart from the enhanced dependency graphs and miscellaneous items in CoNLL-U files), we use the Stanza library (Qi et al., 2020) trained on version 2.7 of the UD treebanks for each treebank released as part of the training data for the shared task.4 Note that our parser does not pre-suppose any input features other than the input text but we predict the base features using our pre-processing pipeline For the enhanced UD parser, we use a Transformer encoder in the form of XLM-R (Conneau et al., 2020) with a first-order arc-factored model which utilizes the edge and label scoring method of (Kiperwasser and Goldberg, 2016). In initial experiments, we found this model to perform better than biaffine attention (Dozat and Manning, 2016) for the task of EUD parsing. This finding was also made by (Lindemann et al., 2019) and (Straka and Strakov´a, 2019) for the task of semantic parsing across numerous Graphbanks (Oepen et al., 2019). Straka and Strakov´a (2019) suggest that biaffine attention may be less suitable for predicting whether an edge exists between any pair of nodes using a predefined threshold and is perhaps more suited for dependency parsing, where words are competing with one another to be classified as"
2021.iwpt-1.22,D19-1279,0,0.0175972,"sentations to UD in the form of enhanced UD relations for English. They use a rule-based system that converts basic UD trees to enhanced UD graphs based on dependency structures identified to require enhancement. Nivre 2 https://github.com/jbrry/ IWPT-2021-shared-task The IWPT 2020 Shared Task on Parsing Enhanced Universal Dependencies The first shared task on parsing Enhanced Universal Dependencies (Bouma et al., 2020) brought renewed attention to the problem of predicting enhanced UD graphs. Ten teams submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inferen"
2021.iwpt-1.22,P19-1450,0,0.0178779,"part of the training data for the shared task.4 Note that our parser does not pre-suppose any input features other than the input text but we predict the base features using our pre-processing pipeline For the enhanced UD parser, we use a Transformer encoder in the form of XLM-R (Conneau et al., 2020) with a first-order arc-factored model which utilizes the edge and label scoring method of (Kiperwasser and Goldberg, 2016). In initial experiments, we found this model to perform better than biaffine attention (Dozat and Manning, 2016) for the task of EUD parsing. This finding was also made by (Lindemann et al., 2019) and (Straka and Strakov´a, 2019) for the task of semantic parsing across numerous Graphbanks (Oepen et al., 2019). Straka and Strakov´a (2019) suggest that biaffine attention may be less suitable for predicting whether an edge exists between any pair of nodes using a predefined threshold and is perhaps more suited for dependency parsing, where words are competing with one another to be classified as the head in a softmax layer. The consistency of these findings across EUD and semantic parsing Graphbanks may provide evidence that enhanced UD is closer to semantic dependency parsing than basic"
2021.iwpt-1.22,de-marneffe-etal-2006-generating,0,0.15849,"Missing"
2021.iwpt-1.22,2021.eacl-demos.10,0,0.487601,"Missing"
2021.iwpt-1.22,W18-6012,0,0.350407,"Missing"
2021.iwpt-1.22,K19-2001,0,0.17854,"Missing"
2021.iwpt-1.22,2020.acl-demos.14,0,0.0729078,"dge-scoring and labeling model to predict the enhanced graph. Finally, we run a post-processing script to ensure all of our outputs are valid Enhanced UD graphs. Our system places 6th out of 9 participants with a coarse Enhanced Labeled Attachment Score (ELAS) of 83.57. We carry out additional post-deadline experiments which include using Trankit for pre-processing, XLMRoBERTaLARGE , treebank concatenation, and multitask learning between a basic and an enhanced dependency parser. All of these modifications improve our initial score and our final system has a coarse ELAS of 88.04. 1 1. Stanza (Qi et al., 2020) for sentence segmentation, tokenization and the prediction of all UD features apart from the enhanced graph. 2. A Transformer-based dependency parsing model to predict Enhanced UD graphs. Introduction The IWPT 2021 Parsing Shared Task: From Raw Text to Enhanced Universal Dependencies (Bouma et al., 2021) is the second task involving the prediction of Enhanced Universal Dependencies (EUD) graphs1 following the 2020 task (Bouma et al., 2020). EUD graphs are an extension of basic UD trees, designed to be more useful in shallow natural language understanding tasks (Schuster and Manning, 2016) and"
2021.iwpt-1.22,L16-1376,0,0.194834,"Missing"
2021.iwpt-1.22,K17-3009,0,0.0628734,"Missing"
2021.iwpt-1.22,K19-2012,0,0.0430282,"Missing"
2021.iwpt-1.22,2020.iwpt-1.22,0,0.243224,"s submitted to the task. The winning system (Kanerva et al., 2020) utilized the UDify model (Kondratyuk and Straka, 2019), which uses a BERT model (Devlin et al., 2019) as the encoder with multitask classifiers for POStagging, morphological prediction and dependency parsing built on top. They developed a system for encoding the enhanced representation into the basic dependencies so it can be predicted in the same way as a basic dependency tree but with enriched dependency types that can then be converted into the enhanced structure. In an unofficial submission shortly after the task deadline, Wang et al. (2020) outperform the winning system using second-order inference methods with Mean-Field Variational Inference. Most systems used pretrained Transformers to obtain token representations, either by using the Transformer directly (Kanerva et al., 2020; Gr¨unewald and Friedrich, 2020; He and Choi, 2020) or passing the encoded representation to BiLSTM layers where they are combined with other features such as context-free FastText word embeddings (Wang et al., 2020), character features and features obtained from predicted POS tags, morphological features and basic UD trees (Barry et al., 2020), or are"
2021.naacl-main.39,Q17-1010,0,0.0461493,"t of the classifier comes from the CLWE models. We report the average accuracy over ten runs. Language pairs In this paper, we focus on projecting foreign language embeddings into the English space. We choose the eight languages included in MLDoc for both the BLI and CLDC tasks. Within the seven non-English languages, Japanese, Russian and Chinese are languages distant from English and the others are languages similar to English. For the task of BLI, we also investigate Turkish, another language distant from English. Monolingual word embeddings We use the pretrained FastText embedding models (Bojanowski et al., 2017) for our experiments. These embeddings of 300 dimensions are pretrained on Wikipedia dumps and publicly available.6 Following previous works, we use the first 200,000 most 5 6 https://github.com/zhangmozhi/retrofit_clwe https://fasttext.cc/docs/en/pretrained-vectors.html BLI Task - with refinement de es fr it ja ru tr zh PROC RCSLS 73.1 83.6 82.2 77.5 37.9 64.3 63.1 40.0 73.1 83.1 83.1 78.9 39.3 64.6 63.1 43.0 MUSE VecMap 73.7 83.0 82.2 78.5 29.3 62.7 60.5 38.1 73.6 83.7 82.9 78.5 34.7 63.1 61.3 36.4 Ours GRef 74.1 83.7 82.4 78.6 34.1 64.0 61.2 38.2 Ours LRef 66.6 79.3 77.8 70.3 23.7 46.5 39.7"
2021.naacl-main.39,D18-1214,0,0.0354404,"Missing"
2021.naacl-main.39,P19-1070,0,0.0248263,"Missing"
2021.naacl-main.39,P17-1042,0,0.0213616,"ictionary. Early work from mately isomorphic. We assume instead that, Mikolov et al. (2013) uses a seed dictionary of fiveespecially across distant languages, the mapthousand word pairs. Since then, the size of the ping is only piece-wise linear, and propose a multi-adversarial learning method. This novel seed dictionary has been gradually reduced, from method induces the seed cross-lingual dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully"
2021.naacl-main.39,P15-1119,0,0.029859,"part, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it often exhibits poor perin two embedding spaces of different language"
2021.naacl-main.39,P18-1073,0,0.221034,"l version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages—"
2021.naacl-main.39,W16-1614,0,0.0225084,"d cross-lingual dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinem"
2021.naacl-main.39,D18-1330,0,0.0121284,"by learning a multi-linear mapping instead of only a single-linear mapping. Therefore, we use the GAN-based system MUSE (Conneau et al., 2018)8 as our main unsupervised baseline. Since the unsupervised method proposed by Artetxe et al. (2018)9 is considered a robust CLWE system, we also use it as our second unsupervised baseline (VecMap in the tables). In the setting with refinement, we use the iterative refinement with stochastic dictionary induction for all the unsupervised systems.10 We also include two supervised systems, Procrustes (PROC) (Conneau et al., 2018) and Relaxed CSLS (RCSLS) (Joulin et al., 2018), to better understand 7 The original pretrained Latvian fastText model only consists of 171,000 words. 8 https://github.com/facebookresearch/MUSE 9 https://github.com/artetxem/vecmap 10 We disabled the re-weighting technique since it’s not applicable for L-Ref. However, adding re-weighting to VecMap, MUSE and G-Ref doesn’t change the gaps between them. 469 our method. Both PROC and RCSLS are robust supervised systems for learning CLWE and have been widely used previously (Glavaš et al., 2019; Zhang et al., 2020). We also wanted to include the supervised system proposed by Nakashole (2018), wh"
2021.naacl-main.39,C12-1089,0,0.0431514,"nd target distributions, and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar la"
2021.naacl-main.39,D18-1047,0,0.0799538,"Because the isomorphism assumption is not observed in reality, we argue that a 464 1 2 https://github.com/facebookresearch/MUSE https://fasttext.cc/docs/en/pretrained-vectors.html To reduce the influence of infrequent words, we only consider the first fifty-thousand most frequent source words. As we can see in Figure 1, the distribution of accuracies of different subspaces is not uniform or even nearly so. This is true for both language pairs, but particularly for the distant languages, where the general mapping does not work at all in some subspaces. Similar phenomena were also discovered by Nakashole (2018) where source words are grouped into different categories. This lack of uniformity in results corroborates the appropriateness of designing a model that learns different linear mappings for different subspaces instead of only learning a single linear mapping for the entire source space. 3 Multi-adversarial CLWE learning To learn different mappings for different source subspaces, we propose a method for training one GAN for each source subspace. These multidiscriminator GANs encourage the distribution of mapped word embeddings from a specific source subspace to match the distribution of word em"
2021.naacl-main.39,P19-1018,0,0.0221705,"encourage words from a specific 3 source subspace to be trained against words from a We use different language discriminator models Dli for each subspace i, even though their training samples all come matching target subspace, we need to align the two from the same distributions. This leads to more stable training, cross-language subspaces. The second problem presumably because initially these language discriminators are randomly different. we need to solve for our multi-adversarial method 466 to work is how to discover this alignment. Although metrics such as Gromov-Hausdorff distance (GH) (Patra et al., 2019) and Eigenvalue Divergence (EVD) (Dubossarsky et al., 2020) can be used to measure the similarity between two distributions and find the most similar target subspace for a given source subspace, matching between two sub-distributions may amplify any bias generated during the clustering. To avoid this problem, we only run the clustering on the source side. For a given source embedding space denote its subspaces after clus V1s , we 2 i n tering as Vs , Vs , ..., Vs , ..., Vs , where n represent the number of subspaces. To align target words to their matching source subspace, we propose to first"
2021.naacl-main.39,L18-1560,0,0.0224698,"eau et al. (2018) for the task of BLI. This dataset contains high quality dictionaries for more than 150 language pairs. For each language pair, it provides a training dictionary of 5000 words and a test dictionary of 1500 words. This dataset allows us to have a better understanding of the performance of our proposal on many different language pairs. For each language pair, we retrieve the best translations of source words in the test dictionary using CSLS, and we report the accuracy with precision at one (P@1). CLDC setting We use the multilingual classification benchmark (MLDoc) provided by Schwenk and Li (2018) for the task of CLDC. MLDoc contains training and test documents with balanced class priors for eight languages: German (de), English (en), Spanish (es), French (fr), Italian (it), Japanese (ja), Russian (ru) and Chinese (zh). We follow previous works (Glavaš et al., 2019; Zhang et al., 2020) and train a CNN classifier on English using 10,000 documents and test the classifier on the other seven languages.5 Each language contains 4000 test documents. The input of the classifier comes from the CLWE models. We report the average accuracy over ten runs. Language pairs In this paper, we focus on p"
2021.naacl-main.39,P18-1072,0,0.0365931,"Missing"
2021.naacl-main.39,D19-1449,0,0.0334078,"Missing"
2021.naacl-main.39,2020.emnlp-main.257,0,0.0218187,"Missing"
2021.naacl-main.39,D19-1450,1,0.811119,"aining the GANs Training the GANs described in Sections 3.2 and 3.3 can be challenging. Based on previous work and our experience, we employ the following techniques during training. Orthogonalization Previous work shows that enforcing the mapping matrix W to be orthogonal during the training can improve the performance (Smith et al., 2017). In the system of Conneau et al. (2018), they follow the work of Cisse et al. (2017) and approximate setting W to an orthogonal matrix with W ← (1+β)W −β(W W > )W . This orthogonalization usually performs well when setting β to 0.001 (Conneau et al., 2018; Wang et al., 2019). Parameter-free hierarchical clustering A major issue in clustering an embedding space is how Cross-Domain Similarity Local Scaling The to find a clustering that adapts to the space, without trained mapping matrix W can be used for retrievfixed parameters. To avoid having to identify the number of subspaces in advance, we use hierarchi- ing the translation for a given source word ws by cal clustering. Recent work proposes a parameter- searching a target word wt whose embedding vecfree method called First Integer Neighbor Clus- tor vt is close to W vs . But Conneau et al. (2018) tering Hierarc"
2021.naacl-main.39,N15-1104,0,0.026297,"s. This cosine-based criterion has been shown to correlate well with the quality of W (Conneau et al., 2018; Hartmann et al., 2019). language (ws0 1 , ws0 2 , ..., ws0 10000 ). The mutual translation pairs (wsi , wti ) such that wsi = ws0 i constitute the seed dictionary. This guarantees that the induced seed dictionary will be bidirectional. Mapping refinement The refinement step is based on the Procrustes Analysis (Schönemann, 1966). With the seed dictionary, the mapping can be updated using the objective in equation (1), and forced to be orthogonal using singular value decomposition (SVD) (Xing et al., 2015). Later work combines the Procrustes Analysis with stochastic dictionary induction (Artetxe et al., 2018) and greatly improves the performance of the standard refinement (Hartmann et al., 2019). More specifically, in order to prevent local optima, after each iteration some elements of the similarity matrix are randomly dropped, so that the similarity distributions of words change randomly and the new seed dictionary for the next iteration varies. Random restarts Previous work (Vuli´c et al., 2019; Glavaš et al., 2019) shows that using GANs to train the mapping matrix W is not stable. Hartmann"
2021.naacl-main.39,P17-1179,0,0.0189729,"l dictioseveral-thousand to fifty word pairs (Smith et al., nary through multiple mappings, each induced 2017), reaching a minimal version of only sharing to fit the mapping for one subspace. Our exnumerals (Artetxe et al., 2017). periments on unsupervised bilingual lexicon More recent works on unsupervised learning induction and cross-lingual document classifihave shown that mappings across embedding cation show that this method improves performance over previous single-mapping methods, spaces can also be learned without any bilingual evespecially for distant languages. idence (Barone, 2016; Zhang et al., 2017; Conneau et al., 2018; Hoshen and Wolf, 2018; Alvarez-Melis 1 Introduction and background and Jaakkola, 2018; Artetxe et al., 2018). More conWord embeddings, continuous vectorial represen- cretely, these fully unsupervised methods usually consist of two main steps (Hartmann et al., 2019): tations of words, have become a fundamental an unsupervised step which aims to induce the seed initial step in many natural language processing (NLP) tasks for many languages. In recent years, dictionary by matching the source and target distributions, and then a pseudo-supervised refinement their cross-ling"
2021.naacl-main.39,2020.acl-main.201,0,0.23031,"ss languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it often exhibits poor perin two embedding spaces of different languages ap- formance on typologically-distant language pairs, 463 Proceedings of the 2021 Conference of the North Am"
2021.naacl-main.39,P19-1307,0,0.0482446,"Missing"
2021.naacl-main.39,D13-1141,0,0.0315019,"and then a pseudo-supervised refinement their cross-lingual counterpart, cross-lingual word step based on this seed dictionary. embeddings (CLWE) —maps of matching words across languages— have been shown to be useful in The system proposed by Conneau et al. (2018) many important cross-lingual transfer and model- can be considered the first successful unsupervised ing tasks such as machine translation, cross-lingual system for learning CLWE. They first use generadocument classification and zero-shot dependency tive adversarial networks (GANs) to learn a single parsing (Klementiev et al., 2012; Zou et al., 2013; linear mapping to induce the seed dictionary, folGuo et al., 2015; Conneau et al., 2018; Glavaš et al., lowed by the Procrustes Analysis (Schönemann, 2019; Zhang et al., 2020). 1966) to refine the linear mapping based on the In these representations, matching words across induced seed dictionary. While this GAN-based different languages are represented by similar vec- model has competitive or even better performance tors. Following the observation of Mikolov et al. compared to supervised methods on typologically(2013) that the geometric positions of similar words similar language pairs, it o"
2021.newsum-1.1,D19-1387,0,0.245268,"generated summaries are more abstractive and at the same time achieve high ROUGE scores when compared to human reference summaries. We verify the effectiveness of our design decisions with extensive evaluations. 1 Introduction Abstractive summarization has improved drastically in recent years due to more efficient decoder architectures, like the Transformer (Vaswani et al., 2017), and language model pretraining, such as BERT (Devlin et al., 2019). As a result of these advances, current state-of-the-art models reach the performance of extractive systems, and even surpass them on some datasets (Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2020). Part of this success, however, is due to the development of stronger copy mechanisms such as the pointer-generator network (See et al., 2017) or attention to the source document (Rush et al., 2015). The so-generated summaries copy long sequences from the input document, strung together with filler words. While this achieves better results in the predominant evaluation metric ROUGE (Lin, 2004), it comes at the cost of the summaries’ abstractiveness and coherence, two qualities that we expect from human-written summaries. 1 Our code is available at http"
2021.newsum-1.1,D18-1325,1,0.830389,"(2016) use hierarchical attention in the encoder with a word- and a sentence-level RNN. The attention weights at the word level are re-weighted by the sentence-level attention weights. Celikyilmaz et al. (2018) divide the document into paragraphs, which are encoded separately by agents. Each agent performs attention within its paragraph, and the decoder attends to the agents. Gehrmann et al. (2018) first employ a content selector at the word level to decide which words are candidates for copying. They then use a pointergenerator network with just the admissible tokens to generate the summary. Miculicich et al. (2018) use hierarchical attention networks (Yang et al., 8 sentence to be generated, and then generate its left and right context. In contrast to these approaches, our sentence generator outputs a latent representation rsent for the entire sentence, which is used to condition the word generator. We do not tie this representation to specific words. 6 Kedar Dhamdhere, Mukund Sundararajan, and Qiqi Yan. 2019. How important is a neuron. In International Conference on Learning Representations. Rotem Dror, Gili Baumer, Segev Shlomov, and Roi Reichart. 2018. The hitchhiker’s guide to testing statistical si"
2021.newsum-1.1,2020.emnlp-demos.6,0,0.0402679,"Missing"
2021.newsum-1.1,K16-1028,0,0.174227,"that we use to evaluate our model, and give implementation details (§ 3.3) to replicate our experiments. Dataset statistics are shown in Table 1. 2.2 3.1 3 Word Generator Our word generator is also a Transformer decoder. The regular Transformer decoder consists of layers l with self-attention, cross-attention and feedforward sublayers. They are defined as follows: sl = LN(hl−1 + SelfAtt(hl−1 )) l l l (3) hl = LN(cl + FFN(cl )) (4) Datasets CNN/DailyMail. The CNN/DailyMail corpus was initially introduced as a question answering dataset in Hermann et al. (2015) and adapted for summarization by Nallapati et al. (2016), and has been widely used. The corpus’s summaries are a concatenation of bullet points describing the highlights of the news article. They are therefore designed to be concise, but do not necessarily form a fluent summary. Extractive approaches perform well on CNN/DailyMail (Liu and Lapata, 2019). (2) c = LN(s + CrossAtt(s , renc )) Experimental Setup where LN is layer normalization (Ba et al., 2016), SelfAtt stands for self-attention, CrossAtt is the cross-attention to the encoder outputs renc , and FFN is the feed-forward sublayer consisting of two fully-connected layers with an intermediat"
2021.newsum-1.1,N16-1174,0,0.163602,"Missing"
2021.newsum-1.1,N16-1000,0,0.445451,"valuate our model, and give implementation details (§ 3.3) to replicate our experiments. Dataset statistics are shown in Table 1. 2.2 3.1 3 Word Generator Our word generator is also a Transformer decoder. The regular Transformer decoder consists of layers l with self-attention, cross-attention and feedforward sublayers. They are defined as follows: sl = LN(hl−1 + SelfAtt(hl−1 )) l l l (3) hl = LN(cl + FFN(cl )) (4) Datasets CNN/DailyMail. The CNN/DailyMail corpus was initially introduced as a question answering dataset in Hermann et al. (2015) and adapted for summarization by Nallapati et al. (2016), and has been widely used. The corpus’s summaries are a concatenation of bullet points describing the highlights of the news article. They are therefore designed to be concise, but do not necessarily form a fluent summary. Extractive approaches perform well on CNN/DailyMail (Liu and Lapata, 2019). (2) c = LN(s + CrossAtt(s , renc )) Experimental Setup where LN is layer normalization (Ba et al., 2016), SelfAtt stands for self-attention, CrossAtt is the cross-attention to the encoder outputs renc , and FFN is the feed-forward sublayer consisting of two fully-connected layers with an intermediat"
2021.newsum-1.1,P19-1504,0,0.0174818,"ts optimal use for future work. 5 2016) to encode the context of previous sentences, which is used to inform the translation of the next word. In contrast to these methods, we employ hierarchy on the decoder side, and generate a sentence representation for the next sentence. 5.2 Tan et al. (2017) use word- and sentence-level RNNs in both encoder and decoder. They also predict a next sentence embedding, but use a graph model as importance for the encoded sentences instead of attention. The word-level decoder RNN is conditioned by initializing the first hidden state with the sentence embedding. Perez-Beltrachini et al. (2019) use a CNN word encoder/decoder and an LSTM sentence decoder for multi-document summarization. They predict a next sentence embedding with attention, which they add to the input of each convolutional decoder layer. An auxiliary loss pushes sentence embeddings to be close to LDA topics of summary sentences. Both models do not employ Transformers, and consequently their conditioning is very different from ours. Several papers have investigated sentence-level language modeling. Ippolito et al. (2020) pick the most likely continuation from a set of candidate sentences. Their task provides a contex"
2021.newsum-1.1,D15-1044,0,0.0538163,"marization has improved drastically in recent years due to more efficient decoder architectures, like the Transformer (Vaswani et al., 2017), and language model pretraining, such as BERT (Devlin et al., 2019). As a result of these advances, current state-of-the-art models reach the performance of extractive systems, and even surpass them on some datasets (Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2020). Part of this success, however, is due to the development of stronger copy mechanisms such as the pointer-generator network (See et al., 2017) or attention to the source document (Rush et al., 2015). The so-generated summaries copy long sequences from the input document, strung together with filler words. While this achieves better results in the predominant evaluation metric ROUGE (Lin, 2004), it comes at the cost of the summaries’ abstractiveness and coherence, two qualities that we expect from human-written summaries. 1 Our code is available at https://github.com/ idiap/sentence-planner. 1 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 1–14 November 10, 2021. ©2021 Association for Computational Linguistics Document Document Input Encoder Input Encoder (shar"
2021.newsum-1.1,P17-1099,0,0.0447961,"h extensive evaluations. 1 Introduction Abstractive summarization has improved drastically in recent years due to more efficient decoder architectures, like the Transformer (Vaswani et al., 2017), and language model pretraining, such as BERT (Devlin et al., 2019). As a result of these advances, current state-of-the-art models reach the performance of extractive systems, and even surpass them on some datasets (Liu and Lapata, 2019; Lewis et al., 2019; Zhang et al., 2020). Part of this success, however, is due to the development of stronger copy mechanisms such as the pointer-generator network (See et al., 2017) or attention to the source document (Rush et al., 2015). The so-generated summaries copy long sequences from the input document, strung together with filler words. While this achieves better results in the predominant evaluation metric ROUGE (Lin, 2004), it comes at the cost of the summaries’ abstractiveness and coherence, two qualities that we expect from human-written summaries. 1 Our code is available at https://github.com/ idiap/sentence-planner. 1 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 1–14 November 10, 2021. ©2021 Association for Computational Linguis"
2021.newsum-1.1,W19-2303,0,0.0136879,"The case for the attribution to the cross-attention output is analogous. We report the relative attribution to rsent in Table 3. The result is averaged over the first 100 examples in our 6 The mean number of sentences and (to a lesser extent) their average length can be influenced by a length penalty hyperparameter α, which is set between 0.6 and 1 (Liu and Lapata, 2019). B ERT S UM E XTA BS with no penalty (α = 1) produces the same number of sentences and words as the sentence planner with the largest penalty (α = 0.6), but a large gap in ROUGE-(1/2/L) remains: (0.7/0.6/0.6). Consistent with Sun et al. (2019), we find that ROUGE scores increase with length and α, but we also find that novel bigrams decrease. In order to not favor one side of the trade-off over the other, we stick with the setting of α = 0.95 from Liu and Lapata (2019) for both models. 5 Model BSEA (our implementation) + Sentence generator + LMSE (= Sentence planner) ROUGE R-1 R-2 R-L 43.37 (0.37) 43.97 (0.30) 44.40 (0.14) 17.92 (0.17) 18.28 (0.11) 18.31 (0.13) 37.73 (0.31) 38.32 (0.22) 38.69 (0.10) Table 4: Ablation study showing ROUGE scores on Curation Corpus when adding the individual components of our model. Mean and std (in b"
2021.newsum-1.1,P17-1108,0,0.0557105,"Missing"
2021.tacl-1.8,N07-1049,0,0.0760987,"nd Titov, 2017), machine translation (Chen et al., 2017), relation extraction (Zhang et al., 2018), and natural language interfaces (Pang et al., 2019). There are several approaches to compute the dependency tree. Transition-based parsers predict the dependency graph one edge at a time through a sequence of parsing actions (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Titov and Henderson, 2007; Zhang and Nivre, 2011). As in our approach, transformationbased (Satta and Brill, 1996) and corrective modeling parsers use various methods (e.g., Knight and Graehl, 2005; Hall and Nov´ak, 2005; Attardi and Ciaramita, 2007; Hennig and K¨ohn, 2017; Zheng, 2017) to correct an initial parse. We take a graph-based approach to this correction. Graph-based parsers (Eisner, 1996; McDonald et al., 2005a; Koo and Collins, 2010) compute scores for every possible dependency edge and then apply a decoding algorithm to find the highest scoring total tree. Typically, neural graph-based models consist of two components: an encoder that learns context-dependent vector representations for the nodes of the dependency graph, and a decoder that computes the dependency scores for each pair of nodes and then applies a decoding algor"
2021.tacl-1.8,D19-1435,0,0.0232301,"eness of G2GTr for transitionbased dependency parsing and its compatibility with pre-trained BERT (Devlin et al., 2019). This parsing model predicts one edge of the parse graph at a time, conditioning on the graph of previous edges, so it is an autoregressive model. The G2GTr architecture could be used to predict all the edges of a graph in parallel, but such predictions are non-autoregressive. They thus cannot fully model the interactions between edges. For sequence prediction, this problem has been addressed with non-autoregressive iterative refinement (Novak et al., 2016; Lee et al., 2018; Awasthi et al., 2019; Lichtarge et al., 2018). Interactions between different positions in the string are modeled by conditioning on a previous version of the same string. In this paper, we propose a new graph prediction architecture that takes advantage of the full graphto-graph functionality of G2GTr to apply a G2GTr model to refine the output graph recursively. This architecture predicts all edges of the graph in parallel, and is therefore non-autoregressive, but can still capture any between-edge dependency by conditioning on the previous version of the graph, like an auto-regressive model. This proposed Recu"
2021.tacl-1.8,I17-1007,0,0.041398,"Missing"
2021.tacl-1.8,P18-1130,0,0.118432,"Missing"
2021.tacl-1.8,J93-2004,0,0.0810124,"efinement results in improvements, even in the challenging case of state-of-the-art dependency parsers. The evaluation demonstrates improvements with several initial parsers, including previous state-of-the-art dependency parsers, and the empty parse. We also introduce a strong Transformerbased dependency parser pre-trained with BERT (Devlin et al., 2019), called Syntactic Transformer (SynTr), using it both for our initial parser and as the basis of our refinement model. Results on 13 languages from the Universal Dependencies Treebanks (Nivre et al., 2018), English and Chinese Penn Treebanks (Marcus et al., 1993; Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009) show significant improvements over all initial parsers and the stateof-the-art.1 In this paper, we make the following contributions: • We propose a novel architecture for the iterative refinement of arbitrary graphs (RNGTr) that combines non-autoregressive edge prediction with conditioning on the complete graph. • We propose a RNGTr model of syntactic dependency parsing. • We demonstrate significant improvements over the previous state-of-the-art dependency parsing results on Universal Dependency Treebanks, Penn Treeba"
2021.tacl-1.8,H05-1066,0,0.430645,"Missing"
2021.tacl-1.8,C12-2077,0,0.0139215,"scores for each pair of nodes and then applies a decoding algorithm to find the highestscoring dependency tree. There are several approaches to capture correlations between dependency edges in graph-based models. In first-order models, such as Maximum Spanning Tree (MST) (Edmonds, 1967; Chu and Liu, 1965; McDonald et al., 2005b), the score for an edge must be computed without being sure what other edges the model will choose. The model itself only imposes the discrete tree constraint between edges. Higher-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang and McDonald, 2012; Tchernowitz et al., 2016) keep some between-edge information, but require more decoding time. In this paper, we apply first-order models, specifically the MST algorithm, and show that it is possible to keep correlations between edges without increasing the time complexity by recursively conditioning each edge score on a previous prediction of the complete dependency graph. 3 RNG Transformer The RNG Transformer architecture is illustrated in Figure 1, in this case, applied to dependency 1 Our implementation is available at: https://github .com/idiap/g2g-transformer."
2021.tacl-1.8,2020.findings-emnlp.294,1,0.700291,"Missing"
2021.tacl-1.8,P96-1034,0,0.0934372,"rsing is a critical component in a variety of natural language understanding tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Chen et al., 2017), relation extraction (Zhang et al., 2018), and natural language interfaces (Pang et al., 2019). There are several approaches to compute the dependency tree. Transition-based parsers predict the dependency graph one edge at a time through a sequence of parsing actions (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Titov and Henderson, 2007; Zhang and Nivre, 2011). As in our approach, transformationbased (Satta and Brill, 1996) and corrective modeling parsers use various methods (e.g., Knight and Graehl, 2005; Hall and Nov´ak, 2005; Attardi and Ciaramita, 2007; Hennig and K¨ohn, 2017; Zheng, 2017) to correct an initial parse. We take a graph-based approach to this correction. Graph-based parsers (Eisner, 1996; McDonald et al., 2005a; Koo and Collins, 2010) compute scores for every possible dependency edge and then apply a decoding algorithm to find the highest scoring total tree. Typically, neural graph-based models consist of two components: an encoder that learns context-dependent vector representations for the no"
2021.tacl-1.8,C04-1010,0,0.125279,"Dependency Treebanks, Penn Treebanks, and the German CoNLL 2009 corpus. 2 Dependency Parsing Syntactic dependency parsing is a critical component in a variety of natural language understanding tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Chen et al., 2017), relation extraction (Zhang et al., 2018), and natural language interfaces (Pang et al., 2019). There are several approaches to compute the dependency tree. Transition-based parsers predict the dependency graph one edge at a time through a sequence of parsing actions (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Titov and Henderson, 2007; Zhang and Nivre, 2011). As in our approach, transformationbased (Satta and Brill, 1996) and corrective modeling parsers use various methods (e.g., Knight and Graehl, 2005; Hall and Nov´ak, 2005; Attardi and Ciaramita, 2007; Hennig and K¨ohn, 2017; Zheng, 2017) to correct an initial parse. We take a graph-based approach to this correction. Graph-based parsers (Eisner, 1996; McDonald et al., 2005a; Koo and Collins, 2010) compute scores for every possible dependency edge and then apply a decoding algorithm to find the highest scoring total tree. Typically, neural grap"
2021.tacl-1.8,N18-2074,0,0.113705,"state-ofthe-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested. 1 Introduction Self-attention models, such as Transformer (Vaswani et al., 2017), have been hugely successful in a wide range of natural language processing (NLP) tasks, especially when combined with language-model pre-training, such as BERT (Devlin et al., 2019). These architectures contain a stack of self-attention layers that can capture long-range dependencies over the input sequence, while still representing its sequential order using absolute position encodings. Alternatively, Shaw et al. (2018) propose to define sequential order with relative position encodings, which are input to the self-attention functions. Recently, Mohammadshahi and Henderson (2020) extended this sequence input method to the input of arbitrary graph relations via the selfattention mechanism, and combined it with an attention-like function for graph relation prediction, resulting in their proposed Graph-to-Graph 120 Transactions of the Association for Computational Linguistics, vol. 9, pp. 120–138, 2021. https://doi.org/10.1162/tacl a 00358 Action Editor: Yue Zhang. Submission batch: 1/2020; Revision batch: 8/20"
2021.tacl-1.8,D16-1068,0,0.0222827,"n applies a decoding algorithm to find the highestscoring dependency tree. There are several approaches to capture correlations between dependency edges in graph-based models. In first-order models, such as Maximum Spanning Tree (MST) (Edmonds, 1967; Chu and Liu, 1965; McDonald et al., 2005b), the score for an edge must be computed without being sure what other edges the model will choose. The model itself only imposes the discrete tree constraint between edges. Higher-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang and McDonald, 2012; Tchernowitz et al., 2016) keep some between-edge information, but require more decoding time. In this paper, we apply first-order models, specifically the MST algorithm, and show that it is possible to keep correlations between edges without increasing the time complexity by recursively conditioning each edge score on a previous prediction of the complete dependency graph. 3 RNG Transformer The RNG Transformer architecture is illustrated in Figure 1, in this case, applied to dependency 1 Our implementation is available at: https://github .com/idiap/g2g-transformer. 121 The RNGTr model can be formalized in terms of an"
2021.tacl-1.8,N18-1202,0,0.0256842,"portant for the results reported in Section 6.2 is in the way BERT token segmentation is handled. When BERT segments a word into subwords, UDify seems only to encode the first segment, whereas SynTr encodes all segments and only decodes with the first segment, as discussed in Section 5.3. Also, UDify decodes with an attention-based mixture of encoder layers, whereas SynTr only uses the last layer. 5.2 Baseline Models For UD Treebanks, we compare to several baseline parsing models. We use the monolingual parser proposed by Kulmizev et al. (2019), which uses BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018) embeddings as additional input features. In addition, we compare to the multilingual multitask models proposed by Kondratyuk and Straka (2019) and Straka (2018). UDify (Kondratyuk and Straka, 2019) is a multilingual multitask model. UDPipe (Straka, 2018) is one of the winners of CoNLL 2018 Shared Task (Zeman et al., 2018). For a fair comparison, we report the scores of UDPipe from Kondratyuk and Straka (2019) using gold segmentation. UDify is on average the best performing of these baseline models, so we use it as one of our initial parsers in the RNGTr model. For Penn Treebanks and the Germa"
2021.tacl-1.8,K18-2001,0,0.023486,"Missing"
2021.tacl-1.8,P16-1218,0,0.0471359,"Missing"
2021.tacl-1.8,D12-1030,0,0.0251153,"each pair of nodes and then applies a decoding algorithm to find the highestscoring dependency tree. There are several approaches to capture correlations between dependency edges in graph-based models. In first-order models, such as Maximum Spanning Tree (MST) (Edmonds, 1967; Chu and Liu, 1965; McDonald et al., 2005b), the score for an edge must be computed without being sure what other edges the model will choose. The model itself only imposes the discrete tree constraint between edges. Higher-order models (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012; Zhang and McDonald, 2012; Tchernowitz et al., 2016) keep some between-edge information, but require more decoding time. In this paper, we apply first-order models, specifically the MST algorithm, and show that it is possible to keep correlations between edges without increasing the time complexity by recursively conditioning each edge score on a previous prediction of the complete dependency graph. 3 RNG Transformer The RNG Transformer architecture is illustrated in Figure 1, in this case, applied to dependency 1 Our implementation is available at: https://github .com/idiap/g2g-transformer. 121 The RNGTr model can be"
2021.tacl-1.8,P15-1032,0,0.0418042,"Missing"
2021.tacl-1.8,P11-2033,0,0.136,"n CoNLL 2009 corpus. 2 Dependency Parsing Syntactic dependency parsing is a critical component in a variety of natural language understanding tasks, such as semantic role labeling (Marcheggiani and Titov, 2017), machine translation (Chen et al., 2017), relation extraction (Zhang et al., 2018), and natural language interfaces (Pang et al., 2019). There are several approaches to compute the dependency tree. Transition-based parsers predict the dependency graph one edge at a time through a sequence of parsing actions (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Titov and Henderson, 2007; Zhang and Nivre, 2011). As in our approach, transformationbased (Satta and Brill, 1996) and corrective modeling parsers use various methods (e.g., Knight and Graehl, 2005; Hall and Nov´ak, 2005; Attardi and Ciaramita, 2007; Hennig and K¨ohn, 2017; Zheng, 2017) to correct an initial parse. We take a graph-based approach to this correction. Graph-based parsers (Eisner, 1996; McDonald et al., 2005a; Koo and Collins, 2010) compute scores for every possible dependency edge and then apply a decoding algorithm to find the highest scoring total tree. Typically, neural graph-based models consist of two components: an encode"
2021.tacl-1.8,D18-1244,0,0.0709545,"Missing"
2021.tacl-1.8,C02-1145,0,0.0581893,"improvements, even in the challenging case of state-of-the-art dependency parsers. The evaluation demonstrates improvements with several initial parsers, including previous state-of-the-art dependency parsers, and the empty parse. We also introduce a strong Transformerbased dependency parser pre-trained with BERT (Devlin et al., 2019), called Syntactic Transformer (SynTr), using it both for our initial parser and as the basis of our refinement model. Results on 13 languages from the Universal Dependencies Treebanks (Nivre et al., 2018), English and Chinese Penn Treebanks (Marcus et al., 1993; Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009) show significant improvements over all initial parsers and the stateof-the-art.1 In this paper, we make the following contributions: • We propose a novel architecture for the iterative refinement of arbitrary graphs (RNGTr) that combines non-autoregressive edge prediction with conditioning on the complete graph. • We propose a RNGTr model of syntactic dependency parsing. • We demonstrate significant improvements over the previous state-of-the-art dependency parsing results on Universal Dependency Treebanks, Penn Treebanks, and the German"
D07-1099,P05-1023,1,0.872299,"Missing"
D07-1099,N03-1014,1,0.923782,"he parse history, the ISBN graphical model can specify conditional dependency edges between latent variables which are arbitrarily far apart in the parse history. The source state of such an edge is determined by the partial parse structure built at the time of the destination state, thereby allowing the conditional dependency edges to be appropriate for the structural nature of the parsing problem. In particular, they allow conditional dependencies to be local in the parse structure, not just local in the history sequence. In this they are similar to the class of neural networks proposed in (Henderson, 2003) for constituent parsing. In fact, in (Titov and Henderson, 2007a) it was shown that this neural network can be viewed as a coarse approximation to the corresponding ISBN model. Traditional statistical parsing models also condition on features which are local in the parse structure, but these features need to be explicitly defined before learning, and require careful feature selection. This is especially difficult for languages unknown to the parser developer, since the number of possible features grows exponentially with the structural distance considered. The ISBN model uses an alternative a"
D07-1099,P04-1013,1,0.915627,"Missing"
D07-1099,W07-2416,0,0.0672376,"Missing"
D07-1099,J93-2004,0,0.0350868,"Missing"
D07-1099,W06-2932,0,0.037245,"N dependency parser in the multilingual shared task setup and achieved competitive accuracy on every language, and the third best average score overall. The proposed model requires minimal design effort because it relies mostly on automatic feature induction, which is highly desirable when using new treebanks or languages. The parsing time needed to achieve high accuracy is also quite small, making this model a good candidate for use in practical applications. The fact that our model defines a probability model over parse trees, unlike the previous stateof-the-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, such as in language processing pipelines or for language modeling. Also, as with any generative model, it should be easy to improve the parser’s accuracy with discriminative reranking, such as discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without the introduction of any additional linguistic features. Acknowledgments This work was funded by Swiss NSF grant 200020109685, UK EPSRC grant EP/E019501/1, and EU FP6 grant 507802 for projec"
D07-1099,P05-1013,0,0.0177838,"al Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch James Henderson University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, United Kingdom james.henderson@ed.ac.uk Abstract CoNLL-X shared task (Buchholz and Marsi, 2006). This parser employs a latent variable model, Incremental Sigmoid Belief Networks (ISBNs), to define a generative history-based model of projective parsing. We used the pseudo-projective transformation introduced in (Nivre and Nilsson, 2005) to cast non-projective parsing tasks as projective. Following (Nivre et al., 2006), the encoding scheme called HEAD in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. In the following sections we will briefly discuss our modifications to the ISBN parser, experimental setup, and achieved results. We use a generative history-based model to predict the most likely derivation of a dependency parse. Our probabilistic model is based on Incremental Sigmoid Belief Networks, a recently proposed class of latent variable models for structure prediction. Their ability to automatically induce features results in multilingual parsing which is robust enough to"
D07-1099,W04-2407,0,0.162111,"(Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) each into two elementary decisions: first the parser creates the corresponding arc, then it assigns a relation r to the arc. Similarly, we decompose the decision to shift a word into a decision to shift and a prediction of the word. We used part-of-speech tags and fine"
D07-1099,W06-2933,0,0.206262,"ble Model Ivan Titov University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch James Henderson University of Edinburgh 2 Buccleuch Place Edinburgh EH8 9LW, United Kingdom james.henderson@ed.ac.uk Abstract CoNLL-X shared task (Buchholz and Marsi, 2006). This parser employs a latent variable model, Incremental Sigmoid Belief Networks (ISBNs), to define a generative history-based model of projective parsing. We used the pseudo-projective transformation introduced in (Nivre and Nilsson, 2005) to cast non-projective parsing tasks as projective. Following (Nivre et al., 2006), the encoding scheme called HEAD in (Nivre and Nilsson, 2005) was used to encode the original non-projective dependencies in the labels of the projectivized dependency tree. In the following sections we will briefly discuss our modifications to the ISBN parser, experimental setup, and achieved results. We use a generative history-based model to predict the most likely derivation of a dependency parse. Our probabilistic model is based on Incremental Sigmoid Belief Networks, a recently proposed class of latent variable models for structure prediction. Their ability to automatically induce featu"
D07-1099,P07-1080,1,0.698229,"e labeled attachment score in the task, despite using no discriminative methods. We also demonstrate that the parser is quite fast, and can provide even faster parsing times without much loss of accuracy. 1 2 Introduction The multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) e"
D07-1099,W07-2218,1,0.787169,"e labeled attachment score in the task, despite using no discriminative methods. We also demonstrate that the parser is quite fast, and can provide even faster parsing times without much loss of accuracy. 1 2 Introduction The multilingual track of the CoNLL-2007 shared task (Nivre et al., 2007) considers dependency parsing of texts written in different languages. It requires use of a single dependency parsing model for the entire set of languages; model parameters are estimated individually for each language on the basis of provided training sets. We use a recently proposed dependency parser (Titov and Henderson, 2007b)1 which has demonstrated state-of-theart performance on a selection of languages from the 1 The ISBN parser will be soon made downloadable from the authors’ web-page. The Probability Model Our probability model uses the parsing order proposed in (Nivre et al., 2004), but instead of performing deterministic parsing as in (Nivre et al., 2004), this ordering is used to define a generative historybased model, by adding word prediction to the Shift parser action. We also decomposed some parser actions into sub-sequences of decisions. We split arc prediction decisions (Left-Arcr and Right-Arcr ) e"
D07-1099,W06-2920,0,\N,Missing
D07-1099,D07-1096,0,\N,Missing
D11-1083,J98-2004,0,0.0506858,"he carry, κ. In HMT it is possible to estimate the cost of n-grams that partially overlap ι’s span considering the boundary words. We can obtain the heuristic cost for an item, ι, adding to formula (3) the factor, est(κ), for the estimation of interaction with missing context: heuristicCost(ι) = cost(ι) + est(κ) (4) And use heuristicCost(ι) to guide BD pruning decisions. Anyway, even if a good interaction estimation is available, in practice it is not possible to avoid search errors while pruning. More sophisticated parsing models allow the use of non-bottom-up features within a BD framework. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string. Corazza et al. (1994), and Klein and Manning (2003) propose an A* parsing algorithm that estimates the upper bound of the parse completion scores using contextual summary features. These models achieve time efficiency and state-of-the-art accuracy for PCFG parsing, but still use a BD framework that doesn’t allow the application of a broader class of non-bottom-up contextual features. In HMT, knowing the sentence-wide context in which a sub-phrase is translated"
D11-1083,J07-2003,0,0.604029,"est it on Hierarchical Machine Translation, a well known tree structure prediction problem. The structure of the proposed system allows the incorporation of non-bottom-up features and relies on a more sophisticated decoding approach. We show that the proposed approach can find better translations using a smaller portion of the search space. 2 Beyond Bottom-up Decoding 1 Introduction Tree Structure Prediction (TSP) techniques have become relevant in many Natural Language Processing (NLP) applications, such as Syntactic Parsing, Semantic Role Labeling and Hierarchical Machine Translation (HMT) (Chiang, 2007). HMT approaches have a higher complexity than PhraseBased Machine Translation techniques, but exploit a more sophisticated reordering model, and can produce translations with higher Syntactic-Semantic quality. TSP requires as inputs: a weighted grammar, G, and a sequence of symbols or a set of sequences encoded as a Lattice (Chappelier et al., 1999). The TSP decoding requires scoring candidate trees, cost(t). Some TSP tasks require only local features. For these cases cost(t) depends only on the local score of the rules that compose t : cost(t) = X cost(ri ) (1) ri ∈t This is the case for Con"
D11-1083,P10-4002,0,0.137565,"estimation of the missing part of the derivation. We compute this factor with the following formula:  X est(˚ ι) = localCost(˚ ι, l) + contextEst(˚ ι, l) − l∈L˚ ι (7) , we estimate the cost For each missing link, l ∈ L˚− ι of the corresponding derivation branch with two factors: localCost(˚ ι, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚ ι, l) that estimates the contextual score of the branch and its interaction with ˚ ι. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚ ι, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), 904 which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚ ι, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) estimates the LM score"
D11-1083,2010.iwslt-papers.8,1,0.855818,"Missing"
D11-1083,D09-1007,0,0.0178172,"ocal fundamental feature that approximates the adequacy of the translation with the sum of logprobabilities of composing n-grams. CKY-like BD approaches build candidate trees in a bottom-up fashion, allowing the use of Dynamic Programming techniques to simplify the search space by mering sub-trees with the same state, and also easing application of pruning techniques (such as Cube Pruning, e.g. Chiang (2007), Gesmundo (2010)). For clarity of presentation and following HMT practice, we will henceforth restrict our focus to binary grammars. Standard CKY works by building objects known as items (Hopkins and Langmead, 2009). Each item, ι, corresponds to a candidate sub-tree. Items are built linking a rule instantiation, r, to two sub-items that represents left context, ι1 , and right context, ι2 ; formally: ι ≡ h ι1 ⋗ r ⋖ ι2 i. An item is a triple that contains a span, a postcondition and a carry. The span contains the indexes of the starting and ending input words delimiting the continuous sequence covered by the sub-tree represented by the item. The postcondition is a string that represents r’s head nonterminal label, telling us which rules may be applied. The carry, κ, stores extra information required to cor"
D11-1083,P07-1019,0,0.1253,"st(˚ ι) accounts for the estimation of the missing part of the derivation. We compute this factor with the following formula:  X est(˚ ι) = localCost(˚ ι, l) + contextEst(˚ ι, l) − l∈L˚ ι (7) , we estimate the cost For each missing link, l ∈ L˚− ι of the corresponding derivation branch with two factors: localCost(˚ ι, l) that computes the context-free score of the branch with highest score that could be attached to l; and contextEst(˚ ι, l) that estimates the contextual score of the branch and its interaction with ˚ ι. Because our model is implemented in the Forest Rescoring framework (e.g. Huang and Chiang (2007), Dyer et al. (2010), Li et al. (2009)), localCost(˚ ι, l) can be efficiently computed exactly. In HMT it is possible to exhaustively represent and search the context-free-forest (ignoring the LM), 904 which is done in the Forest Rescoring framework before our task of decoding with the LM. We exploit this context-free-forest to compute localCost(˚ ι, l): for missing child links the localCost(·) is the Inside score computed using the (max, +) semiring (also known as the Viterbi score), and for missing parent links the localCost(·) is the corresponding Outside score. The factor contextEst(·) est"
D11-1083,W01-1812,0,0.22419,"rch for Non-Bottom-Up Tree Structure Prediction Andrea Gesmundo Department of Computer Science University of Geneva andrea.gesmundo@unige.ch Abstract James Henderson Department of Computer Science University of Geneva james.henderson@unige.ch input sequence is often a sentence for NLP applications. Tree structures generating the input sequence can be composed using rules, r, from the weighted grammar, G. TSP techniques return as output a tree structure or a set of trees (forest) that generate the input string or lattice. The output forest can be represented compactly as a weighted hypergraph (Klein and Manning, 2001). TSP tasks require finding the tree, t, with the highest score, or the best-k such trees. Mainstream TSP relies on Bottom-up Decoding (BD) techniques. With this paper we propose a new framework as a generalization of the CKY-like Bottom-up approach. We also design and test an instantiation of this framework, empirically showing that wider contextual information leads to higher accuracy for TSP tasks that rely on non-local features, like HMT. State of the art Tree Structures Prediction techniques rely on bottom-up decoding. These approaches allow the use of context-free features and bottom-up"
D11-1083,N03-1016,0,0.0146566,"rmula (3) the factor, est(κ), for the estimation of interaction with missing context: heuristicCost(ι) = cost(ι) + est(κ) (4) And use heuristicCost(ι) to guide BD pruning decisions. Anyway, even if a good interaction estimation is available, in practice it is not possible to avoid search errors while pruning. More sophisticated parsing models allow the use of non-bottom-up features within a BD framework. Caraballo and Charniak (1998) present best-first parsing with Figures of Merit that allows conditioning of the heuristic function on statistics of the input string. Corazza et al. (1994), and Klein and Manning (2003) propose an A* parsing algorithm that estimates the upper bound of the parse completion scores using contextual summary features. These models achieve time efficiency and state-of-the-art accuracy for PCFG parsing, but still use a BD framework that doesn’t allow the application of a broader class of non-bottom-up contextual features. In HMT, knowing the sentence-wide context in which a sub-phrase is translated is extremely important. It is obviously important for word choice: as a simple example consider the translation of the frequent English word “get” into Chinese. The choice of the correct"
D11-1083,P09-1019,0,0.123535,"Missing"
D11-1083,W09-0424,0,0.0618392,"Missing"
D11-1083,D07-1104,0,0.186471,"Missing"
D11-1083,P08-1023,0,0.0526792,"Missing"
D11-1083,P07-1096,0,0.249633,"Natural Language Processing, pages 899–908, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics (2) For example, in HMT the Language Model (LM) is a non-local fundamental feature that approximates the adequacy of the translation with the sum of logprobabilities of composing n-grams. CKY-like BD approaches build candidate trees in a bottom-up fashion, allowing the use of Dynamic Programming techniques to simplify the search space by mering sub-trees with the same state, and also easing application of pruning techniques (such as Cube Pruning, e.g. Chiang (2007), Gesmundo (2010)). For clarity of presentation and following HMT practice, we will henceforth restrict our focus to binary grammars. Standard CKY works by building objects known as items (Hopkins and Langmead, 2009). Each item, ι, corresponds to a candidate sub-tree. Items are built linking a rule instantiation, r, to two sub-items that represents left context, ι1 , and right context, ι2 ; formally: ι ≡ h ι1 ⋗ r ⋖ ι2 i. An item is a triple that contains a span, a postcondition and a carry. The span contains the indexes of the starting and ending input words delimiting the continuous sequence"
D11-1083,W06-3119,0,0.0235254,"Missing"
D15-1027,P14-1023,0,0.074385,"ve to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for"
D15-1027,D13-1170,0,0.00157363,"entation of the utterance and the representation of the label from their constituent words, then we check if these representations match or not. In the following we explain in details this representation learning model. 3.1 Utterance Representation Learning In this section we explain how to build the utterance representation from its constituent words. In addition to words, we use bigrams, since they have been shown previously to be effective features for this task (Henderson et al., 2012). Following the success in transfer learning from parsing to understanding tasks (Henderson et al., 2013; Socher et al., 2013), we use dependency parse bigrams in our features as well. We learn to build a local representation at each word position in the utterance by using the word representation, adjacent word representations, and the head word representation. Let φ(w) be a d dimensional vector representing the word w, and φ(Ui ) be a h dimensional vector which is the local representation at word position i. We compute the local representation as follows: Figure 1: The multi-label classifier building the classifier, each label’s hyperplane is independent of other labels. To extend this model to a zero-shot learning"
D15-1027,J13-4006,1,0.645854,"ach, we build the representation of the utterance and the representation of the label from their constituent words, then we check if these representations match or not. In the following we explain in details this representation learning model. 3.1 Utterance Representation Learning In this section we explain how to build the utterance representation from its constituent words. In addition to words, we use bigrams, since they have been shown previously to be effective features for this task (Henderson et al., 2012). Following the success in transfer learning from parsing to understanding tasks (Henderson et al., 2013; Socher et al., 2013), we use dependency parse bigrams in our features as well. We learn to build a local representation at each word position in the utterance by using the word representation, adjacent word representations, and the head word representation. Let φ(w) be a d dimensional vector representing the word w, and φ(Ui ) be a h dimensional vector which is the local representation at word position i. We compute the local representation as follows: Figure 1: The multi-label classifier building the classifier, each label’s hyperplane is independent of other labels. To extend this model to"
D15-1027,P12-1092,0,0.012951,"sually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize to unseen words and labels. For every utterance we learn how to compose"
D15-1027,P06-1115,0,0.14049,"ssociated attributes and values. For example, the utterance ”I would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings traine"
D15-1027,N13-1090,0,0.0126784,"e data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because word embeddings trained on unlabeled data reflect the similarity between words, they help the model generalize from the words in the original training corpora to the words in the new extended domain, and help generalize from small amounts of data in the extended domain. The contribution of this paper is to build a representation learning classifier for the SLU task that can generalize to unseen words and labels. For every utterance we learn how to compose the word vectors to f"
D15-1027,N04-1030,0,0.0506137,"companied with their associated attributes and values. For example, the utterance ”I would like Chinese food” is labelled with inform(food=Chinese), in which inform is the dialogue action that provides the value of the attribute food that is Chinese. Dialogue systems often use hand-crafted grammars for SLU, such as Phoenix (Ward, 1994), which are expensive to develop, and expensive to extend or adapt to new attributes and values. Statistical SLU models are usually trained on the data obtained from a specific domain and location, using a structured output classifier that can be discriminative (Pradhan et al., 2004; Kate and Mooney, 2006; Henderson et al., 2012) or generative (Schwartz et al., 1996; He and Young, 2005). 244 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 244–249, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. of words in various NLP tasks (Baroni et al., 2014; Yazdani and Popescu-Belis, 2013; Collobert et al., 2011; Collobert and Weston, 2008; Huang et al., 2012; Mikolov et al., 2013b). In addition to parameter sharing, these representations enable us to leverage large scale unlabelled data. Because"
D15-1058,D07-1074,0,0.0761099,"t of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Document-level KB tags We incorporate information from KB tags by building document-sp"
D15-1058,D11-1072,0,0.184778,"Missing"
D15-1058,D07-1073,0,0.662482,"ow how KB tags can be exploited as a useful complement to traditional NER supervision. 2 Background Gazetteers have long been used to augment statistical NER models, adding general evidence of tokens used in names (Nadeau and Sekine, 2007). These are usually drawn from wide-coverage sources like Wikipedia and census lists (Ratinov and Roth, 2009) and can be incorporated into sequence models by designing binary features that indicate whether a token appears in a gazetteer entry. Features can be refined by specifying which part of an entry a token matches using tag encoding schemes such as IOB (Kazama and Torisawa, 2007). Using multiple gazetteers allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has"
D15-1058,W03-1301,0,0.0545499,"allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case o"
D15-1058,P11-1082,0,0.0209573,"technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Docum"
D15-1058,W09-1119,0,0.132725,"tional Linguistics. degrades as we use fewer KB tags, simulating the use-case where a busy knowledge worker spends less time annotating. We find that KB augmentation means we require fewer tags to reach the same performance, which reduces the cost of obtaining KB tags. We show how KB tags can be exploited as a useful complement to traditional NER supervision. 2 Background Gazetteers have long been used to augment statistical NER models, adding general evidence of tokens used in names (Nadeau and Sekine, 2007). These are usually drawn from wide-coverage sources like Wikipedia and census lists (Ratinov and Roth, 2009) and can be incorporated into sequence models by designing binary features that indicate whether a token appears in a gazetteer entry. Features can be refined by specifying which part of an entry a token matches using tag encoding schemes such as IOB (Kazama and Torisawa, 2007). Using multiple gazetteers allows feature weights to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazette"
D15-1058,P11-1138,0,0.0205177,", heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create user-specific gazetteers of personal information. The background NER model is initially trained Figure 1: An entity-tagged document, KB tags with canonical name and type and KB with aliases. without access to the user-specific information and later adapted on the users’s smartphone. 3 Document-level KB tags We incorporate information from KB tags by building document-specific gazetteers. Figu"
D15-1058,P08-1001,0,0.0644965,"ncrease coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. Jung et al. (2015) query a user’s smartphone data services to create"
D15-1058,W06-3328,0,0.0314222,"s to capture different name types and sources. Given their purpose to increase coverage beyond names included in training data, gazetteers are usually large, general and static, remaining the same during training and prediction time. Beyond their use as sources for gazetteers, the link structure in and around KBs has been used to create training data. A prominent technique is to follow links back from KB articles to documents that mention the subject of the article, heuristically labelling high-precision matches to create training data. This has been used for genetic KBs (Morgan et al., 2003; Vlachos and Gasperin, 2006), and Wikipedia (Kazama and Torisawa, 2007; Richman and Schone, 2008; Nothman et al., 2013). These works do not consider our setting where gold-standard entities are given at inference time as their goal is to generate training data. KB s have also been used to help other natural language processing tasks such as coreference resolution (Rahman and Ng, 2011), topic modelling (Kataria et al., 2011) and named entity linking (Cucerzan, 2007; Ratinov et al., 2011). Finally, it may be that supervised data is only available in some circumstances, for example in the case of personalising NER models. J"
D15-1058,W03-0419,0,\N,Missing
D15-1201,W03-1812,0,0.10571,"Missing"
D15-1201,D10-1115,0,0.298376,"onstructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative functions. In this work, we too assume that composition is arguably a complex function. We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to ov"
D15-1201,P14-1023,0,0.0110436,"p to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs. 4 Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic compositio"
D15-1201,W07-1106,0,0.10957,"Missing"
D15-1201,W15-0904,1,0.318328,"ike detection algorithm based on hidden compositionality annotations. The best composition is the one that is the best fit on all the data points except the noncompositional ones. Since we don’t use annotated data at training time, we assume annotations to be hidden variables and iteratively alternate between optimizing the composition function and optimizing the hidden compositionality annotations. We show that this iterative algorithm increases the accuracy of non-compositionality detection compared to the case when training is done on all examples. We run our experiments on the data set of Farahmand et al. (2015) who provide a set of English NCs which are annotated with noncompositionality judgments. We show that quadratic regression significantly outperforms additive and multiplicative baselines and all other models in modelling semantic composition and identifying the non-compositional NCs. In short, the contributions of our work are: to empirically evaluate various composition functions ranging from simple to overly complex in order to find the most accurate function; to propose, to the best of our knowledge for the first time, a method of identifying non-compositional phrases as phrases for which"
D15-1201,P12-1092,0,0.0273841,"s of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014). 1735 3 Representation of Words and Compounds In order to represent words and compounds we use word embeddings, which are a form of vector space models. Vector space models represent the semantics of words and phrases with real valued vectors. Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c). They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015). In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds. We chose an English Wikipedia dump as our corpus. After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50. We learn the embeddings of these compounds as single tokens usi"
D15-1201,W06-1203,0,0.14682,"Missing"
D15-1201,D13-1147,0,0.685717,"Missing"
D15-1201,P99-1041,0,0.0356361,"s are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999). It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by conventional statistical measures such as mutual information. Detecting non-compositionality in an automatic fashion has been the aim of much previous research. In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned. This implies that the compounds for which"
D15-1201,W03-1810,0,0.365606,"vels. In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002). Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance. Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999). It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by conventional statistical measures such as mutual information. Detecting non-compositionality in an automatic fashion has been the aim of much previous research. In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned. This i"
D15-1201,D13-1170,0,0.0096119,"ny polynomial beyond quadratic transformation without overfitting. The case of a quadratic ψ transformation is: ψ(x) = x21 , · · · x2n , x1 x2 , · · · xn−1 xn , x1 , · · · xn {z } |{z } |{z } | Pure quadratic interaction terms linear terms Similar to the linear case we can have sparse version of the polynomial regression in which we allow the presence of only a few non-zero elements in the θ matrix. The sparsity regularizer is more important in the case of polynomial regression as we have many more parameters. The quadratic model is similar to Recursive Neural Tensor compositionality model of Socher et al. (2013). But in our model the tensor is symmetric around the diagonal. Figure 2 shows the pure quadratic transformation matrices. 1737 4.3 Model Additive model (Salehi et al., 2015); (Reddy et al., 2011) Multiplicative model (Reddy et al., 2011) Sparse Linear Linear Sparse Pure Quad. Pure Quad. Sparse Interaction Interaction Quadratic Sparse NN (H=1000) NN (H=1000) Neural Networks A feed forward neural network is a universal approximator (Cybenko, 1989): feed-forward network with a single hidden layer can approximate any continuous function, provided it has enough hidden units. Therefore we use neura"
D15-1201,D07-1039,0,0.623823,"Missing"
D15-1201,P98-2210,0,0.154801,"Missing"
D15-1201,H05-1113,0,0.130205,"Missing"
D15-1201,N13-1090,0,0.0529312,"models mentioned so far are based on conventional4 or count based vector space representation of the words. More recent works however are based on representation learning of word embeddings. Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition. They learn the adjective matrices by linear regression. In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters. Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole. Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs. They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of"
D15-1201,P08-1028,0,0.474147,"utional semantics, semantic composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, 2 MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significan"
D15-1201,I11-1024,0,0.871915,"composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication, 2 MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform addit"
D15-1201,N15-1099,0,0.403716,"n commonly assumed to be a trivial predetermined function such as addition, multiplication, 2 MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs. 1733 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative"
D15-1201,D12-1110,0,0.530903,"uage Processing, pages 1733–1742, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015). Nevertheless there is some work that regards composition as a more complex function. For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network. The two latter works show that complex composition models significantly outperform additive and multiplicative functions. In this work, we too assume that composition is arguably a complex function. We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to overfitting, can not completely capture semantic composition. Nevertheless modelling composition by means of a powerful function can be"
D15-1201,C98-2205,0,\N,Missing
D18-1325,W17-4803,0,0.0221884,"unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We i"
D18-1325,2012.eamt-1.60,0,0.17861,"oder alignment). Finally, we combine complementary target-source sides of the context by joining HAN encoder and HAN decoder. Figure 1 shows the integration of the HAN encoder with the NMT model; a similar architecture is ap˜ t is used by the plied to the decoder. The output h NMT model as replacement of ht during the final classification layer. 3 3.1 Experimental Setup Datasets and Evaluation Metrics We carry out experiments with Chinese-to-English (Zh-En) and Spanish-to-English (Es-En) sets on three different domains: talks, subtitles, and news. TED Talks is part of the IWSLT 2014 and 2015 (Cettolo et al., 2012, 2015) evaluation campaigns1 . We use dev2010 for development; and tst20102012 (Es-En), tst2010-2013 (Zh-En) for testing. The Zh-En subtitles corpus is a compilation of TV subtitles designed for research on context (Wang et al., 2018). In contrast to the other sets, it has three references to compare. The Es-En corpus is a subset of OpenSubtitles2018 (Lison and Tiedemann, 2016)2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries113 corpus which has document-level delimitation. We evaluate on WMT sets (Bojar et al., 2013): newstest20"
D18-1325,N13-1074,0,0.0267637,"Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from previous sentence translation"
D18-1325,P17-4012,0,0.0555708,". A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007)). 3.2 Model Configuration and Training As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by Tu et al. (2018), with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called “base model” in the 1 https://wit3.fbk.eu 2 http://www.opensubtitles.org 3 http://opus.nlpl.eu/News-Commentary11.php original paper (Vaswani et al., 2017). The encoder and decoder are composed of 6 hidden layers each. All hidden states have dimension of 512, dropout of 0.1, and 8 heads for the multi-head attention. The target and source vocabulary size is 30K. The optimization and regularization methods were the same as proposed by Vaswani et al. (2017). Inspired by Tu et al. (2018) we trained the mod"
D18-1325,W04-3250,0,0.103913,"a subset of OpenSubtitles2018 (Lison and Tiedemann, 2016)2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries113 corpus which has document-level delimitation. We evaluate on WMT sets (Bojar et al., 2013): newstest2008 for development, and newstest2009-2013 for testing. A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007)). 3.2 Model Configuration and Training As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by Tu et al. (2018), with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called “base model” in the 1 https://wit3.fbk.eu 2 http://www.opensubtitles.org 3 http://opus.nlpl.eu/News-Commentary11.php original paper (Vaswani et al., 2017). The encoder an"
D18-1325,P07-2045,0,0.016144,"son and Tiedemann, 2016)2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries113 corpus which has document-level delimitation. We evaluate on WMT sets (Bojar et al., 2013): newstest2008 for development, and newstest2009-2013 for testing. A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007)). 3.2 Model Configuration and Training As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by Tu et al. (2018), with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called “base model” in the 1 https://wit3.fbk.eu 2 http://www.opensubtitles.org 3 http://opus.nlpl.eu/News-Commentary11.php original paper (Vaswani et al., 2017). The encoder and decoder are composed of 6 hidden layer"
D18-1325,W17-4810,0,0.0345514,"nsidering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder. More recently, Tu et al. (2018) have shown that a cache-based memory network performs better than the above encoder-based methods. The cache-based memory keeps past context as a set of words, where each cell corresponds to one unique word keeping the hidden representations learned by the NMT while translating it."
D18-1325,L16-1147,0,0.0490912,"tasets and Evaluation Metrics We carry out experiments with Chinese-to-English (Zh-En) and Spanish-to-English (Es-En) sets on three different domains: talks, subtitles, and news. TED Talks is part of the IWSLT 2014 and 2015 (Cettolo et al., 2012, 2015) evaluation campaigns1 . We use dev2010 for development; and tst20102012 (Es-En), tst2010-2013 (Zh-En) for testing. The Zh-En subtitles corpus is a compilation of TV subtitles designed for research on context (Wang et al., 2018). In contrast to the other sets, it has three references to compare. The Es-En corpus is a subset of OpenSubtitles2018 (Lison and Tiedemann, 2016)2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries113 corpus which has document-level delimitation. We evaluate on WMT sets (Bojar et al., 2013): newstest2008 for development, and newstest2009-2013 for testing. A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007)). 3."
D18-1325,D11-1084,0,0.236471,"e NMT improves the of anaphoric pronouns, and Maruf and Haffari (2018) proposed a document-level NMT using memory-networks. y esto es un escape de su estado atormentado . and that is an escape from his tormented state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et a"
D18-1325,P18-1118,0,0.159296,"the number of content words. Coherence: Average cosine similarity of consecutive sentences (i.e. average of LSA word-vectors) Currently Translated Sentence as NMT’s input/output, Jean et al. (2017) add a context encoder for the previous source sentence, Wang et al. (2017) includes a HRNN to summarize source-side context, and Tu et al. (2018) use a dynamic cache memory to store representations of previously translated words. Recently, Bawden et al. (2018) proposed test-sets for evaluating discourse in NMT, Voita et al. (2018) shows that context-aware NMT improves the of anaphoric pronouns, and Maruf and Haffari (2018) proposed a document-level NMT using memory-networks. y esto es un escape de su estado atormentado . and that is an escape from his tormented state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discou"
D18-1325,W17-4813,0,0.0263918,"this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture i"
D18-1325,W12-0117,0,0.0365623,"to es un escape de su estado atormentado . and that is an escape from his tormented state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sen"
D18-1325,W13-3303,0,0.0601112,"th the encoder and decoder benefit from context in complementary ways. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does no"
D18-1325,N18-1124,1,0.723579,"mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from previous sentence translations into the current sentence translation. The model significantly outperforms two competitive baselines, and the ablation study shows that target and source context is complementary. It also improves lexical cohesion"
D18-1325,W17-1505,0,0.0407983,"th HAN encoder further improves translation performance, showing that they contribute complementary information. The three ways of incorporating information into the decoder all perform similarly. Table 3 shows the performance of our best HAN model with a varying number k of previous sentences in the test-set. We can see that the best performance for TED talks and news is archived with 3, while for subtitles it is similar between 3 and 7. 4.2 Accuracy of Pronoun/Noun Translations We evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Miculicich Werlen and Popescu-Belis, 2017b), which can be extended for nouns. The list of evaluated pronouns is predefined in the metric, while the list of nouns was extracted using NLTK POS tagging (Bird, 2006). The upper part 2949 TED Talks Subtitles News Zh–En Es–En Zh–En 4 Es–En Es–En Models BLEU ∆ BLEU ∆ BLEU ∆ BLEU ∆ BLEU ∆ NMT transformer 16.87 35.44 28.60 35.20 21.36 + cache (Tu et al., 2018) 17.32 (+0.45)∗∗∗ 36.46 (+1.02)∗∗∗ 28.86 (+0.26) 35.49 (+0.29) 22.36 (+1.00)∗∗∗ ∗∗∗ ∗ ∗ ∗∗∗ + HAN encoder 17.61 (+0.74)∗∗∗ †† 36.91 (+1.47)†† 29.35 (+0.75)† 35.96 (+0.76)† 22.36 (+1.00) ∗∗∗ ∗∗∗ ∗ ∗∗∗ + HAN decoder 17.39 (+0.52) 37.01 (+1."
D18-1325,W17-4802,0,0.230519,"th HAN encoder further improves translation performance, showing that they contribute complementary information. The three ways of incorporating information into the decoder all perform similarly. Table 3 shows the performance of our best HAN model with a varying number k of previous sentences in the test-set. We can see that the best performance for TED talks and news is archived with 3, while for subtitles it is similar between 3 and 7. 4.2 Accuracy of Pronoun/Noun Translations We evaluate coreference and anaphora using the reference-based metric: accuracy of pronoun translation (Miculicich Werlen and Popescu-Belis, 2017b), which can be extended for nouns. The list of evaluated pronouns is predefined in the metric, while the list of nouns was extracted using NLTK POS tagging (Bird, 2006). The upper part 2949 TED Talks Subtitles News Zh–En Es–En Zh–En 4 Es–En Es–En Models BLEU ∆ BLEU ∆ BLEU ∆ BLEU ∆ BLEU ∆ NMT transformer 16.87 35.44 28.60 35.20 21.36 + cache (Tu et al., 2018) 17.32 (+0.45)∗∗∗ 36.46 (+1.02)∗∗∗ 28.86 (+0.26) 35.49 (+0.29) 22.36 (+1.00)∗∗∗ ∗∗∗ ∗ ∗ ∗∗∗ + HAN encoder 17.61 (+0.74)∗∗∗ †† 36.91 (+1.47)†† 29.35 (+0.75)† 35.96 (+0.76)† 22.36 (+1.00) ∗∗∗ ∗∗∗ ∗ ∗∗∗ + HAN decoder 17.39 (+0.52) 37.01 (+1."
D18-1325,P02-1040,0,0.102464,"signed for research on context (Wang et al., 2018). In contrast to the other sets, it has three references to compare. The Es-En corpus is a subset of OpenSubtitles2018 (Lison and Tiedemann, 2016)2 . We randomly select two episodes for development and testing each. Finally, we use the Es-En News-Commentaries113 corpus which has document-level delimitation. We evaluate on WMT sets (Bojar et al., 2013): newstest2008 for development, and newstest2009-2013 for testing. A similar corpus for Zh-En is too small to be comparable. Table 2 shows the corpus statistics. For evaluation, we use BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text, and we measure significance with the paired bootstrap resampling method proposed by Koehn (2004) (implementations by Koehn et al. (2007)). 3.2 Model Configuration and Training As baselines, we use a NMT transformer, and a context-aware NMT transformer with cache memory which we implemented for comparison following the best model described by Tu et al. (2018), with memory size of 25 words. We used the OpenNMT (Klein et al., 2017) implementation of the transformer network. The configuration is the same as the model called “base model” in the 1 https://wit3.fbk.eu"
D18-1325,I17-1102,1,0.889006,"Missing"
D18-1325,E17-1089,0,0.0117226,"nted state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT m"
D18-1325,E17-2104,0,0.0277144,"heir state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by d"
D18-1325,W17-4814,0,0.32027,"enefit from context in complementary ways. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the repr"
D18-1325,stefanescu-etal-2014-latent,0,0.0430598,"n 4.2, repetitions do not always make the translation better. Although HAN boosts lexical cohesion, the scores are still far from the human reference, so there is room for improvement in this aspect. For coherence, we use a metric based on Latent Semantic Analysis (LSA) (Foltz et al., 1998). LSA is used to obtain sentence representations, then cosine similarity is calculated from one sentence to Table 3: Performance for variable context sizes k with the HAN encoder + HAN decoder. the next, and the results are averaged to get a document score. We employed the pre-trained LSA model Wiki-6 from (Stefanescu et al., 2014). Table 4 (bottom-right) shows the average coherence score of documents. The joint HAN model consistently obtains the best coherence score, but close to other HAN models. Most of the improvement comes from the HAN decoder. 4.4 Qualitative Analysis Table 5 shows an example where HAN helped to generate the correct translation. The first box shows the current sentence with the analyzed word in bold; and the second, the past context at source and target. For the context visualization we use the toolkit provided by Pappas and Popescu-Belis (2017). Red corresponds to sentences, and blue to words. Th"
D18-1325,W10-2602,0,0.147299,"Missing"
D18-1325,W17-4811,0,0.173432,"entary ways. 1 Introduction Neural machine translation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder"
D18-1325,Q17-1007,0,0.0419634,"Missing"
D18-1325,Q18-1029,0,0.272977,"au et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder. More recently, Tu et al. (2018) have shown that a cach"
D18-1325,P18-1117,0,0.113602,"a human reference. Lexical cohesion: Ratio of repeated and lexically similar words over the number of content words. Coherence: Average cosine similarity of consecutive sentences (i.e. average of LSA word-vectors) Currently Translated Sentence as NMT’s input/output, Jean et al. (2017) add a context encoder for the previous source sentence, Wang et al. (2017) includes a HRNN to summarize source-side context, and Tu et al. (2018) use a dynamic cache memory to store representations of previously translated words. Recently, Bawden et al. (2018) proposed test-sets for evaluating discourse in NMT, Voita et al. (2018) shows that context-aware NMT improves the of anaphoric pronouns, and Maruf and Haffari (2018) proposed a document-level NMT using memory-networks. y esto es un escape de su estado atormentado . and that is an escape from his tormented state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Ta"
D18-1325,D16-1050,0,0.0212238,"al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from previous sentence translations into the current sentence translation. The model significantly outperforms two competitive baselines, and the ablation study shows that target and source context is complementary. It also improves lexical cohesion and coherence, and t"
D18-1325,D17-1301,0,0.159866,"ation (NMT) (Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017) trains an encoder-decoder network on sentence pairs to maximize the likelihood of predicting a target-language sentence given the corresponding source-language sentence, without considering the document context. By ignoring discourse connections between sentences and other valuable contextual information, this simplification potentially degrades the coherence and cohesion of a translated document (Hardmeier, 2012; Meyer and Webber, 2013; Sim Smith, 2017). Recent studies (Tiedemann and Scherrer, 2017; Jean et al., 2017; Wang et al., 2017; Tu et al., 2018) have demonstrated that adding contextual information to the NMT model improves the general translation performance, and more importantly, improves the coherence and cohesion of the translated text (Bawden et al., 2018; Lapshinova-Koltunski and Hardmeier, 2017). Most of these methods use an additional encoder (Jean et al., 2017; Wang et al., 2017) to extract contextual information from previous source-side sentences. However, this requires additional parameters and it does not exploit the representations already learned by the NMT encoder. More recently, Tu et al. (2018) have"
D18-1325,D16-1027,0,0.0221695,"oun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from previous sentence translations into the current sentence translation. The model significantly outperforms two competitive baselines, and the ablation study shows that target and source context is"
D18-1325,D12-1097,0,0.0417068,"joint HAN achieves the best accuracy with a significant improvement compared to other models, showing that target and source contextual information are complementary. Similarity for pronouns, the joint model has the best result for TED talks and news. However, HAN encoder alone is better in the case of subtitles. Here HAN decoder produces mistakes by repeating past translated personal pronouns. Subtitles is a challenging corpus for personal pronoun disambiguation because it usually involves dialogue between multiple speakers. 4.3 Cohesion and Coherence Evaluation We use the metric proposed by Wong and Kit (2012) to evaluate lexical cohesion. It is defined as the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. The lexical similarity is obtained using WordNet. Table 4 (bottom-left) displays the average ratio per tested document. In some cases, HAN decoder achieves the best score because it produces a larger quantity of repetitions than other models. However, as previously demonstrated in 4.2, repetitions do not always make the translation better. Although HAN boosts lexical cohesion, the scores are still far from the"
D18-1325,1983.tc-1.13,0,0.633684,"Missing"
D18-1325,D13-1163,0,0.0466443,"ormentado . and that is an escape from his tormented state . and this is an escape from its < unk > state . and this is an escape from their state . and this is an escape from his < unk > state . Context from Previous Sentences HAN decoder context with target. Query: his (En) Src.: Ref.: Base: Cache: HAN: HAN encoder context with source. Query: su (Es) 6 Table 5: Example of pronoun disambiguation using HAN (TED Talks Es-En). mann, 2010; Gong et al., 2011). However, most of the work explicitly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We"
D18-1325,E17-2061,0,0.0183248,"citly models discourse phenomena (Sim Smith, 2017) such as lexical cohesion (Meyer and Popescu-Belis, 2012; Xiong et al., 2013; Lo´aiciga and Grisot, 2016; Pu et al., 2017; Mascarell, 2017), coherence (Born et al., 2017), and coreference (Rios Gonzales and Tuggener, 2017; Miculicich Werlen and PopescuBelis, 2017a). Hardmeier et al. (2013) introduced the document-level SMT paradigm. Sentence-level NMT Initial studies on NMT enhanced the sentence-level context by using memory networks (Wang et al., 2016), self-attention (Miculicich Werlen et al., 2018; Zhang et al., 2016), and latent variables (Yang et al., 2017). Document-level NMT Tiedemann and Scherrer (2017) use the concatenation of multiple sentences Conclusion We proposed a hierarchical multi-head HAN NMT model5 to capture inter-sentence connections. We integrated context from source and target sides by directly connecting representations from previous sentence translations into the current sentence translation. The model significantly outperforms two competitive baselines, and the ablation study shows that target and source context is complementary. It also improves lexical cohesion and coherence, and the translation of nouns and pronouns. The"
D18-1325,N16-1174,0,0.0744028,"already learned by the NMT encoder. More recently, Tu et al. (2018) have shown that a cache-based memory network performs better than the above encoder-based methods. The cache-based memory keeps past context as a set of words, where each cell corresponds to one unique word keeping the hidden representations learned by the NMT while translating it. However, in this method, the word representations are stored irrespective of the sentences where they occur, and those vector representations are disconnected from the original NMT network. We propose to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentence-level abstractions. In contrast to the hierarchical recurrent neural network (HRNN) used by (Wang et al., 2017), here the attention allows dynamic access to the context by selectively focusing on different sentences and words for each predicted word. In addition, we integrate two HANs in the NMT model to account for target and source context. The HAN encoder helps in the disambiguation of source-word representations, while the HAN decoder improves the target-side lexical cohesion and coherence. The integr"
D18-1325,P13-4033,0,\N,Missing
D18-1325,P06-4018,0,\N,Missing
D18-1325,2015.iwslt-evaluation.1,0,\N,Missing
D18-1325,N18-1118,0,\N,Missing
D18-1325,P16-5005,0,\N,Missing
D19-1450,D16-1250,0,0.12013,"t procedure as Lample et al. (2018). After the core mapping matrix is learned, we use it to translate the ten thousand most frequent source words (s1 , s2 , ..., s10000 ) into the target language. We then take these ten thousand translations (t1 , t2 , ..., t10000 ) and translate them back into the source language. We call these translation (s01 , s02 , ..., s010000 ). We then consider all the word pairs (si , ti ) where si = s0i as our seed dictionary and use this dictionary to update our preliminary mapping matrix using the objective in equation 1. Moreover, following Xing et al. (2015) and Artetxe et al. (2016), we force the refined matrix W ∗ to be orthogonal by using singular value decomposition (SVD): W ∗ = ZU > , U ΣZ > = SV D(V s > V t ) 3.3 (6) Cross-Domain Similarity Local Scaling Previous work (Radovanovi´c et al., 2010; Dinu et al., 2015) has shown that standard nearest neighbour techniques are not effective to retrieve target similar words in high-dimensional spaces. The work of Lample et al. (2018) showed that using cross-domain similarity local scaling (CSLS) to retrieve target similar words is more accurate than standard nearest neighbour techniques. Instead of just considering the simi"
D19-1450,P17-1042,0,0.381239,"monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This generality comes at the expense of performance. To overcome this limitation, Lample et al. (2018) refine the preliminary mapping matrix trained by generative adversarial networks (GANs) and obtain a model that is again comparable to supervised models for several language pairs. Despite these big improvements, the performance of these refined GAN models depends largely on the quality of the preliminary mapp"
D19-1450,P18-1073,0,0.44959,"ranslations are retrieved by using CSLS. Bold face indicates the best result overall and italics indicate the best result between the two columns without refinement. pus. Each dictionary has a training set of 5000 entries and a test set of 1500 entries. Compared to BLI-1, this dataset is much noisier and the entries are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastT"
D19-1450,P14-2131,0,0.043752,"which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has"
D19-1450,D18-1024,0,0.0528092,") + Evs ∼pvs |c log(1 − Dcl (G(vs ), vc )) 3 Shared Components Both the GAN and the concept-based GAN models use a variety of further methods that have been shown to improve results. 3.1 Orthogonalization Previous studies (Xing et al., 2015; Smith et al., 2017) show that enforcing the mapping matrix W to be orthogonal can improve the performance and 4421 make the adversarial training more stable. In this work, we perform the same update step proposed by Cisse et al. (2017) to approximate setting W to an orthogonal matrix:   W ← (1 + β) W − β W W > W (5) According to Lample et al. (2018) and Chen and Cardie (2018), when setting β to less than 0.01, the orthogonalization usually performs well. 3.2 Post-refinement Previous work has shown that the core crosslingual mapping can be improved by refining it by bootstrapping from a dictionary extracted from the learned mapping itself (Lample et al., 2018). Since this refinement process is not the focus of this work, we perform the same refinement procedure as Lample et al. (2018). After the core mapping matrix is learned, we use it to translate the ten thousand most frequent source words (s1 , s2 , ..., s10000 ) into the target language. We then take these ten"
D19-1450,W17-6508,0,0.0160341,"g dictionary of 5000 words and an evaluation dictionary of 1500 words. This dataset allows us to have a better understanding of the performance of our method on many different language pairs. We choose nine languages for testing and compare our method to our supervised and unsupervised baselines described in section 5: English (en), German (de), Finnish (fi), French (fr), Spanish (es), Italian (it), Russian (ru), Turkish (tr) and Chinese (zh). We classify similar and distant languages based on a combination of structural properties (directional dependency distance, as proposed and measured in Chen and Gerdes (2017)) and lexical properties, as measured by the clustering of current large-scale multilingual sentence embeddings.4 We consider en  de, en  fr, en  es, en  it as similar language pairs and en  fi, en  ru, en  tr, en  zh as distant language pairs. BLI-2 Unlike BLI-1, the dataset of Dinu et al. (2015) and its extensions provided by Artetxe et al. (2017, 2018b) only consists of dictionaries of 4 language pairs trained on a Europarl parallel cor4 See for example the clustering in https://code.fb.com/airesearch/laser-multilingual-sentence-embeddings/ 4423 Supervised Similar language pairs Dis"
D19-1450,N19-1423,0,0.0245121,"Missing"
D19-1450,E14-1049,0,0.535658,"tural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This"
D19-1450,Q17-1010,0,0.0918733,"s are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastText6 to train our monolingual word embedding models with 300 dimensions for each language and default settings. The 5 Most of these methods have been tested by Artetxe et al. (2018b) by using their own implementations. 6 https://github.com/facebookresearch/fastText training corpus comes from a Wikipedia dump.7 For Euro"
D19-1450,P14-1006,0,0.0486385,"r the top 3 translation candidates, both models are able to predict the correct translations well. Interestingly, the word ”battery” is most commonly translated as Chinese ”电 池(dianchi)”, correctly predicted by our model, and the top 3 translation candidates provided by 13 As optimisation of the refinement is not the objective of this paper, we follow the work of Lample et al. (2018) and run only five iterations for post-refinement. 4425 ing (Jiang et al., 2015, 2016; Ammar et al., 2016a). Generally, such spaces can be trained directly from bilingual sentence aligned or document aligned text (Hermann and Blunsom, 2014; Chandar A P et al., 2014; Søgaard et al., 2015; Vuli´c and Moens, 2013). However the performance of directly trained models is limited by their vocabulary size. Figure 3: Accuracy of Chinese-English bilingual lexicon induction task for models trained from different concept numbers. Figure 4: Average error reduction of our method compared to unsupervised adversarial method for bilingual lexicon induction on BLI-1 dataset (Lample et al., 2018). Since the Finnish-English pair is an outlier for the unsupervised method, we report both the average with and without this pair. our model are all rela"
D19-1450,P15-1119,0,0.0405513,"we can see that for these selected English words, our model performs better than the unsupervised GANs model of Lample et al. (2018), but when we consider the top 3 translation candidates, both models are able to predict the correct translations well. Interestingly, the word ”battery” is most commonly translated as Chinese ”电 池(dianchi)”, correctly predicted by our model, and the top 3 translation candidates provided by 13 As optimisation of the refinement is not the objective of this paper, we follow the work of Lample et al. (2018) and run only five iterations for post-refinement. 4425 ing (Jiang et al., 2015, 2016; Ammar et al., 2016a). Generally, such spaces can be trained directly from bilingual sentence aligned or document aligned text (Hermann and Blunsom, 2014; Chandar A P et al., 2014; Søgaard et al., 2015; Vuli´c and Moens, 2013). However the performance of directly trained models is limited by their vocabulary size. Figure 3: Accuracy of Chinese-English bilingual lexicon induction task for models trained from different concept numbers. Figure 4: Average error reduction of our method compared to unsupervised adversarial method for bilingual lexicon induction on BLI-1 dataset (Lample et al."
D19-1450,N18-1202,0,0.0748602,"Missing"
D19-1450,P81-1022,0,0.250062,"Missing"
D19-1450,P07-2045,0,0.00444506,"stText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use different monolingual word embeddings for BLI-1 and for BLI-2. For BLI-1, we use FastText6 to train our monolingual word embedding models with 300 dimensions for each language and default settings. The 5 Most of these methods have been tested by Artetxe et al. (2018b) by using their own implementations. 6 https://github.com/facebookresearch/fastText training corpus comes from a Wikipedia dump.7 For European languages, words are lower-cased and tokenized by the scripts of Moses (Koehn et al., 2007).8 For Chinese, we first use OpenCC9 to convert traditional characters to simplified characters and then use Jieba10 to perform tokenization. For each language, we only keep the words that appear more than five times. For BLI-2, following the work of Artetxe et al. (2018a),11 we use their pretrained CBOW embeddings of 300 dimensions. For English, Italian and German, the models are trained on the WacKy corpus. The Finnish model is trained from Common Crawl and the Spanish model is trained from WMT News Crawl. Concept Aligned Data For concept-aligned articles, we use the Linguatools Wikipedia co"
D19-1450,W13-3512,0,0.0509454,"similarity between a source embedding and its neighbours in the target language, and rs (vt ) represent the mean similarity between a target embedding and its neighbours in the source language. In this work, we use CSLS to build a dictionary for our postrefinement. 4 Training In our weakly-supervised model, the sampling and model selection procedures are important. Sampling Procedure As the statistics show in Table 1, even after filtering, the vocabulary of concept-aligned articles is still large. But it has been shown that the embeddings of frequent words are the most informative and useful (Luong et al., 2013; Lample et al., 2018), so we only keep the one-hundred thousand most frequent words for learning W . For each training step, then, the input word s of our generator is randomly sampled from the vocabulary that is common both to the source monolingual word embedding S and the source Wikipedia concept-aligned articles. After the input source word s is sampled, we sample a concept c according to the frequency of s in each source article of the ensemble of concepts. Then we uniformly sample a target word t from the subvocabulary of the target article of concept c.3 Model Selection It is not as di"
D19-1450,D16-1096,0,0.0530842,"erformance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main i"
D19-1450,W16-1614,0,0.0303292,"tead of using a pre-build dictionary for initialization, they sort the value of the word vectors in both the source and the target distribution, treat two vectors that have similar permutations as possible translations and use them as the initialization dictionary. Additionally, their unsupervised framework also includes many optimization augmentations, such as stochastic dictionary induction and symmetric re-weighting, among others. Theoretically, employing GANs for training cross-lingual word embedding is also a promising way to avoid the use of bilingual evidence. As far as we know, Miceli Barone (2016) was the first 4426 MUSE Our Method English to Chinese film 电 影(film), 该片(this film), 本片(this film) 电 影(film), 影片 (film), 本片(this film) debate 辩 论(debate), 争论(dispute), 讨论(discuss) 辩 论(debate), 争论(dispute), 议题(topic) 电池(battery), 锂电池(lithium battery), 火炮(artillery), 炮(cannon), 控制器(controller) 装甲车辆(armored car) English to French f´evrier(february), janvier(january), f´evrier(february), january novembre(november), septembre(september) janvier(january) auteur(author), e´ crivain(writer), auteur(author), e´ crivain(writer), author romancier(novelist) romancier(novelist) universit´e(university), un"
D19-1450,P15-1165,0,0.14992,"Missing"
D19-1450,D13-1168,0,0.0289175,"Missing"
D19-1450,N15-1104,0,0.55293,"ibution pv t or the generated target language distribution, pG(vs ) . The simpler discriminator Dl is useful when the concept-based discriminator is not stable. The objective function of the multi-discriminator model, shown in equation (4), combines all these elements. min max Evt ∼pvt log Dl (vt ) G Dl ,Dcl + Evs ∼pvs log(1 − Dl (G(vs ))) (4) + Evt ∼pvt |c log Dcl (vt , vc ) + Evs ∼pvs |c log(1 − Dcl (G(vs ), vc )) 3 Shared Components Both the GAN and the concept-based GAN models use a variety of further methods that have been shown to improve results. 3.1 Orthogonalization Previous studies (Xing et al., 2015; Smith et al., 2017) show that enforcing the mapping matrix W to be orthogonal can improve the performance and 4421 make the adversarial training more stable. In this work, we perform the same update step proposed by Cisse et al. (2017) to approximate setting W to an orthogonal matrix:   W ← (1 + β) W − β W W > W (5) According to Lample et al. (2018) and Chen and Cardie (2018), when setting β to less than 0.01, the orthogonalization usually performs well. 3.2 Post-refinement Previous work has shown that the core crosslingual mapping can be improved by refining it by bootstrapping from a dic"
D19-1450,D18-1268,0,0.0199835,"a strong CSLS-based refinement to the core mapping matrix trained by GANs. Even in this case, though, without refinement, the core mappings are not as good as hoped for some distant language pairs. More recently, Chen and Cardie (2018) extends the work of Lample et al. (2018) from the bilingual setting to the multi-lingual setting. Instead of training crosslingual word embeddings for only one language pair, their approach allows them to train crosslingual word embeddings for many language pairs at the same time. Another recent piece of work which is similar to Lample et al. (2018) comes from Xu et al. (2018). Their approach can be divided into 2 steps: first, using Wasserstein GAN (Arjovsky et al., 2017) to train a preliminary mapping between two monolingual distributions and then minimizing the Sinkhorn Distance across distributions. Although their method performs better than Lample et al. (2018) in several tasks, the improvement mainly comes from the second step, showing that the problem of how to train a better preliminary mapping has not been resolved. 7 Conclusions and Future Work In this paper, we propose a weakly-supervised adversarial training method for cross-lingual word embedding mappi"
D19-1450,P17-1179,0,0.440758,"ation task. However, traditional methods for mapping two monolingual word embeddings require high quality aligned sentences or dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016b). Reducing the need for parallel data, then, has become the main issue for cross-lingual word embedding mapping. Some recent work aiming at reducing resources has shown competitive crosslingual mappings across similar languages, using a pseudo-dictionary, such as identical character strings between two languages (Smith et al., 2017), or a simple list of numerals (Artetxe et al., 2017). In a more general method, Zhang et al. (2017) have shown that learning mappings across languages via adversarial training (Goodfellow et al., 2014) can avoid using bilingual evidence. This generality comes at the expense of performance. To overcome this limitation, Lample et al. (2018) refine the preliminary mapping matrix trained by generative adversarial networks (GANs) and obtain a model that is again comparable to supervised models for several language pairs. Despite these big improvements, the performance of these refined GAN models depends largely on the quality of the preliminary mappings. It is probably for this reason that these"
D19-1450,D14-1162,0,0.0926268,"ing across languages is better done at the concept level than at the word level. We propose a concept-based adversarial training method which improves the performance of previous unsupervised adversarial methods for most languages, and especially for typologically distant language pairs. 1 Introduction Distributed representations of words which map each word to a continuous vector have proven useful in capturing important linguistic information. Vectors of words that are semantically or syntactically similar have been shown to be close to each other in the same space (Mikolov et al., 2013a,c; Pennington et al., 2014), making them widely useful in many natural language processing tasks such as machine translation and parsing (Bansal et al., 2014; Mi et al., 2016), both in a single language and across different languages. Mikolov et al. (2013b) first observed that the geometric positions of similar words in different languages were related by a linear relation. Zou et al. (2013) showed that a cross-lingually shared word embedding space is more useful than a monolingual space in an end-to-end machine translation task. However, traditional methods for mapping two monolingual word embeddings require high quali"
D19-1450,N16-1156,0,0.12336,"mple et al. (2018), VecMap is the supervised model of Artetxe et al. (2017). Word translations are retrieved by using CSLS. Bold face indicates the best result overall and italics indicate the best result between the two columns without refinement. pus. Each dictionary has a training set of 5000 entries and a test set of 1500 entries. Compared to BLI-1, this dataset is much noisier and the entries are selected from different frequency ranges. However, BLI-2 has been widely used for testing by previous methods (Faruqui and Dyer, 2014; Dinu et al., 2015; Xing et al., 2015; Artetxe et al., 2016; Zhang et al., 2016; Artetxe et al., 2017; Smith et al., 2017; Lample et al., 2018; Artetxe et al., 2018a,b).5 Using BLI-2 allows us to have a direct comparison with the state-of-the-art. Monolingual Word Embeddings The quality of monolingual word embeddings has a considerable impact on cross-lingual embeddings (Lample et al., 2018). Compared to CBOW and Skip-gram embeddings, FastText embeddings (Bojanowski et al., 2017) capture syntactic information better. The ideal situation would be to use FastText embeddings for both the BLI-1 and BLI-2 datasets. However, much previous work uses CBOW embeddings, so we use d"
D19-1450,D13-1141,0,0.151404,"Missing"
E03-1002,P93-1005,0,0.0493196,"the input sentence. At each step, the process chooses a characteristic of the tree or predicts a word in the sentence. This sequence of decisions is the derivation of the tree, which we will denote d1,..., dm . Because there is a one-to-one mapping from phrase 131 structure trees to our derivations, the probability of a derivation P(di,..., d m) is equal to the joint probability of the derivation's tree and the input sentence. The probability of the input sentence is a constant across all the candidate derivations, so we only need to find the most probable derivation. in history-based models (Black et al., 1993), the probability estimate for each derivation decision di is conditioned on the previous derivation decisions d1,..., d,_1, which is called the derivation history at step i. This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions. P (d i ,..., = di_i) The probabilities P(d i ld 1 ,..., d 1 ) ' are the parameters of the parser's probability model. To define the parameters di_i) we need to choose the ordering of the decisions in a derivation, such as a top-down or s"
E03-1002,P01-1010,0,0.0267218,"Missing"
E03-1002,A00-2018,0,0.583514,"ducing Features of the Derivation History The most important step in designing a statistical parser with a history-based probability model is choosing a method for estimating the parameters d,_1). The main difficulty with this estimation is that the history d 1 ,..., di _ 1 is of unbounded length. Most probability estimation methods require that there be a finite set of features on which the probability is conditioned. The standard way to handle this problem is to handcraft a finite set of features which provides a sufficient summary of the unbounded history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). The probabilities are then assumed to be independent of all the infoimation about the history which is not captured by the chosen features. The difficulty with this approach is that the choice of features can have a large impact on the performance of the system, but it is not feasible to search the space of possible feature sets by hand. One alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded 2 We extended the left-corner parsing model in a few minor ways using grammar transforms. We replace Chomsky adjunction structures (i.e. structures of t"
E03-1002,P01-1017,0,0.0962962,"Missing"
E03-1002,P02-1034,0,0.0183236,"...] [Y ...]]) with a special ""modifier"" link in the tree (becoming [X ... [mod Y • requiring nodes which are popped from the stack to choose between attaching with a normal link or a modifier link. We also compiled some frequent chains of non-branching nodes (such as [S [VP ...1]) into a single node with a new label (becoming [S-VP ...]). These transforms are undone before any evaluation is performed on the output trees. We do not believe these transforms have a major impact on performance, but we have not currently run tests without them. feature sets, but then efficiency becomes a problem. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins' previous work on re-ranking using a finite set of features (Collins, 2000). In this work we use a method for automatically inducing a finite set of features for representing the derivation history. The method is a form of multi-layered artificial neural network called Simple Synchrony Networks (Lane and Henderson, 2001; Henderson, 2000). The outputs of this network are probab"
E03-1002,2000.iwpt-1.14,1,0.904418,"parse. A major challenge in this approach is choosing a representation of the parse history from which the probability for the next parser decision can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the unbounded parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2001). In the work presented here, we automatically induce a finite set of features to represent the unbounded parse history. We perform this induction using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000). Because this architecture is specifically designed for processing structures, it allows us to impose structurally specified and linguistically appropriate biases on the search for a good history representation. The resulting parser achieves performance far greater than previous approaches to neural network parsing (Ho and Chan, 1999; Costa et al., 2001), and only marginally below the current state-of-the-art for parsing the Penn Treebank. We propose a hybrid parsing system consisting of two components, a neural network which estimates the parameters of a probability model for phrase structur"
E03-1002,J98-4004,0,0.0299735,"her, and thus determines the inductive bias discussed in the previous section. The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any). For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial (Johnson, 1998), as has conditioning on the left-corner child (Roark and Johnson, 1999). Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i — 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases. As mentioned above, D(top) also includes top, itself, which means that the input"
E03-1002,1997.iwpt-1.18,0,0.342467,"re the parameters of the parser's probability model. To define the parameters di_i) we need to choose the ordering of the decisions in a derivation, such as a top-down or shift-reduce ordering. The ordering which we use here is that of a form of left-corner parser (Rosenkrantz and Lewis, 1970). A left-corner parser decides to introduced a node into the parse tree after the subtree rooted at the node's first child has been fully parsed. Then the subtrees for the node's remaining children are parsed in their left-to-right order. We use the binarized version of a leftcorner parser, described in (Manning and Carpenter, 1997), where the parse of each non-leftmost child begins with the parent node predicting the child's leftmost terminal, and ends with the child's root nonterminal attaching to the parent. An example of this ordering is shown by the numbering on the left in figure 1. The process which generates a tree begins with a stack that contains a node labeled ROOT (step 0) and must end in the same configuration (step 9), as shown on the right of the figure. The possible derivation decisions are: predict the next tag-word pair and push it on the stack (steps 1, 4, and 6), replace the node on top of the stack w"
E03-1002,J93-2004,0,0.0238786,"to be used for words which were not in the training set, and smoothing across tag-word pairs whose low frequency would prevent accurate learning by themselves. We do not do any morphological analysis of unknown words, although we would expect some improvement in performance if we did. A variety of frequency thresholds were tried, as reported in section 6. The same representation of tag-word pairs was used in the input as was used for prediction. 135 6 The Experimental Results The generality and efficiency of the above parsing model makes it possible to test a SSN parser on the Penn Treebank (Marcus et al., 1993), and thereby compare its performance directly to other statistical parsing models in the literature. To test the effects of varying vocabulary sizes on performance and tractability, we trained three different models. The simplest model (""Tags"") includes no words in the vocabulary, relying completely on the information provided by the part-of-speech tags of the words. The second model (""Freq>200"") uses all tag-word pairs which occur at least 200 times in the training set. The remaining words were all treated as instances of the unknown-word. This resulted in a vocabulary size of 512 tag-word p"
E03-1002,W96-0213,0,0.116337,"best non-lexicalized and the best lexicalized models on the testing set. 6 Standard measures of performance are shown in table 1. 7 The top panel of table 1 lists the results for the non-lexicalized model (SSN-Tags) and the available results for three other models which only use part-of-speech tags as inputs, another neural network parser (Costa et al., 2001), an earlier statis5 In these experiments the tags are included in the input to the system, but, for compatibility with other parsers, we did not use the hand-corrected tags which come with the corpus. We used a publicly available tagger (Ratnaparkhi, 1996) to tag the words and then used these in the input to the system. 6 We found that 80 hidden units produced better performance than 60 or 100. Momentum was applied throughout training. Weight decay regularization was applied at the beginning of training but reduced to zero by the end of training. 7 A11 our results are computed with the evalb program following the now-standard criteria in (Collins, 1999). 136 Costa-et-a101 Manning&Carpenter97 Charniak97 (PCFG) SSN-Tags Ratnaparkhi99 Collins99 Charniak00 Collins00 Bod01 SSN-Freq>200 Length<40 LR LP All LR LP NA NA 57.8 64.9 77.6 71.2 83.9 79.9 75"
E03-1002,P99-1054,0,0.017311,"revious section. The principle we apply when designing D(top,) and f is that we want the inductive bias to reflect structural locality. For this reason, D(top) includes nodes which are structurally local to top,. These nodes are the left-corner ancestor of top, (which is below top, on the stack), top 's left-corner child (its leftmost child, if any), and top 's most recent child (which was top,_ 1 , if any). For right-branching structures, the leftcorner ancestor is the parent, conditioning on which has been found to be beneficial (Johnson, 1998), as has conditioning on the left-corner child (Roark and Johnson, 1999). Because these inputs include the history features of both the leftcorner ancestor and the most recent child, a derivation step i always has access to the history features from the previous derivation step i — 1, and thus (by induction) any information from the entire previous derivation history could in principle be stored in the history features. Thus this model is making no a priori hard independence assumptions, just a priori soft biases. As mentioned above, D(top) also includes top, itself, which means that the inputs to g always include the history features for the most recent derivatio"
E03-1002,P80-1024,0,0.394898,"vation decisions d1,..., d,_1, which is called the derivation history at step i. This allows us to use the chain rule for conditional probabilities to derive the probability of the entire derivation as the multiplication of the probabilities for each of its decisions. P (d i ,..., = di_i) The probabilities P(d i ld 1 ,..., d 1 ) ' are the parameters of the parser's probability model. To define the parameters di_i) we need to choose the ordering of the decisions in a derivation, such as a top-down or shift-reduce ordering. The ordering which we use here is that of a form of left-corner parser (Rosenkrantz and Lewis, 1970). A left-corner parser decides to introduced a node into the parse tree after the subtree rooted at the node's first child has been fully parsed. Then the subtrees for the node's remaining children are parsed in their left-to-right order. We use the binarized version of a leftcorner parser, described in (Manning and Carpenter, 1997), where the parse of each non-leftmost child begins with the parent node predicting the child's leftmost terminal, and ends with the child's root nonterminal attaching to the parent. An example of this ordering is shown by the numbering on the left in figure 1. The"
E03-1002,J03-4003,0,\N,Missing
E06-2009,W03-2123,1,0.88171,"Missing"
E06-2009,C00-1073,0,0.023374,"trics combined with a fixed penalty for dialogue length (see (Henderson et al., 2005)). This agent can be called whenever the system has to decide on the next dialogue move. In the original hand-coded system this decision is made by way of a dialogue plan (using the “deliberate” solvable). The RL agent can be used to drive the entire dialogue policy, or can be called only in certain circumstances. This makes it usable for whole dialogue strategies, but also, if desired, it can be targetted only on specific dialogue management decisions (e.g. implicit vs. explicit confirmation, as was done by (Litman et al., 2000)). One important research issue is that of tranferring learnt strategies between domains. We learnt a strategy for the C OMMUNICATOR flight booking dialogues (Lemon et al., 2005; Henderson et al., 2005), but this is generated by rather different scenarios than the in-car dialogues. However, both are “slot-filling” or information-seeking applications. We defined a mapping (described below) between the states and actions of both systems, in order to construct an interface between the learnt policies for C OMMUNICATOR and the in-car baseline system. 5.2 Mapping between COMMUNICATOR and the In-car"
E14-1002,J93-2003,0,0.0392486,"n on top-down structure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f , from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f (x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized lear"
E14-1002,P10-4002,0,0.0496242,"Missing"
E14-1002,W11-2103,0,0.058364,"Missing"
E14-1002,P05-1033,0,0.0508784,"ction and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f , from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f (x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT applications, it is not possible to enumerate all y ∈ Y. 10 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 10–19, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics is the synchro"
E14-1002,D11-1083,1,0.890464,"Missing"
E14-1002,J07-2003,0,0.505047,"on t. The BLEU score can be computed only when a terminal state is reached and a full translation is available. Thus, the rewards are all zero except at terminal states, called a Pure De3 Preliminary experiments with updating only the features for a ˆ and a′ produced substantially worse results. 14 4 Undirected Features has an associated CFF feature, which is an upper bound on the score of its missing branch. More precisely, it is an upper bound on the context-free component of this score. This upper bound can be exactly and efficiently computed using the Forest Rescoring Framework (Huang and Chiang, 2007; Huang, 2008). This framework separates the MT decoding in two steps. In the first step only the context-free factors are considered. The output of the first step is a hypergraph called the contextfree-forest, which compactly represents an exponential number of synchronous-trees. The second step introduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over"
E14-1002,N10-1115,0,0.0615218,"ffers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-structure by recursively creating the easiest arc in a non-directional manner. This model also integrates the tasks of learning the order of inference and training the parser in a single Perceptron. By “non-directional” they mean the removal of the constraint of scanning the sentence from left to right, which is typical of shift-reduce models. However this algorithm still builds the tree structures in a bottom-up fashion. This model has a O(n log n) decoding complexity and accuracy performance close to the O(n2 )"
E14-1002,P04-1015,0,0.377463,"model also integrates the tasks of learning the order of inference and training the parser in a single Perceptron. By “non-directional” they mean the removal of the constraint of scanning the sentence from left to right, which is typical of shift-reduce models. However this algorithm still builds the tree structures in a bottom-up fashion. This model has a O(n log n) decoding complexity and accuracy performance close to the O(n2 ) graph-based parsers (Mcdonald et al., 2005). Similarities can be found between DRL and previous work that applies discriminative training to structured prediction: Collins and Roark (2004) present an Incremental Parser trained with the Perceptron algorithm. Their approach is specific to dependency parsing and requires a function to test exact match of tree structures to trigger parameter updates. On the other hand, DRL can be applied to any structured prediction task and can handle any kind of reward function. LASO (Daum´e III and Marcu, 2005; Daum´e III et al., 2005) and SEARN (Daum´e III et al., 2009; Daum´e III et al., 2006) are generic frameworks for discriminative training for structured prediction: LASO requires a function that tests correctness of partial structures to t"
E14-1002,W02-1001,0,0.303672,"n the UMT scoring function’s parameters, using the BLEU score as the global loss function. DRL learns a weight vector for a linear classifier that discriminates between decisions based on which one leads to a complete translation-derivation with a better BLEU score. Promotions/demotions of translations are performed by applying a Perceptron-style update on the sequence of decisions that produced the translation, thereby training local decisions to optimize the global BLEU score of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f , that maps a source sentence, x ∈ X , into a structure defined b"
E14-1002,W05-1506,0,0.266552,"Missing"
E14-1002,P07-1019,0,0.0854723,"translation t. The BLEU score can be computed only when a terminal state is reached and a full translation is available. Thus, the rewards are all zero except at terminal states, called a Pure De3 Preliminary experiments with updating only the features for a ˆ and a′ produced substantially worse results. 14 4 Undirected Features has an associated CFF feature, which is an upper bound on the score of its missing branch. More precisely, it is an upper bound on the context-free component of this score. This upper bound can be exactly and efficiently computed using the Forest Rescoring Framework (Huang and Chiang, 2007; Huang, 2008). This framework separates the MT decoding in two steps. In the first step only the context-free factors are considered. The output of the first step is a hypergraph called the contextfree-forest, which compactly represents an exponential number of synchronous-trees. The second step introduces contextual features by applying a process of state-splitting to the context-free-forest, rescoring with non-context-free factors, and efficiently pruning the search space. To efficiently compute CFF features we run the Inside-Outside algorithm with the (max, +) semiring (Goodman, 1999) over"
E14-1002,N03-1017,0,0.0114448,"xt. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f , from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f (x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT applications, it is not"
E14-1002,W09-0424,0,0.0457644,"Missing"
E14-1002,P06-1096,0,0.0753489,"Missing"
E14-1002,D07-1104,0,0.0605078,"Missing"
E14-1002,P05-1012,0,0.0787025,"11 Machine Translation shared task (Callison-Burch et al., 2011)). To measure the significance of the variation, we compute the sign test and measure the one-tail p-value for the presented models in comparison to HMT b30. From the values re6 Related Work Models sharing similar intuitions have been previously applied to other structure prediction tasks. For example, Nivre et al. (2006) presents a linear time syntactic dependency parser, which is constrained in a left-to-right decoding order. This model offers a different accuracy/complexity balance than the quadratic time graph-based parser of Mcdonald et al. (2005). Other approaches learning a model specifically for greedy decoding have been applied with suc16 cess to other less complex tasks. Shen et al. (2007) present the Guided Learning (GL) framework for bidirectional sequence classification. GL successfully combines the tasks of learning the order of inference and training the local classifier in a single Perceptron-like algorithm, reaching state of the art accuracy with complexity lower than the exhaustive counterpart (Collins, 2002). Goldberg and Elhadad (2010) present a similar training approach for a Dependency Parser that builds the tree-struc"
E14-1002,nivre-etal-2006-maltparser,0,0.0711742,"Missing"
E14-1002,P03-1021,0,0.120469,"Missing"
E14-1002,P07-1096,0,0.540103,"bottom of the rank still have a small probability to be selected. The if at line 6 tests if the translation produced by path(T (s, a′ ), πw ) has higher BLEU score than the one produced by path(T (s, a ˆ), πw ). For the update statement at line 7 we use the Averaged Perceptron technique (Freund and Schapire, 1999). Algorithm 2 can be easily adapted to implement the efficient Averaged Perceptron updates (e.g. see Section 2.1.1 of (Daum´e III, 2006)). In preliminary experiments, we found that other more aggressive update technique, such as Passive-Aggressive (Crammer et al., 2006), Aggressive (Shen et al., 2007), or MIRA (Crammer and Singer, 2003), lead to worst accuracy. To see why this might be, consider that a MT decoder needs to learn to construct structures (t, h), while the training data specifies the gold translation t∗ but gives no information on the hidden-correspondence structure h. As discussed in (Liang et al., 2006), there are output structures that match the reference translation using a wrong internal structure (e.g. assuming wrong internal alignment). While in other cases the output translation can be a valid alternative translation but gets a low BLEU score because it differs from t∗"
E14-1002,D07-1078,0,0.0632661,"Missing"
E14-1002,P01-1067,0,0.116832,"ure and surrounding context. This allows the introduction of a new class of contextual features that are not constrained to condition only on the bottom-up context. The model builds translation-derivations efficiently in a greedy fashion. It is trained to learn to choose jointly the best action and the best inference order. Experiments show that the decoding time is halved and forestrescoring is 6 times faster, while reaching accuracy not significantly different from state of the art. 1 Introduction Machine Translation (MT) can be addressed as a structured prediction task (Brown et al., 1993; Yamada and Knight, 2001; Koehn et al., 2003). MT’s goal is to learn a mapping function, f , from an input sentence, x, into y = (t, h), where t is the sentence translated into the target language, and h is the hidden correspondence structure (Liang et al., 2006). In Hierarchical MT (HMT) (Chiang, 2005) the hidden correspondence structure is the synchronous-tree composed by instantiations of synchronous rules from the input grammar, G. Statistical models usually define f as: f (x) = arg maxy∈Y Score(x, y), where Score(x, y) is a function whose parameters can be learned with a specialized learning algorithm. In MT app"
E14-1002,N06-1033,0,0.0333053,"the global BLEU score of the final translation, while keeping the efficiency and simplicity of the Perceptron Algorithm (Rosenblatt, 1958; Collins, 2002). Our experiments show that UMT with DRL reduces decoding time by over half, and the time to rescore translations with the Language Model by 6 times, while reaching accuracy non-significantly different from the state of the art. 2 Undirected Machine Translation In this section, we present the UMT framework. For ease of presentation, and following synchronous-grammar based MT practice, we will henceforth restrict our focus to binary grammars (Zhang et al., 2006; Wang et al., 2007). A UMT decoder can be formulated as a function, f , that maps a source sentence, x ∈ X , into a structure defined by y = (t, h) ∈ Y, where t is the translation in the target language, and h Partial synchronous-trees, τ , are expanded by performing connection-actions. Given a τ we can connect to it a new rule, rˆ, using one available nonterminal represented by postcondition, pi ∈ P , and obtain a new partial synchronous-tree τˆ. Formally: τˆ ≡ h τ ⋖ a ˆ i, where, a ˆ = [ˆ r , pi ], represents the connection-action. 11 erates the main loop, until τ is complete and is returne"
E14-1002,P05-1032,0,\N,Missing
E14-1002,J99-4004,0,\N,Missing
J08-4002,W03-2123,1,0.704684,"2005; Georgila et al., submitted) to assign Speech Acts and Tasks to the user utterances, and to compute state representations for each point in the dialogue (i.e., after every utterance). Although we annotated the whole 2000 and 2001 corpora, because we need the results of user questionnaires (as discussed subsequently), we only make use of the 2001 data for the experiments reported here. The 2001 data has eight systems, 1,683 dialogues, and 125,388 total states, two thirds of which result from system actions and one third from user actions. The annotation system is implemented using DIPPER (Bos et al. 2003) and OAA (Cheyer and Martin 2001), using several OAA agents (see Georgila, Lemon, and Henderson, 2005, and Georgila et al., submitted, for more details). Following the ISU approach, we represented states using Information States, which are feature structures intended to record all the information about the preceding portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in an Information State is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Given this corpus, we need to lea"
J08-4002,P06-1024,1,0.840119,"Missing"
J08-4002,P04-1044,1,0.798229,"Missing"
J08-4002,P06-2085,1,0.919667,"y of the states of information slots (e.g., destination city ﬁlled with high conﬁdence) in the application (Goddeau and Pineau 2000; Levin, Pieraccini, and Eckert 2000; Singh et al. 2000a, 2000b, 2002; Young 2000; Schefﬂer and Young 2002; Williams, Poupart, and Young 2005a, 2005b; Williams and Young 2005; Pietquin and Dutoit 2006b), with perhaps some additional low-level information (such as acoustic features [Pietquin 2004]). Only recently have researchers experimented with using enriched representations of dialogue context (Gabsdil and Lemon 2004; Lemon et al. 2005; Frampton and Lemon 2006; Rieser and Lemon 2006c), as we do in this article. From this work it is known that adding context features leads to better dialogue strategies, compared to, for example, simply using the status of ﬁlled or conﬁrmed information slots as has been studied in all prior work (Frampton and Lemon 2006). In this article we explore methods for scalable, tractable learning when using all the available context features. Reinforcement Learning requires estimating how good different actions will be in different dialogue contexts. Because most previous work has only differentiated between a small number of possible dialogue con"
J08-4002,2005.sigdial-1.6,1,0.727657,"learning approaches is the extent to which they train on data from simulated users of different kinds, rather than train on data gathered from real user interactions (as is done in this article). Simulated users are generally preferred due to the much smaller development effort involved, and the fact that trialand-error training with humans is tedious for the users. However, the issues of how to construct and then evaluate simulated users are open problems. Clearly there is a dependency between the accuracy of the simulation used for training and the eventual dialogue policy that is learned (Schatzmann et al. 2005). Current research attempts to develop metrics for user simulation that are predictive of the overall quality of the ﬁnal learned dialogue policy (Schatzmann, Georgila, and Young 2005; Schatzmann et al. 2005; Georgila, Henderson, and Lemon 2005; Georgila, Henderson, and Lemon 2006; Rieser and Lemon 2006a; Schatzmann et al. 2006; Williams 2007). Furthermore, several approaches use simple probabilistic simulations encoded by hand, using intuitions about reasonable user behaviors (e.g., Pietquin 2004; Frampton and Lemon 2005; Pietquin and Dutoit 2006a), whereas other work (e.g., Schefﬂer and Youn"
J08-4002,H01-1015,0,0.0687517,"portion of the dialogue that is relevant to making dialogue management decisions. An example of some of the types of information recorded in our Information States is shown in Figure 1, including ﬁlled slots, conﬁrmed slots, and previous speech acts. Previous work has raised the question of whether dialogue management policies can be learned (Levin and Pieraccini 1997) for systems that have only a limited view of the dialogue context, for example, not including prior speech act history (see the following). One prominent representation of the set of possible system actions is the DATE scheme (Walker and Passonneau 2001). In particular, this representation is used in the C OMMUNICATOR corpus annotation (Walker, Passonneau, and Boland 2001), discussed herein. The DATE scheme classiﬁes system actions in terms of their Conversational Domain, Speech Act, and Task. For example, one possible system action is about task, Figure 1 Example ﬁelds from an Information State annotation. User-provided information is in square brackets. 489 Computational Linguistics Volume 34, Number 4 request info, dest city, which corresponds to a system utterance such as What is your destination city? The speciﬁc instantiation of this"
J08-4002,P98-2219,0,0.0208242,"Missing"
J08-4002,P01-1066,0,0.149979,"olicy that is different from that used to generate the data (called “off-policy” learning), but these methods have been found not to work well with linear function approximation (Sutton and Barto 1998). They also do not solve the problem of straying from the region of state space that has been observed in the data, discussed subsequently. 491 Computational Linguistics Volume 34, Number 4 1.2 The COMMUNICATOR Domain and Data Annotation To empirically evaluate our proposed learning method, we apply it to the C OMMU NICATOR domain using the C OMMUNICATOR corpora. The C OMMUNICATOR corpora (2000 [Walker et al. 2001] and 2001 [Walker et al. 2002b]) consist of human–machine dialogues (approximately 2,300 dialogues in total). The users always try to book a ﬂight, but they may also try to select a hotel or car rental. The dialogues are primarily “slotﬁlling” dialogues, with some information being presented to the user after the system thinks it has ﬁlled (or conﬁrmed) the relevant slots. These corpora have been previously annotated using the DATE scheme, for the Conversational Domain, Speech Act, and Task of each system utterance (Walker and Passonneau 2001; Walker, Passonneau, and Boland 2001). In addition"
J08-4002,2005.sigdial-1.4,0,0.152387,"Missing"
J08-4002,W03-2111,0,0.256834,"Missing"
J08-4002,P00-1013,0,\N,Missing
J08-4002,C98-2214,0,\N,Missing
J08-4002,E06-2009,1,\N,Missing
J13-4006,W06-2922,0,0.0164481,"anar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Attardi’s actions Left2 and Right2, which create dependency arcs between the second element on the stack and the front of the input queue. In the Attardi algorithm, every attachment to an element below the top of the stack requires the use of one of the new actions, whose frequency is much lower than the normal attachment actions, and therefore harder to learn. This contrasts with the Swap action, which handles reordering with a single acti"
J13-4006,P98-1013,0,0.0854963,"Missing"
J13-4006,P93-1005,0,0.21067,"e use latent variables to model the interaction between syntax and semantics. Latent variables serve as an interface between semantics and syntax, capturing properties of both structures relevant to the prediction of semantics given syntax and, conversely, syntax given semantics. Unlike hand-crafted features, latent variables are induced automatically from data, thereby avoiding a priori hard independence assumptions. Instead, the structure of the latent variable model is used to encode soft biases towards learning the types of features we expect to be useful. We define a history-based model (Black et al. 1993) for joint parsing of semantic and syntactic structures. History-based models map structured representations to sequences of derivation steps, and model the probability of each step conditioned on the entire sequence of previous steps. There are standard shift-reduce algorithms (Nivre, Hall, and Nilsson 2004) for mapping a syntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the"
J13-4006,D12-1133,0,0.0438463,"2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in q"
J13-4006,burchardt-etal-2006-salsa,0,0.097902,"participants, or representations of objects involving their properties. The participants and properties in a frame are designated with a set of semantic roles called frame elements. One example is the MOTION DIRECTIONAL frame, and its associated frame elements include the THEME (the moving object), the GOAL (the ultimate destination), the SOURCE, and the PATH. The collection of sentences used to exemplify frames in the English FrameNet has been sampled to produce informative lexicographic examples, but no attempt has been made to produce representative distributions. The German SALSA corpus (Burchardt et al. 2006), however, has been annotated with FrameNet annotation. This extension to exhaustive corpus coverage and a new language has only required a few novel frames, demonstrating the cross-linguistic validity of this annotation scheme. FrameNets for other languages, Spanish and Japanese, are also under construction. Another semantically annotated corpus—the one we use in this work for experiments on English—is called Proposition Bank (PropBank) (Palmer, Gildea, and Kingsbury 2005). PropBank is based on the assumption that the lexicon is not a list of irregularities, but that systematic correlations c"
J13-4006,W05-0620,0,0.103932,"Missing"
J13-4006,A00-2018,0,0.0291314,"mber 2012; accepted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a se"
J13-4006,W08-2134,0,0.0835302,"tions separately, with a pipeline of state-of-the-art systems, and then reranks the joint representation in a Table 7 Scores of the fully connected model on the final testing sets of the CoNLL-2008 shared task (percentages). WSJ Brown WSJ+Brown Syntactic LAS P 88.4 80.4 87.5 79.9 65.9 78.4 Semantic R F1 75.5 60.8 73.9 77.6 63.3 76.1 P Macro R F1 84.2 73.1 83.0 82.0 70.6 80.7 83.0 71.8 81.8 981 Computational Linguistics Volume 39, Number 4 Table 8 Comparison with other models on the CoNLL-2008 test set (percentages). C O NLL M EASURES M ODEL Johansson and Nugues (2008b) Ciaramita et al. (2008) Che et al. (2008) Zhao and Kit (2008) This article Henderson et al. (2008) Llu´ıs and M`arquez (2008) C ROSSING A RCS Synt LAS Semantic F1 Macro F1 P 89.3 87.4 86.7 87.7 87.5 87.6 85.8 81.6 78.0 78.5 76.7 76.1 73.1 70.3 85.5 82.7 82.7 82.2 81.8 80.5 78.1 67.0 59.9 56.9 58.5 62.1 72.6 53.8 Semantics R F1 44.5 34.2 32.4 36.1 29.4 1.7 19.2 53.5 43.5 41.3 44.6 39.9 3.3 28.3 final step (Johansson and Nugues 2008b). Similarly, Che et al. (2008) also implement a pipeline consisting of state-of-the-art components where the final inference stage is performed using Integer Linear Programming to ensure global coherence o"
J13-4006,W08-2139,0,0.0541097,"Missing"
J13-4006,P05-1033,0,0.175384,"forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure"
J13-4006,W10-1811,0,0.0444172,"Missing"
J13-4006,W08-2138,0,0.114405,"Missing"
J13-4006,cmejrek-etal-2004-prague,0,0.0485572,"Missing"
J13-4006,J81-4005,0,0.781848,"Missing"
J13-4006,P07-1071,0,0.0242847,"Missing"
J13-4006,N04-1035,0,0.0110215,"trivial to develop systems that actually succeed in exploiting this intuitively obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with"
J13-4006,P11-2051,0,0.0133446,"main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a lingu"
J13-4006,P11-2003,1,0.92649,"used to estimate the probability of this chosen parser action, also shown in red. The edges to the state that is used to make this decision are specified by identifying the most recent previous state that shares some property with this state. In Figure 8, these edges are labeled with the property, such as having the same word on the top of the stack (S=S) or the top of the stack being the same as the current leftmost child of the top of the stack (S=LS). The argument for the incremental specification of model structure can be applied to any Bayesian network architecture, not just SBNs (e.g., Garg and Henderson 2011). We focus on ISBNs because, as shown in Section 4.1.5, they are closely related to the empirically successful neural network models of Henderson (2003), and they have achieved very good results on the sub-problem of parsing syntactic dependencies (Titov and Henderson 2007d). 4.1.4 ISBNs for Derivations of Structures. The general form of ISBN models that have been proposed for modeling derivations of structures is illustrated in Figure 9. Figure 9 illustrates a situation where we are given a derivation history preceding the elementary decision dik in decision Di , and we wish to compute a prob"
J13-4006,P09-1069,0,0.00946418,"where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili"
J13-4006,W05-0602,0,0.0203331,"ations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu an"
J13-4006,W09-1205,1,0.929971,"Missing"
J13-4006,J02-3001,0,0.528478,"supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011)"
J13-4006,P10-1151,0,0.037242,"Missing"
J13-4006,W09-1201,0,0.0282323,"Missing"
J13-4006,W05-1505,0,0.0299227,"rcs, rather than to change the order of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our S"
J13-4006,P12-1110,0,0.0280159,"omputational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and"
J13-4006,P11-2012,0,0.0126839,"; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and structures to unknown words and nonsense sentences. These two levels of representation of language, however, are closely correlated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic rol"
J13-4006,N03-1014,1,0.346312,"pted for publication: 1 November 2012. doi:10.1162/COLI a 00158 © 2013 Association for Computational Linguistics Computational Linguistics Volume 39, Number 4 nature of the approach, this extension of the ISBN architecture to weakly synchronized syntacticsemantic derivations is also an exemplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of r"
J13-4006,W08-2122,1,0.960335,"her drop of recall on non-planar dependencies. Applying the same planarization approach to semantic dependency structures is not trivial and would require a novel planarization algorithm, because semantic dependency graphs are highly disconnected structures, and direct application of any planarization algorithm, such as the one proposed in Nivre and Nilsson (2005), is unlikely to be appropriate. For instance, a method that extends the planarization method to semantic predicate-argument structures by exploiting the connectedness of the corresponding syntactic dependency trees has been tried in Henderson et al. (2008). Experimental results reported in Section 6 indicate that the method that we will illustrate in the following paragraphs yields better performance. A different way to tackle non-planarity is to extend the set of parsing actions to a more complex set that can parse any type of non-planarity (Attardi 2006). This approach is discussed in more detail in Section 7. We adopt a conservative version of this approach 3 Note that this planarity definition is stricter than the definition normally used in graph theory where the entire plane is used. Some parsing algorithms require projectivity: this is a"
J13-4006,W07-2416,0,0.0276781,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D08-1008,0,0.0432658,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,W08-2123,0,0.0264032,"tic representations is central for any system that taps into the meaning of text. Standard approaches to automatic semantic role labeling use hand-crafted features of syntactic and semantic representations within linear models trained with supervised learning. For example, Gildea and Jurafsky (2002) formulate the shallow semantic task of semantic role labeling (SRL) as a classification problem, where the semantic role to be assigned to each constituent is inferred on the basis of its co-occurrence counts with syntactic features extracted from parse trees. More recent and accurate SRL methods (Johansson and Nugues 2008a; Punyakanok, Roth, and Yih 2008) use complex sets of lexico-syntactic features and declarative constraints to infer the semantic structure. Whereas supervised learning is more flexible, general, and adaptable than hand-crafted systems, linear models require complex features and the number of these features grows with the complexity of the task. To keep the number of features tractable, model designers impose hard constraints on the possible interactions within the semantic or syntactic structures, such as conditioning on grandparents but not great-great-grandparents. Likewise, hard constrain"
J13-4006,kawahara-etal-2002-construction,0,0.0253574,"Missing"
J13-4006,D11-1140,0,0.0212437,"nt, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas"
J13-4006,P11-1112,0,0.0168707,"sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to"
J13-4006,P10-1113,0,0.0686563,"Missing"
J13-4006,D07-1072,0,0.0138803,"Missing"
J13-4006,C10-1081,0,0.00983689,"y 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical"
J13-4006,W08-2124,0,0.466359,"Missing"
J13-4006,P11-1023,0,0.0121846,"ey 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independent, as demonstrated by speakers’ ability to understand the meaning of ungrammatical text or speech and to assign grammatical categories and s"
J13-4006,J93-2004,0,0.048808,"Missing"
J13-4006,J08-2001,0,0.0213094,"Missing"
J13-4006,P05-1010,0,0.0303616,"Missing"
J13-4006,E06-1011,0,0.0289654,"er of the target string. The switching of elements of the semantic structure used in Wong and Mooney (2007) is more similar to the word reordering technique of Hajiˇcov´a et al. (2004) than to our Swap operation, because the reordering occurs before, rather than during, the derivation. The notion of planarity has been widely discussed in many works cited herein, and in the dependency parsing literature. Approaches to dealing with non-planar graphs belong to two conceptual groups: those that manipulate the graph, either by pre-processing or by post-processing (Hall and Novak 2005; McDonald and Pereira 2006), and those that adapt the algorithm to deal with non-planarity. Among the approaches that, like ours, devise an algorithm to deal with non-planarity, Yngve (1960) proposed a limited manipulation of registers to handle discontinuous constituents, which guaranteed that parsing/generation could be performed with a stack of very limited depth. An approach to non-planar parsing that is more similar to ours has been proposed in Attardi (2006). Attardi’s dependency parsing algorithm adds six new actions that allow this algorithm to parse any type of non-planar tree. Our Swap action is related to Att"
J13-4006,W08-2101,1,0.933675,"Missing"
J13-4006,J01-3003,1,0.121086,"orrelated. From a linguistic point of view, the assumption that syntactic distributions will be predictive of semantic role assignments is based on linking theory (Levin 1986). Linking theory assumes the existence of a ranking of semantic roles that are mapped by default on a ranking of grammatical functions and syntactic positions, and it attempts to predict the mapping of the underlying semantic component of a predicate’s meaning onto the syntactic structure. For example, Agents are always mapped in syntactically higher positions than Themes. Linking theory has been confirmed statistically (Merlo and Stevenson 2001). It is currently common to represent the syntactic and semantic role structures of a sentence in terms of dependencies, as illustrated in Figure 1. The complete graph of both the syntax and the semantics of the sentences is composed of two half graphs, which Figure 1 A semantic dependency graph labeled with semantic roles (lower half) paired with a syntactic dependency tree labeled with grammatical relations. 950 Henderson et al. Joint Syntactic and Semantic Parsing share all their vertices—namely, the words. Internally, these two half graphs exhibit different properties. The syntactic graph"
J13-4006,W04-2705,0,0.0149074,"y express consistent semantic roles across verbs, whereas arguments receiving an AM-X label are supposed to be adjuncts, and the roles they express are consistent across all verbs. A0 and A1 arguments are annotated based on the proto-role theory presented in Dowty (1991) and correspond to proto-agents and proto-patients, respectively. Although PropBank, unlike FrameNet, does not attempt to group different predicates evoking the same prototypical situation, it does distinguish between different senses of polysemous verbs, resulting in multiple framesets for such predicates. NomBank annotation (Meyers et al. 2004) extends the PropBank framework to annotate arguments of nouns. Only the subset of nouns that take arguments are annotated in NomBank and only a subset of the non-argument siblings of nouns are marked as ARG-M. The most notable specificity of NomBank is the use of support chains, marked as SU. Support chains are needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of"
J13-4006,A00-2030,0,0.0353462,"Missing"
J13-4006,R09-1051,0,0.195166,"Missing"
J13-4006,P07-1098,0,0.0109335,"oaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at least partly independen"
J13-4006,W05-1509,1,0.808134,"ub-problems needed to be solved to find a solution for these primary tasks, then one would expect an improvement from inducing shared representations. Multi-task learning methods have been shown to be beneficial in many domains, including natural language processing (Ando and Zhang 2005a, 2005b; Argyriou, Evgeniou, and Pontil 2006; Collobert and Weston 2008). Their application in the context of syntactic-semantic parsing has been very limited, however. The only other such successful multi-task learning approach we are aware of targets a similar, but more restricted, task of function labeling (Musillo and Merlo 2005). Musillo and Merlo (2005) conclusively show that jointly learning functional and syntactic information can significantly improve syntax. Our joint learning approach is an example of a multi-task learning approach in that the induced representations in the vectors of latent variables can capture hidden sub-problems relevant to predicting both syntactic and semantic structures. The rest of this article will first describe the data that are used in this work and their relevant properties. We then present our probabilistic model of joint syntactic parsing 953 Computational Linguistics Volume 39,"
J13-4006,N06-2026,1,0.911364,"Missing"
J13-4006,P08-2054,1,0.840744,"st, our ISBN latent variable models do not require heuristics to control the complexity of the augmented grammars or to search for predictive latent representations. Furthermore, probabilistic context-free grammars augmented with latent annotations do impose context-free independence assumptions between the latent labels, contrary to our models. Finally, our ISBN models have been successfully applied to both phrase-structure and dependency parsing. State-of-the-art results on unlexicalized dependency parsing have recently been achieved with latent variable probabilistic context-free grammars (Musillo and Merlo 2008; Musillo 2010). These latent variable grammars are compact and interpretable from a linguistic perspective, and they integrate grammar transforms that constrain the flow of latent information, thereby drastically limiting the space of latent annotations. For example, they encode the notion of X-bar projection in their constrained latent variables. 8. Conclusions and Future Work The proposed joint model achieves competitive performance on both syntactic and semantic dependency parsing for several languages. Our experiments also demonstrate the benefit of joint learning of syntax and semantics."
J13-4006,P09-1040,0,0.219321,"Missing"
J13-4006,W06-2933,0,0.0211291,"those used in previous dependency parsing work. No independence assumptions are made in the probability decomposition itself. This allows the probability estimation technique (discussed in Section 4) to make maximal use of its latent variables to learn correlations between the different parser actions, both within and between structures. 3.1 Synchronized Derivations We first specify the syntactic and semantic derivations separately, before specifying how they are synchronized in a joint generative model. The derivations for syntactic dependency trees are based on a shift-reduce style parser (Nivre et al. 2006; Titov and Henderson 2007d). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. A syntactic configuration of the parser is defined by the current stack, the queue of remaining input words, and the partial labeled dependency structure constructed by previous parser actions. The parser starts with an empty stack and terminates when it 957 Computational Linguistics Volume 39, Number 4 reaches a con"
J13-4006,W09-3811,0,0.0251302,"Missing"
J13-4006,P05-1013,0,0.029498,"Missing"
J13-4006,J05-1004,0,0.311193,"Missing"
J13-4006,P06-1055,0,0.0132839,"rchitecture to design a joint model of syntactic–semantic dependency parsing. In traditional fully supervised parsing models, designing a joint syntactic–semantic parsing model would require extensive feature engineering. These features pick out parts of the corpus annotation that are relevant to predicting other parts of the corpus annotation. If features are missing then predicting the annotation cannot be done accurately, and if there are too many features then the model cannot be learned accurately. Latent variable models, such as ISBNs and Latent PCFGs (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006), have the advantage that the model can induce new, more predictive, features by composing elementary features, or propagate information to include predictive but non-local features. These latent annotations are induced during learning, allowing the model to both predict them from other parts of the annotation and use them to predict the desired corpus annotation. In ISBNs, we use latent variables to induce features of the parse history D1 , . . . , Di−1 that are used to predict future parser decisions Di , . . . , Dm . The main difference between ISBNs and Latent PCFGs is that ISBNs have vect"
J13-4006,W05-1512,0,0.0551746,"Missing"
J13-4006,J08-2005,0,0.301645,"Missing"
J13-4006,P03-1002,0,0.0145641,"lying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dialogue systems (Basili et al. 2009; Van der Plas, Henderson, and Merlo 2009), automatic data generation (Gao and Vogel 2011; Van der Plas, Merlo, and Henderson 2011) and authorship attribution (Hedegaard and Simonsen 2011), among others. The recovery of the full meaning of text requires structured analyses of both its grammar and its semantics. These two forms of linguistic knowledge are usually thought to be at"
J13-4006,W08-2121,0,0.0315325,"Missing"
J13-4006,taule-etal-2008-ancora,0,0.0186112,"Missing"
J13-4006,P07-1080,1,0.904311,"re needed because nominal long distance dependencies are not captured under the Penn Treebank’s system of empty categories. They are used for all those cases in which the nominal argument is outside the noun phrase. For example, in a support verb construction, such as Mary took dozens of walks, the arcs linking walks to of , of to dozens, and dozens to took are all marked as support. The data we use for English are the output of an automatic process of conversion of the original PTB, PropBank, and NomBank into dependency structures, performed by the algorithm described in Johansson and Nugues (2007). These are the data provided to participants to the CoNLL-2008 and CoNLL-2009 shared tasks (http://ifarm.nl/signll/conll/). An example is shown in Figure 3. This representation encodes both the grammatical functions and the semantic labels that describe the sentence. Argument labels in PropBank and NomBank are assigned to constituents, as shown in Figure 2. After the conversion to dependency the PropBank and NomBank labels Figure 3 An example from the PropBank corpus of verbal predicates and their semantic roles (lower half) paired with syntactic dependencies derived from the Penn Treebank. 9"
J13-4006,D07-1099,1,0.919153,"Missing"
J13-4006,W07-2218,1,0.414946,"uences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for syntactic structure prediction (Henderson 2003), which has shown very good performance for both constituency (Titov and Henderson 2007a) and dependency parsing (Titov and Henderson 2007d). Instead of hand-crafting features of the previous parsing decisions, as is standard in history-based models, ISBNs estimate the probability of the next parsing actions conditioned on a vector of latent-variable features of the parsing history. These features are induced automatically to maximize the likelihood of the syntactic–semantics graphs given in the training set, and therefore they encode important correlations between syntactic and semantic decisions. This makes joint learning of syntax and semantics a crucial component of our appr"
J13-4006,P11-1145,1,0.410843,"yntactic dependency graph to a derivation sequence, and similar algorithms can be defined for mapping a semantic dependency graph to a derivation sequence, as discussed subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and t"
J13-4006,E12-1003,1,0.526699,"subsequently. But defining a joint syntactic–semantic derivation presents a challenge. Namely, given the complex nature of correspondences between the structures, it is not obvious how to synchronize individual semantic– syntactic steps in the derivation. Previous joint statistical models of dependency syntax and SRL have either ignored semantic arcs not corresponding to single syntactic arcs (Thompson, Levy, and Manning 2003; Titov and Klementiev 2011) or resorted to pre-/post-processing strategies that modify semantic or syntactic structures (Llu´ıs and M`arquez 2008; Lang and Lapata 2011; Titov and Klementiev 2012). In a constituency setting, Li, Zhou, and Ng (2010) explore different levels of coupling of syntax and semantics, and find that only explicit interleaving or explicit feature selection yield improvements in performance. Instead of synchronizing individual steps, we (1) decompose both the syntactic derivation and the semantic derivation into subsequences, where each subsequence corresponds to a single word in the sentence, and then (2) synchronize syntactic and semantic subsequences corresponding to the same word with each other. To decide which steps correspond to a given word, we use a simpl"
J13-4006,J08-2002,0,0.369439,"Missing"
J13-4006,D09-1088,0,0.0527065,"Missing"
J13-4006,N09-2032,1,0.888657,"Missing"
J13-4006,P11-2052,1,0.891556,"Missing"
J13-4006,N06-1056,0,0.0444389,"obvious 988 Henderson et al. Joint Syntactic and Semantic Parsing correlation. Li, Zhou, and Ng’s approach is also different from ours in that they do not attempt to induce common representations useful for both tasks or for many languages, and as such cannot be regarded as multi-task, nor as multilingual, learning. Synchronous grammars provide an elegant way to handle multiple levels of representation. They have received much attention because of their applications in syntaxbased statistical machine translation (Galley et al. 2004; Chiang 2005; Nesson and Shieber 2008) and semantic parsing (Wong and Mooney 2006, 2007). Results indicate that these techniques are among the best both in machine translation and in the database query domain. Our method differs from those techniques that use a synchronous grammar, because we do not rewrite pairs of synchronized non-terminals, but instead synchronize chunks of derivation sequences. This difference is in part motivated by the fact that the strings for our two structures are perfectly aligned (being the same string), so synchronizing on the chunks of derivations associated with individual words eliminates any further alignment issues. We have also proposed n"
J13-4006,P07-1121,0,0.099988,"emplification of its applicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and"
J13-4006,J97-3002,0,0.0302763,"bsequence forms a linguistically meaningful chunk in that it includes all the decisions about the arcs on the left side of the associated word, both its parents and its children. Thus, synchronizing the syntactic and semantic subsequences according to their associated word places together subsequences that are likely to be correlated. Note that such pairs of syntactic and semantic subsequences will, in general, have different numbers of steps on each side and these numbers of steps are, in general, unbounded. Therefore, instead of defining atomic synchronized rules as in synchronous grammars (Wu 1997; Chiang 2005), we resort to parametrized models that exploit the internal structure of the paired subsequences. This derivational, joint approach to handling these complex representations leads to a new proposal on how to learn them, which avoids extensive and complex feature engineering, as discussed in the following. 1.2 Joint Learning of Syntax and Semantics Our probabilistic model is learned using Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov 2010), a recent development of an early latent variable model 952 Henderson et al. Joint Syntactic and Semantic Parsing for synta"
J13-4006,N09-2004,0,0.0248142,"Missing"
J13-4006,C00-2137,0,0.0196533,"ntic measures in the table). For the CoNLL-2008 scores the predicate sense labeling includes predicate identification, but for the CoNLL-2009 scores predicate identification was given in the task input. The syntactic LAS and the semantic F1 are then averaged with equal weight to produce an overall score called Macro F1 .10 When we evaluate the impact of the Swap action on crossing arcs, we also calculate precision, recall, and F-measure on pairs of crossing arcs.11 In our experiments, the statistical significance levels we report are all computed using a stratified shuffling test (Cohen 1995; Yeh 2000) with 10,000 randomized trials. 6.1 Monolingual Experimental Set-up We start by describing the monolingual English experiments. We train and evaluate our English models on data provided for the CoNLL-2008 shared task on joint learning of syntactic and semantic dependencies. The data is derived by merging a dependency transformation of the Penn Treebank with PropBank and NomBank (Surdeanu et al. 2008). An illustrative example of the kind of labeled structures that we need to parse is given in Figure 3. Training, development, and test data follow the usual partition as sections 02–21, 24, and 23"
J13-4006,D07-1071,0,0.0291049,"pplicability to other problems where two independent, but related, representations are being learned. 1. Introduction Success in statistical syntactic parsing based on supervised techniques trained on a large corpus of syntactic trees—both constituency-based (Collins 1999; Charniak 2000; Henderson 2003) and dependency-based (McDonald 2006; Nivre 2006; Bohnet and Nivre 2012; Hatori et al. 2012)—has paved the way to applying statistical approaches to the more ambitious goals of recovering semantic representations, such as the logical form of a sentence (Ge and Mooney 2005; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Ge and Mooney 2009; Kwiatkowski et al. 2011) or learning the propositional argument-structure of its main predicates (Miller et al. 2000; Gildea and Jurafsky 2002; Carreras and M`arquez 2005; M`arquez et al. 2008; Li, Zhou, and Ng 2010). Moving towards a semantic level of representation of language and text has many potential applications in question answering and information extraction (Surdeanu et al. 2003; Moschitti et al. 2007), and has recently been argued to be useful in machine translation and its evaluation (Wu and Fung 2009; Liu and Gildea 2010; Lo and Wu 2011; Wu et al. 2011), dial"
J13-4006,W08-2127,0,0.19821,"Missing"
J13-4006,W04-2407,0,\N,Missing
J13-4006,J03-4003,0,\N,Missing
J13-4006,C98-1013,0,\N,Missing
J13-4006,P05-1001,0,\N,Missing
J13-4006,D07-1096,0,\N,Missing
J13-4006,W06-2303,1,\N,Missing
K15-1015,P04-1015,0,0.232772,"disambiguated, because of the independence assumptions with words beyond the lookahead or because of the difficulty of inferring from an unstructured lookahead, the correctness 146 and Henderson, 2007b). These sequences of actions provide us with positive examples. For discriminative training, we also need incorrect parses to act as the negative examples. In particular, we want negative examples that will allow us to optimize the pruning decisions made by the parser. To optimize the pruning decisions made by the parsing model, we use the parsing model itself to generate the negative examples (Collins and Roark, 2004). Using the current parameters of the model, we apply our search-based decoding strategy to find our current approximation to the highest scoring complete parse, which is the output of our current parsing model. If this parse differs from the correct one, then we train the parameters of all the correctness probabilities in each parse so as to increase the score of the correct parse and decrease the score of the incorrect output parse. By repeatedly decreasing the score of the incorrect best parse as the model parameters are learned, training will efficiently decreases the score of all incorrec"
K15-1015,P14-1129,0,0.0266408,"abeled data effectively. For example, the parameter vector for the feature word-on-top-of-stack is decomposed into the multiplication of a parameter vector representing the word and a parameter matrix representing top-of-stack. But sparsity is not always a problem, since the frequency of such features follows a power law distribution, so there are some very frequent feature combinations. Previous work has noticed that the vector-matrix multiplication of these frequent feature combinations takes most of the computation time during testing, so they cache these computations (Bengio et al., 2003; Devlin et al., 2014; Chen and Manning, 2014). We note that these computations also take most of the computation time during training, and that the abundance of data for these feature combinations removes the statistical motivation for decomposing them. We propose to treat the cached vectors for high frequency feature combinations as parameters in their own right, using them both during training and during testing. The Parsing Model In shift-reduce dependency parsing, at each step of the parse, the configuration of the parser consists of a stack S of words, the queue Q of words and the partial labeled dependency"
K15-1015,W04-2407,0,0.0171448,"Missing"
K15-1015,W06-2933,0,0.0188086,"Missing"
K15-1015,W08-2122,1,0.818333,"(Titov and Henderson, 2007b), MALT parser, MST parser, and the feed-forward neural network parser of (Chen and Manning, 2014) (“C&M”). All these models and our own models are trained only on the CoNLL 2009 syntactic data; they use no external word embeddings or other unsupervised training on additional data. This is one reason for choosing these models for comparison. In addition, the generative model (“Generative INN, large beam” in Table 1) was compared extensively to state-ofart parsers on various languages and tasks in previous work (Titov and Henderson, 2007b; Titov and Henderson, 2007a; Henderson et al., 2008). Therefore, here our objective is not repeating an extensive comparison to the available parsers. Table 1 shows the labeled and unlabeled accuracy of attachments for these models. The MALT and MST parser scores come from (Surdeanu and Manning, 2010), which compared different parsing models using CoNLL 2008 shared task dataset, which is the same as CoNLL 2009 for English syntactic parsing. The results for the generative INN with a large beam were taken from (Henderson and Titov, 2010), which uses an architecture with 80 hidden units. We replicate this setting for the other generative INN resul"
K15-1015,J13-4006,1,0.437575,"g. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 1 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The more accurate parsers also use a RNN architecture"
K15-1015,N03-1014,1,0.906424,"ster training and a form of backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 1 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shift-reduce parsing architecture so that they can use search based decoding algorithms with effective pruning strategies. The"
K15-1015,P13-1045,0,0.0438171,"o speed up parsing time by caching computations for frequent feature combinations, including during training, giving us both faster training and a form of backoff smoothing. The resulting parser is over 35 times faster than its generative counterpart with nearly the same accuracy, producing state-of-art dependency parsing results while requiring minimal feature engineering. 1 1.1 Neural Network Parsing Neural networks have also been popular in parsing. These models can be divided into those whose design are motivated mostly by inducing useful vector representations (e.g. (Socher et al., 2011; Socher et al., 2013; Collobert, 2011)), and those whose design are motivated mostly by efficient inference and decoding (e.g. (Henderson, 2003; Henderson and Titov, 2010; Henderson et al., 2013; Chen and Manning, 2014)). The first group of neural network parsers are all deep models, such as RNNs, which gives them the power to induce vector representations for complex linguistic structures without extensive feature engineering. However, decoding in these models can only be done accurately if they are used to rerank the best parse trees of another parser (Socher et al., 2013). The second group of parsers use a shi"
K15-1015,P04-1013,1,0.69286,"ary experiments applying this approach to our INN parser did not work as well as having local normalization of action decisions. We hypothesize that the main reason for this result is that updating on entire chunks does not provide a sufficiently focused training signal. With a locally normalized action score, such as softmax, increasing the score of the correct chunk has the effect of decreasing the score of all the incorrect actions at each individual action decision. This update is an approximation to a discriminative update on all incorrect parses that continue from an incorrect decision (Henderson, 2004). Another possible reason is that local normalization prevents one action’s score from dominating the score of the whole parse, as can happen with high frequency decisions. In general, this problem can not be solved just by using norm regularization on weights. where WHO is the weight matrix from hidden representations to the outputs. 3 Discrimination of Partial Parses Unlike in a generative model, the above formulas for computing the probability of a tree make independence assumptions in that words to the right of wat k are assumed to be independent of Dt . And even for words in the lookahead"
K15-1015,D07-1099,1,0.923554,"results in language modeling (Mikolov et al., 2010), language 142 Proceedings of the 19th Conference on Computational Language Learning, pages 142–152, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics den vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just locality in the derivation sequence (Henderson, 2003; Titov and Henderson, 2007b). As in all deep neural network architectures, this chaining of nonlinear vector computations gives the model a very powerful mechanism to induce complex features from combinations of features in the history, which is difficult to replicate with handcoded features. est for inducing vector representations of complex linguistic structures. 1.2 Incremental Recurrent Neural Network Architecture In both approaches to neural network parsing, RNN models have the advantage that they need minimal feature engineering and therefore they can be used with little effort for a variety of languages and appl"
K15-1015,W07-2218,1,0.94987,"results in language modeling (Mikolov et al., 2010), language 142 Proceedings of the 19th Conference on Computational Language Learning, pages 142–152, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics den vector from previous decisions based on the partial parse structure that has been built by the previous decisions. So any information about the unbounded parse history can potentially be passed to the current decision through a chain of hidden vectors that reflects locality in the parse tree, and not just locality in the derivation sequence (Henderson, 2003; Titov and Henderson, 2007b). As in all deep neural network architectures, this chaining of nonlinear vector computations gives the model a very powerful mechanism to induce complex features from combinations of features in the history, which is difficult to replicate with handcoded features. est for inducing vector representations of complex linguistic structures. 1.2 Incremental Recurrent Neural Network Architecture In both approaches to neural network parsing, RNN models have the advantage that they need minimal feature engineering and therefore they can be used with little effort for a variety of languages and appl"
K15-1015,J11-1005,0,0.0600085,"lows us to use lookahead instead of word prediction. As mentioned above, generative word prediction is costly, both to compute and because it requires larger beams to be effective. With lookahead, it is possible to condition on words that are farther ahead in the string, and thereby avoid hypothesizing parses that are incompatible with those future words. This allows the parser to prune much more aggressively without losing accuracy. Discriminative learning further improves this aggressive pruning, because it can optimize for the discrete choice of whether to prune or not (Huang et al., 2012; Zhang and Clark, 2011). Our proposed model primarily differs from previous discriminative models of shift-reduce dependency parsing in the nature of the discriminative choices that are made and the way these decisions are modeled and learned. Rather than learning to make pruning decisions at each parse action, we learn to choose between sequences of actions that occur in between two shift actions. This way of grouping action sequences into chunks associated with each word has been used previously for efficient pruning strategies in generative parsing (Henderson, 2003), and for synchronizing syntactic parsing and se"
K15-1015,N10-1091,0,\N,Missing
K15-1015,W09-1201,0,\N,Missing
K15-1015,D14-1082,0,\N,Missing
K15-1015,N12-1015,0,\N,Missing
K17-3024,N03-1014,1,0.73595,"Missing"
K17-3024,W07-2218,1,0.840147,"roductory overview (Zeman et al., 2017). This task makes true cross-linguistic comparison possible thanks to the universal dependency annotation project, which underlies the data used in this shared task. We train exactly the same parsing model on every language, thereby allowing further comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in"
K17-3024,K15-1015,1,0.902265,"comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in fact improve performance over “traditional” recurrent neural networks. We are listed in the table of results as the CLCL (Geneva) entry. As with previous work using the Incremental Neural Network architecture (e.g. Henderson, 2003), the main philosophy of our submis2 Data We use only the p"
K17-3024,P81-1022,0,0.728836,"Missing"
K17-3024,K17-3001,0,0.0586965,"Missing"
K17-3024,L16-1680,0,0.104119,"Missing"
K17-3024,D07-1099,1,0.763831,"roductory overview (Zeman et al., 2017). This task makes true cross-linguistic comparison possible thanks to the universal dependency annotation project, which underlies the data used in this shared task. We train exactly the same parsing model on every language, thereby allowing further comparisons. In addition, the feature induction abilities of the recurrent neural network help minimise any remaining cross-lingual differences due to pre-processing or annotation. Introduction The system described in this paper is the grandchild of the first transition-based neural network dependency parser (Titov and Henderson, 2007b), which was the University of Geneva’s entry in the CoNLL 2007 multilingual dependency parsing shared task (Titov and Henderson, 2007a). The system has undergone some developments and modifications, in particular the faster discriminative version introduced by Yazdani and Henderson (2015), but in many respects the design and implementation of this parser is unchanged since 2007. One of our motivations for our submission to this CoNLL 2017 multilingual dependency parsing shared task is to provide a baseline to evaluate to what extent recent advances in neural network models and training do in"
N03-1014,P01-1010,0,0.0157799,"Missing"
N03-1014,A00-2018,0,0.886252,"es requires choosing between an unbounded (or even infinite) number of possible phrase structure trees. The standard approach to this problem is to decompose this choice into an unbounded sequence of choices between a finite number of possible parser actions. This sequence is the parse for the phrase structure tree. We can then define a probabilistic model of phrase structure trees by defining a probabilistic model of each parser action in its parse context, and apply machine learning techniques to learn this model of parser actions. Many statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000) are based on a history-based model of parser actions. In these models, the probability of each parser action is conditioned on the history of previous actions in the parse. But here again we are faced with an unusual situation for machine learning problems, conditioning on an unbounded amount of information. A major challenge in designing a history-based statistical parser is choosing a finite representation of the unbounded parse history from which the probability of the next parser action can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to rep"
N03-1014,P02-1034,0,0.0370283,"d-crafted set of features to represent the derivation history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). Ratnaparkhi (1999) defines a very general set of features for the histories of a shift-reduce parsing model, but the results are not as good as models which use a more linguistically informed set of features for a top-down parsing model (Collins, 1999; Charniak, 2000). In addition to the method proposed in this paper, another alternative to choosing a finite set of features is to use kernel methods, which can handle unbounded feature sets. However, this causes efficiency problems. Collins and Duffy (2002) define a kernel over parse trees and apply it to re-ranking the output of a parser, but the resulting feature space is restricted by the need to compute the kernel efficiently, and the results are not as good as Collins’ previous work on re-ranking using a finite set of features (Collins, 2000). Future work could use the induced history representations from our work to define efficiently computable tree kernels. The only other broad coverage neural network parser (Costa et al., 2001) also uses a neural network architecture which is specifically designed for processing structures. We believe t"
N03-1014,2000.iwpt-1.14,1,0.923767,"al parser is choosing a finite representation of the unbounded parse history from which the probability of the next parser action can be accurately estimated. Previous approaches have used a hand-crafted finite set of features to represent the parse history (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000). In the work presented here, we automatically induce a finite set of real valued features to represent the parse history. We perform the induction of a history representation using an artificial neural network architecture, called Simple Synchrony Networks (SSNs) (Lane and Henderson, 2001; Henderson, 2000). This machine learning method is specifically designed for processing unbounded structures. It allows us to avoid making a priori independence assumptions, unlike with hand-crafted history features. But it also allows us to make use of our a priori knowledge by imposing structurally specified and linguistically appropriate biases on the search for a good history representation. The combination of automatic feature induction and linguistically appropriate biases results in a history-based parser with state-of-the-art performance. When trained on just part-of-speech tags, the resulting parser a"
N03-1014,W03-3011,1,0.765012,"sting set averaged around 30 seconds per sentence for SSN-Tags, 1 minute per sentence for SSN-Freq 200, and 2 minutes per sentence for SSN-Freq 20 (on a 502 MHz Sun Blade computer, average 22.5 words per sentence). But by reducing the number of alternatives considered in the search for the most probable parse, we can greatly increase parsing speed without much loss in accuracy. With the SSN-Freq 200 model, accuracy slightly better than (Collins, 1999) can be achieved at 2.7 seconds per sentence, and accuracy slightly better than (Ratnaparkhi, 1999) can be achieved at 0.5 seconds per sentence (Henderson, 2003) (on validation sentences at most 100 words long, average 23.3 words per sentence).     about lexical heads. There is also a decrease in performance when the left-corner child is represented with just its label (lc-child label). This implies that the first child does tend to carry information which is relevant throughout the sub-derivation for the node, and suggests that this child deserves a special status in a history representation. Interestingly, a smaller, although still substantial, degradation occurs when the previous history representation for the same node is replaced with its nod"
N03-1014,J98-4004,0,0.0201868,"Missing"
N03-1014,1997.iwpt-1.18,0,0.110405,"Missing"
N03-1014,J93-2004,0,0.049322,"Missing"
N03-1014,W96-0213,0,0.0436597,"ion results and our previous experience with networks similar to the models SSN-Tags and SSN-Freq 200. We trained two or three networks for each of the three vocabulary sizes and chose the best ones based on their validation performance. Training times vary but are long, being around 4 days for a SSN-Tags model, 6 days for a SSN-Freq 200 model, and 10 days for a SSN-Freq 20 model (on a 502 MHz Sun Blade computer). We then tested the best models for each vocabulary size on the testing set.4 Standard measures of performance are shown in table 1.5      3 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags used in these experiments, rather than the handcorrected tags which come with the corpus. 4 All the best networks had 80 hidden units. Weight decay regularization was applied at the beginning of training but reduced to 0 by the end of training. Training was stopped when maximum performance was reached on the validation set, using a post-word beam width of 5. 5 All our results are computed with the evalb program following the standard criteria in (Collins, 1999), and using the standard training (sections 2–22, 39,832 sentences), validation (section 24, 1346 sentence), and t"
N03-1014,P99-1054,0,0.0297802,"Missing"
N03-1014,P80-1024,0,0.298019,"Missing"
N03-1014,J03-4003,0,\N,Missing
N09-2032,N01-1016,0,0.0396628,"NAACL HLT 2009: Short Papers, pages 125–128, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ous work in domain adaptation by Foster (2007), where similar proportions of ungrammatical and grammatical data are combined to train a parser on ungrammatical written text, and by Weilhammer et al. (2006), who use interpolation between two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings."
N09-2032,N03-1014,1,0.87695,"ith and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse hist"
N09-2032,E06-2009,1,0.81171,"etween two separately trained models, one on an artificial corpus of user utterances generated by a hand-coded domain-specific grammar and one on available corpora. Whereas much previous work on parsing speech has focused on speech repairs, e.g. Charniak and Johnson (2001), we focus on parsing NSUs. 2 The first component: a model of NSUs To construct a model of NSUs we studied a subset of the data under consideration: TownInfo. This small corpus of transcribed spoken human-machine dialogues in the domain of hotel/restaurant/bar search is gathered using the TownInfo tourist information system (Lemon et al., 2006). The NSUs we find in our data are mainly of the type answers, according to the classification given in Fern´andez (2006). More specifically, we find short answers, plain and repeated affirmative answers, plain and helpful rejections, but also greetings. Current linguistic theory provides several approaches to dealing with NSUs (Merchant, 2004; Progovac et al., 2006; Fern´andez, 2006). Following the linguistic analysis of NSUs as non-sentential small clauses (Progovac et al., 2006) that do not have tense or agreement functional nodes, we make the assumption that they are phrasal projections. T"
N09-2032,J93-2004,0,0.0347848,"Missing"
N09-2032,W08-2101,1,0.855716,"ated NSUs except for VPs and modifiers. 5 augmented model: One with and one without the target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent"
N09-2032,J05-1004,0,0.0274982,"ems. 3 Because NSUs can be interpreted only in context, the same NSU can correspond to several syntactic categories: South for example, can be an noun, an adverb, or an adjective. In case of ambiguity, we divided the score up for the several possible tags. This accounts for the fractional counts. Category NP JJ PP NN VP # Occ. 19.0 12.7 12.0 11.7 11.0 Perc. 15.2 10.1 9.6 9.3 8.8 Category RB DT CD Total frag. Full sents # Occ. 1.7 1.0 1.0 70.0 55.0 Perc. 1.3 0.8 0.8 56.0 44.0 Table 1: Distribution of types of NSUs and full sentences in the TownInfo development set. merged with PropBank labels (Palmer et al., 2005). We included all the sentences from this dataset in our artificial corpus, giving us 39,832 full sentences. In accordance with the target distribution we added 50,699 NSUs extracted from the same dataset. We sampled NSUs according to the distribution given in Table 1. After the extraction we added a root FRAG node to the extracted NSUs4 and we capitalised the first letter of each NSU to form an utterance. There are two additional pre-processing steps. First, for some types of NSUs maximal projections are added. For example, in the subset from the target source we saw many occurrences of nouns"
N09-2032,P07-1080,1,0.883131,"he target component.5 These models were tested on two test sets: a small corpus of 150 transcribed utterances taken from the TownInfo corpus, annotated with gold syntactic and semantic annotation by two of the authors6 : the TownInfo test set. The second test set is used to compare the performance of the parser on WSJ-style sentences and consists of section 23 of the merged Penn Treebank/Propbank corpus. We will refer to this test set as the non-augmented test set. 5.1 The parsing model is the one proposed in Merlo and Musillo (2008), which extends the syntactic parser of Henderson (2003) and Titov and Henderson (2007) with annotations which identify semantic role labels, and has competitive performance. The parser uses a generative history-based probability model for a binarised left-corner derivation. The probabilities of derivation decisions are modelled using the neural network approximation (Henderson, 2003) to a type of dynamic Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007). The ISBN models the derivation history with a vector of binary latent variables. These latent variables learn to represent features of the parse history which are useful for making"
N09-2032,D07-1096,0,\N,Missing
N13-1094,N10-1017,0,0.0617595,"Missing"
N13-1094,D08-1106,0,0.0617667,"relation extraction is based on iterative bootstrapping (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006; Pantel and Pennacchiotti, 2006), which can be applied with only small amounts of supervision and which scales well to very large datasets. A well-known problem with iterative bootstrapping is a phenomenon known as semantic drift (Curran et al., 2007): as bootstrapping proceeds it is likely that unreliable patterns will lead to false extractions. These extraction errors are amplified in the following iterations and the extracted relation will drift away However, the analysis of Komachi et al. (2008) has shown that semantic drift is an inherent property of iterative bootstrapping algorithms and therefore poses a fundamental problem. They have shown that iterative bootstrapping without pruning corresponds to an eigenvector computation and thus as the number of iterations increases the resulting ranking will always converge towards the same static ranking of tuples, regardless of the particular choice of seed instances. In this paper, we describe an alternative method, that is not susceptible to semantic drift. We represent our data as a bipartite graph, whose vertices correspond to pattern"
N13-1094,P09-1045,0,0.0176507,"son@xrce.xerox.com Joel Lang University of Geneva 7 Route de Drize 1227 Carouge, Switzerland joel.lang@unige.ch Abstract from the intended target. Semantic drift often results in low precision extractions and therefore poses a major limitation of iterative bootstrapping algorithms. Previous work on iterative bootstrapping has addressed the issue of reducing semantic drift for example by bagging the results of various runs employing differing seed tuples, constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process (T. McIntosh, 2010; McIntosh and Curran, 2009; Curran et al., 2007). Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision. Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions. This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph. In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results i"
N13-1094,D10-1035,0,0.0189224,"nce james.henderson@xrce.xerox.com Joel Lang University of Geneva 7 Route de Drize 1227 Carouge, Switzerland joel.lang@unige.ch Abstract from the intended target. Semantic drift often results in low precision extractions and therefore poses a major limitation of iterative bootstrapping algorithms. Previous work on iterative bootstrapping has addressed the issue of reducing semantic drift for example by bagging the results of various runs employing differing seed tuples, constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process (T. McIntosh, 2010; McIntosh and Curran, 2009; Curran et al., 2007). Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision. Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions. This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph. In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, a"
N13-1094,P06-1015,0,\N,Missing
P04-1013,E03-1005,0,0.0310584,"ollins99 88.1 Collins&Duffy02 88.6 Charniak00 89.6 Collins00 89.6 DGSSN-Freq≥20, rerank 89.8 Bod03 90.7 LP Fβ=1∗ 87.5 86.9 88.3 88.2 88.9 88.7 89.5 89.5 89.9 89.7 90.4 90.1 90.8 90.7 * Fβ=1 for previous models may have rounding errors. Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (Fβ=1 ) on the entire testing set. For comparison to previous results, table 2 lists the results for our best model (DGSSNFreq≥20, rerank)9 and several other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Collins and Duffy, 2002; Charniak, 2000; Collins, 2000; Bod, 2003) on the entire testing set. Our best performing model is more accurate than all these previous models except (Bod, 2003). This DGSSN parser achieves this result using much less lexical knowledge than other approaches, which mostly use at least the words which occur at least 5 times, plus morphological features of the remaining words. However, the fact that the DGSSN uses a large-vocabulary tagger (Ratnaparkhi, 1996) as a preprocessing stage may compensate for its smaller vocabulary. Also, the main reason for using a smaller vocabulary is the computational complexity of computing probabilities"
P04-1013,A00-2018,0,0.306361,"mponents, a probability model for sequences of parser decisions, a Simple Synchrony Network which estimates the parameters of the probability model, and a procedure which searches for the most probable parse given these parameter estimates. This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a). We also present the training methods, and experiments on the proposed parsing models. 2 Two History-Based Probability Models As with many previous statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), we use a history-based model of parsing. Designing a history-based model of parsing involves two steps, first choosing a mapping from the set of phrase structure trees to the set of parses, and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse. We use the same mapping for both our probability models, but we use two different ways of conditioning the probabilities, one generative and one discriminative. As we will show in section 6, these two different ways of parameterizing the probability model"
P04-1013,P02-1034,0,0.129255,"sentences of length at most 100. LR Ratnaparkhi99 86.3 Collins99 88.1 Collins&Duffy02 88.6 Charniak00 89.6 Collins00 89.6 DGSSN-Freq≥20, rerank 89.8 Bod03 90.7 LP Fβ=1∗ 87.5 86.9 88.3 88.2 88.9 88.7 89.5 89.5 89.9 89.7 90.4 90.1 90.8 90.7 * Fβ=1 for previous models may have rounding errors. Table 2: Percentage labeled constituent recall (LR), precision (LP), and a combination of both (Fβ=1 ) on the entire testing set. For comparison to previous results, table 2 lists the results for our best model (DGSSNFreq≥20, rerank)9 and several other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Collins and Duffy, 2002; Charniak, 2000; Collins, 2000; Bod, 2003) on the entire testing set. Our best performing model is more accurate than all these previous models except (Bod, 2003). This DGSSN parser achieves this result using much less lexical knowledge than other approaches, which mostly use at least the words which occur at least 5 times, plus morphological features of the remaining words. However, the fact that the DGSSN uses a large-vocabulary tagger (Ratnaparkhi, 1996) as a preprocessing stage may compensate for its smaller vocabulary. Also, the main reason for using a smaller vocabulary is the computati"
P04-1013,W03-3011,1,0.841467,"g one training criteria over another. By inducing the history representations specifically to fit the chosen model and training criteria, we avoid having to choose independence assumptions which might bias our results. Each complete parsing system we propose consists of three components, a probability model for sequences of parser decisions, a Simple Synchrony Network which estimates the parameters of the probability model, and a procedure which searches for the most probable parse given these parameter estimates. This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a). We also present the training methods, and experiments on the proposed parsing models. 2 Two History-Based Probability Models As with many previous statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), we use a history-based model of parsing. Designing a history-based model of parsing involves two steps, first choosing a mapping from the set of phrase structure trees to the set of parses, and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous"
P04-1013,N03-1014,1,0.697312,"g one training criteria over another. By inducing the history representations specifically to fit the chosen model and training criteria, we avoid having to choose independence assumptions which might bias our results. Each complete parsing system we propose consists of three components, a probability model for sequences of parser decisions, a Simple Synchrony Network which estimates the parameters of the probability model, and a procedure which searches for the most probable parse given these parameter estimates. This paper outlines each of these components, but more details can be found in (Henderson, 2003b), and, for the discriminative model, in (Henderson, 2003a). We also present the training methods, and experiments on the proposed parsing models. 2 Two History-Based Probability Models As with many previous statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), we use a history-based model of parsing. Designing a history-based model of parsing involves two steps, first choosing a mapping from the set of phrase structure trees to the set of parses, and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous"
P04-1013,P01-1042,0,0.0615843,"ameter. The second probability model is discriminative, because it specifies the conditional probability of the output tree given the input sentence. More generally, discriminative models try to maximize this conditional probability, but often do not actually calculate the probability, as with Support Vector Machines (Vapnik, 1995). We take the approach of actually calculating an estimate of the conditional probability because it differs minimally from the generative probability model. In this form, the distinction between our two models is sometimes referred to as “joint versus conditional” (Johnson, 2001; Klein and Manning, 2002) rather than “generative versus discriminative” (Ng and Jordan, 2002). As with the generative model, we use the chain rule to decompose the entire conditional probability into a sequence of probabilities for individual parser decisions, where yield(dj ,..., dk ) is the sequence of words wi from the shift(wi ) actions in dj ,..., dk . P (d1 ,..., dm |yield(d1 ,..., dm )) = Πi P (di |d1 ,..., di−1 , yield(di ,..., dm )) Note that d1 ,..., di−1 specifies yield(d1 ,..., di−1 ), so it is sufficient to only add yield(d i ,..., dm ) to the conditional in order for the entire"
P04-1013,W02-1002,0,0.030892,"ond probability model is discriminative, because it specifies the conditional probability of the output tree given the input sentence. More generally, discriminative models try to maximize this conditional probability, but often do not actually calculate the probability, as with Support Vector Machines (Vapnik, 1995). We take the approach of actually calculating an estimate of the conditional probability because it differs minimally from the generative probability model. In this form, the distinction between our two models is sometimes referred to as “joint versus conditional” (Johnson, 2001; Klein and Manning, 2002) rather than “generative versus discriminative” (Ng and Jordan, 2002). As with the generative model, we use the chain rule to decompose the entire conditional probability into a sequence of probabilities for individual parser decisions, where yield(dj ,..., dk ) is the sequence of words wi from the shift(wi ) actions in dj ,..., dk . P (d1 ,..., dm |yield(d1 ,..., dm )) = Πi P (di |d1 ,..., di−1 , yield(di ,..., dm )) Note that d1 ,..., di−1 specifies yield(d1 ,..., di−1 ), so it is sufficient to only add yield(d i ,..., dm ) to the conditional in order for the entire input sentence to be incl"
P04-1013,J93-2004,0,0.0368534,"for d1 ,..., di . In general, the partial parse with the highest probability is chosen as the next one to be extended, but to perform the search efficiently it is necessary to prune the search space. The main pruning is that only a fixed number of the most probable derivations are allowed to continue past the shifting of each word. Setting this post-word beam width to 5 achieves fast parsing with reasonable performance in all models. For the parsers with generative probability models, maximum accuracy is achieved with a post-word beam width of 100. 6 The Experiments We used the Penn Treebank (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs. 5 5 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. For each tag, there is an We report results for three different vocabulary sizes, varying in the frequency with which tagword pairs must occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs, a threshold of 20 resulted in 4215 tag-word pairs, and a threshold of 5 resulted in 11,9"
P04-1013,W96-0213,0,0.0779485,". The main pruning is that only a fixed number of the most probable derivations are allowed to continue past the shifting of each word. Setting this post-word beam width to 5 achieves fast parsing with reasonable performance in all models. For the parsers with generative probability models, maximum accuracy is achieved with a post-word beam width of 100. 6 The Experiments We used the Penn Treebank (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs. 5 5 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. For each tag, there is an We report results for three different vocabulary sizes, varying in the frequency with which tagword pairs must occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs, a threshold of 20 resulted in 4215 tag-word pairs, and a threshold of 5 resulted in 11,993 tag-word pairs For the generative model we trained networks for the 508 (“GSSN-Freq≥200”) and 4215 (“GSSN-Freq≥20”) word vocabularies. The need to calculate word predictions makes training time"
P04-1013,P80-1024,0,0.198742,"parses, and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse. We use the same mapping for both our probability models, but we use two different ways of conditioning the probabilities, one generative and one discriminative. As we will show in section 6, these two different ways of parameterizing the probability model have a big impact on the ease with which the parameters can be estimated. To define the mapping from phrase structure trees to parses, we use a form of left-corner parsing strategy (Rosenkrantz and Lewis, 1970). In a left-corner parse, each node is introduced after the subtree rooted at the node’s first child has been fully parsed. Then the subtrees for the node’s remaining children are parsed in their left-to-right order. Parsing a constituent starts by pushing the leftmost word w of the constituent onto the stack with a shift(w) action. Parsing a constituent ends by either introducing the constituent’s parent nonterminal (labeled Y ) with a project(Y) action, or attaching to the parent with an attach action.1 A complete parse consists of a sequence of these actions, d 1 ,..., dm , such that perfor"
P04-1013,J03-4003,0,\N,Missing
P05-1023,E03-1005,0,0.023571,"Missing"
P05-1023,A00-2018,0,0.0254505,". These weights define a normalized exponential model, with the network’s hidden layer as the input features. When we tried using the complete set of weights in some small scale experiments, training the classifier was more computationally expensive, and actually performed slightly worse than just using the output weights. Using just the output weights also allows us to make some approximations in the TOP reranking kernel which makes the classifier learning algorithm more efficient. 3.1 A History-Based Probability Model As with many other statistical parsers (Ratnaparkhi, 1999; Collins, 1999; Charniak, 2000), Henderson (2003) uses a history-based model of parsing. He defines the mapping from phrase structure trees to parse sequences using a form of left-corner parsing strategy (see (Henderson, 2003) for more details). The parser actions include: introducing a new constituent with a specified label, attaching one constituent to another, and predicting the next word of the sentence. A complete parse consists of a sequence of these actions, d1 ,..., dm , such that performing d1 ,..., dm results in a complete phrase structure tree. Because this mapping to parse sequences is one-to-one, and the word p"
P05-1023,P02-1034,0,0.592804,"ize measures related directly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected"
P05-1023,P04-1015,0,0.0403825,". “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model, not directly in the kern"
P05-1023,N03-1014,1,0.783763,"g. 1 For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel is then used with the Voted Perceptron algorithm (Freund and Schapire, 1998) to reranking the top 20 parses from the probabilistic model. This method achieves a significant improvement over the accuracy of the probabilistic model alone. 2 Kernels Derived from Probabilistic Models In recent years, several methods have been proposed for constructing kernels from trained probabilistic models. As usual, these kernels are then used with linear classifiers to learn the desired task. As well as some empirical successes, these methods are motivated by theoretical results which s"
P05-1023,P04-1013,1,0.825686,"tly well understood, both in terms of reflecting generalizations and controlling computational cost. Because many NLP problems are unbounded in size and complexity, it is hard to specify all possible relevant kernel features without having so many features that the computations become intractable and/or the data becomes too sparse.1 Second, the kernel is defined using the trained parameters of the probabilistic model. Thus the kernel is in part determined by the training data, and is automatically tailored to reflect properties of parse trees which are relevant to parsing. 1 For example, see (Henderson, 2004) for a discussion of why generative models are better than models parameterized to estimate the a posteriori probability directly. 181 Proceedings of the 43rd Annual Meeting of the ACL, pages 181–188, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics In this paper, we propose a new method for deriving a kernel from a probabilistic model which is specifically tailored to reranking tasks, and we apply this method to natural language parsing. For the probabilistic model, we use a state-of-the-art neural network based statistical parser (Henderson, 2003). The resulting kernel"
P05-1023,J93-2004,0,0.0296727,"mputation and the parameters of the mappings h(d1 ,..., di−1 ). With multilayered networks such as SSNs, this training is not guaranteed to converge to a global optimum, but in practice a network whose criteria value is close to the optimum can be found. 4 Large-Margin Optimization Once we have defined a kernel over parse trees, general techniques for linear classifier optimization can be used to learn the given task. The most sophisticated of these techniques (such as Support Vector Machines) are unfortunately too computationally expensive to be used on large datasets like the Penn Treebank (Marcus et al., 1993). Instead we use a method which has often been shown to be virtually as good, the Voted Perceptron (VP) (Freund and Schapire, 1998) algorithm. The VP algorithm was originally applied to parse reranking in (Collins and Duffy, 2002) with the Tree kernel. We modify the perceptron training algorithm to make it more suitable for parsing, where zero-one classification loss is not the evaluation measure usually employed. We also develop a variant of the kernel defined in section 2.3, which is more efficient when used with the VP algorithm. Given a list of candidate trees, we train the classifier to s"
P05-1023,W96-0213,0,0.182419,"are present in the candidate derivation for the sentence, and thus in the first vector component. We have applied this technique to the TOP reranking kernel, the result of which we will call the efficient TOP reranking kernel. 5 The Experimental Results We used the Penn Treebank WSJ corpus (Marcus et al., 1993) to perform empirical experiments on the proposed parsing models. In each case the input to the network is a sequence of tag-word pairs.2 We report results for two different vocabulary sizes, varying in the frequency with which tag-word pairs must 2 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. occur in the training set in order to be included explicitly in the vocabulary. A frequency threshold of 200 resulted in a vocabulary of 508 tag-word pairs (including tag-unknown word pairs) and a threshold of 20 resulted in 4215 tag-word pairs. We denote the probabilistic model trained with the vocabulary of 508 by the SSN-Freq≥200, the model trained with the vocabulary of 4215 by the SSN-Freq≥20. Testing the probabilistic parser requires using a beam search through the space of possible parses. We used a form of beam search which prunes the search after the prediction o"
P05-1023,W03-0402,0,0.551335,"ctly to expected testing performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the"
P05-1023,W03-1012,0,0.0588209,"ng performance (i.e. “large margin” methods), rather than the probabilistic measures used in statistical models, which are only indirectly related to expected testing performance. Work on kernel methods in natural Ivan Titov Department of Computer Science University of Geneva 24, rue G´en´eral Dufour CH-1211 Gen`eve 4, Switzerland ivan.titov@cui.unige.ch language has focussed on the definition of appropriate kernels for natural language tasks. In particular, most of the work on parsing with kernel methods has focussed on kernels over parse trees (Collins and Duffy, 2002; Shen and Joshi, 2003; Shen et al., 2003; Collins and Roark, 2004). These kernels have all been hand-crafted to try reflect properties of parse trees which are relevant to discriminating correct parse trees from incorrect ones, while at the same time maintaining the tractability of learning. Some work in machine learning has taken an alternative approach to defining kernels, where the kernel is derived from a probabilistic model of the task (Jaakkola and Haussler, 1998; Tsuda et al., 2002). This way of defining kernels has two advantages. First, linguistic knowledge about parsing is reflected in the design of the probabilistic model"
P05-1023,W04-3201,0,0.0508062,"he position in the list ordered by the F1 score. We expect that an improvement could be achieved by combining our approach of scaling updates by the F1 loss with the all pairs approach of (Shen and Joshi, 2004). Use of the F1 loss function during training demonstrated 187 better performance comparing to the 0-1 loss function when applied to a structured classification task (Tsochantaridis et al., 2004). All the described kernel methods are limited to the reranking of candidates from an existing parser due to the complexity of finding the best parse given a kernel (i.e. the decoding problem). (Taskar et al., 2004) suggested a method for maximal margin parsing which employs the dynamic programming approach to decoding and parameter estimation problems. The efficiency of dynamic programming means that the entire space of parses can be considered, not just a candidate list. However, not all kernels are suitable for this method. The dynamic programming approach requires the feature vector of a tree to be decomposable into a sum over parts of the tree. In particular, this is impossible with the TOP and Fisher kernels derived from the SSN model. Also, it isn’t clear whether the algorithm remains tractable fo"
P05-1023,C00-2137,0,0.222983,"Missing"
P05-1023,J03-4003,0,\N,Missing
P07-1080,P05-1010,0,0.0330288,"ite accurate parsing models. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured signific"
P07-1080,P05-1022,0,0.0572489,"ion of the graph, as would be necessary for an undirected graphical model. 634 Figure 1: Illustration of an ISBN. 3 The Probabilistic Model of Parsing In this section we present our framework for syntactic parsing with dynamic Sigmoid Belief Networks. We first specify the form of SBN we propose, namely ISBNs, and then two methods for approximating the inference problems required for parsing. We only consider generative models of parsing, since generative probability models are simpler and we are focused on probability estimation, not decision making. Although the most accurate parsing models (Charniak and Johnson, 2005; Henderson, 2004; Collins, 2000) are discriminative, all the most accurate discriminative models make use of a generative model. More accurate generative models should make the discriminative models which use them more accurate as well. Also, there are some applications, such as language modeling, which require generative models. 3.1 The Graphical Model In ISBNs, we use a history-based model, which decomposes the probability of the parse as: P (T ) = P (D 1 , ..., Dm ) = Y P (Dt |D1 , . . . , Dt−1 ), t where T is the parse tree and D 1 , . . . , Dm is its equivalent sequence of parser decisio"
P07-1080,A00-2018,0,0.928079,"e. However, they can be approximated sufficiently well to build fast and accurate statistical parsers which induce features during training. We use SBNs in a generative history-based model of constituent structure parsing. The probability of an unbounded structure is decomposed into a sequence of probabilities for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the"
P07-1080,W96-0213,0,0.338312,"Missing"
P07-1080,P02-1035,0,0.0191114,"els. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experim"
P07-1080,P05-1023,1,0.877504,"Missing"
P07-1080,N03-1014,1,0.518164,"for individual derivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treat"
P07-1080,N03-1028,0,0.0548333,"as considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). 638 tional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in section 2.2, undirected graphical models do not seem to be suitable for historybased full parsing models. Sigmoid Belief Networks were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. 6 Conclusions This paper proposes a new gen"
P07-1080,P04-1013,1,0.948169,"rivation decisions, each decision conditioned on the unbounded history of previous decisions. The most common approach to handling the unbounded nature of the histories is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. Another previous approach is to use neural networks to compute a compressed representation of the history and condition decisions on this representation (Henderson, 2003; Henderson, 2004). It is possible that an unbounded amount of information is encoded in the compressed representation via its continuous values, but it is not clear whether this is actually happening due to the lack of any principled interpretation for these continuous values. Like the former approach, we assume that there are a finite set of features which encode the relevant information about the parse history. But unlike that approach, we allow feature values to be ambiguous, and represent each feature as a distribution over (binary) values. In other words, these history features are treated as latent varia"
P07-1080,W04-3201,0,0.0503689,"Missing"
P07-1080,P06-1110,0,0.0245139,"Missing"
P07-1080,C00-2137,0,0.0119761,"l is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Condi3 We measured significance of all the experiments in this paper with the randomized significance test (Yeh, 2000). 638 tional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in section 2.2, undirected graphical models do not seem to be suitable for historybased full parsing models. Sigmoid Belief Networks were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning met"
P07-1080,H05-1064,0,0.0190351,"n, allow us to build quite accurate parsing models. The main drawback of our proposed mean field approach is the relative computational complexity of the numerical procedure used to maximize Lt,k V . But this approximation has succeeded in showing that a more accurate approximation of ISBNs results in a more accurate parser. We believe this provides strong justification for more accurate approximations of ISBNs for parsing. 5 Related Work There has not been much previous work on graphical models for full parsing, although recently several latent variable models for parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Riezler et al., 2002). In (Koo and Collins, 2005), an undirected graphical model is used for parse reranking. Dependency parsing with dynamic Bayesian networks was considered in (Peshkin and Savova, 2005), with limited success. Their model is very different from ours. Roughly, it considered the whole sentence at a time, with the graphical model being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Undirected graphical models, in particular Con"
P07-1080,J93-2004,0,\N,Missing
P07-1080,J04-4004,0,\N,Missing
P07-1080,J03-4003,0,\N,Missing
P08-2019,W03-2123,1,0.734069,"owever, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Decision Making with MM POMDPs Q(at , bt ) ≈ 4 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state j i ,a updates (for f (rt−1 t−1 , ht )), statistical ASR-SLU j j (for P (ht |ut )/P (ht )), and a statistical user model i )). The state list is pruned as (for P (hjt |at−1 , rt−1 described in section 2, where the “core features” are the filled information slot values and whether they have been confirmed. For example, the system will merge two states which agree that the user only wants a cheap hotel, even if they disagree on the sequence of dialogue acts which lead to this information. It also never pr"
P08-2019,P03-1031,0,0.0137565,"s over POMDP states is similar to the approach in (Young et al., 2007), where POMDP belief states are represented using a set of partitions of POMDP states. For any set of partitions, the mixture model approach could express the same model by defining one MDP state per partition and giving it a uniform distribution inside its partition and zero probability outside. However, the mixture model approach is more flexible, because the distributions in the mixture do not have to be uniform within their non-zero region, and these regions do not have to be disjoint. A list of states was also used in (Higashinaka et al., 2003) to represent uncertainty, but no formal semantics was provided for this list, and therefore only heuristic uses were suggested for it. 5 Decision Making with MM POMDPs Q(at , bt ) ≈ 4 Initial Experiments We have implemented a Mixture Model POMDP architecture as a multi-state version of the DIPPER “Information State Update” dialogue manager (Bos et al., 2003). It uses equation (3) to compute belief state updates, given separate models for MDP state j i ,a updates (for f (rt−1 t−1 , ht )), statistical ASR-SLU j j (for P (ht |ut )/P (ht )), and a statistical user model i )). The state list is pr"
P08-2019,2007.sigdial-1.11,1,0.813542,"bed, and hand annotated. ASR hypotheses which result in the same user input are merged (summing their probabilities), and the resulting list of at most three ASRSLU hypotheses are passed to the dialogue manager. Thus the number of MDP states in the dialogue manager grows by up to three times at each step, before pruning. For the user model, the system uses an ngram user model, as described in (Georgila et al., 2005), trained on the annotated TownInfo corpus.2 The system’s dialogue management policy is a Mixture Model Q-MDP (MM Q-MDP) policy. As with the MDP states, the MDP Q function is from (Lemon and Liu, 2007). It was trained in an MDP system using reinforcement learning with simulated users (Lemon and Liu, 2007), and was not modified for use in our MM Q-MDP policy. We tested this system with 10 different users, each attempting 9 tasks in the TownInfo domain (searching for hotels and restaurants in a fictitious town), resulting in 90 test dialogues. The users each attempted 3 tasks with the MDP system of (Lemon and Liu, 2007), 3 tasks with a state-of-the-art handcoded system (see (Lemon et al., 2006)), and 3 tasks with the MM Q-MDP system. Ordering of systems and tasks was controlled, and 3 of the"
P11-2003,W09-1210,0,0.0373351,"Missing"
P11-2003,W09-1201,0,0.0711823,"Missing"
P11-2003,D07-1097,0,0.0340511,"Missing"
P11-2003,W08-2122,1,0.930421,"Missing"
P11-2003,W08-2123,0,0.114607,"Missing"
P11-2003,H05-1066,0,0.130615,"Missing"
P11-2003,P08-1108,0,0.0327393,"Missing"
P11-2003,W04-2407,0,0.161213,"Missing"
P11-2003,nivre-etal-2006-maltparser,0,0.608018,"03; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures. We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions. Unlike the Sigmoid Be"
P11-2003,W06-2933,0,0.308796,"03; Collobert and Weston, 2008), Bayesian networks (Titov and Henderson, 2007a), and Deep Belief Networks (Hinton et al., 2006). In this paper, we investigate how these models can be applied to dependency parsing. We focus on Shift-Reduce transition-based parsing proposed by Nivre et al. (2004). In this class of algorithms, at any given step, the parser has to choose among a set of possible actions, each representing an incremental modification to the partially built tree. To assign probabilities to these actions, previous work has proposed memory-based classifiers (Nivre et al., 2004), SVMs (Nivre et al., 2006b), and Incremental Sigmoid Belief Networks (ISBN) (Titov and Henderson, 2007b). In a related earlier work, Ratnaparkhi (1999) proposed a maximum entropy model for transition-based constituency parsing. Of these approaches, only ISBNs induce highdimensional latent representations to encode parse history, but suffer from either very approximate or slow inference procedures. We propose to address the problem of inference in a high-dimensional latent space by using an undirected graphical model, Restricted Boltzmann Machines (RBMs), to model the individual parsing decisions. Unlike the Sigmoid Be"
P11-2003,N10-1091,0,0.0303838,"Missing"
P11-2003,W08-2121,0,0.0981904,"Missing"
P11-2003,P07-1080,1,0.896013,"nd decision vectors in SBNs become undirected in RBMs. A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing. In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables. 2 An ISBN Parsing Model Our TRBM parser uses the same historybased probability model as the ISBN parser of Titov and Henderson (2007b): P (tree) = Πt P (vt |v1 , ..., vt−1 ), where each 11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Figure 1: An ISBN network. Shaded nodes represent decision variables and ‘H’ represents a vector of latent (c) variables. WHH denotes the weight matrix for directed connection of type c between two latent vectors. vt is a parser decision of the type Left-Arc, Right-Arc, Reduce or Shift. These decisions are further decomposed into sub-decision"
P11-2003,D07-1099,1,0.960443,"nd decision vectors in SBNs become undirected in RBMs. A complete parsing model consists of a sequence of RBMs interlinked via directed edges, which gives us a form of Temporal Restricted Boltzmann Machines (TRBM) (Taylor et al., 2007), but with the incrementally specified model structure required by parsing. In this paper, we analyze and contrast ISBNs with TRBMs and show that the latter provide an accurate and theoretically sound model for parsing with highdimensional latent variables. 2 An ISBN Parsing Model Our TRBM parser uses the same historybased probability model as the ISBN parser of Titov and Henderson (2007b): P (tree) = Πt P (vt |v1 , ..., vt−1 ), where each 11 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 11–17, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics Figure 1: An ISBN network. Shaded nodes represent decision variables and ‘H’ represents a vector of latent (c) variables. WHH denotes the weight matrix for directed connection of type c between two latent vectors. vt is a parser decision of the type Left-Arc, Right-Arc, Reduce or Shift. These decisions are further decomposed into sub-decision"
P11-2052,2009.jeptalnrecital-long.4,0,0.0459134,"Missing"
P11-2052,J94-4004,0,0.462313,"es sentences with missing predicate labels based on PoS-information in the French sentence. 2.1 Learning joint syntactic-semantic structures We know from previous work that there is a strong correlation between syntax and semantics (Merlo and van der Plas, 2009), and that this correlation has been successfully applied for the unsupervised induction of semantic roles (Lang and Lapata, 2010). However, previous work in machine translation leads us to believe that transferring the correlations between syntax and semantics across languages would be problematic due to argumentstructure divergences (Dorr, 1994). For example, the English verb like and the French verb plaire do not share correlations between syntax and semantics. The verb like takes an A0 subject and an A1 direct object, whereas the verb plaire licences an A1 subject and an A0 indirect object. We therefore transfer semantic roles crosslingually based only on lexical alignments and add syntactic information after transfer. In Figure 1, we see that cross-lingual transfer takes place at the semantic level, a level that is more abstract and known to port relatively well across languages, while the correlations with syntax, that are known"
P11-2052,2007.tmi-papers.10,0,0.0368477,"points, respectively, lower than the upper bound from manual annotations. 1 Introduction As data-driven techniques tackle more and more complex natural language processing tasks, it becomes increasingly unfeasible to use complete, accurate, hand-annotated data on a large scale for training models in all languages. One approach to addressing this problem is to develop methods that automatically generate annotated data by transferring annotations in parallel corpora from languages for which this information is available to languages for which these data are not available (Yarowsky et al., 2001; Fung et al., 2007; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; La"
P11-2052,W08-2122,1,0.893548,"Missing"
P11-2052,P02-1050,0,0.0491108,"Missing"
P11-2052,P06-2057,0,0.0855077,"Missing"
P11-2052,N10-1137,0,0.306482,"07; Pad´o and Lapata, 2009). Previous work on the cross-lingual transfer of semantic annotations (Pad´o, 2007; Basili et al., 2009) In this paper, we generate high-quality broadcoverage semantic annotations using an automatic approach that does not rely on a semantic ontology for the target language. Furthermore, to our knowledge, we report the first results on using joint syntactic-semantic learning to improve the quality of the semantic annotations from automatic crosslingual transfer. Results on correlations between syntax and semantics found in previous work (Merlo and van der Plas, 2009; Lang and Lapata, 2010) have led us to make use of the available syntactic annotations on the target language. We use the semantic annotations resulting from cross-lingual transfer combined with syntactic annotations to train a joint syntactic-semantic parser for the target language, which, in turn, re-annotates the corpus (See Figure 1). We show that the semantic annotations produced by this parser are of higher quality than the data on which it was trained. Given our goal of producing broad-coverage annotations in a setting based on an aligned corpus, our choices of formal representation and of labelling scheme di"
P11-2052,J93-2004,0,0.0416764,"c roles across languages (Pad´o, 2007), we select only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results W"
P11-2052,P09-1033,1,0.722461,"Missing"
P11-2052,W07-1513,0,0.10955,"Missing"
P11-2052,J03-1002,0,0.00759072,"Missing"
P11-2052,2007.jeptalnrecital-long.25,0,0.0744954,"Missing"
P11-2052,J05-1004,0,0.697041,"only those parallel sentences in Europarl that are direct translations from English to French, or vice versa. In the end, we have a word-aligned parallel corpus of 276-thousand sentence pairs. Syntactic annotation is available for French. The French Treebank (Abeill´e et al., 2003) is a treebank of 21,564 sentences annotated with constituency annotation. We use the automatic dependency conversion of the French Treebank into dependency format provided to us by Candito and Crabb´e and described in Candito et al. (2009). The Penn Treebank corpus (Marcus et al., 1993) merged with PropBank labels (Palmer et al., 2005) and NomBank labels (Meyers, 2007) is used to train the syntactic-semantic parser described in Subsection 3.1 to annotate the English part of the parallel corpus. 3.3 Test sets For testing, we used the hand-annotated data described in (van der Plas et al., 2010). One-thousand French sentences are extracted randomly from our parallel corpus without any constraints on the semantic parallelism of the sentences, unlike much previous work. We randomly split those 1000 sentences into test and development set containing 500 sentences each. 4 Results We evaluate our methods for automatic annotation ge"
P11-2052,W07-2218,1,0.874923,"Missing"
P11-2052,W10-1814,1,0.905206,"Missing"
P11-2052,2009.eamt-1.30,0,0.0516121,"Missing"
P11-2052,N09-2004,0,0.156095,"Missing"
P11-2052,H01-1035,0,0.610552,"Missing"
P12-2029,J02-3001,0,0.442917,"t the expected semantic role sequence. Sentence a is in active voice with sequence (A0, P REDICAT E, A1) and sentence b is in passive voice with sequence (A1, P REDICAT E, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (20"
P12-2029,W06-1601,0,0.628392,"ocal classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space. Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a generative model in contrast to their discriminative one. Further, unlike Grenager and Manning (2006), we do not explicitly generate the linking of semantic roles and syntactic relations, thus keeping the parameter space tractable. The main contribution of this work is an unsupervised model that uses global role o"
P12-2029,P04-1061,0,0.04459,"stic theory of topological fields (Diderichsen, 1966; Drach, 1937). This simplifying assumption that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space limited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the (ST OP/CON T IN U E) indicator variable on the adjacency value (adj) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised dependency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, 147 the probability to ST OP should be very high when generating the first SR. We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the InsideOutside algorithm (Baker, 1979). In the M-step, we add the counts corresponding to the Bayesian priors to the expected counts and use the resulting counts to calculate the MAP estimate of the para"
P12-2029,P11-1112,0,0.802182,"unige.ch Abstract We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequenc"
P12-2029,D11-1122,0,0.704408,"unige.ch Abstract We propose a probabilistic generative model for unsupervised semantic role induction, which integrates local role assignment decisions and a global role ordering decision in a unified model. The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequenc"
P12-2029,J08-2001,0,0.346929,"Missing"
P12-2029,J05-1004,0,0.200668,". The role sequence is divided into intervals based on the notion of primary roles, and each interval generates a sequence of secondary roles and syntactic constituents using local features. The global role ordering consists of the sequence of primary roles only, thus making it a partial ordering. 1 Introduction Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in (a.) [ Mark ]A0 drove [ the car ]A1 (b.) [ The car ]A1 was driven by [ Mark ]A0 This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequence (A0, P REDICAT E, A1) and sentence b is in passive voice with sequence (A1, P REDICAT E, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), c"
P12-2029,C04-1197,0,0.0833057,"Missing"
P12-2029,W08-2121,0,0.446781,"Missing"
P12-2029,J08-2002,0,0.15002,"t, nonlocal features such as predicate voice could provide information about the expected semantic role sequence. Sentence a is in active voice with sequence (A0, P REDICAT E, A1) and sentence b is in passive voice with sequence (A1, P REDICAT E, A0). Additional global preferences, such as arguments A0 and A1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features. Supervised SRL systems have mostly used local classifiers that assign a role to each constituent independently of others, and only modeled limited correlations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repetition constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have explored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of semantic roles and syntactic relations. However, as the space of possible linkings is large, language-speci"
P12-2058,J07-2003,0,0.0970395,"beam size. Empirically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algor"
P12-2058,P10-4002,0,0.114003,"Missing"
P12-2058,2010.iwslt-papers.8,1,0.817047,"best solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation system. Let L = hx0 , . . . , xk−1 i be a list over R, that is, an ordered sequence of real numbers, possibly with repetitions. We write |L |= k to denote the length of L. We say that L is descending if xi ≥ xj for every i, j with 0 ≤ i < j < k. Let L1 = hx0 , . . . , xk−1 i and L2 = hy0 , . . . , yk′ −1 i be two descending lists ove"
P12-2058,W11-2123,0,0.110797,"Missing"
P12-2058,W05-1506,0,0.257813,"chine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and per"
P12-2058,P07-1019,0,0.0210545,"irically, we show a gain in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in t"
P12-2058,2005.mtsummit-papers.11,0,0.0634399,"Missing"
P12-2058,D07-1104,0,0.0523893,"Missing"
P12-2058,P03-1021,0,0.0446584,"Missing"
P12-2058,N09-2036,0,0.0214687,"in running time of a standard machine translation system, at a small loss in accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate it"
P12-2058,P10-1017,0,0.0180403,"accuracy. 1 Introduction Since its first appearance in (Huang and Chiang, 2005), the Cube Pruning (CP) algorithm has quickly gained popularity in statistical natural language processing. Informally, this algorithm applies to scenarios in which we have the k-best solutions for two input sub-problems, and we need to compute the kbest solutions for the new problem representing the combination of the two sub-problems. CP has applications in tree and phrase based machine translation (Chiang, 2007; Huang and Chiang, 2007; Pust and Knight, 2009), parsing (Huang and Chiang, 2005), sentence alignment (Riesa and Marcu, 2010), and in general in all systems combining inexact beam decoding with dynamic programming under certain monotonic conditions on the definition of the scores in the search space. Standard implementations of CP run in time O(k log(k)), with k being the size of the input/output beams (Huang and Chiang, 2005). Gesmundo and Henderson (2010) propose Faster CP (FCP) which optimizes the algorithm but keeps the O(k log(k)) time complexity. Here, we propose a novel heuristic algorithm for CP running in time O(k) and evaluate its impact on the efficiency and performance of a real-world machine translation"
P16-1193,W11-2501,0,0.0786605,"tions of Word2Vec result in more accurate unsupervised models of hyponymy. We evaluate on detecting hyponymy relations between words because hyponymy is the canonical type of lexical entailment; most of the semantic features of a hypernym (e.g. “animal”) must be included in the semantic features of the hyponym (e.g. “dog”). We evaluate in both a fully unsupervised setup and a semi-supervised setup. 5.1 Hyponymy with Word2Vec Vectors For our evaluation on hyponymy detection, we replicate the experimental setup of Weeds et al. (2014), using their selection of word pairs2 from the BLESS dataset (Baroni and Lenci, 2011).3 2 https://github.com/SussexCompSem/ learninghypernyms 3 Of the 1667 word pairs in this data, 24 were removed because we do not have an embedding for one of the words. These noun-noun word pairs include positive hyponymy pairs, plus negative pairs consisting of some other hyponymy pairs reversed, some pairs in other semantic relations, and some random pairs. Their selection is balanced between positive and negative examples, so that accuracy can be used as the performance measure. For their semisupervised experiments, ten-fold cross validation is used, where for each test set, items are remo"
P16-1193,E12-1004,0,0.135341,"., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evaluations which require supervised learning about individual words, instead limiting ourselves to semisupervised learning where the words in the training and test sets are disjoint. For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014"
P16-1193,Q15-1027,0,0.090277,"distribution for that feature’s dimension. While nicely motivated theoretically, the model appears to be more computationally expensive than the one proposed here, particularly for inferring vectors. They do make unsupervised predictions of hyponymy relations with their learned vector distributions, using KL-divergence between the distributions for the two words. They evaluate their models on the hyponymy data from (Baroni et al., 2012). As discussed further in section 5.2, our best models achieve non-significantly better average precision than their best models. The semi-supervised model of Kruszewski et al. (2015) also models entailment in a vector space, but they use a discrete vector space. They train a mapping from distributional semantic vectors to Boolean vectors such that feature inclusion respects a training set of entailment relations. They then use feature inclusion to predict hyponymy, and other lexical entailment relations. This approach is similar to the one used in our semisupervised experiments, except that their discrete entailment prediction operator is very different from our proposed entailment operators. 5 Evaluation To evaluate whether the proposed framework is an effective model of"
P16-1193,S12-1012,0,0.0738118,"unt of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evaluations which require supervised learning about individual wo"
P16-1193,W14-1610,0,0.0152371,"ector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evaluations which require supervised learning about individual words, instead limiting ourselves to semisupervised learning where the words in the training and test sets are disjoint. For these reasons, in our evaluations we replicate the experimental setup of Weeds et al. (2014), for both unsupervised and semi-supervised models. Within this setup, we compare to the results of the models evaluated by Weeds et al. (2014) and to previously proposed vector-space operators. This includ"
P16-1193,N15-1098,0,0.0329079,"gical entailment between nothing known (unk), two different features f and g known, and the complement of f (¬f ) known. Introduction Modelling entailment is a fundamental issue in computational semantics. It is also important for many applications, for example to produce abstract summaries or to answer questions from text, where we need to ensure that the input text entails the output text. There has been a lot of interest in modelling entailment in a vector-space, but most of this work takes an empirical, often ad-hoc, approach to this problem, and achieving good results has been difficult (Levy et al., 2015). In this work, we propose a new framework for modelling entailment in a vector-space, and illustrate its effective∗ This work was partially supported by French ANR grant CIFRE N 1324/2014. ness with a distributional-semantic model of hyponymy detection. Unlike previous vector-space models of entailment, the proposed framework explicitly models what information is unknown. This is a crucial property, because entailment reflects what information is and is not known; a representation y entails a representation x if and only if everything that is known given x is also known given y. Thus, we mode"
P16-1193,P98-2127,0,0.0119254,"the other two interpretations. As we will see in Section 5, this prediction holds. 4 Related Work There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to fu"
P16-1193,S15-1021,0,0.0233257,"and a progressively worse approximation to the second and first models. Therefore, if the entailment-based distributional semantic model we propose is accurate, then we would expect the best accuracy in hyponymy detection using the third interpretation of Word2Vec vectors, and progressively worse accuracy for the other two interpretations. As we will see in Section 5, this prediction holds. 4 Related Work There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu"
P16-1193,W14-1608,0,0.427035,"the entailment-based distributional semantic model we propose is accurate, then we would expect the best accuracy in hyponymy detection using the third interpretation of Word2Vec vectors, and progressively worse accuracy for the other two interpretations. As we will see in Section 5, this prediction holds. 4 Related Work There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluat"
P16-1193,C14-1097,0,0.0946149,"Missing"
P16-1193,E14-4008,0,0.104895,"onymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not concerned with models or evaluations which require supervised learning about individual words, instead limiting ourselves to semisupervised learni"
P16-1193,W03-1011,0,0.0644066,"in Section 5, this prediction holds. 4 Related Work There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni et al., 2012)). We also limit our evaluation of lexical entailment to hyponymy, not including other related lexical relations (cf. (Weeds et al., 2014; Vylomova et al., 2015; Turney and Mohammad, 2014; Levy et al., 2014)), leaving more complex cases to future work on compositional semantics. We are also not c"
P16-1193,C04-1146,0,0.349165,"Missing"
P16-1193,C14-1212,0,0.254344,"econd and first models. Therefore, if the entailment-based distributional semantic model we propose is accurate, then we would expect the best accuracy in hyponymy detection using the third interpretation of Word2Vec vectors, and progressively worse accuracy for the other two interpretations. As we will see in Section 5, this prediction holds. 4 Related Work There has been a significant amount of work on using distributional-semantic vectors for hyponymy detection, using supervised, semi-supervised or unsupervised methods (e.g. (Yu et al., 2015; Necsulescu et al., 2015; Vylomova et al., 2015; Weeds et al., 2014; Fu et al., 2015; Rei and Briscoe, 2014)). Because our main concern is modelling entailment within a vector space, we do not do a thorough comparison to models which use measures computed outside the vector space (e.g. symmetric measures (LIN (Lin, 1998)), asymmetric measures (WeedsPrec (Weeds and Weir, 2003; Weeds et al., 2004), balAPinc (Kotlerman et al., 2010), invCL (Lenci and Benotto, 2012)) and entropy-based measures (SLQS (Santus et al., 2014))), nor to models which encode hyponymy in the parameters of a vector-space operator or classifier (Fu et al., 2015; Roller et al., 2014; Baroni"
P16-1193,C98-2122,0,\N,Missing
P92-1019,P83-1020,0,0.211711,"nterminals, which are arbitrary feature structures, and terminals, which are atomic instances of strings. Unlike most formalisms, SUG allows the specification of the structural relations to be equally partial. For example, if a description specifies children for a node, this does not preclude that node from acquiring other children, such as modifiers. This partiality also allows grammar entries to underspecify ordering constraints between nodes, thus allowing for variations in word order. This partiality in structural information is imperative to allow incremental parsing without disjunction (Marcus et al., 1983). In addition to the immediate dominance relation for specifying parent-child relationships and linear precedence for specifying ordering constraints, SUG allows chains of immediate dominance relationships to be partially specified using the dominance relation. A dominance constraint between two nodes specifies that there must be a chain of zero or more immediate dominance constraints between the two nodes, but it does not say anything about the chain. This relation is necessary to express long distance dependencies in a single grammar entry. Some examples of SUG phrase structure descriptions"
Q18-1044,P05-1048,0,0.0615184,"rman pair shown in Table 8 demonstrate that our baseline is strong and that our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete s"
Q18-1044,E09-1005,0,0.0401028,"luster all occurrences wi of a given word type Wt , represented as word vectors ui , according to the similarity of their senses, as inferred from the similarity of the context vectors. We compare the algorithms empirically in §5. K-means Clustering. The original k-means algorithm (MacQueen, 1967) aims to partition a set of items, which are here tokens w1 , w2 , . . . , wn of the same word type Wt , represented through their 5 k X X code.google.com/archive/p/word2vec/. 637 Random Walks. Finally, we also consider for comparison a WSD method based on random walks on the WordNet knowledge graph (Agirre and Soroa, 2009; Agirre et al., 2014) available from the UKB toolkit.6 In the graph, senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. From each input sentence, we extract its content words (nouns, verbs, adjectives, and adverbs) that have an entry in the WordNet weighted graph. The method calculates the probability of a random walk over the graph from a target word’s sense ending on any other sense in the graph, and determines the sense with the highest probability for each analyzed word. In this case, the random walk algori"
Q18-1044,D07-1007,0,0.0606955,"ive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of discrete symbols in a con"
Q18-1044,2012.eamt-1.60,0,0.0106597,"98 3,844 1,475 3,915 1,647 2015. Second, for EN/FR and EN/ES, we use data from WMT 2014 (Bojar et al., 2014)9 with 5.3M sentences for EN/FR and 3.8M sentences for EN/ES. Here, the development sets are NewsTest 2008 and 2009, and the testing sets are NewsTest 2012 and 2013 for both language pairs. The source sides of these larger additional sets contain around 3,500 unique English word forms with more than one sense in WordNet, and our system generates ca. 8K different noun labels and 2.5K verb labels for each set. Finally, for comparison purposes and model selection, we use the WIT3 Corpus10 (Cettolo et al., 2012), a collection of transcripts of TED talks. We use 150K sentence pairs for training, 5K for development and 50K for testing. Pre-processing. Before assigning sense labels, we tokenize all the texts and identify the parts of speech using the Stanford POS tagger.11 Then, we filter out the stopwords and the nouns that are proper names according to the Stanford Name Entity Recognizer.11 Furthermore, we convert the plural forms of nouns to their singular forms and the verb forms to infinitives using the stemmer and lemmatizer from NLTK,12 which is essential because WordNet has description entries o"
Q18-1044,P07-1005,0,0.0522818,"cent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of discrete symbols in a continuous vector space"
Q18-1044,S10-1078,0,0.0838147,"Missing"
Q18-1044,P17-4012,0,0.0474407,"s Settings. Unless otherwise stated, we adopt the following settings in the k-means algorithm, with the implementation provided in Scikit-learn (Pedregosa et al., 2011). We use the definition of each sense for initializing the centroids, and later compare this choice with the use of examples. We set kt , the initial number of clusters, to the number of WordNet senses of each ambiguous word type Wt , and set the window size for the context surrounding each occurrence to c = 8. Neural MT. We build upon the attentionbased neural translation model (Bahdanau et al., 2015) from the OpenNMT toolkit (Klein et al., 2017).13 We use LSTM and not GRU. For the proposed ATT and ATT ini models, we add an Words 2,006 3,876 1,976 3,194 1,987 3,558 1,915 2,210 Table 1: Size of data sets used for machine translation from English to five different target languages (TL). FR = French; DE = German; ES = Spanish; ZH = Chinese; NL = Dutch. 4 Data, Metrics, and Implementation Data Sets. We train and test our sense-aware MT systems on the data shown in Table 1: the UN Corpus7 (Rafalovitch and Dale, 2009) and the Europarl Corpus8 (Koehn, 2005). We first experiment with our models using the same data set and protocol as in our p"
Q18-1044,S15-2049,0,0.125091,"is PageRank (Grin and Page, 1998), which computes a relative structural importance or “rank” for each node. method considered as a practical interpretation of a Dirichlet process (Ferguson, 1973) for nonparametric clustering. In the original analogy, each token is compared to a customer in a restaurant, and each cluster is a table where customers can be seated. A new customer can choose to sit at a table with other customers, with a probability proportional to the numbers of customers at that table, or sit at a new, empty table. In an application to multisense word embeddings, Li and Jurafsky (2015) proposed that the probability to “sit at a table” should also depend on the contextual similarity between the token and the sense modeled by the table. We build upon this idea and bring several modifications that allow for an instantiation with sense-related knowledge from WordNet, as follows. For each word type Wt appearing in the data, we start by fixing the maximal number kt of senses or clusters as the number of senses of Wt in WordNet. This avoids an unbounded number of clusters (as in the original CRP algorithm) and the risk of cluster sparsity by setting a non-arbitrary limit based on"
Q18-1044,2005.mtsummit-papers.11,0,0.0602538,"Missing"
Q18-1044,S10-1079,0,0.0420461,"Missing"
Q18-1044,D15-1200,0,0.161846,"walk algorithm is PageRank (Grin and Page, 1998), which computes a relative structural importance or “rank” for each node. method considered as a practical interpretation of a Dirichlet process (Ferguson, 1973) for nonparametric clustering. In the original analogy, each token is compared to a customer in a restaurant, and each cluster is a table where customers can be seated. A new customer can choose to sit at a table with other customers, with a probability proportional to the numbers of customers at that table, or sit at a new, empty table. In an application to multisense word embeddings, Li and Jurafsky (2015) proposed that the probability to “sit at a table” should also depend on the contextual similarity between the token and the sense modeled by the table. We build upon this idea and bring several modifications that allow for an instantiation with sense-related knowledge from WordNet, as follows. For each word type Wt appearing in the data, we start by fixing the maximal number kt of senses or clusters as the number of senses of Wt in WordNet. This avoids an unbounded number of clusters (as in the original CRP algorithm) and the risk of cluster sparsity by setting a non-arbitrary limit based on"
Q18-1044,N18-1121,0,0.1812,"Missing"
Q18-1044,D14-1113,0,0.0930834,"sistently better than the baselines, with the largest improvements achieved by NMT on EN/FR and EN/ES. The neural systems outperform the phrasebased statistical ones (Pu et al., 2017), which are shown for comparison in the upper part of the table. We compare our proposal to the recent system proposed by Yang et al. (2017), on the 500K-line EN/FR Europarl data set (the differences between their system and ours are listed in §7). We carefully implemented their model by following their paper, since their code is not available. Using the sense embeddings of the multi-sense skip-gram model (MSSG) (Neelakantan et al., 2014) as they 643 0.8 Baseline Our 0.6method Proportion C. I. C. I. 0.6 0.4 0.2 0.0 Table 6: Confusion matrix for our WSD+NMT (ATT ini ) system and our WSD+SMT system against their respective baselines (NMT and SMT), over the Europarl test data, for two language pairs. O>B O=B O&lt;B 2 - Good 1 - Acceptable 0 - Wrong 0.8 Proportion WSD+ NMT WSD+ SMT 1.0 1.0 Baselines EN/FR EN/ES Correct Incorrect Correct Incorrect 134,552 17,145 146,806 16,523 10,551 101,228 8,183 58,387 124,759 13,408 139,800 11,194 9,676 115,633 7,559 71,346 0.4 0.2 deal face mark subject Candidate (a) System ratings. 0.0 deal face"
Q18-1044,D15-1166,0,0.0527114,"t vector ui the algorithm decides whether the token is assigned to one of the sense clusters Sj to which previous tokens have been assigned, or whether it is assigned to a new empty cluster, by selecting the option that has the highest probability, which is computed as follows:   Nj (λ1 s(ui , dtj ) + λ2 s(ui , µj ))     if N 6= 0 (non-empty sense) j P ∝ (3)  γs(ui , dtj )     if N = 0 (empty sense) j 3 Integration with Neural MT 3.1 Baseline Neural MT Model We now present several models integrating WSD into NMT, starting from an attention-based NMT baseline (Bahdanau et al., 2015; Luong et al., 2015). Given a source sentence X with words wx , X = (w1x , w2x , ..., wTx ), the model computes a conditional distribution over translations, expressed as p(Y = (w1y , w2y , ..., wTy 0 )|X). The neural network model consists of an encoder, a decoder, and an attention mechanism. First, each source word wtx ∈ V is projected from a one-hot word vector into a continuous vector space representation xt . Then, the resulting sequence of word vectors is read by the bidirectional encoder, which consists of forward and backward recurrent networks (RNNs). The forward RNN reads the sequence in → − → − → − lef"
Q18-1044,P02-1040,0,0.104018,"Missing"
Q18-1044,S10-1081,0,0.0441185,"Missing"
Q18-1044,W17-4701,1,0.33706,"aging. 2.2 S = arg min S ||u − µi ||2 (1) i=1 u∈Si At the first iteration, when there are no clusters yet, the algorithm selects k random points as centroids of the k clusters. Then, at each subsequent iteration t, the algorithm calculates for each candidate cluster a new centroid of the observations, defined as their average vector, as follows: µit+1 = 1 X uj |Sit | t (2) uj ∈Si In an earlier application of k-means to phrasebased statistical MT, but not neural MT, we made several modifications to the original k-means algorithm to make it adaptive to the word senses observed in training data (Pu et al., 2017). We maintain these changes and summarize them briefly here. The initial number of clusters kt for each ambiguous word type Wt is set to the number of its senses in WordNet, either considering only the senses that have a definition or those that have an example. The centroids of the clusters are initialized to the vectors representing the senses from WordNet, either using their definition vectors dtj or their example vectors etj . These initializations are thus a form of weak supervision of the clustering process. Finally, and most importantly, after running the k-means algorithm, the number o"
Q18-1044,H05-1097,0,0.0554247,"our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of dis"
Q18-1044,2009.mtsummit-posters.15,0,0.0250179,"Missing"
Q18-1044,N18-1124,1,0.825847,"., 2016) or generated from both sides of word dependencies (Su et al., 2015). However, apart from the sense graph, WordNet also provides textual information such as sense definitions and examples, which should be useful for WSD, but were not Choi et al. (2017) attempts to improve NMT by integrating context vectors associated to source words into the generation process during decoding. The model proposed by Zhang et al. (2017) is aware of previous attended words on the source side in order to better predict which words will be attended in future. The self-attentive residual decoder designed by Werlen et al. (2018) leverages the contextual information from previously translated words on the target side. BLEU scores on the English–German pair shown in Table 8 demonstrate that our baseline is strong and that our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that m"
Q18-1044,W17-4702,0,0.041133,"generates only one embedding for each word type, regardless of its possibly different senses, as analyzed, for example, by Hill et al. (2017). Several studies proposed efficient nonparametric models for monolingual word sense representation (Neelakantan et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2016; Liu et al., 2017), but left open the question whether sense representations can help neural MT by reducing word ambiguity. Recent studies integrate the additional sense assignment with neural MT based on these approaches, either by adding such sense assignments as additional features (Rios et al., 2017) or by merging the context information on both sides of parallel data for encoding and decoding (Choi et al., 2017). Yang et al. (2017) recently proposed to add sense information by using weighted sense embeddings as input to neural MT. The sense labels were generated by a MSSG model (Neelakantan et al., 2014), and the context vector used for sense weight generation was computed from the output of a bidirectional RNN. Finally, the weighted average sense embeddings were used in place of the word embedding for the NMT encoder. The numerical results given in §6 show that our options for using sen"
Q18-1044,D15-1145,0,0.0168514,"rming self-learned word sense induction instead of using pre-specified word senses as traditional WSD does. However, they created the risk of discovering sense clusters that do not correspond to the senses of words actually needed for MT. Hence, they left open an important question, namely, whether WSD based on semantic resources such as WordNet (Fellbaum, 1998) can be successfully integrated with SMT. Several studies integrated sense information as features to SMT, either obtained from the sense graph provided by WordNet (Neale et al., 2016) or generated from both sides of word dependencies (Su et al., 2015). However, apart from the sense graph, WordNet also provides textual information such as sense definitions and examples, which should be useful for WSD, but were not Choi et al. (2017) attempts to improve NMT by integrating context vectors associated to source words into the generation process during decoding. The model proposed by Zhang et al. (2017) is aware of previous attended words on the source side in order to better predict which words will be attended in future. The self-attentive residual decoder designed by Werlen et al. (2018) leverages the contextual information from previously tr"
Q18-1044,P14-1137,0,0.0175742,"–0.57 BLEU points compared with baselines. Recently, Tang et al. (2016) used only the supersenses from WordNet (coarse-grained semantic labels) for automatic WSD, using maximum entropy classification or sense embeddings learned using word2vec. When combining WSD with SMT using a factored model, Tang et al. improved BLEU scores by 0.7 points on average, though with large differences between their three test subsets (IT Q&A pairs). Although these reformulations of the WSD task proved helpful for SMT, they did not determine whether actual source-side senses are helpful or not for end-to-end SMT. Xiong and Zhang (2014) attempted to answer this question by performing self-learned word sense induction instead of using pre-specified word senses as traditional WSD does. However, they created the risk of discovering sense clusters that do not correspond to the senses of words actually needed for MT. Hence, they left open an important question, namely, whether WSD based on semantic resources such as WordNet (Fellbaum, 1998) can be successfully integrated with SMT. Several studies integrated sense information as features to SMT, either obtained from the sense graph provided by WordNet (Neale et al., 2016) or gener"
Q19-1009,D14-1179,0,0.0279188,"Missing"
Q19-1009,D16-1026,0,0.01998,"binary cross-entropy loss. Assuming θ contains all the parameters of the model, the training loss is computed as follows: (10) where is component-wise multiplication. The probability for hi to belong to one of the k known labels is modeled by a linear unit that maps any point in the joint space into a score which indicates the validity of the combination: (ij ) (ij ) pval = gjoint w + b k N 1 XX L(θ) = − H(yij , yˆij ) Nk where H is the binary cross-entropy between the gold label yij and predicted label yˆij for a document i and a candidate label j . We handle multiple languages according to Firat et al. (2016) and Pappas and Popescu-Belis (2017). Assuming that Θ = {θ1 , θ2 , ..., θM } are all the parameters required for each of the M languages, we use a joint multilingual objective based on the sum of cross-entropy losses: (11) Ne X M X k 1X (l) (l) L(Θ) = − H(yij , yˆij ) Z i (i ) 1 + e−Pval l (16) j =1 where Z = Ne M k with Ne being the number of examples per epoch. At each iteration, a document-label pair for each language is sampled. In addition, multilingual models share a certain subset of the encoder parameters during training while the output layer parameters are kept language-specific, as"
Q19-1009,N18-1172,0,0.0310957,"existing neural text classification models ignore label descriptions and semantics. Moreover, they are based on typical output layer parametrizations that are dependent on the label set size, and thus are not able to scale well to large label sets nor to generalize to unseen labels. Our output layer parametrization addresses these limitations and could potentially improve such models. 5.2 Output Representation Learning There exist studies that aim to learn output representations directly from data without any semantic grounding to word embeddings (Srikumar and Manning, 2014; Yeh et al., 2018; Augenstein et al., 2018). Such methods have a label-set-size dependent parametrization, which makes them data hungry, less scalable on large label sets, and incapable of generalizing to unseen classes. Wang et al. (2018) addressed the lack of semantic grounding to word embeddings by proposing an efficient method based on label-attentive text representations which are helpful for text classification. However, in contrast to our study, their parametrization is still label-set-size dependent and thus their model is not able to scale well to large label sets nor to generalize to unseen labels. 5.3 6 Conclusion We propose"
Q19-1009,N15-1011,0,0.0292589,"glish general and specific labels of the DW data set. In both cases, the performance of GILE-HNN tends to increase as the percentage of labels sampled increases, but it levels off for the higher percentages. 5.1 Related Work Neural text Classification Research in neural text classification was initially based on feed-forward networks, which required unsupervised pre-training (Collobert et al., 2011; Mikolov et al., 2013; Le and Mikolov, 2014) and later on they focused on networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential 150 called zero-shot classification. However, there are fewer such studies for text classification. Dauphin et al. (2014) predicted semantic utterances of text by mapping them in the same semantic space with the class labels using an unsupervised learning objective. Yazdani and Henderson (2015) proposed a zero-shot spoken language understanding model based on a bilinear input-label model able"
Q19-1009,D15-1166,0,0.0165527,"Missing"
Q19-1009,D14-1181,0,0.00822434,"Missing"
Q19-1009,C12-1089,0,0.0758228,"Missing"
Q19-1009,Q17-1026,0,0.0204224,"posed hierarchical recurrent neural networks and showed that they were superior to CNNbased models. Yang et al. (2016) demonstrated that a hierarchical attention network with bidirectional gated encoders outperforms previous alternatives. Pappas and Popescu-Belis (2017) adapted such networks to learn hierarchical document structures with shared components across different languages. The issue of scaling to large label sets has been addressed previously by output layer approximations (Morin and Bengio, 2005) and with the use of sub-word units or character-level modeling (Sennrich et al., 2016; Lee et al., 2017) which is mainly applicable to structured prediction problems. Despite the numerous studies, most of the existing neural text classification models ignore label descriptions and semantics. Moreover, they are based on typical output layer parametrizations that are dependent on the label set size, and thus are not able to scale well to large label sets nor to generalize to unseen labels. Our output layer parametrization addresses these limitations and could potentially improve such models. 5.2 Output Representation Learning There exist studies that aim to learn output representations directly fr"
Q19-1009,P05-1015,0,0.345216,"-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zeroresource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios. 1 Introduction Text classification is a fundamental NLP task with numerous real-world applications such as topic recognition (Tang et al., 2015; Yang et al., 2016), sentiment analysis (Pang and Lee, 2005; Yang et al., 2016), and question answering (Chen et al., 2015; Kumar et al., 2015). Classification 139 Transactions of the Association for Computational Linguistics, vol. 7, pp. 139–155, 2019. Action Editor: Eneko Agirre. Submission batch: 9/2018; Revision batch: 1/2019; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 2 with cross-entropy loss to optimize classification performance.1 The need for capturing complex label relationships is addressed by two nonlinear transformations that have the same target joint space dimensionality. T"
Q19-1009,D15-1167,0,0.239328,"abel embedding with controllable capacity and a joint-space-dependent classification unit that is trained with cross-entropy loss to optimize classification performance. We evaluate models on full-resource and low- or zeroresource text classification of multilingual news and biomedical text with a large label set. Our model outperforms monolingual and multilingual models that do not leverage label semantics and previous joint input-label space models in both scenarios. 1 Introduction Text classification is a fundamental NLP task with numerous real-world applications such as topic recognition (Tang et al., 2015; Yang et al., 2016), sentiment analysis (Pang and Lee, 2005; Yang et al., 2016), and question answering (Chen et al., 2015; Kumar et al., 2015). Classification 139 Transactions of the Association for Computational Linguistics, vol. 7, pp. 139–155, 2019. Action Editor: Eneko Agirre. Submission batch: 9/2018; Revision batch: 1/2019; Published 4/2019. c 2019 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 2 with cross-entropy loss to optimize classification performance.1 The need for capturing complex label relationships is addressed by two nonlinear transformat"
Q19-1009,W18-6308,1,0.85444,"unsupervised learning objective. Yazdani and Henderson (2015) proposed a zero-shot spoken language understanding model based on a bilinear input-label model able to generalize to previously unseen labels. Nam et al. (2016) proposed a bilinear joint document-label embedding that learns shared word representations between documents and labels. More recently, Shu et al. (2017) proposed an approach for open-world classification that aims to identify novel documents during testing but it is not able to generalize to unseen classes. Perhaps the model most similar to ours is from the recent study by Pappas et al. (2018) on neural machine translation, with the difference that they have single-word label descriptions and they use a label-set-dependent bias in a softmax linear prediction unit, which is designed for structured prediction. Hence, their model can neither handle unseen labels nor multi-label classification, as we do here. Compared with previous joint input-label models, the proposed model has a more general and flexible parametrization, which allows the output layer capacity to be controlled. Moreover, it is not restricted to linear mappings, which have limited expressivity, but uses nonlinear mapp"
Q19-1009,P18-1216,0,0.134355,"Missing"
Q19-1009,I17-1102,1,0.852179,"Missing"
Q19-1009,D15-1044,0,0.0298754,"Missing"
Q19-1009,P16-1162,0,0.0208288,"Tang et al. (2015) proposed hierarchical recurrent neural networks and showed that they were superior to CNNbased models. Yang et al. (2016) demonstrated that a hierarchical attention network with bidirectional gated encoders outperforms previous alternatives. Pappas and Popescu-Belis (2017) adapted such networks to learn hierarchical document structures with shared components across different languages. The issue of scaling to large label sets has been addressed previously by output layer approximations (Morin and Bengio, 2005) and with the use of sub-word units or character-level modeling (Sennrich et al., 2016; Lee et al., 2017) which is mainly applicable to structured prediction problems. Despite the numerous studies, most of the existing neural text classification models ignore label descriptions and semantics. Moreover, they are based on typical output layer parametrizations that are dependent on the label set size, and thus are not able to scale well to large label sets nor to generalize to unseen labels. Our output layer parametrization addresses these limitations and could potentially improve such models. 5.2 Output Representation Learning There exist studies that aim to learn output represen"
Q19-1009,D17-1314,0,0.0231867,"ential 150 called zero-shot classification. However, there are fewer such studies for text classification. Dauphin et al. (2014) predicted semantic utterances of text by mapping them in the same semantic space with the class labels using an unsupervised learning objective. Yazdani and Henderson (2015) proposed a zero-shot spoken language understanding model based on a bilinear input-label model able to generalize to previously unseen labels. Nam et al. (2016) proposed a bilinear joint document-label embedding that learns shared word representations between documents and labels. More recently, Shu et al. (2017) proposed an approach for open-world classification that aims to identify novel documents during testing but it is not able to generalize to unseen classes. Perhaps the model most similar to ours is from the recent study by Pappas et al. (2018) on neural machine translation, with the difference that they have single-word label descriptions and they use a label-set-dependent bias in a softmax linear prediction unit, which is designed for structured prediction. Hence, their model can neither handle unseen labels nor multi-label classification, as we do here. Compared with previous joint input-la"
Q19-1009,N16-1174,0,0.0552523,"Missing"
Q19-1009,D15-1027,1,0.821346,"iven that semantic representations of words have been shown to be useful for representing the input, it is reasonable to expect that they are going to be useful for representing the labels as well. Previous work has leveraged knowledge from the label texts through a joint input-label space, initially for image classification (Weston et al., 2011; Mensink et al., 2012; Frome et al., 2013; Socher et al., 2013). Such models generalize to labels both seen and unseen during training, and scale well on very large label sets. However, as we explain in Section 2, existing input-label models for text (Yazdani and Henderson, 2015; Nam et al., 2016) have the following limitations: (i) their embedding does not capture complex label relationships due to its bilinear form, (ii) their output layer parametrization is rigid because it depends on the dimensionality of the encoded text and labels, and (iii) they are outperformed on seen labels by classification baselines trained with cross-entropy loss (Frome et al., 2013; Socher et al., 2013). In this paper, we propose a new joint inputlabel model that generalizes over previous such models, addresses their limitations, and does not compromise performance on seen labels (see F"
Q19-1009,D15-1106,0,\N,Missing
W04-0305,E03-1005,0,0.0374973,"Missing"
W04-0305,C00-1017,0,0.226841,"Missing"
W04-0305,A00-2018,0,0.0644489,"robabilities are summed to decide how to prune to a single choice of at+2 . By expressing the family of deterministic parsers with lookahead in terms of a pruning strategy on a basic parsing model, we are able to easily investigate the effects of different lookahead lengths on the maximum performance of a deterministic parser in this family. To complete the specification of the family of deterministic parsers, we simple have to specify the basic parsing model, as done in the next section. 3 A Generative Left-Corner Probability Model As with several previous statistical parsers (Collins, 1999; Charniak, 2000), we use a generative history-based probability model of parsing. Designing a history-based model of parsing involves two steps, first choosing a mapping from the set of phrase structure trees to the set of parses, and then choosing a probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse. For the model to be generative, these decisions must include predicting the words of the sentence. To support incremental parsing, we want to map phrase structure trees to parses which predict the words of the sentence in their left"
W04-0305,N03-1014,1,0.800694,"inistic parsing takes the extreme position that there can only be one analysis for any sentence prefix. We investigate methods which make such a strong constraint feasible, in particular the use of lookahead. In this paper we do not try to construct a single deterministic parser, but instead consider a family of deterministic parsers and empirically measure the optimal performance of a deterministic parser in this family. As has been previously proposed by Brants and Crocker (2000), we take a corpus-based approach to this empirical investigation, using a previously defined statistical parser (Henderson, 2003). The statistical parser uses an incremental history-based probability model based on left-corner parsing, and the parameters of this model are estimated using a neural network. Performance of this basic model is state-of-the-art, making these results likely to generalize beyond this specific system. We specify the family of deterministic parsers in terms of pruning the search for the most probable parse. Both deterministic parsing and the use of k-word lookahead are characterized as constraints on pruning this search. We then derive the optimal pruning strategy given these constraints and the"
W04-0305,1997.iwpt-1.18,0,0.0328235,"hild has been fully parsed. Then the subtrees for the node’s remaining children are parsed in their left-to-right order. In the form of left-corner parsing we use, parsing a constituent starts by pushing the leftmost word w of the constituent onto the stack with a shift(w) action. Parsing a constituent ends by either introducing the constituent’s parent nonterminal (labeled Y ) with a project(Y) action, or attaching to the parent with a attach action. More precisely, this parsing strategy is a version of left-corner parsing which first applies right-binarization to the grammar, as is done in (Manning and Carpenter, 1997) except that we binarize down to nullary rules rather than to binary rules. This means that choosing the children for a node is done one child at a time, and that ending the sequence of children is a separate choice. We also extended the parsing strategy slightly to handle Chomsky adjunction structures (i.e. structures of the form [X [X . . .] [Y . . .]]) as a special case. The Chomsky adjunction is removed and replaced with a special “modifier” link in the tree (becoming [X . . . [mod Y . . .]]). This means that the parser’s set of basic actions includes modify, as well as attach, shift(w), a"
W04-0305,J93-2004,0,0.039177,"-deterministic pruning tend to be very small compared to the most probable alternatives. The non-deterministic pruning also reduces the set of partial parses which are chosen between during the subsequent deterministic pruning. But this undoubtedly has no significant effect, since experimental results have shown that the level of non-deterministic pruning discussed above does not effect performance even without deterministic pruning. 6 The Experiments To investigate the effects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank (Marcus et al., 1993) datasets. The input to the network is a sequence of tag-word pairs.1 We report results for a vocabulary size of 508 tag-word pairs (a frequency threshold of 200). We first trained a network to estimate the parameters of the basic probability model. We determined appropriate training parameters and network size based on intermediate validation 1 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. This tagger is run before the parser, so there may be some information about future words which is available in the disambiguated tag which is not available in the word itself"
W04-0305,W96-0213,0,0.0169306,"n without deterministic pruning. 6 The Experiments To investigate the effects of lookahead on our family of deterministic parsers, we ran empirical experiments on the standard the Penn Treebank (Marcus et al., 1993) datasets. The input to the network is a sequence of tag-word pairs.1 We report results for a vocabulary size of 508 tag-word pairs (a frequency threshold of 200). We first trained a network to estimate the parameters of the basic probability model. We determined appropriate training parameters and network size based on intermediate validation 1 We used a publicly available tagger (Ratnaparkhi, 1996) to provide the tags. This tagger is run before the parser, so there may be some information about future words which is available in the disambiguated tag which is not available in the word itself. We don’t think this has had a significant impact on the results reported here, but currently we are working on doing the tagging internally to the parser to avoid this problem. 90 88 86 84 deterministic recall deterministic precision non-deterministic recall non-deterministic precision 82 80 0 2 4 6 8 10 12 14 16 Figure 1: Labeled constituent recall and precision as a function of the number of word"
W04-0305,P80-1024,0,0.721243,"choosing a probability model in which the probability of each parser decision is conditioned on the history of previous decisions in the parse. For the model to be generative, these decisions must include predicting the words of the sentence. To support incremental parsing, we want to map phrase structure trees to parses which predict the words of the sentence in their left-to-right order. To support deterministic parsing, we want our parses to specify information about the phrase structure tree at appropriate points in the sentence. For these reasons, we choose a form of left-corner parsing (Rosenkrantz and Lewis, 1970). In a left-corner parse, each node is introduced after the subtree rooted at the node’s first child has been fully parsed. Then the subtrees for the node’s remaining children are parsed in their left-to-right order. In the form of left-corner parsing we use, parsing a constituent starts by pushing the leftmost word w of the constituent onto the stack with a shift(w) action. Parsing a constituent ends by either introducing the constituent’s parent nonterminal (labeled Y ) with a project(Y) action, or attaching to the parent with a attach action. More precisely, this parsing strategy is a versi"
W04-0305,J03-4003,0,\N,Missing
W06-1666,P05-1022,0,0.198683,"Missing"
W06-1666,P02-1034,0,0.0317966,"terms of a history-based generative probability model. These parameters are estimated using a neural network, the weights of which form the second level of parameterization. This approach allows the probability model to have an infinite number of parameters; the neural network only estimates the bounded number of parameters which are relevant to a given partial parse. We define data-defined kernels in terms of the second level of parameterization (the network weights). For the last set of experiments, we used the probabilistic model described in (Collins, 1999) (model 2), and the Tree Kernel (Collins and Duffy, 2002). However, in these experiments we only used the estimates from the discriminative classifier, so the details of the probabilistic model are not relevant. SSN TRK SSN-Estim LLK-Learn LK-Learn FK-Estim TRK-Estim R 80.9 81.1 81.4 81.2 81.5 81.4 81.5 P 81.7 82.4 82.3 82.4 82.2 82.6 82.8 F1 81.3 81.7 81.8 81.8 81.8 82.0 82.1 CM 18.3 18.2 18.3 17.6 17.8 18.3 18.6 Table 1: Percentage labeled constituent recall (R), precision (P), combination of both (F1 ) and percentage complete match (CM) on the testing set. lary if it occurred at least 20 time in the training set, which (with tag-unknown-word pair"
W06-1666,J93-2004,0,0.0279253,"s as a sample from the discriminative distribution. Kernel, like the TOP kernel for reranking, can be motivated by the minimization of the classification error of a linear classifier w T φLLK (x, y), where θˆ LLK φθˆ (x, y) is the feature extractor of the kernel given by: 4 ˆ = log( v(x, y, θ) φLLK (x, y) = θˆ ˆ ˆ ˆ ∂v(x, y, θ) ,..., ∂v(x, y, θ) ), (v(x, y, θ), ∂θ1 ∂θl where Expected Loss Learning log( X 0 ˆ P (y 0 |x, θ)∆(y , y)). 5 Experimental Evaluation To perform empirical evaluations of the proposed methods, we considered the task of parsing the Penn Treebank Wall Street Journal corpus (Marcus et al., 1993). First, we perform experiments with SVM Struct (Tsochantaridis et al., 2004) as the learner. Since SVM Struct already uses the loss function during training to rescale the margin or slack variables, this learner allows us to test the hypothesis that loss functions are useful in parsing not only to define the optimization criteria but also to define the classifier and to define the feature space. However, SVM Struct training for large scale parsing experiments is computationally expensive2 , so here we use only a small portion of the available training data to perform evaluations of the differ"
W06-1666,J05-1003,0,0.127938,"ou are only interested in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on E"
W06-1666,W96-0213,0,0.121512,"ed on loss, using the Loss Logit Kernel (equation (13)) and the Loss Kernel (equation (12)), respectively. FKEstim and TRK-Estim are the models which esti5.2 Experiments with SVM Struct Both the neural network probabilistic model and the kernel based classifiers were trained on section 0 (1,921 sentences, 40,930 words). Section 24 (1,346 sentences, 29,125 words) was used as the validation set during the neural network learning and for choosing parameters of the models. Section 23 (2,416 sentences, 54,268 words) was used for the final testing of the models. We used a publicly available tagger (Ratnaparkhi, 1996) to provide the part-of-speech tags for each word in the sentence. For each tag, there is an unknown-word vocabulary item which is used for all those words which are not sufficiently frequent with that tag to be included individually in the vocabulary. For these experiments, we only included a specific tag-word pair in the vocabu3 All our results are computed with the evalb program (Collins, 1999). 564 of the loss with data-defined kernels (12) and (13) was achieved when the parameter A is close to the inverse of the first component of the learned decision vector, which confirms the motivation"
W06-1666,W03-0402,0,0.0809515,"s), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560–5"
W06-1666,P05-1023,1,0.213563,"actly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (E"
W06-1666,W04-3201,0,0.0641018,"Missing"
W06-1666,N03-1014,1,0.854318,"(x) P (y|x)∆(y, y P 0) y∈G(x) P (y|x) . (4) For the reranking case, often the probabilistic model only estimates the joint probability P (x, y). However, neither this difference nor the denominator in (4) affects the classification. Thus, replacing the true probabilities with their estimates, we can define the classifier where G(x) denotes a candidate list provided by a baseline probabilistic model for the input x. In this paper we propose different approaches to loss approximation. We apply them to the parse reranking problem where the baseline probabilistic model is a neural network parser (Henderson, 2003), and to parse reranking of candidates provided by the (Collins, 1999) model. The resulting reranking method achieves very significant improvement in the considered loss function and improvement in most other standard measures of accuracy. In the following three sections we will discuss three approaches to learning such a classifier. The first two derive a classification criteria for use with a predefined probability model (the first generative, the second discriminative). The third defines a kernel for use with a classification method for minimizing loss. All use previously proposed learning"
W06-1666,P04-1013,1,0.885953,"Missing"
W06-1666,W05-1506,0,0.0549458,"Missing"
W06-1666,H05-1064,0,0.01165,"in getting the label exactly correct (i.e. 0-1 loss), and you think the estimates are accurate. But if you are interested in a loss function where the loss is small when you choose a candidate which is similar to the correct candidate, then it is better to choose one of the candidates in the group. With this choice the loss will only be large if the outlier turns out to be correct, while if the outlier is chosen then the loss will be large if any of the group are correct. In other words, the expected loss of Introduction The reranking approach is widely used in parsing (Collins and Koo, 2005; Koo and Collins, 2005; Henderson and Titov, 2005; Shen and Joshi, 2003) as well as in other structured classification problems. For structured classification tasks, where labels are complex and have an internal structure of interdependency, the 0-1 loss considered in classical formulation of classification algorithms is not a natural choice and different loss functions are normally employed. To tackle this problem, several approaches have been proposed to accommodate loss functions in learning algorithms (Tsochantaridis et al., 2004; Taskar et al., 560 Proceedings of the 2006 Conference on Empirical Methods in Nat"
W06-1666,C00-2137,0,0.0316772,"Missing"
W06-1666,N04-1022,0,0.12775,"Missing"
W06-1666,J03-4003,0,\N,Missing
W06-1666,P04-1043,0,\N,Missing
W06-1666,P96-1024,0,\N,Missing
W06-2902,P02-1034,0,0.33889,"used to train this parser to optimize performance on a small in-domain corpus. Large margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize a measure which is directly related to the expected testing performance. They achieve especially good performance compared to other classifiers when only a small amount of training data is available. Most of the large margin methods need the definition of a kernel. Work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g. (Collins and Duffy, 2002)), which are chosen on the basis of domain knowledge. In (Henderson and Titov, 2005) it was proposed to apply a class of kernels derived from probabilistic models to the natural language parsing problem. In (Henderson and Titov, 2005), the kernel is constructed using the parameters of a trained probabilistic model. This type of kernel is called a datadefined kernel, because the kernel incorporates information from the data used to train the probabilistic model. We propose to exploit this property to transfer information from a large corpus to a statis6 Proceedings of the 10th Conference on Com"
W06-2902,P97-1003,0,0.208877,"Missing"
W06-2902,W01-0521,0,0.0294224,"k probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data and a small amount of in-domain data. In this paper, we propose using data-defined kernels and large margin methods to specifically address porting a parser to a new domain. Data-defined kernels are used to construct a new parser"
W06-2902,P05-1023,1,0.691912,"rge margin methods have demonstrated substantial success in applications to many machine learning problems, because they optimize a measure which is directly related to the expected testing performance. They achieve especially good performance compared to other classifiers when only a small amount of training data is available. Most of the large margin methods need the definition of a kernel. Work on kernels for natural language parsing has been mostly focused on the definition of kernels over parse trees (e.g. (Collins and Duffy, 2002)), which are chosen on the basis of domain knowledge. In (Henderson and Titov, 2005) it was proposed to apply a class of kernels derived from probabilistic models to the natural language parsing problem. In (Henderson and Titov, 2005), the kernel is constructed using the parameters of a trained probabilistic model. This type of kernel is called a datadefined kernel, because the kernel incorporates information from the data used to train the probabilistic model. We propose to exploit this property to transfer information from a large corpus to a statis6 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 6–13, New York City, June 20"
W06-2902,N03-1014,1,0.777253,"train a probabilistic model on both the large corpus and the target corpus. The kernel is derived from this trained model. In both scenarios, the kernel is used in a SVM classifier (Tsochantaridis et al., 2004) trained on a small amount of data from the target domain. This classifier is trained to rerank the candidate parses selected by the associated probabilistic model. We use the Penn Treebank Wall Street Journal corpus as the large corpus and individual sections of the Brown corpus as the target corpora (Marcus et al., 1993). The probabilistic model is a neural network statistical parser (Henderson, 2003), and the data-defined kernel is a TOP reranking kernel (Henderson and Titov, 2005). With both scenarios, the resulting parser demonstrates improved accuracy on the target domain over the probabilistic model alone. In additional experiments, we evaluate the hypothesis that the primary issue for porting parsers between domains is differences in the distributions of words in structures, and not in the distributions of the structures themselves. We partition the parameters of the probability model into those which define the distributions of words and those that only involve structural decisions,"
W06-2902,H05-1064,0,0.0119417,"ables (e.g. for PCFG models), the features of the data-defined kernel (except for the first feature) are a function of the counts used to estimate the model. For a PCFG, each such feature is a function of one rule’s counts, where the counts from different candidates are weighted using the probability estimates from the model. With latent variables, the meaning of the variable (not just its value) is learned from the data, and the associated features of the data-defined kernel capture this induced meaning. There has been much recent work on latent variable models (e.g. (Matsuzaki et al., 2005; Koo and Collins, 2005)). We choose to use an earlier neural network based probabilistic model of pars9 ing (Henderson, 2003), whose hidden units can be viewed as approximations to latent variables. This parsing model is also a good candidate for our experiments because it achieves state-of-the-art results on the standard Wall Street Journal (WSJ) parsing problem (Henderson, 2003), and data-defined kernels derived from this parsing model have recently been used with the Voted Perceptron algorithm on the WSJ parsing task, achieving a significant improvement in accuracy over the neural network parser alone (Henderson"
W06-2902,J93-2004,0,0.0315923,"babilistic model trained on the source domain (and possibly also the target domain) is used to define a kernel, which is then used in a large margin classifier trained only on the target domain. With a SVM classifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data an"
W06-2902,P05-1010,0,0.054943,"Missing"
W06-2902,N03-1027,0,0.667449,"ssifier and a neural network probabilistic model, this method achieves improved performance over the probabilistic model alone. 1 Introduction In recent years, significant progress has been made in the area of natural language parsing. This research has focused mostly on the development of statistical parsers trained on large annotated corpora, in particular the Penn Treebank WSJ corpus (Marcus et al., 1993). The best statistical parsers have shown good results on this benchmark, but these statistical parsers demonstrate far worse results when they are applied to data from a different domain (Roark and Bacchiani, 2003; Gildea, 2001; Ratnaparkhi, 1999). This is an important problem because we cannot expect to have large annotated corpora available for most domains. While identifying this problem, previous work has not proposed parsing methods which are specifically designed for porting parsers. Instead they propose methods for training a standard parser with a large amount of out-of-domain data and a small amount of in-domain data. In this paper, we propose using data-defined kernels and large margin methods to specifically address porting a parser to a new domain. Data-defined kernels are used to construct"
W06-2902,W03-0402,0,0.0281828,"training of standard large margin methods, like SVMs, isn’t feasible on a large corpus, it is quite tractable to train them on a small target corpus.1 Also, the choice of the large margin classifier is motivated by their good generalization properties on small datasets, on which accurate probabilistic models are usually difficult to learn. We hypothesize that differences in vocabulary across domains is one of the main difficulties with parser portability. To address this problem, we propose constructing the kernel from a probabilistic model which has been reparameterized to better suit 1 In (Shen and Joshi, 2003) it was proposed to use an ensemble of SVMs trained the Wall Street Journal corpus, but we believe that the generalization performance of the resulting classifier is compromised in this approach. 8 the target domain vocabulary. As in other lexicalized statistical parsers, the probabilistic model we use treats words which are not frequent enough in the training set as ‘unknown’ words (Henderson, 2003). Thus there are no parameters in this model which are specifically for these words. When we consider a different target domain, a substantial proportion of the words in the target domain are treat"
W06-2902,C00-2137,0,0.0533585,"Missing"
W06-2902,J03-4003,0,\N,Missing
W07-2218,W06-2920,0,0.0386929,"itioning features. To define this model we use a recently proposed class of Bayesian Networks for structured prediction, Incremental Sigmoid Belief Networks. We demonstrate that the proposed model achieves state-of-the-art results on three different languages. We also demonstrate that the features induced by the ISBN’s latent variables are crucial to this success, and show that the proposed model is particularly good on long dependencies. 1 Introduction Dependency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unli"
W07-2218,P05-1022,0,0.0279498,"Missing"
W07-2218,A00-2018,0,0.113921,"and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges on"
W07-2218,dzeroski-etal-2006-towards,0,0.0119801,"feature set and disable the feature induction abilities of the model by removing all the edges between latent variables vectors. Comparison of this restricted model with the full ISBN model shows how important the feature induction is. Also, comparison of this restricted model with the MALT parser, which uses the same set of features, indicates whether our generative estimation method and use of beam search is beneficial. 6.1 Experimental Setup We used the CoNLL-X distributions of Danish DDT treebank (Kromann, 2003), Dutch Alpino treebank (van der Beek et al., 2002) and Slovene SDT treebank (Dzeroski et al., 2006). The choice of these treebanks was motivated by the fact that they all are freely distributed and have very different sizes of their training sets: 195,069 tokens for Dutch, 94,386 tokens for Danish and only 28,750 tokens for Slovene. As it is generally believed that discriminative models win over generative models with a large amount of training data, so we expected to see similar trend in our results. Test sets are about equal and contain about 5,000 scoring tokens. We followed the experimental setup of the shared task and used all the information provided for the languages: gold standard p"
W07-2218,P05-1023,1,0.935194,"dependencies. Additional experiments suggest that both feature induction abilities and use of the beam search contribute to this improvement. The fact that our model defines a probability model over parse trees, unlike the previous state-ofthe-art methods (Nivre et al., 2006; McDonald et al., 2006), makes it easier to use this model in applications which require probability estimates, e.g. in language processing pipelines. Also, as with any generative model, it may be easy to improve the parser’s accuracy by using discriminative retraining techniques (Henderson, 2004) or data-defined kernels (Henderson and Titov, 2005), with or even without introduction of any additional linguistic features. In addition, there are some applications, such as language modeling, which require generative models. Another advantage of generative models is that they do not suffer from the label bias problems (Bottou, 1991), which is a potential problem for conditional or deterministic history-based models, such as (Nivre et al., 2004). In the remainder of this paper, we will first review general ISBNs and how they can be approximated. Then we will define the generative parsing model, based on the algorithm of (Nivre et al., 2004),"
W07-2218,N03-1014,1,0.944228,"se a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on P"
W07-2218,P04-1013,1,0.934251,"variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, p"
W07-2218,H05-1064,0,0.010175,"it can not completely eliminate it. 153 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for con"
W07-2218,P05-1010,0,0.0183894,"eliminate it. 153 7 Related Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking"
W07-2218,W06-2932,0,0.581035,"language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities whi"
W07-2218,W04-2407,0,0.496553,"the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories in these models is to choose a pre-defined set of features which can be unambiguously derived from the history (e.g. (Charniak, 2000; Collins, 1999; Nivre et al., 2004)). Decision probabilities are then assumed to be independent of all information not represented by this finite set of features. ISBNs instead use a vector of binary 144 Proceedings of the 10th Conference on Parsing Technologies, pages 144–155, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics latent variables to encode the information about the parser history. This history vector is similar to the hidden state of a Hidden Markov Model. But unlike the graphical model for an HMM, which specifies conditional dependency edges only between adjacent states in the se"
W07-2218,W06-2933,0,0.0615868,"ndency parsing has been a topic of active research in natural language processing during the last several years. The CoNLL-X shared task (Buchholz and Marsi, 2006) made a wide selection of standardized treebanks for different languages available for the research community and allowed for easy comparison between various statistical methods on a standardized benchmark. One of the surprising things discovered by this evaluation is that the best results are achieved by methods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov a"
W07-2218,W05-1512,0,0.0253468,"ted Work There has not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency"
W07-2218,P02-1035,0,0.0109569,"as not been much previous work on latent variable models for dependency parsing. Dependency parsing with Dynamic Bayesian Networks was considered in (Peshkin and Savova, 2005), with limited success. Roughly, the model considered the whole sentence at a time, with the DBN being used to decide which words correspond to leaves of the tree. The chosen words are then removed from the sentence and the model is recursively applied to the reduced sentence. Recently several latent variable models for constituent parsing have been proposed (Koo and Collins, 2005; Matsuzaki et al., 2005; Prescher, 2005; Riezler et al., 2002). In (Matsuzaki et al., 2005) non-terminals in a standard PCFG model are augmented with latent variables. A similar model of (Prescher, 2005) uses a head-driven PCFG with latent heads, thus restricting the flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define th"
W07-2218,N03-1028,0,0.0199101,"e flexibility of the latent-variable model by using explicit linguistic constraints. While the model of (Matsuzaki et al., 2005) significantly outperforms the constrained model of (Prescher, 2005), they both are well below the state-of-the-art in constituent parsing. In (Koo and Collins, 2005), an undirected graphical model for constituent parse reranking uses dependency relations to define the edges. Thus, it should be easy to apply a similar method to reranking dependency trees. Undirected graphical models, in particular Conditional Random Fields, are the standard tools for shallow parsing (Sha and Pereira, 2003). However, shallow parsing is effectively a sequence labeling problem and therefore differs significantly from full parsing. As discussed in (Titov and Henderson, 2007), undirected graphical models do not seem to be suitable for history-based parsing models. Sigmoid Belief Networks (SBNs) were used originally for character recognition tasks, but later a dynamic modification of this model was applied to the reinforcement learning task (Sallans, 2002). However, their graphical model, approximation method, and learning method differ significantly from those of this paper. The extension of dynamic"
W07-2218,P07-1080,1,0.088657,"ods which are quite different from state-of-the-art models for constituent parsing, e.g. the deterministic parsing method of (Nivre et al., 2006) and the minimum spanning tree parser of (McDonald et al., 2006). All the most accurate dependency parsing models are fully discriminative, unlike constituent parsing where all the state of the art methods have a generaIn this paper we propose a generative latent variable model for dependency parsing. It is based on Incremental Sigmoid Belief Networks (ISBNs), a class of directed graphical model for structure prediction problems recently proposed in (Titov and Henderson, 2007), where they were demonstrated to achieve competitive results on the constituent parsing task. As discussed in (Titov and Henderson, 2007), computing the conditional probabilities which we need for parsing is in general intractable with ISBNs, but they can be approximated efficiently in several ways. In particular, the neural network constituent parsers in (Henderson, 2003) and (Henderson, 2004) can be regarded as coarse approximations to their corresponding ISBN model. ISBNs use history-based probability models. The most common approach to handling the unbounded nature of the parse histories"
W07-2218,N07-1051,0,\N,Missing
W07-2218,D07-1097,0,\N,Missing
W07-2218,D07-1100,0,\N,Missing
W07-2218,J93-2004,0,\N,Missing
W07-2218,W07-2201,0,\N,Missing
W07-2218,W07-2207,0,\N,Missing
W07-2218,J03-4003,0,\N,Missing
W07-2218,P08-2054,0,\N,Missing
W07-2218,W07-2202,0,\N,Missing
W07-2218,P06-1055,0,\N,Missing
W07-2218,W08-2122,1,\N,Missing
W07-2218,W07-2416,0,\N,Missing
W07-2218,D07-1072,0,\N,Missing
W08-2122,J93-2004,0,0.0363444,"s statistical parsing and tagging, have recently paved the way to statistical learning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid"
W08-2122,W08-2101,1,0.889757,"Missing"
W08-2122,W04-2705,0,0.277552,"Missing"
W08-2122,N06-2026,1,0.878058,"Missing"
W08-2122,W06-2933,0,0.320816,"e Probability Model Our probability model is a joint generative model of syntactic and semantic dependencies. The two dependency structures are specified as the sequence of actions for a synchronous parser, which requires each dependency structure to be projec178 CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 178–182 Manchester, August 2008 tivised separately. 2.1 Synchronous derivations The derivations for syntactic dependency trees are the same as specified in (Titov and Henderson, 2007b), which are based on the shift-reduce style parser of (Nivre et al., 2006). The derivations use a stack and an input queue. There are actions for creating a leftward or rightward arc between the top of the stack and the front of the queue, for popping a word from the stack, and for shifting a word from the queue to the stack. The derivations for semantic dependency graphs use virtually the same set of actions, but impose fewer constraints on when they can be applied, due to the fact that a word in a semantic dependency graph can have more than one parent. An additional action predicates was introduced to label a predicate with sense s. Let Td be a syntactic dependen"
W08-2122,J05-1004,0,0.0727012,"arning techniques for levels of semantic representation, such as recovering the logical form of a sentence for information extraction and question-answering applications (e.g. (Wong and Mooney, 2007)) or jointly learning the syntactic structure of the sentence and the propositional argument-structure of its main predicates (Musillo and Merlo, 2006; Merlo and Musillo, 2008). In this vein, the CoNLL 2008 shared task sets the challenge of learning jointly both syntactic dependencies (extracted from the Penn Treebank (Marcus et al., 1993) ) and semantic dependencies (extracted both from PropBank (Palmer et al., 2005) ∗ c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic"
W08-2122,W08-2121,0,0.159555,"Missing"
W08-2122,P07-1080,1,0.599944,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,W07-2218,1,0.911479,"ke 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 0 Authors in alphabetical order. and NomBank (Meyers et al., 2004) under a unified representation. We propose a solution that uses a generative history-based model to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies. Our probabilistic model is based on Incremental Sigmoid Belief Networks (ISBNs), a recently proposed latent variable model for syntactic structure prediction, which has shown very good behaviour for both constituency (Titov and Henderson, 2007a) and dependency parsing (Titov and Henderson, 2007b). The ability of ISBNs to induce their features automatically enables us to extend this architecture to learning a synchronous parse of syntax and semantics without modification of the main architecture. By solving the problem with synchronous parsing, a probabilistic model is learnt which maximises the joint probability of the syntactic and semantic dependencies and thereby guarantees that the output structure is globally coherent, while at the same time building the two structures separately. This extension of the ISBN architecture is the"
W08-2122,P07-1121,0,0.0403536,"Missing"
W08-2122,W06-2303,1,\N,Missing
W09-1205,burchardt-etal-2006-salsa,0,0.0412684,"Missing"
W09-1205,W08-2122,1,0.777374,"g. Titov et al. (2009) found that only using the Swap action as a last resort is the best strategy for English (compared to using it preemptively to address future crossing arcs) and we use the same strategy here for all languages. Syntactic graphs do not use a Swap action. We adopt the HEAD method of Nivre and Nilsson (2005) to de-projectivise syntactic dependencies outside of parsing.1 3 Features and New Developments The synchronous derivations described above are modelled with a type of Bayesian Network called an Incremental Sigmoid Belief Network (ISBN) (Titov and Henderson, 2007a). As in Henderson et al. (2008), the ISBN model distinguishes two types of latent states: syntactic states, when syntactic decisions are considered, and semantic states, when semantic decision are considered. Latent states are vectors of binary latent variables, which are conditioned on variables from previous states via a pattern of connecting edges determined by the previous decisions. These latent-to-latent connections are used to engineer soft biases which reflect the relevant domains of locality in the structure being built. For these we used the set of connections proposed in Titov et al. (2009), which includes latent"
W09-1205,kawahara-etal-2002-construction,0,0.0146671,"ment score. All development effort took about two personmonths, mostly by someone who had no previous experience with the system. Most of this time was spent on the above differences in the task definition between the 2008 and 2009 shared tasks. 4 Results and Discussion We participated in the joint task of the closed challenge, as described in Hajiˇc et al. (2009). The datasets used in this challenge are described in Taul´e et al. (2008) (Catalan and Spanish), Palmer and Xue (2009) (Chinese), Hajiˇc et al. (2006) (Czech), Surdeanu et al. (2008) (English), Burchardt et al. (2006) (German), and Kawahara et al. (2002) (Japanese). Rank Average Catalan Chinese Czech 3 82.14 82.66 76.15 83.21 1 @85.77 @87.86 76.11 @80.38 3 78.42 77.44 76.05 86.02 macro F1 syntactic acc semantic F1 English German Japanese Spanish 86.03 79.59 84.91 82.43 88.79 87.29 92.34 @87.64 83.24 71.78 77.23 77.19 Table 2: The three main scores for our system. Rank is within task. Rank macro F1 syn Acc sem F1 3 2 3 Ave 75.93 78.01 73.63 Cze-ood Eng-ood Ger-ood @80.70 75.76 71.32 @76.41 80.84 76.77 84.99 70.65 65.25 Table 3: Results on out-of-domain for our system. Rank is within task. The official results on the testing set are shown in ta"
W09-1205,P08-1069,0,0.0241132,"Missing"
W09-1205,P05-1013,0,0.0314403,"Missing"
W09-1205,W06-2933,0,0.024019,"Missing"
W09-1205,W08-2121,0,0.154607,"Missing"
W09-1205,taule-etal-2008-ancora,0,0.083404,"Missing"
W09-1205,P07-1080,1,0.811623,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1099,1,0.816134,"een previously imple1 The statistics in Table 1 suggest that, for some languages, swapping might be beneficial for syntax as well. 39 mented. For the former modifications, the system was adapted to allow the use of the PFEAT and FILLPRED fields in the data, which both resulted in improved accuracy for all the languages. The PFEAT data field (automatically predicted morphological features) was introduced in the system in two ways, as an atomic feature bundle that is predicted when predicting the word, and split into its elementary components when conditioning on a previous word, as was done in Titov and Henderson (2007b). Because the testing data included a specification of which words were annotated as predicates (the FILLPRED data field), we constrained the parser’s output so as to be consistent with this specification. For rare predicates, if the predicate was not in the parser’s lexicon (extracted from the training set), then a sense was taken from the list of senses reported in the Lexicon and Frame Set resources available for the closed challenge. If this information was not available, then a default sense was constructed based on the automatically predicted lemma (PLEMMA) of the predicate. We also ma"
W09-1205,D07-1096,0,\N,Missing
W11-2909,P11-2003,1,0.823392,"sequences. A DBN specifies a bounded BN that models one position in the sequence, including input and output variables. A new instance of this BN is created for each position in the given sequence, with the outputs for one position’s BN equated with the inputs of the subsequent position’s BN. DBNs are equivalent to BNAs with finite state automata. The bounded BN of the DBN corresponds to the T of the BNA. The DBN has no need for an explicit representation of the BNA’s configuration structure because 4 This argument for directed models was recognised previously in (Titov and Henderson, 2007). Garg and Henderson (2011) proposes a model which mixes directed and undirected edges, but which still has this property because undirected edges are all local to individual derivation steps. 68 which are not on the top of the stack, and therefore the derivation structure would not be contextfree. Given the restriction of locality, it is probably possible to devise chains of variables which pass the necessary information through the context-free structure of the tree. But without such a transformation, our analysis of inference algorithms in the next section suggests that it would be difficult to devise efficient algor"
W11-2909,P05-1010,0,0.168217,"graphical models and formal language theory, we show that, for a large class of automata, the complexity of inference with a BNA is bounded by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. 1 Introduction Work in Computational Linguistics has developed increasingly sophisticated probabilistic models of language. For example, Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005) have been developed with latent head labels (Prescher, 2005), multiple latent variables decorating each nonterminal (Musillo and Merlo, 2008), and a hierarchy of latent nonterminal subcategories (Liang et al., 2007). In this paper we propose a general framework which facilitates the specification and parsing of such complex models by exploiting graphical models to express local bounded statistical relationships, while still allowing the use of grammar formalisms to express the unbounded nature of natural language. Graphical models were developed as a unification of probability theory and grap"
W11-2909,N03-1014,1,0.738424,"gests a close relationship to the context-free derivation structures discussed in the next section. The previous work on graphical models which is closest to our proposal is Incremental Sigmoid Belief Networks (ISBNs) (Henderson and Titov, 2010). Sigmoid Belief Networks (SBNs) (Neal, 1992) are a type of Bayesian Network, and ISBNs use the same technique as here for modelling arbitrarily large structures, namely incrementally constructing an SBN that generates the derivation of the structure. Henderson and Titov (2010) used this framework to reinterpret previous work on neural network parsing (Henderson, 2003) as an approximation to inference in ISBNs, and proposed another approximate inference method for natural language parsing. However, the control mechanism needed to construct an SBN for a derivation was not formalised. From the grammar formalism side, the model that is closest to our proposal is Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005). LPCFGs are PCFGs where the nonterminal labels are augmented with a latent variable. There has been much work recently on different training methods and different restrictions to the pattern of values which the latent variable"
W11-2909,P08-2054,0,0.130646,"by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. 1 Introduction Work in Computational Linguistics has developed increasingly sophisticated probabilistic models of language. For example, Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005) have been developed with latent head labels (Prescher, 2005), multiple latent variables decorating each nonterminal (Musillo and Merlo, 2008), and a hierarchy of latent nonterminal subcategories (Liang et al., 2007). In this paper we propose a general framework which facilitates the specification and parsing of such complex models by exploiting graphical models to express local bounded statistical relationships, while still allowing the use of grammar formalisms to express the unbounded nature of natural language. Graphical models were developed as a unification of probability theory and graph theory. They 63 Proceedings of the 12th International Conference on Parsing Technologies, pages 63–74, c 2011 Association for Computational"
W11-2909,P06-1055,0,0.0275239,"oximate inference method for natural language parsing. However, the control mechanism needed to construct an SBN for a derivation was not formalised. From the grammar formalism side, the model that is closest to our proposal is Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005). LPCFGs are PCFGs where the nonterminal labels are augmented with a latent variable. There has been much work recently on different training methods and different restrictions to the pattern of values which the latent variables are allowed to take (e.g. (Matsuzaki et al., 2005; Prescher, 2005; Petrov et al., 2006; Musillo and Merlo, 2008)). LPCFGs can be modelled as BNAs with push-down automata. The bounded BN of the equivalent BNA includes both a variable for the visible nonterminal label of the LPCFG and a variable for the latent extension of the nonterminal label. As a more specific example, the latent head labels of (Prescher, 2005) could be specified using a BN with a switching variable that selects which child’s head variable is propagated to the head variable of the parent. Musillo and Merlo (2008) extend LPCFGs to include multiple latent variables decorating each nonterminal, with linguistical"
W11-2909,D07-1072,0,0.0205481,"xity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. 1 Introduction Work in Computational Linguistics has developed increasingly sophisticated probabilistic models of language. For example, Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005) have been developed with latent head labels (Prescher, 2005), multiple latent variables decorating each nonterminal (Musillo and Merlo, 2008), and a hierarchy of latent nonterminal subcategories (Liang et al., 2007). In this paper we propose a general framework which facilitates the specification and parsing of such complex models by exploiting graphical models to express local bounded statistical relationships, while still allowing the use of grammar formalisms to express the unbounded nature of natural language. Graphical models were developed as a unification of probability theory and graph theory. They 63 Proceedings of the 12th International Conference on Parsing Technologies, pages 63–74, c 2011 Association for Computational Linguistics October 5-7, 2011, Dublin City University. 2 to specify its st"
W11-2909,W05-1512,0,0.136576,"ge class of automata, the complexity of inference with a BNA is bounded by the complexity of inference in the bounded Bayesian Network times the complexity of inference for the equivalent stochastic automaton. This illustrates that BNAs provide a useful framework for developing and analysing models and algorithms for structure prediction. 1 Introduction Work in Computational Linguistics has developed increasingly sophisticated probabilistic models of language. For example, Latent Probabilistic Context-Free Grammars (LPCFGs) (Matsuzaki et al., 2005) have been developed with latent head labels (Prescher, 2005), multiple latent variables decorating each nonterminal (Musillo and Merlo, 2008), and a hierarchy of latent nonterminal subcategories (Liang et al., 2007). In this paper we propose a general framework which facilitates the specification and parsing of such complex models by exploiting graphical models to express local bounded statistical relationships, while still allowing the use of grammar formalisms to express the unbounded nature of natural language. Graphical models were developed as a unification of probability theory and graph theory. They 63 Proceedings of the 12th International Confe"
W11-2909,H92-1027,0,0.172514,"context-free derivations, we can feasibly compute the marginal probability of any derivation using belief propagation. So far in this subsection we have only considered inference for a given derivation structure. In general, inference in BNAs requires marginalising both over labellings and over the structure itself, as is commonly required for unsupervised or partially-supervised grammar induction. Marginalising over structures can be done with the inside-outside algorithm for Context-Free Grammars (Baker, 1979) (given in figure 3) or the inside-outside algorithm for Tree-Adjoining Grammars (Schabes, 1992). These are dynamic programming algorithms. The inside calculations are very similar to bottom-up parsing algorithms such as CKY (Younger, 1967), except they compute sums of probabilities instead of taking the maximum probability. The outside calculations are also done in a single pass, but top-down. Both inside and outside calculations work by incrementally considering equivalence classes of increasingly large sub-graphs of the derivation structure. For example in the equations in figure 3, each step computes a sum over the various ways that an operation can be used to construct a larger sub-"
W11-2909,D08-1016,0,0.0276699,"n practice with PCFGs to transform them into a form where the depth of the tree is bounded by the length of the string. This transformation (such as binarisation, or Chomsky normal form) does not generate the same tree structures, but it does generate the same strings. With the transformed PCFG, given the string length, it is possible to construct the whole relevant bounded-depth model structure for that string, for example using case-factor diagrams (McAllester et al., 2004), or sum-product networks (Poon and Domingos, 2011). Inference can then be done, for example, using belief propagation (Smith and Eisner, 2008). However, this approach is limited to inference problems where the string length is known, which in turn limits the potential training methods. We cannot necessarily use this pre-construction method if we want to take a generative approach, where the string length is determined by a generative process, or if we want to do incremental interpretation, where we want to do inference on prefixes of the string without knowing to total string length. In this situation, we might need to consider an infinite amount of model structure, for all the possible string lengths. In particular, this is true fo"
W13-4026,W12-1509,1,0.682243,"Missing"
W13-4026,P13-1123,1,0.0875028,"Missing"
W13-4026,W11-2014,0,0.0111653,"d phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive search through a Spoken Dialogue System (SDS). Th"
W13-4026,E09-1085,0,0.210008,"uthentic Afghan cuisine. USR What is the address and phone number? SYS The address 2424 Van Ness Ave .... Table 1: Example dialogue excerpt for restaurant information in San Francisco 2 Background Previous work includes systems that can deal with ‘micro-turns’ (i.e. sub-utterance processing units), resulting in dialogues that are more fluid and responsive. This has been backed up by a large body of psycholinguistic literature that indicates that human-human interaction is in fact incremental (Levelt, 1989). It has been shown that incremental dialogue behaviour can improve the user experience (Skantze and Schlangen, 2009; Baumann et al., 2011; Selfridge et al., 2011) and enable the system designer to model several dialogue phenomena that play a vital role in human discourse (Levelt, 1989) but have so far been absent from systems. These dialogue phenomena that will be demonstrated by the Parlance system include more natural turntaking through rapid system responses, generation of backchannels and user barge-ins. The system differentiates from other incremental systems in that it is entirely data-driven with an infrastructure that potentially scales well. Introduction The Parlance system provides interactive se"
W14-4336,P13-1123,1,0.850652,"Missing"
W14-4336,W13-4026,1,0.906157,"LG) components. We demonstrate a mobile application in English and Mandarin to test and evaluate components of the Parlance dialogue system for interactive search under real-world conditions. 1 Introduction With the advent of evaluations “in the wild”, emphasis is being put on converting research prototypes into mobile applications that can be used for evaluation and data collection by real users downloading the application from the market place. This is the motivation behind the work demonstrated here where we present a modular framework whereby research components from the Parlance project (Hastie et al., 2013) can be plugged in, tested and evaluated in a mobile environment. The goal of Parlance is to perform interactive search through speech in multiple languages. The domain for the demonstration system is interactive search for restaurants in Cambridge, UK for Mandarin and San Francisco, USA for English. The scenario is that Mandarin speaking tourists would be able to download the application and use it to learn about restaurants in English speaking towns and cities. 2 Figure 1: Overview of the Parlance Mandarin mobile application system architecture Figure 2: Overview of the Parlance English mobi"
W16-1809,W03-1812,0,0.473843,"Missing"
W16-1809,I11-1130,0,0.0524065,"Missing"
W16-1809,J90-1003,0,0.397984,"Missing"
W16-1809,J93-1003,0,0.310931,"Missing"
W16-1809,W14-0802,1,0.899705,"Missing"
W16-1809,W15-0905,1,0.877148,"Missing"
W16-1809,W15-0904,1,0.875146,"Missing"
W16-1809,N10-1029,0,0.255353,"Missing"
W16-1809,D11-1067,0,0.249835,"Missing"
W16-1809,S12-1021,0,0.0360645,"Missing"
W16-1809,D07-1028,0,0.0906215,"Missing"
W16-1809,S13-1038,0,0.253108,"Missing"
W16-1809,N10-1089,0,0.213442,"Missing"
W16-1809,W03-1810,0,0.17698,"Missing"
W16-1809,D07-1039,0,0.384949,"Missing"
W16-1809,W09-2907,0,0.26686,"Missing"
W16-1809,D15-1290,0,0.195817,"Missing"
W16-1809,P06-1120,0,0.024854,"Missing"
W16-1809,J93-1007,0,0.840798,"Missing"
W16-1809,D15-1201,1,0.911967,"Missing"
W16-1809,W12-3311,0,0.0363545,"Missing"
W16-1809,I11-1024,0,0.203801,"Missing"
W18-6308,N18-1172,0,0.0542202,"tations grounded to word semantics for zero-shot image classification (Weston et al., 2011; Socher et al., 2013; Zhang et al., 2016), but there are fewer such studies for NLP tasks. (Yazdani and Henderson, 2015) proposed a zero-shot spoken language understanding model based on a bilinear joint space trained with hinge loss, and (Nam et al., 2016b), proposed a similar joint space trained with a WARP loss for zero-shot biomedical semantic indexing. In addition, there exist studies which aim to learn output representations directly from data such as (Srikumar and Manning, 2014; Yeh et al., 2018; Augenstein et al., 2018); their lack of semantic grounding to the input embeddings and the vocabulary-dependent parametrization, however, makes them data hungry and less scalable on large label sets. All these models, exhibit similar theoretical limitations as the softmax linear unit with weight tying which were described in Sections 2.2. To our knowledge, there is no existing study which has considered the use of such joint inputoutput labels for neural machine translation. Compared to previous joint input-label models our model is more flexible and not restricted to linear mappings, which have limited expressivity,"
W18-6308,D13-1176,0,0.0543068,"near relationships between words similar to distributional methods (Mikolov et al., 2013). The hard equality of parameters imposed by W = E T forces the model to re-use this implicit structure in the output layer and increases the modeling burden of the decoder itself by requiring it to match this structure through ht . Assuming that the latent linear structure which E learns is of the form E ≈ El W where El ∈ IR|V|×k and W ∈ IRk×d and d = dh , then Eq. 4 becomes: where fΘ returns a column vector with an element for each yt . Different models have been proposed to approximate the function fΘ (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Without loss of generality, we focus here on LSTM-based encoder-decoder model with attention Luong et al. (2015). 2.1 Output Layer parametrizations 2.1.1 Softmax Linear Unit The most common output layer (Figure 3a), consists of a linear unit with a weight matrix W ∈ IRdh ×|V |and a bias vector b ∈ IR|V |followed by a softmax activation function, where V is the vocabulary, noted as NMT. For brevity, we focus our analysis specifically on the nominator of the normalized exponential whic"
W18-6308,P17-4012,0,0.131755,"ation, however, is that semantic information of words or sub-word units learned by the input embedding are not considered when learning to predict output words. Hence, they rely on a large amount of examples per class to learn proper word or sub-word unit output classifiers. One way to consider information learned by input embeddings, albeit restrictively, is with weight tying i.e. sharing the parameters of the input embeddings with those of the output classifiers (Press and Wolf, 2017; Inan et al., 2016) which is effective for language modeling and machine translation (Sennrich et al., 2017; Klein et al., 2017). Despite its usefulness, we find that weight tying has three limitations: (a) It biases all the words with similar input embeddings to have a similar chance to be generated, which may not always be the case (see Table 1 for examples). Ideally, it would be better to learn distinct relationships useful for encoding and decoding without forcing any general bias. (b) The relationship between outputs is only implicitly captured by weight tying because there is no parameter sharing across output classifiers. (c) It requires that the size of the translation context vector and the input embeddings ar"
W18-6308,P07-2045,0,0.00985227,"Missing"
W18-6308,Q17-1026,0,0.0689783,"Missing"
W18-6308,D14-1179,0,0.0492074,"Missing"
W18-6308,D15-1166,0,0.0521443,"ling burden of the decoder itself by requiring it to match this structure through ht . Assuming that the latent linear structure which E learns is of the form E ≈ El W where El ∈ IR|V|×k and W ∈ IRk×d and d = dh , then Eq. 4 becomes: where fΘ returns a column vector with an element for each yt . Different models have been proposed to approximate the function fΘ (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Gehring et al., 2017; Vaswani et al., 2017). Without loss of generality, we focus here on LSTM-based encoder-decoder model with attention Luong et al. (2015). 2.1 Output Layer parametrizations 2.1.1 Softmax Linear Unit The most common output layer (Figure 3a), consists of a linear unit with a weight matrix W ∈ IRdh ×|V |and a bias vector b ∈ IR|V |followed by a softmax activation function, where V is the vocabulary, noted as NMT. For brevity, we focus our analysis specifically on the nominator of the normalized exponential which characterizes softmax. Given the decoder’s hidden representation ht with dimension size dh , the output probability distribution at a given time, yt , conditioned on the input sentence X and the previously predicted output"
W18-6308,P16-1160,0,0.0366561,"Missing"
W18-6308,P16-2058,0,0.0409902,"Missing"
W18-6308,W14-3348,0,0.0329916,"(Newstest2016), and the English-German corpus 5.8M for training, 3K for development (Newstest2014), and 3K for testing (Newstest2015). We preprocess the texts using the BPE algorithm (Sennrich et al., 2016) with 32K, 64K and 128K operations. Following the standard evaluation practices in the field (Bojar et al., 2017), the translation quality is measured using BLEU score (Papineni et al., 2002) (multi-blue) on tokenized text and the significance is measured with the paired bootstrap re-sampling method proposed by (Koehn et al., 2007).3 The quality on infrequent words is measured with METEOR (Denkowski and Lavie, 2014) which has originally been proposed to measure performance on function words. 4.3 Translation Performance Table 2 displays the results on four translation sets from English-German and English-Finish language pairs when varying the number of BPE operations. The NMT-tied model outperforms the 2 http://www.statmt.org/wmt17/ multi-bleu.perl and bootstrap-hypothesis-difference-significance.pl scripts. 3 4 Training the models with a full 128K vocabulary without sampling runs out of memory on our machines. 78 NMT-joint Model Layer form NMT W T ht En→ Fi Fi→ En En→ De De→ En BLEU |Θ| 15.85 65.0M NMT-t"
W18-6308,P02-1040,0,0.100371,"Missing"
W18-6308,D15-1027,1,0.406108,"etrizations. and output classifiers, but also shares parameters across output classifiers and translation contexts to better capture the similarity structure of the output space and leverage prior knowledge about this similarity. This flexible sharing allows it to distinguish between features of words which are useful for encoding, generating, or both. Figure 1 shows examples of the proposed model’s input and output representations, compared to those of a softmax linear unit with or without weight tying. This proposal is inspired by joint input-output models for zero-shot text classification (Yazdani and Henderson, 2015; Nam et al., 2016a), but innovates in three important directions, namely in learning complex non-linear relationships, controlling the effective capacity of the output layer and handling structured prediction problems. Our contributions are summarized as follows: • We provide empirical evidence of the superiority of the proposed structure-aware output layer on morphologically simple and complex languages as targets, including under challenging conditions, namely varying vocabulary sizes, architecture depth, and output frequency. The evaluation is performed on 4 translation pairs, namely Engli"
W18-6308,E17-2025,0,0.046519,"o. The idea behind these alternatives is to overcome the vocabulary size issue by modeling the morphology of rare words. One limitation, however, is that semantic information of words or sub-word units learned by the input embedding are not considered when learning to predict output words. Hence, they rely on a large amount of examples per class to learn proper word or sub-word unit output classifiers. One way to consider information learned by input embeddings, albeit restrictively, is with weight tying i.e. sharing the parameters of the input embeddings with those of the output classifiers (Press and Wolf, 2017; Inan et al., 2016) which is effective for language modeling and machine translation (Sennrich et al., 2017; Klein et al., 2017). Despite its usefulness, we find that weight tying has three limitations: (a) It biases all the words with similar input embeddings to have a similar chance to be generated, which may not always be the case (see Table 1 for examples). Ideally, it would be better to learn distinct relationships useful for encoding and decoding without forcing any general bias. (b) The relationship between outputs is only implicitly captured by weight tying because there is no paramet"
W18-6308,E17-3017,0,0.0531533,"Missing"
W18-6308,P16-1162,0,0.336569,") predicts the target sentence one word at a time, and thus models the task as a sequence classification problem where the classes correspond to words. Typically, words are treated as categorical variables which lack description and semantics. This makes training speed and parametrization dependent on the size of the target vocabulary (Mikolov et al., 2013). Previous studies overcome this problem by truncating the vocabulary to limit its size and mapping out-of-vocabulary words to a single “unknown” token. Other approaches attempt to use a limited number of frequent words plus sub-word units (Sennrich et al., 2016), the combination of which can cover the full vocabulary, or to perform 73 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 73–83 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64008 Query visited (Verb past tense) generous (Adjective) friend (Noun) NMT Input Output attacked visiting conquered attended contacted visit occupied visits consulted discovered modest spacious extensive generosity substantial generously ambitious massive sumptuous huge wife friends hu"
