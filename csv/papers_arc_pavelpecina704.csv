2020.acl-main.613,Document Translation vs. Query Translation for Cross-Lingual Information Retrieval in the Medical Domain,2020,-1,-1,2,0,23041,shadi saleh,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present a thorough comparison of two principal approaches to Cross-Lingual Information Retrieval: document translation (DT) and query translation (QT). Our experiments are conducted using the cross-lingual test collection produced within the CLEF eHealth information retrieval tasks in 2013{--}2015 containing English documents and queries in several European languages. We exploit the Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) paradigms and train several domain-specific and task-specific machine translation systems to translate the non-English queries into English (for the QT approach) and the English documents to all the query languages (for the DT approach). The results show that the quality of QT by SMT is sufficient enough to outperform the retrieval results of the DT approach for all the languages. NMT then further boosts translation quality and retrieval quality for both QT and DT for most languages, but still, QT provides generally better retrieval results than DT."
W19-5337,{E}nglish-{C}zech Systems in {WMT}19: Document-Level Transformer,2019,15,1,5,0,227,martin popel,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"We describe our NMT systems submitted to the WMT19 shared task in EnglishâCzech news translation. Our systems are based on the Transformer model implemented in either Tensor2Tensor (T2T) or Marian framework. We aimed at improving the adequacy and coherence of translated documents by enlarging the context of the source and target. Instead of translating each sentence independently, we split the document into possibly overlapping multi-sentence segments. In case of the T2T implementation, this {``}document-level{''}-trained system achieves a +0.6 BLEU improvement (p {\textless} 0.05) relative to the same system applied on isolated sentences. To assess the potential effect document-level models might have on lexical coherence, we performed a semi-automatic analysis, which revealed only a few sentences improved in this aspect. Thus, we cannot draw any conclusions from this week evidence."
W17-4719,Findings of the {WMT} 2017 Biomedical Translation Shared Task,2017,9,7,11,0,4214,antonio yepes,Proceedings of the Second Conference on Machine Translation,0,None
W16-2361,{CUNI} System for {WMT}16 Automatic Post-Editing and Multimodal Translation Tasks,2016,32,8,5,0.952381,13977,jindvrich libovicky,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"Neural sequence to sequence learning recently became a very promising paradigm in machine translation, achieving competitive results with statistical phrase-based systems. In this system description paper, we attempt to utilize several recently published methods used for neural sequential learning in order to build systems for WMT 2016 shared tasks of Automatic Post-Editing and Multimodal Machine Translation."
R15-1040,Feature Extraction for Native Language Identification Using Language Modeling,2015,28,0,3,0,30316,vincent krivz,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"This paper reports on the task of Native Language Identification (NLI). We developed a machine learning system to identify the native language of authors of English texts written by non-native English speakers. Our system is based on the language modeling approach and employs crossentropy scores as features for supervised learning, which leads to a significantly reduced feature space. Our method uses the SVM learner and achieves the accuracy of 82.4 % with only 55 features. We compare our results with the previous similar work by Tetreault et al. (2012) and analyze more details about the use of language modeling for NLI. We experiment with the TOEFL11 corpus (Blanchard et al., 2013) and provide an exact comparison with results achieved in the First Shared Task in NLI (Tetreault et al., 2013)."
W14-3302,Findings of the 2014 Workshop on Statistical Machine Translation,2014,75,148,8,0,292,ondvrej bojar,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the results of the WMT14 shared tasks, which included a standard news translation task, a separate medical translation task, a task for run-time estimation of machine translation quality, and a metrics task. This year, 143 machine translation systems from 23 institutions were submitted to the ten translation directions in the standard translation task. An additional 6 anonymized systems were included, and were then evaluated both automatically and manually. The quality estimation task had four subtasks, with a total of 10 teams, submitting 57 entries"
W14-3326,Machine Translation of Medical Texts in the Khresmoi Project,2014,37,10,5,0,2976,ondvrej duvsek,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper presents the participation of the Charles University team in the WMT 2014 Medical Translation Task. Our systems are developed within the Khresmoi project, a large integrated project aiming to deliver a multi-lingual multi-modal search and access system for biomedical information and documents. Being involved in the organization of the Medical Translation Task, our primary goal is to set up a baseline for both its subtasks (summary translation and query translation) and for all translation directions. Our systems are based on the phrasebased Moses system and standard methods for domain adaptation. The constrained/unconstrained systems differ in the training data only."
W14-3353,Tolerant {BLEU}: a Submission to the {WMT}14 Metrics Task,2014,8,4,2,0.952381,13977,jindvrich libovicky,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes a machine translation metric submitted to the WMT14 Metrics Task. It is a simple modification of the standard BLEU metric using a monolingual alignment of reference and test sentences. The alignment is computed as a minimum weighted maximum bipartite matching of the translated and the reference sentence words with respect to the relative edit distance of the word prefixes and suffixes. The aligned words are included in the n-gram precision computation with a penalty proportional to the matching distance. The proposed tBLEU metric is designed to be more tolerant to errors in inflection, which usually does not effect the understandability of a sentence, and therefore be more suitable for measuring quality of translation into morphologically richer languages."
uresova-etal-2014-multilingual,Multilingual Test Sets for Machine Translation of Search Queries for Cross-Lingual Information Retrieval in the Medical Domain,2014,11,4,3,0,23434,zdevnka urevsova,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents development and test sets for machine translation of search queries in cross-lingual information retrieval in the medical domain. The data consists of the total of 1,508 real user queries in English translated to Czech, German, and French. We describe the translation and review process involving medical professionals and present a baseline experiment where our data sets are used for tuning and evaluation of a machine translation system."
W13-3208,Determining Compositionality of Expresssions Using Various Word Space Models and Methods,2013,16,6,3,0,40884,lubomir krvcmavr,Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,0,None
W13-1006,Determining Compositionality of Word Expressions Using Word Space Models,2013,35,2,3,0,40884,lubomir krvcmavr,Proceedings of the 9th Workshop on Multiword Expressions,0,"This research focuses on determining semantic compositionality of word expressions using word space models (WSMs). We discuss previous works employing WSMs and present differences in the proposed approaches which include types of WSMs, corpora, preprocessing techniques, methods for determining compositionality, and evaluation testbeds. We also present results of our own approach for determining the semantic compositionality based on comparing distributional vectors of expressions and their components. The vectors were obtained by Latent Semantic Analysis (LSA) applied to the ukWaC corpus. Our results outperform those of all the participants in the Distributional Semantics and Compositionality (DISCO) 2011 shared task."
W13-1016,Syntactic Identification of Occurrences of Multiword Expressions in Text using a Lexicon with Dependency Structures,2013,17,9,3,0,17929,eduard bejvcek,Proceedings of the 9th Workshop on Multiword Expressions,0,"We deal with syntactic identification of occurrences of multiword expression (MWE) from an existing dictionary in a text corpus. The MWEs we identify can be of arbitrary length and can be interrupted in the surface sentence. We analyse and compare three approaches based on linguistic analysis at a varying level, ranging from surface word order to deep syntax. The evaluation is conducted using two corpora: the Prague Dependency Treebank and Czech National Corpus. We use the dictionary of multiword expressions SemLex, that was compiled by annotating the Prague Dependency Treebank and includes deep syntactic dependency trees of all MWEs."
P13-2112,Simpler unsupervised {POS} tagging with bilingual projections,2013,10,16,4,0,25446,long duong,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present an unsupervised approach to part-of-speech tagging based on projections of tags in a word-aligned bilingual parallel corpus. In contrast to the existing state-of-the-art approach of Das and Petrov, we have developed a substantially simpler method by automatically identifying xe2x80x9cgoodxe2x80x9d training sentences from the parallel corpus and applying self-training. In experimental results on eight languages, our method achieves state-of-the-art results."
I13-1177,Increasing the Quality and Quantity of Source Language Data for Unsupervised Cross-Lingual {POS} Tagging,2013,11,3,4,0,25446,long duong,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Bilingual corpora offer a promising bridge between resource-rich and resource-poor languages, enabling the development of natural language processing systems for the latter. English is often selected as the resource-rich language, but another choice might give better performance. In this paper, we consider the task of unsupervised cross-lingual POS tagging, and construct a model that predicts the best source language for a given target language. In experiments on 9 languages, this model improves on using a single fixed source language. We then show that further improvements can be made by combining information from multiple source languages."
avramidis-etal-2012-richly,"A Richly Annotated, Multilingual Parallel Corpus for Hybrid Machine Translation",2012,12,3,6,0,5140,eleftherios avramidis,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In recent years, machine translation (MT) research has focused on investigating how hybrid machine translation as well as system combination approaches can be designed so that the resulting hybrid translations show an improvement over the individual ÂcomponentÂ translations. As a first step towards achieving this objective we have developed a parallel corpus with source text and the corresponding translation output from a number of machine translation engines, annotated with metadata information, capturing aspects of the translation process performed by the different MT systems. This corpus aims to serve as a basic resource for further research on whether hybrid machine translation algorithms and system combination techniques can benefit from additional (linguistically motivated, decoding, and runtime) information provided by the different systems involved. In this paper, we describe the annotated corpus we have created. We provide an overview on the component MT systems and the XLIFF-based annotation format we have developed. We also report on first experiments with the ML4HMT corpus data."
shaalan-etal-2012-arabic,{A}rabic Word Generation and Modelling for Spell Checking,2012,16,23,3,0,6182,khaled shaalan,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Arabic is a language known for its rich and complex morphology. Although many research projects have focused on the problem of Arabic morphological analysis using different techniques and approaches, very few have addressed the issue of generation of fully inflected words for the purpose of text authoring. Available open-source spell checking resources for Arabic are too small and inadequate. Ayaspell, for example, the official resource used with OpenOffice applications, contains only 300,000 fully inflected words. We try to bridge this critical gap by creating an adequate, open-source and large-coverage word list for Arabic containing 9,000,000 fully inflected surface words. Furthermore, from a large list of valid forms and invalid forms we create a character-based tri-gram language model to approximate knowledge about permissible character clusters in Arabic, creating a novel method for detecting spelling errors. Testing of this language model gives a precision of 98.2{\%} at a recall of 100{\%}. We take our research a step further by creating a context-independent spelling correction tool using a finite-state automaton that measures the edit distance between input words and candidate corrections, the Noisy Channel Model, and knowledge-based rules. Our system performs significantly better than Hunspell in choosing the best solution, but it is still below the MS Spell Checker."
federmann-etal-2012-ml4hmt,The {ML}4{HMT} Workshop on Optimising the Division of Labour in Hybrid Machine Translation,2012,22,3,6,0.295685,6017,christian federmann,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We describe the ÂShared Task on Applying Machine Learning Techniques to Optimise the Division of Labour in Hybrid Machine TranslationÂ (ML4HMT) which aims to foster research on improved system combination approaches for machine translation (MT). Participants of the challenge are requested to build hybrid translations by combining the output of several MT systems of different types. We first describe the ML4HMT corpus used in the shared task, then explain the XLIFF-based annotation format we have designed for it, and briefly summarize the participating systems. Using both automated metrics scores and extensive manual evaluation, we discuss the individual performance of the various systems. An interesting result from the shared task is the fact that we were able to observe different systems winning according to the automated metrics scores when compared to the results from the manual evaluation. We conclude by summarising the first edition of the challenge and by giving an outlook to future work."
C12-2011,Improved Spelling Error Detection and Correction for {A}rabic,2012,23,18,2,0.833333,24071,mohammed attia,Proceedings of {COLING} 2012: Posters,0,"A spelling error detection and correction application is based on three main components: a dictionary (or reference word list), an error model and a language model. While most of the attention in the literature has been directed to the language model, we show how improvements in any of the three components can lead to significant cumulative improvements in the overall performance of the system. We semi-automatically develop a dictionary of 9.3 million fully inflected Arabic words using a morphological transducer and a large corpus. We improve the error model by analysing error types and creating an edit distance based re-ranker. We also improve the language model by analysing the level of noise in different sources of data and selecting the optimal subset to train the system on. Testing and evaluation experiments show that our system significantly outperforms Microsoft Word 2010, OpenOffice Ayaspell and Google Docs."
C12-1135,Simple and Effective Parameter Tuning for Domain Adaptation of Statistical Machine Translation,2012,25,15,1,1,23042,pavel pecina,Proceedings of {COLING} 2012,0,"Current state-of-the-art Statistical Machine Translation systems are based on log-linear models that combine a set of feature functions to score translation hypotheses during decoding. The models are parametrized by a vector of weights usually optimized on a set of sentences and their reference translations, called development data. In this paper, we explore a (common and industry relevant) scenario where a system trained and tuned on general domain data needs to be adapted to a specific domain for which no or only very limited in-domain bilingual data is available. It turns out that such systems can be adapted successfully by re-tuning model parameters using surprisingly small amounts of parallel in-domain data, by cross-tuning or no tuning at all. We show in detail how and why this is effective, compare the approaches and effort involved. We also study the effect of system hyperparameters (such as maximum phrase length and development data size) and their optimal values in this scenario."
2012.eamt-1.10,Efficiency-based evaluation of aligners for industrial applications,2012,-1,-1,3,0.353814,9426,antonio toral,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
2012.eamt-1.38,Domain Adaptation of Statistical Machine Translation using Web-Crawled Resources: A Case Study,2012,44,17,1,1,23042,pavel pecina,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"We tackle the problem of domain adaptation of Statistical Machine Translation by exploiting domain-specific data acquired by domain-focused web-crawling. We design and evaluate a procedure for automatic acquisition of monolingual and parallel data and their exploitation for training, tuning, and testing in a phrase-based Statistical Machine Translation system. We present a strategy for using such resources depending on their availability and quantity supported by results of a large-scale evaluation on the domains of Natural Environment and Labour Legislation and two language pairs: Englishxe2x80x90French, English-Greek. The average observed increase of BLEU is substantial at 49.5% relative."
W11-4417,An Open-Source Finite State Morphological Transducer for {M}odern {S}tandard {A}rabic,2011,14,23,2,0.833333,24071,mohammed attia,Proceedings of the 9th International Workshop on Finite State Methods and Natural Language Processing,0,"We develop an open-source large-scale finitestate morphological processing toolkit (AraComLex) for Modern Standard Arabic (MSA) distributed under the GPLv3 license. The morphological transducer is based on a lexical database specifically constructed for this purpose. In contrast to previous resources, the database is tuned to MSA, eliminating lexical entries no longer attested in contemporary use. The database is built using a corpus of 1,089,111,204 words, a pre-annotation tool, machine learning techniques, and knowledge-based pattern matching to automatically acquire lexical knowledge. Our morphological transducer is evaluated and compared to LDC's SAMA (Standard Arabic Morphological Analyser)."
J11-3010,Book Reviews: Syntax-Based Collocation Extraction by Violeta Seretan,2011,-1,-1,1,1,23042,pavel pecina,Computational Linguistics,0,None
2011.eamt-1.11,Towards a User-Friendly Webservice Architecture for Statistical Machine Translation in the {PANACEA} project,2011,-1,-1,2,0.394737,9426,antonio toral,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,None
2011.eamt-1.40,Towards Using Web-Crawled Data for Domain Adaptation in Statistical Machine Translation,2011,22,25,1,1,23042,pavel pecina,Proceedings of the 15th Annual conference of the European Association for Machine Translation,0,"This paper reports on the ongoing work focused on domain adaptation of statistical machine translation using domain-specific data obtained by domain-focused web crawling. We present a strategy for crawling monolingual and parallel data and their exploitation for testing, language modelling, and system tuning in a phrase-based machine translation framework. The proposed approach is evaluated on the domains of Natural Environment and Labour Legislation and two language pairs: Englishxe2x80x90French and Englishxe2x80x90Greek."
W10-3704,Automatic Extraction of {A}rabic Multiword Expressions,2010,17,50,4,0.833333,24071,mohammed attia,Proceedings of the 2010 Workshop on Multiword Expressions: from Theory to Applications,0,"In this paper we investigate the automatic acquisition of Arabic Multiword Expressions (MWE). We propose three complementary approaches to extract MWEs from available data resources. The rst approach relies on the correspondence asymmetries between Arabic Wikipedia titles and titles in 21 different languages. The second approach collects English MWEs from Princeton WordNet 3.0, translates the collection into Arabic using Google Translate, and utilizes different search engines to validate the output. The third uses lexical association measures to extract MWEs from a large unannotated corpus. We experimentally explore the feasibility of each approach and measure the quality and coverage of the output against gold standards."
W10-3707,Handling Named Entities and Compound Verbs in Phrase-Based Statistical Machine Translation,2010,25,23,3,0,13776,santanu pal,Proceedings of the 2010 Workshop on Multiword Expressions: from Theory to Applications,0,"Data preprocessing plays a crucial role in phrase-based statistical machine translation (PB-SMT). In this paper, we show how single-tokenization of two types of multi-word expressions (MWE), namely named entities (NE) and compound verbs, as well as their prior alignment can boost the performance of PB-SMT. Single-tokenization of compound verbs and named entities (NE) provides significant gains over the baseline PB-SMT system. Automatic alignment of NEs substantially improves the overall MT performance, and thereby the word alignment quality indirectly. For establishing NE alignments, we transliterate source NEs into the target language and then compare them with the target NEs. Target language NEs are first converted into a canonical form before the comparison takes place. Our best system achieves statistically significant improvements (4.59 BLEU points absolute, 52.5% relative improvement) on an Englishxe2x80x94Bangla translation task."
W10-1720,{MATREX}: The {DCU} {MT} System for {WMT} 2010,2010,29,40,7,0,38356,sergio penkale,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the DCU machine translation system in the evaluation campaign of the Joint Fifth Workshop on Statistical Machine Translation and Metrics in ACL-2010. We describe the modular design of our multi-engine machine translation (MT) system with particular focus on the components used in this participation. We participated in the English--Spanish and English--Czech translation tasks, in which we employed our multi-engine architecture to translate. We also participated in the system combination task which was carried out by the MBR decoder and confusion network decoder."
W10-1742,An Augmented Three-Pass System Combination Framework: {DCU} Combination System for {WMT} 2010,2010,15,5,2,0,21659,jinhua du,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the augmented three-pass system combination framework of the Dublin City University (DCU) MT group for the WMT 2010 system combination task. The basic three-pass framework includes building individual confusion networks (CNs), a super network, and a modified Minimum Bayes-risk (mConMBR) decoder. The augmented parts for WMT2010 tasks include 1) a rescoring component which is used to re-rank the N-best lists generated from the individual CNs and the super network, 2) a new hypothesis alignment metric -- TERp -- that is used to carry out English-targeted hypothesis alignment, and 3) more different backbone-based CNs which are employed to increase the diversity of the mConMBR decoding phase. We took part in the combination tasks of English-to-Czech and French-to-English. Experimental results show that our proposed combination framework achieved 2.17 absolute points (13.36 relative points) and 1.52 absolute points (5.37 relative points) in terms of BLEU score on English-to-Czech and French-to-English tasks respectively than the best single system. We also achieved better performance on human evaluation."
strakova-pecina-2010-czech,{C}zech Information Retrieval with Syntax-based Language Models,2010,9,2,2,0,229,jana strakova,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In recent years, considerable attention has been dedicated to language modeling methods in information retrieval. Although these approaches generally allow exploitation of any type of language model, most of the published experiments were conducted with a classical n-gram model, usually limited only to unigrams. A few works exploiting syntax in information retrieval can be cited in this context, but significant contribution of syntax based language modeling for information retrieval is yet to be proved. In this paper, we propose, implement, and evaluate an enrichment of language model employing syntactic dependency information acquired automatically from both documents and queries. Our experiments are conducted on Czech which is a morphologically rich language and has a considerably free word order, therefore a syntactic language model is expected to contribute positively to the unigram and bigram language model based on surface word order. By testing our model on the Czech test collection from Cross Language Evaluation Forum 2007 Ad-Hoc track, we show positive contribution of using dependency syntax in this context."
spoustova-etal-2010-building,Building a Web Corpus of {C}zech,2010,8,8,3,1,46309,drahomira spoustova,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Large corpora are essential to modern methods of computational linguistics and natural language processing. In this paper, we describe an ongoing project whose aim is to build a largest corpus of Czech texts. We are building the corpus from Czech Internet web pages, using (and, if needed, developing) advanced downloading, cleaning and automatic linguistic processing tools. Our concern is to keep the whole process language independent and thus applicable also for building web corpora of other languages. In the paper, we briefly describe the crawling, cleaning, and part-of-speech tagging procedures. Using a prototype corpus, we provide a comparison with a current corpora (in particular, SYN2005, part of the Czech National Corpora). We analyse part-of-speech tag distribution, OOV word ratio, average sentence length and Spearman rank correlation coefficient of the distance of ranks of 500 most frequent words. Our results show that our prototype corpus is now quite homogenous. The challenging task is to find a way to decrease the homogeneity of the text while keeping the high quality of the data."
W09-0403,A Simple Automatic {MT} Evaluation Metric,2009,6,5,3,0,40500,petr homola,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,This paper describes a simple evaluation metric for MT which attempts to overcome the well-known deficits of the standard BLEU metric from a slightly different angle. It employes Levenshtein's edit distance for establishing alignment between the MT output and the reference translation in order to reflect the morphological properties of highly inflected languages. It also incorporates a very simple measure expressing the differences in the word order. The paper also includes evaluation on the data from the previous SMT workshop for several language pairs.
spoustova-etal-2008-validating,Validating the Quality of Full Morphological Annotation,2008,4,1,2,1,46309,drahomira spoustova,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,In our paper we present a methodology used for low-cost validation of quality of Part-of-Speech annotation of the Prague Dependency Treebank based on multiple re-annotation of data samples carefully selected with the help of several different Part-of-Speech taggers.
P06-2084,Combining Association Measures for Collocation Extraction,2006,16,80,1,1,23042,pavel pecina,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We introduce the possibility of combining lexical association measures and present empirical results of several methods employed in automatic collocation extraction. First, we present a comprehensive summary overview of association measures and their performance on manually annotated data evaluated by precision-recall graphs and mean average precision. Second, we describe several classification methods for combining association measures, followed by their evaluation and comparison with individual measures. Finally, we propose a feature selection algorithm significantly reducing the number of combined measures with only a small performance degradation."
P06-1119,Leveraging Reusability: Cost-Effective Lexical Acquisition for Large-Scale Ontology Translation,2006,13,5,5,0,49992,craig murray,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies."
cinkova-etal-2006-semi,Semi-automatic Building of {S}wedish Collocation Lexicon,2006,10,2,2,0,16866,silvie cinkova,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This work focuses on semi-automatic extraction of verb-noun collocations from a corpus, performed to provide lexical evidence for the manual lexicographical processing of Support Verb Constructions (SVCs) in the Swedish-Czech Combinatorial Valency Lexicon of Predicate Nouns. Efficiency of pure manual extractionprocedure is significantly improved by utilization of automatic statistical methods based lexical association measures."
2006.eamt-1.18,Leveraging Recurrent Phrase Structure in Large-scale Ontology Translation,2006,-1,-1,5,0,49992,craig murray,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
P05-2003,An Extensive Empirical Study of Collocation Extraction Methods,2005,17,73,1,1,23042,pavel pecina,Proceedings of the {ACL} Student Research Workshop,0,This paper presents a status quo of an ongoing research study of collocations -- an essential linguistic phenomenon having a wide spectrum of applications in the field of natural language processing. The core of the work is an empirical evaluation of a comprehensive list of automatic collocation extraction methods using precision-recall measures and a proposal of a new approach integrating multiple basic methods and statistical classification. We demonstrate that combining multiple independent techniques leads to a significant performance improvement in comparison with individual basic methods.
