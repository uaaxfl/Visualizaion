2020.latechclfl-1.16,Sonnet Combinatorics with {O}u{P}o{C}o,2020,-1,-1,1,1,18533,thierry poibeau,"Proceedings of the The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In this paper, we describe OuPoCo, a system producing new sonnets by recombining verses from existing sonnets, following an idea that Queneau described in his book {``}Cent Mille Milliards de po{\`e}mes, Gallimard{''}, 1961. We propose to demonstrate different outputs of our implementation (a Web site, a Twitter bot and a specifically developed device, called {`}La Bo{\^\i}te {\`a} po{\'e}sie{'}) based on a corpus of 19th century French poetry. Our goal is to make people interested in poetry again, by giving access to automatically produced sonnets through original and entertaining channels and devices."
2020.cl-4.5,Multi-{S}im{L}ex: A Large-Scale Evaluation of Multilingual and Crosslingual Lexical Semantic Similarity,2020,153,1,10,0,4035,ivan vulic,Computational Linguistics,0,"We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering data sets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language data set is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 crosslingual semantic similarity data sets. Because of its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and crosslingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and crosslingual representation models, including static and contextualized word embeddings (such as fastText, monolingual and multilingual BERT, XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised crosslingual word embeddings. We also present a step-by-step data set creation protocol for creating consistent, Multi-Simlex{--}style resources for additional languages. We make these contributions{---}the public release of Multi-SimLex data sets, their creation protocol, strong baseline results, and in-depth analyses which can be helpful in guiding future developments in multilingual lexical semantics and representation learning{---}available via a Web site that will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages."
J19-3005,Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing,2019,51,5,6,0.31746,1279,edoardo ponti,Computational Linguistics,0,"Linguistic typology aims to capture structural and semantic variation across the world{'}s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge."
W18-6015,The First {K}omi-{Z}yrian {U}niversal {D}ependencies Treebanks,2018,0,0,4,0,110,niko partanen,Proceedings of the Second Workshop on Universal Dependencies ({UDW} 2018),0,"Two Komi-Zyrian treebanks were included in the Universal Dependencies 2.2 release. This article contextualizes the treebanks, discusses the process through which they were created, and outlines the future plans and timeline for the next improvements. Special attention is paid to the possibilities of using UD in the documentation and description of endangered languages."
W18-0201,Dependency Parsing of Code-Switching Data with Cross-Lingual Feature Representations,2018,0,2,4,0,110,niko partanen,Proceedings of the Fourth International Workshop on Computational Linguistics of Uralic Languages,0,"This paper describes the test of a dependency parsing method which is based on bidirectional LSTM feature representations and multilingual word embedding, and evaluates the results on mono-and multilingual data. The results are similar in all cases, with a slightly better results achieved using multilingual data. The languages under investigation are Komi-Zyrian and Russian. Examination of the results by relation type shows that some language specific constructions are correctly recognized even when they appear in naturally occurring code-switching data. Tiivistelma Tutkimus arvioi dependenssianalyysin menetelmaa, joka perustuu kaksisuun-taiseen LSTM-piirrerepresentaatioon ja monikieliseen 'word embedding'-malliin, seka arvioi tuloksia yksi-ja monikielisissa aineistoissa. Tulokset ovat samanta-paisia, mutta hieman korkeampia moni-kuin yksikielisissa aineistoissa. Tutkitut kielet ovat komisyrjaani ja venaja. Tulosten yksityiskohtaisempi analyysi riippu-vuuksien mukaan osoittaa, etta tietyt kielikohtaiset suhteet on tunnistettu oikein jopa niiden esiintyessa luonnollisissa koodinvaihtoa sisaltavissa lauseissa. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:"
L18-1352,Multilingual Dependency Parsing for Low-Resource Languages: Case Studies on North Saami and {K}omi-{Z}yrian,2018,0,0,3,1,27837,kyungtae lim,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
K18-2014,{SE}x {B}i{ST}: A Multi-Source Trainable Parser with Deep Contextualized Lexical Representations,2018,0,4,4,1,27837,kyungtae lim,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We describe the SEx BiST parser (Semantically EXtended Bi-LSTM parser) developed at Lattice for the CoNLL 2018 Shared Task (Multilingual Parsing from Raw Text to Universal Dependencies). The main characteristic of our work is the encoding of three different modes of contextual information for parsing: (i) Treebank feature representations, (ii) Multilingual word representations, (iii) ELMo representations obtained via unsupervised learning from external resources. Our parser performed well in the official end-to-end evaluation (73.02 LAS {--} 4th/26 teams, and 78.72 UAS {--} 2nd/26); remarkably, we achieved the best UAS scores on all the English corpora by applying the three suggested feature representations. Finally, we were also ranked 1st at the optional event extraction task, part of the 2018 Extrinsic Parser Evaluation campaign."
W17-6524,{UDL}ex: Towards Cross-language Subcategorization Lexicons,2017,27,0,3,0,927,giulia rambelli,Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017),0,None
W17-2204,Enjambment Detection in a Large Diachronic Corpus of {S}panish Sonnets,2017,-1,-1,3,1,31983,pablo ruiz,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"Enjambment takes place when a syntactic unit is broken up across two lines of poetry, giving rise to different stylistic effects. In Spanish literary studies, there are unclear points about the types of stylistic effects that can arise, and under which linguistic conditions. To systematically gather evidence about this, we developed a system to automatically identify enjambment (and its type) in Spanish. For evaluation, we manually annotated a reference corpus covering different periods. As a scholarly corpus to apply the tool, from public HTML sources we created a diachronic corpus covering four centuries of sonnets (3750 poems), and we analyzed the occurrence of enjambment across stanzaic boundaries in different periods. Besides, we found examples that highlight limitations in current definitions of enjambment."
W17-0605,Preliminary Experiments concerning Verbal Predicative Structure Extraction from a Large {F}innish Corpus,2017,0,0,2,0,32130,guersande chaminade,Proceedings of the Third Workshop on Computational Linguistics for Uralic Languages,0,None
K17-3006,A System for Multilingual Dependency Parsing based on Bidirectional {LSTM} Feature Representations,2017,8,3,2,1,27837,kyungtae lim,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"In this paper, we present our multilingual dependency parser developed for the CoNLL 2017 UD Shared Task dealing with {``}Multilingual Parsing from Raw Text to Universal Dependencies{''}. Our parser extends the monolingual BIST-parser as a multi-source multilingual trainable parser. Thanks to multilingual word embeddings and one hot encodings for languages, our system can use both monolingual and multi-source training. We trained 69 monolingual language models and 13 multilingual models for the shared task. Our multilingual approach making use of different resources yield better results than the monolingual approach for 11 languages. Our system ranked 5 th and achieved 70.93 overall LAS score over the 81 test corpora (macro-averaged LAS F1 score)."
L16-1300,More than Word Cooccurrence: Exploring Support and Opposition in International Climate Negotiations with Semantic Parsing,2016,16,1,3,1,31983,pablo ruiz,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Text analysis methods widely used in digital humanities often involve word co-occurrence, e.g. concept co-occurrence networks. These methods provide a useful corpus overview, but cannot determine the predicates that relate co-occurring concepts. Our goal was identifying propositions expressing the points supported or opposed by participants in international climate negotiations. Word co-occurrence methods were not sufficient, and an analysis based on open relation extraction had limited coverage for nominal predicates. We present a pipeline which identifies the points that different actors support and oppose, via a domain model with support/opposition predicates, and analysis rules that exploit the output of semantic role labelling, syntactic dependencies and anaphora resolution. Entity linking and keyphrase extraction are also performed on the propositions related to each actor. A user interface allows examining the main concepts in points supported or opposed by each participant, which participants agree or disagree with each other, and about which issues. The system is an example of tools that digital humanities scholars are asking for, to render rich textual information (beyond word co-occurrence) more amenable to quantitative treatment. An evaluation of the tool was satisfactory."
C16-2062,Exploring a Continuous and Flexible Representation of the Lexicon,2016,6,0,2,0,35682,pierre marchal,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,We aim at showing that lexical descriptions based on multifactorial and continuous models can be used by linguists and lexicographers (and not only by machines) so long as they are provided with a way to efficiently navigate data collections. We propose to demonstrate such a system.
C16-1155,The Role of Intrinsic Motivation in Artificial Language Emergence: a Case Study on Colour,2016,18,0,2,1,35766,miquel cornudella,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Human languages have multiple strategies that allow us to discriminate objects in a vast variety of contexts. Colours have been extensively studied from this point of view. In particular, previous research in artificial language evolution has shown how artificial languages may emerge based on specific strategies to distinguish colours. Still, it has not been shown how several strategies of diverse complexity can be autonomously managed by artificial agents . We propose an intrinsic motivation system that allows agents in a population to create a shared artificial language and progressively increase its expressive power. Our results show that with such a system agents successfully regulate their language development, which indicates a relation between population size and consistency in the emergent communicative systems."
W15-2407,Language Emergence in a Population of Artificial Agents Equipped with the Autotelic Principle,2015,26,0,2,1,35766,miquel cornudella,Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning,0,"Experiments on the emergence of a shared language in a population of agents usually rely on the control of the complexity by the experimenter. In this article we show how agents provided with the autotelic principle, a system by which agents can regulate their own development, progressively develop an emerging language evolving from one word to multi-word utterances, increasing its discriminative power."
S15-2060,{EL}92: Entity Linking Combining Open Source Annotators via Weighted Voting,2015,16,3,2,1,31983,pablo ruiz,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"Our participation at SemEval's Multilingual All-Words Sense Disambiguation and Entity Linking task is described. An English entity linking (EL) system is presented, which combines the annotations of four public open source EL services. The annotations are combined through a weighted voting scheme inspired on the ROVER method, which had not been previously tested on EL outputs. Results on the task's EL items were competitive."
S15-1025,Combining Open Source Annotators for Entity Linking through Weighted Voting,2015,16,3,2,1,31983,pablo ruiz,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Our participation at SemEval's Multilingual All-Words Sense Disambiguation and Entity Linking task is described. An English entity linking (EL) system is presented, which combines the annotations of four public open source EL services. The annotations are combined through a weighted voting scheme inspired on the ROVER method, which had not been previously tested on EL outputs. Results on the task's EL items were competitive."
N15-3010,{ELCO}3: Entity Linking with Corpus Coherence Combining Open Source Annotators,2015,11,0,2,1,31983,pablo ruiz,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"Entity Linking (EL) systems' performance is uneven across corpora or depending on entity types. To help overcome this issue, we propose an EL workflow that combines the outputs of several open source EL systems, and selects annotations via weighted voting. The results are displayed on a UI that allows the users to navigate the corpus and to evaluate annotation quality based on several metrics."
W14-4604,Processing Mutations in {B}reton with Finite-State Transducers,2014,4,0,1,1,18533,thierry poibeau,Proceedings of the First Celtic Language Technology Workshop,0,"One characteristic feature of Celtic languages is mutation, i.e. the fact that the initial consonant of words may change according to the context. We provide a quick description of this linguistic phenomenon for Breton along with a formalization using finite state transducers. This approach allows an exact and compact description of mutations. The result can be used in various contexts, especially for spell checking and language teaching."
W14-0610,Social and Semantic Diversity: Socio-semantic Representation of a Scientific Corpus,2014,20,2,1,1,18533,thierry poibeau,"Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We propose a new method to extract key- words from texts and categorize these keywords according to their informational value, derived from the analysis of the ar- gumentative goal of the sentences they ap- pear in. The method is applied to the ACL Anthology corpus, containing papers on the computational linguistic domain pub- lished between 1980 and 2008. We show that our approach allows to highlight inter- esting facts concerning the evolution of the topics and methods used in computational linguistics."
omodei-etal-2014-reconstructing,Reconstructing the Semantic Landscape of Natural Language Processing,2014,0,0,3,0,38828,elisa omodei,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper investigates the evolution of the computational linguistics domain through a quantitative analysis of the ACL Anthology (containing around 12,000 papers published between 1985 and 2008). Our approach combines complex system methods with natural language processing techniques. We reconstruct the socio-semantic landscape of the domain by inferring a co-authorship and a semantic network from the analysis of the corpus. First, keywords are extracted using a hybrid approach mixing linguistic patterns with statistical information. Then, the semantic network is built using a co-occurrence analysis of these keywords within the corpus. Combining temporal and network analysis techniques, we are able to examine the main evolutions of the field and the more active subfields over time. Lastly we propose a model to explore the mutual influence of the social and the semantic network over time, leading to a socio-semantic co-evolutionary system."
F14-2032,Argumentative analysis of the {ACL} {A}nthology (Analyse argumentative du corpus de l{'}{ACL} ({ACL} {A}nthology)) [in {F}rench],2014,-1,-1,4,0,38828,elisa omodei,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
N13-1134,A Tensor-based Factorization Model of Semantic Compositionality,2013,31,32,2,0.538591,5597,tim cruys,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper, we present a novel method for the computation of compositionality within a distributional framework. The key idea is that compositionality is modeled as a multi-way interaction between latent factors, which are automatically constructed from corpus data. We use our method to model the composition of subject verb object triples. The method consists of two steps. First, we compute a latent factor model for nouns from standard co-occurrence data. Next, the latent factors are used to induce a latent model of three-way subject verb object interactions. Our model has been evaluated on a similarity task for transitive phrases, in which it exceeds the state of the art."
landragin-etal-2012-analec,{ANALEC}: a New Tool for the Dynamic Annotation of Textual Data,2012,5,10,2,0,16608,frederic landragin,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We introduce ANALEC, a tool which aim is to bring together corpus annotation, visualization and query management. Our main idea is to provide a unified and dynamic way of annotating textual data. ANALEC allows researchers to dynamically build their own annotation scheme and use the possibilities of scheme revision, data querying and graphical visualization during the annotation process. Each query result can be visualized using a graphical representation that puts forward a set of annotations that can be directly corrected or completed. Text annotation is then considered as a cyclic process. We show that statistics like frequencies and correlations make it possible to verify annotated data on the fly during the annotation. In this paper we introduce the annotation functionalities of ANALEC, some of the annotated data visualization functionalities, and three statistical modules: frequency, correlation and geometrical representations. Some examples dealing with reference and coreference annotation illustrate the main contributions of ANALEC."
C12-1165,Multi-way Tensor Factorization for Unsupervised Lexical Acquisition,2012,44,15,3,0.664334,5597,tim cruys,Proceedings of {COLING} 2012,0,"This paper introduces a novel method for joint unsupervised aquisition of verb subcategorization frame (SCF) and selectional preference (SP) information. Treating SCF and SP induction as a multi-way co-occurrence problem, we use multi-way tensor factorization to cluster frequent verbs from a large corpus according to their syntactic and semantic behaviour. The method extends previous tensor factorization approaches by predicting whether a syntactic argument is likely to occur with a verb lemma (SCF) as well as which lexical items are likely to occur in the argument slot (SP), and integrates a variety of lexical and syntactic features, including co-occurrence information on grammatical relations not explicitly represented in the SCFs. The SCF lexicon that emerges from the clusters achieves an F-score of 68.7 against a gold standard, while the SP model achieves an accuracy of 77.8 in a novel evaluation that considers all of a verbxe2x80x99s arguments simultaneously."
R11-1038,A New Scheme for Annotating Semantic Relations between Named Entities in Corpora,2011,19,0,2,0,44517,mani ezzat,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"Although several studies have developed models and type hierarchies for named entity annotation, no such resource is available for semantic relation annotation, despite its utility for various applications (e.g. question answering, information extraction). In this paper, we show that there are two issues in semantic relation description, one concerning knowledge engineering (what to annotate?) and the other concerning language engineering (how to deal with modality and modifiers?). We propose a new annotation scheme, making it possible to have both a precise and tractable annotation. A practical experiment shows that annotators using our scheme were able to quickly annotate a large number of sentences with very high inter-annotator agreement."
D11-1025,A Weakly-supervised Approach to Argumentative Zoning of Scientific Documents,2011,45,35,3,0.454545,4740,yufan guo,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"Argumentative Zoning (AZ) -- analysis of the argumentative structure of a scientific paper -- has proved useful for a number of information access tasks. Current approaches to AZ rely on supervised machine learning (ML). Requiring large amounts of annotated data, these approaches are expensive to develop and port to different domains and tasks. A potential solution to this problem is to use weakly-supervised ML instead. We investigate the performance of four weakly-supervised classifiers on scientific abstract data annotated for multiple AZ classes. Our best classifier based on the combination of active learning and self-training outperforms our best supervised classifier, yielding a high accuracy of 81% when using just 10% of the labeled data. This result suggests that weakly-supervised learning could be employed to improve the practical applicability and portability of AZ across different information access tasks."
D11-1094,Latent Vector Weighting for Word Meaning in Context,2011,35,42,2,0.664334,5597,tim cruys,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a novel method for the computation of word meaning in context. We make use of a factorization model in which words, together with their window-based context words and their dependency relations, are linked to latent dimensions. The factorization model allows us to determine which dimensions are important for a particular context, and adapt the dependency-based feature vector of the word accordingly. The evaluation on a lexical substitution task -- carried out for both English and French -- indicates that our approach is able to reach better results than state-of-the-art methods in lexical substitution, while at the same time providing more accurate meaning representations."
C10-1119,Investigating the cross-linguistic potential of {V}erb{N}et-style classification,2010,33,21,2,0,21249,lin sun,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Verb classes which integrate a wide range of linguistic properties (Levin, 1993) have proved useful for natural language processing (NLP) applications. However, the real-world use of these classes has been limited because for most languages, no resources similar to VerbNet (Kipper-Schuler, 2005) are available. We apply a verb clustering approach developed for English to French - a language for which no such experiment has been conducted yet. Our investigation shows that not only the general methodology but also the best performing features are transferable between the languages, making it possible to learn useful VerbNet style classes for French automatically without language-specific tuning."
R09-1009,Integrating Document Structure into a Multi-Document Summarizer,2009,9,1,2,1,18576,aurelien bossard,Proceedings of the International Conference {RANLP}-2009,0,"In this paper, we present a novel approach for automatic summarization. CBSEAS, the system implementing this approach, integrates a method to detect redundancy at its very core, in order to produce more expressive summaries than previous approaches. The evaluation of our system during TAC 2008 xe2x80x94the Text Analysis Conferencexe2x80x94 revealed that, even if our system performed well on blogs, it had some failings on news stories. A post-mortem analysis of the weaknesses of our original system showed the importance of text structure for automatic summarization, even in the case of short texts like news stories. We describe some ongoing work dealing with these issues and show that first experiments provide a significant improvement of the results."
E09-2002,"{CBSEAS}, a Summarization System {--} Integration of Opinion Mining Techniques to Summarize Blogs",2009,12,8,3,1,18576,aurelien bossard,Proceedings of the Demonstrations Session at {EACL} 2009,0,"In this paper, we present a novel approach for automatic summarization. Our system, called CBSEAS, integrates a new method to detect redundancy at its very core, and produce more expressive summaries than previous approaches. Moreover, we show that our system is versatile enough to integrate opinion mining techniques, so that it is capable of producing opinion oriented summaries. The very competitive results obtained during the last Text Evaluation Conference (TAC 2008) show that our approach is efficient."
2009.jeptalnrecital-long.5,Annotation fonctionnelle de corpus arbor{\\'e}s avec des Champs Al{\\'e}atoires Conditionnels,2009,-1,-1,6,0,19792,erwan moreau,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"L{'}objectif de cet article est d{'}{\'e}valuer dans quelle mesure les {``}fonctions syntaxiques{''} qui figurent dans une partie du corpus arbor{\'e} de Paris 7 sont apprenables {\`a} partir d{'}exemples. La technique d{'}apprentissage automatique employ{\'e}e pour cela fait appel aux {``}Champs Al{\'e}atoires Conditionnels{''} (Conditional Random Fields ou CRF), dans une variante adapt{\'e}e {\`a} l{'}annotation d{'}arbres. Les exp{\'e}riences men{\'e}es sont d{\'e}crites en d{\'e}tail et analys{\'e}es. Moyennant un bon param{\'e}trage, elles atteignent une F1-mesure de plus de 80{\%}."
poibeau-messiant-2008-still,Do we Still Need Gold Standards for Evaluation?,2008,17,14,1,1,18533,thierry poibeau,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The availability of a huge mass of textual data in electronic format has increased the need for fast and accurate techniques for textual data processing. Machine learning and statistical approaches have been increasingly used in NLP since a decade, mainly because they are quick, versatile and efficient. However, despite this evolution of the field, evaluation still rely (most of the time) on a comparison between the output of a probabilistic or statistical system on the one hand, and a non-statistic, most of the time hand-crafted, gold standard on the other hand. In this paper, we take the example of the acquisition of subcategorization frames from corpora as a practical example. Our study is motivated by the fact that, even if a gold standard is an invaluable resource for evaluation, a gold standard is always partial and does not really show how accurate and useful results are."
messiant-etal-2008-lexschem,{L}ex{S}chem: a Large Subcategorization Lexicon for {F}rench Verbs,2008,18,32,2,0,46540,cedric messiant,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents LexSchem - the first large, fully automatically acquired subcategorization lexicon for French verbs. The lexicon includes subcategorization frame and frequency information for 3297 French verbs. When evaluated on a set of 20 test verbs against a gold standard dictionary, it shows 0.79 precision, 0.55 recall and 0.65 F-measure. We have made this resource freely available to the research community on the web."
2008.jeptalnrecital-court.13,Regroupement automatique de documents en classes {\\'e}v{\\'e}nementielles,2008,-1,-1,2,1,18576,aurelien bossard,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article porte sur le regroupement automatique de documents sur une base {\'e}v{\'e}nementielle. Apr{\`e}s avoir pr{\'e}cis{\'e} la notion d{'}{\'e}v{\'e}nement, nous nous int{\'e}ressons {\`a} la repr{\'e}sentation des documents d{'}un corpus de d{\'e}p{\^e}ches, puis {\`a} une approche d{'}apprentissage pour r{\'e}aliser les regroupements de mani{\`e}re non supervis{\'e}e fond{\'e}e sur k-means. Enfin, nous {\'e}valuons le syst{\`e}me de regroupement de documents sur un corpus de taille r{\'e}duite et nous discutons de l{'}{\'e}valuation quantitative de ce type de t{\^a}che."
W07-1015,Automatically Restructuring Practice Guidelines using the {GEM} {DTD},2007,12,2,2,0,49021,amanda bouffier,"Biological, translational, and clinical language processing",0,"This paper describes a system capable of semi-automatically filling an XML template from free texts in the clinical domain (practice guidelines). The XML template includes semantic information not explicitly encoded in the text (pairs of conditions and actions/recommendations). Therefore, there is a need to compute the exact scope of conditions over text sequences expressing the required actions. We present a system developed for this task. We show that it yields good performance when applied to the analysis of French practice guidelines."
S07-1093,{UP}13: Knowledge-poor Methods (Sometimes) Perform Poorly,2007,6,6,1,1,18533,thierry poibeau,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This short paper presents a system developed at the Universite Paris 13 for the Semeval 2007 Metonymy Resolution Task (task #08, location name track; see Markert and Nissim, 2007). The system makes use of plain word forms only. In this paper, we evaluate the accuracy of this minimalist approach, compare it to a more complex one which uses both syntactic and semantic features, and discuss its usefulness for metonymy resolution in general."
2005.jeptalnrecital-long.18,Sur le statut r{\\'e}f{\\'e}rentiel des entit{\\'e}s nomm{\\'e}es,2005,-1,-1,1,1,18533,thierry poibeau,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous montrons dans cet article qu{'}une m{\^e}me entit{\'e} peut {\^e}tre d{\'e}sign{\'e}e de multiples fa{\c{c}}ons et que les noms d{\'e}signant ces entit{\'e}s sont par nature polys{\'e}miques. L{'}analyse ne peut donc se limiter {\`a} une tentative de r{\'e}solution de la r{\'e}f{\'e}rence mais doit mettre en {\'e}vidence les possibilit{\'e}s de nommage s{'}appuyant essentiellement sur deux op{\'e}rations de nature linguistique : la synecdoque et la m{\'e}tonymie. Nous pr{\'e}sentons enfin une mod{\'e}lisation permettant de rendre explicite les diff{\'e}rentes d{\'e}signations en discours, en unifiant le mode de repr{\'e}sentation des connaissances linguistiques et des connaissances sur le monde."
W04-1207,Event-Based Information Extraction for the Biomedical Domain: the Caderige Project,2004,19,26,11,0,44285,erick alphonse,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"This paper gives an overview of the Caderige project. This project involves teams from different areas (biology, machine learning, natural language processing) in order to develop highlevel analysis tools for extracting structured information from biological bibliographical databases, especially Medline. The paper gives an overview of the approach and compares it to the state of the art."
poibeau-goujon-2004-semi,Semi-automatic Acquisition of Command Grammar,2004,0,0,1,1,18533,thierry poibeau,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
C04-1092,Automatic extraction of paraphrastic phrases from medium-size corpora,2004,21,4,1,1,18533,thierry poibeau,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents a versatile system intended to acquire paraphrastic phrases from a representative corpus. In order to decrease the time spent on the elaboration of resources for NLP system (for example Information Extraction, IE hereafter), we suggest to use a knowledge acquisition module that helps extracting new information despite linguistic variation (textual entailment). This knowledge is automatically derived from the text collection, in interaction with a large semantic network."
E03-1015,The Multilingual Named Entity Recognition Framework,2003,11,24,1,1,18533,thierry poibeau,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper presents a multilingual system designed to recognize named entities in a wide variety of languages (currently more than 12 languages are concerned). The system includes original strategies to deal with a wide variety of encoding character sets, analysis strategies and algorithms to process these languages."
E03-1082,The Multilingual Named Entity Recognition Framework,2003,11,24,1,1,18533,thierry poibeau,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper presents a multilingual system designed to recognize named entities in a wide variety of languages (currently more than 12 languages are concerned). The system includes original strategies to deal with a wide variety of encoding character sets, analysis strategies and algorithms to process these languages."
W02-1113,Generating Extraction Patterns from a Large Semantic Network and an Untagged Corpus,2002,20,9,1,1,18533,thierry poibeau,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"This paper presents a module dedicated to the elaboration of linguistic resources for a versatile Information Extraction system. In order to decrease the time spent on the elaboration of resources for the IE system and guide the end-user in a new domain, we suggest to use a machine learning system that helps defining new templates and associated resources. This knowledge is automatically derived from the text collection, in interaction with a large semantic network."
poibeau-etal-2002-evaluating,Evaluating resource acquisition tools for Information Extraction,2002,9,5,1,1,18533,thierry poibeau,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper evaluates two different approaches for the elaboration of semantic classes. The framework is an Information Extraction, which needs large amount of domain-dependent resources. An endogenous approach (corpus-based learning) is contrasted with a heterogeneous one (the use of a large semantic network). The two techniques are evaluated. Cet article vise a evaluer deux approches differentes pour la constitution de classes semantiques. Nous nous placons dans la perspective dxe2x80x99une application dxe2x80x99extraction dxe2x80x99information, pour laquelle la notion de classe semantique est primordiale. Une approche endogene (acquisition a partir dxe2x80x99un corpus) est contrastee avec une approche exogene (a travers un reseau semantique riche). Lxe2x80x99article presente une evaluation fine de ces deux techniques et leur complementarite possible."
C02-1062,Inferring Knowledge from a Large Semantic Network,2002,8,7,2,0,50131,dominique dutoit,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"In this paper, we present a rich semantic network based on a differential analysis. We then detail implemented measures that take into account common and differential features between words. In a last section, we describe some industrial applications."
2002.jeptalnrecital-long.23,{\\'E}valuer l{'}acquisition semi-automatique de classes s{\\'e}mantiques,2002,-1,-1,1,1,18533,thierry poibeau,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Cet article vise {\`a} {\'e}valuer deux approches diff{\'e}rentes pour la constitution de classes s{\'e}mantiques. Une approche endog{\`e}ne (acquisition {\`a} partir d{'}un corpus) est contrast{\'e}e avec une approche exog{\`e}ne ({\`a} travers un r{\'e}seau s{\'e}mantique riche). L{'}article pr{\'e}sente une {\'e}valuation fine de ces deux techniques.
2001.jeptalnrecital-tutoriel.6,Intex et ses applications informatiques,2001,-1,-1,2,0,5624,max silberztein,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Tutoriels,0,"Intex est un environnement de d{\'e}veloppement utilis{\'e} pour construire, tester et accumuler rapidement des motifs morpho-syntaxiques qui apparaissent dans des textes {\'e}crits en langue naturelle. Un survol du syst{\`e}me est pr{\'e}sent{\'e} dans [Silberztein, 1999] , le manuel d{'}instruction est disponible [Silberztein 2000]. Chaque description {\'e}l{\'e}mentaire est repr{\'e}sent{\'e}e par une grammaire locale, qui est habituellement entr{\'e}e en machine gr{\^a}ce {\`a} l{'}{\'e}diteur de graphe d{'}Intex. Une caract{\'e}ristique importante d{'}Intex est que chaque grammaire locale peut {\^e}tre facilement r{\'e}employ{\'e}e dans d{'}autres grammaires locales. Typiquement, les d{\'e}veloppeurs construisent des graphes {\'e}l{\'e}mentaires qui sont {\'e}quivalents {\`a} des transducteurs {\`a} {\'e}tats finis, et r{\'e}emploient ces graphes dans d{'}autres graphes de plus en plus complexes. Une seconde caract{\'e}ristique d{'}Intex est que les objets trait{\'e}s (grammaires, dictionnaires et textes) sont repr{\'e}sent{\'e}s de fa{\c{c}}on interne par des transducteurs {\`a} {\'e}tats finis. En cons{\'e}quence, toutes les fonctionnalit{\'e}s du syst{\`e}me se ram{\`e}nent {\`a} un nombre limit{\'e} d{'}op{\'e}rations sur des transducteurs. Par exemple, appliquer une grammaire {\`a} un texte revient {\`a} construire l{'}union des transducteurs {\'e}l{\'e}mentaires, la d{\'e}terminiser, puis {\`a} calculer l{'}intersection du r{\'e}sultat avec le transducteur du texte. Cette architecture permet d{'}utiliser des algorithmes efficaces (par ex. lorsqu{'}on applique un transducteur d{\'e}terministe {\`a} un texte pr{\'e}alablement index{\'e}), et donne {\`a} Intex la puissance d{'}une machine de Turing (gr{\^a}ce {\`a} la possibilit{\'e} d{'}appliquer des transducteurs en cascade). Dans ce tutoriel, nous montrerons comment utiliser un outil linguistique tel qu{'}Intex dans des environnements informatiques. Nous nous appuierons sur des applications de filtrage et d{'}extraction d{'}information, r{\'e}alis{\'e}es notamment au centre de recherche de Thales. Les applications suivantes seront d{\'e}taill{\'e}es, tant sur le plan linguistique qu{'}informatique filtrage d{'}information a partir d{'}un flux AFP [Meunier et al. l999] extraction de tables d{'}interaction entre g{\`e}nes {\`a} partir de bases de donn{\'e}es textuelles en g{\'e}nomique. [Poibeau 2001] Le tutoriel montrera comment Intex peut {\^e}tre employ{\'e} comme moteur de filtrage d{'}un flux de d{\'e}p{\^e}ches de type AFP dans un cadre industriel. Il d{\'e}taillera {\'e}galement les fonctionnalit{\'e}s de transformations des textes (transduction) permettant de passer rapidement de structures linguistiques vari{\'e}es {\`a} des formes normalis{\'e}es permettant de remplir une base de donn{\'e}es. Sur le plan informatique, on d{\'e}taillera l{'}appel aux routines Intex, les param{\'e}trages possibles (d{\'e}coupage en phrases, choix des dictionnaires...), et on survolera les nouvelles possibilit{\'e}s d{'}int{\'e}gration (Intex API)."
2001.jeptalnrecital-poster.7,Extraction de noms propres {\\`a} partir de textes vari{\\'e}s: probl{\\'e}matique et enjeux,2001,2,1,2,0,16534,leila kosseim,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cet article porte sur l{'}identification de noms propres {\`a} partir de textes {\'e}crits. Les strat{\'e}gies {\`a} base de r{\`e}gles d{\'e}velopp{\'e}es pour des textes de type journalistique se r{\'e}v{\`e}lent g{\'e}n{\'e}ralement insuffisantes pour des corpus compos{\'e}s de textes ne r{\'e}pondant pas {\`a} des crit{\`e}res r{\'e}dactionnels stricts. Apr{\`e}s une br{\`e}ve revue des travaux effectu{\'e}s sur des corpus de textes de nature journalistique, nous pr{\'e}sentons la probl{\'e}matique de l{'}analyse de textes vari{\'e}s en nous basant sur deux corpus compos{\'e}s de courriers {\'e}lectroniques et de transcriptions manuelles de conversations t{\'e}l{\'e}phoniques. Une fois les sources d{'}erreurs pr{\'e}sent{\'e}es, nous d{\'e}crivons l{'}approche utilis{\'e}e pour adapter un syst{\`e}me d{'}extraction de noms propres d{\'e}velopp{\'e} pour des textes journalistiques {\`a} l{'}analyse de messages {\'e}lectroniques."
2001.jeptalnrecital-long.27,Extraction d{'}information dans les bases de donn{\\'e}es textuelles en g{\\'e}nomique au moyen de transducteurs {\\`a} nombre fini d{'}{\\'e}tats,2001,-1,-1,1,1,18533,thierry poibeau,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,Cet article d{\'e}crit un syst{\`e}me d{'}extraction d{'}information sur les interactions entre g{\`e}nes {\`a} partir de grandes bases de donn{\'e}es textuelles. Le syst{\`e}me est fond{\'e} sur une analyse au moyen de transducteurs {\`a} nombre fini d{'}{\'e}tats. L{'}article montre comment une partie des ressources (verbes d{'}interaction) peut {\^e}tre acquise de mani{\`e}re semi-automatique. Une {\'e}valuation d{\'e}taill{\'e}e du syst{\`e}me est fournie.
