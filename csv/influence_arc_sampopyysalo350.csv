2020.coling-main.78,C18-1139,0,0.523915,"reatly influenced by deep transfer learning methods capable of creating contextual This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 904 Proceedings of the 28th International Conference on Computational Linguistics, pages 904–914 Barcelona, Spain (Online), December 8-13, 2020 representations of text, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018; Baevski et al., 2019). Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance. In this paper, we present a comprehensive exploration of the use of cross-sentence context for named entity recognition, focusing on the recent BERT deep transfer learning models (Devlin et al., 2018) based on self-attention and the transfor"
2020.coling-main.78,N19-1078,0,0.0446723,"y moved from approaches using word/character representations and manually engineered features (Passos et al., 2014; Chiu and Nichols, 2016) toward approaches directly utilizing deep learning-based contextual representations (Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Baevski et al., 2019) while adding few manually engineered features, if any. While successful in terms of NER performance, these approaches have tended to predict tags for one sentence at a time, discarding information from surrounding sentences. One recent method taking sentence context into account is that of Akbik et al. (2019), which addresses a weakness of an earlier contextual string embedding method (Akbik et al., 2018), specifically the issue of rare word representations occurring in underspecified contexts. Akbik et al. (2019) make the intuitive assumption that such occurrences happen when a named entity is expected to be known to the reader, i.e. the name is either introduced earlier in text or is of general in-domain knowledge. Their approach is to maintain a memory of contextual representations of each unique word/string in text and pool together contextual embeddings of a string occurring in text with the"
2020.coling-main.78,D19-1539,0,0.405091,"deep transfer learning methods capable of creating contextual This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 904 Proceedings of the 28th International Conference on Computational Linguistics, pages 904–914 Barcelona, Spain (Online), December 8-13, 2020 representations of text, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018; Baevski et al., 2019). Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance. In this paper, we present a comprehensive exploration of the use of cross-sentence context for named entity recognition, focusing on the recent BERT deep transfer learning models (Devlin et al., 2018) based on self-attention and the transformer architecture (Vaswa"
2020.coling-main.78,Q16-1026,0,0.0237846,"s in context consistently improves NER results on all of the tested languages and CMV further improves the results in most cases. Comparing performance to the current state-of-the-art NER results in the 5 languages, we find that our approach establishes new state-of-the-art results for English, Dutch, and Finnish, the best BERT-based results on German, and effectively matches the performance of a BERT-based method in Spanish. 2 Related work The state-of-the-art in NER has recently moved from approaches using word/character representations and manually engineered features (Passos et al., 2014; Chiu and Nichols, 2016) toward approaches directly utilizing deep learning-based contextual representations (Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Baevski et al., 2019) while adding few manually engineered features, if any. While successful in terms of NER performance, these approaches have tended to predict tags for one sentence at a time, discarding information from surrounding sentences. One recent method taking sentence context into account is that of Akbik et al. (2019), which addresses a weakness of an earlier contextual string embedding method (Akbik et al., 2018), specifically the iss"
2020.coling-main.78,2020.acl-main.747,0,0.0792716,"Missing"
2020.coling-main.78,W03-0425,0,0.245663,"ommon choice. This window fits dozens of typical sentences of input at a time, allowing us to include extensive sentence context. Here, we first study the effect of predicting tags for individual sentences when they are moved around the window, surrounded by their original document context from the source data. Second, we utilize different predictions for the same sentences to potentially further improve performance, combining predictions using majority voting, adapting an approach that has been used already in early NER implementations (Tjong Kim Sang et al., 2000; Van Halteren et al., 2001; Florian et al., 2003). We evaluate these approaches on five languages, contrasting NER results using BERT without cross-sentence information, sentences in context, and aggregation using Contextual Majority Voting (CMV) on well-established benchmark datasets. We show that using sentences in context consistently improves NER results on all of the tested languages and CMV further improves the results in most cases. Comparing performance to the current state-of-the-art NER results in the 5 languages, we find that our approach establishes new state-of-the-art results for English, Dutch, and Finnish, the best BERT-based"
2020.coling-main.78,P06-1141,0,0.0873453,"ed approaches. Rule/knowledgebased approaches such as that of Mikheev et al. (1998) typically match strings to lexicons and similar domain knowledge sources, possibly going through text multiple times with refinement based on entities found on earlier passes. Later, manually engineered features were used to incorporate information from the surrounding text, whole documents, data sets and also from external sources. The number of different features and classifiers grew during the years and it was normal that the features also contained cross-sentence information in some form as for example in (Krishnan and Manning, 2006). Dense representations of text such as word, character, string and subword embeddings first started to appear in NER methods as additional features given to classifiers (Collobert et al., 2011). Step by step, feature engineering has been demoted to a lesser role, as the most recent deep learning approaches learn to create meaningful and context-sensitive representations of text by pre-training with vast amounts of unlabelled data. These contextual representations are often used directly as features for existing NER architectures or fine-tuned with labelled data to match a certain task. In rec"
2020.coling-main.78,2020.lrec-1.302,0,0.088104,"Missing"
2020.coling-main.78,2020.acl-main.519,0,0.440832,"nd CoNLL’03 NER benchmarks, demonstrates that our proposed approach can improve on the state-of-the-art NER results on English, Dutch, and Finnish, achieves the best reported BERT-based results on German, and is on par with other BERT-based approaches in Spanish. We release all methods implemented in this work under open licenses. 1 Introduction Named entity recognition (NER) approaches have evolved through various methodological phases, broadly including rule/knowledge-based, unsupervised, feature engineering and supervised learning, and feature inferring approaches (Yadav and Bethard, 2018; Li et al., 2020a). The use of cross-sentence information in some form has been a normal part of many NER methods in the former categories, but its role has diminished with the current feature inferring deep learning based approaches. Rule/knowledgebased approaches such as that of Mikheev et al. (1998) typically match strings to lexicons and similar domain knowledge sources, possibly going through text multiple times with refinement based on entities found on earlier passes. Later, manually engineered features were used to incorporate information from the surrounding text, whole documents, data sets and also"
2020.coling-main.78,P19-1524,0,0.0142449,"unt for NER was proposed by Luo et al. (2020), where in addition to token representations, also sentence and document level representations are calculated and used for classification using a CRF model. A sliding window is used by Wu and Dredze (2019) so that part of the input is preserved as context when the window is moved forward in text. Baevski et al. (2019) state that they use longer paragraphs in pre-training their model, but it is not mentioned in the paper if such longer paragraphs are used also in fine-tuning the model or predicting tags for NER. Some other approaches such as that of Liu et al. (2019a) include explicit global information in the form of e.g. gazetteers. Also, some approaches formulate NER as a span finding task instead of sequence labelling (Banerjee et al., 2019; Li et al., 2020b). These approaches would likely allow the use of longer sequences, but the incorporation of cross-sentence information is not explicitly proposed by the authors. 905 In the paper introducing BERT, Devlin et al. (2018) write in the description of their NER evaluation “we include the maximal document context provided by the data.” However, no detailed description of how this inclusion was implement"
2020.coling-main.78,P19-1233,0,0.0340541,"unt for NER was proposed by Luo et al. (2020), where in addition to token representations, also sentence and document level representations are calculated and used for classification using a CRF model. A sliding window is used by Wu and Dredze (2019) so that part of the input is preserved as context when the window is moved forward in text. Baevski et al. (2019) state that they use longer paragraphs in pre-training their model, but it is not mentioned in the paper if such longer paragraphs are used also in fine-tuning the model or predicting tags for NER. Some other approaches such as that of Liu et al. (2019a) include explicit global information in the form of e.g. gazetteers. Also, some approaches formulate NER as a span finding task instead of sequence labelling (Banerjee et al., 2019; Li et al., 2020b). These approaches would likely allow the use of longer sequences, but the incorporation of cross-sentence information is not explicitly proposed by the authors. 905 In the paper introducing BERT, Devlin et al. (2018) write in the description of their NER evaluation “we include the maximal document context provided by the data.” However, no detailed description of how this inclusion was implement"
2020.coling-main.78,2020.lrec-1.567,1,0.811246,"ll the tokens corresponding to one word in text are masked instead of completely random tokens, which often leaves some of the tokens in multi-token words unmasked. We aimed to apply sufficiently large, widely-used benchmark datasets for evaluating NER results, assessing our methods primarily on the CoNLL’02 and CoNLL’03 Shared task Named entity recognition datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which cover four of our five target languages. For the fifth language, Finnish, we use two recently published named entity recognition corpora (Ruokolainen et al., 2019; Luoma et al., 2020)9,10 . These two Finnish datasets are annotated in a compatible way, and for this study they are combined into a single corpus by simple concatenation, following Luoma et al. (2020). 1 https://github.com/google-research/bert/issues/581 https://github.com/google-research/bert/issues/569 3 https://github.com/wietsedv/bertje 4 https://github.com/google-research/bert 5 https://github.com/TurkuNLP/FinBERT 6 https://deepset.ai/german-bert 7 https://github.com/dccuchile/beto 8 https://github.com/google-research/bert 9 https://github.com/mpsilfve/finer-data 10 https://github.com/TurkuNLP/turku-ner-cor"
2020.coling-main.78,M98-1021,0,0.102737,"thods implemented in this work under open licenses. 1 Introduction Named entity recognition (NER) approaches have evolved through various methodological phases, broadly including rule/knowledge-based, unsupervised, feature engineering and supervised learning, and feature inferring approaches (Yadav and Bethard, 2018; Li et al., 2020a). The use of cross-sentence information in some form has been a normal part of many NER methods in the former categories, but its role has diminished with the current feature inferring deep learning based approaches. Rule/knowledgebased approaches such as that of Mikheev et al. (1998) typically match strings to lexicons and similar domain knowledge sources, possibly going through text multiple times with refinement based on entities found on earlier passes. Later, manually engineered features were used to incorporate information from the surrounding text, whole documents, data sets and also from external sources. The number of different features and classifiers grew during the years and it was normal that the features also contained cross-sentence information in some form as for example in (Krishnan and Manning, 2006). Dense representations of text such as word, character,"
2020.coling-main.78,W14-1609,0,0.0168987,"w that using sentences in context consistently improves NER results on all of the tested languages and CMV further improves the results in most cases. Comparing performance to the current state-of-the-art NER results in the 5 languages, we find that our approach establishes new state-of-the-art results for English, Dutch, and Finnish, the best BERT-based results on German, and effectively matches the performance of a BERT-based method in Spanish. 2 Related work The state-of-the-art in NER has recently moved from approaches using word/character representations and manually engineered features (Passos et al., 2014; Chiu and Nichols, 2016) toward approaches directly utilizing deep learning-based contextual representations (Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Baevski et al., 2019) while adding few manually engineered features, if any. While successful in terms of NER performance, these approaches have tended to predict tags for one sentence at a time, discarding information from surrounding sentences. One recent method taking sentence context into account is that of Akbik et al. (2019), which addresses a weakness of an earlier contextual string embedding method (Akbik et al., 20"
2020.coling-main.78,N18-1202,0,0.397893,"general and NER in particular have been greatly influenced by deep transfer learning methods capable of creating contextual This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 904 Proceedings of the 28th International Conference on Computational Linguistics, pages 904–914 Barcelona, Spain (Online), December 8-13, 2020 representations of text, to the extent that many of the state-of-the-art NER systems mainly differ from one another on the basis of how these contextual representations are created (Peters et al., 2018; Devlin et al., 2018; Akbik et al., 2018; Baevski et al., 2019). Using such models, sequence tagging tasks are often approached one sentence at a time, essentially discarding any information available in the broader surrounding context, and there is only little recent study on the use of cross-sentence context – sentences around the sentence of interest – to improve sequence tagging performance. In this paper, we present a comprehensive exploration of the use of cross-sentence context for named entity recognition, focusing on the recent BERT deep transfer learning models (Devlin et al., 2018)"
2020.coling-main.78,W95-0107,0,0.101866,"asets for the CoNLL shared task languages contain four different classes of named entities: Person (PER), Organization (ORG), Location (LOC) and Miscellaneous (MISC). The Finnish NER datasets also use the PER, ORG, and LOC types along with three others, Product (PROD), Event (EVENT), and Date (DATE). For implementation purposes we converted all the datasets to the same format prior to experiments: The character encoding of each file was converted to UTF-8, and the NER labelling scheme was converted to IOB2 (Ratnaparkhi and Marcus, 1998) also for corpora that were originally in the IOB scheme (Ramshaw and Marcus, 1995). By contrast to the older IOB scheme, in the IOB2 scheme the label for the first token of a named entity is always marked with a B-prefix (e.g. BPER), even if the previous token is not part of a named entity. The key statistics for the NER datasets are presented in Table 1. Finally, we note that all the datasets except CoNLL’02 Spanish provide information on document boundaries using special -DOCSTART- tokens at the start of each new document. 4 Methods As the starting point for exploring the cross-sentence information for NER using BERT, we use a NER pipeline implementation introduced by Vir"
2020.coling-main.78,P19-1527,0,0.0637711,"Missing"
2020.coling-main.78,C00-2124,0,0.286663,"d during pre-training, with 512 wordpieces a common choice. This window fits dozens of typical sentences of input at a time, allowing us to include extensive sentence context. Here, we first study the effect of predicting tags for individual sentences when they are moved around the window, surrounded by their original document context from the source data. Second, we utilize different predictions for the same sentences to potentially further improve performance, combining predictions using majority voting, adapting an approach that has been used already in early NER implementations (Tjong Kim Sang et al., 2000; Van Halteren et al., 2001; Florian et al., 2003). We evaluate these approaches on five languages, contrasting NER results using BERT without cross-sentence information, sentences in context, and aggregation using Contextual Majority Voting (CMV) on well-established benchmark datasets. We show that using sentences in context consistently improves NER results on all of the tested languages and CMV further improves the results in most cases. Comparing performance to the current state-of-the-art NER results in the 5 languages, we find that our approach establishes new state-of-the-art results fo"
2020.coling-main.78,J01-2002,0,0.391355,"Missing"
2020.coling-main.78,D19-1077,0,0.0634864,"each unique word/string in text and pool together contextual embeddings of a string occurring in text with the contextual embeddings of the same string earlier in text. This pooled contextual embedding is then concatenated with the current contextual embedding to get the final embedding to use in classification. Another recent approach taking broader context into account for NER was proposed by Luo et al. (2020), where in addition to token representations, also sentence and document level representations are calculated and used for classification using a CRF model. A sliding window is used by Wu and Dredze (2019) so that part of the input is preserved as context when the window is moved forward in text. Baevski et al. (2019) state that they use longer paragraphs in pre-training their model, but it is not mentioned in the paper if such longer paragraphs are used also in fine-tuning the model or predicting tags for NER. Some other approaches such as that of Liu et al. (2019a) include explicit global information in the form of e.g. gazetteers. Also, some approaches formulate NER as a span finding task instead of sequence labelling (Banerjee et al., 2019; Li et al., 2020b). These approaches would likely a"
2020.coling-main.78,C18-1182,0,0.0257796,"including the CoNLL’02 and CoNLL’03 NER benchmarks, demonstrates that our proposed approach can improve on the state-of-the-art NER results on English, Dutch, and Finnish, achieves the best reported BERT-based results on German, and is on par with other BERT-based approaches in Spanish. We release all methods implemented in this work under open licenses. 1 Introduction Named entity recognition (NER) approaches have evolved through various methodological phases, broadly including rule/knowledge-based, unsupervised, feature engineering and supervised learning, and feature inferring approaches (Yadav and Bethard, 2018; Li et al., 2020a). The use of cross-sentence information in some form has been a normal part of many NER methods in the former categories, but its role has diminished with the current feature inferring deep learning based approaches. Rule/knowledgebased approaches such as that of Mikheev et al. (1998) typically match strings to lexicons and similar domain knowledge sources, possibly going through text multiple times with refinement based on entities found on earlier passes. Later, manually engineered features were used to incorporate information from the surrounding text, whole documents, da"
2020.findings-emnlp.387,W19-4828,0,0.0285849,"ious neural approaches. The bidirectional BERT (Devlin et al., 2018) language model has since been widely adopted as the baseline for transformer models, and it has been successfully applied to a broad range of NLP tasks from standard language modeling to question answering, text summarization, and machine translation. A number of papers have been dedicated to studying why and how this model performs so well, including comparison to classical NLP (Tenney et al., 2019), investigation of the newly introduced multi-head attention mechanism (Michel et al., 2019), and analyses of what BERT learns (Clark et al., 2019). A good recent review is presented by Rogers et al. (2020). Following BERT, a number of variations of language models using the transformer architecture have been introduced, including extended models such as XLM (Lample and Conneau, 2019) and XLNet (Yang et al., 2019) as well as more efficient ones like DistillBERT (Sanh et al., 2019), ALBERT (Lan et al., 2019), and ELECTRA (Clark et al., 2020). However, the vast majority of these studies have focused only on English models. While Google has released a multilingual BERT model trained on 100+ languages, only recently have monolingual models f"
2020.findings-emnlp.387,D19-1279,0,0.075998,"Missing"
2020.findings-emnlp.387,P19-1452,0,0.0353696,"s ago, when the first Transformerbased model (Vaswani et al., 2017) demonstrated a significant increase in state-of-the-art results compared to previous neural approaches. The bidirectional BERT (Devlin et al., 2018) language model has since been widely adopted as the baseline for transformer models, and it has been successfully applied to a broad range of NLP tasks from standard language modeling to question answering, text summarization, and machine translation. A number of papers have been dedicated to studying why and how this model performs so well, including comparison to classical NLP (Tenney et al., 2019), investigation of the newly introduced multi-head attention mechanism (Michel et al., 2019), and analyses of what BERT learns (Clark et al., 2019). A good recent review is presented by Rogers et al. (2020). Following BERT, a number of variations of language models using the transformer architecture have been introduced, including extended models such as XLM (Lample and Conneau, 2019) and XLNet (Yang et al., 2019) as well as more efficient ones like DistillBERT (Sanh et al., 2019), ALBERT (Lan et al., 2019), and ELECTRA (Clark et al., 2020). However, the vast majority of these studies have foc"
2020.findings-emnlp.387,tiedemann-2012-parallel,0,0.0560875,"Missing"
2020.findings-emnlp.387,K18-2001,0,0.0364765,"Missing"
2020.findings-emnlp.387,2020.tacl-1.54,0,0.0298687,"al., 2018) language model has since been widely adopted as the baseline for transformer models, and it has been successfully applied to a broad range of NLP tasks from standard language modeling to question answering, text summarization, and machine translation. A number of papers have been dedicated to studying why and how this model performs so well, including comparison to classical NLP (Tenney et al., 2019), investigation of the newly introduced multi-head attention mechanism (Michel et al., 2019), and analyses of what BERT learns (Clark et al., 2019). A good recent review is presented by Rogers et al. (2020). Following BERT, a number of variations of language models using the transformer architecture have been introduced, including extended models such as XLM (Lample and Conneau, 2019) and XLNet (Yang et al., 2019) as well as more efficient ones like DistillBERT (Sanh et al., 2019), ALBERT (Lan et al., 2019), and ELECTRA (Clark et al., 2020). However, the vast majority of these studies have focused only on English models. While Google has released a multilingual BERT model trained on 100+ languages, only recently have monolingual models for other languages started to appear: FlauBERT for French ("
2020.findings-emnlp.387,S13-2056,0,0.0263147,"Missing"
2020.iwpt-1.17,S15-2153,0,0.132715,"Missing"
2020.iwpt-1.17,K17-3001,1,0.866065,"Missing"
2020.lrec-1.497,de-marneffe-etal-2006-generating,1,\N,Missing
2020.lrec-1.497,zeman-2008-reusable,1,\N,Missing
2020.lrec-1.497,de-marneffe-etal-2014-universal,1,\N,Missing
2020.lrec-1.497,W08-1301,1,\N,Missing
2020.lrec-1.497,petrov-etal-2012-universal,0,\N,Missing
2020.lrec-1.497,P13-1051,1,\N,Missing
2020.lrec-1.497,P15-2111,0,\N,Missing
2020.lrec-1.497,L16-1376,1,\N,Missing
2020.lrec-1.497,L16-1262,1,\N,Missing
2020.lrec-1.497,W18-6012,1,\N,Missing
2020.lrec-1.567,W16-5104,0,0.0188314,"English, NER methods have long aimed for language independence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 Tokens 31 906 14 281 13 232 22 287 14 390 16 982 19 932 23 920 12 477 32 709 202 116 Table 1: Universal Dependencies Finnish TDT corpus statistics by genre. Th"
2020.lrec-1.567,D19-1539,0,0.0329712,"processing (NLP), a key part of information extraction, and a prerequisite for many text mining goals. NER has been a major focus of annotation and method development efforts for decades (Grishman and Sundheim, 1996) and well-established reference corpora annotated for named entities are available for many languages (Tjong Kim Sang, 2002; Sang and De Meulder, 2003; Hovy et al., 2006). For languages with corpora of sufficient size and coverage, machine learning methods for NER can achieve very high performance, approaching human annotation quality (Chiu and Nichols, 2016; Devlin et al., 2018; Baevski et al., 2019). Although a degree of success has been demonstrated in multilingual and crosslingual methods for NER (Al-Rfou et al., 2015), manually annotated corpora for each language remain required to fully realize the benefits of the most recent advances in NER methodology, and many lower-resourced languages still lack broad-coverage corpora of sufficient size and quality to train NER methods comparable to those available for high-resource languages. In this paper, we focus on Finnish, a Uralic language spoken (nearly exclusively) by approx. 5 million people of the 5.5 million Finnish population. Alongs"
2020.lrec-1.567,Q16-1026,0,0.057543,"ER) is a fundamental task in natural language processing (NLP), a key part of information extraction, and a prerequisite for many text mining goals. NER has been a major focus of annotation and method development efforts for decades (Grishman and Sundheim, 1996) and well-established reference corpora annotated for named entities are available for many languages (Tjong Kim Sang, 2002; Sang and De Meulder, 2003; Hovy et al., 2006). For languages with corpora of sufficient size and coverage, machine learning methods for NER can achieve very high performance, approaching human annotation quality (Chiu and Nichols, 2016; Devlin et al., 2018; Baevski et al., 2019). Although a degree of success has been demonstrated in multilingual and crosslingual methods for NER (Al-Rfou et al., 2015), manually annotated corpora for each language remain required to fully realize the benefits of the most recent advances in NER methodology, and many lower-resourced languages still lack broad-coverage corpora of sufficient size and quality to train NER methods comparable to those available for high-resource languages. In this paper, we focus on Finnish, a Uralic language spoken (nearly exclusively) by approx. 5 million people o"
2020.lrec-1.567,E14-2016,0,0.0295981,"dence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 Tokens 31 906 14 281 13 232 22 287 14 390 16 982 19 932 23 920 12 477 32 709 202 116 Table 1: Universal Dependencies Finnish TDT corpus statistics by genre. The source data for the grammar examples section of the corpus"
2020.lrec-1.567,doddington-etal-2004-automatic,0,0.0921637,"ebank and Finnish NER annotation guidelines and tools from the FiNER effort to create an open broad-coverage corpus suitable for training modern NER methods for Finnish. Our results demonstrate that the resulting annotation has high internal consistency, is compatible with existing resources, and can support accurate Finnish NER using deep transfer learning methods. 2. Related work The direction of NER research has for long been guided by influential programs and shared tasks such as the Message Understanding Conferences (Grishman and Sundheim, 1996), the Automatic Content Extraction program (Doddington et al., 2004) and the CoNLL shared tasks on language-independent NER (Tjong Kim Sang, 2002; Sang and De Meulder, 2003). While these efforts have included also many other targets, they have been instrumental in cementing the recognition of person, organization and location names as a core NER goal, with the identification of time expressions as a frequent associated theme. While early efforts focused largely on English, NER methods have long aimed for language independence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and Engl"
2020.lrec-1.567,C96-1079,0,0.835683,"e identification of names in blog posts and transcribed speech are also identified. The newly introduced Turku NER corpus and related resources introduced in this work are released under open licenses via https://turkunlp.org/turku-ner-corpus. Keywords: Named Entity Recognition, Finnish, Corpus, Annotation, Deep Learning 1. Introduction Named entity recognition (NER) is a fundamental task in natural language processing (NLP), a key part of information extraction, and a prerequisite for many text mining goals. NER has been a major focus of annotation and method development efforts for decades (Grishman and Sundheim, 1996) and well-established reference corpora annotated for named entities are available for many languages (Tjong Kim Sang, 2002; Sang and De Meulder, 2003; Hovy et al., 2006). For languages with corpora of sufficient size and coverage, machine learning methods for NER can achieve very high performance, approaching human annotation quality (Chiu and Nichols, 2016; Devlin et al., 2018; Baevski et al., 2019). Although a degree of success has been demonstrated in multilingual and crosslingual methods for NER (Al-Rfou et al., 2015), manually annotated corpora for each language remain required to fully"
2020.lrec-1.567,C18-1177,0,0.0317584,"Missing"
2020.lrec-1.567,W15-1842,0,0.0207413,"s supporting the representation. 4.2. https://turkunlp.org/turku-ner-corpus NER methods We apply a number of machine learning approaches as well as a previously introduced rule-based system to assess the corpus and the NER task it represents. FiNER tagger (Kettunen and Löfberg, 2017; Ruokolainen et al., 2019) is a dictionary- and rule-based tagger for Finnish NER that has been developed together with the FiNER corpus. The system is based on a combination of morphological analysis and tagging tools (Pirinen, 2015) and an extensive dictionary of known names together with pattern-matching rules (Hardwick et al., 2015) to detect and classify targeted mentions. Simple CRF We apply a simple baseline tagger using explicitly defined features derived from the surface forms of words using the CRFsuite (Okazaki, 2007) implementation of first-order linear chain conditional random fields (CRFs) (Lafferty et al., 2001), a probabilistic sequence labeling model underlying many NER methods. Specifically, we use the features defined by the CRFsuite NER feature extraction example, including focus and context word prefixes, suffixes, shape features, and combinations of these. We refer to the documentation and implementatio"
2020.lrec-1.567,N06-2015,0,0.277555,"under open licenses via https://turkunlp.org/turku-ner-corpus. Keywords: Named Entity Recognition, Finnish, Corpus, Annotation, Deep Learning 1. Introduction Named entity recognition (NER) is a fundamental task in natural language processing (NLP), a key part of information extraction, and a prerequisite for many text mining goals. NER has been a major focus of annotation and method development efforts for decades (Grishman and Sundheim, 1996) and well-established reference corpora annotated for named entities are available for many languages (Tjong Kim Sang, 2002; Sang and De Meulder, 2003; Hovy et al., 2006). For languages with corpora of sufficient size and coverage, machine learning methods for NER can achieve very high performance, approaching human annotation quality (Chiu and Nichols, 2016; Devlin et al., 2018; Baevski et al., 2019). Although a degree of success has been demonstrated in multilingual and crosslingual methods for NER (Al-Rfou et al., 2015), manually annotated corpora for each language remain required to fully realize the benefits of the most recent advances in NER methodology, and many lower-resourced languages still lack broad-coverage corpora of sufficient size and quality t"
2020.lrec-1.567,W19-6142,0,0.0177313,"the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 Tokens 31 906 14 281 13 232 22 287 14 390 16 982 19 932 23 920 12 477 32 709 202 116 Table 1: Universal Dependencies Finnish TDT corpus statistics by genre. The source data for the grammar examples section of the corpus did not originally have document structu"
2020.lrec-1.567,W19-6123,0,0.0147139,"med for language independence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 Tokens 31 906 14 281 13 232 22 287 14 390 16 982 19 932 23 920 12 477 32 709 202 116 Table 1: Universal Dependencies Finnish TDT corpus statistics by genre. The source data for the gramma"
2020.lrec-1.567,K18-2013,0,0.197131,"those available for high-resource languages. In this paper, we focus on Finnish, a Uralic language spoken (nearly exclusively) by approx. 5 million people of the 5.5 million Finnish population. Alongside with the minority language Swedish, Finnish is the official language of the country. Finnish has a long and strong tradition of linguistic research (Setälä, 1880; Hakulinen et al., 2004) and a number of modern language resources and NLP tools are available for the language, such as manually annotated treebanks (Haverinen et al., 2014) and morphological and syntactic analyzers (Pirinen, 2015; Kanerva et al., 2018). However, resources for Finnish named entity recognition are lacking: a manually annotated corpus for Finnish NER, FiNER, was only recently made available (Ruokolainen et al., 2019), and its training data only covers a single specialized text domain, namely technology news. ∗ Equal contribution In this paper, we present a new manually annotated NER corpus that emphasizes broad coverage of different genres, topics, and styles of writing. We draw on the source texts and existing manual annotation of the original Universal Dependencies (Nivre et al., 2016) Finnish treebank and Finnish NER annota"
2020.lrec-1.567,W17-0204,0,0.25018,"ts. Initial exploratory annotation was created for part of the data by 11 students as part of NLP course projects. This annotation was then made consistent and extended to cover the entire corpus by one primary annotator working with an annotation coordinator. Issues and open questions encountered in the annotation were logged and compiled into annotation guidelines extending and further specifying the application of the FiNER guidelines to phenomena encountered in the new text domains. To improve consistency with the FiNER annotation, the corpus texts were then tagged using the FiNER tagger (Kettunen and Löfberg, 2017) and differences in annotation examined manually to identify potential divergences from the annotation criteria of the FiNER effort. This use of the tool could be viewed as potentially introducing a bias in favour of the FiNER tagger: for example, annotation errors of omission that are caught by the tagger are more likely to be fixed than ones that are not. However, we consider this risk of bias to be minor and note that the tagger represents a baseline method for our study, and any possible bias introduced by this cross-check would thus work against the methods for Finnish NER proposed in thi"
2020.lrec-1.567,P16-1101,0,0.0414107,"odel underlying many NER methods. Specifically, we use the features defined by the CRFsuite NER feature extraction example, including focus and context word prefixes, suffixes, shape features, and combinations of these. We refer to the documentation and implementation distributed with CRFsuite7 for the full details of the feature representation. We note that this baseline is intentionally knowledge-poor, not incorporating e.g. dictionary or word vector features. BiLSTM-CNN-CRF We use the NCRF++ neural sequence labeling toolkit (Yang and Zhang, 2018) implementation of the NER model proposed by Ma and Hovy (2016), which uses a concatenation of word vectors and character representations computed using convolutional neural networks (CNNs) to represent words as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with a CRF output layer. This class of models represented the state of the art in neural NER methods prior to the introduction of deep transfer learning methods such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). 7 6 B-PER I-PER O B-ORG I-ORG O O http://www.chokkan.org/software/ crfsuite/ 4618 BERT (Devlin et al., 2018) is a state-of-the-art deep transfer learning appr"
2020.lrec-1.567,L16-1262,1,0.858773,"Missing"
2020.lrec-1.567,N18-1202,0,0.0177554,"not incorporating e.g. dictionary or word vector features. BiLSTM-CNN-CRF We use the NCRF++ neural sequence labeling toolkit (Yang and Zhang, 2018) implementation of the NER model proposed by Ma and Hovy (2016), which uses a concatenation of word vectors and character representations computed using convolutional neural networks (CNNs) to represent words as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with a CRF output layer. This class of models represented the state of the art in neural NER methods prior to the introduction of deep transfer learning methods such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). 7 6 B-PER I-PER O B-ORG I-ORG O O http://www.chokkan.org/software/ crfsuite/ 4618 BERT (Devlin et al., 2018) is a state-of-the-art deep transfer learning approach based on the transformer model (Vaswani et al., 2017) and pre-training on large unannotated corpora. BERT can be readily applied to sequence labeling tasks such as NER by attaching a timedistributed dense layer on top of the model output layer and fine-tuning the model on data with named entity annotation. In this work, we apply the recently introduced FinBERT model8 pre-trained from scratch on Finnis"
2020.lrec-1.567,pinnis-2012-latvian,0,0.0640914,"Missing"
2020.lrec-1.567,W15-1844,0,0.11564,"s comparable to those available for high-resource languages. In this paper, we focus on Finnish, a Uralic language spoken (nearly exclusively) by approx. 5 million people of the 5.5 million Finnish population. Alongside with the minority language Swedish, Finnish is the official language of the country. Finnish has a long and strong tradition of linguistic research (Setälä, 1880; Hakulinen et al., 2004) and a number of modern language resources and NLP tools are available for the language, such as manually annotated treebanks (Haverinen et al., 2014) and morphological and syntactic analyzers (Pirinen, 2015; Kanerva et al., 2018). However, resources for Finnish named entity recognition are lacking: a manually annotated corpus for Finnish NER, FiNER, was only recently made available (Ruokolainen et al., 2019), and its training data only covers a single specialized text domain, namely technology news. ∗ Equal contribution In this paper, we present a new manually annotated NER corpus that emphasizes broad coverage of different genres, topics, and styles of writing. We draw on the source texts and existing manual annotation of the original Universal Dependencies (Nivre et al., 2016) Finnish treebank"
2020.lrec-1.567,W15-1821,1,0.724226,"a central goal of our effort is to create a corpus that allows state-of-the-art NER methods to be trained for Finnish to achieve a high level of recognition performance across multiple domains. 3. Corpus annotation We next introduce the source data, the annotation targets and the manual annotation process of the newly created Turku NER corpus and briefly discuss some details of the annotation guidelines. 3.1. Data We draw the texts for our Finnish NER corpus from the Universal Dependencies (UD) (Nivre et al., 2016) version of the Turku Dependency Treebank (TDT) corpus (Haverinen et al., 2014; Pyysalo et al., 2015). TDT is a broadcoverage corpus spanning a range of text domains including news, blog posts, and legal texts (Table 1) with manual annotation for morphology and dependency syntax. One of the benefits of adding a named entity annotation layer to the treebank data is that its existing annotations can support the annotation effort. First, like all UD treebanks, the TDT corpus annotation includes part-of-speech tagging according to the Universal POS tagset defined by the UD project. Train Dev Test Total http://www.digitoday.fi/ Sentences 12 217 1 364 1 555 15 136 Tokens 162 746 18 308 21 062 202 1"
2020.lrec-1.567,W03-0419,0,0.252405,"Missing"
2020.lrec-1.567,Y09-2045,0,0.0210973,"as a core NER goal, with the identification of time expressions as a frequent associated theme. While early efforts focused largely on English, NER methods have long aimed for language independence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 Tokens 31 906 14 281"
2020.lrec-1.567,D18-1309,0,0.0475398,"Missing"
2020.lrec-1.567,E12-2021,1,0.861177,"Missing"
2020.lrec-1.567,taule-etal-2008-ancora,0,0.062573,"n and location names as a core NER goal, with the identification of time expressions as a frequent associated theme. While early efforts focused largely on English, NER methods have long aimed for language independence, supported by resources such as the comparably annotated corpora introduced for Spanish and Dutch in the CoNLL 2002 shared task and English and German in 2003. Today, NER corpora covering a range of domains and entity types are available for many major languages and basic NER resources are an expected component of the basic NLP toolkit for any language (e.g. (Hovy et al., 2006; Taulé et al., 2008; Singh et al., 2009; Munkhjargal et al., 2015)). For Nordic and other countries near Finland, NER corpora and models exist for Swedish (Almgren et al., 2016), Norwegian (Johansen, 2019) Danish (Derczynski et al., 2014), Icelandic (Ingólfsdóttir et al., 2019), Latvian and Lithua4615 Section Wikipedia articles Wikinews articles University online news Blog entries Student magazine articles Grammar examples Europarl speeches JRC-Acquis legislation Financial news Fiction Total Documents 200 100 50 77 23 80 80 29 50 65 754 Sentences 2 269 1 120 942 1 781 1 058 2 002 1 082 1 141 1 002 2 739 15 136 T"
2020.lrec-1.567,W02-2024,0,0.382973,"Missing"
2020.lrec-1.567,W13-2412,0,0.0414525,"Missing"
2020.lrec-1.567,P18-4013,0,0.0153917,"CRFs) (Lafferty et al., 2001), a probabilistic sequence labeling model underlying many NER methods. Specifically, we use the features defined by the CRFsuite NER feature extraction example, including focus and context word prefixes, suffixes, shape features, and combinations of these. We refer to the documentation and implementation distributed with CRFsuite7 for the full details of the feature representation. We note that this baseline is intentionally knowledge-poor, not incorporating e.g. dictionary or word vector features. BiLSTM-CNN-CRF We use the NCRF++ neural sequence labeling toolkit (Yang and Zhang, 2018) implementation of the NER model proposed by Ma and Hovy (2016), which uses a concatenation of word vectors and character representations computed using convolutional neural networks (CNNs) to represent words as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with a CRF output layer. This class of models represented the state of the art in neural NER methods prior to the introduction of deep transfer learning methods such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). 7 6 B-PER I-PER O B-ORG I-ORG O O http://www.chokkan.org/software/ crfsuite/ 4618 BERT (Devlin"
2020.wac-1.3,Q17-1010,0,0.00695365,"ch data as possible, for instance by using a dedicated crawl or extracting data from existing crawl-based datasets, such as Common Crawl1 . As crawling and compilation pipelines are based on automatic processes, the resulting data can contain boilerplate texts, machine translations, and even text in languages other than that targeted in the corpus construction. Furthermore, there is typically no information on the kinds of registers that the web language resources represent. Although both linguistic and NLP efforts have achieved significant advances using web data (e.g. Mikolov et al. (2013), Bojanowski et al. (2017), Yang et al. (2019)), for a number of end uses, better structured web language resources with clean, full texts and register information would be essential to realizing their full potential. Currently, a number of large web-crawled datasets are available. However, resources emphasizing the collection of clean texts, such as WaCky (Baroni et al., 2009) and COW (Sch¨afer, 2016b), represent only a limited number of languages. The ones with a more extensive selection of languages, such as OSCAR (Ortiz Su´arez et al., 2019), have not gone through detailed text cleaning processes. Moreover, registe"
2020.wac-1.3,L18-1550,0,0.0161585,"ith low HTML tag density belong often to main text body. Cow corpora (Sch¨afer et al., 2013) are processed based on a detailed pipeline with a tool classifying paragraphs as boilerplate or not (Sch¨afer, 2016a) and a another one classifying entire documents as coherent text or not (Sch¨afer et al., 2013). These are based on manually annotated data and a document-level unsupervised method to evaluate the text quality based on short and very frequent words. To create the monolingual OSCAR subcorpora, Ortiz Su´arez et al. (2019) processed Common Crawl data using a pipeline based on the system by Grave et al. (2018), which included language detection using fastText (Joulin et al., 2016), basic heuristic cleaning, and hashing-based deduplication, but no boilerplate removal. A number of readily available boilerplate removal packages exist. JusText3 is a frequently applied boilerplate removal package in python. Trafilatura4 is a recently developed web-scraping python library that preserves also some of the web page structure. According to an evaluation included Documents 18.2 million 19.4 million Tokens 10.5 billion 7.7 billion Table 1: Sizes of the deduplicated CoNLL 2017 Common Crawl-based datasets for Fr"
2020.wac-1.3,W19-6130,1,0.353165,"Missing"
2020.wac-1.3,L16-1712,0,0.0653347,"Missing"
2021.eacl-srw.24,D14-1181,0,0.0070199,"Missing"
2021.eacl-srw.24,W19-6130,1,0.816902,"Missing"
2021.eacl-srw.24,2020.wac-1.3,1,0.778408,"Missing"
2021.eacl-srw.24,2020.lrec-1.302,0,0.0904363,"Missing"
2021.eacl-srw.24,2020.findings-emnlp.150,0,0.0529507,"Missing"
2021.eacl-srw.24,2021.ccl-1.108,0,0.0211807,"Missing"
2021.eacl-srw.24,K15-1034,0,0.0253548,"pus can match or surpass previously published monolingual models, and 2) that lightweight monolingual classification requiring very little training data can reach or surpass our zero-shot performance. We further analyse classification results finding that certain registers continue to pose challenges in particular for cross-lingual transfer. 1 Introduction Text genre or register (Biber, 1988), such as discussion forum, news article or poem, is one of the most important predictors of linguistic variation (Biber, 2012). Thus, register affects crucially also the automatic processing of language (Mahajan et al., 2015; Webber, 2009; Van der Wees et al., 2018). Yet, despite its importance, register information is not available in web-crawled datasets that are widely used e.g. for pre-training language models in modern NLP. This is a challenge, as better structured language resources would also enable more detailed understanding and more sophisticated use of this data. While web register identification would allow better realization of the potential offered by web† A challenge in modeling web registers is that web documents drawn from the unrestricted web do not always fit discrete classes but could rather b"
2021.eacl-srw.24,2020.semeval-1.296,0,0.0338185,"hitecture of BERT. The authors argue that XLM and mBERT are undertuned and that the improved and prolonged training procedure of RoBERTa in combination with more data – on average two orders of magnitude more for low-resource languages – is key to improving cross-lingual performance. XLM-R is trained on 2.5TB of filtered Common Crawl (Wenzek et al., 2020) data comprising of monolingual texts in 100 languages. It is claimed to be the first multilingual model to outperform monolingual models, as well as Multilingual BERT in a number of experiments (Conneau et al., 2020; Libovick´y et al., 2020; Tanase et al., 2020). 2 The code is available at: https://github.com/ TurkuNLP/Multilingual-register-corpora 185 We also apply a CNN (Convolutional Neural Network) based architecture following Kim (2014), as our baseline model. We modify the cross-lingual CNN used by Laippala et al. (2019) to a multilabel setting. We use the multilingual word vectors introduced by Conneau et al. (2018). The CNN employs a convolution layer with ReLU activation, Model TrainTest CNN Fi mBERT Fi XLM-R large Fi FinBERT Fi CNN Fr mBERT Fr XLM-R large Fr FlauBERT large Fr CNN Sv mBERT Sv XLM-R large Sv Sv KB-BERT CNN En mBERT En XLM-R l"
2021.eacl-srw.24,L18-1604,0,0.179904,"Missing"
2021.eacl-srw.24,2020.lrec-1.494,0,0.0924161,"Missing"
2021.eacl-srw.24,2020.emnlp-demos.6,0,0.0443066,"ning and evaluation on Finnish, French, Swedish, and English (as reference), and 2) a zero-shot crosslingual setup with training on English and evaluation on the other languages. BERT, Bidirectional Encoder Representations from Transformers (Devlin et al., 2019) is a stateof-the-art deep bidirectional language model pretrained on large unlabelled corpora. BERT’s architecture is a multi-layer Transformer encoder that is based on the original Transformer architecture introduced by Vaswani et al. (2017). We use cased BERT models (TensorFlow versions) through the Huggingface Transformers library (Wolf et al., 2020) with the following language-specific models: the original English BERT, Finnish FinBERT (Virtanen et al., 2019), French FlauBERT (Le et al., 2020) and Swedish KB-BERT (Malmsten et al., 2020). Additionally, we use Multilingual BERT (mBERT) (Devlin et al., 2019), which was pretrained on monolingual Wikipedia corpora from 104 languages with a shared multilingual vocabulary. XLM-RoBERTa (XLM-R, Conneau et al. (2020)) is a multilingual language model that follows the Cross-lingual Language Modeling (XLM) approach (Conneau and Lample, 2019) and is based on the RoBERTa model (Liu et al., 2019), whic"
2021.motra-1.11,N19-1131,0,0.0528733,"Missing"
2021.nodalida-main.1,W16-2501,1,0.885859,"Missing"
2021.nodalida-main.1,2020.acl-main.747,0,0.122644,"Missing"
2021.nodalida-main.1,N19-1423,0,0.0391996,"er open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with transformer-based models in general and BERT in particular"
2021.nodalida-main.1,2020.findings-emnlp.387,1,0.690183,"at all. We introduce a fully automated pipeline for creating languagespecific BERT models from Wikipedia data and apply this pipeline to create 42 new such models. 2 Related work Considerable recent effort by various groups has focused on introducing dedicated BERT models covering single languages or a small number of (often closely related) languages. Dedicated monolingual models include e.g. BERTje1 (de Vries et al., 2019) for Dutch, CamemBERT2 (Martin et al., 2020) for French, FinBERT3 (Virtanen et al., 2019) for Finnish, RuBERT4 (Kuratov and Arkhipov, 2019) for Russian, and Romanian BERT (Dumitrescu et al., 2020); more focused multilingual models include e.g. the bilingual Finnish-English model of Chang et al. (2020) and the trilingual Finnish-Estonian-English and Croatian-Slovenian-English models of Ulˇcar and ˇ Robnik-Sikonja (2020). Many of these studies have demonstrated the newly introduced models to allow for substantial improvements over mBERT in various languagespecific downstream task evaluations, thus supporting the continued value of creating monolingual and focused multilingual models. However, these efforts still cover only a fairly limited number of languages, and do not offer a straight"
2021.nodalida-main.1,P18-1031,0,0.0201775,", but decreases for others. All of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been pa"
2021.nodalida-main.1,D19-1279,0,0.0571478,"dent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with transformer-based models in general and BERT in particular fuelling a broad range of advances and serving as the basis of many recent studies of neural language models (e.g. Lan et al., 2019; Liu et al., 2019; Sanh et al., 2019). As is the case for most studies on new deep neural language models, the original study introducing BERT addressed only English. The authors later released a Chinese model as well as a multilingual model, mBERT, trained on t"
2021.nodalida-main.1,D18-2012,0,0.0739757,"Missing"
2021.nodalida-main.1,2020.lrec-1.497,1,0.89205,"Missing"
2021.nodalida-main.1,D14-1162,0,0.0887301,"tantially improved performance for some languages, but decreases for others. All of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT lan"
2021.nodalida-main.1,N18-1202,0,0.0507358,"of the resources introduced in this work are available under open licenses from https://github.com/ turkunlp/wikibert . 1 Introduction Transfer learning using language models pretrained on large unannotated corpora has allowed for substantial recent advances at a broad range of natural language processing (NLP) tasks. By contrast to earlier distributional semantics approaches such as random indexing (Kanerva et al., 2000) and context-independent neural approaches such as word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014), models such as ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019) create contextualized representations of meaning, capable of providing both contextualized word embeddings as well as embeddings for text segments longer than words. Recent pre-trained neural language models have been rapidly advancing the state of the art in a range of natural language understanding and NLP tasks (Wang et al., 2018, 2019; Strakov´a et al., 2019; Kondratyuk and Straka, 2019). The transformer architecture (Vaswani et al., 2017) and the BERT language model of Devlin et al. (2019) have been particularly influential, with"
2021.nodalida-main.1,P19-1493,0,0.0379076,"Missing"
2021.nodalida-main.1,2021.ccl-1.108,0,0.0615209,"Missing"
2021.nodalida-main.1,L16-1680,0,0.0605072,"Missing"
2021.nodalida-main.1,P19-1527,0,0.027327,"Missing"
2021.nodalida-main.1,L16-1262,1,0.844251,"Missing"
2021.nodalida-main.1,W18-5446,0,0.0671215,"Missing"
2021.nodalida-main.14,P19-4007,0,0.0332717,"Missing"
2021.nodalida-main.14,N19-1423,0,0.127688,"under an open license from https://github.com/TurkuNLP/turku-one A degree of language independence has long been a central goal in NER research. One notable example are the CoNLL shared tasks on Language-Independent Named Entity Recognition in 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The Spanish, Dutch, English and German datasets introduced in these shared tasks were all annotated for the same types of entity mentions – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 lang"
2021.nodalida-main.14,N06-2015,0,0.153575,"ted on the basis of the Universal Dependencies (Nivre et al., 2016) representation of the manually annotated Turku Dependency Treebank (TDT) (Haverinen et al., 2014; Pyysalo et al., 2015), a multi-domain corpus spanning ten different genres. The Turku NER annotation follows the types and annotation guidelines of the FiNER corpus. An evaluation by Luoma et al. (2020) demonstrated the compatibility of the two Finnish NER corpora by showing that models trained on the simple concatenation of the two corpora outperformed ones trained on either resource in isolation. 2.3 OntoNotes corpus OntoNotes (Hovy et al., 2006; Weischedel et al., 2013) is a large, multilingual (English, Chinese, and Arabic), multi-genre corpus annotated with several layers covering text structure as well as shallow semantics. In this work, we focus exclusively on the OntoNotes English language NER annotation and refer to this part of the data simply as OntoNotes for brevity. Specifically, we use the NER annotations of the OntoNotes v5.0 release (Weischedel et al., 2013), cast into CoNLL-like format by Pradhan et al. (2013).2 Sections of the corpus lacking NER annotation (such as the Old and New Testament texts) are excluded. The On"
2021.nodalida-main.14,K18-2013,1,0.795596,"nslated OntoNotes English terms tagged with the relevant types.4 Numeric types To annotate OntoNotes numeric types (CARDINAL, ORDINAL, etc.) in the Turku NER corpus section of the data, we mapped the manual part-of-speech and feature annotation of the source corpus (TDT) to initial annotations that were then manually revised to identify the more specific types such as PERCENT, QUANTITY and MONEY based on context. For the FiNER texts, annotation for these types followed a similar process with the exception that automatic part-of-speech and feature annotation created by the Turku neural parser (Kanerva et al., 2018) was used as a starting point as no manual syntactic annotation was available for the texts. Fine-grained tokenization The FiNER annotation guidelines specify that annotated name men4 The accuracy of this initial dictionary-based tagging step was not evaluated separately. Language Finnish Finnish Finnish English English English Model FinBERT mBERT biBERT BERT mBERT biBERT Train data Finnish Combined (Fi+En) Combined (Fi+En) English Combined (Fi+En) Combined (Fi+En) Development data Finnish Finnish Finnish English English English Test data Finnish Finnish Finnish English English English Table 4"
2021.nodalida-main.14,D19-1279,0,0.0207932,"ons – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lin"
2021.nodalida-main.14,2020.acl-main.519,0,0.0904599,"Missing"
2021.nodalida-main.14,2020.lrec-1.567,1,0.859839,"Missing"
2021.nodalida-main.14,2020.coling-main.78,1,0.85986,"ised and corrected in a full, manual annotation pass. All manual revisions of the data were performed by a single annotator familiar with the corpora as well as the FiNER and OntoNotes guidelines. While the single-annotator setting regrettably precludes us from reporting inter-annotator agreement, our monolingual and cross-lingual results below suggest that the consistency of the annotation has not decreased from that of the source corpora. 4 Methods We next present the applied NER method and detail the experimental setup. 4.1 NER method We use the BERT-based named entity tagger introduced by Luoma and Pyysalo (2020). In brief, the method is based on adding a simple timedistributed dense layer on top of BERT to predict IOB2 named entity tags in a locally greedy manner. The model is both trained and applied with examples consisting of sentences catenated with their context sentences, resulting in multiple predictions for each token (appearing in both “focus” and context sentences). These predictions are then summarized using majority voting. For brevity, we refer to Luoma and Pyysalo (2020) for further details.7 Here, we do not use the wrapping of data in documentwise manner as in (Luoma and Pyysalo, 2020)"
2021.nodalida-main.14,P19-1493,0,0.0194198,"ned on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lingual transfer ability of mBERT also spikes interest in the study of multilingual representations, both on mBERT (Pires et al., 2019; K et al., 2020), and on multilingual encoders in general (Ravishankar et al., 2019; Zhao et al., 2020; Choenni and Shutova, 2020). Corpus OntoNotes FiNER Turku NER Language English Finnish Finnish Tokens 2.0M 290K 200K Entities 162K 29K 11K Domain(s) News, magazines, conversation Technology news, Wikipedia News, magazines, blogs, Wikipedia, speech, fiction, etc. Table 1: Corpus features and statistics. OntoNotes token count only includes sections of the corpus annotated for name mentions. Entity counts include also non-name types such as DATE. In this paper, we aim to assess and realize the"
2021.nodalida-main.14,W13-3516,0,0.0425749,"Missing"
2021.nodalida-main.14,W15-1821,1,0.852822,"Missing"
2021.nodalida-main.14,W19-6205,0,0.0862737,"Missing"
2021.nodalida-main.14,E12-2021,1,0.796146,"Missing"
2021.nodalida-main.14,W02-2024,0,0.03001,"tated for mentions of entity names of interest. While extensive corpora with fine-grained NER annotation have long been available for high-resource languages such as English, NER for many lesser-resourced languages has been limited by smaller, lower-coverage corpora with comparatively coarse annotation. 1 The corpus is available under an open license from https://github.com/TurkuNLP/turku-one A degree of language independence has long been a central goal in NER research. One notable example are the CoNLL shared tasks on Language-Independent Named Entity Recognition in 2002 and 2003 (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The Spanish, Dutch, English and German datasets introduced in these shared tasks were all annotated for the same types of entity mentions – persons, organizations, locations, and miscellaneous – and the datasets still remain key benchmarks for evaluating NER methods today (e.g. (Devlin et al., 2019)). Nevertheless, until recently most NER methods aimed for language independence only in that they supported training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made"
2021.nodalida-main.14,D19-1077,0,0.0151595,"orted training on corpora of more than one language, resulting in multiple separate monolingual models. In recent years, advances in deep learning have made it possible to create multilingual language models that achieve competitive levels of performance when trained and applied on texts representing more than one language (e.g. Kondratyuk and Straka (2019)). One notable model is the multilingual version of the influential BERT model (Devlin et al., 2019), mBERT, trained on more than 100 languages. mBERT performs well on zero-shot cross-lingual transfer experiments, including NER experiments (Wu and Dredze, 2019). Moon et al. (2019) propose an mBERT-based model trained simultaneously on multiple languages. Training and validating on the OntoNotes v5.0 corpus (see Section 2.3) and the CoNLL datasets, they show that multilingual models outperform models trained on one single language and have cross-lingual zero-shot ability. The zeroshot cross-lingual transfer ability of mBERT also spikes interest in the study of multilingual representations, both on mBERT (Pires et al., 2019; K et al., 2020), and on multilingual encoders in general (Ravishankar et al., 2019; Zhao et al., 2020; Choenni and Shutova, 2020"
C10-1088,W04-3224,0,0.0147143,"PAS Figure 5: Format conversion dependencies in six parsers. Formats adopted for the evaluation are shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument str"
C10-1088,W09-1402,0,0.127233,"Missing"
C10-1088,de-marneffe-etal-2006-generating,0,0.160065,"Missing"
C10-1088,W07-2416,0,0.039232,"format containing predicate argument structures along with a phrase structure tree in Enju format, which can be converted into PTB format (Miyao et al., 2009). For direct comparison and for the study of contribution of the formats in which the six parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 5. The Enju PAS output is converted into PTB using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006), and CoNLL generated from PTB by using Treebank Converter (Johansson and Nugues, 2007). With the exception of GDep, all CoNLL outputs are generated by the conversion and thus share dependency types. We note that all of these conversions can introduce some errors in the conversion process. 781 4 Evaluation Setting 4.1 Event Extraction Evaluation Event extraction performance is evaluated using the evaluation script provided by the BioNLP’09 shared task organizers for the development data set, and the online evaluation system of the task for the test data set2 . Results are reported under the official evaluation criterion of the task, i.e. the “Approximate Span Matching/Approximat"
C10-1088,W09-1401,1,0.485832,"rresponding to locations and sites considered in Task 2. Theme phosphorylation Theme TRAF2 2.1 Binding binding Theme Theme TRAF2 CD40 Figure 1: Event Extraction. sults shows that performance against gold standard annotations is not always correlated with event extraction performance. We further find that the dependency types and overall structures employed by the different dependency representations have specific advantages and disadvantages for the event extraction task. 2 Bio-molecular Event Extraction In this study, we adopt the event extraction task defined in the BioNLP 2009 Shared Task (Kim et al., 2009) as a model information extraction task. Figure 1 shows an example illustrating the task of event extraction from a sentence. The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task defined five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument, a multiparticipant binding event (Binding), and three regulation events (Regulation, Positive regulation, and Negative regulation) used to capture both biological regulation and general"
C10-1088,P03-1054,0,0.00137982,"ar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL) (Figure 3) and the predicateargument structure (PAS) format used by Enju (Figure 4). With the exception of Stanford and Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. The six parsers operate in"
C10-1088,W10-1905,1,0.178147,"vents as arguments, creating complex event structures. We consider two subtasks, Task 1 and Task 2, out of the three defined in the shared task. Task 1 focuses on core event extraction, and Task 2 involves augmenting extracted events with secondary arguments (Kim et al., 2009). Events are represented with a textual trigger, type, and arguments, where the trigger is a span of text that states the event in text. In Task 1 the event arguments that need to be extracted are restricted to the core Theme and Cause roles, with secondary arEvent Extraction System For evaluation, we apply the system of Miwa et al. (2010b). The system was originally developed for finding core events (Task 1) using the native output of the Enju and GDep parsers. The system consists of three supervised classification-based modules: a trigger detector, an event edge detector, and a complex event detector. The trigger detector classifies each word into the appropriate event types, the event edge detector classifies each edge between an event and a candidate participant into an argument type, and the complex event detector classifies event candidates constructed by all edge combinations, deciding between event and non-event. The s"
C10-1088,W07-1004,1,0.839951,"e results on the test data set. Results on simple, binding, regulation, and all events are shown. GDep and Enju with PAS are used. Results by Miwa et al. (2010b), Bj¨orne et al. (2009), Riedel et al. (2009), and Baseline for Task 1 and Task 2 are shown for comparison. Baseline results are produced by removing dependency information from the parse results of GDep and Enju. The best score in each result is shown in bold. tem. 6 Related Work Many approaches for parser comparison have been proposed, and most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Parser comparison has also been proposed on specific tasks such as unbounded dependencies (Rimell ¨ et al., 2009) and textual entailment (Onder Eker, 7 2009) . Among them, application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the 785 7 http://pete.yuret.com/ recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those"
C10-1088,W09-1406,1,0.675638,"the event extraction (shown with the gold treebank data in Table 3), but the types and relations of CoNLL were well predicted, and MC and Enju performed better for CoNLL than for SD in total. 5.5 Performance of Event Extraction System Several systems are compared by the extraction performance on the shared task test data in Table 5. GDep and Enju with PAS are used for the evaluation, which is the same evaluation setting with the original system by Miwa et al. (2010b). The performance of the best systems in the original shared task is shown for reference ((Bj¨orne et al., 2009) in Task 1 and (Riedel et al., 2009) in Task 2). The event extraction system performs significantly better than the best systems in the shared task, further outperforming the original system. This shows that the comparison of the parsers is performed with a state-of-the-art sys784 Baseline Bikel Stanford WSJ Stanford eng GDep MC C&C Enju GENIA SD 51.05 53.29 53.51 55.02 55.60 56.09 55.48 56.34 Task 1 CoNLL 53.22 54.38 53.66 55.70 56.01 55.74 56.09 PAS 50.42 56.57 57.94 SD 49.17 51.40 52.02 53.41 53.94 54.27 54.06 55.04 Task 2 CoNLL 51.27 52.04 52.74 54.37 54.51 54.37 54.57 PAS 48.88 55.31 56.40 Table 3: Comparison of F-score res"
C10-1088,D09-1085,0,0.0439988,"Missing"
C10-1088,D07-1111,1,0.192022,"lex formed only with P and P2 NMOD VMOD PMOD SD PTB PAS Figure 5: Format conversion dependencies in six parsers. Formats adopted for the evaluation are shown in solid boxes. SD: Stanford Dependency format, CCG: Combinatory Categorial Grammar output format, PTB: Penn Treebank format, and PAS: Predicate Argument Structure in Enju format. CONJ Figure 3: CoNLL-X dependency tree noun_arg1 arg1 Conll-X CCG prep_arg12 prep_arg12 arg1 arg2 NFAT/AP-1 complex formed only with P and P2 verb_arg1 arg1 adj_arg1 coord_arg12 coord_arg12 arg1 arg1 arg2 Figure 4: Predicate Argument Structure parsers are GDep (Sagae and Tsujii, 2007), the Bikel parser (Bikel) (Bikel, 2004), the Stanford parser with two probabilistic context-free grammar (PCFG) models1 (Wall Street Journal (WSJ) model (Stanford WSJ) and “augmented English” model (Stanford eng)) (Klein and Manning, 2003), the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009), the C&C CCG parser, adapted to biomedical text (C&C) (Rimell and Clark, 2009), and the Enju parser with the GENIA model (Miyao et al., 2009). The formats are Stanford Dependencies (SD) (Figure 2), the CoNLL-X dependency format (CoNLL)"
C10-1088,I05-2038,1,0.573522,"ax have been successfully applied to a number of tasks in BioNLP. Several parsers and representations have been applied in high-performing methods both in domain studies in general and in the BioNLP’09 shared task in particular, but no direct comparison of parsers or representations has been performed. Likewise, a number of evaluation of parser outputs against gold standard corpora have been performed in the domain, but the broader implications of the results of such intrinsic evaluations are rarely considered. The BioNLP’09 shared task involved documents contained also in the GENIA treebank (Tateisi et al., 2005), creating an opportunity for direct study of intrinsic and task-oriented evaluation results. As the treebank can be converted into various dependency formats using existing format conversion methods, evaluation can further be extended to cover the effects of different representations. 1 Introduction Advanced syntactic parsing methods have been shown to effective for many information extraction tasks. The BioNLP 2009 Shared Task, a recent bio-molecular event extraction task, is one such task: analysis showed that the application of a parser correlated with high rank in the task (Kim In this th"
C16-1030,D15-1041,0,0.0206421,"ch into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, wh"
C16-1030,W16-1603,1,0.0607575,"atasets. Character-level models have the potential of capturing morpheme patterns, thereby improving generalisation on both frequent and unseen words. In recent years, there has been an increase in research into these models, resulting in several interesting applications. Ling et al. (2015b) described a character-level neural model for machine translation, performing both encoding and decoding on individual characters. Kim et al. (2016) implemented a language model where encoding is performed by a convolutional network and LSTM over characters, whereas predictions are given on the word-level. Cao and Rei (2016) proposed a method for learning both word embeddings and morphological segmentation with a bidirectional recurrent network over characters. There is also research on performing parsing (Ballesteros et al., 2015) and text classification (Zhang et al., 2015) with character-level neural models. Ling et al. (2015a) proposed a neural architecture that replaces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from"
C16-1030,D14-1080,0,0.085782,"by useful for a wide range of downstream applications. Work in this area has traditionally involved task-specific feature engineering – for example, integrating gazetteers for named entity recognition, or using features from a morphological analyser in POS-tagging. Recent developments in neural architectures and representation learning have opened the door to models that can discover useful features automatically from the data. Such sequence labeling systems are applicable to many tasks, using only the surface text as input, yet are able to achieve competitive results (Collobert et al., 2011; Irsoy and Cardie, 2014). Current neural models generally make use of word embeddings, which allow them to learn similar representations for semantically or functionally similar words. While this is an important improvement over count-based models, they still have weaknesses that should be addressed. The most obvious problem arises when dealing with out-of-vocabulary (OOV) words – if a token has never been seen before, then it does not have an embedding and the model needs to back-off to a generic OOV representation. Words that have been seen very infrequently have embeddings, but they will likely have low quality du"
C16-1030,W04-1213,0,0.127232,".54 87.98 84.21 87.75 87.99 78.63 82.80 83.75 79.74 83.56 84.53 75.46 76.82 77.38 70.75 72.24 72.70 97.55 98.59 98.67 97.39 98.49 98.60 Table 2: Comparison of word-based and character-based sequence labeling architectures on 8 datasets. The evaluation measure used for each dataset is specified in Section 6. • CHEMDNER: The BioCreative IV Chemical and Drug (Krallinger et al., 2015) NER corpus consists of 10,000 abstracts annotated for mentions of chemical and drug names using a single class. We make use of the official splits provided by the shared task organizers. • JNLPBA: The JNLPBA corpus (Kim et al., 2004) consists of 2,404 biomedical abstracts and is annotated for mentions of five entity types: CELL LINE, CELL TYPE, DNA, RNA, and PROTEIN. The corpus was derived from GENIA corpus entity annotations for use in the shared task organized in conjuction with the BioNLP 2004 workshop. • GENIA-POS: The GENIA corpus (Ohta et al., 2002) is one of the most widely used resources for biomedical NLP and has a rich set of annotations including parts of speech, phrase structure syntax, entity mentions, and events. Here, we make use of the GENIA POS annotations, which cover 2,000 PubMed abstracts (approx. 20,0"
C16-1030,N16-1030,0,0.275407,"ber 11-17 2016. We evaluate the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters. 2 Bidirectional LSTM for sequence labeling We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information. Figure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens (w1 , ..., wT ) as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings (x1 , ..., xT ). Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating"
C16-1030,D15-1176,0,0.0909934,"Missing"
C16-1030,J93-2004,0,0.0531267,": The CoNLL-2000 dataset (Tjong Kim Sang and Buchholz, 2000) is a frequently used benchmark for the task of chunking. Wall Street Journal Sections 15-18 from the Penn Treebank are used for training, and Section 20 as the test data. As there is no official development set, we separated some of the training set for this purpose. • CoNLL03: The CoNLL-2003 corpus (Tjong Kim Sang and De Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags. The PTB label set includes 36 main tags and an additional 12 tags covering items such as punctuation. • FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011). We use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context. • BC2GM: The BioCreative II Gene Mention corpus (S"
C16-1030,D16-1209,0,0.0423358,"laces word embeddings with dynamically-constructed characterbased representations. We applied a similar method for operating over characters, but combined them with word embeddings instead of replacing them, as this allows the model to benefit from both approaches. Lample et al. (2016) described a model where the character-level representation is combined with word embeddings through concatenation. In this work, we proposed an alternative architecture, where the representations are combined using an attention mechanism, and evaluated both approaches on a range of tasks and datasets. Recently, Miyamoto and Cho (2016) have also described a related method for the task of language modelling, combining characters and word embeddings using gating. 9 Conclusion Developments in neural network research allow for model architectures that work well on a wide range of sequence labeling datasets without requiring hand-crafted data. While word-level representation learning is a powerful tool for automatically discovering useful features, these models still come with certain weaknesses – rare words have low-quality representations, previously unseen words cannot be modeled at all, and morpheme-level information is not"
C16-1030,P16-1112,1,0.425,"te the neural models on 8 datasets from the fields of NER, POS-tagging, chunking and error detection in learner texts. Our experiments show that including a character-based component in the sequence labeling model provides substantial performance improvements on all the benchmarks. In addition, the attention-based architecture achieves the best results on all evaluations, while requiring a smaller number of parameters. 2 Bidirectional LSTM for sequence labeling We first describe a basic word-level neural network for sequence labeling, following the models described by Lample et al. (2016) and Rei and Yannakoudakis (2016), and then propose two alternative methods for incorporating character-level information. Figure 1 shows the general architecture of the sequence labeling network. The model receives a sequence of tokens (w1 , ..., wT ) as input, and predicts a label corresponding to each of the input tokens. The tokens are first mapped to a distributed vector space, resulting in a sequence of word embeddings (x1 , ..., xT ). Next, the embeddings are given as input to two LSTM (Hochreiter and Schmidhuber, 1997) components moving in opposite directions through the text, creating context-specific representations"
C16-1030,W00-0726,0,0.201026,"ter-based model to shift towards predicting high-quality word embeddings, it is not desireable to optimise the word embeddings towards the character-level representations. This can be achieved by making sure that the optimisation is performed only in one direction; in Theano (Bergstra et al., 2010), the disconnected grad function gives the desired effect. 5 Datasets We evaluate the sequence labeling models and character architectures on 8 different datasets. Table 1 contains information about the number of labels and dataset sizes for each of them. • CoNLL00: The CoNLL-2000 dataset (Tjong Kim Sang and Buchholz, 2000) is a frequently used benchmark for the task of chunking. Wall Street Journal Sections 15-18 from the Penn Treebank are used for training, and Section 20 as the test data. As there is no official development set, we separated some of the training set for this purpose. • CoNLL03: The CoNLL-2003 corpus (Tjong Kim Sang and De Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Jou"
C16-1030,P11-1019,0,0.0312929,"e Meulder, 2003) was created for the shared task on language-independent NER. We use the English section of the dataset, containing news stories from the Reuters Corpus1 . • PTB-POS: The Penn Treebank POS-tag corpus (Marcus et al., 1993) contains texts from the Wall Street Journal, annotated for part-of-speech tags. The PTB label set includes 36 main tags and an additional 12 tags covering items such as punctuation. • FCEPUBLIC: The publicly released subset of the First Certificate in English (FCE) dataset contains short essays written by language learners and manual corrections by examiners (Yannakoudakis et al., 2011). We use a version of this corpus converted into a binary error detection task, where each token is labeled as being correct or incorrect in the given context. • BC2GM: The BioCreative II Gene Mention corpus (Smith et al., 2008) consists of 20,000 sentences from biomedical publication abstracts and is annotated for mentions of the names of genes, proteins and related entities using a single NE class. 1 http://about.reuters.com/researchandstandards/corpus/ 313 CoNLL00 Word-based Char concat Char attention PTB-POS FCEPUBLIC TEST DEV TEST DEV TEST DEV TEST 91.48 92.57 92.92 91.23 92.35 92.67 86.8"
C16-1030,W04-1219,0,0.0523839,"Missing"
D19-5709,W19-5006,0,0.0218193,"peech tagging, chunking and lemmatization and has been trained on English text. When applied on Spanish input, the tags and lemmas will necessarily very frequently be incorrect. We nevertheless opted to apply the system as an off-theshelf baseline as its rich feature set also includes many language-independent features. We leave the NERsuite parameters at their defaults. 3.2 In our second experiment we utilize BERT (Devlin et al., 2018), a transformer (Vaswani et al., 2017) based attentive neural architecture. Whereas pretrained BERT models have shown strong performance for English NER tasks (Peng et al., 2019), to our knowledge no pretrained Spanish BERT models are readily available4 . Thus we conduct our experiments with the multilingual BERT model (Pires et al., 2019) trained on a Wikipedia corpus, covering 104 languages. Whereas Spanish is one of the pretraining languages used for the model, the used Wikipedia corpus is not specific to clinical or biomedical content. We use the cased variant of the model, which preserves the case and accents of the characters. BERT relies on subword units, shared between all the used languages, leading to subword embeddings which can benefit from the commonaliti"
D19-5709,D19-5701,0,0.0291572,"Missing"
D19-5709,P19-1493,0,0.0410424,"Missing"
D19-5709,W09-1401,1,0.773178,"Missing"
D19-5709,W04-1213,0,0.0473256,"r English as well 2 Data The annotation involves four types of entities, labeled in the data as PROTEINAS (proteins, genes, and related entities), NORMALIZABLES (chemicals that can be normalized to external resources), NO NORMALIZABLES (chemicals that cannot), and UNCLEAR (miscellaneous related entities). In the following, we refer to these respectively as Protein, Chemical(+), Chemical(-) and Other. Table 1 briefly summarizes data statistics. We note that compared to English language biomedical NER resources, the number of annotations is somewhat limited; for example, the JNLPBA shared task (Kim et al., 2004) data contains over 50,000 training examples of similar 56 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 56–61 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Figure 1: Illustration of data formats. Left: task data in separate .txt and .ann files. Right: NERsuite format. types, the BioCreative II GM data (Smith et al., 2008) over 18,000 gene mentions, and the BioCreative CHEMDNER (Krallinger et al., 2015) data over 80,000 chemical mentions. We thus expect that methods addressing the PharmaCoNER task to benefit from pretraining or other"
D19-5709,W19-1916,0,0.0942596,"Missing"
D19-5709,N16-1030,0,0.191119,"Missing"
D19-5725,P16-1231,0,0.0143586,"system-predicted tokenization in terms of precision, recall and F1-score. 2.2.3 LAS 65.994 69.318 89.695 89.65 89.536 56.68 3 Baseline system The CRAFT Coreference Resolution Task Coreference resolution, linking strings of text that have the same referent, is a challenging NLP task that offers potential benefit to downstream tasks if done successfully. The challenge arises in linking strings of text over long distances across a document, or possibly between documents. The benefit of doing so can be substantial as coreference resolution has the ability to amplify results of upstream SyntaxNet (Andor et al., 2016), a transition-based neural network framework built using TensorFlow was used as the baseline system for the structural annotation task. The system was composed of two models of similar architecture: a part of speech (POS) tagger and a dependency parser. The Python NLTK punkt (Bird et al., 2009) sentence 176 Statistic Min IC length Max IC length Median IC length Average IC length Total IC IC per document Total mentions Discont. mentions tasks such as concept recognition, thereby potentially improving the performance of downstream tasks, e.g. information extraction, that require explicitly repr"
D19-5725,D19-1371,0,0.0288118,"issive evaluation. The augmented implementations of all metrics used in the CRAFT-CR task have been made publicly available8 . 3.3 Results One team submitted three runs for evaluation in the CRAFT-CR task (Table 3). They augmented the state-of-the-art end-to-end neural coreference resolution system of Lee et al. (2017) by incorporating extra syntactic features including grammatical number agreements between mentions, as well as semantic features using MetaMap to identify entity mentions. They also investigated the use of PubMed word vectors (Chiu et al., 2016) (Run1) and SciBERT word vectors (Beltagy et al., 2019) (Run2, Run3) as inputs to their model. As implemented, the system of Team T010 performed admirably compared to the baseline. F-scores are in line with some previous coreference systems used on CRAFT (Cohen et al., 2017), thus emphasizing the challenge of coreference resolution in general, and of coreference resolution over biomedical text in particular. While the baseline system and Run1 of the participant system produced on average shorter chains than those in the evaluation set (p<0.01, Mann-Whitney U test), Run2 and Run3 of the participant system were both able to generate distributions of"
D19-5725,W13-2024,0,0.07432,"Missing"
D19-5725,W06-2920,0,0.0249889,"automatically parse full-length biomedical journal articles of the CRAFT Corpus into dependency structures for each sentence. The CRAFT-SA task targets dependency parses as opposed to constituency parses in order to emphasize differences that directly affect the meaning of a parsed sentence; differences in constituent parse conventions can result in parse differences that do not affect the resultant meaning of a parsed sentence (Clegg and Shepherd, 2007). There have been previous shared tasks in the general domain NLP community to evaluate dependency parse construction using both the CoNLL-X (Buchholz and Marsi, 2006) and CoNLL-U (Zeman et al., 2018) file formats. Although the dependency parses initially distributed with the CRAFT corpus more closely resemble the older CoNLL-X format, the CRAFT dependency data was transformed into a quasi-CoNLLU format to allow the input provided to participants to be only the text of the documents making for a more realistic scenario compared to the CoNLL-X shared tasks which required participants to match gold standard tokenization for evaluation purposes. 2.1 Data 2.1.1 Data preparation – CoNLL-X The dependency parses distributed as part of the CRAFT corpus are automati"
D19-5725,W16-2922,1,0.784176,"as added to allow for a slightly more flexible, permissive evaluation. The augmented implementations of all metrics used in the CRAFT-CR task have been made publicly available8 . 3.3 Results One team submitted three runs for evaluation in the CRAFT-CR task (Table 3). They augmented the state-of-the-art end-to-end neural coreference resolution system of Lee et al. (2017) by incorporating extra syntactic features including grammatical number agreements between mentions, as well as semantic features using MetaMap to identify entity mentions. They also investigated the use of PubMed word vectors (Chiu et al., 2016) (Run1) and SciBERT word vectors (Beltagy et al., 2019) (Run2, Run3) as inputs to their model. As implemented, the system of Team T010 performed admirably compared to the baseline. F-scores are in line with some previous coreference systems used on CRAFT (Cohen et al., 2017), thus emphasizing the challenge of coreference resolution in general, and of coreference resolution over biomedical text in particular. While the baseline system and Run1 of the participant system produced on average shorter chains than those in the evaluation set (p<0.01, Mann-Whitney U test), Run2 and Run3 of the partici"
D19-5725,P14-2006,0,0.0238771,"e actual text, and then the coreference information was mapped onto the gold standard tokenization provided with the test data. Figure 1: Sample representation of two coreference mentions, high..IOP and low IOP. Note the use of the character a in the chain identifier (64a) to indicate a discontinuous mention for the high..IOP mention. Empty columns 7-11 have been elided for figure layout consideration. 3.2 Scoring 3.4 There are a wide range of coreference resolution scoring metrics available. For historical purposes, the five reference metrics (MUC, B3 , CEAFE, CEAFM, BLANC) of Pradhan et al. (2014) are used to score the CRAFT-CR task. Due to their apparent unreliability and their low agreement rate, the Link-based Entity-Aware (LEA) metric proposed by Moosavi and Strube (2016) is also used to measure coreference system performance. The LEA metric was designed specifically to address the shortcomings of the previously used metrics. By taking into account all coreference links and evaluating resolved coreference relations instead of resolved mentions, the LEA metric accurately assesses recall and precision. The coreference scoring implementations were modified in two ways for the CRAFT-CR"
D19-5725,W12-4501,0,0.0652097,"Missing"
D19-5725,N06-2015,0,0.153166,"Missing"
D19-5725,W18-1817,0,0.0211492,"the 30 CRAFT test documents. For Team T013, their highest scoring run is displayed based on SER. Run identifiers indicate (proper OBO/OBO EXT). Note that Run 2a is an unofficial run as it was submitted after the deadline, however since there were no other teams participating, Run 2a is included in the official results. SER = Slot Error Rate; P = Precision; R = Recall; F1 = F1-score. 181 upstream and downstream of each concept mention. The second stage links each textual mention identified by the CRF to an ontology identifier using a stacked Bi-LSTM approach implemented by the OpenNMT system (Klein et al., 2018). By modeling concept normalization as sequence-to-sequence translation at the character level, the baseline system maps characters in the text spans identified in the first stage to characters in ontology identifiers to normalize concepts. 4.4 clusion of the extension classes generally resulted in improvement of performance when compared to runs using only the proper ontology concepts, possibly attributable to the labels and synonyms that were provided for the extension classes. One exception is for GO MF EXT where performance is expected to suffer with inclusion of the extension class annota"
D19-5725,D17-1018,0,0.0178762,"implementations were augmented to take as input the modified CoNLL-Coref 2011/2012 file format. Second, the implementations were updated to allow overlapping mentions to match instead of enforcing strict mention boundary matching. This option was added to allow for a slightly more flexible, permissive evaluation. The augmented implementations of all metrics used in the CRAFT-CR task have been made publicly available8 . 3.3 Results One team submitted three runs for evaluation in the CRAFT-CR task (Table 3). They augmented the state-of-the-art end-to-end neural coreference resolution system of Lee et al. (2017) by incorporating extra syntactic features including grammatical number agreements between mentions, as well as semantic features using MetaMap to identify entity mentions. They also investigated the use of PubMed word vectors (Chiu et al., 2016) (Run1) and SciBERT word vectors (Beltagy et al., 2019) (Run2, Run3) as inputs to their model. As implemented, the system of Team T010 performed admirably compared to the baseline. F-scores are in line with some previous coreference systems used on CRAFT (Cohen et al., 2017), thus emphasizing the challenge of coreference resolution in general, and of c"
D19-5725,P16-1060,0,0.018054,"two coreference mentions, high..IOP and low IOP. Note the use of the character a in the chain identifier (64a) to indicate a discontinuous mention for the high..IOP mention. Empty columns 7-11 have been elided for figure layout consideration. 3.2 Scoring 3.4 There are a wide range of coreference resolution scoring metrics available. For historical purposes, the five reference metrics (MUC, B3 , CEAFE, CEAFM, BLANC) of Pradhan et al. (2014) are used to score the CRAFT-CR task. Due to their apparent unreliability and their low agreement rate, the Link-based Entity-Aware (LEA) metric proposed by Moosavi and Strube (2016) is also used to measure coreference system performance. The LEA metric was designed specifically to address the shortcomings of the previously used metrics. By taking into account all coreference links and evaluating resolved coreference relations instead of resolved mentions, the LEA metric accurately assesses recall and precision. The coreference scoring implementations were modified in two ways for the CRAFT-CR task. First, because the CRAFT-CR data allows for mentions with discontinuous spans, the implementations were augmented to take as input the modified CoNLL-Coref 2011/2012 file form"
D19-5725,K18-2001,0,0.0402519,"Missing"
D19-5728,W09-1402,1,0.83615,"Missing"
D19-5728,K18-2013,1,0.891217,"of the input sentences. CRAFT SA also adopts the format and evaluation tools of the CoNLL tasks, and its representation matches the universal representation of these tasks in part. The CRAFT task is differentiated from the many corpus resources applied in the CoNLL tasks specifically in focusing on biomedical domain texts, and CRAFT is unique among syntactically annotated biomedical corpora in that its texts are drawn from full-text articles, rather than only article titles and abstracts. We participated in the CRAFT SA task using an approach that builds primarily on the Turku neural parser (Kanerva et al., 2018), a native dependency parsing system that previously ranked among the best systems in the CoNLL 2018 task. As the parser is fully retrainable, designed to accept the format used for the CRAFT data, and agnostic to the details of the representation, it was possible to train it for the CRAFT task with little modification. Additionally, as the parser has not been deWe present the approach taken by the TurkuNLP group in the CRAFT Structural Annotation task, a shared task on dependency parsing. Our approach builds primarily on the Turku neural parser, a native dependency parser that ranked among th"
D19-5728,Q17-1010,0,0.0120724,"er was ranked second on LAS and MLAS, and first on BLEX on the CoNLL-2018 Shared Task, making it highly competitive. 4.2 of which produce a lemma by removing and possibly substituting characters from the word prefix and suffix. As in tagging, an averaged perceptron then disambiguates among the candidates. The dependency parser is a transition-based parser with a feed-forward neural network serving as the classifier that decides on the next transition taken by the parser. 4.3 For inducing new sets of word vectors, we used the word2vec6 (Mikolov et al., 2013) and FastText7 (Joulin et al., 2016; Bojanowski et al., 2017) tools. In brief, these tools generate a vector representation for each token based on the similarity of the contexts in which they appear in a large corpus of unannotated text. Word vectors were induced on texts extracted from PubMed abstracts and PMC Open Access publications (Section 3.3) using both the skip-gram and continuous bagof-words (CBOW) models implemented in both tools. Model parameters were primarily kept at their default values, but we performed a series of experiments with different values of the window parameter, which has been found to be particularly impactful in previous wor"
D19-5728,W04-3111,0,0.188427,"Missing"
D19-5728,W06-2920,0,0.0137916,"h Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsing and its applications. The CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) addressed multilingual dependency parsing, and while data was provided for different languages in the same format, the underlying representation (e.g. dependency types) was not standardized in these tasks. These tasks also included only prediction of syntactic trees, whereas tokenization and part-of-speech tags were given for the participants. In recent years, there has been an increased interest in native dependency parsing, reflected in efforts such as Universal Dependencies (UD) (Nivre et al., 2016) and the CoNLL 2017 and 2018 shared tasks on multilingual parsing using"
D19-5728,P05-1022,0,0.019375,"November 4, 2019. 2019 Association for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initialization with in-domain word vectors. 2 Documents Sentences Tokens Train 67 21 731 561 032 Test 30 9 099 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks avai"
D19-5728,W16-2922,1,0.924707,"ddings by Ginter et al. (2017) trained on general English extracted from Wikipedia and Internet crawls. These embeddings are trained using the word2vec (Mikolov et al., 2013) tool with lowercased data, skip-gram algorithm, window size of 10 and 100 dimensions. The vectors were originally provided for the CoNLL 2017 and 2018 multilingual parsing shared task, and thus used by many of the participating systems in their English parsing models. We also considered a number of word vectors induced specifically on biomedical text for domain tasks, including those created by Pyysalo et al. (2013)1 and Chiu et al. (2016)2 . 4.1 Turku Parser Our primary parser used in all experiments is the Turku Neural Parser Pipeline4 (Kanerva et al., 2018), a full parser pipeline meant for end-to-end analysis from raw text into UD. The pipeline includes sentence and word segmentation, part-ofspeech and morphological tagging, syntactic parsing, and lemmatization. The segmentation component in the Turku pipeline is built using UDPipe (Straka and Strakov´a, 2017), where the token and sentence boundaries are jointly predicted using a singlelayer bidirectional GRU network. Universal (UPOS) and language-specific (XPOS) part-ofspe"
D19-5728,P14-5010,0,0.00250899,"available in this particular hybrid SD/UD CoNLL-U representation. We expand on this issue below in Section 6.2. 3.3 Unlabelled data 3.2 4 Methods To induce new word vectors (Section 4.3) and conduct co-training experiments (Section 5.2), we used unlabelled texts from PubMed titles and abstracts and PubMed Central (PMC) full texts. The data was drawn from the PubMed 2017 baseline distribution and a 2017 download of the PMC Open Access subset.3 The texts were segmented into sentences using the GENIA sentence splitter and then tokenized using the PTBTokenizer included in Stanford CoreNLP tools (Manning et al., 2014) and the tokenized sentences shuffled randomly. The resulting dataset consists of 12.5 billion tokens in 500 million sentences. As the text of the full-text articles of the CRAFT corpus contains characters outside of the basic ASCII character set, we created word vectors on the original extracted texts instead of first applying a mapping to ASCII characters as was done in a number of similar previous efforts (e.g. (Pyysalo et al., 2013)). Word vectors We considered a number of previously released word vectors for initializing the parser. As a baseline we use the English word embeddings by Gint"
D19-5728,W08-1301,0,0.0773769,"Missing"
D19-5728,P08-2026,0,0.0465163,"iation for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initialization with in-domain word vectors. 2 Documents Sentences Tokens Train 67 21 731 561 032 Test 30 9 099 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers"
D19-5728,K17-3002,0,0.0189026,"The segmentation component in the Turku pipeline is built using UDPipe (Straka and Strakov´a, 2017), where the token and sentence boundaries are jointly predicted using a singlelayer bidirectional GRU network. Universal (UPOS) and language-specific (XPOS) part-ofspeech tags, as well as morphological features 3 We used 2017 data as we had a plain text version readily available from previous work. 4 https://turkunlp.org/ Turku-neural-parser-pipeline/ 1 http://bio.nlplab.org/ 2 https://github.com/cambridgeltl/ BioNLP-2016 208 (FEATS) are predicted with a modified version of the one published by Dozat et al. (2017), a time-distributed classifier over tokens in a sentence embedded using bidirectional LSTM network. The tagger has two separate classification layers, one for universal part-of-speech and one originally used for language-specific part-ofspeech tags. The bidirectional encoding is shared between both classifiers. In the modified version (Kanerva et al., 2018), the second classifier is used to jointly predict the language-specific POS tags together with morphological features by simply concatenating the two input columns into one. The syntactic analysis is based on a graph-based parser by Dozat"
D19-5728,N06-1020,0,0.048667,"Missing"
D19-5728,W10-1905,1,0.795518,"st work focusing on the analysis of English news texts (Marcus et al., 1994). Syntactic analyses are required also by many methods for the analysis of biomedical text; for example, information extraction methods commonly rely on the shortest path over syntactic dependencies to identify how entities mentioned in text are related (Airola et al., 2008; Bj¨orne et al., 2009; Liu et al., 2013; Luo et al., 2016). The performance of parsers is known to be domaindependent: to create high-quality analyses of e.g. biomedical texts, the tools should be trained on annotated corpora reflecting the domain (Miwa et al., 2010). Syntactically annotated corpora of domain texts are thus required for much of biomedical NLP. These resources should also preferably follow the relevant standards in the representation of 206 Proceedings of the 5th Workshop on BioNLP Open Shared Tasks, pages 206–215 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics veloped or previously applied to biomedical English, we consider a number of modifications and adaptions to improve on its performance, finding in particular that the strong baseline performance of the parser can be further improved through initi"
D19-5728,K18-2001,1,0.896613,"Missing"
D19-5728,P08-1006,0,0.0454783,"9 232 619 Table 1: CRAFT Structural Annotation statistics Background Documents Sentences Tokens Biomedical domain models have been available for a number of constituency parsers (e.g. Charniak and Johnson (2005), McClosky and Charniak (2008)) and have been widely applied in domain information extraction efforts, frequently in conjunction with heuristic conversions into dependency representations such as Stanford dependencies (De Marneffe and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsin"
D19-5728,L16-1262,1,0.785909,"Missing"
D19-5728,W07-1004,1,0.640338,"e and Manning, 2008). There have also been native dependency parsers available for the domain, such as Pro3Gres (Schneider and Rinaldi, 2004) and, later, GDep (Miyao et al., 2008), nevertheless the abovementioned McClosky-Charniak parser with Stanford dependencies conversion was the workhorse of biomedical dependency parsing for nearly a decade. Also the treebanks available for training the parsers in the biomedical domain have traditionally been constituency-based, for instance the Penn BioIE (Kulick et al., 2004) and especially the GENIA treebank (Tateisi et al., 2005). The BioInfer corpus (Pyysalo et al., 2007) was the first domain corpus to adopt Stanford Dependencies as the native annotation scheme, coinciding with a generally growing interest in dependency parsing and its applications. The CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) addressed multilingual dependency parsing, and while data was provided for different languages in the same format, the underlying representation (e.g. dependency types) was not standardized in these tasks. These tasks also included only prediction of syntactic trees, whereas tokenization and part-of-speech tags were given for the pa"
D19-5728,W04-1505,0,0.0292052,"Missing"
D19-5728,K17-3009,0,0.0541379,"Missing"
E12-2021,W05-0620,0,0.0614673,"Missing"
E12-2021,doddington-etal-2004-automatic,0,0.0628567,"an be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). F"
E12-2021,J02-3001,0,0.0249721,", allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and mult"
E12-2021,W11-1801,1,0.354037,"Missing"
E12-2021,W03-3017,0,0.0113928,"dheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003) (See Figure 1 for examples). Further, both the BRAT client and server implement full support for the Unicode standard, which allow the tool to support the annotation of text using e.g. Chinese or Devan¯agar¯ı characters. BRAT is distributed with examples from over 20 corpora for a variety of tasks, involving texts in seven different languages and including examples from corpora such as those introduced for the CoNLL shared tasks on language-independent named entity recognition (Tjong Kim Sang and De Meulder, 2003) and multilingual dependency parsing (Buchholz and Marsi, 2006). BRAT also imple"
E12-2021,N06-4006,0,0.00738235,"the semantic class disambiguation component (Stenetorp et al., 2011a). Although further research is needed to establish the benefits of this approach in various annotation tasks, we view the results of this initial experiment as promising regarding the potential of our approach to using machine learning to support annotation efforts. 5 informed by experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepa"
E12-2021,W11-1816,1,0.207231,"Missing"
E12-2021,X96-1048,0,0.137156,"lly configurable and can be set up to support most text annotation tasks. The most basic annotation primitive identifies a text span and assigns it a type (or tag or label), marking for e.g. POS-tagged tokens, chunks or entity mentions (Figure 1 top). These base annotations can be connected by binary relations – either directed or undirected – which can be configured for e.g. simple relation extraction, or verb frame annotation BRAT (Figure 1 middle and bottom). n-ary associations of annotations are also supported, allowing the annotation of event structures such as those targeted in the MUC (Sundheim, 1996), ACE (Doddington et al., 2004), and BioNLP (Kim et al., 2011) Information Extraction (IE) tasks (Figure 2). Additional aspects of annotations can be marked using attributes, binary or multi-valued flags that can be added to other annotations. Finally, annotators can attach free-form text notes to any annotation. In addition to information extraction tasks, these annotation primitives allow BRAT to be configured for use in various other tasks, such as chunking (Abney, 1991), Semantic Role Labeling (Gildea and Jurafsky, 2002; Carreras and M`arquez, 2005), and dependency annotation (Nivre, 2003)"
E12-2021,W08-0605,1,0.44524,"experience from several annotation tasks and research efforts spanning more than a decade. A variety of previously introduced annotation tools and approaches also served to guide our design decisions, including the fast annotation mode of Knowtator (Ogren, 2006), the search capabilities of the XConc tool (Kim et al., 2008), and the design of web-based systems such as MyMiner (Salgado et al., 2010), and GATE Teamware (Cunningham et al., 2011). Using machine learning to accelerate annotation by supporting human judgements is well documented in the literature for tasks such as entity annotation (Tsuruoka et al., 2008) and translation (Mart´ınezG´omez et al., 2011), efforts which served as inspiration for our own approach. BRAT , along with conversion tools and extensive documentation, is freely available under the open-source MIT license from its homepage at http://brat.nlplab.org Acknowledgements The authors would like to thank early adopters of BRAT who have provided us with extensive feedback and feature suggestions. This work was supported by Grant-in-Aid for Specially Promoted Research (MEXT, Japan), the UK Biotechnology and Biological Sciences Research Council (BBSRC) under project Automated Biologic"
E12-2021,W06-2920,0,\N,Missing
E12-2021,W03-0419,0,\N,Missing
E12-2021,P05-1013,0,\N,Missing
K17-3001,K17-3023,0,0.0375672,"Missing"
K17-3001,P16-1231,1,0.301678,"M Table 1: The supporting data overview: the number of words (M = million; K = thousand) for each language. http://commoncrawl.org/ Except for Ancient Greek, which was gathered from the Perseus Digital Library. 3 http://github.com/CLD2Owners/cld2 4 http://unicode.org/reports/tr15/ 3 verted to Unicode character NO-BREAK SPACE (U+00A0).5 The dimensionality of the word embeddings was chosen to be 100 after thorough discussion – more dimensions may yield better results and are commonly used, but even with just 100, the uncompressed word embeddings for the 45 languages take 135 GiB. Also note that Andor et al. (2016) achieved state-of-the-art results with 64 dimensions. The word embeddings were precomputed using word2vec (Mikolov et al., 2013) with the following options: word2vec -min-count 10 -size 100 -window 10 -negative 5 -iter 2 -threads 16 -cbow 0 -binary 0. The precomputed word embeddings are available on-line (Ginter et al., 2017). 2.3 this shared task, i.e., not included in any previous UD release. The PUD treebank consists of 1000 sentences currently in 18 languages (15 K to 27 K words, depending on the language), which were randomly picked from on-line newswire and Wikipedia;7 usually only a fe"
K17-3001,W06-2920,0,0.0145655,"categorization of the different approaches of the participating systems. Introduction Ten years ago, two CoNLL shared tasks were a major milestone for parsing research in general and dependency parsing in particular. For the first time dependency treebanks in more than ten languages were available for learning parsers. Many of them were used in follow-up work, evaluating parsers on multiple languages became standard, and multiple state-of-the-art, open-source parsers became available, facilitating production of dependency structures to be used in downstream applications. While the two tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) were extremely important in setting the scene for the following years, there were also limitations that complicated application of their results: (1) gold-standard to1 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 1–19, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. kenization and part-of-speech tags in the test data moved the tasks away from real-world scenarios, and (2) incompatible annotation schemes made cross-linguistic comparison impossible. CoNLL 2017 has picked"
K17-3001,K17-3017,0,0.147208,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3005,0,0.0752704,"Missing"
K17-3001,K17-3026,0,0.0310687,"E 90.88 82.31 82.46 LyS-FASTPARSE 90.88 82.31 79.14 NAIST SATO 90.88 82.31 82.46 Orange – Deski˜n 90.88 38.81 15.38 UALING 90.88 82.31 82.46 UParse 90.88 82.31 82.46 naistCL 90.88 82.31 82.46 Table 5: Universal POS tags, features and lemmas (ordered by UPOS F1 scores). duce suboptimal results when deployed on a machine different from the one where it was trained. Several teams used the library and may have been affected; for the Uppsala team (de Lhoneux et al., 2017) the issue led to official LAS = 65.11 (23rd place) instead of 69.66 (9th place). In the second case, the ParisNLP system (De La Clergerie et al., 2017) used a wrong method of recognizing the input language, which was not supported in the test data (but unfortunately it was possible to get along with it in development and trial data). Simply crashing could mean that the task moderator would show the team their diagnostic output and they would fix the bug; however, the parser was robust enough to switch to a languageagnostic mode and produced results that were not great, but also not so bad to alert the moderator and make him investigate. Thus the official LAS of the system is 60.02 (27th place) while without the bug it could have been 70.35 ("
K17-3001,K17-3021,0,0.0954088,"emains with participants, and since open sourcing the software underlying a paper is still the exception rather than the rule. To ensure both, TIRA supplies participants with a virtual machine, offering a range of commonly used operating systems in order not to limit the choice of technology stacks and development environments. Once deployed and tested, the virtual machines are archived to preserve the software within. Many participants agreed to share their code so that we decided to collect the respective projects in a kind of open source proceedings at GitHub.14 4.3 by Straka and Strakov´a (2017) as one of the competing systems. Straka and Strakov´a (2017) describe both these versions in more detail. The baseline models were released together with the UD 2.0 training data, one model for each treebank. Because only training and development data were available during baseline model training, we put aside a part of the training data for hyperparameter tuning, and evaluated the baseline model performance on development data. We called this data split baseline model split. The baseline models, the baseline model split, and also UD 2.0 training data with morphology predicted by 10-fold jack"
K17-3001,K17-3022,1,0.891655,"Missing"
K17-3001,K17-3025,0,0.0327614,"Missing"
K17-3001,K17-3024,0,0.050508,"Missing"
K17-3001,K17-3027,0,0.0537913,"Missing"
K17-3001,K17-3014,0,0.0756362,"Missing"
K17-3001,K17-3015,0,0.0745209,"Missing"
K17-3001,K17-3007,0,0.0511894,"Missing"
K17-3001,L16-1262,1,0.869327,"Missing"
K17-3001,W14-6111,0,0.0253686,"Missing"
K17-3001,W17-0411,1,0.831758,"ossible when the system run completed; before that, even the task moderator would not see whether the system was really producing output and not just sitting in an endless loop. Especially given the scale of operations this year, this turned out to be a major obstacle for some participants; TIRA needs to be improved by offering more finegrained process monitoring tools, both for organizers and participants. Content-word Labeled Attachment Score (CLAS) has been proposed as an alternative parsing metric that is tailored to the UD annotation style and more suitable for cross-language comparison (Nivre and Fang, 2017). It differs from LAS in that it only considers relations between content words. Attachment of function words is disregarded because it corresponds to morphological features in other languages (and morphology is not evaluated in this shared task). Furthermore, languages with many function words (e.g., English) have longer sentences than morphologically rich languages (e.g., Finnish), hence a single error in Finnish costs the parser significantly more than an error in English. CLAS also disregards attachment of punctuation. As CLAS is still experimental, we have designated full LAS as our main"
K17-3001,K17-3003,0,0.0845341,"Missing"
K17-3001,W17-0412,1,0.869806,"Missing"
K17-3001,L16-1680,1,0.0475333,"Missing"
K17-3001,K17-3009,1,0.104147,"Missing"
K17-3001,tiedemann-2012-parallel,0,0.0126153,"oses (so that follow-up research is not obstructed). We deliberately did not place upper bounds on data sizes (in contrast to e.g. Nivre et al. (2007)), despite the fact that processing large amounts of data may be difficult for some teams. Our primary objective was to determine the capability of current parsers with the data that is currently available. In practice, the task was formally closed, i.e., we listed the approved data resources so that all participants were aware of their options. However, the selection was rather broad, ranging from Wikipedia dumps over the OPUS parallel corpora (Tiedemann, 2012) to morphological transducers. Some of the resources were proposed by the participating teams. 2.2 Supporting Data To enable the induction of custom embeddings and the use of semi-supervised methods in general, the participants were provided with supporting resources primarily consisting of large text corpora for (nearly) all of the languages in the task, as well as embeddings pre-trained on these corpora. 1 Outside CoNLL, there were several other parsing tasks in the meantime, which naturally also explored previously unadressed aspects—for example SANCL (Petrov and McDonald, 2012) or SPMRL (S"
K17-3001,K17-3016,0,0.0605417,"Missing"
K17-3001,K17-3020,0,0.0375614,"Missing"
K17-3001,K17-3013,0,0.0456211,"Missing"
K17-3001,D07-1096,1,\N,Missing
K17-3001,K17-3002,1,\N,Missing
K17-3001,K17-3019,0,\N,Missing
K17-3001,K17-3012,1,\N,Missing
K17-3001,K17-3006,0,\N,Missing
K17-3001,K17-3010,0,\N,Missing
K17-3001,K17-3018,0,\N,Missing
K17-3001,K17-3028,1,\N,Missing
K17-3001,K17-3011,0,\N,Missing
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
L16-1607,anick-etal-2014-identification,0,0.0203449,"time mobile devices, Mac lower bound, cost, sentence Asia, space, between human, Eugene Charniak CRF, algorithm qualitative, new five, two-fold, several can, cannot, need to it, they Miyao and Tsujii 2008, [1] English, natural language NLP, biomedicine ERLA, universities F=0.98 Table 1: Entity tags, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such a"
L16-1607,I11-1001,0,0.225982,"based analysis such as information extraction (IE) concerning the methodological aspects of research papers and patents for analyzing technical trends and discovering emerging research fields. Their focus is on determining how things such as systems and data are developed and used. Consequently, in the annotated corpora used for establishing the systems for these purposes, things described in a document are labeled and classified according to their role in a certain context, such as application domain, method, and product. Some studies attach role-based labels to entity mentions. For example, Gupta and Manning (2011), in establishing a method for identifying the technical trends from abstracts in the ACL anthology 2 , extracted the FOCUS (main contribution of the article), DOMAIN (application domain), and TECHNIQUE (a method or tool used to achieve the FOCUS). The corpus used for the study attaches these labels directly to mentions of the corresponding entities. Similarly, Fukuda et al. (2012) annotated and classified entities in patent documents as TECHNOLOGY 2 3836 https://aclweb.org/anthology/ Type THING OCCURRENT PROCESS TIME CONTINUANT ARTIFACT DATA-ITEM LOCATION PERSON PLAN QUALITY QUANTITY MODALITY"
L16-1607,S10-1004,0,0.0371118,"DOMAIN (an area of study); and FORMULA (a mathematical formula). In addition, we defined the following compound or “ambiguous” types to handle systematic ambiguity in 3 Annotated Data Our dataset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indic"
L16-1607,P03-1054,0,0.00501384,"riginal annotation on the same part of the abstract shown in Figure 1, converted to standoff format for displaying in brat. Their annotation is sparser than ours (Figure 1), annotating only terms related to the topic of the paper as a whole. For extraction, they used heuristic rules based on trigger words and Stanford dependencies such as “A term is FOCUS if it is the direct object of the verb present” as seed rules. Then, the rule set was enhanced by iteratively adding the head words of extracted phrases as the triggers. The abstracts were tokenized using the Stanford parser (version 3.4.1) (Klein and Manning 2003), and the tokens are labeled with binary labels for inclusion in GuptaManning terms for each topic class (FOCUS, DOMAIN, and TECHNIQUE). Then, the support vector classifier from the python scikit-learn 0.17 package (Pedregosa et al., 2011) with a linear kernel was used to predict the labels. We tested several combinations of the features from the Stanford parser and our annotation. The features from the Stanford parser were parts of speech (P in Table 5) and the triplet of type, direction (head or argument), and the part of speech of the token it depends/depended on, for each dependency involv"
L16-1607,J08-1002,1,0.854996,"Missing"
L16-1607,D14-1200,0,0.0358364,"Missing"
L16-1607,W09-3716,0,0.0330807,"otactic component (...). The three components mentioned are entities of different types, i.e., lexicon is a dataset (DATA-ITEM), and the others are program functions (PLAN). The current convention uses IS-A relation to relate components and lexicon etc., which is impossible without violating Rule-IS. This suggests that we need to define a new relation for role-playing and a new type or types for “role” words such as components. We also found that ambiguity and metonymic constructions cause annotation difficulty. These violations suggest a need for a type-coercion mechanism, such as dot-types (Pustejovsky et al. 2009). For example, when a process uses parameters, the names of the parameters can denote “the invocation of the process with the parameters” (e. g. RM pairs extracted can perform the mapping, where RM pairs extracted denotes a process using the pairs as parameters) and the name of data structure is used for both the data structure itself and the content of the data (Bigrams and trigrams are commonly used in statistical natural language processing). They lead to the annotation of APPLY-TO relation between DATAITEM and PROCESS, which violates Rule-APP. Another type of the problem is the ambiguity b"
L16-1607,W04-2401,0,0.0369304,"Missing"
L16-1607,W15-0403,0,0.0213946,"gs, definitions and examples: names in monospaced font denotes the class in IAO (algorithms, materials, tools, and data used in invention), EFFECT (effects of a technology that can be expressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (20"
L16-1607,W14-2410,0,0.0296543,"ressed as a pair comprising an attribute and a value), and ATTRIBUTE and VALUE (attribute and value in the effect). Anick et al. (2014) extracted technology terms defined as Artifact (object created as a result of some process), Process/Technique (method for creation) or Field (a discipline or a scientific area relating to creation) using a corpus in which mentions of entities playing these roles are labeled. Roth and Klein (2015) extracted terms that denote an ACTION, ACTOR, OBJECT, and PROPERTY, using an annotated dataset in which entity mentions are labeled based on the ontology defined by Roth et al. (2014). In their ontology concepts are classified according to roles that things can play in a particular operation, such as a participant, actor, object, and property. Another type of approach to capturing the structure of entity roles is to annotate the relationship between entities to label the entities as “things in a certain context” and “how they are related to other things in the same context”. Kameda et al. (2013), using Related Work sections from the proceedings of the Association for the Advancement of Artificial Intelligence (AAAI2010), identified the papertopic relation along with the me"
L16-1607,D07-1111,0,0.0605503,"Missing"
L16-1607,E12-2021,1,0.768208,"taset was constructed from 400 abstracts of research papers (250 abstracts from the ACL anthology and 150 from the ACM digital library 3 ). In the ACL subset, 150 abstracts were randomly selected from the entire set and the remaining 100 were randomly selected from the set used by Gupta and Manning (2011). The abstracts in the ACM subset were randomly selected from the set used for the SEMEVAL-2010 task 5 (Kim et al., 2010). Errors in text resulting from PDF conversion were manually corrected. Annotation was performed by a single annotator (the second author). A screenshot of the brat system (Stenetorp et al., 2012) is given in Figure 1 as an annotation example. In 1959 sentences in the ACL set, 14887 entities and 13310 relations were identified. In the 1213 sentences in the ACM set, the numbers of identified entities and relations were 12463 and 11201, respectively. The distributions of entity and relation types, in proportion, in the two domains are shown in Figures 2 and 3. The results shown in Figure 2 indicate that software (PLAN, PLAN-OR-PROCESS) is more frequently discussed than hardware (ARTIFACT) in both the general computer science/technology domain (ACM) and the natural language processing sub"
L16-1607,W13-2318,1,0.777851,"Missing"
L16-1607,tateisi-etal-2014-annotation,1,0.890874,"gence (AAAI2010), identified the papertopic relation along with the method-purpose relation among concepts described in the paper in order to construct a network representing the methods developed in one study and used by others and to evaluate the influence of the research. Nassour-Kassis et al. (2015) identified the mentions of tasks and attributes and linked them with one of 6 types (Means-End, Instance-of, Consists-of, Associated-with, Contributes-to, and Compares-to) of relations, using ten articles on summarization for building a conceptual map in the natural language processing domain. Tateisi et al. (2014) developed a corpus on research articles from Journal of Information Processing Society of Japan (IPSJ Journal) where relationship among OBJECTS (named entities), MEASURE (judgment and evaluation, including numbers), and TERM (general technical concepts other than OBJECT and MEASURE) are identified and labeled with one of 16 types such as Apply-to (method-purpose), Evaluate (evaluation objectevaluation result), and Attribute (object-attribute), and developed a prototype of a keyword-based search system in which results can be filtered according to the relations involving the keyword. Those wor"
N15-3011,P13-4010,0,0.0733617,"Missing"
N15-3011,levy-andrew-2006-tregex,0,0.118323,"Missing"
P15-4016,H94-1020,0,0.545178,"DF serializations: JSON-LD, N-Triples and N-Quads, Notation3, RDF/XML, TriG, TriX, and Turtle. With the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of"
P15-4016,N06-4006,0,0.0351016,"ith the exception of named graphs for serializations that do not support them, conversion between these representations is guaranteed to preserve all information. In addition to the general, reversible format translation services provided by the OA Adapter, we provide scripts for offline conversion of various annotation file formats into the OA JSON-LD format to allow existing datasets to be imported into OA stores. The following are currently supported: Penn Treebank format (including PTB II PAS) (Marcus et al., 1994), a number of variants of CoNLL formats, including CoNLL-U,5 Knowtator XML (Ogren, 2006), and the standoff format used by the BRAT annotation tool (Stenetorp et al., 2012). We also provide supporting tools for importing files with OA JSON-LD data to a store and exporting to files over the RESTful OA API. Validation OA JSON-LD data can be validated on three levels: 1) whether the data is syntactically wellformed JSON, 2) whether it conforms to the JSON-LD specification, and 3) whether the abstract information content fulfills the OA data model. The first two can be accomplished using any one of the available libraries that implement the full JSON-LD syntax and API specifications.6"
P15-4016,W12-3610,0,0.0188254,"format (Sporny et al., 2014) and is the recommended serialization of OA. Every JSON-LD document is both a JSON document and a representation of RDF data. Figure 2 shows an example of a simple annotation using the OA JSON-LD representation.2 ""@id"": ""@type"": ""target"": ""body"": Action Read annotation Read all annotations Update annotation Delete annotation Create annotation Table 1: HTTP verbs, resources, and actions. Read-only services support only the two GET requests. is an RDF-based graph representation compatible with linguistic annotation formalisms such as LAF/GrAF (Ide and Suderman, 2007; Verspoor and Livingston, 2012). At its most basic level, the OA model differentiates between three key components: annotation, body, and target, where the annotation expresses that the body is related to the target of the annotation (Figure 1). The body can carry arbitrarily complex embedded data. { Resource Annotation Collection Annotation Annotation Collection 3.1 OA Store The OA Store is a reference implementation of persistent, server-side annotation storage that allows clients to create, read, update and delete annotations using the API. The store uses MongoDB, which is well suited to the task as it is a documentorien"
P15-4016,wright-2014-restful,0,0.0306169,"o all four databases. They query a PostgreSQL back-end for text and annotations, which are formatted as OA JSON-LD using the standard Python json module. 4.4 5 Related work Our approach builds directly on the OA data model (Bradshaw et al., 2013), which harmonizes the earlier Open Annotation Collaboration (Haslhofer et al., 2011) and Annotation Ontology Initiative (Ciccarese et al., 2011) efforts and is currently developed further under the auspices of the W3C Web Annotation WG.8 Approaches building on RESTful architectures and JSON-LD are also being pursued by the Linguistic Data Consortium (Wright, 2014) and the Language Application Grid (Ide et al., 2014), among others. A number of annotation stores following similar protocols have also been released recently, including Lorestore (Hunter and Gerber, 2012), PubAnnotation (Kim and Wang, 2012), the Annotator.js store9 , and NYU annotations10 . 6 Conclusions and future work We have proposed to share annotations using a minimal RESTful interface for Open Annotation data in JSON-LD. We introduced reference implementations of a server, client, validation and conversion tools, and demonstrated the integration of several independently developed annot"
P15-4016,W07-1501,0,\N,Missing
P15-4016,W12-2425,0,\N,Missing
P15-4016,ide-etal-2014-language,0,\N,Missing
P15-4016,E12-2021,1,\N,Missing
W04-1203,P99-1065,0,0.0256684,"nteraction subgraphs can be partially overlapping, because a single link can be part of more than one interaction subgraph. Figure 1 shows an example of an annotated text fragment. 4 Evaluation criteria We evaluated the performance of the LG parser according to the following three quantitative criteria: • Number of dependencies recovered • Number of fully correct linkages • Number of interaction subgraphs recovered The number of recovered dependencies gives an estimate of the probability that a dependency will be correctly identified by the LG parser (this criterion is also employed by, e.g., Collins et al. (1999)). The number of fully correct linkages, i.e. linkages where all annotated dependencies are recovered, measures the fraction of sentences that are parsed without error. However, a fully correct linkage is not necessary to extract protein-protein interactions from a sentence; to estimate how many interactions can potentially be recovered, we measure the number of interaction subgraphs for which all dependencies were recovered. For each criterion, we measure the performance for the first linkage returned by the parser. However, the first linkage as ordered by the heuristics of the LG parser was"
W07-1004,I05-1006,0,0.310811,"Missing"
W07-1004,de-marneffe-etal-2006-generating,0,0.168812,"Missing"
W07-1004,levy-andrew-2006-tregex,0,0.0261922,"ctic dependencies between the words. The scheme defines a hierarchy of 48 grammatical relations, or dependency types. The most generic relation, dependent, can be specialized as auxiliary, argument, or modifier, which again have several subtypes (de Marneffe et al., 2006). The Stanford conversion transforms phrase structure parses into the Stanford scheme. First, the semantic head of each constituent is identified using head rules similar to those of Collins (1999) and untyped dependencies are then extracted and labeled with the most specific grammatical relations possible using Tregex rules (Levy and Andrew, 2006). The system additionally provides a set of collapsing rules, suggested to be beneficial for IE applications (de Marneffe et al., 2006; Clegg and Shepherd, 2007). These rules collapse some dependencies by incorporating certain parts of speech (mostly Spx CC Pv CC Ss Js A/AN A/AN Cs E MVs Dsu Mp Pv Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-regulated . advcl> <nsubjpass conj> <auxpass cc> <advmod <mark <nsubjpass pobj> <nmod <nmod <det prep> <auxpass Vimentin and actin were also up-regulated , whereas an isoform of myosin heavy chain was down-r"
W07-1004,1993.iwpt-1.22,0,0.290886,"for dependency comparable to that of PTB for constituency, no widely adopted standard currently exists. In this paper, we present a step towards unifying the diverse syntax schemes in use in IE systems and corpora such as the GENIA Treebank1 and the recently introduced BioInfer corpus (Pyysalo et al., 2007). Clegg and Shepherd (2007) have recently proposed to use the Stanford dependency scheme (de Marneffe et al., 2006) as a common, application-oriented syntax representation. To assess this choice, we develop a set of conversion rules for transforming the Link Grammar (LG) dependency scheme (Sleator and Temperley, 1993) to 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ ∼genia 25 BioNLP 2007: Biological, translational, and clinical language processing, pages 25–32, c Prague, June 2007. 2007 Association for Computational Linguistics the Stanford scheme and then create a version of the BioInfer corpus in the Stanford scheme by applying the conversion rules and manually correcting the errors. By making the BioInfer corpus available in the Stanford scheme, we also increase the value of the corpus for biomedical IE. The transformation has the further benefit of allowing Link Grammar output to be normalized into a more ap"
W07-1004,A00-2018,0,\N,Missing
W07-1004,J03-4003,0,\N,Missing
W08-0601,H05-1091,0,0.0342895,"ach to PPI extraction. Finally, we discuss the effects that different evaluation strategies, choice of corpus and applied metrics have on measured performance, and conclude. P2 &lt;nsubj &lt;cop &lt;det &lt;nn &lt;nn P1 is a P2 binding protein &lt;xsubj xcomp&gt; &lt;aux &lt;nsubj P1 fails to 2 dobj&gt; bind P2 Figure 1: Stanford dependency parses (“collapsed” representation) where the shortest path, shown in bold, excludes important words. We next present our graph representation, formalize the notion of graph kernels, and present our learning method of choice, the sparse RLS. 2.1 et al., 2003) and shortest path kernels (Bunescu and Mooney, 2005) have been proposed and successfully used for relation extraction. However, these methods lack the expressive power to consider representations derived from general, possibly cyclic, dependency graph structures, such as those generated by the Stanford tools. The subsequence kernel approach does not consider parses at all, and the shortest path approach is limited to representing only a single path in the full dependency graph, which excludes relevant words even in many simple cases (Figure 1). Tree kernels can represent more complex structures, but are still restricted to tree representations."
W08-0601,I05-1006,0,0.085227,", 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers such as that introduced by Charniak and Lease (2005). Additionally, a number of conversions from phrase structure parses to dependency structures that make the relationships between words more directly accessible have been introduced. These include conversions into representations such as the Stanford dependency scheme (de Marneffe et al., 2006) that are explicitly designed for information extraction purposes. However, specialized feature representations and kernels are required to make learning from such structures possible. Approaches such as subsequence kernels (Bunescu and Mooney, 2006), tree kernels (Zelenko 1 BioNLP 2008: Current Trends i"
W08-0601,de-marneffe-etal-2006-generating,0,0.0576609,"Missing"
W08-0601,E06-1051,0,0.542388,"nteract are positive examples and other co-occuring  pairs negative. Thus, from each sentence, n2 examples are generated, where n is the number of occurrences of protein names in the sentence. Finally, we form the graph representation described earlier for each candidate interaction. We evaluate the method with 10-fold documentlevel cross-validation on all of the corpora. This guarantees the maximal use of the available data, and also allows comparison to relevant earlier work. In particular, on the AImed corpus we apply the exact same 10-fold split that was used by Bunescu et al. (2006) and Giuliano et al. (2006). Performance is measured according to the following criteria: interactions are considered untyped, undirected pairwise relations between specific protein mentions, that is, if the same protein name occurs multiple 1 Available at http://mars.cs.utu.fi/PPICorpora. 5 times in a sentence, the correct interactions must be extracted for each occurrence. Further, we do not consider self-interactions as candidates and remove them from the corpora prior to evaluation. The majority of PPI extraction system evaluations use the balanced F-score measure for quantifying the performance of the systems. This"
W08-0601,W07-1004,1,0.928632,"ic literature is a task of significant interest in the BioNLP field. The most commonly addressed problem has been the extraction of binary interactions, where the system identifies which protein pairs in a sentence have a biologically relevant relationship between them. Proposed solutions include both hand-crafted rule-based systems and machine learning approaches (see e.g. (Bunescu et al., 2005)). A wide range of results have been reported for the systems, but as we will show, differences in The public availability of large annotated PPIcorpora such as AImed (Bunescu et al., 2005), BioInfer (Pyysalo et al., 2007a) and GENIA (Kim et al., 2008), provides an opportunity for building PPI extraction systems automatically using machine learning. A major challenge is how to supply the learner with the contextual and syntactic information needed to distinguish between interactions and non-interactions. To address the ambiguity and variability of the natural language expressions used to state PPI, several recent studies have focused on the development, adaptation and application of NLP tools for the biomedical domain. Many high-quality domain-specific tools are now freely available, including full parsers suc"
W09-1301,doddington-etal-2004-automatic,0,0.0136738,"e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two entities without implication o"
W09-1301,S07-1003,0,0.00870172,"Missing"
W09-1301,W09-1401,1,0.666271,"well studied and several biomedProceedings of the Workshop on BioNLP, pages 1–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ical NER systems are available (see e.g. (Wilbur et al., 2007; Leaman and Gonzalez, 2008)), and most domain IE approaches are NE-driven: a typical way to cast the RE task is as deciding for each pair of co-occurring NEs whether a relevant relation is stated for them in context. Like the previous LLL and BioCreative2-PPI relation extraction tasks (N´edellec, 2005; Krallinger et al., 2007), the BioNLP’09 shared task on event extraction (Kim et al., 2009) similarly proceeds from NEs, requiring participants to detect events and determine the roles given NEs play in them. Any domain IE approach targeting nontrivial causal NE relations or events necessarily involves decisions relating to static relations. Consider, for example, the decision whether to extract a relation between NE1 and NE2 in the following cases (affects should here be understood as a placeholder for any relevant statement of causal relation): 1) NE1 affects NE2 gene 2) NE1 affects NE2 promoter 3) NE1 affects NE2 mutant 4) NE1 affects NE2 antibody 5) NE1 affects NE2 activator The"
W09-1301,de-marneffe-etal-2006-generating,0,0.054107,"Missing"
W09-1301,W01-0511,0,0.0903008,"nt relations To avoid unnecessary division of relations that imply in our context similar interpretation and processing, we define a task-specific Variant relation that encompasses a set of possible relation types holding between an NE and its variants along multiple different axes. One significant class of cases annotated as Variant includes expressions such as NE gene and NE protein, under the interpretation that NE refers to the abstract information that is “realized” as either DNA, RNA or protein form, and the entity to one of these realizations (for alternative interpretations, see e.g. (Rosario and Hearst, 2001; Heimonen et al., 2008)). The Variant relation is also used to annotate NEentity relations where the entity expresses a different state of the NE, such as a phosphorylated or mutated state. While each possible post-translational modification, for example, could alternatively be assigned a specific relation type, in the present IE context these would only increase the difficulty of the task without increasing the applicability of the resulting annotation. the corpus contained frequent cases where the stated relationship of the NE to the entity involved different types of relevant relations (e."
W09-1301,M95-1002,0,0.226052,"biologists and e.g. database curation efforts, most domain RE efforts target relations involving biologically relevant changes in the involved entities, commonly to the complete exclusion of static relations. However, static relations such as entity membership in a family and one entity being a part of another are not only 1 relevant IE targets in themselves but can also play an important supporting role in IE systems not primarily targeting them. In this paper, we investigate the role of static relations in causal RE and event extraction. Here, we use relation extraction in the MUC and ACE (Sundheim, 1995; Doddington et al., 2004) sense to refer to the task of extracting binary relations, ordered pairs of entities, where both participating entities must be specified and their roles (agent, patient, etc.) are fixed by the relation. By contrast, event extraction is understood to involve events (things that happen) and representations where the number and roles of participants may vary more freely. We refer to relations where one one entity causes another to change as causal relations; typical domain examples are phosphorylation and activation. Static relations, by contrast, hold between two enti"
W09-1301,I05-2038,1,0.170621,"ation criteria, NE and entity types, granularity of relations, etc.), we find the outcome — which was neither planned for nor forced on the data — a very encouraging sign of the sufficiency of the task setting for this and related domain IE tasks. 3.4 We created the data set by building on the annotation of the GENIA Event corpus (Kim et al., 2008), making use of the rich set of annotations already contained in the corpus: term annotation for NEs and other entities (Ohta et al., 2002), annotation of events between these terms, and treebank structure closely following the Penn Treebank scheme (Tateisi et al., 2005). Other/Out annotation We apply a catch-all category, Other/Out, for annotating candidate (NE, entity) pairs between which there is no relevant static relation. This label is thus applied to a number of quite different cases: causal relations, both implied (e.g. NE receptors, NE response element) and explicitly stated (NE binds the [site]), relations where the entity is considered too far removed from the NE to support reliable inference of a role for the NE in causal relations/events involving the entity (e.g. [antibodies] for NE), and cases where no relation is stated (e.g. NE and other [pro"
W09-1401,P05-1022,0,0.128444,"of event extraction, we prepared publicly available BioNLP resources readily available for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C"
W09-1401,M98-1001,0,0.713763,"sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance cap"
W09-1401,de-marneffe-etal-2006-generating,0,0.604282,"Missing"
W09-1401,W04-1213,1,0.414702,"ted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent interest of the bio-TM community is shifting toward IE. Similarly to LLL and BioCreative, the BioNLP’09 Shared Task (the BioNLP task, hereafter) also addresses bio-IE, but takes a definitive step further toward f"
W09-1401,P08-2026,0,0.440867,"ailable for the shared task. Several fundamental BioNLP tools were provided through U-Compare (Kano et al., 2009)2 , which included tools for tokenization, sentence segmentation, part-of-speech tagging, chunking and syntactic parsing. Participants were also provided with the syntactic analyses created by a selection of parsers. We applied two mainstream Penn Treebank (PTB) phrase structure parsers: the Bikel parser3 , implementing Collins’ parsing model (Bikel, 2004) and trained on PTB, and the reranking parser of (Charniak and Johnson, 2005) with the self-trained biomedical parsing model of (McClosky and Charniak, 2008)4 . We also applied the GDep5 , native dependency parser trained on the GENIA Treebank 2 http://u-compare.org/ http://www.cis.upenn.edu/∼dbikel/software.html 4 http://www.cs.brown.edu/∼dmcc/biomedical.html 5 http://www.cs.cmu.edu/∼sagae/parser/gdep/ 3 Team UTurku JULIELab Task 1-1-- Org 3C+2BI 1C+2L+2B ConcordU 1-3 3C Word Porter OpenNLP Porter Stanford UT+DBCLS 12- 2C Porter VIBGhent UTokyo 1-3 1-- 2C+1B 3C Porter, GTag UNSW UZurich 1-1-- 1C+1B 3C ASU+HU+BU 123 6C+2BI LingPipe, Morpha Porter Cam UAntwerp 1-12- 3C 3C Porter GTag UNIMAN 1-- 4C+2BI Porter GTag SCAI UAveiro USzeged 1-1-1-3 1C 1C+"
W09-1401,W09-1313,1,0.461582,"differences in annotation principles compared to other biomedical NE corpora. For instance, the NE annotation in the widely applied GENETAG corpus (Tanabe et al., 2005) does not differentiate proteins from genes, while GENIA annotation does. Such differences have caused significant inconsistency in methods and resources following different annotation schemes. To remove or reduce the inconsistency, GENETAG-style NE annotation, which we term gene-or-gene-product (GGP) annotation, has been added to the GENIA corpus, with appropriate revision of the original annotation. For details, we refer to (Ohta et al., 2009). The NE annotation used in the BioNLP task data is based on this annotation. 3.2 Argument revision The GENIA event annotation was made based on the GENIA event ontology, which uses a loose typing system for the arguments of each event class. For example, in Figure 2(a), it is expressed that the binding event involves two proteins, TRAF2 and CD40, and that, in the case of CD40, its cytoplasmic domain takes part in the binding. Without constraints on the type of theme arguments, the following two annotations are both legitimate: (Type:Binding, Theme:TRAF2, Theme:CD40) (Type:Binding, Theme:TRAF2"
W09-1401,W09-1301,1,0.380624,"om (a) PMID7541987 (simplified), (b) PMID10224278, (c) PMID10090931, (d) PMID9243743, (e) PMID7635985. (Type:Binding, Theme1:TRAF2, Theme2:CD40, Site2:cytoplasmic domain) Note that the protein, CD40, and its domain, cytoplasmic domain, are associated by argument numbering. To resolve issues related to the mapping between proteins and related entities systematically, we introduced partial static relation annotation for relations such as Part-Whole, drawing in part on similar annotation of the BioInfer corpus (Pyysalo et al., 2007). For details of this part of the revision process, we refer to (Pyysalo et al., 2009). Figure 2 shows some challenging cases. In (b), the site GATA motifs is not identified as an argument of the binding event, because the protein containing it is not stated. In (c), among the two sites (PEBP2 site and promoter) of the gene GM-CSF, only the more specific one, PEBP2, is annotated. The equivalent entity annotation in the revised GENIA corpus covers also cases other than simple apposition, illustrated in Figure 3. A frequent case in biomedical literature involves use of the slash symbol (“/”) to state synonyms. The slash symbol is ambiguous as it is used also to indicate dimerized"
W09-1401,strassel-etal-2008-linguistic,0,0.0454823,"r event extraction at a different level of specificity. The data was developed based on the GENIA event corpus. The shared task was run over 12 weeks, drawing initial interest from 42 teams. Of these teams, 24 submitted final results. The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges. 1 Introduction The history of text mining (TM) shows that shared tasks based on carefully curated resources, such as those organized in the MUC (Chinchor, 1998), TREC (Voorhees, 2007) and ACE (Strassel et al., 2008) events, have significantly contributed to the progress of their respective fields. This has also been the case in bio-TM. Examples include the TREC Genomics track (Hersh et al., 2007), JNLPBA (Kim et al., 2004), LLL (N´edellec, 2005), and BioCreative (Hirschman et al., 2007). While the first two addressed bio-IR (information retrieval) and bio-NER (named entity recognition), respectively, the last two focused on bio-IE (information extraction), seeking relations between bio-molecules. With the emergence of NER systems with performance capable of supporting practical applications, the recent i"
W09-1401,I05-2038,1,0.385704,"bioinformaticians (BI), biologists (B) and liguists (L). This may be attributed in part to the fact that the event extraction task required complex computational modeling. The role of computer scientists may be emphasized in part due to the fact that the task was novel to most participants, requiring particular efforts in framework design and implementation and computational resources. This also suggests there is room for improvement from more input from biologists. In total, 42 teams showed interest in the shared task and registered for participation, and 24 teams sub7.2 Evaluation results (Tateisi et al., 2005), and a version of the C&C CCG deep parser6 adapted to biomedical text (Rimell and Clark, 2008). The text of all documents was segmented and tokenized using the GENIA Sentence Splitter and the GENIA Tagger, provided by U-Compare. The same segmentation was enforced for all parsers, which were run using default settings. Both the native output of each parser and a representation in the popular Stanford Dependency (SD) format (de Marneffe et al., 2006) were provided. The SD representation was created using the Stanford tools7 to convert from the PTB scheme, the custom conversion introduced by (Ri"
W09-1401,J04-4004,0,\N,Missing
W09-4605,W09-1402,1,0.866311,"Missing"
W09-4605,doddington-etal-2004-automatic,0,0.0506133,"Missing"
W09-4605,E06-1051,0,0.0690424,"Missing"
W09-4605,C08-1053,0,0.190084,"Missing"
W09-4605,I05-1006,0,0.0207855,"Missing"
W09-4605,de-marneffe-etal-2006-generating,0,0.0775516,"Missing"
W09-4605,P08-1006,0,0.0407415,"Missing"
W09-4605,W07-1004,1,0.847509,"IE and has served as the basis for real-world applications for e.g. assisted database curation (Alex et al., 2008), its limitations, such as the restriction to events between entity pairs commonly referred to as binary interactions in the domain literature, are increasingly recognized by the biomedical NLP community. In this paper, we argue for an alternate model and present the first machine-learning approach to the extraction of structured, complex events and relationships among bioentities. To overcome the limitations of the pairwise approach to biomedical IE, two recent corpora, BioInfer (Pyysalo et al., 2007a) and the GENIA Event corpus (Kim et al., 2008a) annotate events and static relationships using a more expressive formalism that differs from the prevailing approach in several key aspects: First, type, direction and the trigger statement in the text stating the relationship (often a verb) are annotated. Second, events can have more than two participants whose roles are specified, allowing the accurate representation of statements such as proteins A, B and C form a complex. Finally, events can also act as arguments of other events, enabling the annotation of nested events such as A causes B t"
W09-4605,M95-1002,0,0.0448994,"ng approach in several key aspects: First, type, direction and the trigger statement in the text stating the relationship (often a verb) are annotated. Second, events can have more than two participants whose roles are specified, allowing the accurate representation of statements such as proteins A, B and C form a complex. Finally, events can also act as arguments of other events, enabling the annotation of nested events such as A causes B to bind C (Figure 1A). These representations largely resemble event extraction as formulated in (later) Message Understanding Conferences (MUC) (see, e.g., Sundheim (1995)) and in the Automatic Content Extraction (ACE) program (see, e.g., Doddington (2004)). BioInfer also annotates static relations (e.g. substructure) and both BioInfer and GENIA annotate non-biological relationships (e.g. coreferences) with specialized mechanisms. In this paper, we use the term complex relationship to encompass both event and generic relationship annotation. Learning to Extract Biological Event and Relation Graphs causes xcomp&gt; &lt;nsubj &lt;nsubj dobj&gt; &lt;aux Profilin causes actin NN VBZ NN Protein CAUSE Protein &lt;agent to TO bind VBZ BIND &lt;participant cofilin . NN . Protein . } } part"
W09-4605,W08-2121,0,0.070167,"Missing"
W10-1903,W09-1403,0,0.198686,"Missing"
W10-1903,W09-1313,1,0.711856,"applied to statements that involve the occurrence of a change in the state of an entity – even if stated as having occurred in the past, or only hypothetically – but not in cases merely discussing the state or properties of entities, even if these can serve as the basis for inference that a specific change has occurred. We found that many of the spans an2.5 Annotation results The new PTM annotation covers 157 PubMed abstracts. Following the model of the BioNLP shared task, all mentions of specific gene or gene product names in the abstracts were annotated, applying the annotation criteria of (Ohta et al., 2009). This new named entity annotation covers 1031 gene/gene product mentions, thus averaging more than six mentions per annotated abstract. In total, 422 events of which 405 are of the novel PTM 23 Event type Glycosylation Hydroxylation Methylation Acetylation Positive reg. Phosphorylation Protein modification TOTAL Count 122 103 90 90 12 3 2 422 applies a pipeline architecture consisting of three supervised classification-based modules: a trigger detector, an event edge detector, and an event detector. In evaluation on the BioNLP shared task test data, the system extracted phosphorylation events"
W10-1903,W09-1401,1,0.848379,"and the specific modified site are expected to be of more practical interest. However, we note that the greater number of multi-argument events is expected to make the dataset more challenging as an extraction target. 3 Evaluation 3.2 To estimate the capacity of the newly annotated resource to support the extraction of the targeted PTM events and the performance of current event extraction methods at open-domain PTM extraction, we performed a set of experiments using an event extraction method competitive with the state of the art, as established in the BioNLP shared task on event extraction (Kim et al., 2009a; Bj¨orne et al., 2009). 3.1 Data Preparation The corpus data was split into training and test sets on the document level with a sampling strategy that aimed to preserve a roughly 3:1 ratio of occurrences of each event type between training and test data. The test data was held out during system development and parameter selection and only applied in a single final experiment. The event extraction system was trained using the 112 abstracts of the training set, further using 24 of the abstracts Methods 6 We note that in the BioNLP shared task data, all arguments were contained within single se"
W10-1903,P06-4005,1,\N,Missing
W10-1903,P06-1128,1,\N,Missing
W10-1904,W09-1402,1,0.418613,"Missing"
W10-1904,de-marneffe-etal-2006-generating,0,0.0630604,"Missing"
W10-1904,W09-1313,1,0.821387,"terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a predictable relation (within the dataset) to the GGP. Th"
W10-1904,W03-1018,1,0.827599,"achieves results close to the best published on the standard GENETAG dataset and was reported to have the best performance in a recent study comparing publicly available taggers (Kabiljo et al., 2009). Titles and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨o"
W10-1904,W09-1301,1,0.800599,"apping from the events involving automatically extracted terms to ones involving the associated genes/proteins. The first challenge, gene/protein name normalization, is a wellstudied task in biomedical NLP for which a number of systems with promising performance have been proposed (Morgan and Hirschman, 2007). The second we believe to be novel. In the following, we propose a method for resolving this task. We base the decision on how to map events referencing broadly defined terms to ones referencing associated gene/protein names in part on a recently introduced dataset of “static relations” (Pyysalo et al., 2009) between named entities and terms (Ohta et al., 2009b). This dataset was created based on approximately 10,000 cases where GGP NEs, as annotated in the GENIA GGP corpus (Ohta et al., 2009a), were embedded in terms, as annotated in the GENIA term corpus (Ohta et al., 2002). For each such case, the relation between the NE and the term was annotated using a set of introduced relation types whose granularity was defined with reference to MeSH terms (see Table 2, Ohta et al., 2009b). From this data, we extracted prefix and suffix strings that, when affixed to a GGP name, produced a term with a pred"
W10-1904,W09-1401,1,0.830255,"Missing"
W10-1904,I05-2038,1,0.208821,"stracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of the event extraction system that performs also extraction of Shared Task subtask 2 and 3 information, providing additional"
W10-1904,P08-2026,0,0.00479175,"es and abstracts of all 17.8M citations in the 2009 distribution of PubMed are processed through the BANNER system. Titles and abstracts of PubMed citations in which at least one named entity was identified, and 29 which therefore contain a possible target for event extraction, are subsequently split into sentences using a maximum-entropy based sentence splitter trained on the GENIA corpus (Kazama and Tsujii, 2003) with limited rule-based post-processing for some common errors. All sentences containing at least one named entity are then parsed with the domain-adapted McClosky-Charniak parser (McClosky and Charniak, 2008; McClosky, 2009), which has achieved the currently best published performance on the GENIA Treebank (Tateisi et al., 2005). The constituency parse trees are then transformed to the collapsed-ccprocessed variant of the Stanford Dependency scheme using the conversion tool2 introduced by de Marneffe et al. (2006). Finally, events are extracted using the Turku Event Extraction System of Bj¨orne et al. which achieved the best performance in the BioNLP’09 Shared Task and remains fully competitive with even the most recent advances (Miwa et al., 2010). We use a recent publicly available revision of"
W10-1905,W07-1004,1,0.870303,"task, further outperforming the original system by Miwa et al. (2010). This result shows that the system applied for the comparison of syntactic parsers achieves state-of-the-art performance at event extraction. This result also shows that the system originally developed only for core events extraction can be easily extended for other arguments simply by treating the other arguments as additional arguments. 5 Related Work Many approaches for parser comparison have been proposed in the BioNLP field. Most comparisons have used gold treebanks with intermediate formats (Clegg and Shepherd, 2007; Pyysalo et al., 2007). Application-oriented parser comparison across several formats was first introduced by Miyao et al. (2009), who compared eight parsers and five formats for the protein-protein interaction (PPI) extraction task. PPI extraction, the recognition of binary relations of between proteins, is one of the most basic information extraction tasks in the BioNLP field. Our findings do not conflict with those of Miyao et al. Event extraction can be viewed as an additional extrinsic evaluation task for syntactic parsers, providing more reliable and evaluation and a broader perspective into parser performanc"
W10-1905,W04-3224,0,0.0429439,"ll and Clark, 2009)5 , and the Enju parser with the GENIA model (Miyao et al., 2009)6 . The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2 , the Bikel parser (Bikel) (Bikel, 2004)3 , the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4 , the C&C CCG parser, adapted to biomedical text 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2 http://www.cs.cmu.edu/∼sagae/parser/ gdep/ 3 http://www.cis.upenn.edu/∼dbikel/ software.html 4 http://www.cs.brown.edu/∼dmcc/ biomedical.html 5 http://svn.ask.it.usyd.edu.au/trac/ candc/ 6 http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 formance on this data. However, it does not invalidate comparison within the dataset. We further note that the models do not inc"
W10-1905,W09-1402,0,0.315036,"Missing"
W10-1905,W09-1406,1,0.494963,"8.48 / 51.95 N/A 23.05 / 48.19 / 31.19 26.32 / 41.81 / 32.30 36.90 / 55.59 / 44.35 Riedel Task 2 Ours 65.77 / 75.29 / 70.21 47.56 / 49.55 / 48.54 38.24 / 53.57 / 44.62 49.48 / 61.87 / 54.99 Riedel N/A 22.35 / 46.99 / 30.29 25.75 / 40.75 / 31.56 35.86 / 54.08 / 43.12 Table 6: Comparison of Recall / Precision / F-score results on the test data set. MC with CoNLL-X format and Enju with Predicate Argument Structure in Enju format are used for the evaluation. Results on simple, binding, regulation, and all events are shown. Results by Miwa et al. (2010) (Miwa), Bj¨orne et al. (2009) (Bj¨orne), and Riedel et al. (2009) (Riedel) for Task 1 and Task 2 are shown for comparison. The best score in each result is shown in bold. semble of the three parser outputs are +0.01 for Task 1, and -0.26 for Task 2. This result suggests that adding more different parsers does not always improve the performance. The ensemble of three parser outputs, however, shows stable performance across categories, scoring in the top two for binding, regulation, and all events, in the top four for simple events. ple, is good for extracting regulation events, but produced weaker results for simple events. The ensembles of two parser output"
W10-1905,D07-1111,1,0.529816,"e in Enju format. 2.2 Parsers and Formats (C&C) (Rimell and Clark, 2009)5 , and the Enju parser with the GENIA model (Miyao et al., 2009)6 . The formats are Stanford Dependencies (SD) (Figure 1), the CoNLL-X dependency format (Figure 2) and the predicate-argument structure (PAS) format used by Enju (Figure 3). With the exception of Enju, the analyses of these parsers were provided by the BioNLP 2009 Shared Task organizers. Analysis of system features in the task found that the use of parser output with one of Five parsers and three formats are adopted for the evaluation. The parsers are GDep (Sagae and Tsujii, 2007)2 , the Bikel parser (Bikel) (Bikel, 2004)3 , the Charniak-Johnson reranking parser, using David McClosky’s self-trained biomedical parsing model (MC) (McClosky, 2009)4 , the C&C CCG parser, adapted to biomedical text 1 http://www-tsujii.is.s.u-tokyo.ac.jp/ GENIA/SharedTask/ 2 http://www.cs.cmu.edu/∼sagae/parser/ gdep/ 3 http://www.cis.upenn.edu/∼dbikel/ software.html 4 http://www.cs.brown.edu/∼dmcc/ biomedical.html 5 http://svn.ask.it.usyd.edu.au/trac/ candc/ 6 http://www-tsujii.is.s.u-tokyo.ac.jp/ enju/ 38 formance on this data. However, it does not invalidate comparison within the dataset."
W10-1905,de-marneffe-etal-2006-generating,0,0.191901,"Missing"
W10-1905,I05-2038,1,0.0972984,"g the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7 , and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8 . We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008), contains abstracts that are also in the GENIA treebank. This implies that the parsers are likely to perform better on the texts used in the shared task than on other biomedical domain text, and similarly that systems building on their output are expected to achieve best per2.3 Event Extraction System The system by Miwa et al. (2010) is adopted for the evaluation. The system was originally"
W10-1905,W07-2416,0,0.0103989,"Phrase Structure Grammar (HPSG) and produces a format containing predicate argument structures (PAS) along with a phrase structure tree in Enju format. To study the contribution of the formats in which the five parsers output their analyses to task performance, we apply a number of conversions between the outputs, shown in Figure 4. The Enju PAS output is converted into Penn Treebank format using the method introduced by (Miyao et al., 2009). SD is generated from PTB by the Stanford tools (de Marneffe et al., 2006)7 , and CoNLLX dependencies are generated from PTB by using Treebank Converter (Johansson and Nugues, 2007)8 . We note that all of these conversions can introduce some errors in the conversion process. With the exception of Bikel, all the applied parsers have models specifically adapted for biomedical text. Further, all of the biomedical domain models have been created with reference and for many parsers with direct training on the data of (a subset of) the GENIA treebank (Tateisi et al., 2005). The results of parsing with these models as provided for the BioNLP Shared Task are used in this comparison. However, we note that the shared task data, drawn from the GENIA event corpus (Kim et al., 2008),"
W10-1905,W09-1418,0,0.0634223,"viously introduced base system is here improved with two modifications. One modification is removing two classes of features from the original features (for details of the original feature representation, we refer to (Miwa et al., 2010)); specifically the features representing governordependent relationships from the target word, and the features representing each event edges in the complex event detector are removed. The other modification is to use head words in a trigger expression as a gold trigger word. This modification is inspired by the part-of-speech (POS) based selection proposed by Kilicoglu and Bergler (2009). 7 http://www-nlp.stanford.edu/software/ lex-parser.shtml 8 http://nlp.cs.lth.se/software/ treebank converter/ 39 The system uses a head word “in” as a trigger word in a trigger expression “in the presence of” instead of using all the words of the expression. In cases where there is no head word information in a parser output, head words are selected heuristically: if a word does not modify another word in the trigger expression, the word is selected as a head word. The system is also modified to find secondary arguments (Task 2 in the BioNLP 2009 Shared Task). The second arguments are treate"
W10-1905,W09-1401,1,0.697119,"t of the BioNLP 2009 Shared Task. Section 2.2 then summarizes the five syntactic parsers and three formats adopted for the comparison. Section 2.3 described how the state-of-the-art event extraction system of Miwa et al. (2010) is modified and used for the comparison. prep pobj cc NFAT/AP-1 complex formed only with P and P2 nsubj dep conj Figure 1: Stanford basic dependency tree VMOD PMOD COORD ROOT root NFAT/AP-1 complex formed only with P and P2 2.1 Bio-molecular Event Extraction The bio-molecular event extraction task considered in this study is that defined in the BioNLP 2009 Shared Task (Kim et al., 2009)1 . The shared task provided common and consistent task definitions, data sets for training and evaluation, and evaluation criteria. The shared task consists of three subtasks: core event extraction (Task 1), augmenting events with secondary arguments (Task 2), and the recognition of speculation and negation of the events (Task 3) (Kim et al., 2009). In this paper we consider Task 1 and Task 2. The shared task defined nine event types, which can be divided into five simple events (Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization) that take one core argument"
W10-1919,P08-2026,0,0.0668102,"Missing"
W10-1919,W09-1402,0,0.0253425,"Missing"
W10-1919,W09-1313,1,0.872817,"Missing"
W10-1919,de-marneffe-etal-2006-generating,0,0.0410254,"Missing"
W10-1919,W09-1401,1,\N,Missing
W10-1921,S07-1003,0,0.0472074,"Missing"
W10-1921,W09-2415,0,0.023149,"Missing"
W10-1921,W09-1301,1,0.931667,"have evolved from extracting simple binary relations between genes or proteins to a more expressive event representation (Kim et al., 2009). Furthermore, new data sets have been developed targeting relations between genes and gene products (GGPs) and a broader category of entities, covering terms that can not be annotated as named entities (NEs) but that are still highly relevant for biomedical information extraction (Ohta et al., 2009b). In contrast to relations involving change or causality, the annotation for this data covers relations such as part-of, here termed “static relations” (SR) (Pyysalo et al., 2009). In addition to providing enhanced representation of biological processes, the SR data set also offers interesting opportunities to improve on event extraction. As an example, consider the sentence presented in Figure 2, in which “c-Rel” and “p50” are both annotated as being subunits of the 144 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 144–152, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics Event type Gene expression Transcription Protein catabolism Localization Phosphorylation Binding Regulation Positive regul"
W10-1921,W09-1419,1,0.907894,"Missing"
W10-1921,W09-1401,1,0.931626,"” and “transcription starts”. These two terms provide more detailed information on the regulation event. Thus, by combining the two types of annotation, a text mining algorithm will be able to provide a more detailed representation of the extracted information. This would be in particular beneficial in practical applications such as abstract summarization or integration of the predictions into complex regulatory pathways. Introduction Recently, biomedical text mining tools have evolved from extracting simple binary relations between genes or proteins to a more expressive event representation (Kim et al., 2009). Furthermore, new data sets have been developed targeting relations between genes and gene products (GGPs) and a broader category of entities, covering terms that can not be annotated as named entities (NEs) but that are still highly relevant for biomedical information extraction (Ohta et al., 2009b). In contrast to relations involving change or causality, the annotation for this data covers relations such as part-of, here termed “static relations” (SR) (Pyysalo et al., 2009). In addition to providing enhanced representation of biological processes, the SR data set also offers interesting opp"
W10-1921,P03-1054,0,0.00281811,"in text by using automatically curated dictionaries. Subsequently, candidate events are formed by combining these triggers with an appropriate number of GGPs co-occurring in the same sentence. For each distinct event type, a classifier is then built using all training examples for that specific type. Final predictions are merged for all types, forming a complex interaction graph for each article in the test set. To distinguish between positive instances and negatives, the framework extracts rich feature vecTo derive syntactic patterns, dependency parsing is applied using the Stanford parser (Klein and Manning, 2003; De Marneffe et al., 2006). Specifically, for each candidate event, the smallest subgraph is built including the relevant nodes for the trigger and the GGP names. Each edge in this subgraph then gives rise to a pattern including the information from the connecting nodes (or vertices) in combination with the syntactic relation specified by the edge. Trigger words and GGP names are blinded by replacing their text with the strings protx and trigger (respectively), resulting in highly general features. Figure 3 depicts an exemplary dependency graph. For the Binding event between c-Rel and p50, th"
W10-1921,D09-1013,0,0.0455981,"Missing"
W10-1921,de-marneffe-etal-2006-generating,0,\N,Missing
W10-1921,W09-1313,1,\N,Missing
W10-1921,S10-1006,0,\N,Missing
W11-0214,W09-1401,1,0.957728,"relations representing associations such as protein-protein interactions (Pyysalo et al., 2008; Tikk et al., 2010). In recent years, an increasing number of resources and methods pursuing more detailed representations of extracted information are becoming available (Pyysalo et al., 2007; Kim et al., 2008; Thompson et al., 2009; Bj¨orne et al., 2010). The main thrust of this move toward structured, finegrained information extraction falls under the heading of event extraction (Ananiadou et al., 2010), an approach popularized and represented in particular by the BioNLP Shared Task (BioNLP ST) (Kim et al., 2009a; Kim et al., 2011). While a detailed representation of extracted information on biomolecular events has several potential applications ranging from semantic search to database curation support (Ananiadou et al., 2010), the number of practical applications making use of this technology has arguably so far been rather limited. In this study, we pursue in particular the opportunities that event extraction holds for pathway annotation support,1 arguing that the match between The construction of pathways is a major focus of present-day biology. Typical pathways involve large numbers of entities o"
W11-0214,W11-1801,1,0.896597,"Missing"
W11-0214,P09-1113,0,0.0159016,"bustness. Conversion from pathways into the event representation opens up a number of opportunities, such as the ability to directly query large-scale event repositories (e.g. (Bj¨orne et al., 2010)) for specific pathway reactions. For pathways where reactions are marked with literature references, conversion further allows event annotations relevant to specific documents to be created automatically, sparing manual annotation costs. While such event annotations will not be bound to specific text expressions, they could be used through the application of techniques such as distant supervision (Mintz et al., 2009). As a first attempt, the conversion introduced in this work is limited in a number of ways, but we hope it can serve as a starting point for both wider adoption of pathway resources for event extraction and further research into accurate conversions between the two. The conversion software, SBML-to-event, is freely available for research purposes. 6 Discussion and Conclusions Over the last decade, the bio-community has invested enormous efforts in the construction of detailed models of the function of a large variety of biological systems in the form of pathways. These efforts toward building"
W11-0214,W09-1313,1,0.927341,"pe Drug. The same holds (with somewhat less specificity) for GENIA I NOR GANIC COMPOUND . Finally, although annotated in GENIA, the category of protein complexes has no correspondence among the entities considered in the BioNLP ST representation. Thus, information extraction systems applying the core BioNLP ST entity types will entirely lack coverage for protein complexes and will not be able 6 While the term P ROTEIN appears to suggest that the class consists only of protein forms, these entities are in fact annotated in the BioNLP ST data according to the GENIA gene/gene product guidelines (Ohta et al., 2009) and thus include also DNA and RNA forms. The type could arguably more accurately be named G ENE OR GENE PRODUCT. Pathway CellDesigner BioPAX State transition BiochemicalReaction Truncation BiochemicalReaction Transcription BiochemicalReaction Translation BiochemicalReaction Association ComplexAssembly Dissociation ComplexAssembly Transport Transport w/reaction Degradation Degradation Catalysis Catalysis Physical stimulation Control Modulation Control Trigger Control Inhibition Control Event ST’11 (see Table 3) Catabolism Catabolism Transcription Transcription Binding Binding Localization Loca"
W11-0214,W11-1803,1,0.87717,"Missing"
W11-0214,W11-1804,1,0.847779,"Missing"
W11-0215,W11-1828,0,0.226162,"Missing"
W11-0215,W09-1402,0,0.0757843,"Missing"
W11-0215,W10-1904,1,0.895723,"Missing"
W11-0215,W09-1403,0,0.369926,"Missing"
W11-0215,W11-1801,1,0.888281,"Missing"
W11-0215,C10-1088,1,0.850253,"criteria of the BioNLP Shared Task. The evaluation is event instance-based and uses the standard precision/recall/F1 -score metrics. We modified the shared task evaluation software to support the newly defined event types and ran experiments with the standard approximate span matching and partial recursive matching criteria (see (Kim et al., 2009)). We further follow the EPI task evaluation in reporting results separately for the extraction of only Theme and Cause arguments (core task) and for the full argument set. 5.2 Event extraction method We applied the EventMine event extraction system (Miwa et al., 2010a; Miwa et al., 2010b), an SVMbased pipeline system using an architecture similar to that of the best-performing system in the BioNLP ST’09 (Bj¨orne et al., 2009); we refer to the studies of Miwa et al. for detailed description of the base system. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDep beta2 (Sagae and Tsujii, 2007) parsers. For the present study, we modified the base EventMine system as follows. First, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candi"
W11-0215,nawaz-etal-2010-meta,0,0.0203,"egulation of modifi1 GO structure and statistics from data retrieved Dec. 2010. Figure 2: Comparison of hypothetical text-bound GO annotation with specific terms (top) and event annotation with general GO terms (bottom). cation, as these are captured using separate events in the applied representation, as illustrated in Figure 1. For an analogous reason, we do not separately include type-level distinctions for “magnitude” variants of terms (e.g. monoubiquitination, polyubiquitination) as these can be systematically modeled as aspects that can mark any event (cf. the low/neutral/high Manner of Nawaz et al. (2010)). Second, a number of the GO terms identify reactions that are in scope of previously defined (nonmodification) event types in existing resources. To avoid introducing redundant or conflicting annotation with e.g. the GENIA Event corpus (Kim et al., 2008) or BioNLP ST resources, we excluded terms that involve predominantly (or exclusively) noncovalent binding (included in the scope of the event type B INDING) and terms involving the removal of or binding between the amino acids of a protein, including protein maturation by peptide bond cleavage (annotated – arguably somewhat inaccurately – as"
W11-0215,W09-1313,1,0.83441,"performance. 4 Annotation This section presents the entity and event annotation approach, document selection, and the statistics of the created annotation. 8 The remarkably high coverage for a single type reflects the Zipfian distribution of the modification types; see e.g. Ohta et al. (2010). 4.1 To maximize compatibility with existing eventannotated resources, we chose to follow the general representation and annotation guidelines applied in the annotation of GENIA/BioNLP ST resources, specifically the BioNLP ST 2011 EPI task corpus. Correspondingly, we followed the GENIA gene/gene product (Ohta et al., 2009) annotation guidelines for marking protein mentions, extended the GENIA event corpus guidelines (Kim et al., 2008) for the annotation of protein modification events, and marked C ATALYSIS events following the EPI task representation. For compatibility, we also marked event negation and speculation as in these resources. We followed the GO definitions for individual modification types, and in the rare cases where a modification discussed in text had no existing GO definition, we extrapolated from the way in which protein modifications are generally defined in GO, consulting other domain ontolog"
W11-0215,W10-1903,1,0.913681,"ced substantially in recent years, shifting from the detection of simple binary associations such as protein-protein interactions toward resources and methods for the extraction of multiple types of structured associations of varying numbers participants in specific roles. These IE approaches are frequently termed event extraction (Ananiadou et al., 2010). While protein modifications have been considered in numerous IE studies in the domain (e.g. (Friedman et al., 2001; Rzhetsky et al., 2004; Hu et al., 2005; Narayanaswamy et al., 2005; Saric et al., 2006; Yuan et al., 2006; Lee et al., 2008; Ohta et al., 2010), event extraction efforts have brought increased focus also on the extraction of protein modifications: in the BioNLP Shared Task series that has popularized event extraction, the 2009 shared task (Kim et al., 2009) involved the extraction of nine event types including one PTM, and in the 2011 follow-up event (Kim et al., 2011) the Epigenetics and Post-translational modifications (EPI) task (Ohta et al., 2011) targeted six PTM types, their re114 Proceedings of the 2011 Workshop on Biomedical Natural Language Processing, ACL-HLT 2011, pages 114–123, c Portland, Oregon, USA, June 23-24, 2011. 2"
W11-0215,W11-1803,1,0.879677,"Missing"
W11-0215,D07-1111,1,0.793565,"he EPI task evaluation in reporting results separately for the extraction of only Theme and Cause arguments (core task) and for the full argument set. 5.2 Event extraction method We applied the EventMine event extraction system (Miwa et al., 2010a; Miwa et al., 2010b), an SVMbased pipeline system using an architecture similar to that of the best-performing system in the BioNLP ST’09 (Bj¨orne et al., 2009); we refer to the studies of Miwa et al. for detailed description of the base system. For analysing sentence structure, we applied the mogura 2.4.1 (Matsuzaki and Miyao, 2007) and GDep beta2 (Sagae and Tsujii, 2007) parsers. For the present study, we modified the base EventMine system as follows. First, to improve efficiency and generalizability, instead of using all words as trigger candidates as in the base system, we filtered candidates using a dictionary extracted from training data and expanded by using the UMLS specialist lexicon (Bodenreider, 2004) and the “hypernyms” and “similar to” relations in WordNet (Fellbaum, 120 1998). Second, to allow generalization across argument types, we added support for solving a single classification problem for event argument detection instead of solving multiple"
W11-0218,W10-1904,1,0.871169,"Missing"
W11-0218,W10-1911,0,0.033661,"Missing"
W11-0218,W04-1213,0,0.0888074,"Missing"
W11-0218,W11-1802,0,0.0476946,"Missing"
W11-0218,W11-1803,1,0.872374,"Missing"
W11-0218,C10-1096,1,0.81728,"elated lexical resource. For example, if we observe the text “Carbonic anhydrase IV” marked as P ROTEIN and have an entry for “Carbonic anhydrase 4” in a lexical resource, a machine learning method can learn to associate the resource with the P ROTEIN category (at specific similarity thresholds) despite syntactic differences. In this study, we aim to construct such a system and to demonstrate that it outperforms strict string matching approaches. We refer to our system as SimSem, as in “Similarity” and “Semantic”. 2.2 SimString SimString1 is a software library utilising the CPMerge algorithm (Okazaki and Tsujii, 2010) to enable fast approximate string matching. The software makes it possible to find matches in a collection with over ten million entries using cosine similarity and a similarity threshold of 0.7 in approximately 1 millisecond with modest modern hardware. This makes it useful for querying a large collection of strings to 1 http://www.chokkan.org/software/simstring/ 137 find entries which may differ from the query string only superficially and may still be members of the same semantic category. As an example, if we construct a SimString database using an American English wordlist2 and query it"
W11-0218,W11-1804,1,0.849299,"Missing"
W11-0218,W09-1119,0,0.022841,"nce. We evaluate our results on six corpora representing a variety of disambiguation tasks. While the integration of approximate string matching features is shown to substantially improve performance on one corpus, results are modest or negative for others. We suggest possible explanations and future research directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but inst"
W11-0218,W08-0609,0,0.020694,"troduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performing multi-class biomedical NER system, this motivates our desire to include multiple semantic categories. 136 Proceedings of the 2011 Worksho"
W11-0218,W03-1306,1,0.875157,"rch directions. Our lexical resources and implementation are made freely available for research purposes at: http://github.com/ninjin/ simsem 1 Introduction The use of dictionaries for boosting performance has become commonplace for Named Entity Recognition (NER) systems (Torii et al., 2009; Ratinov and Roth, 2009). In particular, dictionaries can give an Most previous work studying the use of dictionary resources in entity mention-related tasks has focused on single-class NER, in particular this is true for BioNLP where it has mainly concerned the detection of proteins. These efforts include Tsuruoka and Tsujii (2003), utilising dictionaries for protein detection by considering each dictionary entry using a novel distance measure, and Sasaki et al. (2008), applying dictionaries to restrain the contexts in which proteins appear in text. In this work, we do not consider entity mention detection, but instead focus solely on the related task of disambiguating the semantic category for a given continuous sequence of characters (a textual span), doing so we side-step the issue of boundary detection in favour of focusing on novel aspects of semantic category disambiguation. Also, we are yet to see a high-performi"
W11-0218,W09-1401,1,\N,Missing
W11-1801,S10-1006,0,0.034309,"Missing"
W11-1801,W11-1802,1,0.673169,"ined bio-IE in these directions is emphasized as the main theme of the second event. This paper summarizes the entire BioNLP-ST 2011, covering the relationships between tasks and similar broad issues. Each task is presented in detail in separate overview papers and extraction systems in papers by participants. 1 Proceedings of BioNLP Shared Task 2011 Workshop, pages 1–6, c Portland, Oregon, USA, 24 June, 2011. 2011 Association for Computational Linguistics 2 Main tasks BioNLP-ST 2011 includes four main tracks (with five tasks) representing fine-grained bio-IE. 2.1 Genia task (GE) The GE task (Kim et al., 2011) preserves the task definition of BioNLP-ST 2009, arranged based on the Genia corpus (Kim et al., 2008). The data represents a focused domain of molecular biology: transcription factors in human blood cells. The purpose of the GE task is two-fold: to measure the progress of the community since the last event, and to evaluate generalization of the technology to full papers. For the second purpose, the provided data is composed of two collections: the abstract collection, identical to the BioNLP-ST 2009 data, and the new full paper collection. Progress on the task is measured through the unchang"
W11-1801,W10-1905,1,0.365086,"t.Bossy@jouy.inra.fr Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series repres"
W11-1801,W11-1811,1,0.812584,"Missing"
W11-1801,W10-1903,1,0.814538,"Missing"
W11-1801,W11-1803,1,0.724562,"Missing"
W11-1801,N10-1123,0,0.139216,"Ngan Nguyen Jun’ichi Tsujii University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move to"
W11-1801,W09-1301,1,0.390547,"sk (Pyysalo et al., 2011b) involves the recognition of two binary part-of relations between entities: P ROTEIN -C OMPONENT and S UBUNITC OMPLEX. The task is motivated by specific challenges: the identification of the components of proteins in text is relevant e.g. to the recognition of Site arguments (cf. GE, EPI and ID tasks), and relations between proteins and their complexes relevant to any task involving them. REL setup is informed by recent semantic relation tasks (Hendrickx et al., 2010). The task data, consisting of new annotations for GE data, extends a previously introduced resource (Pyysalo et al., 2009; Ohta et al., 2010a). Supporting tasks 3.3 Gene renaming task (REN) BioNLP-ST 2011 includes three supporting tasks designed to assist in primary the extraction tasks. Other supporting resources made available to participants are presented in (Stenetorp et al., 2011). 3.1 Protein coreference task (CO) The CO task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections betwe"
W11-1801,W10-1919,1,0.755144,"diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domain experts and informed by prior studies on domain information extraction requirements (Pyysalo et al., 2010; Ananiadou et al., 2011), including the support of systems such as PATRIC (http://patricbrc.org). 2.4 Bacteria track The bacteria track consists of two tasks, BB and BI. 2.4.1 Bacteria biotope task (BB) The aim of the BB task (Bossy et al., 2011) is to extract the habitats of bacteria mentioned in textbooklevel texts written for non-experts. The texts are Web pages about the state of the art knowledge about bacterial species. BB targets general relations, Localization and PartOf , and is challenging in that texts contain more coreferences than usual, habitat references are not necessarily nam"
W11-1801,W11-1804,1,0.768915,"Missing"
W11-1801,W11-1812,1,0.906991,"elevant to task topics, including major protein modification types and their reverse reactions. For capturing the ways in which different entities participate in these events, the task extends the GE argument roles with two new roles specific to the domain, Sidechain and Contextgene. The task design and setup are oriented toward the needs of pathway extraction and curation for domain databases (Wu et al., 2003; Ongenaert et al., 2008) and are informed by previous studies on extraction of the target events (Ohta et al., 2010b; Ohta et al., 2010c). 2.3 Infectious diseases task (ID) The ID task (Pyysalo et al., 2011a) concerns the extraction of events relevant to biomolecular mechanisms of infectious diseases from full-text publica2 tions. The task follows the basic design of BioNLPST 2009, and the ID entities and extraction targets are a superset of the GE ones. The task extends considerably on core entities, adding to P ROTEIN four new entity types, including C HEMICAL and O RGANISM. The events extend on the GE definitions in allowing arguments of the new entity types as well as in introducing a new event category for high-level biological processes. The task was implemented in collaboration with domai"
W11-1801,W11-1816,1,0.850837,"Missing"
W11-1801,I05-2038,1,0.684261,"O task (Nguyen et al., 2011) concerns the recognition of coreferences to protein references. It is motivated from a finding from BioNLP-ST 2009 result analysis: coreference structures in biomedical text hinder the extraction results of fine-grained IE systems. While finding connections between event triggers and protein references is a major part of event extraction, it becomes much harder if one is replaced with a coreferencing expression. The CO task seeks to address this problem. The data sets for the task were produced based on MedCO annotation (Su et al., 2008) and other Genia resources (Tateisi et al., 2005; Kim et al., 2008). 3 The REN task (Jourde et al., 2011) objective is to extract renaming pairs of Bacillus subtilis gene/protein names from PubMed abstracts, motivated by discrepancies between nomenclature databases that interfere with search and complicate normalization. REN relations partially overlap several concepts: explicit renaming mentions, synonymy, and renaming deduced from biological proof. While the task is related to synonymy relation extraction (Yu and Agichtein, 2003), it has a novel definition of renaming, one name permanently replacing the other. 4 Schedule Table 2 shows the"
W11-1801,W10-1901,0,0.042682,"University of Tokyo Microsoft Research Asia 7-3-1 Hongo, Bunkyo-ku, Tokyo 5 Dan Ling Street, Haiian District, Beijing nltngan@is.s.u-tokyo.ac.jp jtsujii@microsoft.com Abstract mance. Also, as the complexity of the task was high and system development time limited, we encouraged focus on fine-grained IE by providing gold annotation for named entities as well as various supporting resources. BioNLP-ST 2009 attracted wide attention, with 24 teams submitting final results. The task setup and data since have served as the basis for numerous studies (Miwa et al., 2010b; Poon and Vanderwende, 2010; Vlachos, 2010; Miwa et al., 2010a; Bj¨orne et al., 2010). The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects. 1 Introduction The BioNLP Shared Task (BioNLP-ST, hereafter) series represents a community-wide move toward fine-grain"
W11-1801,W09-1401,1,\N,Missing
W11-1801,W11-1809,1,\N,Missing
W11-1801,W11-1810,1,\N,Missing
W11-1803,W11-1828,0,0.113486,"Missing"
W11-1803,W09-1403,0,0.0278857,"Missing"
W11-1803,P05-1022,0,0.0198621,"sk, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNA methylation resources other than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply ot"
W11-1803,de-marneffe-etal-2006-generating,0,0.0986335,"Missing"
W11-1803,W11-1827,0,0.255622,"her than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply other corpora for training. With the exception of externally acquired unlabeled data such as PubMed-derived word clusters applied by three groups, the task results thus reflect a closed task setting in which only the given data is used for training. 5.2 Evaluation results Table 4 presents a the primary results by event type, and Table 5 summarizes these results. We note that only two teams, UTurku (Bj¨orne and Salakoski, 2011) and ConcordU (Kilicoglu and Bergler, 2011), predicted event modifications, and only UTurku predicted additional (non-core) event arguments (data not shown). The other five systems thus addressed MSRCCP- ConUTurku FAUST NLP UMass Stanford BTMG cordU Size H YDROXYLATION 42.25 10.26 10.20 12.80 9.45 12.84 6.32 139 D EHYDROXYLATION - 1 67.12 51.61 50.00 49.18 40.98 47.06 44.44 130 P HOSPHORYLATION D EPHOSPHORYLATION 0.00 0.00 0.00 0.00 0.00 50.00 0.00 3 75.34 72.95 67.88 72.94 67.44 70.87 69.97 340 U BIQUITINATION D EUBIQUITINATION 54.55 40.00 0.00 31.58 0.00 42.11 14.29 17 60.21 31.21 34.54 23.82 31.02 15.65 8.22 416 DNA METHYLATION DNA"
W11-1803,W11-1801,1,0.73904,"Missing"
W11-1803,W11-1802,0,0.660347,"ation, events are typed n-ary associations of participants (entities or other events) in specific roles. Events are bound to specific expressions in text (the event trigger or text binding) and are primary objects of annotation, allowing them to be marked in turn e.g. as negated or as participants in other events. Figure 2 illustrates these concepts. In its specific formulation, EPI broadly follows the definition of the BioNLP’09 shared task on event extraction. Basic modification events are defined similarly to the P HOSPHORYLATION event type targeted in the ’09 and the 2011 GE and ID tasks (Kim et al., 2011b; Pyysalo et al., 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. 2.1 Entities The EPI task follows the general policy of the BioNLP Shared Task in isolating the basic task of named entity recognition from the event extraction task by providing task participants with manually annotated gene and gene product entities as a starting point for extraction. The entity types follow the BioNLP’09 Shared Task scheme, where genes and their products are simply marked as P ROTEIN.1 In addition to the given P ROTEIN entities, some even"
W11-1803,W11-1826,0,0.0276367,"fic methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability of PTM and DNA methylation resources other than those specifically introduced for the task and the P HOSPHORYLATION annotations in the GE task (Kim et al., 2011b), no participant chose to apply other corpora for training. With the exception of externally acquired unlabeled data such as PubMed-derive"
W11-1803,W11-1806,0,0.132325,". The full task results are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared ta"
W11-1803,W09-1313,1,0.127393,"Missing"
W11-1803,W10-1903,1,0.788921,"Missing"
W11-1803,W11-0215,1,0.844108,"yped n-ary associations of participants (entities or other events) in specific roles. Events are bound to specific expressions in text (the event trigger or text binding) and are primary objects of annotation, allowing them to be marked in turn e.g. as negated or as participants in other events. Figure 2 illustrates these concepts. In its specific formulation, EPI broadly follows the definition of the BioNLP’09 shared task on event extraction. Basic modification events are defined similarly to the P HOSPHORYLATION event type targeted in the ’09 and the 2011 GE and ID tasks (Kim et al., 2011b; Pyysalo et al., 2011b), with the full task extending previously defined arguments with two additional ones, Sidechain and Contextgene. 2.1 Entities The EPI task follows the general policy of the BioNLP Shared Task in isolating the basic task of named entity recognition from the event extraction task by providing task participants with manually annotated gene and gene product entities as a starting point for extraction. The entity types follow the BioNLP’09 Shared Task scheme, where genes and their products are simply marked as P ROTEIN.1 In addition to the given P ROTEIN entities, some events involve other entiti"
W11-1803,W11-1804,1,0.370325,"Missing"
W11-1803,W11-1825,0,0.0576857,"ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of the analyses as supporting resources (Stenetorp et al., 2011). Despite the availability"
W11-1803,W11-1807,0,0.057453,"are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in the previous shared task and the availability of t"
W11-1803,W11-1808,0,0.285928,"extract core targets. The full task results are considered the primary evaluation for the task e.g. for the purposes of determining the ranking of participating systems. 5 5.1 Results Participation Table 3 summarizes the participating groups and the features of their extraction systems. We note that, similarly to the ’09 task, machine learning-based systems remain dominant overall, although there is considerable divergence in the specific methods applied. In addition to domain mainstays such as support vector machines and maximum entropy models, we find increased application of joint models (Riedel et al., 2011; McClosky et al., 2011; Riedel and McCallum, 2011) as opposed to pure pipeline systems (Bj¨orne and Salakoski, 2011; Quirk et al., 2011) . Remarkably, the application of full pars21 ing together with dependency-based representations of syntactic analyses is adopted by all participants, with the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) is applied in all but one system (Liu et al., 2011) and the Stanford Dependency representation (de Marneffe et al., 2006) in all. These choices may be motivated in part by the success of systems using the tools in"
W11-1803,W11-1816,1,0.608476,"Missing"
W11-1803,W09-1401,1,\N,Missing
W11-1804,W11-1828,0,0.0564394,"Missing"
W11-1804,P05-1022,0,0.0320164,"ced to only core arguments, event modifications are removed, and resulting duplicate events removed. We term this the core task. In terms of the subtask division applied in the BioNLP’09 Shared Task and the GE task of 2011, the core task is analogous to subtask 1 and the full task analogous to the combination of subtasks 1–3. 5 5.1 Results Participation Final results to the task were successfully submitted by seven participants. Table 5 summarizes the information provided by the participating teams. We note that full parsing is applied in all systems, with the specific choice of the parser of Charniak and Johnson (2005) with the biomedical domain model of McClosky (2009) and conversion into the Stanford Dependency representation (de Marneffe et al., 2006) being adopted by five participants. Further, five of the seven systems are predominantly machine learning-based. These can be seen as extensions of trends that were noted in analysis of the BioNLP Rank Team Org 1 FAUST 3NLP 2 UMass 1NLP 3 Stanford 3NLP Word CoreNLP, SnowBall CoreNLP, SnowBall CoreNLP 4 ConcordU 2NLP - 5 UTurku 6 PNNL 7 PredX 1BI Porter 1CS, 1NLP, Porter 2BI 1CS, 1NLP LGP NLP Parse Trig. McCCJ + SD Arg. Events Group. Other resources Modif. C"
W11-1804,de-marneffe-etal-2006-generating,0,0.13301,"Missing"
W11-1804,W11-1827,0,0.406561,"ons are largely compatible with ID ones (see detailed results below). This is encouraging for future applications of the event extraction approach: as manual annotation requires considerable effort and time, the ability to use existing annotations is important for the feasibility of adaptation of the approach to new domains. While several participants made use of supporting syntactic analyses provided by the organizers (Stenetorp et al., 2011), none applied the analyses for supporting tasks, such as coreference or entity relation extraction results – at least in cases due to time constraints (Kilicoglu and Bergler, 2011). 5.2 Evaluation results Table 6 presents the primary results by event type, and Table 7 summarizes these results. The full task requires the extraction of additional arguments and event modifications and involves multiple novel challenges from previously addressed domain tasks including a new subdomain, full-text documents, several new entity types and a new event category. 31 Team FAUST UMass Stanford ConcordU UTurku PNNL PredX recall 48.03 46.92 46.30 49.00 37.85 27.75 22.56 prec. 65.97 62.02 55.86 40.27 48.62 52.36 35.18 F-score 55.59 53.42 50.63 44.21 42.57 36.27 27.49 Table 7: Primary ev"
W11-1804,W11-1801,1,0.755037,"Missing"
W11-1804,W11-1802,0,0.446286,"ected by participants addressing the full task. 2.2 Relations The ID task involves one relation, E QUIV, defining entities (of any of the core types) to be equivalent. This relation is used to annotate abbreviations and local aliases and it is not a target of extraction, but provided for reference and applied in evaluation, where references to any of a set of equivalent entities are treated identically. 2.3 Events The primary extraction targets of the ID task are the event types summarized in Table 1. These are a superset of those targeted in the BioNLP ST’09 and its repeat, the 2011 GE task (Kim et al., 2011b). This design makes it possible to study aspects of domain adaptation by having the same extraction targets in two subdomains of biomedicine, that of transcription factors in human blood cells (GE) and infectious diseases. The events in the ID task extend on those of GE in the inclusion of additional entity types as participants in previously considered event types and the introduction of a new type, P ROCESS. We next briefly discuss the semantics of these events, defined (as in GE) with reference to the communitystandard Gene Ontology (Ashburner et al., 2000). We refer to (Kim et al., 2008;"
W11-1804,W11-1806,0,0.20456,"Missing"
W11-1804,W11-1818,0,0.0187268,"Missing"
W11-1804,W09-1313,1,0.0825789,"es is not part of the ID task. As named entity recognition (NER) is considered in other prominent domain evaluations (Krallinger et al., 2008), we have chosen to isolate aspects of extraction performance relating to NER from the main task of interest, event extraction, by providing participants with human-created gold annotations for core entities. These annotations are briefly presented in the following. Mentions of names of genes and their products (RNA and proteins) are annotated with a single type, without differentiating between subtypes, following the guidelines of the GENIA GGP corpus (Ohta et al., 2009). This type is named P RO TEIN to maintain consistency with related tasks (e.g. BioNLP ST’09), despite slight inaccuracy for cases specifically referencing RNA or DNA forms. Two-component systems, consisting of two proteins, frequently have names derived from the names of the proteins involved (e.g. PhoP-PhoR or SsrA/SsrB). Mentions of TCSs are annotated as T WO - COMPONENT- SYSTEM, nesting P ROTEIN annotations if present. Regulons and operons are collections of genes whose expression is jointly regulated. Like the names of TCSs, their names may derive from the names of the involved genes and"
W11-1804,W11-1803,1,0.370525,"Missing"
W11-1804,W10-1919,1,0.809456,"2010 2008–2010 2008–2010 2007–2010 2008–2010 2008–2009 2007–2008 Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composi"
W11-1804,W11-1807,0,0.0968254,"Missing"
W11-1804,W11-1808,0,0.49423,"Missing"
W11-1804,W08-0609,1,0.0820893,"Entity type P ROTEIN C HEMICAL O RGANISM T WO - COMPONENT- SYSTEM prec. 54.64 32.24 90.38 87.69 rec. 39.64 19.05 47.70 47.24 F 45.95 23.95 62.44 61.40 Table 3: Automatic core entity tagging performance. Table 2: Corpus composition. Journals in which selected articles were published with number of articles (#) and publication years. following tools and settings were adopted, with parameters tuned on initial annotation for two documents: design was guided by previous studies on NER and event extraction in a closely related domain (Pyysalo et al., 2010; Ananiadou et al., 2011). P ROTEIN: NeMine (Sasaki et al., 2008) trained on the JNLPBA data (Kim et al., 2004) with threshold 0.05, filtered to only GENE and PROTEIN types. 3.1 O RGANISM: Linnaeus (Gerner et al., 2010) with “variant matching” for species names variants. Document selection The training and test data were drawn from the primary text content of recent full-text PMC open access documents selected by infectious diseases domain experts (Virginia Tech team) as representative publications on two-component regulatory systems. Table 2 presents some characteristics of the corpus composition. To focus efforts on natural language text likely to express"
W11-1804,W11-1816,1,0.465395,"Missing"
W11-1804,W09-1401,1,\N,Missing
W11-1804,W04-1213,0,\N,Missing
W11-1804,C10-1088,1,\N,Missing
W11-1812,W11-1828,0,0.0979135,"Missing"
W11-1812,W09-1402,0,0.0278135,"Missing"
W11-1812,P05-1022,0,0.0187501,"Missing"
W11-1812,de-marneffe-etal-2006-generating,0,0.214276,"Missing"
W11-1812,W11-1824,0,0.318099,"he available training data. 6 Discussion The REL task was explicitly cast in a support role for the main event extraction tasks, and REL participants were encouraged to make their predictions of the task extraction targets for the various main task datasets available to main task participants. The UTurku team responded to this call for supporting analyses, running their top-ranking REL task system on all main task datasets and making its output available as a supporting resource (Stenetorp et al., 2011). In the main tasks, we are so far aware of one application of this data: the BMI@ASU team (Emadzadeh et al., 2011) applied the UTurku REL predictions as part of their GE task system for resolving the Site arguments in events such as B IND ING and P HOSPHORYLATION (see Figure 1). While more extensive use of the data would have been desirable, we find this application of the REL analyses very appropriate to our general design for the role of the supporting and main tasks and hope to see other groups pursue similar possibilities in future work. 86 7 Conclusions We have presented the preparation, resources, results and analysis of the Entity Relations (REL) task, a supporting task of the BioNLP Shared Task 20"
W11-1812,W11-1827,0,0.0177329,"Relation matching is exact: for a submitted relation to match a gold one, both its type and the related entities must match. 5 5.1 Closky (2009), converted into Stanford Dependency form using the Stanford tools (de Marneffe et al., 2006). These specific choices may perhaps be influenced by the success of systems building on them in the 2009 shared task (e.g. Bj¨orne et al. (2009)). While UTurku (Bj¨orne and Salakoski, 2011) and VIBGhent (Van Landeghem et al., 2011) further agree in the choice of Support Vector Machines for the recognition of entities and the extraction of relations, ConcordU (Kilicoglu and Bergler, 2011) and HCMUS (Le Minh et al., 2011) pursue approaches building on dictionary- and rule-based extraction. Only the VIBGhent system makes use of resources external to those provided for the task, extracting specific semantic entity types from the GENIA corpus as well as inducing word similarities from a large unannotated corpus of PubMed abstracts. 5.2 Results Participation Table 2 summarizes the participating groups and approaches. We find a remarkable number of similarities between the approaches of the systems, with all four utilizing full parsing and a dependency representation of the syntacti"
W11-1812,W11-1801,1,0.80836,"Missing"
W11-1812,W11-1802,0,0.0934158,"256 Protein 9,297 2,080 3,589 Relation 1,857 480 497 P ROTEIN -C OMPONENT 1,302 314 334 S UBUNIT-C OMPLEX 555 166 163 Table 1: REL dataset statistics. and a complex that it is a subunit of. Following the biological motivation and the general practice in the shared task to term genes and gene products P RO TEIN for simplicity, we named these two relations P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to"
W11-1812,W11-1822,0,0.122014,"Missing"
W11-1812,W11-1811,1,0.729969,"Missing"
W11-1812,W09-1313,1,0.356791,"P RO TEIN for simplicity, we named these two relations P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to all relations of the targeted types stated within sentence scope in the GENIA corpus. For compatibility with the BioNLP ST’09 and its repeat as the GE task in 2011 (Kim et al., 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST’09 data"
W11-1812,W11-1803,1,0.742344,"Missing"
W11-1812,W09-1301,1,0.95227,"ity Relations in BioNLP ST’11. This paper presents the Entity Relations (REL) supporting task. Task Setting In the design of the REL task, we followed the general policy of the shared task in assuming named entity recognition (NER) as a given starting point: participants were provided with manually annotated gold standard annotations identifying gene/protein names in all of the training, development, and final test data. By limiting effects due to NER performance, the task remains more specifically focused on the key challenge studied. Following the results and analysis from previous studies (Pyysalo et al., 2009; Ohta et al., 2010), we chose to limit the task specifically to relations involving a gene/protein named entity (NE) and one other entity. Fixing one entity involved in each relation to an NE helps assure that the relations are “anchored” to real-world entities, and the specific choice of the gene/protein NE class further provides a category with several existing systems and substantial ongoing efforts addressing the identification of those referents through named entity recognition and normalization (Leaman and Gonzalez, 2008; Hakenberg et al., 2008; Krallinger et al., 2008; Morgan et al., 2"
W11-1812,W11-1804,1,0.69244,"Missing"
W11-1812,W11-1816,1,0.600281,"Missing"
W11-1812,I05-2038,1,0.155548,"tions P ROTEIN -C OMPONENT and S UBUNIT-C OMPLEX. Figure 1 shows an illustration of a simple relation with an associated event (not part of REL). Events with Site arguments such as that shown in the figure are targeted in the GE, EPI, and ID tasks (Kim et al., 2011b; Ohta et al., 2011; Pyysalo et al., 2011) that REL is intended to support. 3 Data The task dataset consists of new annotations for the GENIA corpus (Kim et al., 2008), building on the existing biomedical term annotation (Ohta et al., 2002), the gene and gene product name annotation (Ohta et al., 2009) and the syntactic annotation (Tateisi et al., 2005) of the corpus. The general features of the annotation are presented by Pyysalo et al. (2009), describing a previous release of a subset of the data. The REL task annotation effort extended the coverage of the previously released annotation to all relations of the targeted types stated within sentence scope in the GENIA corpus. For compatibility with the BioNLP ST’09 and its repeat as the GE task in 2011 (Kim et al., 2011b), the REL task training/development/test set division of the GENIA corpus abstracts matches that of the BioNLP ST’09 data. The statistics of the corpus are presented in Tabl"
W11-1812,W10-1921,1,0.819205,"Missing"
W11-1812,W11-1821,0,0.0685336,"Missing"
W11-1812,W09-1401,1,\N,Missing
W11-1816,I05-2038,1,\N,Missing
W11-1816,W11-1825,0,\N,Missing
W11-1816,de-marneffe-etal-2006-generating,0,\N,Missing
W11-1816,W11-1802,1,\N,Missing
W11-1816,W11-1803,1,\N,Missing
W11-1816,J93-2004,0,\N,Missing
W11-1816,W11-1812,1,\N,Missing
W11-1816,D10-1096,0,\N,Missing
W11-1816,N10-1123,0,\N,Missing
W11-1816,J08-1002,1,\N,Missing
W11-1816,J04-4004,0,\N,Missing
W11-1816,W06-2920,0,\N,Missing
W11-1816,W09-1401,1,\N,Missing
W11-1816,P96-1025,0,\N,Missing
W11-1816,D07-1111,1,\N,Missing
W11-1816,P05-1022,0,\N,Missing
W11-1816,P06-1055,0,\N,Missing
W11-1816,P08-1006,1,\N,Missing
W11-1816,W10-3006,0,\N,Missing
W11-1816,P04-1014,0,\N,Missing
W11-1816,W07-2416,0,\N,Missing
W11-1816,C10-1088,1,\N,Missing
W11-1816,W11-1809,0,\N,Missing
W11-1816,W11-1824,0,\N,Missing
W11-1816,W11-1810,0,\N,Missing
W11-1816,W11-1801,1,\N,Missing
W11-1816,W11-1804,1,\N,Missing
W12-2410,W11-1828,1,0.799541,"Missing"
W12-2410,W09-1402,1,0.89609,"Missing"
W12-2410,W10-1904,1,0.854996,"Missing"
W12-2410,P05-1022,0,0.0136407,"et VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and foll"
W12-2410,W03-1018,0,0.0217666,"ed by CKI . McCJ-parser + Stanford Conversion <nsubjpass <nn NN Serine prep_of> NN phosphorylation D IN of REL detection <auxpass NN T-bet VBZ is agent> VBN mediated IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Ea"
W12-2410,W09-1401,1,0.800261,"by existing PubMed-scale event extraction efforts. The methods and data introduced in this study are freely available from bionlp.utu.fi. 1 Introduction Biomedical domain information extraction has in recent years seen a shift from focus on the extraction of simple pairwise relations (Pyysalo et al., 2008; Tikk et al., 2010) towards the extraction of events, represented as structured associations of arbitrary numbers of participants in specific roles (Ananiadou et al., 2010). Domain event extraction has been popularized in particular by the BioNLP Shared Task (ST) challenges in 2009 and 2011 (Kim et al., 2009; Kim et al., 2011). While the BioNLP ST’09 emphasized protein interactions and regulatory relationships, the expressive event formalism can also be applied to the extraction of statements regarding the properties of individual proteins. Accordingly, the EPI (Epigenetics and Post-Translational Modifications) subchallenge of the BioNLP ST’11 provided corpora and competitive evaluations for the detection of epigenetics and post-translational modification (PTM) events, while the REL (Entity Relations) subchallenge covers structural and complex membership relations of proteins (Ohta et al., 2011b;"
W12-2410,W11-1801,1,0.780089,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,de-marneffe-etal-2006-generating,0,0.0781296,"Missing"
W12-2410,N10-1004,0,0.0126786,"d IN by . . NN CKI EPI detection TEES 2.3 <Theme <Site Entity Serine Theme> Cause> Phosphorylation phosphorylation Protein of T-bet Protein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion"
W12-2410,W10-1903,1,0.868585,"Missing"
W12-2410,W11-1803,1,0.805324,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W09-1301,1,0.907084,"Missing"
W12-2410,W11-1812,1,0.693859,"ein Catalysis is mediated by CKI . is mediated by CKI . <Protein-Component Entity Serine E Protein phosphorylation REL of T-bet tion, protein/gene names are detected and sentences are parsed. TEES handles all these preprocessing steps via a pipeline of tool wrappers for the GENIA Sentence Splitter (Kazama and Tsujii, 2003), the BANNER named entity recognizer (Leaman and Gonzalez, 2008), the McClosky-Charniak-Johnson (McCCJ) parser (Charniak and Johnson, 2005; McClosky, 2010) and the Stanford tools (de Marneffe et al., 2006). For a detailed description of TEES we refer to Bj¨orne and Salakoski (2011) and for the computational requirements of PubMed-scale event extraction to Bj¨orne et al. (2010). EPI conversion to ST format and database import Figure 1: Event and relation extraction. Article text is split into sentences (A), where gene/protein entities are detected and normalized to their Entrez Gene IDs (B). Each sentence with at least one entity is then parsed (C). EPI events and REL relations are extracted from the parsed sentences (D) and following conversion to the BioNLP ST format are imported into a database (E). (Adapted from Bj¨orne and Salakoski (2011)). The extraction of events"
W12-2410,W11-1816,1,0.886081,"Missing"
W12-2410,W10-1921,1,0.898429,"Missing"
W12-2412,W11-1828,0,0.0929443,"Missing"
W12-2412,W11-1820,0,0.0684248,"Missing"
W12-2412,P11-1098,0,0.0165262,"c span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise fol"
W12-2412,doddington-etal-2004-automatic,0,0.282956,"IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the requirement to match trigger spans 2. only require entity texts, not spans, to match The first alternative criterion has also been previously consider"
W12-2412,W11-1824,0,0.0264993,"Missing"
W12-2412,P10-1160,0,0.0661515,"Missing"
W12-2412,W11-1827,0,0.568696,"Missing"
W12-2412,W11-1801,1,0.941981,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1822,0,0.0679042,"Missing"
W12-2412,W11-1826,0,0.0323279,"Missing"
W12-2412,P11-1163,0,0.0329128,"l only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation and each event extracted by a system is considered in"
W12-2412,P09-1113,0,0.0205423,"tly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and event annotation is associated with a specific span of text. Contrasted to the alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating"
W12-2412,W11-1803,1,0.935394,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,D07-1075,0,0.0343243,"IE systems to identify a specific span of text supporting extracted information,5 the requirement of the BioNLP ST setting that the output of event extraction systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode"
W12-2412,W11-1804,1,0.922475,"ed in context to determine whether they express an event, as well as a related class of events whose type must be disambiguated with reference to context (“ambiguous type”) are comparatively frequent in the three tasks, while EPI in particular involves many cases where a trigger is shared between multiple events – an issue for approaches that assume each token can be assigned at most a single class. Finally, we noted a number of cases that we judged to be errors in the gold annotation; the number is broadly in line with the reported inter-annotator agreement for the data (see e.g. Ohta et al. (2011)). While there is an unavoidable subjective component to evaluations such as this, we note that a similar evaluation performed following the BioNLP Shared Task 2009 using test set data reached broadly comparable results (Kim et al., 2011a). The newly compiled dataset represents the first opportunity for those without direct access to the test set data and submissions to directly assess the task results, as demonstrated here. We hope that this resource will 103 New Perspectives to Event Extraction As discussed in Section 2, the BioNLP ST event extraction task is “text-bound”: each entity and ev"
W12-2412,W11-1812,1,0.828716,"ferring to the real-world entities in text, the overall task is “text-bound” in the sense of requiring not only the extraction of targeted statements from text, but also the identification of specific regions of text expressing each piece of extracted information. Events can further be marked with modifiers identifying additional features such as being explicitly negated or stated in a speculative context. Figure 1 shows an illustration of event annotations. This BioNLP ST 2009 formulation of the event extraction task was followed also in three 2011 main tasks: the GE (Kim et al., 2011c), ID (Pyysalo et al., 2011a) and EPI (Ohta et al., 2011) tasks. A variant of this representation that omits event triggers was applied in the BioNLP ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3"
W12-2412,W11-1825,0,0.02855,"P ST 2011 bacteria track (Bossy et al., 2011), and simpler, binary relationtype representations were applied in three supporting tasks (Nguyen et al., 2011; Pyysalo et al., 2011b; Jourde et al., 2011). Due to the challenges of consistent evaluation and processing for tasks involvIn this section, we present the new collection of automatically created event analyses and demonstrate one use of the data through an evaluation of events that no system could successfully extract. 3.1 Following the BioNLP ST 2011, the MSR-NLP group called for the release of outputs from various participating systems (Quirk et al., 2011) and made analyses of their system available.2 Despite the obvious benefits of the availability of these resources, we are not aware of other groups following this example prior to the time of this publication. To create the combined resource, we approached each group that participated in the three targeted BioNLP ST 2011 main tasks to ask for their support to the creation of a dataset including analyses from their event extraction systems. This suggestion met with the support of all but a few groups that were approached.3 The groups providing analyses from their systems into this merged resou"
W12-2412,D11-1001,0,0.0233845,"alternative approach where annotations are document-level only, this approach has a number of important benefits, such as allowing machine learning methods for event extraction to be directly trained on fully and specifically annotated data without the need to apply frequently errorprone heuristics (Mintz et al., 2009) or develop machine learning methods addressing the mapping between text expressions and document-level annotations (Riedel et al., 2010). Many of the most successful event extraction approaches involve direct training of machine learning methods using the textbound annotations (Riedel and McCallum, 2011; Bj¨orne and Salakoski, 2011; McClosky et al., 2011). However, while the availability of text-bound annotations in data provided to task participants is clearly a benefit, there are drawbacks to the choice of exclusive focus on text-bound annotations in system output, including issues relating to evaluation and the applicability of methods to the task. In the following section, we discuss some of these issues and propose alternatives to representation and evaluation addressing them. 4.1 Evaluation The evaluation of the BioNLP ST is instance-based and text-bound: each event in gold annotation"
W12-2412,W11-1808,0,0.31655,"Missing"
W12-2412,W11-1816,1,0.901463,"Missing"
W12-2412,H91-1059,0,0.261268,"on systems must identify specific text spans for each entity and event makes it complex or impossible to address the task using a number of IE methods that might otherwise represent feasible approaches to event extraction. 5 For example, for curation support tasks, this allows the human curator to easily check the correctness of extracted information and helps to select “evidence sentences”, as included in many databases. 104 For example, Patwardhan and Riloff (2007) and Chambers and Jurafsky (2011) consider an IE approach where the extraction targets are MUC-4 style document-level templates (Sundheim, 1991), the former a supervised system and the latter fully unsupervised. These methods and many like them for tasks such as ACE (Doddington et al., 2004) work on the document level, and can thus not be readily applied or evaluated against the existing annotations for the BioNLP shared tasks. Enabling the application of such approaches to the BioNLP ST could bring valuable new perspectives to event extraction. 4.3 Alternative evaluation We propose a new mode of evaluation that otherwise follows the primary BioNLP ST evaluation criteria, but incorporates the following two exceptions: 1. remove the re"
W12-2412,W10-1921,1,0.866603,"Missing"
W12-2412,W11-1821,0,0.0439833,"Missing"
W12-2412,W11-0204,0,0.0629482,"Missing"
W12-2412,W11-1805,0,0.0284448,"Missing"
W12-2412,W11-1802,1,\N,Missing
W12-2412,W11-1809,0,\N,Missing
W12-2412,W11-1811,1,\N,Missing
W12-2412,W11-1810,0,\N,Missing
W12-3806,W11-1828,0,0.0482957,"Missing"
W12-3806,W10-3001,0,0.0645895,"Missing"
W12-3806,W10-3010,0,0.161775,"sk, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine how cue-and-scope recognition systems can be used to produce a state-of-the-art negation/speculation detection system for the EE task. 2 Resources Several existing resources can support the inv"
W12-3806,W11-1827,0,0.0114523,"us used to train the CLiPSNESP system, the GE test set does not, and thus test set results are not expected to be overfit. We noted when performing development set experiments that training machine learning-based methods on the negation/speculation annotations of the event-annotated corpora was problematic due to the sparseness of these flags in the annotation. To address this issue, we merge the training data of the three corpora in all experiments with machine learning methods. 5.2 Baseline methods We use the event analyses created by the UTurku (Bj¨orne and Salakoski, 2011) and UConcordia (Kilicoglu and Bergler, 2011) systems for the BioNLP 2011, the only systems that included negation and speculation analyses. To investigate the impact on a system that did not include a negation/speculation component, we further consider analyses created Negation (R/P/F) EPI GE ID H HR 29.23/31.67/30.40 27.69/32.73/30.00 53.92/52.84/53.38 53.24/71.89/61.18 44.00/31.88/36.97 44.00/37.93/40.74 M ME MC MCE 47.69/20.00/28.18 60.00/66.10/62.90 40.00/74.29/52.00 58.46/73.08/64.96 43.00/25.25/31.82 58.36/70.08/63.69 58.36/76.34/66.15 61.77/83.03/70.84 46.00/26.74/33.82 54.00/69.23/60.67 52.00/61.90/56.52 58.00/70.73/63.74 Table"
W12-3806,W09-1401,1,0.94131,"ion/Speculation Annotations: A Bridge Not Too Far Pontus Stenetorp1 Sampo Pyysalo2,3 Tomoko Ohta2,3 Sophia Ananiadou2,3 and Jun’ichi Tsujii2,3,4 1 Department of Computer Science, University of Tokyo, Tokyo, Japan 2 School of Computer Science, University of Manchester, Manchester, United Kingdom 3 National Centre for Text Mining, University of Manchester, Manchester, United Kingdom 4 Microsoft Research Asia, Beijing, People’s Republic of China {pontus,smp,okap}@is.s.u-tokyo.ac.jp sophia.ananiadou@manchester.ac.uk jtsujii@microsoft.com Abstract some marking of certainty and polarity (LDC, 2005; Kim et al., 2009; Saur and Pustejovsky, 2009; Kim et al., 2011a; Thompson et al., 2011). We study two approaches to the marking of extra-propositional aspects of statements in text: the task-independent cue-and-scope representation considered in the CoNLL-2010 Shared Task, and the tagged-event representation applied in several recent event extraction tasks. Building on shared task resources and the analyses from state-of-the-art systems representing the two broad lines of research, we identify specific points of mismatch between the two perspectives and propose ways of addressing them. We demonstrate the feas"
W12-3806,W11-1801,1,0.855017,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1802,0,0.0691394,"Missing"
W12-3806,W11-1806,0,0.142886,"2.43 48.08/51.02/49.50 25.65/10.84/15.24 22.08/42.24/29.00 27.27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data."
W12-3806,W09-1304,0,0.0270192,"icallyoriented and task-oriented perspectives on negation/speculation detection. In this study, we make use of the following resources. First, we study the three BioNLP 2011 Shared Task corpora that include annotation for negation and speculation: the GE, EPI and ID main task corpora (Table 1). Second, we make use of supporting analyses provided for these corpora in response to a call sent by the BioNLP Shared Task organisers to the developers of third-party systems (Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking t"
W12-3806,W10-3006,0,0.158592,"Missing"
W12-3806,W11-1803,1,0.674875,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1804,1,0.674411,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W12-2412,1,0.850965,"(Stenetorp et al., 2011). Specifically, we use the output of the BiographTA NeSp Scope Labeler (here referred to as CLiPS-NESP) (Morante and Daelemans, 2009; Morante et al., 2010) provided by the University of Antwerp CLiPS center. This system provides cue-and-scope analyses for negation and speculation and was demonstrated to have state-of-the-art performance at the relevant CoNLL-2010 Shared Task. Finally, we make use of the event analyses created by systems that participated in the BioNLP Shared Task, made available to the research community for the majority of the shared task submissions (Pyysalo et al., 2012). These analyses represent the stateof-the-art in event extraction and their capability to detect event structures as well as marking them for negation and speculation. The above three resources present us with many opportunities to relate scope-based annotations to three highly relevant event-based corpora containing negation/speculation annotations. 3 Manual Analysis To gain deeper insight into the data and the challenges in combining the cue-and-scope and eventoriented perspectives, we performed a manual analysis of the corpus annotations using the manually Name Negated Events Speculated Ev"
W12-3806,W11-1807,0,0.0123498,"27/50.30/35.37 31.82/53.85/40.00 45.83/10.58/17.19 29.17/28.00/28.57 37.50/31.03/33.96 33.33/42.11/37.21 Table 5: Results for Speculation for our two heuristics and the four combinations of ML features. by the FAUST system, which achieved the highest performance at two of the three tasks considered (Riedel et al., 2011). The UTurku system is a pipeline ML-based EE system, while the UConcordia system is strictly rule-based. FAUST is an ML-based model combination system incorporating information from the parser-based Stanford system (McClosky et al., 2011) and the jointly-modelled UMass system (Riedel and McCallum, 2011). We also performed preliminary experiments for the other released submissions to the BioNLP 2011 Shared Task, but due to space limitations focus only on the three above-mentioned systems. results tables we abbreviate the feature set names as done in Table 3 and use H for the heuristic method and R for its root extension. As our machine learning component we use LIBLINEAR (Fan et al., 2008) with a L2-regularised L2-loss SVM model. We optimise the SVM regularisation parameter C using 10-fold cross-validation on the training data. We use the training, development and test set partition provided"
W12-3806,W11-1808,0,0.183789,"addressing the latter task in detail. Only three out of the 24 participants in the BioNLP Shared Task 2009 submitted results for the non-mandatory negation/speculation task, and although negation and speculation were also considered in three main tasks for the 2011 follow-up event (Kim et al., 2011a), the trend continued, with only two participants addressing the negation/speculation aspects of the task. We are aware of only two studies exploring the relationship between the cue-and-scope and event-based representations: in a manual analysis of scope overlap with tagged events, Vincze et al. (2011) identified a number of issues and mismatches in annotation scope and criteria, which may explain in part the lack of methods combining these two lines of research. Kilicoglu and Bergler (2010) approached the problem from the opposite direction and used an existing EE system to extract cue-and-scope annotations in the CoNLL-2010 Shared Task. In this work, we take a high-level perspective, 48 seeking to bridge the linguistically oriented framework and the more application-oriented event framework to overcome the mismatches demonstrated by Vincze et al. (2011). Specifically, we aim to determine"
W12-3806,W11-1816,1,0.908546,"Missing"
W12-3806,W08-0606,0,0.206797,"ons of text statements have explicitly included Although extra-propositional aspects are recognised as important, there is no clear consensus on how to address their annotation and extraction from text. Some comparatively early efforts focused on the detection of negation cue phrases associated with specific (previously detected) terms through regular expression-based rules (Chapman et al., 2001). A number of later efforts identified the scope of negation cues with phrases in constituency analyses in sentence structure (Huang and Lowe, 2007). Drawing in part on this work, the BioScope corpus (Vincze et al., 2008) applied a representation where both cues and their associated scopes are marked as contiguous spans of text (Figure 1 bottom). This approach was also applied in the CoNLL-2010 Shared Task (Farkas et al., 2010), in which 13 participating groups proposed approaches for Task 2, which required the identification of uncertainty cues and their associated scopes in text. In the following, we will term this task-independent, linguisticallymotivated approach as the cue-and-scope representation (please see Vincze et al. (2008) for details regarding the representation). For IE efforts, more task-oriente"
W12-4304,H92-1045,0,0.108814,"g. For PubMed, we simply selected a random set of citations and extracted their abstract and title texts. For PMC, we initially extracted all non-overlapping section texts (PMC XML <sec> elements) as well as caption texts (<caption> elements), and then selected a random set of extracts. This selection seeks to maximize the diversity of the texts in the full-text section of the corpus, and the selection of extracts larger than isolated sentences aims to allow the corpus to be used to study methods making use of broader context, e.g. by incorporating constraints such as one sense per discourse (Gale et al., 1992). 2 3 We selected a total of 500 documents using this protocol, half from PubMed and half from PMC document extracts. (Descriptive statistics of the abstracts and full-text extracts subcorpora are given later in Table 3.) 2.6 Annotation Process Primary annotation was created by a PhD biologist with extensive experience in domain information extraction and text annotation (TO). The use of any relevant resources, such as the full article being annotated or species-specific anatomy ontologies in the OBO foundry, was encouraged for resolving unclear or ambiguous cases during annotation. Initial an"
W12-4304,W10-1909,0,0.127256,"rehensive analysis must include entities at multiple levels of biological organization, from the molecular to the organism level. The detection of references to anatomical entities such as “kidney” and “blood” is thus required for the automatic structured analysis of biomedical scientific text. Although a wealth of lexical and ontological resources covering anatomical entities are available (Rosse and Mejino, 2003; Smith et al., 2007; Bodenreider, 2004; Haendel et al., 2009), such resources do not alone confer the ability to reliably detect mentions of anatomical entities in natural language (Gerner et al., 2010a; Travillian et al., 2011; Pyysalo et al., 2012b). To support the development and evaluation of reliable anatomical entity mention detection methods, corpus resources annotated specifically for the task are necessary. In this study, we aim to create a reference standard for evaluating methods for anatomical entity mention detection and for training machine learningbased methods for the task. We seek to select a set of texts that are representative of the relevant scientific literature, i.e. open-domain in the sense of avoiding bias toward, for example, specific species, levels of biological o"
W12-4304,W04-1213,0,0.115942,"ue to occupying different levels at different stages of development, we adopt a separate D EVELOPING ANATOMICAL STRUCTURE category, as done also in e.g. Uberon (Haendel et al., 2009). 1 http://obofoundry.org/ 28 Annotation Scope We diverge from the scope of anatomy ontologies in two important aspects in our annotation. First, ontologies of anatomy commonly incorporate everything from molecules to whole organisms within their scope. However, in entity mention detection, many molecular level anatomical entities fall within the scope of the established gene/protein mention detection tasks (e.g. (Kim et al., 2004; Tanabe et al., 2005)), and whole organism mentions similarly largely within what is covered by existing methods and resources for organism mention detection (Gerner et al., 2010b; Naderi et al., 2011). To avoid overlap with established tasks and to focus on the novel aspects of anatomical entity mention detection, we exclude biological macromolecules and mentions of organism names from the scope of our annotation, as argued in (Pyysalo et al., 2012b). Second, these ontologies typically represent canonical anatomy, an idealized state that is rarely (if ever) encountered in reality (Bada and H"
W12-4304,W11-1901,0,0.0257235,"mical entities (e.g. “muscle tissue”). Both names and nominal mentions are annotated similarly, without distinction. We exclude pronouns (it, that) from annotation even when they un29 cytoplasm Tissue Part-of Cell of phagocytic microglia Frag Tissue thyroid and eye muscle membranes Figure 2: Part-of relation marking entity mention spanning a prepositional phrase (above) and Frag relation marking coordination with ellipsis (below). ambiguously refer to an anatomical entity; we consider the identification and resolution of such mentions part of the distinct coreference resolution task (see e.g. Pradhan et al. (2011)). In addition to names and nominal mentions, we mark adjectives that have an unambiguous sense of relating to a specific anatomical entity. Thus, for example, both “kidney” and “renal” (relating to the kidneys) are annotated as O RGAN in expressions such as “kidney failure” and “renal failure”. The choice to annotate adjectival references is motivated by the expected needs of applications making use of automatically detected anatomical entity mentions. For example, for semantic search targeting documents relating to organ failure, a document discussing “renal failure” is obviously relevant an"
W12-4304,E12-2021,1,0.874312,"Missing"
W12-4304,W03-0419,0,0.0245807,"Missing"
W13-2001,W11-1802,1,0.929598,"tand-off: the texts of the documents are kept separate from the annotations that refer to specific spans of texts through character offsets. More detail and examples can be found on the BioNLP-ST’13 web site. 2.1 Genia Event Extraction (GE) Originally the design and implementation of the GE task was based on the Genia event corpus (Kim et al., 2008) that represents domain knowledge of NFκB proteins. It was first organized as the sole task of the initial 2009 edition of BioNLP-ST (Kim et al., 2009). While in 2009 the data sets consisted only of Medline abstracts, in its second edition in 2011 (Kim et al., 2011b), it was extended to include full text articles to measure the generalization of the technology to full text papers. For its third edition this year, the GE task is organized with the goal of making it a more “real” task useful for knowledge base construction. The first design choice is to construct the data sets with recent full papers only, so that the extracted pieces of information could represent up-to-date knowledge of the domain. Second, the coreference annotations are integrated into the event annotations, to encourage the use of these co-reference features in the solution of the eve"
W13-2001,P05-1022,0,0.0190745,"Date 4 Participation GE 1-2-3 EVEX BioNLP-ST’13 organization BioNLP-ST’13 was split in three main periods. During thirteen weeks from mid-January to the first week of April, the participants prepared their systems with the training data. Supporting resources were delivered to participants during this period. Supporting resources were provided by the organizers and by three external providers after a public call for contribution. They range from tokenizers to entity detection tools, mostly focusing on syntactic parsing (Enju (Miyao and Tsujii, 2008), Stanford (Klein and Manning, 2002), McCCJ (Charniak and Johnson, 2005)). The test data were made available for 10 days before the participants had to submit their final results using on-line services. The evaluation results were TEES-2.1 • • BioSEM • NCBI • DlutNLP • HDS 4NLP • NICTA • USheff • UZH • HCMUS • • • • • CG PC GRO GRN BB 1 - 2-3 • • NaCTeM • • NCBI • RelAgent • UET-NII • ISI • OSEE U. of Ljubljana K.U. Leuven IRISATexMex Boun • • • • • • • • • • • • • • LIPN • LIMSI • • • Table 3: Participating teams per task. BioNLP-ST 2013 received 38 submissions from 22 teams (Table 3). One third, or seven teams, participated in multiple tasks. Only one team, UTur"
W13-2001,W11-0214,1,0.829223,"t Error Rate (Makhoul et al., 1999) that is more adapted to graph comparison than the usual Recall, Precision and F-score measures. Pathway Curation (PC) The PC task focuses on the automatic extraction of biomolecular reactions from text with the aim of supporting the development, evaluation and maintenance of biomolecular pathway models. The PC task setting and its document selection protocol account for both signaling and metabolic pathways. The 23 event types, including chemical modifications (Pyysalo et al., 2011b), are defined primarily with respect to the Systems Biology Ontology (SBO) (Ohta et al., 2011b; Ohta et al., 2011c), involving 4 SBO entity types. The PC task corpus was newly annotated for the task and consists of 525 PubMed abstracts, chosen for the relevance to specific pathway reactions selected from SBML models registered in BioModels and PANTHER DB repositories (Mi and Thomas, 2009). The corpus was manually annotated for over 12,000 events on top of close to 16,000 entities. 2.4 Gene Regulation Network in Bacteria (GRN) 2.6 Bacteria Biotopes (BB) The Bacteria Biotope (BB) task concerns the extraction of locations in which bacteria live and the categorization of these habitats wi"
W13-2001,W12-4304,1,0.844492,"Missing"
W13-2001,W09-1401,1,0.905104,"Missing"
W13-2001,W11-1801,1,0.712795,"Missing"
W13-2001,J08-1002,0,\N,Missing
W13-2001,W11-0215,1,\N,Missing
W13-2001,W11-1810,1,\N,Missing
W13-2008,W11-1828,0,0.0112182,"Missing"
W13-2008,W13-2003,0,0.0164694,"Missing"
W13-2008,W11-1826,0,0.0290686,"Missing"
W13-2008,P05-1022,0,0.140045,"Missing"
W13-2008,W13-2010,0,0.0333957,"Missing"
W13-2008,W11-1806,0,0.0334856,"Missing"
W13-2008,W11-1816,1,0.890054,"Missing"
W13-2008,W13-2012,1,0.200549,"Missing"
W13-2008,J08-1002,0,0.0344355,"Missing"
W13-2008,W12-4304,1,0.922588,"ns. For other categories of annotation, correct (gold standard) annotations are provided also for test data. 2.1 Table 1: Entity types. Indentation corresponds to is-a structure. Labels in gray identify groupings defined for organization only, not annotated types. Development Theme Cancer Equiv Cancer progression of chronic myeloid leukemia (CML) Figure 2: Example Equiv relation. drawn primarily from the Common Anatomy Reference Ontology (Haendel et al., 2008), a small, species-independent upper-level ontology based on the Foundational Model of Anatomy (Rosse and Mejino Jr, 2003). We refer to Ohta et al. (2012) for more detailed discussion of the anatomical entity type definitions. Entities The entity types defined in the CG task are shown in Table 1. The molecular level entity types largely match the scope of types such as P ROTEIN and C HEMICAL included in previous ST tasks (Kim et al., 2012; Pyysalo et al., 2012b). However, the CG types are more fine grained, and the types P RO TEIN DOMAIN OR REGION and DNA DOMAIN OR REGION are used in favor of the non-specific type E NTITY, applied in a number of previous tasks for additional event arguments (see Section 2.3). The definitions of the anatomical e"
W13-2008,W13-2011,0,0.0422743,"Missing"
W13-2008,D07-1111,0,0.0149447,"Missing"
W13-2008,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2008,W11-1801,1,\N,Missing
W13-2008,E12-2021,1,\N,Missing
W13-2009,P05-1022,0,0.013255,"could provide further insight into the relative strengths and weaknesses of these two systems. Table 6: Primary evaluation results Event Extraction System8 (Bj¨orne et al., 2011) (TEES). The two systems share the same overall architecture, a one-best pipeline with SVMbased stages for event trigger detection, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMi"
W13-2009,W08-0608,0,0.0128679,"Missing"
W13-2009,W11-1828,0,0.0171244,"Missing"
W13-2009,W11-0214,1,0.596369,"in interactions (Krallinger et al., 2007; Pyysalo et al., 2008; Tikk et al., 2010). However, most such efforts have employed simple representations, such as entity pairs, that are not sufficient for capturing molecular reactions to the level of detail required to support the curation of pathway models. Additionally, previous efforts have not directly involved the semantics (e.g. reaction type definitions) of such models. Perhaps in part due to these reasons, natural language processing and information extraction methods have not been widely embraced by biomedical pathway curation communities (Ohta et al., 2011c; Ohta et al., 2011a). We believe that the extraction of structured event representations (Figure 1) pursued in the BioNLP Shared Tasks offers many opportunities to make significant contributions to support the development, evaluation and maintenance of biomolecular pathways. The Pathway Curation (PC) task, a main task of the BioNLP Shared Task 2013, is proposed as a step toward realizing these opportunities. The PC task aims to evaluate the applicability of event extraction systems to pathway curation and to encourage the further development of methods for related tasks. The design of the ta"
W13-2009,W12-4304,1,0.900591,"Missing"
W13-2009,W11-0215,1,0.854069,"iyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event extraction tasks over the last years and have shown consistently competitive performance with the state of the art (Bj¨orne and Salakoski, 2011; Miwa et al., 2012). It is thus reasonable to assume that the higher performance achieved by the Evaluation results Table 6 summarizes the primary evaluation results. The two systems demonstrate broadly similar performance in terms of F-scores, wi"
W13-2009,D07-1111,1,0.639577,"prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of"
W13-2009,I05-2038,1,0.757182,"s of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011), Ohta et al. (2011b) and Ohta et al. (2011c). 4.2 5 Although participation in this initial run of the PC task was somewhat limited, the two participating systems have been applied to a large variety of event"
W13-2009,J08-1002,1,0.790519,"ion, triggerargument relation detection, argument grouping into event structures, and modification prediction. The feature representations of both systems draw on substructures of dependency-like representations of sentence syntax, derived from full parses of input sentences. TEES applies the Charniak and Johnson (2005) parser with the McClosky (2009) biomedical model, converting the phrasestructure parses into dependencies using the Stanford tools (de Marneffe et al., 2006). By contrast, EventMine uses a combination of the predicateargument structure analyses created by the deep parser Enju (Miyao and Tsujii, 2008) and the output of the the GDep best-first shift-reduce dependency parser (Sagae and Tsujii, 2007). All three parsers have models trained in part on the biomedical domain GENIA treebank (Tateisi et al., 2005). Interestingly, both systems make use of the GE task data, but the application of EventMine extends on this considerably by applying a stacked model (Miwa et al., 2013b) with predictions also from models trained on the BioNLP ST 2011 EPI and ID tasks (Pyysalo et al., 2012) as well as from four corpora introduced outside of the shared tasks by Thompson et al. (2011), Pyysalo et al. (2011),"
W13-2009,de-marneffe-etal-2006-generating,0,\N,Missing
W13-2009,W09-1401,1,\N,Missing
W13-2009,E12-2021,1,\N,Missing
W15-1815,P14-2048,0,0.0417197,"Missing"
W15-1815,P13-1157,0,0.0297284,"similar studies include Ilisei et al. (2010) presenting a languageindependent system based on average sentence length or lexical richness, Popescu (2011) using solely character 5-grams (ignoring sentence boundaries) to detect English translations, and Avner, Ordan and Wintner (2014) concentrating on morphological properties in Hebrew. Previous studies on classifying machine translated texts mostly rely on different combinations of lexical and grammatical features as well. Aharoni, Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their relative infrequency, some previous work also concentrate on classifying informality. Unlike those concerning translation classification, these concentrate on lexical rather than morphological features. Lahiri, Mitra, and Lu (2011) explore the Formality Score, a frequency list based on the differences of word classes in a corpus. Mosquera and Moreda (2011) define the most relevant features of informality to be the frequencies of spelling mistakes, interjections,"
W15-1815,S13-1035,0,0.049393,"nalyses. The current version consists of 3.2 billion tokens and 241 million sentences. This article has two main objectives. The first aim is to develop classification methods in order to detect informality, machine translations and human translations from the Parsebank. This would facilitate the use of the Parsebank, as searches or applications could be targeted only at certain parts of the corpus. In the classification, the features used include syntactic n-grams, little subtrees of dependency syntax analyses developed for Finnish by Kanerva et al. (2014), originally produced for English by Goldberg and Orwant (2013). Secondly, the study points research directions for the analysis of the linguistic characteristics of the text classes. The automatic classification based on the data-driven combination of lexical, syntactic and morphological features offers a new approach to the linguistic study of these texts and their characteristics, as traditional linguistic studies often concentrate on the analysis of a limited number of preselected features. The study consists of three sets of classification experiments and their analyses. In the first, texts are classified according to the level of formality to standa"
W15-1815,J03-3001,0,0.0493192,"ation performs well for the indomain experiments, delexicalized methods with morpho-syntactic features prove to be more tolerant to variation caused by genre or source language. In addition, the results show that the features used in the classification provide interesting pointers for further, more detailed studies on the linguistic characteristics of these texts. 1 Introduction With its growing size and coverage, the Internet has become an attractive source of material for linguistic resources, used both for linguistics and natural language processing (NLP) applications (Baroni et al., 2009; Kilgarriff and Grefenstette, 2003). However, automatically collected, very large corpora covering all the text that can be found are very heterogeneous, which may complicate their usage. In linguistics, the origin of the corpus texts is of primary importance (Biber et al., 1998; Sinclair, 1996), and also in many NLP applications, such as automatic syntactic analysis, linguistic variation across different domains affects the results significantly (Laippala et al., 2014). This paper presents the first results on the linguistic variation in the Finnish-language Internet by analyzing informality, machine translations and human tra"
W15-1815,W11-4523,0,0.0303982,"Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their relative infrequency, some previous work also concentrate on classifying informality. Unlike those concerning translation classification, these concentrate on lexical rather than morphological features. Lahiri, Mitra, and Lu (2011) explore the Formality Score, a frequency list based on the differences of word classes in a corpus. Mosquera and Moreda (2011) define the most relevant features of informality to be the frequencies of spelling mistakes, interjections, and emoticons. These same individual features have also been studied as signs of informality in many linguistic studies (Lehti and Laippala, 2014). 3 3.1 Data Finnish Internet Parsebank and Syntactic N-grams The current version of the Finnish Internet Parsebank consists of 3.2 billion tokens and 241 million sentences. It is produced by crawling the Finnish web with the Spiderling web crawler2 . Being designed for collecting text corpora, it can be targeted to crawl only pages in a speci"
W15-1815,R11-1091,0,0.255473,"l of translated texts. Baker (1993) was the first to define potential translation universals: features that all translated texts hypothetically share. The existence of translationese has also been tested by studies applying machine learning. Baroni and Bernardini (2006) use monolingual corpora to experiment with for instance lemmas and POS tags, providing evidence that an algorithm can perform better than humans in recognizing human translated texts. Other similar studies include Ilisei et al. (2010) presenting a languageindependent system based on average sentence length or lexical richness, Popescu (2011) using solely character 5-grams (ignoring sentence boundaries) to detect English translations, and Avner, Ordan and Wintner (2014) concentrating on morphological properties in Hebrew. Previous studies on classifying machine translated texts mostly rely on different combinations of lexical and grammatical features as well. Aharoni, Koppel, and Goldberg (2014) use a set of function words, POS tags and a mix of the two to classify texts, whereas Arase and Zhou (2013) concentrate on indicators based on sentenceinternal coherence, also called the phrase salad phenomenon (Lopez, 2008). Despite their"
W15-1821,C12-1015,0,0.076097,"Missing"
W15-1821,C10-1011,0,0.215062,"atures to the Marmot tagger. We initially apply a hard constraint approach, where the output of the tagger is used to select one of these readings (the reading with the highest overlap of tags and a priority for readings matching the main POS), effectively disambiguating OMorFi output. For words not recognized by OMorFi, the reading produced by Marmot is used as-is, and the wordform itself is used in place of the lemma. This has so far been the strategy taken when learning to parse Finnish (Bohnet et al., 2013). The tagged text is then parsed with the Mate tools graph-based dependency parser (Bohnet, 2010).7 As baseline, we consider the most recent Finnish dependency parser trained and evaluated on the original distribution of TDT. Note that the test sets differ: the baseline is evaluated on a test set matching the data it was trained on, which differs from the new test set in several aspects such as the treatment of named entities. The results are thus broadly comparable, but not directly so. 6 http://turkunlp.github.io/ Finnish-dep-parser/ 7 https://code.google.com/p/mate-tools Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015) 169 Baseline (Haverinen et al"
W15-1821,Q13-1034,1,0.905517,"Missing"
W15-1821,W13-2308,0,0.0432522,"nd resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). We also provide a first set of experiments comparing the parsing scores of language-specific treebank annotation to that of a UD treebank, providing an evaluation of both the conversion quality and the feasibility of UD annotation as a parsing target. In a related but separate effort within the UD initiativ"
W15-1821,de-marneffe-etal-2014-universal,1,0.919781,"Missing"
W15-1821,de-marneffe-etal-2006-generating,0,0.146088,"Missing"
W15-1821,E12-1007,0,0.0564683,"Missing"
W15-1821,W13-5609,1,0.649864,"ilable under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). We also provide a first set of experiments comparing the parsing scores of language-specific treebank annotation to that of a UD treebank, providing an evaluation of both the conversion quality and the feasibility of UD annotation as a parsing target. In a related but separate effort within the UD initiative, the FinnTreeBank 12 ("
W15-1821,W14-4606,0,0.0564409,"Missing"
W15-1821,P13-2017,0,0.0766526,"Missing"
W15-1821,D13-1032,0,0.0980004,"Missing"
W15-1821,silveira-etal-2014-gold,0,0.0495424,"Missing"
W15-1821,simi-etal-2014-less,0,0.0409146,"Missing"
W15-1821,E12-2021,1,0.893639,"Missing"
W15-1821,P13-2103,0,0.023125,"f-the-art parser trained on a languagespecific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously introduced Turku Dependency Treebank (TDT) (Haverinen et al., 2013b). W"
W15-1821,zeman-2008-reusable,0,0.308489,"y present parsing experiments comparing the performance of a stateof-the-art parser trained on a languagespecific annotation schema to performance on the corresponding UD annotation. The results show improvement compared to the source annotation, indicating that the conversion is accurate and supporting the feasibility of UD as a parsing target. The introduced tools and resources are available under open licenses from http://bionlp.utu.fi/ud-finnish.html. 1 UD builds on the Google Universal part-ofspeech (POS) tagset (Petrov et al., 2012), the Interset interlingua of morphosyntactic features (Zeman, 2008), and Stanford Dependencies (de Marneffe et al., 2006; Tsarfaty, 2013; de Marneffe et al., 2014). In addition to the abstract annotation scheme, UD defines also a treebank storage format, CoNLL-U. A first version of UD treebank data, building on the Google Universal Dependency Treebanks (McDonald et al., 2013) and many other previously released resources (Bosco et al., 2013; Haverinen et al., 2013b), was recently released1 (Nivre et al., 2015). In this paper, we present the adaptation of the UD guidelines to Finnish and the creation of the UD Finnish treebank by conversion of the previously in"
W15-1821,petrov-etal-2012-universal,0,\N,Missing
W15-2124,de-marneffe-etal-2014-universal,1,0.859804,"Missing"
W15-2124,S13-1035,0,0.0159772,"les and by using the dependency syntax information to analyze the language of the web corpus. We conclude with a discussion of the requirements of extending from this case study on Finnish to create consistently annotated web-scale parsebanks for a large number of languages. 1 Introduction The enormous potential of the web as a source of material for linguistic research in a wide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 2012). interface, thus building a large-scale corpus and pairing it with the tools necessary for its efficient use. Using real-world examples, we show how the large web corpus"
W15-2124,C12-1015,0,0.0537307,"Missing"
W15-2124,J03-3001,0,0.698978,"innish web-scale parsebank. We further integrate this data into an online dependency search system and demonstrate its applicability by showing linguistically motivated search examples and by using the dependency syntax information to analyze the language of the web corpus. We conclude with a discussion of the requirements of extending from this case study on Finnish to create consistently annotated web-scale parsebanks for a large number of languages. 1 Introduction The enormous potential of the web as a source of material for linguistic research in a wide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 201"
W15-2124,Q13-1034,1,0.915741,"Missing"
W15-2124,C10-1011,0,0.0765138,"Missing"
W15-2124,N15-3011,1,0.802082,"h the basic this data (primarily) by deterministic conversion, the results are thus not fully comparable with results for the UD Finnish corpus. 6 Note that results are for the original SD annotation of the TDT corpus. While the UD Finnish treebank is created from 214 Figure 3: A screenshot of the online query interface, showing a simple query for transitive verbs. Item All tokens Lemma count Sentence count Unique token count Unique sentence count Tokens without duplicates Number 3,662,727,698 28,585,422 275,690,022 39,688,642 178,547,962 2,554,094,599 and the extended layers of the analysis (Luotolahti et al., 2015). This detailed corpus search enables fast and easy retrieval of material for many linguistic questions that otherwise would require manual work to address. The query system allows search for any arbitrary subtree structure, including arbitrarily nested negations. For instance, one can search for verbs which have their subject in the partitive case, unless that subject has a numeral modifier, and unless the verb is governed by the clausal complement relation. In addition to the constraints on the syntactic structure, any combination of normal and negated constraints on the morphology of the wo"
W15-2124,W14-4606,0,0.0200094,"ide range of areas is well established (Kilgarriff and Grefenstette, 2003), with many new opportunities created by web-scale resources ranging from simple N -grams (Brants and Franz, 2006) to syntactically analyzed text (Goldberg and Orwant, 2013). Yet, while the use of multilingual web data to support linguistic research is well recognized (Way 1 http://universaldependencies.github. io/docs/ 211 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 211–220, Uppsala, Sweden, August 24–26 2015. corpora, which range in size from 24,000 tokens (Irish) (Lynn et al., 2014) to over 1,5 million tokens (Czech) (Bejˇcek et al., 2012). interface, thus building a large-scale corpus and pairing it with the tools necessary for its efficient use. Using real-world examples, we show how the large web corpus with the syntactic annotation can be used for gathering data on rare phenomena in linguistic research. For linguistic research web corpora, containing broad scope of text, are well suited for the search of rare linguistics constructs as well as those which do not often appear on official text, such as the use of colloquial terms and structures. Other motivations beyond"
W15-2124,W15-1821,1,0.872646,"Missing"
W15-2124,D13-1032,0,0.0617608,"Missing"
W15-2124,E12-2021,1,0.855308,"Missing"
W15-2124,J03-3004,0,0.046451,"Missing"
W15-2124,W13-3728,1,0.905182,"Missing"
W15-2124,petrov-etal-2012-universal,0,0.057271,"Missing"
W15-2124,W11-4644,0,0.0406162,"Missing"
W15-2124,prasad-etal-2008-penn,0,0.0160259,"ubstantially increase the size of any language-specific corpus over that created here, we expect the total computational cost of scaling from one language to ten to be simply an order of magnitude greater than that here. Thus, we estimate that the total computational cost of creating the first set of UD web parsebanks to be on the order of 100,000 CPU core hours. While this is a non-trivial cost, it is well within our resources. be used to retrieve material to study phenomena crossing the limits of individual sentences, such as semantic relations between text elements and discourse structure (Prasad et al., 2008; Laippala et al., 2015). As the search tool allows the restriction of the query to certain sentence elements, it can be delimited to sentence-initial elements, such as sentence-initial, individual conjunctions that instead of co-ordinating sentence-internal clauses or phrases refer to previous text elements and express relations between sentences and the discourse structure. This can provide useful information both on the frequency of different conjunctions used in this position and on discourse structure more in general. The distribution of the most frequently used conjunctions in this funct"
W16-2501,N09-1003,0,0.644427,"Missing"
W16-2501,J15-4004,1,0.698951,"erform and they are often used to estimate the quality of representations before using them in downstream applications. The underlying assumption is that intrinsic evaluations can, to some degree, predict extrinsic performance. In this study, we demonstrate that this assumption fails to hold for many standard datasets. We generate a set of word representations with varying context window sizes and compare their performance in intrinsic and extrinsic evaluations, showing that these evaluations yield mutually inconsistent results. Among all the benchmarks explored in our study, only SimLex-999 (Hill et al., 2015) is a good predictor of downstream performance. This may be related to the fact that it stands out among other benchmark datasets in distinguishing highly similar concepts (male, man) from highly related but dissimilar ones (computer, keyboard). The quality of word representations is frequently assessed using correlation with human judgements of word similarity. Here, we question whether such intrinsic evaluation can predict the merits of the representations for downstream tasks. We study the correlation between results on ten word similarity benchmarks and tagger performance on three standard"
W16-2501,P06-4018,0,0.00545276,"2012) Radinsky et al. (2011) Halawi et al. (2012) Luong et al. (2013) Hill et al. (2015) Table 2: Intrinsic evaluation datasets 2.2 Corpora and Pre-processing To create word vectors, we gather a large corpus of unannotated English text, drawing on publicly available resources identified in word2vec distribution materials. Table 1 lists the text sources and their sizes. We extract raw text from the Wikipedia dump using the Wikipedia Extractor2 ; the other sources are textual. We pre-process all text with the Sentence Splitter and the Treebank Word Tokenizer provided by the NLTK python library (Bird, 2006). In total, there are 3.8 billion tokens (19 million distinct types) in the processed text. 2.3 #Tokens (Train/Test) 337,195 / 129,892 211,727 / 47,377 203,621 / 46,435 2.5 Extrinsic evaluation To evaluate the word representations in downstream tasks, we use them in three standard sequence labeling tasks selected by Collobert et al. (2011): POS tagging of Wall Street Journal sections of Penn Treebank (PTB) (Marcus et al., 1993), chunking of CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000), and NER of CoNLL’03 shared task data (Tjong Kim Sang and De Meulder, 2003). We use the stand"
W16-2501,P14-2050,0,0.0258938,"ext windows, representations tend to capture the topic or domain or a word, while smaller windows tend to emphasize the learning of word function. This is because the role/function of a word is categorized by its proximate syntactic context, while a large window captures words that are less informative for this categorization (Turney, 2012). For example, in the sentence Australian scientist discovers star with telescope, the context of the word discovers in a window of size 1 includes scientist and star, while a larger context window will include more words related by topic such as telescope (Levy and Goldberg, 2014). The association of large window sizes with greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect f"
W16-2501,Q15-1016,0,0.154162,"word function. This is because the role/function of a word is categorized by its proximate syntactic context, while a large window captures words that are less informative for this categorization (Turney, 2012). For example, in the sentence Australian scientist discovers star with telescope, the context of the word discovers in a window of size 1 includes scientist and star, while a larger context window will include more words related by topic such as telescope (Levy and Goldberg, 2014). The association of large window sizes with greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect for biomedical domain tasks and word vectors (Chiu et al., 2016). We further considered the differentiation between relatedness"
W16-2501,P12-1015,0,0.205967,"Missing"
W16-2501,W13-3512,0,0.205281,"Missing"
W16-2501,J93-2004,0,0.0645654,"the Wikipedia Extractor2 ; the other sources are textual. We pre-process all text with the Sentence Splitter and the Treebank Word Tokenizer provided by the NLTK python library (Bird, 2006). In total, there are 3.8 billion tokens (19 million distinct types) in the processed text. 2.3 #Tokens (Train/Test) 337,195 / 129,892 211,727 / 47,377 203,621 / 46,435 2.5 Extrinsic evaluation To evaluate the word representations in downstream tasks, we use them in three standard sequence labeling tasks selected by Collobert et al. (2011): POS tagging of Wall Street Journal sections of Penn Treebank (PTB) (Marcus et al., 1993), chunking of CoNLL’00 shared task data (Tjong Kim Sang and Buchholz, 2000), and NER of CoNLL’03 shared task data (Tjong Kim Sang and De Meulder, 2003). We use the standard train/test splits and evaluation criteria for each dataset, evaluating PTB POS tagging using token-level accuracy and CoNLL’00/03 chunking and NER using chunk/entity-level F -scores as implemented in the conlleval evaluation script. Table 3 shows basic statistics for each dataset. Intrinsic evaluation We perform intrinsic evaluations on the ten benchmark datasets presented in Table 2. We follow the standard experimental pro"
W16-2501,W16-2922,1,0.192482,"greater topicality is discussed also by Hill et al. (2015) and Levy et al. (2015). 5 Conclusion One of the primary goals of intrinsic evaluation is to provide insight into the quality of a representation before it is used in downstream applications. However, we found that the majority of word similarity datasets fail to predict which representations will be successful in sequence labelling tasks, with only one intrinsic measure, SimLex-999, showing high correlation with extrinsic measures. In concurrent work, we have also observed a similar effect for biomedical domain tasks and word vectors (Chiu et al., 2016). We further considered the differentiation between relatedness (association) and similarity (synonymy) as an explanatory factor, noting that the majority of intrinsic evaluation datasets do not systematically make this distinction. Our results underline once more the importance of including also extrinsic evaluation when assessing NLP methods and resources. To encourage extrinsic evaluation of vector space representations, we make all of our newly introduced methods available to the community under open licenses from https://github.com/ cambridgeltl/RepEval-2016. This phenomenon provides a po"
W16-2501,P14-5004,0,0.0380063,"otated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, good models are simply those that provide 2.1 Ma"
W16-2501,D15-1036,0,0.0859089,"onstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, good models are simply those that provide 2.1 Materials and Methods Word Vectors We generat"
W16-2501,S14-2048,0,0.0521007,"Missing"
W16-2501,W00-0726,0,0.365583,"Missing"
W16-2501,W03-0419,0,0.289727,"Missing"
W16-2501,P10-1040,0,0.0720468,"ariety of word vectors induced from an unannotated corpus of 3.8 billion words, and demonstrate that most intrinsic evaluations are poor predictors of downstream performance. We argue that this issue can be traced in part to a failure to distinguish specific similarity from relatedness in intrinsic evaluation datasets. We make our evaluation tools openly available to facilitate further study. 1 Introduction 2 The use of vector representations of words is now pervasive in natural language processing, and the importance of their evaluation is increasingly recognized (Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013a; Faruqui and Dyer, 2014; Chen et al., 2013; Schnabel et al., 2015). Such evaluations can be broadly divided into intrinsic and extrinsic. The most common form of intrinsic evaluation uses word pairs annotated by humans to determine their degree of similarity (for varying definitions of similarity). These are then used to directly assess word representations based on how they rank the word pairs. In contrast, in extrinsic evaluation, word representations are used as input to a downstream task such as part-of-speech (POS) tagging or named entity recognition (NER). Here, g"
W16-2501,W14-3302,0,\N,Missing
W16-2922,P06-4018,0,0.0126278,"Open Access articles. The PubMed database has more than 25 million citations that cover the titles and abstracts of biomedical scientific publications. A version of PMC articles is distributed in text format1 whereas PubMed is distributed in XML. Thus, we use a PubMed text extractor2 to extract title and abstract texts from the PubMed source XML. Both PubMed and PMC were pre-processed with the Genia Sentence Splitter (GeniaSS) (Sætre et al., 2007), which is optimized for bio-medical text. We further tokenize the sentences with the Tree bank Word Tokenizer provided by the NLTK python library (Bird, 2006). The corpus statistics are shown in Table 1. 2.2 0.0125 / 0.025 / 0.05 / 0.1 25 / 50 / 100 / 200 / 400 / 500 / 800 1 / 2 / 4 / 5 / 8 / 16 / 20 / 25 / 30 Table 2: Hyper-parameters and tested values. Default values shown in bold. Materials and Methods 2.1 Values 1 / 2 / 3 / 5 / 8 /10 / 15 0 / 1e-1 / 1e-2 / 1e-3 / 1e-4 1e-5 / 1e-6 / 1e-7 / 1e-8 / 1e-9 0 / 5 / 10 / 20 / 50 / 100 / 200 400 / 800 / 1000 / 1200 / 2400 2.3 Hyper-parameters We test the following key hyper-parameters: Negative sample size (neg): the representation of a word is learned by maximizing its predicted probability to co-occur"
W16-2922,D14-1162,0,0.118238,"more, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016. 1 Introduction As one of the main inputs of many NLP methods, word representations have long been a major focus of research. Recently, the embedding of words into a low-dimensional space using neural networks was suggested (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014). These approaches represent each word as a dense vector of real numbers, where words that are semantically related to one another map to similar vectors. Among neural embedding approaches, the skip-gram model of Mikolov et al. (2013a) has achieved cutting-edge results in many NLP tasks, including sentence completion, analogy and sentiment analysis (Mikolov et al., 2013a; Mikolov et al., 2013b; Fern´andez et al., 2014). 166 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 166–174, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistic"
W16-2922,W16-2501,1,0.223336,"vs. CBOW) and hyper-parameter settings (negative sampling, sub sample rate, min-count, learning rate, vector dimension, context window size). For corpora, sentence-shuffled PubMed texts appear to produce the best performance, exceeding that of the notably larger combination with PMC texts. For hyper-parameter settings, it is evident that performance can be notably improved over the default parameters, but the effects of the different hyper-parameters on performance are mixed and sometimes counterintuitive. We have previously found a similar result in general domain work (with Wikipedia text) (Chiu et al., 2016). Several directions remain open for future work. First, in addition to tuning individual parameters in isolation, we can study the effect of tuning two or more parameters simultaneously. In addition, the number of training iterations was not considered in the experiments here, and careful tuning of this parameter both separately and jointly with associated parameters such as alpha may offer further opportunities for improvement. Discussion In this study, we have created vectors with PubMed, PMC and the combination of the two with a large variety of different model, preprocessing and parameter"
W16-2922,S14-2048,0,0.0133937,"Missing"
W16-2922,J15-4004,1,0.534673,"d extrinsic evaluation results for window size (Unit: ρ: dashed line, F-score: solid line) 3.2.3 Setting PubMed skip-gram 10 200 0.05 1e-4 2, 30 5 tion to primarily capture word function (Turney, 2012). It is possible that for intrinsic evaluation datasets such as UMNSRS it is more important to model topical rather than functional similarity. Conversely, it is intuitively clear that for tasks such as named entity recognition the modeling of functional similarity such as co-hyponymymy is centrally important. For further discussion on the effect of the context window size parameter, we refer to Hill et al. (2015) and Levy et al. (2015). Context Window Size (win) We find contradictory results from changing the size of the context window parameter (Figure 6). All three sets of vectors show a notable increase in the intrinsic measures when the context window size grows (Table 16). However, the extrinsic evaluation shows the opposite pattern (Table 17): all results in extrinsic tasks have an early perofmance peak with a narrow window (e.g. win = 1), followed by a gradual decrease when window size increases. One possible explanation may be that a larger window emphasizes the learning of domain/topic simila"
W16-2922,W04-1213,0,0.602102,"-of-the-art performance that can be further improved with parameter tuning, we focus on its performance on biomedical data with different inputs and hyper-parameters. We use all available biomedical scientific literature for learning word embeddings using models implemented in word2vec. For intrinsic evaluation, we use the standard UMNSRS-Rel and UMNSRS-Sim datasets (Pakhomov et al., 2010), which enable us to measure similarity and relatedness separately. For extrinsic evaluation, we apply a neural network-based named entity recognition (NER) model to two standard benchmark NER tasks, JNLPBA (Kim et al., 2004) and the BioCreative II Gene Mention task (Smith et al., 2008). Apart from showing that the optimization of hyper-parameters boosts the performance of vectors, we also find that one such parameter leads to contradictory results between intrinsic and extrinsic evaluations. We further observe that a larger corpus does not necessarily guarantee better reThe quality of word embeddings depends on the input corpora, model architectures, and hyper-parameter settings. Using the state-of-the-art neural embedding tool word2vec and both intrinsic and extrinsic evaluations, we present a comprehensive stud"
W16-2922,P10-1040,0,0.0142778,"intrinsic and extrinsic evaluations. Furthermore, we find that bigger corpora do not necessarily produce better biomedical domain word embeddings. We make our evaluation tools and resources as well as the created state-of-the-art word embeddings available under open licenses from https://github.com/ cambridgeltl/BioNLP-2016. 1 Introduction As one of the main inputs of many NLP methods, word representations have long been a major focus of research. Recently, the embedding of words into a low-dimensional space using neural networks was suggested (Bengio et al., 2003; Collobert and Weston, 2008; Turian et al., 2010; Mikolov et al., 2013b; Pennington et al., 2014). These approaches represent each word as a dense vector of real numbers, where words that are semantically related to one another map to similar vectors. Among neural embedding approaches, the skip-gram model of Mikolov et al. (2013a) has achieved cutting-edge results in many NLP tasks, including sentence completion, analogy and sentiment analysis (Mikolov et al., 2013a; Mikolov et al., 2013b; Fern´andez et al., 2014). 166 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 166–174, c Berlin, Germany, August 12, 20"
W16-2922,Q14-1041,0,0.013299,"Missing"
W16-2922,Q15-1016,0,0.464773,"on. The magnitude of these updates is controlled by the learning rate. Vector dimension (dim): The vector dimension is the size of the learned word vector. While a higher dimension tends to capture better word representations, their training is more computationally costly and produces a larger word embedding matrix. Context window size (win): The size of the context window defines the range of words to be included as the context of a target word. For instance, a window size of 5 takes five words before and after a target word as its context for training. We refer to Mikolov et al. (2013a) and Levy et al. (2015) for further details regarding these parameters. 2.4 2.6 Given that the ultimate evaluation for word vectors is their performance in downstream applications, we also assess the quality of the vectors by performing NER using two well-established biomedical reference standards: the BioCreative II Gene Mention task corpus (BC2) (Smith et al., 2008) and the JNLPBA corpus (PBA) (Kim et al., 2004). Both of these corpora consist of approximately 20,000 sentences from PubMed abstracts manually annotated for mentions of biomedical entity names. Following the window approach architecture with word-level"
W16-2922,W15-3820,0,0.198816,"od Word Embeddings for Biomedical NLP Billy Chiu Gamal Crichton Anna Korhonen Sampo Pyysalo Language Technology Lab DTAL, University of Cambridge {hwc25|gkoc2|alk23}@cam.ac.uk, sampo@pyysalo.net Abstract Although word embeddings have been studied extensively in recent work (e.g. Lapesa and Evert (2014)), most such studies only involve general domain texts and evaluation datasets, and their results do not necessarily apply to biomedical NLP tasks. In the biomedical domain, Stenetorp et al. (2012) studied the effect of corpus size and domain on various word clustering and embedding methods, and Muneeb et al. (2015) compared two state-of-the-art word embedding tools: word2vec and Global Vectors (GloVe) on a word-similarity task. They showed that skip-gram significantly out-performs other models and that its performance can be further improved by using higher dimensional vectors. The word2vec tool was also used to create biomedical domain word representations by Pyysalo et al. (2013) and Kosmopoulos et al. (2015). Given that word2vec has been shown to achieve state-of-the-art performance that can be further improved with parameter tuning, we focus on its performance on biomedical data with different input"
W16-3009,N10-1004,0,0.0234799,"o run a basic preprocessing pipeline of tokenization, POS tagging, and parsing, as well as to remove cross-sentence relations. Like our approach, TEES targets the extraction of associations between entities that occur in the same sentence. To support this functionality, it can detect and eliminate relations that cross sentence boundaries in its input. We use this feature of TEES as an initial preprocessing step to remove such relations from the data. To obtain tokens, POS tags and parse graphs, TEES uses the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The phrase structure trees produced by the parser are further processed with the Stanford conversion tool (de Marneffe et al., 2006) to create dependency graphs. The Stanford system can produce several variants of the Stanford Dependencies (SD) representation. Here, we use the collapsed variant, which is designed to be useful for information extraction and language understanding tasks (de Marneffe and Manning, 2008). Our approach builds on the shortest dependency path between each pair of entities. However, while dependency parse graphs connect words to others in the same sentence, a number"
W16-3009,W13-2003,1,0.577396,"Missing"
W16-3009,W11-1809,0,0.0535061,"Missing"
W16-3009,W13-2001,1,0.868609,"Missing"
W16-3009,H05-1091,0,0.721626,"e there are various ways of converting the shared task annotations into examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs"
W16-3009,D09-1143,0,0.0186528,"ed task annotations into examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate"
W16-3009,P05-1022,0,0.0164901,"eloped by members of the TurkuNLP group (Bj¨orne and Salakoski, 2013), to run a basic preprocessing pipeline of tokenization, POS tagging, and parsing, as well as to remove cross-sentence relations. Like our approach, TEES targets the extraction of associations between entities that occur in the same sentence. To support this functionality, it can detect and eliminate relations that cross sentence boundaries in its input. We use this feature of TEES as an initial preprocessing step to remove such relations from the data. To obtain tokens, POS tags and parse graphs, TEES uses the BLLIP parser (Charniak and Johnson, 2005) with the biomedical domain model created by McClosky (2010). The phrase structure trees produced by the parser are further processed with the Stanford conversion tool (de Marneffe et al., 2006) to create dependency graphs. The Stanford system can produce several variants of the Stanford Dependencies (SD) representation. Here, we use the collapsed variant, which is designed to be useful for information extraction and language understanding tasks (de Marneffe and Manning, 2008). Our approach builds on the shortest dependency path between each pair of entities. However, while dependency parse gr"
W16-3009,W11-0216,0,0.0192759,"nto examples for classification, the numbers we report here may differ from those reported by other participating teams. 3.2 Shortest Dependency Path The syntactic structure connecting two entities e1 and e2 in various forms of syntactic analysis is known to contain most of the words relevant to characterizing the relationship R(e1 , e2 ), while excluding less relevant and uninformative words. This observation has served as the basis for many successful relation extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate the shortest dependency p"
W16-3009,W11-1815,0,0.399828,"Missing"
W16-3009,W08-1301,0,0.0397729,"Missing"
W16-3009,de-marneffe-etal-2006-generating,0,0.0640478,"Missing"
W16-3009,D15-1206,0,0.077098,"ion extraction approaches in both general and biomedical domain NLP (Bunescu and Mooney, 2005; Airola et al., 2008; Nguyen et al., 2009; Chowdhury et al., 2011). The TEES system also heavily relies on the shortest dependency path for defining and ex1 Official evaluation results on the test data are of course comparable to those of other systems: any cross-sentence relations in the test data count against our submission as false negatives. 74 3.3 tracting features (Bj¨orne et al., 2012; Bj¨orne and Salakoski, 2013). Recently, this idea was applied in an LSTM-based relation extraction system by Xu et al. (2015). Since the dependency parse is directed (i.e. the path from e1 to e2 differs from that from e2 to e1 ), they separate the shortest dependency path into two sub-paths, each from an entity to the common ancestor of the two entities, generate features along the two sub-paths, and feed them into different LSTM networks, to process the information in a direction sensitive manner. To avoid doubling the number of LSTM chains (and hence the number of weights), we convert the dependency parse to an undirected graph, find the shortest path between the two entities (BACTERIA and H ABITAT/G EOGRAPHICAL),"
W16-5101,W16-2922,1,0.793637,"ion 3.4, the word vectors used to initialize the embedding layer of the network can have a significant effect on performance. We trained the models using each of the word vectors shown in Table 3 with oversampling (see above) and evaluated development set performance using the maximum F-score and AUC metrics. The results are summarized in Figure 2. Surprisingly, we find that the general domain Google News vectors give very competitive performance despite their high out-of-vocabulary rate (see Table 3), outperforming all in-domain vectors with the exception of the window size 2 word vectors of Chiu et al. (2016). Even these biomedical word vectors only show very modest advantage over the Google News vectors for AUC. In the last development set experiments below and the final test set experiments, we apply the PubMed-based vectors induced with window size 2 from Chiu et al. that were shown to give the best results here. 97.8 97.75 97.7 97.65 97.6 97.55 97.5 97.45 97.4 97.35 97.3 97.25 1 1 filter size 2 3 2 filter sizes 4 5 3 filter sizes 6 7 4 filter sizes 8 9 10 5 filter sizes Figure 3: Macro-average AUC with respect to a varying number of filter sizes. Each point on the graph represents the maximum"
W16-5101,J81-4005,0,0.763341,"Missing"
W16-5101,D14-1181,0,0.11742,"eatures are extracted from the training data and are then filtered by frequency to remove features that are too common or too rare, leaving behind only the most discriminating features. We use a linear kernel and fine-tune the regularization parameter c on the development dataset using the same process applied for the BoW model. As there are significantly more negatively labelled documents than positives, we use inverse class weighting in order to correct for the class imbalance when training the classifiers. 3.3 Convolutional Neural Network We base our CNN architecture on the simple model of Kim (2014). In brief, this model consists of an initial embedding layer that maps input texts into matrices, followed by convolutions of different filter sizes and 1-max pooling, and finally a fully connected layer. The architecture is illustrated in Figure 1. We implemented the neural network using Keras (Chollet, 2015). Model hyperparameters and the training setup were initially based on those applied by Kim (2014), summarized in the following: Parameter Word vector size Filter sizes Number of filters Dropout probability Minibatch size Value 300 (Google News vectors) 3, 4, and 5 300 (100 of each size)"
W16-5101,W16-2918,0,0.016505,"CNN). CNNs were first proposed for image processing (LeCun and Bengio, 1995) and have been recently shown to achieve state-of-the-art performance in a range of NLP tasks, in particular in text classification (Zhang et al., 2015; Severyn and Moschitti, 2015; Zhang and Wallace, 2015). While neural network-based methods in general and “deep” networks in particular are increasingly popular for general domain NLP, there has been comparatively little work applying this class of methods to biomedical text. One recent study applying a CNN model to biomedical text classification task was presented by (Limsopatham and Collier, 2016), who applied CNNs to the task of adverse drug reaction detection in social media messages (Ginn et al., 2014). In addition to the specific subdomain of the source texts and the novel categories represented by the hallmarks of cancer, one factor that sets apart the task here from this previous work is the length of the texts: instead of sentences or brief social media messages, our task involves the classification of publication abstracts typically consisting of hundreds of words. 2 Data For training and evaluating our methods, we use the corpus of 1852 biomedical publication abstracts annotat"
W16-5101,W13-2008,1,0.860593,"Missing"
W16-5101,S15-2079,0,0.0172903,"ext Mining (BioTxtM 2016), pages 1–9, Osaka, Japan, December 12th 2016. In this work, our focus is on studying biomedical text classification using machine learning methods that emphasize feature learning rather than manual feature engineering. We adopt the task setting and dataset of Baker et al. (2016), but instead of SVMs, we focus on convolutional neural networks (CNN). CNNs were first proposed for image processing (LeCun and Bengio, 1995) and have been recently shown to achieve state-of-the-art performance in a range of NLP tasks, in particular in text classification (Zhang et al., 2015; Severyn and Moschitti, 2015; Zhang and Wallace, 2015). While neural network-based methods in general and “deep” networks in particular are increasingly popular for general domain NLP, there has been comparatively little work applying this class of methods to biomedical text. One recent study applying a CNN model to biomedical text classification task was presented by (Limsopatham and Collier, 2016), who applied CNNs to the task of adverse drug reaction detection in social media messages (Ginn et al., 2014). In addition to the specific subdomain of the source texts and the novel categories represented by the hallmarks of"
W16-5101,D09-1067,1,0.777256,"12) and included as features using a BoW-style representation. Noun bigrams Compound nouns (without lemmatization) are combined to generate bigram features. Nouns pairs often represent specific, discriminative concepts such as “gene mutation”. Grammatical relations triples The C&C Parser with a biomedical domain model (Rimell and Clark, 2009) is used to parse the documents, and the dobj (direct object), ncsubj (non-clausal subject) and iobj (indirect object) relations, and their head and dependent words then represented as features. Verb classes The hierarchical classification of 399 verbs of Sun and Korhonen (2009) is used to generate features for verbs, utilizing all three levels of abstraction by allocating three bits in the feature representation for each concrete class, i.e. one bit for each level of the verb class hierarchy. Named entities (NE) The ABNER NER tool (Settles, 2005) is used to identify five named entity types that are particularly relevant to cancer research: proteins, DNA, RNA, cell lines and cell types. Features are then created pairing each entity type and its associated words. Medical subject headings (MeSH) The MeSH headings assigned to the documents in the biomedical publication"
W17-6511,N09-1003,0,0.0283734,"Missing"
W17-6511,Q13-1034,1,0.898996,"Missing"
W17-6511,P14-2050,0,0.0388234,"distributional semantics approaches (Bengio et al., 2003; Collobert et al., 2011). The methods introduced by Mikolov et al. (2013a) and implemented in their popular word2vec tool have been proven both effective and a good foundation for further exploration. In addition to representing word contexts as sliding windows of words in linear sequence, recent work has included efforts of building the word vectors using dependency-based approaches (Levy and Gold2.2 Dependency-based word embeddings Observing that the SGNS model is not inherently restricted to working with contexts consisting of words, Levy and Goldberg (2014) extended the model to work with arbitrary contexts, focusing 1 https://code.google.com/p/word2vec/ 83 Proceedings of the Fourth International Conference on Dependency Linguistics (Depling 2017), pages 83-91, Pisa, Italy, September 18-20 2017 nsubj cop det amod in particular on dependency-based contexts consisting of combinations of a neigbouring word in the dependency graph and its dependency relation to the target word (e.g. scientist/nsubj). Compared to embeddings based on linear contexts of words, they showed dependency-based embeddings to emphasize functional over topical similarity and t"
W17-6511,W13-3512,0,0.122728,"Missing"
W17-6511,W15-2124,1,0.871923,"Missing"
W17-6511,P12-1015,0,0.0206098,"Missing"
W17-6511,W16-2501,1,0.822797,"for training embeddings. Of the 64 treebanks in the release, 9 do not fulfill these criteria (French-ParTUT, GalicianTreeGal, Irish, Kazakh, Latin, Slovenian-SST, Ukrainian and Uyghur do not have development data, Gothic does not have raw data) and are not included in the evaluation. Models are trained on the training section of a treebank and tested on the development section.7 Word vectors are frequently evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the chara"
W17-6511,P14-5004,0,0.0136124,"evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the characteristics of the English word vectors using nearest neighbours and give the intrinsic evaluation results for these vectors before presenting the results of our primary multilingual parsing experiments. Extrinsic evaluation Our primary evaluation is based on dependency parsing, where we evaluate parsing accuracy using different pre-trained word embeddings during parser training. We use the UDPipe pipeline4 for"
W17-6511,D13-1032,0,0.0575446,"Missing"
W17-6511,W16-2506,0,0.0122627,"ngs. Of the 64 treebanks in the release, 9 do not fulfill these criteria (French-ParTUT, GalicianTreeGal, Irish, Kazakh, Latin, Slovenian-SST, Ukrainian and Uyghur do not have development data, Gothic does not have raw data) and are not included in the evaluation. Models are trained on the training section of a treebank and tested on the development section.7 Word vectors are frequently evaluated by assessing how well their distance correlates with human judgments of word similarity. Although these intrinsic evaluations have known issues (see e.g. Batchkarov et al. (2016), Chiu et al. (2016), Faruqui et al. (2016)) and we agree with the criticism that they are frequently poor indicators of the merits of representations, we include this common form of intrinsic evaluation here for reference purposes. We provide results using a comprehensive collection of English datasets annotated for word similarity and relatedness. Specifically, we used the evaluation service introduced by Faruqui and Dyer (2014) to evaluate on the 13 datasets available on the service3 at the time of this writing. The datasets are summarized below in Table 3. 3.4 4 Results We next informally illustrate the characteristics of the Engli"
W17-6511,L16-1262,1,0.854686,"Missing"
W17-6511,D16-1235,0,0.024257,"Missing"
W17-6511,W15-1821,1,0.8326,"Missing"
W17-6511,L16-1680,0,0.0267194,"Missing"
W17-6511,P14-5003,0,0.0637101,"Missing"
W17-6511,D14-1034,0,\N,Missing
W17-6511,W16-2502,0,\N,Missing
W19-6130,N15-1170,0,0.0539239,"Missing"
W19-6130,W14-3706,0,0.0144309,"ant predictors of linguistic variation and affects how we interpret the text (Biber, 2012). Automatic identification of registers could thus improve the potential of online data, in particular for linguistically oriented research (Webber, 2009; Giesbrecht and Evert, 2009). However, the automatic identification of registers has proven to be difficult. Studies of Web Genre Identification (WGI) have been limited by small and scattered data sets which have resulted in lack of robustness and generalization of the models (Sharoff et al., 2010; Petrenz and Webber, 2011; Pritsos and Stamatatos, 2018; Asheghi et al., 2014). Furthermore, although online data is available in many languages and NLP systems are increasingly focused on multilingual settings (e.g., Zeman et al. (2018)), WGI studies have focused nearly exclusively on English texts. The only large-scale data set representing the full range of online registers is CORE — the Corpus of Online Registers of English — which is based on an unrestricted sample of English documents from the searchable web (Egbert et al., 2015). In this paper, we extend the scope of modeling online registers to cross- and multilingual settings. We 1) present the first non-Englis"
W19-6130,K18-2013,0,0.0242556,"Missing"
W19-6130,D14-1181,0,0.0125005,"olahti et al., 2015), a web-crawled corpus that currently consists of nearly 4 billion words. The annotations were done jointly by a supervisor and a dedicated annotator. The Finnish annotations aim to follow the CORE annotation guidelines as closely as posFigure 1: Illustration of text classification approach. Tokens are prefixed with language tags to differentiate e.g. the English word on from the Finnish word on ‘is’. Multilingual word vectors are used and the same network applied regardless of language to allow cross-lingual and multilingual training and classification. (Following in part Kim (2014)) sible. The process advances through a decision tree, where the annotator 1) evaluates the mode of the text (spoken or written), 2) determines whether the text is interactive (multiple authors) or noninteractive (one author) and 3) identifies the general register of the text. Finally, the most accurate sub-register is selected if applicable. If the text appears to have more than one appropriate register, the annotator may choose up to three registers. Texts with several registers are called hybrid texts. In this paper, we focus on the main register level because of the small size of the Finni"
W19-6130,C12-1089,0,0.069859,"Missing"
W19-6130,P15-2128,0,0.042038,"Missing"
W19-6130,W15-2124,1,0.79136,"rs providing different kinds of instructions, such as actual How-to pages, Recipes and Technical support pages. The Informational persuasion (Info-Persuasion) main register covers texts that use facts to persuade, such as Editorials and Descriptions with intent to sell. Finally, the Lyrical main register includes, e.g., Song lyrics and Poems, and the Spoken main register, e.g., Interviews and Video transcripts. For a detailed description of the CORE annotation process and corpus quality, we refer to Egbert et al. (2015). The Finnish data is based on a sample of the Finnish Internet Parsebank (Luotolahti et al., 2015), a web-crawled corpus that currently consists of nearly 4 billion words. The annotations were done jointly by a supervisor and a dedicated annotator. The Finnish annotations aim to follow the CORE annotation guidelines as closely as posFigure 1: Illustration of text classification approach. Tokens are prefixed with language tags to differentiate e.g. the English word on from the Finnish word on ‘is’. Multilingual word vectors are used and the same network applied regardless of language to allow cross-lingual and multilingual training and classification. (Following in part Kim (2014)) sible. T"
W19-6130,L16-1262,1,0.811818,"Missing"
W19-6130,J11-2004,0,0.0194478,"how-to-page or advertisement – is one of the most important predictors of linguistic variation and affects how we interpret the text (Biber, 2012). Automatic identification of registers could thus improve the potential of online data, in particular for linguistically oriented research (Webber, 2009; Giesbrecht and Evert, 2009). However, the automatic identification of registers has proven to be difficult. Studies of Web Genre Identification (WGI) have been limited by small and scattered data sets which have resulted in lack of robustness and generalization of the models (Sharoff et al., 2010; Petrenz and Webber, 2011; Pritsos and Stamatatos, 2018; Asheghi et al., 2014). Furthermore, although online data is available in many languages and NLP systems are increasingly focused on multilingual settings (e.g., Zeman et al. (2018)), WGI studies have focused nearly exclusively on English texts. The only large-scale data set representing the full range of online registers is CORE — the Corpus of Online Registers of English — which is based on an unrestricted sample of English documents from the searchable web (Egbert et al., 2015). In this paper, we extend the scope of modeling online registers to cross- and mult"
W19-6130,W17-2619,0,0.0419646,"Missing"
W19-6130,L18-1560,0,0.0121661,"lit. The test data was held out during method development and parameter selection and only used for the final experiments. 4 Methods Our approach is based on a simple convolutional neural network (CNN) architecture following Kim (2014) and illustrated in Figure 1. Documents are first tokenized using the Turku Neural Parser (Kanerva et al., 2018) trained on language-specific Universal Dependencies (Nivre et al., 2016) resources. The input is represented as a word vector sequence to a convolution layer with ReLU activation, followed by max-pooling and a fullyconnected output layer. Similarly to Schwenk and Li (2018), we use pretrained multilingual word embeddings for multi- and cross-lingual classification; to differentiate between the same word forms in different languages, we simply prefix a language tag to each token and modify word vector indexing analogously. We use English and Finnish word vectors from the Multilingual Unsupervised and Supervised Embeddings (MUSE) library1 (Conneau et al., 2018) in all experiments. As MUSE word vectors are uncased, we lowercase text following tokenization. Based on initial experiments on the development set, we set the maximum number of word vectors to 100000, the"
W19-6130,sharoff-etal-2010-web,0,0.0245128,"a document is a blog, how-to-page or advertisement – is one of the most important predictors of linguistic variation and affects how we interpret the text (Biber, 2012). Automatic identification of registers could thus improve the potential of online data, in particular for linguistically oriented research (Webber, 2009; Giesbrecht and Evert, 2009). However, the automatic identification of registers has proven to be difficult. Studies of Web Genre Identification (WGI) have been limited by small and scattered data sets which have resulted in lack of robustness and generalization of the models (Sharoff et al., 2010; Petrenz and Webber, 2011; Pritsos and Stamatatos, 2018; Asheghi et al., 2014). Furthermore, although online data is available in many languages and NLP systems are increasingly focused on multilingual settings (e.g., Zeman et al. (2018)), WGI studies have focused nearly exclusively on English texts. The only large-scale data set representing the full range of online registers is CORE — the Corpus of Online Registers of English — which is based on an unrestricted sample of English documents from the searchable web (Egbert et al., 2015). In this paper, we extend the scope of modeling online re"
W19-6130,D10-1103,0,0.0925108,"Missing"
W19-6130,W16-2326,0,0.0228654,"Missing"
W19-6130,P09-1076,0,0.0212607,"line (Tiedemann et al., 2016; Zeman et al., 2018; Devlin et al., 2018). However, the diversity of online data is also a challenge to its use. Documents have little or no information on their communicative purpose or, specifically, on their register (genre) (Biber, 1988). Register – whether a document is a blog, how-to-page or advertisement – is one of the most important predictors of linguistic variation and affects how we interpret the text (Biber, 2012). Automatic identification of registers could thus improve the potential of online data, in particular for linguistically oriented research (Webber, 2009; Giesbrecht and Evert, 2009). However, the automatic identification of registers has proven to be difficult. Studies of Web Genre Identification (WGI) have been limited by small and scattered data sets which have resulted in lack of robustness and generalization of the models (Sharoff et al., 2010; Petrenz and Webber, 2011; Pritsos and Stamatatos, 2018; Asheghi et al., 2014). Furthermore, although online data is available in many languages and NLP systems are increasingly focused on multilingual settings (e.g., Zeman et al. (2018)), WGI studies have focused nearly exclusively on English texts"
W19-6130,K18-2001,0,0.0270232,"Missing"
