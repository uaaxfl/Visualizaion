2019.icon-1.16,W06-0901,0,0.0868757,"iate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model can help us utilise the close interaction between one or more similar tasks. Li et al. (2013) pr"
2019.icon-1.16,Q17-1010,0,0.00608438,"borate our claim, we device two different systems, i). monolingual baseline system, and ii). multi-lingual system. The ‘monolingual baseline’ system only takes input data (sentence wise) from one language and extracts the arguments. For word representation, it uses monolingual word embeddings. The ‘multi-lingual’ argument extraction system uses separate language layers and multi-lingual word embeddings for joint training on all the three languages. 3.0.1 Monolingual Word Embedding The monolingual word-embeddings that are used in our experiments are also known as fastText1 . It was proposed by Bojanowski et al. (2017), and is based on the skipgram model. However instead of using one-hot vector encoding for each word while training, a vector representation of a word that considers character n-grams occurring in the word is formed. To get this representation, the n-grams from all the words for ‘n’ greater than 2 and smaller than 7 are extracted. After this, a dictionary of all the extracted n-grams is created. A given word w, can now be denoted by Γw ⊂ {1, ...., G} i.e the set of n-grams appearing in the word; where G is the size of the n-gram dictionary. With each n-gram in G, a vector representation zg is"
2019.icon-1.16,P11-1098,0,0.0264339,"ated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this"
2019.icon-1.16,P15-1017,0,0.0682416,"onse to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event argument extraction. First is our monolingual system built using CNN (Convolutional Neural Network) and Bi-LSTM (Bi-Directional L"
2019.icon-1.16,N09-2053,0,0.0304979,"side token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model can help us utilise the close interaction between one or more similar tasks. Li et al. (2013) presented a joint mode"
2019.icon-1.16,doddington-etal-2004-automatic,0,0.155659,"Missing"
2019.icon-1.16,P11-1113,0,0.0210488,"i). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upst"
2019.icon-1.16,P11-1114,0,0.0219108,"abel multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional Random Fields (CRF) model that can select any features and normalizing these features in overall situation helps in obtaining optimal results. While, these models can get affected by the error propagated from upstream tasks, a joint model ca"
2019.icon-1.16,D16-1085,0,0.0128527,"ated Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event argument extraction. First is our monolingual system built using CNN (Convolutional Neural Network) and Bi-LSTM (Bi-Directional Long Short 136 Term Memory). To exploit the information from related languages, we develop a second system that can use information from all the languages for training. This multi-lingual syste"
2019.icon-1.16,P08-1030,0,0.105931,"Missing"
2019.icon-1.16,D07-1075,0,0.049315,"Therefore, it has been formulated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the conte"
2019.icon-1.16,D14-1181,0,0.0026806,"n zg is associated. A word represention is obtained by summing up all the n-grams, as described in Equation 1: Vw = ∑ zg W ← (1 + β)W − β(W W T )W (2) Here, β was set to 0.01 for the transformation. For our experiments we trained mapping matrices Whindi and Wbengali that map the Hindi and Bengali word embeddings to the vector space of English embeddings. 3.1 Monolingual Baseline Model The ‘monolingual baseline’ model (c.f Figure 1) is based on Bi-Directional Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997) and Convolutional Neural Networks (CNN) (Kim, 2014). The input to the model is a sentence, represented by a sequence of monolingual word embeddings. Since Bi-LSTM and CNN take sequences of equal lengths, the shorter sequences are padded by zero (1) g∈Gw The continuous skip-gram model used these word vectors Vw , to obtain word-embedding representa1 https://github.com/facebookresearch/ fastText 137 Figure 1: monolingual baseline model for argument extraction Figure 2: Multi-lingual baseline model for argument extraction vectors. This sequence is passed through BiLSTM and CNN having filter size 2 and 3. The Bi-LSTM gives contextual representatio"
2019.icon-1.16,D09-1016,0,0.0345474,"w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system ba"
2019.icon-1.16,P18-1074,0,0.0222127,"g. Com135 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 135–142 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) cially helpful in improving the performance when some argument is under-represented in the ‘monolingual’ training data. 1.1 chunk level information from specific contexts. Nguyen and Grishman (2018) explored graph convolutional network over dependency trees and entity mention-guided pooling. For low resource languages, Liu et al. (2018) came up with Gated Multi-Lingual Attention (GMLATT) and Lin et al. (2018) developed a multi-lingual multi-task architecture alleviating data sparsity problem in related tasks and languages. Problem Definition Argument extraction entails classifying each word in the sentence into some argument or not argument. Therefore, it has been formulated as a sequence labelling task. Given a sentence of form w1 , w2 , ..., wn , the task is to predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label"
2019.icon-1.16,P12-1088,0,0.0289935,"predict the sequence of event-arguments, of the form l1 , l2 , ..., ln . Six different types of arguments were annotated in the dataset: i). Place, ii). Time, iii). Reason, iv). Casualties, v). Participant and vi). After-effects. To label multi-word event-arguments, IOB-style encoding is used where B, I and O denote the beginning, intermediate and outside token of an event. Previously, in event argument extraction researchers have experimented with pattern based methods (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011) and machine learning based methods (Patwardhan and Riloff, 2009; Lu and Roth, 2012) most of which utilise the various kinds of features obtained from the context of a sentence. Higher level representations such as crosssentence or cross-event information were also explored by Hong et al. (2011) and Huang and Riloff (2011). Maximum Entropy based classifiers were applied for event and argument labeling by Ahn (2006); Chen and Ji (2009); Zhao et al. (2008). The disadvantage with ME classifier is that it gets stuck in local optima and fails to fully capture the context features. To overcome this Hou et al. (2012) proposes a event argument extraction system based on Conditional R"
2019.icon-1.16,N16-1034,0,0.0609126,"e or more similar tasks. Li et al. (2013) presented a joint model for Chinese Corpus which identifies arguments and determines their roles for event extraction using various kinds of discourse-level information. On ACE2005 dataset Sha et al. (2018) proposed a dependency bridge recurrent neural network (dbRNN) built upon LSTM units for event extraction. They use dependency bridges over Bi-LSTM to join syntactically similar words. A tensor layer is applied to get the various argument-argument interactions. Event triggers and arguments are then jointly extracted utilising a max-margin criterion. Nguyen et al. (2016) presented a GRU model to jointly predict events and its arguments. • Input Hindi Sentence: गृह मं ालय मुंबई के बम वफोट के म ेनजर इस बात क वशेष तौर पर जांच कर रहा है क अ रधाम मं दर और १९९३ के मुंबई बम वफोट के फसल क त या के प म तो यह हमले नह ए • Translation: In view of the Mumbai bomb blasts, the Home Ministry is specially investigating the fact that these attacks did not take place as response to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task"
2019.icon-1.16,P15-2060,0,0.0131808,"In view of the Mumbai bomb blasts, the Home Ministry is specially investigating the fact that these attacks did not take place as response to the Akshardham Temple and the 1993 Bombay bomb blasts • Output: O O I_Place O O O O O O O O O O O O O O O O I_Place I_Place O I_Time O I_Place O O O O O O O O O O O O O O 2 Related Works A major task in information extraction is detection of event triggers, event classification and event argument extraction. Recent works on event trigger detection and classification discuss efficient feature representation techniques which can help in event extraction. Nguyen and Grishman (2015) proposed a convolutional neural network for event extraction which automatically learns features from text. Chen et al. (2015) introduced dynamic convolutional neural network (DMCNN), which adopt a dynamic multi-pooling layer in accordance with the event triggers and its arguments. In 2016, Nguyen and Grishman (2016) improved their CNN model by introducing the non-consecutive convolution by skipping irrelevant words in a sequence. Feng et al. (2018) designed a combined model of LSTM’s and CNN’s which helped in capturing both sequence level and We introduce two systems for the task of event ar"
2019.icon-1.19,P16-2060,0,0.0624271,"e on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017)"
2019.icon-1.19,L18-1550,0,0.0175464,"words are created for Hindi and Bengali respectively. We create separate .vec file for the two OOV vocabularies. We similarly transform these vectors of two different languages in a shared space using the existing alignment matrices3 . It is seen that the performance has significantly improved using crosslingual embeddings for OOV words compared to the method of using zero vectors for representing them. 5 Datasets and Experiments 5.1 Dataset Words इंडो नशया म 7.2 क ती ता का भूकंप आया Embedding Each word of the input instance is converted to a numeric representation with the help of fastText (Grave et al., 2018) word embeddings having dimension 300 (de ). The pre-trained word vectors are downloaded from fastText website2 . To learn a mapping between mono-lingual word embeddings and obtain cross-lingual embeddings in order to bridge the language gap between two languages, we use the existing alignment matrices3 which align monolingual vectors from two lanEvent & Argument Trigger Detection B_Arg O B_Arg O O O B_Event O Event & Argument Classification Place None Magnitude None None None Earthquake None Table 2: Sample annotation for the sentence given in Example-2 in Task Description and Contribution Se"
2019.icon-1.19,P11-1113,0,0.0289994,"ing (MTL), which essentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automati"
2019.icon-1.19,P08-1030,0,0.0565471,"cture. Multi-task learning (MTL), which essentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network"
2019.icon-1.19,P13-1008,0,0.024244,"y idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event se"
2019.icon-1.19,P15-1017,0,0.106982,"010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) pr"
2019.icon-1.19,P10-1081,0,0.0324153,"sentially means performing more than one related task simultaneously, has been proven to be effective for various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen"
2019.icon-1.19,P16-1201,0,0.0123909,". Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also pr"
2019.icon-1.19,P17-1164,0,0.0230994,"eini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they focus on English and Chinese. Liu et al. (2018b) propose a novel framework to"
2019.icon-1.19,D18-1156,0,0.0408197,"Missing"
2019.icon-1.19,N16-1034,0,0.0221363,"ropose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they f"
2019.icon-1.19,P15-2060,0,0.0181637,"o two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language in"
2019.icon-1.19,D16-1085,0,0.0165577,"thm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more crucial information. In another work, Nguyen and Grishman (2016) propose a skip-gram based CNN model which allows nonconsecutive convolution. Ghaeini et al. (2016) propose a forward-backward Recurrent Neural Network (RNN) to detect event triggers which can be in the form of both words or phrases. Feng et al. (2018) propose a language independent neural network which uses both CNN and Bi-LSTM for Event detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue"
2019.icon-1.19,D14-1090,0,0.0189585,"is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according to event triggers and arguments in multi-event sentences, to capture more"
2019.icon-1.19,D18-1122,0,0.0385385,"Missing"
2019.icon-1.19,N16-1033,0,0.0124412,"ent detection. Liu et al. (2016) propose to improve the performance of event detection by using the events automatically detected from FrameNet. Though these neural based systems perform well, they still suffer from error propagation issue. To overcome this issue, Nguyen et al. (2016) propose a joint framework with bidirectional RNN. However Liu et al. (2017) observe that joint model achieves insigniﬁcant improvements on event detection task. They analyze the problem of joint models on the task of event detection, and propose to use the annotated argument information explicitly for this task. Yang and Mitchell (2016) also propose a joint model for event and entity extraction but in document level instead of sentence level in contrast to most of the previous works. In recent years Liu et al. (2018a) introduce a cross language attention model for event detection where they focus on English and Chinese. Liu et al. (2018b) propose a novel framework to jointly extract multiple event triggers and arguments. Sha et al. (2018) propose a novel dependency bridge RNN which includes syntactic dependency relationships. Dependency relationship is also used by Nguyen and Grishman (2018). They investigate a CNN based on"
2019.icon-1.19,D11-1001,0,0.030187,"or various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according"
2019.icon-1.19,W11-1807,0,0.0267833,"or various NLP tasks in recent times (Ruder, 2017). The key idea behind MTL is that the inductive transfer of knowledge, learned for a particular task, can help to improve the performance 2 Related Works Being a very important problem in NLP, Event Extraction has already been explored by the research community for a long time. Some feature based approaches have decomposed the entire event extraction task into two sub-tasks and solved them separately (Ji and Grishman, 2008; Hong et al., 2011; Liao and Grishman, 2010). But the main problem of this approach is error propagation which is dealt by Riedel and McCallum (2011a), Riedel and McCallum (2011b), Li et al. (2013), Venugopal et al. (2014) using a joint event extraction algorithm. However both of the 160 D M Sharma, P Bhattacharyya and R Sangal. Proc. of the 16th Intl. Conference on Natural Language Processing, pages 160–169 Hyderabad, India, December 2019. ©2019 NLP Association of India (NLPAI) above approaches have used hand-designed feature. Nguyen and Grishman (2015) propose a Convolutional Neural Network (CNN) for automatic feature extraction. Chen et al. (2015) introduce a dynamic multi-pooling CNN which uses a dynamic multi-pooling layer according"
2019.icon-1.2,P17-2067,0,0.398674,"t musician Texas republican 0 0 2 0 2 an oped column. 4 abortion, federalbudget, health-care plannedparenthood -action-fund Advocacy group Washington, D.C. none 1 0 0 0 0 a radio ad 5 health-care nancy-pelosi House Minority Leader California democrat 3 7 11 2 3 a news conference 6 correctionsandupdates, crime, criminal -justice, sexuality garnetcoleman Texas democrat 1 0 1 0 1 a committee hearing president, ceo of Apartments for America, Inc. fake news detection. Further integrating these two elements improves the performance of the classifier. as entirely false. This problem was addressed by Wang (2017) where they introduced Liar dataset comprising of a substantial volume of short political statements having six different class annotations determining the amount of fake content of each statement. In his work, he showed comparative studies of several statistical and deep learning based models for the classification task and found that the CNN model performed best. Long et al. (2017) in their work used the Liar dataset, and proposed a hybrid attention-based LSTM model for this task, which outperformed W.Yang’s hybrid CNN model, establishing a new state-of-the-art. Problems related to these top"
2019.icon-1.2,D14-1181,0,0.00317989,"viewed concerning binary classification. Likewise, most of the published works also has viewed fake news detection as a binary classification problem (i.e., fake or true). But by observing very closely it can be seen that fake news articles can be classified into multiple classes depending on the fakeness of the news. For instance, there can be certain exaggerated or misleading information attached to a true statement or news. Thus, the entire news or statement can neither be accepted as completely true nor can be discarded In our current work we propose an ensemble architecture based on CNN (Kim, 2014) and Bi11 LSTM (Hochreiter and Schmidhuber, 1997), and this has been evaluated on Liar (Wang, 2017) dataset. Our proposed model tries to capture the pattern of information from the short statements and learn the characteristic behavior of the source speaker from the different attributes provided in the dataset, and finally integrate all the knowledge learned to produce fine-grained multi-class classification. 2 Methodology We propose a deep multi-label classifier for classifying a statement into six fine-grained classes of fake news. Our approach is based on an ensemble model that makes use of"
2019.icon-1.2,I17-2043,0,0.0738317,"a committee hearing president, ceo of Apartments for America, Inc. fake news detection. Further integrating these two elements improves the performance of the classifier. as entirely false. This problem was addressed by Wang (2017) where they introduced Liar dataset comprising of a substantial volume of short political statements having six different class annotations determining the amount of fake content of each statement. In his work, he showed comparative studies of several statistical and deep learning based models for the classification task and found that the CNN model performed best. Long et al. (2017) in their work used the Liar dataset, and proposed a hybrid attention-based LSTM model for this task, which outperformed W.Yang’s hybrid CNN model, establishing a new state-of-the-art. Problems related to these topics have mostly been viewed concerning binary classification. Likewise, most of the published works also has viewed fake news detection as a binary classification problem (i.e., fake or true). But by observing very closely it can be seen that fake news articles can be classified into multiple classes depending on the fakeness of the news. For instance, there can be certain exaggerate"
2019.icon-1.27,Q17-1010,0,0.0127402,"e. Few notable works in this line could be found in (Yin and Roth, 2018; Nie et al., 2019). 2 3.1 3 Proposed Methods We propose two deep Learning based models to address the problem of fake information detection in the multi-domain platform. In the following subsections, we will discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-trained fastText model3 (Bojanowski et al., 2017). FastText embedding model is an extended version of Word2Vec (Mikolov et al., 2013). Word2Vec (predicts embedding of a word based on given context and vice-versa) and Glove (exploits count and word co-occurrence matrix to predict embedding of a word) (Pennington et al., 2014) both treat each word as an atomic entity. The fastText model produces embedding of each word by combining the embedding of each character n-gram of that word. The model works better on rare words and also produces embedding for A sufficient number of works could be found in the literature in fake news detection. Nowadays"
2019.icon-1.27,N19-1423,0,0.015853,"ta and Tested on Domain wise Data) Figure 4: Word Level Attention on News Topic In these Figures, words with more deeper colour indicate that they are getting more attention. We can observe, the words secretary, education in 4 and President, Donald in 5 are the words having deeper colour, i.e. these words are getting more weight compared to others. These words are Named Entities (NEs). It could be concluded that NEs phrases are important in fake news detection in multi domain setting. 4.1 • It would be interesting for this work to encode the domain information in the Deep Neural Nets. • BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) embedding based model and make a comparison with fastText and ELMo based models in the context of fake news detection. Error Analysis We extract the mis-classified and also the truly classified instances produced by the best performing system. We perform a rigorous analysis of these instances and try to find out the pattern in the mis-classified instances and the linguistics differences between those two categories of instances. It is found that the model fails mostly in the Entertainment followed by the sports and the Business domain etc. To name a few, we are s"
2019.icon-1.27,D14-1162,0,0.0830753,"ll discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-trained fastText model3 (Bojanowski et al., 2017). FastText embedding model is an extended version of Word2Vec (Mikolov et al., 2013). Word2Vec (predicts embedding of a word based on given context and vice-versa) and Glove (exploits count and word co-occurrence matrix to predict embedding of a word) (Pennington et al., 2014) both treat each word as an atomic entity. The fastText model produces embedding of each word by combining the embedding of each character n-gram of that word. The model works better on rare words and also produces embedding for A sufficient number of works could be found in the literature in fake news detection. Nowadays the detection of fake news is a hot area of research and gained much more research interest among the researchers. We could detect fake news at two levels, namely the conceptual level and operational level. Rubin et al. (2015) defined that conceptually there are three types o"
2019.icon-1.27,C18-1287,0,0.0254715,"Missing"
2019.icon-1.27,N16-1138,0,0.0337415,"Missing"
2019.icon-1.27,N18-1202,0,0.0456749,"itics, and Entertainment). The Celebrity dataset is crawled directly from the web of celebrity gossips. It covers celebrity news. The AMT manually generated fake version of a news based on the real news. We extract the data domain wise to get the statistics of the dataset. It is observed that each domain contains equal number of instances (i.e. 80). The class distribution among each domain is also evenly distributed. The statistics of these two datasets is shown in the following Table 1. Model 2 We propose another approach whose embedding layer is based on Embedding for Language Model (ELMo) (Peters et al., 2018) and the MLP Network, which is same as we applied in Model 1. The diagram of this model is shown in the Figure 3. Embedding Layer: Embedding from Language Model (ELMo) has several advantages over the other word vector methods, and found to be a good performer in many challenging NLP problems. It has key features like i. Contextual i.e. representation of each word is based on entire corpus in which it is used ii. Deep i.e. it combines all layers of a deep pre-trained neural network and iii. Character based i.e. it provides representations which are based on character, thus allowing the network"
2019.icon-1.27,C18-1158,0,0.0411402,"Missing"
2019.icon-1.27,P18-1022,0,0.0637221,"Missing"
2019.icon-1.27,W16-0802,0,0.110144,"Missing"
2019.icon-1.27,D18-1010,0,0.0288674,"Missing"
2019.icon-1.27,N18-1074,0,0.0684144,"participating systems of the Fake News Challenge. The work of Saikh et al. (2019) detected fake news through stance detection and also correlated this stance classification problem with Textual Entailment (TE). They tackled this problem using statistical machine learning and deep learning approaches separately and with combination of both of these. This system achieved the state of the art result. Another remarkable work in this line is the verification of a human- generated claim given the whole Wikipedia as evidence. The dataset, namely (Fact Extraction and Verification (FEVER)) proposed by Thorne et al. (2018) served this purpose. Few notable works in this line could be found in (Yin and Roth, 2018; Nie et al., 2019). 2 3.1 3 Proposed Methods We propose two deep Learning based models to address the problem of fake information detection in the multi-domain platform. In the following subsections, we will discuss the methods. Related Work Model 1 This model comprises of multiple layers as shown in the Figure 1. The layers are A. Embedding Layer B. Encoding Layer (Bi-GRU) C. Word level Attention D. Multi-layer Perceptron (MLP). A. Embedding Layer: The embedding of each word is obtained using pre-traine"
2019.icon-1.27,P17-2067,0,0.0568217,"Missing"
2019.icon-1.27,N16-1174,0,0.0234722,"ultiplication to (1-z) with h. Take the summation of i and ii. The bidirectional GRUs consists of the forward GRU, which reads the sentence from the first word (w1 ) to the last word (wL ) and the backward GRU, that reads in reverse direction. We concatenate the representation of each word obtained from both the passes. C. Word Level Attention: We apply the attention model at word level (Bahdanau et al., 2015; Xu et al., 2015). The objective is to let the model decide which words are importance compared to other words while predicting the target class (fake/legit). We apply this as applied in Yang et al. (2016). The diagram is shown in the Figure 2. We take the aggregation of those words’ representation which are multiplied with attention weight to get sentence representation. We do this process for both the news topic and the corresponding document. This particular technique of the word attention mechanism, has not been tried for solving such a problem. In equation 1, z is the update gate at time step t. This z is the summation of the multiplications of xt with it’s own weight U(z) and st−1 (holds the information of previous state) with it’s own W(z). A sigmoid α is applied on the summation to sque"
2020.aacl-main.31,P19-1455,0,0.0300492,"Missing"
2020.aacl-main.31,D19-1566,1,0.830157,"uce a deep-learningbased multi-modal Textual Kernels Model (TKM) and compare it with various existing deep learning architectures on the proposed MMHS150K dataset. Motivation: Swieczkowska et al. (2020) proposes a novel chaining method of neural networks for identifying motivational texts where the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using various syntactic and semantic features, such as frequency of words, the intensity of adverbs and adjectives, the gap between po"
2020.aacl-main.31,2020.acl-main.401,1,0.725064,"ning BERT (Devlin et al., 2018) model for humour detection on three languages (Chinese, Spanish and Russian). Sarcasm: Starting from the traditional approaches, such as rule-based methods (Veale and Hao, 2010), lexical features (Carvalho et al., 2009), and incongruity (Joshi et al., 2015) to all the way up to multi-modal deep learning techniques (Schi282 fanella et al., 2016), sarcasm detection has been showing its presence. Castro et al. (2019) created a multi-modal conversational dataset, MUStARD from the famous TV shows, and provided baseline SVM approaches for sarcasm detection. Recently, Chauhan et al. (2020) proposed a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis to explore how sentiment and emotion helps sarcasm. The author used the MUStARD dataset and extended the MUStARD dataset with sentiment (implicit and explicit) and emotion (implicit and explicit) labels. Offensive: Razavi et al. (2010) used a threelevel classification model taking advantage of various features from statistical models and rulebased patterns and various dictionary-based features. Chen et al. (2012) proposed a feature-based Lexical Syntactic Feature (LSF) architecture to detect the o"
2020.aacl-main.31,D18-1382,1,0.747096,"the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using various syntactic and semantic features, such as frequency of words, the intensity of adverbs and adjectives, the gap between positive and negative terms, the structure of the sentence, synonyms and others (Barbieri and Saggion, 2014). More recently, Badlani et al. (2019) proposed a convolution-based model to extract the embedding by fine-tuning the same for the tasks of sentiment, sarcasm, humour, and hate-speech and"
2020.aacl-main.31,P15-2124,1,0.796519,"nd Pulman, 2007). Some of the recent multi-modal approaches include utilizing information from the various modalities, such as acoustic, visual, and text, using deep learning models (Bertero and Fung, 2016; Yang et al., 2019; Swamy et al., 2020). Yang et al. (2020) employs a paragraph decomposition technique coupled with fine-tuning BERT (Devlin et al., 2018) model for humour detection on three languages (Chinese, Spanish and Russian). Sarcasm: Starting from the traditional approaches, such as rule-based methods (Veale and Hao, 2010), lexical features (Carvalho et al., 2009), and incongruity (Joshi et al., 2015) to all the way up to multi-modal deep learning techniques (Schi282 fanella et al., 2016), sarcasm detection has been showing its presence. Castro et al. (2019) created a multi-modal conversational dataset, MUStARD from the famous TV shows, and provided baseline SVM approaches for sarcasm detection. Recently, Chauhan et al. (2020) proposed a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis to explore how sentiment and emotion helps sarcasm. The author used the MUStARD dataset and extended the MUStARD dataset with sentiment (implicit and explicit) and emotio"
2020.aacl-main.31,P19-1046,0,0.024098,"the offensive contents. Gomez et al. (2020) created a multi-modal hate-speech dataset from Twitter (MMHS150K) to introduce a deep-learningbased multi-modal Textual Kernels Model (TKM) and compare it with various existing deep learning architectures on the proposed MMHS150K dataset. Motivation: Swieczkowska et al. (2020) proposes a novel chaining method of neural networks for identifying motivational texts where the output from one model is passed on to the second model. Sentiment: An important task to leverage multimodality information effectively is to combine them using various strategies. Mai et al. (2019) employs a hierarchical feature fusion strategy, Divide, Conquer, and Combine for affective computing. Chauhan et al. (2019) uses the Inter-modal Interaction Module (IIM) to combine information from a pair of modalities for multi-modal sentiment and emotion analysis. Some of the other techniques include a contextual inter-modal attention based framework for multi-modal sentiment classification (Ghosal et al., 2018; Akhtar et al., 2019). Multi-task: Some of the early attempts to correlate the tasks like sarcasm, humour, and offensive statements include a features based classification using vari"
2020.aacl-main.31,2020.semeval-1.156,0,0.061348,"Missing"
2020.aacl-main.33,D19-1366,0,0.0355433,"Missing"
2020.aacl-main.33,N18-1169,0,0.0529168,"Missing"
2020.aacl-main.33,S14-2004,0,0.235349,"level polarity has been well explored as a style transfer task. Zhang et al. (2018) used unsupervised machine translation techniques for polarity transfer in sentences. Yang et al. (2018) equal contribution Service - Negative Salads - Positive Chicken - Positive Figure 1: An example of the proposed aspect-level sentiment style transfer task Introduction § Service - Positive Salads - Positive Chicken - Negative In this paper we explore a more fine-grained style transfer task, where each aspect’s polarities can be changed individually. Recent interest in Aspect-Based Sentiment Analysis (ABSA) (Pontiki et al., 2014) has shown that sentiment information can vary within a sentence, with differing sentiments expressed towards different aspect terms of target entities (e.g. ‘food’, ‘service’ in a restaurant domain). We introduce the task of aspect-level sentiment transfer - the task of rewriting sentences to transfer them from a given set of aspect-term polarities (such as ‘positive sentiment’ towards the service of a restaurant and a ‘positive sentiment’ towards the taste of the food) to a different set of aspect-term polarities (such as ‘negative sentiment’ towards the service of a restaurant and a ‘positi"
2020.aacl-main.33,P18-1080,0,0.0192363,"ngle, and differing styles can often be present together when discussing different topics and entities. Our work intends to take the first step towards a more controllable form of finegrained style transfer with the task of aspect-level sentiment style transfer. 2 Related Work In this section we present an overview of the related literature. 2.1 Sentiment Transfer To the best of our knowledge, our current work is the first to tackle aspect-level sentiment transfer. Most of the previous works involving sentiment transfer (Li et al., 2018b; Yang et al., 2018; Shen et al., 2017; Xu et al., 2018; Prabhumoye et al., 2018; Wu et al., 2019) consider the style that is present throughout the sentence and seek to transfer only the overall sentiment polarities expressed. Tian et al. (2018) proposed a new training objective for content preservation during style transfer. They used Part-of-Speech (PoS) tagging to collect nouns at inputs, and expect them to be present at the output for content preservation. To achieve this, they proposed a PoS preservation constraint and ‘Content Conditional Language Modelling’. They tested their system on sentiment style transfer task. Wang et al. (2019) proposed a method that can al"
2020.aacl-main.33,W14-4012,0,0.0397672,"Missing"
2020.aacl-main.33,E99-1023,0,0.163673,"sed polarity classification model which were trained on the SemEval ABSA training data, to generate aspect-level sentiment data from the Yelp reviews dataset. Table 1 shows some statistics from the datasets. 4.1.1 Aspect based Sentiment Analysis with BERT A pipeline of BERT-based models was trained for target-extraction and aspect-level polarity classification over the SemEval dataset. These are the models used to extract target-aspects and their polarities from the Yelp dataset. The target extraction task was posed as a sequential token classification problem with BERT using the IOB2 format (SANG, 1999). This BERT model was fed the whole sentence as the input segment and it obtained an F1-score of 0.8012 (evaluation carried out similar to Sang and Buchholz (2000)). The sentiment-polarity prediction task is posed as a sentence-pair classification problem using BERT, with the sentence provided as the first segment and the aspect-term as the second segment. This model obtained an F1-score of 0.9080 for the positive po307 Classifier Score Classifier Score Classifier Score Classifier Score (Overall) (1-Aspect) (2-Aspects) (3-or-more Aspects) BERT-Baseline (BB) 0.5158 0.4983 0.5448 0.5036 BB + MLM"
2020.aacl-main.33,N19-1423,0,0.217718,"be able to correctly process the aspectpolarity query and accordingly delete, replace and generate text sequence to satisfy the query. (iii). The polarities of the aspects not in the query should not be affected. (iv). The non-attribute content and fluency of the text should be preserved. We explore this task in an unsupervised setting (as is common with most style-transfer tasks due to the lack of an aligned parallel corpus) using only monolingual unaligned corpora. In this work, a novel encoder-decoder architecture is proposed to perform unsupervised aspect-level sentiment transfer. A BERT (Devlin et al., 2019) based encoder is used that is trained to understand aspect-specific polarity information. We also propose using a ‘polarity injection’ method, where saliency-weighted aspect-specific polarity information is added to the hidden representations from the encoder to complete the query for the decoder. 1.1 Motivation The Aspect-Based Sentiment Analysis (ABSA) task shows that differing sentiments can be present within the same sentence, localized to different entities or parts of the text. The notion of styles in natural language can be used to refer to the attributes, such as sentiment, formality"
2020.aacl-main.33,W00-0726,0,0.0318882,"dataset. Table 1 shows some statistics from the datasets. 4.1.1 Aspect based Sentiment Analysis with BERT A pipeline of BERT-based models was trained for target-extraction and aspect-level polarity classification over the SemEval dataset. These are the models used to extract target-aspects and their polarities from the Yelp dataset. The target extraction task was posed as a sequential token classification problem with BERT using the IOB2 format (SANG, 1999). This BERT model was fed the whole sentence as the input segment and it obtained an F1-score of 0.8012 (evaluation carried out similar to Sang and Buchholz (2000)). The sentiment-polarity prediction task is posed as a sentence-pair classification problem using BERT, with the sentence provided as the first segment and the aspect-term as the second segment. This model obtained an F1-score of 0.9080 for the positive po307 Classifier Score Classifier Score Classifier Score Classifier Score (Overall) (1-Aspect) (2-Aspects) (3-or-more Aspects) BERT-Baseline (BB) 0.5158 0.4983 0.5448 0.5036 BB + MLM pretraining (BB0.5298 0.5433 0.5310 0.5145 MLM) BB-MLM + one-zero polar0.5415 0.5675 0.5276 0.5290 ity injection BB-MLM + saliency-based 0.5918 0.6125 0.5828 0.57"
2020.aacl-main.33,D13-1176,0,0.0223727,"show that the system is successful in controlling aspect-level sentiments. 1 Query: Service - Negative Salads - Positive Chicken - Positive The service was slow, but the salads were great and the chicken was tasty and fresh. used language models as discriminators to achieve style (polarity) transfer in sentences. Li et al. (2018a) proposed a simpler method where they deleted the attribute markers and devise a method to replace or generate the target attribute-key phrases in the sentence. With a rapid increase in the quality of generated text, due to the rise of neural text generation models (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Vaswani et al., 2017), controllable text generation is quickly becoming the next frontier in the field of text generation. Controllable text generation is the task of generating realistic sentences whose attributes can be controlled. The attributes to control can be: (i). Stylistic: Like politeness, sentiment, formality etc, (ii). Content: Like information, entities, keywords etc. or (iii). Ordering: Like ordering of information, events, plots etc. Controlling sentence level polarity has been well explored as a style transfer task. Zhang et al. (2018"
2020.aacl-main.33,N19-1242,0,0.0623264,"Missing"
2020.aacl-main.90,agerri-etal-2014-ixa,0,0.0254988,"layers to better encode the multilingual and code-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority"
2020.aacl-main.90,W14-3914,0,0.0140445,"This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from the large-scale ima"
2020.aacl-main.90,bojar-etal-2014-hindencorp,0,0.0606349,"Missing"
2020.aacl-main.90,D18-1024,0,0.0918512,"g of dimension 300 using the word embedding algorithm (Bojanowski et al., 2016). In order to obtain the embedding of Roman script, we transliterate6 the Hindi sentence into the Roman script. These sentences are used to train the code-mixed embedding using the same embedding algorithm (Bojanowski et al., 2016), and we generate the embedding of dimension 300. These three word embeddings have the same dimensions but they are different in vector spaces. Finally, we align monolingual vectors of Hindi and Roman words into the vector space of English word embedding using the approach as discussed in Chen and Cardie (2018). While training, the model loss is computed using the categorical cross entropy function. Optimal hyper-parameters are set to: maximum no. of words in a question=15, CNN filter size={2, 3}, # of shared CNN layers=1, # of shared Bi-LSTM layers=2, hidden dimension =1000, # of attention heads=4, image level and object level feature dimension =2048, # of spatial location in image level feature =100, # of objects in object level feature=36, # of rank in bi-linear pooling=3, # of bilinear attention maps=8, # of epochs=100, initial learning rate=0.002. Optimal values of the hyperparameters are chose"
2020.aacl-main.90,D16-1044,0,0.252498,"swering Deepak Gupta‡ , Pabitra Lenka†∗, Asif Ekbal‡ , Pushpak Bhattacharyya‡ ‡ Indian Institute of Technology Patna, India † International Institute of Information Technology Bhubaneswar, India ‡ {deepak.pcs16, asif, pb}@iitp.ac.in † pabitra.lenka18@gmail.com Abstract and predicts the decision by analyzing the complex scene(s). VQA requires language understanding, fine-grained visual processing and multiple steps of reasoning to produce the correct answer. As the existing research on VQA are mainly focused on natural language questions written in English (Antol et al., 2015; Hu et al., 2017; Fukui et al., 2016; Anderson et al., 2018; Li et al., 2018; Xu and Saenko, 2016; Shih et al., 2016), their applications are often limited. In this paper, we propose an effective deep learning framework for multilingual and codemixed visual question answering. The proposed model is capable of predicting answers from the questions in Hindi, English or Codemixed (Hinglish: Hindi-English) languages. The majority of the existing techniques on Visual Question Answering (VQA) focus on English questions only. However, many applications such as medical imaging, tourism, visual assistants require a multilinguality-enable"
2020.aacl-main.90,L18-1278,1,0.921004,"ode-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from"
2020.aacl-main.90,K18-1012,1,0.872702,"ode-mixed questions. This bridges the gap between VQA and multilinguality. 4. We perform extensive evaluation and ablation studies for English, Hindi and Code-mixed VQA. The evaluation shows that our proposed multilingual model achieves state-of-the-art performance in all these settings. 2 Related Work Multilingual and Code-Mixing: Recently, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving multilingual (Garcia and Gamallo, 2015; Gupta et al., 2019; Agerri et al., 2014) and code-mixed languages (Gupta et al., 2018a; Bali et al., 2014; Gupta et al., 2016; Rudra et al., 2016; Gupta et al., 2014). Developing a VQA system in a code-mixed scenario is, itself, very novel in the sense that there has not been any prior research towards this direction. VQA Datasets: Quite a few VQA datasets (Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017; Johnson et al., 2017; Shimizu et al., 2018; Hasan et al., 2018; Wang et al., 2018) have been created to encourage multi-disciplinary research involving Natural Language Processing (NLP) and 901 Computer Vision. In majority of these datasets, the images are taken from"
2020.aacl-main.90,P18-1249,0,0.0264607,"esentation of ith language obtained from the j th layer can be denoted as Qi,j = {q1i,j , q2i,j , . . . , qTi,j }. In our work, we use one layer of CNN and two layers of Bi-LSTM to encode multilingual questions. At each layer of encoding, we apply language specific weight to obtain the language specific encoding layer representation. We denote the question representation obtained from the final encoding layer after applying the language specific attention as h = {ht }Tt=1 . 4.1.2 Self-Attention on Question Inspired from the success of self-attention on various NLP tasks (Vaswani et al., 2017; Kitaev and Klein, 2018), we adopt self-attention to our model for better representation of a word by looking at the other words in the input question. The encoding obtained from multilingual encoding layer (c.f. Section 4.1.1) is passed to the self-attention layer. The multi-head self-attention mechanism (Vaswani et al., 2017) used in our model can be precisely described as follows: QK T Attention(Q, K, V ) = softmax( √ )V dh (5) where, Q, K, V and dh are the query, key, value matrices and dimension of the hidden representation obtained from the multilingual encoding layer, respectively. These matrices are obtained"
2020.aacl-main.90,W04-1013,0,0.0127666,"enough to offer multilingual and code-mixing words belonging to the was mostasked frequent languagecrein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qte }Tt=1 . The convo"
2020.aacl-main.90,D15-1166,0,0.0238489,"f different window sizes is apto provide an accurate natural language answer Aˆ plied on the embedded input. The final output qtc from all the possible answers A. Mathematically: at a time step t is computed by the max-pooling operation over different window size filters. Mathˆ I; φ) Aˆ = arg max p(A|Q, (1) ematically, qtc = max(qtl1 ,c , qtl2 ,c , . . . , qtlL ,c ). The ˆ A∈A final representation computed by CNN layer can be where φ is the network parameters. The architec- denoted as {q c }T . Inspired from the success in t t=1 ture of our proposed methodology is depicted in other NLP tasks (Luong et al., 2015; Yue-Hei Ng Fig 3. Our proposed model has the following com- et al., 2015), we employ stacking of multiple Biponents: LSTM (Hochreiter and Schmidhuber, 1997) layers to capture the semantic representation of an entire 4.1 Multilingual Question Encoding question. The input to the first layer of LSTM is the Given a question5 Q = {q1 , q2 , . . . , qT } having convoluted representation of the question {qtc }Tt=1 . T words, we obtain the multilingual embedding 5 r qtr = Bi-LSTM(qt−1 , qtc ) It denotes the question in English, Hindi or Code-mixed 903 (3) 286 287 288 289 290 291 292 293 294 295 296"
2020.aacl-main.90,P02-1040,0,0.106618,"from our MCVQA dataset. A ble enough to offer multilingual and code-mixing words belonging to the was mostasked frequent languagecrein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qt"
2020.aacl-main.90,D16-1121,0,0.0246249,"Missing"
2020.aacl-main.90,C18-1163,0,0.0368273,"Missing"
2020.aacl-main.90,2006.amta-papers.25,0,0.0114059,"crein Piotr Bojanowski, Grave, Armand bilingual (En, Hi) expert to manually capability. For aEdouard better multilingual andJoulin, codeand Tomas Mikolov. 2016. Enriching Word Vecthe sentence. ate the code-mixed questions and translate the En- mixing capability at a higher level, we introduce the tors with Subword Information. arXiv preprint glish questions into Hindi. We shared encoding layers. In order to capture the noW compute the BLEU LF = N arXiv:1607.04606. (Papineni et al., 2002), ROUGE (Lin, 2004) and tion of a phrase, first the embedded input {qte }Tt=1 Translation Error Rate (TER) (Snover et al., 2006) is passed to a CNN layer. Mathematically, we comon the human translated questions and the trans- pute inner product between the filter Fl ∈ Rl×d and lations obtained from the Google Translate. We 3 the windows of l word embedding. In order to mainachieve high BLEU and Rouge scores (BLEU 3: tain the length of the question after convolution, we 80.22; ROUGE - L: 92.20) and lower TER (9.63). perform appropriate zero-padding to the start and end of the embedded input {qte }Tt=1 . The convo4 Methodology for MVQA luted feature qtl,c for l length filter is computed as follows: Problem Statement: Giv"
2020.acl-main.401,N19-1034,1,0.676575,"earchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in detecting sarcasm, this has not been attempted much, one of the main reasons being the non-availability of multi-modal datasets. Recently, researchers (Castro et al., 2019) have started exploiting multi-modal sources of information for sarcasm detection. It is true that the modalities like acoustic and visual often provide more evidences about the context of the utterance in compar"
2020.acl-main.401,C16-1151,0,0.0266981,"ation across the modalities to effectively classify sarcasm, sentiment, and emotion. (c). We annotate the recently released Sarcasm dataset, MUStARD with sentiment and emotion classes (both implicit and explicit), and (d). We present the state-of-the-art for sarcasm prediction in multi-modal scenario. 2 Related Work A survey of the literature suggests that a multimodal approach towards sarcasm detection is a fairly new approach rather than a text-based classification. Traditionally, rule-based classification (Joshi et al., 2017; Veale and Hao, 2010) approaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair"
2020.acl-main.401,P19-1455,0,0.185013,"Missing"
2020.acl-main.401,D19-1566,1,0.807181,"proaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in det"
2020.acl-main.401,W14-4012,0,0.0231513,"Missing"
2020.acl-main.401,D18-1382,1,0.514853,"a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask learning framework has been introduced for sentiment and emotion recognition. Although multi-modal sources of information (e.g., audio, visual, along with text) offers more evidence in detecting sarcasm, this has not been attempted much, one of the main reasons being the non-availability of multi-modal datasets. Recently, researchers (Castro et al., 2019) have started exploiting multi-modal sources of information for sarcasm de"
2020.acl-main.401,P19-1046,0,0.0960793,"proach towards sarcasm detection is a fairly new approach rather than a text-based classification. Traditionally, rule-based classification (Joshi et al., 2017; Veale and Hao, 2010) approaches were used for sarcasm detection. Poria et al. (2016) have exploited sentiment and emotion features extracted from the pre-trained models for sentiment, emotion, and personality on a text corpus, and use them to predict sarcasm through a Convolutional Neural Network. In recent times, the use of multi-modal sources of information has gained significant attention to the researchers for affective computing. Mai et al. (2019) proposed a new two-level strategy (Divide, Conquer, and Combine) for feature fusion through a Hierarchical Feature Fusion Network for multimodal affective computing. Chauhan et al. (2019) exploits the interaction between a pair of modalities through an application of Inter-modal Interaction Module (IIM) that closely follows the concepts of an auto-encoder for the multi-modal sentiment and emotion analysis. Ghosal et al. (2018) proposed a contextual inter-modal attention based framework for multi-modal sentiment classification. In other work (Akhtar et al., 2019), an attention-based multitask"
2020.coling-main.249,P18-1128,0,0.0143589,"ovement of 4.02 and 3.18 points compared to NQG and Max-out Pointer model, respectively, in terms of BLEU-4 metric. To analyze the contribution of each component of the proposed model, we perform an ablation study reported in Table 4. Our results suggest that providing multitask learning with shared encoder helps the model to improve the QG performance from 19.55 to 20.64 BLEU-4. Introducing the supporting facts information obtained from the answer-aware supporting fact prediction task further improves the 2 We follow the bootstrap test (Efron and Tibshirani, 1994) using the setup provided by Dror et al. (2018). 2768 QG performance from 20.64 to 21.28 BLEU-4. Joint training of QG with the supporting facts prediction provides stronger supervision for identifying and utilizing the supporting facts information. In other words, by sharing the document encoder between both the tasks, the network encodes better representation (supporting facts aware) of the input document. Such presentation is capable of efficiently filtering out the irrelevant information when processing multiple documents and performing multi-hop reasoning for question generation. Further, the MultiHop-Enhanced Reward (MER) with Rouge r"
2020.coling-main.249,P17-1123,0,0.0188867,"ting questions. The former regime consists of rule-based approaches (Heilman and Smith, 2010; Chali and Hasan, 2015) that rely on human-designed features such as named-entity information, etc. to leverage the semantic information from a context for question generation. In the second category, question generation problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy whe"
2020.coling-main.249,P16-1014,0,0.0845371,"Missing"
2020.coling-main.249,P18-1064,0,0.0240707,"a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer gener"
2020.coling-main.249,L18-1440,1,0.908711,"Missing"
2020.coling-main.249,K18-1012,1,0.85926,"Missing"
2020.coling-main.249,W13-2114,0,0.0390926,"Missing"
2020.coling-main.249,D15-1166,0,0.0436525,"the sentence to be a supporting fact. The architecture of this network is illustrated in Figure 1 (left). This network is then trained with a binary cross entropy loss and the ground-truth supporting facts labels: Lsp = − N nj 1 XX δ j log(pji ) + (1 − δyj 6=1 ) log(1 − pji ) i N j=1 i=1 yi =1 (4) where N is the number of document list, S the number of candidate sentences in a particular training example, δij and pji represent the ground truth supporting facts label and the output Sigmoid probability, respectively. 3.1.3 Question Decoder We use a LSTM network with global attention mechanism (Luong et al., 2015) to generate the question ˆ = {y1 , y2 , . . . , ym } one word at a time. We use copy mechanism (See et al., 2017; Gulcehre et al., Q 2016) to deal with rare or unknown words. At each timestep t, st = LST M (st−1 , yt−1 ) (5) The attention distribution αt and context vector ct are obtained using the following equations: et,i = sTt ∗ hi exp(et,i ) αt,i = PN j=1 exp(et,j ) ct = N X (6) αt,i hi i=1 The probability distribution over the question vocabulary is then computed as, Pvocab = softmax(tanh(Wq ∗ [ct ⊕ st ])) (7) where Wq is a weight matrix. The probability of picking a word (generating) fr"
2020.coling-main.249,P17-1018,0,0.209642,"n problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominant"
2020.coling-main.249,K17-1028,0,0.0198355,"he first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are s"
2020.coling-main.249,Q18-1021,0,0.118683,"QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are spread across multiple documents. 3 Proposed Approach Problem Statement: In multi-hop question generation, we consider a document list L with nL documents, and an m-word answer A. Let the total number of words in all the document"
2020.coling-main.249,D18-1259,0,0.379055,"2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang et al., 2018) that promote multi-step inference across several documents. So far, multi-hop datasets have been predominantly used for answer generation tasks (Seo et al., 2017a; Tay et al., 2018; Zhang et al., 2018). Our work can be seen as an extension to single hop question generation where a non-trivial number of supporting facts are spread across multiple documents. 3 Proposed Approach Problem Statement: In multi-hop question generation, we consider a document list L with nL documents, and an m-word answer A. Let the total number of words in all the documents Di ∈ L combined be N . Let a document list"
2020.coling-main.249,D19-1253,0,0.0976439,"Missing"
2020.coling-main.249,D18-1424,0,0.210277,"c. to leverage the semantic information from a context for question generation. In the second category, question generation problem is treated as a sequence-to-sequence (Sutskever et al., 2014) learning problem, which involves automatic learning of useful features from the context by leveraging the sheer volume of training data. The first neural encoder-decoder model for question generation was proposed in Du et al. (2017). However, this work does not take the answer information into consideration while generating the question. Thereafter, several neural-based QG approaches (Sun et al., 2018; Zhao et al., 2018; Chen et al., 2018) have been proposed that utilize the answer position information and copy mechanism. Wang et al. (2017a) and Guo et al. (2018) demonstrated an appreciable improvement in the performance of the QG task when trained in a multi-task learning framework. The model proposed by Seo et al. (2017b) and Weissenborn et al. (2017) for single-document QA experience a significant drop in accuracy when applied in multiple documents settings. This shortcoming of single-document QA datasets is addressed by newly released multi-hop datasets (Welbl et al., 2018; Talmor and Berant, 2018; Yang"
2020.coling-main.249,N10-1086,0,\N,Missing
2020.coling-main.249,P02-1040,0,\N,Missing
2020.coling-main.249,W05-0909,0,\N,Missing
2020.coling-main.249,W04-1013,0,\N,Missing
2020.coling-main.249,D14-1162,0,\N,Missing
2020.coling-main.249,W17-2623,0,\N,Missing
2020.coling-main.249,P17-1099,0,\N,Missing
2020.coling-main.249,D18-1427,0,\N,Missing
2020.coling-main.393,W11-0705,0,0.0410433,"est of the paper is structured as follows. In Section 2, we present a brief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning"
2020.coling-main.393,N19-1034,1,0.87851,"Missing"
2020.coling-main.393,W17-5526,0,0.0606686,"Missing"
2020.coling-main.393,D19-1566,1,0.844274,"lated work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affect analysis and explored a contextual inter-modal attention framework tha"
2020.coling-main.393,L18-1252,0,0.0576593,"on. Our present work differs from these single and multi-label emotion and sentiment classification works as we tend to classifying emotions and sentiments on dialogue conversations that require contextual information of the previous utterances, thereby making the task more challenging and interesting. Every human-machine interactions are grounded in conversations driven by emotions. Hence, identifying the emotion in dialogue is essential for building robust systems capable of such interactions. Recently, investigations on emotion detection in conversations has been in demand. The authors in (Chen et al., 2018) released a dataset taken from Friends TV series for detecting emotions in dialogues. Similarly, in (Yeh et al., 2019) an attention framework was designed for identifying emotions in spoken dialog systems. In (Hazarika et al., 2018b), memory networks were adopted to capture contextual information for emotion detection in conversations. To capture the contextual information in conversations, DialogueRNN (Majumder et al., 2019) employs three gated recurrent units (GRU) for effectively modeling the past utterances of the speaker and the listener in dyadic conversations for emotion detection. As c"
2020.coling-main.393,D18-1382,1,0.752804,"rief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affect analysis and explored a contextual inter-modal"
2020.coling-main.393,D19-1015,0,0.0153091,"classification of sentiments (Poria et al., 2017; Majumder et al., 2018). The authors in (Majumder et al., 2018) proposed a novel hierarchical feature fusion strategy for integrating different modalities, such as audio, video and text for identifying the sentiments. The authors in (Poria et al., 2019) extended the EmotionLines dataset by incorporating audio and visual modalities for correct identification of emotions and sentiments in conversations. The MELD dataset has been further used for building different neural frameworks for jointly identifying emotion and sentiment from conversations (Ghosal et al., 2019; Zhang et al., 2019b; Zhang et al., 2019a). As opposed to these existing works on multimodal emotion and sentiment classification on dialogue data, our present works provides a balanced multimodal multi-label emotion, intensity and sentiment dataset for the classification of multiple emotions and sentiment in the given utterance. 3 Multimodal Multi-label Emotion, Intensity and Sentiment Dialogue (MEISD) Dataset We create the MEISD dataset1 from the 10 famous TV shows belonging to different genres: (i). Comedy: Friends, The Big Bang Theory, How I Met Your Mother, The Office; (ii). Drama: House"
2020.coling-main.393,D18-1280,0,0.0795982,"balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agen"
2020.coling-main.393,N18-1193,0,0.0988445,"balanced Multimodal Multi-label Emotion, Intensity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agen"
2020.coling-main.393,S18-1019,0,0.014699,"ring utterances and their multi-modal information. With the advancements in Artificial Intelligence (AI), emotion classification and sentiment analysis have become a significant task due to its importance in many downstream tasks, such as customer behavior modeling, response generation for conversational agents, multimodal interactions etc. Hence, to maximize user satisfaction and providing a better experience to the customer, it is important to understand the correct emotion and sentiment of the customer. Recently, multi-label emotion classification has been investigated for textual data in (Kim et al., 2018; He and Xia, 2018; Yu et al., 2018; Huang et al., 2019). Using multiple Convolution Neural Network (CNN) networks along with self-attention, the authors in (Kim et al., 2018) performed multi-label emotion classification on twitter data. Similarly, the authors in (Yu et al., 2018) improved the performance of multi-label emotion classification on twitter data by using transfer learning. Lately, sequence-to-sequence framework (Huang et al., 2019) has been employed for multi-label emotion classification. Our present work differs from these single and multi-label emotion and sentiment classificati"
2020.coling-main.393,D14-1181,0,0.00738607,"Torch 3 framework. Based on the validation set, we set the threshold value of 0.2 for the classification of multiple emotions in a given utterance. For all the baselines, in the final output layer we apply softmax activation function for emotion and sentiment classification while we apply sigmoid activation function for intensity prediction. text-CNN: In this approach, we only use the textual information for identifying the emotion and sentiment of every utterance in a dialogue. In this framework, we use the word embeddings of the utterances as input to the convolutional neural network (CNN) (Kim, 2014) for obtaining the sentence representation. In this model, we do not use the contextual information or the additional information from the different modalities for identifying the emotion or sentiment of an utterance. bcLSTM: This baseline employing bi-directional RNN for capturing the contextual information was proposed by (Poria et al., 2017). It employs a two-step hierarchical mechanism that captures the unimodal context first followed by the bi-modal context features. In this methodology, we incorporate the provision of capturing information from all the three modalities. A CNN-LSTM approa"
2020.coling-main.393,W17-5205,0,0.0497398,"Missing"
2020.coling-main.393,S18-1001,0,0.0246803,"conversational context for correctly identifying the emotions, intensity and sentiments in a dialogue. DialogueRNN + BERT: We propose a stronger baseline built upon the DialogueRNN for correct classification of emotion and sentiment, and for intensity prediction. We are able to improve the performance of DialogueRNN by using BERT(Devlin et al., 2018) embedding instead of Glove embedding to represent the textual features. 4.3 Evaluation Metrics For multi-label emotion classification, we use the automatic metrics as mentioned below following the works of (Huang et al., 2019; Yang et al., 2018; Mohammad et al., 2018): Jaccard Index (Rogers and Tanimoto, 1960), Hamming Loss (Schapire and Singer, 1999) and Micro-averaged F1-score (Manning et al., 2008). For sentiment analysis we report Micro-averaged F1-score while for intensity prediction we report Pearson correlation co-efficient (Mohammad and Bravo-Marquez, 2017) in a similar manner as (Akhtar et al., 2019b). 5 Result and Discussion In this section, we provide the results for all the three tasks, i.e. multi-label emotion classification, intensity prediction and sentiment analysis on our proposed MEISD dataset. In Table 5, we provide the results of all th"
2020.coling-main.393,D14-1162,0,0.0833781,"es Train Valid Test 58 22 120 31 1039 114 280 702 93 205 No. of Utterances Train Valid Test 4386 1430 5810 1623 9989 1109 2610 14040 1860 4100 Table 4: Comparison of different multimodal conversational datasets and our proposed MEISD dataset 4 Experiments The extraction of features along with the details of the baseline models to evaluate our proposed MEISD dataset is described in this section. We also discuss the metrics used to evaluate the models on the proposed dataset. 4.1 Feature Extraction Textual Features: For textual features, we take the pre-trained 300-dimensional GloVe embeddings (Pennington et al., 2014) of every word as features. Audio Features: We encode audio tracks with the pre-trained VGGish network (Hershey et al., 2017), which is trained on Audioset (Gemmeke et al., 2017) consisting of 100 million YouTube videos. It has been shown to improve the audio emotion and sentiment classification. We extract audio features of dimension 128 from the last fully connected layer. Visual Features: Due to computational cost, we only consider the middle frame of the video to extract visual feature Vk . We use 2048-dimension pooled features from the last block of Resnet-101 (He et al., 2016) pre-traine"
2020.coling-main.393,P13-1096,0,0.0253225,"Missing"
2020.coling-main.393,P17-1081,0,0.30135,"nsity, and Sentiment Dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features, and then establish a baseline setup for further research. 1 Introduction With the advancements in Artificial Intelligence (AI), the gap between Natural Language Processing (NLP) and Computer Vision (CV) has been bridged by extensive research in multi-modal information analysis. The ability to use different modalities such as text, audio and video for different tasks, such as emotion classification (Tripathi and Beigi, 2018; Hazarika et al., 2018a), sentiment analysis (Poria et al., 2017), dialogue generation (Yoshino et al., 2019; Das et al., 2017) have helped in building robust systems. The potential to understand correct emotion and sentiment in a conversation is crucial for developing strong human-machine interaction systems. Dialogue systems are of two types i.e., goal-oriented systems (Asri et al., 2017) or open chit-chat systems (Serban et al., 2017). In both these systems, understanding the user’s emotions is crucial to maximizing the user experience and satisfaction. Nowadays, there is a huge demand for developing social agents capable of having real conversations wit"
2020.coling-main.393,P19-1050,0,0.310258,"Missing"
2020.coling-main.393,D13-1170,0,0.00392318,"ructured as follows. In Section 2, we present a brief survey of the related work. In Section 3, we describe the details of the dataset that we create. In Section 4, we explain the methodology. The experimental setup, along with the evaluation metrics, is reported in Section 5. In Section 6, we present the results along with the necessary analysis. Finally, we conclude in Section 7 with future work directions. 2 Related Work Most of the early research on emotion classification and sentiment analysis was performed separately upon textual datasets mostly taken from twitter (Agarwal et al., 2011; Socher et al., 2013; Colneriˆc and Demsar, 2018; Ghosal et al., 2018; Chauhan et al., 2019). In (Chauhan et al., 2019), the authors proposed a RNN framework capable of learning inter-modal interaction among the different modalities using the auto-encoder mechanism. As emotion and sentiment are two very closely related tasks, in recent time there is a trend on modeling both sentiment and emotion of an utterance simultaneously (Akhtar et al., 2019a; Akhtar et al., 2019b; Kumar et al., 2019; Akhtar et al., 2020). In (Akhtar et al., 2020), the authors employed the concept of multi-task learning for multi-modal affec"
2020.coling-main.393,C18-1330,0,0.0175392,"nits (GRU) to model conversational context for correctly identifying the emotions, intensity and sentiments in a dialogue. DialogueRNN + BERT: We propose a stronger baseline built upon the DialogueRNN for correct classification of emotion and sentiment, and for intensity prediction. We are able to improve the performance of DialogueRNN by using BERT(Devlin et al., 2018) embedding instead of Glove embedding to represent the textual features. 4.3 Evaluation Metrics For multi-label emotion classification, we use the automatic metrics as mentioned below following the works of (Huang et al., 2019; Yang et al., 2018; Mohammad et al., 2018): Jaccard Index (Rogers and Tanimoto, 1960), Hamming Loss (Schapire and Singer, 1999) and Micro-averaged F1-score (Manning et al., 2008). For sentiment analysis we report Micro-averaged F1-score while for intensity prediction we report Pearson correlation co-efficient (Mohammad and Bravo-Marquez, 2017) in a similar manner as (Akhtar et al., 2019b). 5 Result and Discussion In this section, we provide the results for all the three tasks, i.e. multi-label emotion classification, intensity prediction and sentiment analysis on our proposed MEISD dataset. In Table 5, we provi"
2020.coling-main.393,D18-1137,0,0.0195007,"l information. With the advancements in Artificial Intelligence (AI), emotion classification and sentiment analysis have become a significant task due to its importance in many downstream tasks, such as customer behavior modeling, response generation for conversational agents, multimodal interactions etc. Hence, to maximize user satisfaction and providing a better experience to the customer, it is important to understand the correct emotion and sentiment of the customer. Recently, multi-label emotion classification has been investigated for textual data in (Kim et al., 2018; He and Xia, 2018; Yu et al., 2018; Huang et al., 2019). Using multiple Convolution Neural Network (CNN) networks along with self-attention, the authors in (Kim et al., 2018) performed multi-label emotion classification on twitter data. Similarly, the authors in (Yu et al., 2018) improved the performance of multi-label emotion classification on twitter data by using transfer learning. Lately, sequence-to-sequence framework (Huang et al., 2019) has been employed for multi-label emotion classification. Our present work differs from these single and multi-label emotion and sentiment classification works as we tend to classifying"
2020.coling-main.393,P18-1208,0,0.0153658,"iment analysis very interesting as well as challenging. In Figure 3, we show the emotion shift of a speaker as the dialogue grows. Figure 3: A dialogue from the MEISD dataset showcasing the emotion shift as the conversation grows. The text in blue represents the sentiment label while the text in red represents the emotion label of every utterance. 3.2 Comparison with Related Datasets The available datasets for multimodal emotion detection and sentiment classification are nonconversational. The examples of such datasets are MOUD (P´erez-Rosas et al., 2013), MOSI (Zadeh et al., 2016) and MOSEI (Zadeh et al., 2018) that have been deeply investigated by the researchers for both the tasks. Two dyadic conversational datasets, IEMOCAP (Busso et al., 2008) and SEMAINE (McKeown et al., 2011) have gained popularity for encouraging research on emotion detection for conversations. Recently, MELD (Poria et al., 2019) dataset was released to inspire research on multiparty conversations using information from different modalities. IEMOCAP Dataset: The IEMOCAP (Interactive Emotional Dyadic Motion Capture Database) dataset (Busso et al., 2008) comprises of videos of dyadic interactions between pairs of 10 speakers ac"
2020.eamt-1.21,P19-1122,0,0.0617446,"Missing"
2020.eamt-1.21,J99-2004,0,0.450548,"oehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the current state-of-the-art NMT model. In a reference-simulated setting, we found that our target-language syntax-informed interactive setup can significantly reduce human effort in a Frenchto-English translation task. We also extract syntactic features from constituency-based parse trees of the source French sentences following Akoury et al. (2019), and use them as the conditional context in the interactive-predictive Transformer framework. Experiments show that this context"
2020.eamt-1.21,J09-1002,0,0.0879581,"Missing"
2020.eamt-1.21,1997.mtsummit-papers.1,0,0.723576,"nd model supertags and constituency parse tree-based features collectively as the conditional context for interactive prediction in NMT. Our experimental results indicate that these syntactic feature types are complementary. As a result, this collaborative strategy turns out to be the bestperforming in the French-to-English task while significantly outperforming those setups that include either feature type on WPA and WSR. To the best of our knowledge, this is the very first study that investigates the possibility of integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignme"
2020.eamt-1.21,E12-1025,0,0.0473397,"Missing"
2020.eamt-1.21,P07-1037,1,0.783792,"Missing"
2020.eamt-1.21,W18-1820,0,0.0624757,"Missing"
2020.eamt-1.21,2016.amta-researchers.9,0,0.295217,"oyens, de prendre les choses en main.’ to English. The reference translation is ‘we decide therefore, citizens, to take control of things’ which is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore"
2020.eamt-1.21,W04-3250,0,0.0148959,"Missing"
2020.eamt-1.21,N03-1017,0,0.0815021,"h is used here to simulate the user. The user corrects the first wrong word (things) from the hypothesis. The validated prefix (magenta phrase) and the last modified word (control) are fed back to the NMT system which generates a correct suffix (of things). As of today, NMT (Bahdanau et al., 2015; Vaswani et al., 2017) represents the state-of-theart in MT research. This has led researchers to test interactive-predictive protocol on NMT too, and papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003) as far as interactive-predictive translation is concerned. In a different MT research context, N˘adejde et al. (2017) have successfully integrated CCG (combinatory categorical grammar) syntactic categories (Steedman, 2000) into the target-side of the then state-of-the-art recurrent neural network (RNN) MT models (Bahdanau et al., 2015). In this work, we investigate the possibility of modelling the target-language syntax in the form of supertags (Bangalore and Joshi, 1999; Steedman, 2000) as a conditional context in an interactivepredictive protocol on Transformer (Vaswani et al., 2017), the c"
2020.eamt-1.21,P14-2094,0,0.0219439,"integrating syntactic knowledge into an interactive MT model. 2 Related Work Foster et al. (1997) were the first to introduce the idea of interactive-predictive MT as an alternative to pure post-editing MT. There have been a number of papers that explored this strategy in order to minimise human effort in translation and cover many use-cases involving SMT: e.g. applying online (Ortiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic kn"
2020.eamt-1.21,W19-6610,0,0.0153564,"al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully syntactified inter"
2020.eamt-1.21,D14-1107,0,0.0125701,"ts contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch size of 1,800 tokens. EasyCCG4 (Lewis and Steedman, 2014), a CCG supertagger, is used for generating the CCG sequence for the English sentences. Transformer (Baseline) Source Syntactified (SS) Target Syntactified (TS) Fully Syntactified (FS) 26.90 26.96 27.10 27.36 (p-value: 0.059) Table 3: The BLEU scores of baseline and syntactified NMT systems. Table 3 shows the performance of our baseline and syntax-sensitive NMT systems in terms of BLEU. The second and third rows represent the NMT models that incorporate source- and targetlanguage syntactic contexts, respectively, which we call source- (SS) and target-syntactified (TS) NMT systems, respectively"
2020.eamt-1.21,W17-4707,0,0.0327765,"Missing"
2020.eamt-1.21,J16-1004,0,0.0277097,"Missing"
2020.eamt-1.21,P02-1040,0,0.107611,"he sub-word units of a word inherit the CCG category of the word. As an example, we show an English sentence with supertags in Table 1. We see from row E of Table 1 that CCG ‘N’ of a word ‘Oberth¨ur’ is distributed over its sub-words (i.e. Ober@@ th@@ u¨ @@ and r). Our first experimental setup is referred to as PredCCG. Akoury et al. (2019) showed that integrating target-side ground-truth syntactic information into Transformer at decoding time significantly improved translation quality, and their syntaxbased model outperformed the baseline Transformer model by a large margin in terms of BLEU (Papineni et al., 2002). In reality, there is no way of obtaining the target-side ground-truth syntactic information at decoding time. However, in interactive-predictive mode, we found a way to obtain a slightly better CCG sequence for the partial translation (i.e. validated prefix) and inject them into the model at run-time, which we believe can positively impact the model’s subsequent predictions. In other words, in our second setup, we integrate a CCG supertagger into our INMT framework, and apply that on the validated prefix and unchecked suffix on the fly. The tagger is invoked when the user makes a correction."
2020.eamt-1.21,K18-1015,0,0.0135389,"(Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT This section presents our fully"
2020.eamt-1.21,2014.eamt-1.5,0,0.116043,"Missing"
2020.eamt-1.21,C16-2004,0,0.0626817,"Missing"
2020.eamt-1.21,P16-1007,0,0.0162171,"tiz-Mart´ınez, 2016) and active (Gonz´alezRubio et al., 2012) learning techniques, use of translation memories (Barrachina et al., 2009; Green et al., 2014), predicting the partially typed words and prefix matching (Koehn et al., 2014), word-graphs for reducing response time (SanchisTrilles et al., 2014), alignment based post-editing (Simianer et al., 2016), segment-based approaches (Peris et al., 2017), suggesting more than one suffix (Koehn, 2009), and exploring multimodal interaction (Alabau et al., 2014). This use-case has also been moderately tested on NMT, e.g. (Knowles and Koehn, 2016; Wuebker et al., 2016; Peris and Casacuberta, 2018; Lam et al., 2019). To the best of our knowledge, no one has investigated the interactive-predictive protocol on the state-of-theart Transformer. The strategy of exploiting syntactic knowledge from the source and/or target languages for improving the translation quality is not new in MT research. It was successfully applied in the era of classical MT (Hassan et al., 2007; Haque et al., 2011), and is continually being applied to improve the current state-of-the-art NMT models, e.g. (Luong et al., 2016; N˘adejde et al., 2017). 3 Fully Syntactified Interactive NMT Th"
2020.eamt-1.21,C00-2137,0,0.0138574,"Missing"
2020.eamt-1.21,L16-1561,0,0.0119616,"ble 2, a new CCG tag sequence is generated for the hypothesis, and we see that CCG (N) of the newly added token ‘,’ is correct. Finally, INMT predicts another suggestion (row 9 of Table 2) where we see the remaining predictions are correct in the context. We call this experimental setup OnflyCCG. Note that the model is trained at sub-word level and generates sub-words at output; however, word level tokens are presented to the user. Naturally, On the fly CCG supertagger is applied to a hypothesis of word level. 5.2 MT systems We carry out experiments with French-to-English with the UN corpus2 (Ziemski et al., 2016). The training and development sets contain 12,238,995 and 1,500 sentences, respectively. We use 1,500 sentences from the WMT15 news test set newstest2015 as our test set. In order to build our MT systems, we use the Sockeye3 (Hieber et al., 2018) toolkit. Our training setups are as follows. The tokens of the training, evaluation and validation sets are segmented into sub-word units using BPE. We performed 30,000 join operations. We use 6 layers in the encoder and decoder sides, an 8-head attention, hidden layer of size 512, embedding vector of size 512, learning rate 0.0002, and minimum batch"
2020.findings-emnlp.206,N19-1423,0,0.288809,"e, beliefs, and moral values of the respective communities. Linguists have studied the phenomenon of codemixing, put forward many linguistic hypotheses (Belazi et al., 1994; Pfaff, 1979; Poplack, 1978), and formulated various constraints (Sankoff and Poplack, 1981; Di Sciullo et al., 1986; Joshi, 1982) to define a general rule for code-mixing. However, for all the scenarios of code-mixing, particularly for the syntactically divergent languages (Berk-Seligson, 1986), these limitations cannot be postulated as a universal rule. In recent times, the pre-trained language model based architectures (Devlin et al., 2019; Radford et al., 2019) have become the state-of the-art models for language understanding and generation. The underlying data to train such models comes from the huge amount of corpus, available in the form of Wikipedia, book corpus etc. Although, these are readily available in various languages, there is a scarcity of such amount of data in code-mixed form which could be used to train the state-of-the-art transformer (Vaswani et al., 2017) based language model, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLM (Lample and Conneau, 2019) etc. The existing benchmark datasets"
2020.findings-emnlp.206,P18-1128,0,0.0133917,"M feature decreases the Bleu score by 1.36 points. We observe the near similar impact of LM feature on each language pair. Finally, the transfer learning is also proven to be an integral component of the proposed model as it contributes to the maximum of 2.25 Bleu score for en-fr and minimum of 1.02 Bleu score of en-te code-mixed language pair. The difference between the maximum and minimum contribution may be attributed to the fact that, we have sufficient parallel corpus (197, 922) to train the en-fr NMT model as compared to the en-te parallel corpus (10, 105). We follow the bootstrap test (Dror et al., 2018) which confirms that the performance improvement over the baselines are statistically significant as (p &lt; 0.005). 5.3 Human 4.19 2.34 3.26 Qualitative Analysis We assess the quality of the generated code-mixed text, and show these samples in Table 4. We observe that the code-mixed sentences generated using the PG model are able to copy the entities from the given English sentence, but the generated code-mixed sentences are incomplete and not fluent compared to the reference sentences. For example, in en-hi pair the PG based code-mixed sentence missed the ‘main’ word and it copies ‘India’s’ rat"
2020.findings-emnlp.206,N13-1073,0,0.0426302,"English and substitute their aligned counterparts with the identified English words to synthesize the English embedded code-mixed sentences. The input to our synthetic code-mixed generation algorithm (details are in Appendix) is a parallel sentence pair. We use the Indic-nlp-library1 to tokenize the sentences of the Indic languages. Moses based tokenizer2 is used to translate the European and English language texts. Thereafter, we learn the alignment matrix, which guides to select the words or phrases to be mixed in the language. We use the official implementation3 of the fastalign algorithm (Dyer et al., 2013) to obtain the alignment matrix. The alignment matrix is used to construct the aligned phrases between the parallel sentences. We extract the PoS (mainly adjective), named entity (NE) and noun phrase (NP) from the English sentences, and insert them into the appropriate places of the sentences in the other language (i.e. the target language) counterparts. We use the Stanford library4 Stanza (Qi et al., 2020) to 1 https://github.com/anoopkunchukuttan/indic_ nlp_library 2 https://github.com/moses-smt/mosesdecoder 3 https://github.com/clab/fast_align 4 https://github.com/stanfordnlp/stanza Figure"
2020.findings-emnlp.206,D18-1346,0,0.122992,"eness of our proposed approach on eight different language pairs, viz. English-Hindi (en-hi), English-Bengali (en-bn), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering"
2020.findings-emnlp.206,P16-1014,0,0.0132895,"l and generation paradigms. For the given input sentence E : {e1 , e2 , . . . , em }, we extract the language model feature L : {l1 , l2 , . . . , lm }. The extracted language model features are fused to the linguistic features as follows: 4.3 Decoding with Pointer Generator We use the one-layer LSTM network with the attention mechanism (Bahdanau et al., 2015) to generate the code-mixed sentence y1 , y2 , . . . , yn one word at a time. In order to deal with the rare or unknown words, the decoder has the flexibility to copy the words from documents via the pointing mechanism (See et al., 2017; Gulcehre et al., 2016). The LSTM decoder reads the word embedding ut−1 and the hidden state st−1 to generate the hidden state st at time step t. Concretely, h∗t = tanh(Wh ht + bh ) lt∗ = tanh(Wl lt + bl ) g = σ(Wg .[ht ⊕ lt ]) st = LST M (st−1 , ut−1 ) (2) ft = g ⊙ h∗t + (1 − g) ⊙ lt∗ where, ⊕ and ⊙ are the concatenation and elementwise multiplication operator. First, we project both the features ht and lt into the same vector space h∗t and lt∗ via feed-forward network. Thereafter, we learn the gated value g which controls the flow of each feature. The gated value g controls how much of each feature should be the p"
2020.findings-emnlp.206,W16-6331,1,0.891348,"Missing"
2020.findings-emnlp.206,K18-1012,1,0.9198,"exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpus (English to respective language). Thereafter, we utilize this data to develop a neural based model to generate the code-mixed sentences from t"
2020.findings-emnlp.206,C82-1023,0,0.764438,"ing is a common expression of multilingualism in informal text and speech, where there is a switch between the two languages, frequently with one in the character set of the other language. This has been a mean of communication in a multi-cultural and multi-lingual society, and varies according to the culture, beliefs, and moral values of the respective communities. Linguists have studied the phenomenon of codemixing, put forward many linguistic hypotheses (Belazi et al., 1994; Pfaff, 1979; Poplack, 1978), and formulated various constraints (Sankoff and Poplack, 1981; Di Sciullo et al., 1986; Joshi, 1982) to define a general rule for code-mixing. However, for all the scenarios of code-mixing, particularly for the syntactically divergent languages (Berk-Seligson, 1986), these limitations cannot be postulated as a universal rule. In recent times, the pre-trained language model based architectures (Devlin et al., 2019; Radford et al., 2019) have become the state-of the-art models for language understanding and generation. The underlying data to train such models comes from the huge amount of corpus, available in the form of Wikipedia, book corpus etc. Although, these are readily available in vari"
2020.findings-emnlp.206,D18-1330,0,0.0178967,"the sentences into sub-words and use this vocabulary10 to index the sub-words. For the language pairs: enbn, en-ml, en-ta, en-te, we use the learned BPE codes11 on 100 languages from the XLM model to segment the sentences into sub-words and use the correspondent vocabulary to index the sub-words. The same set of vocabulary is used to extract the pre-trained language model feature and the corresponding NMT model for the transfer learning. We use the aligned multilingual word embedding12 of dimension 300 for the language pairs: en-es, ende, en-fr, en-hi and en-bn from Bojanowski et al. (2017); Joulin et al. (2018). For the rest of the language pairs, we obtain the monolingual embedding13 from Bojanowski et al. (2017) and use the MUSE library released by Lample et al. (2018) to align the vector in the same vector space. The embeddings of NE and PoS information are randomly initialized with the dimension of 20. 8 http://www.iitp.ac.in/~ai-nlp-ml/resources. html 9 https://dl.fbaipublicfiles.com/XLM/codes_ xnli_15 10 https://dl.fbaipublicfiles.com/XLM/vocab_ xnli_15 11 https://dl.fbaipublicfiles.com/XLM/codes_ xnli_100 12 https://fasttext.cc/docs/en/ aligned-vectors.html 13 https://fasttext.cc/docs/en/ pre"
2020.findings-emnlp.206,kocmi-bojar-2017-curriculum,0,0.0187515,"mixed sentence reveals that the translated target text (XX7 ) and code-mixed (En-XX) shares many words. For example: • Source (En): The situation in Mumbai has not yet come to normal. • Target (Hi): मुब ं ई म िःथित अभी तक सामा य नह ं हुई है | • Code-Mixed (En-Hi): Mumbai म situation अभी normal नह ं हुई है । In the above sentences, Target (Hi) and Codemixed (En-Hi) share many words (underlined words). Because of this underlying similarity between the machine translation and code-mixed sentence generation, we adapted the transfer learning approach used in machine translation (Zoph et al., 2016; Kocmi and Bojar, 2017) for code-mixed text generation. We first train an NMT model on a large corpus of parallel sentences as discussed in Section 3. Next, we initialize the code-mixed text generation model with the already-trained NMT model. This is then trained on the synthetic code-mixed dataset. Rather than initializing the code-mixed model from the random parameters, we initialize it with the weights from the NMT model. By doing this, we achieve strong prior distribution from the NMT model to code-mixed text generation. When 7 ‘te’ XX may belong to ‘es’, ‘de’, ‘fr’, ‘hi’, ‘bn’, ‘ml’, ‘ta’, we train the code-mi"
2020.findings-emnlp.206,2005.mtsummit-papers.11,0,0.218404,"Missing"
2020.findings-emnlp.206,W04-1013,0,0.0128261,"eature. We use beam search of beam size 4 to generate the code-mixed sentence. Adam (Kingma and Ba, 2015) optimizer is used to train the model with (i) β1 = 0.9, (ii) β2 = 0.999, and (iii) ϵ = 10−8 and initial learning rate of 0.0001. The maximum length of English and code-mixed tokens are set to 60 and 30, respectively. We set 5 as minimum decoding steps in each code-mixed language pair. We use the en-hi development dataset to tune the network hyper-parameters. All the model updates use a batch size of 16. We evaluate the generated text using the metrics, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). 14 https://dl.fbaipublicfiles.com/XLM/mlm_tlm_ xnli15_1024.pth 15 https://dl.fbaipublicfiles.com/XLM/mlm_100_ 1280.pth 5.2 Quantitative Analysis We report the results of our proposed model in Table 2 and Table 3. Performance comparisons to the three baselines are reported in Table 2 and Table 3. The Pointer Generator based baseline is the superior amongst all the baselines and achieve the maximum Bleu score of 21.45 for the en-de code-mixed language pair. Our proposed model achieves the maximum Bleu score of 24.89 for the en-fr code-mixed language pair."
2020.findings-emnlp.206,2021.ccl-1.108,0,0.0991104,"Missing"
2020.findings-emnlp.206,P02-1040,0,0.107357,"to extract the language model feature. We use beam search of beam size 4 to generate the code-mixed sentence. Adam (Kingma and Ba, 2015) optimizer is used to train the model with (i) β1 = 0.9, (ii) β2 = 0.999, and (iii) ϵ = 10−8 and initial learning rate of 0.0001. The maximum length of English and code-mixed tokens are set to 60 and 30, respectively. We set 5 as minimum decoding steps in each code-mixed language pair. We use the en-hi development dataset to tune the network hyper-parameters. All the model updates use a batch size of 16. We evaluate the generated text using the metrics, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Banerjee and Lavie, 2005). 14 https://dl.fbaipublicfiles.com/XLM/mlm_tlm_ xnli15_1024.pth 15 https://dl.fbaipublicfiles.com/XLM/mlm_100_ 1280.pth 5.2 Quantitative Analysis We report the results of our proposed model in Table 2 and Table 3. Performance comparisons to the three baselines are reported in Table 2 and Table 3. The Pointer Generator based baseline is the superior amongst all the baselines and achieve the maximum Bleu score of 21.45 for the en-de code-mixed language pair. Our proposed model achieves the maximum Bleu score of 24.89 for the en-fr code-mi"
2020.findings-emnlp.206,P18-1143,0,0.185921,"d data for the various NLP tasks not only limited to the language modelling and speech recognition as it is generally been focused in the literature. In contrast to the previous studies, where only a few of the language pairs were considered for code-mixing, we propose an effective approach which shows its effectiveness in generating codemixed sentences for eight different language pairs of diverse origins and linguistic properties. 2 3 Synthetic Code-Mixed Generation Related Work In the literature, there have been efforts for creating code-mixed texts by leveraging the linguistic properties. Pratapa et al. (2018) explored the equivalence constraint theory to construct artificial code-mixed data to reduce the perplexity of the We follow the matrix language frame (MLF) (Myers-Scotton, 1997; Joshi, 1982) theory to generate the code-mixed text. It is less restrictive and can easily be applied on many language pairs. According to MLF, a code-mixed text will have a 2268 Language (L1) India’s agriculture is their main strength. Especially valuable people like Connor Rooney. en en Glasses and cups, whatever they are, can be turned upside down. en Democracy and development go hand in hand. We abolish national"
2020.findings-emnlp.206,2020.acl-demos.14,0,0.0604211,"Missing"
2020.findings-emnlp.206,D16-1121,0,0.0281257,"Missing"
2020.findings-emnlp.206,P17-1099,0,0.278036,"f the cross-lingual and generation paradigms. For the given input sentence E : {e1 , e2 , . . . , em }, we extract the language model feature L : {l1 , l2 , . . . , lm }. The extracted language model features are fused to the linguistic features as follows: 4.3 Decoding with Pointer Generator We use the one-layer LSTM network with the attention mechanism (Bahdanau et al., 2015) to generate the code-mixed sentence y1 , y2 , . . . , yn one word at a time. In order to deal with the rare or unknown words, the decoder has the flexibility to copy the words from documents via the pointing mechanism (See et al., 2017; Gulcehre et al., 2016). The LSTM decoder reads the word embedding ut−1 and the hidden state st−1 to generate the hidden state st at time step t. Concretely, h∗t = tanh(Wh ht + bh ) lt∗ = tanh(Wl lt + bl ) g = σ(Wg .[ht ⊕ lt ]) st = LST M (st−1 , ut−1 ) (2) ft = g ⊙ h∗t + (1 − g) ⊙ lt∗ where, ⊕ and ⊙ are the concatenation and elementwise multiplication operator. First, we project both the features ht and lt into the same vector space h∗t and lt∗ via feed-forward network. Thereafter, we learn the gated value g which controls the flow of each feature. The gated value g controls how much of each"
2020.findings-emnlp.206,P16-1162,0,0.0248896,"n-Hi code-mixed sentence. We show some samples in Table 1, and more details in the Appendix. 4 Methodology ht = LST M (ht−1 , [ut , nt , pt ]) We depict the architecture of our proposed model in Figure 2. Problem Statement: Given an English sentence E having m words e1 , e2 , . . . , em , the task is to generate the code-mixed sentence Cˆ having a sequence of n words Cˆ = {y1 , y2 , . . . , yn }. 4.1 Sub-word Vocabulary The task of generation using neural networks requires a fixed-sized vocabulary. To deal with the problem of Out-of-Vocabulary (OOV) words, we use the Byte-pair encoding (BPE) (Sennrich et al., 2016), and segment the words into sub-words. The sub-word based tokenization schemes inspired by BPE have become the norm in most of the advanced models including the very popular family of contextual language models like XLM (Lample and Conneau, 2019), GPT-2 (Radford et al., 2019), etc. In this work, we process the language pairs with the vocabulary created using the BPE. 4.2 addition, we also incorporate the linguistic features in the form of NE and PoS. The motivation to use these linguistic features comes from the synthetic code-mixed text generation (c.f. section 3) itself, where these feature"
2020.findings-emnlp.206,D08-1102,0,0.0330777,"sing a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpu"
2020.findings-emnlp.206,D08-1110,0,0.0573687,"sing a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidayat, 2012; Solorio and Liu, 2008a), entity extraction (Gupta et al., 2018a; Bhat et al., 2016; Gupta et al., 2016b), etc, where codemixing phenomena are explored and analyzed. In contrast to sthese existing works, firstly, we provide a linguistically motivated technique to create the code-mixed datasets from multiple languages with the help of parallel corpu"
2020.findings-emnlp.206,W19-4320,0,0.0926813,"), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidaya"
2020.findings-emnlp.206,W18-3207,0,0.0472563,"e attempt to propose a generic method that produces the correct and fluent code-mixed sentences on multiple language pairs. The generated synthetic dataset will be a useful resource for machine translation and multilingual applications. (iii). We demonstrate with detailed empirical evaluations the effectiveness of our proposed approach on eight different language pairs, viz. English-Hindi (en-hi), English-Bengali (en-bn), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 201"
2020.findings-emnlp.206,K19-1026,0,0.393076,"), English-Malayalam (en-ml), English-Tamil (enta), English-Telugu (en-te), English-French (en-fr), English-German (en-de) and English-Spanish (enes). RNN-based language model. Winata et al. (2018) proposed a multitask learning framework to address the issue of data scarcity in code-mixed setting. Particularly, they leveraged the linguistic information using a shared syntax representation, jointly learned over Part-ofSpeech (PoS) and language modeling on codeswitched utterances. Garg et al. (2018) exploited SeqGAN in the generation of the synthetic codemixed language sequences. Most recently, Winata et al. (2019a) utilized the language-agnostic metarepresentation method to represent the code-mixed sentences. There are also other studies (Adel et al., 2013a,b, 2015; Choudhury et al., 2017; Winata et al., 2018; Gonen and Goldberg, 2018; Samanta et al., 2019) for code-mixed language modelling. There are some other NLP areas like parts-ofspeech (Solorio and Liu, 2008b; Gupta et al., 2017; Patel et al., 2016), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a), question answering (Gupta et al., 2018b; Chandu et al., 2017), language identification (Solorio et al., 2014; Gupta et al., 2014; Hidaya"
2020.findings-emnlp.206,D16-1163,0,0.0236565,"closer to the code-mixed sentence reveals that the translated target text (XX7 ) and code-mixed (En-XX) shares many words. For example: • Source (En): The situation in Mumbai has not yet come to normal. • Target (Hi): मुब ं ई म िःथित अभी तक सामा य नह ं हुई है | • Code-Mixed (En-Hi): Mumbai म situation अभी normal नह ं हुई है । In the above sentences, Target (Hi) and Codemixed (En-Hi) share many words (underlined words). Because of this underlying similarity between the machine translation and code-mixed sentence generation, we adapted the transfer learning approach used in machine translation (Zoph et al., 2016; Kocmi and Bojar, 2017) for code-mixed text generation. We first train an NMT model on a large corpus of parallel sentences as discussed in Section 3. Next, we initialize the code-mixed text generation model with the already-trained NMT model. This is then trained on the synthetic code-mixed dataset. Rather than initializing the code-mixed model from the random parameters, we initialize it with the weights from the NMT model. By doing this, we achieve strong prior distribution from the NMT model to code-mixed text generation. When 7 ‘te’ XX may belong to ‘es’, ‘de’, ‘fr’, ‘hi’, ‘bn’, ‘ml’, ‘t"
2020.findings-emnlp.210,W18-6514,0,0.0519008,"Missing"
2020.findings-emnlp.210,W18-5709,0,0.0282432,"Missing"
2020.findings-emnlp.210,D19-5602,0,0.0395331,"Missing"
2020.findings-emnlp.210,D18-1547,0,0.0587877,"Missing"
2020.findings-emnlp.210,P19-1540,1,0.817043,"h reported in (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; Gan et al., 2019) has been effective in narrowing the gap between vision and language. Similarly in (Le et al., 2019; Alamri et al., 2018; Lin et al., 2019a), DSTC7 dataset has been used for response generation by incorporating audio and visual features. The release of the Multi-modal Dialog (MMD) dataset (Saha et al., 2018), having conversations on the fashion domain with the information from both texts and images, has facilitated the research on response generation (Agarwal et al., 2018b,a; Liao et al., 2018; Chauhan et al., 2019; Cui et al., 2019) in a multi-modal setup. Our newly designed framework is different from these existing ones, as our focus here is on creating aspect guided multi-modal dialogue dataset that contains the information of three different domains. Our present work distinguishes from the prior works of multi-modal dialog systems in the sense that we aim at generating responses conditioned on a particular aspect of the product or service in accordance with the conversational history. Our research is novel concerning the following two aspects viz. (i). our research is focused on the task of aspect"
2020.findings-emnlp.210,P19-1360,0,0.0211185,"ffectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating res"
2020.findings-emnlp.210,P19-1258,0,0.0151062,"ffectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating res"
2020.findings-emnlp.210,P19-1648,0,0.0201989,"ianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating responses. Multi-modal Dialogue Systems Recently, research on the dialog system has shifted towards integrating various modalities, such as images, audio, and video, along with text, to obtain the information to build a robust framework. The research reported in (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; Gan et al., 2019) has been effective in narrowing the gap between vision and language. Similarly in (Le et al., 2019; Alamri et al., 2018; Lin et al., 2019a), DSTC7 dataset has been used for response generation by incorporating audio and visual features. The release of the Multi-modal Dialog (MMD) dataset (Saha et al., 2018), having conversations on the fashion domain with the information from both texts and images, has facilitated the research on response generation (Agarwal et al., 2018b,a; Liao et al., 2018; Chauhan et al., 2019; Cui et al., 2019) in a multi-modal setup. Our newly designed framework is diff"
2020.findings-emnlp.210,W18-5712,0,0.0619488,"Missing"
2020.findings-emnlp.210,D14-1162,0,0.0852628,"ction. We use the dropout(Srivastava et al., 2014) with probability 0.45. During decoding, we use a beam search with beam size 10. The model is initialized with the parameters chosen randomly using a Gaussian distribution with the Xavier scheme (Glorot and Bengio, 2010). The hidden size for all the layers is 512. AMSGrad (Reddi et al., 2019) is used as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when the gradient norm is above 5. We use 300-dimensional word-embedding initialized with Glove (Pennington et al., 2014) embedding pre-trained on Twitter. We consider the previous 2 turns for the dialogue history, and the maximum utterance length is set to 50. For image representation, FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2015), pre-trained on ImageNet is used. Automatic evaluation metrics To evaluate our proposed framework at the content level we report Perplexity (Chen et al., 1998). Lesser perplexity scores signify that the generated responses are grammatically correct and fluent. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 20"
2020.findings-emnlp.210,W04-1013,0,0.032589,"ained on Twitter. We consider the previous 2 turns for the dialogue history, and the maximum utterance length is set to 50. For image representation, FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2015), pre-trained on ImageNet is used. Automatic evaluation metrics To evaluate our proposed framework at the content level we report Perplexity (Chen et al., 1998). Lesser perplexity scores signify that the generated responses are grammatically correct and fluent. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the quality of the generated response for capturing the correct information. Human evaluation metrics From the generated responses we randomly take 700 responses from 2323 2 https://pytorch.org/ the test dataset for qualitative evaluation. For a given input along with aspect information, three annotators with post-graduate exposure were assigned to evaluate the correctness, relevance, domain and aspect consistency of the generated responses by the different approaches for the following four metrics: (i) Fluency (F): This metric is used to measure the grammatical correctness of the"
2020.findings-emnlp.210,D19-1463,0,0.0172441,"veral baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating responses. Multi-modal"
2020.findings-emnlp.210,D15-1166,0,0.0164493,"he local image representation for all the images in the dialogue turns and concatenate them together. The concatenated image vector is passed through the linear layer to form the global image context representation as given 2322 where hctx c,k is the final hidden representation of the context for a given turn. 4. Decoder In the decoding section, we build another GRU for generating the response in a sequential manner based on the context hidden representation of the hierarchical encoder (context GRU), and the words decoded previously. We use the input feeding decoding along with the attention (Luong et al., 2015) mechanism for enhancing the performance of the model. Using the decoder state hdec d,t as the query vector, we apply self-attention on the hidden representation of the context-level encoder. The decoder state and the context vector are concatenated and used to calculate a final distribution of the probability over the output tokens. hdec d,t = GRUd (yk,t−1 , hd,t−1 ) ct = k X αt,i hctx c,k , i=1 αt,i = T sof tmax(hctx c,k Wf hd,t ) (6) ˜ t = tanh(W˜ [hd,t ; ct ]) h h ˜ t) P (yt /y<t ) = sof tmax(WV h 5 where, Wf , WV and Wh˜ are the trainable weight matrices. For generating responses with the"
2020.findings-emnlp.210,P18-1136,0,0.0214909,"ided responses. (iv) The proposed model for both automatic and human evaluation shows its effectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been i"
2020.findings-emnlp.210,I17-1047,0,0.0218988,"alog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating responses. Multi-modal Dialogue Systems Recently, research on the dialog system has shifted towards integrating various modalities, such as images, audio, and video, along with text, to obtain the information to build a robust framework. The research reported in (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; Gan et al., 2019) has been effective in narrowing the gap between vision and language. Similarly in (Le et al., 2019; Alamri et al., 2018; Lin et al., 2019a), DSTC7 dataset has been used for response generation by incorporating audio and visual features. The release of the Multi-modal Dialog (MMD) dataset (Saha et al., 2018), having conversations on the fashion domain with the information from both texts and images, has facilitated the research on response generation (Agarwal et al., 2018b,a; Liao et al., 2018; Chauhan et al., 2019; Cui et al., 2019) in a multi-modal s"
2020.findings-emnlp.210,P02-1040,0,0.106517,"ngton et al., 2014) embedding pre-trained on Twitter. We consider the previous 2 turns for the dialogue history, and the maximum utterance length is set to 50. For image representation, FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2015), pre-trained on ImageNet is used. Automatic evaluation metrics To evaluate our proposed framework at the content level we report Perplexity (Chen et al., 1998). Lesser perplexity scores signify that the generated responses are grammatically correct and fluent. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the quality of the generated response for capturing the correct information. Human evaluation metrics From the generated responses we randomly take 700 responses from 2323 2 https://pytorch.org/ the test dataset for qualitative evaluation. For a given input along with aspect information, three annotators with post-graduate exposure were assigned to evaluate the correctness, relevance, domain and aspect consistency of the generated responses by the different approaches for the following four metrics: (i) Fluency (F): This metric is used to measure the grammat"
2020.findings-emnlp.210,D19-1460,0,0.0534732,"Missing"
2020.findings-emnlp.210,P19-1253,0,0.0203201,"etworks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain adaptability for generating responses. Multi-modal Dialogue Systems Recently, research on the dialog system has shifted towards integrating various modalities, such as images, audio, and video, along with text, to obtain the information to build a robust framework. The research reported in (Das et al., 2017; Mostafazadeh et al., 2017; De Vries et al., 2017; Gan et al., 2019) has been effective in narrowing the gap between vision and language. Similarly in (Le et al., 2019; Alamri et al., 2018; Lin et al., 2019a), DSTC7 dataset"
2020.findings-emnlp.210,N19-1375,0,0.0108393,"r both automatic and human evaluation shows its effectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to im"
2020.findings-emnlp.210,P15-1152,0,0.0335124,"s having conversations belonging to the three different domains, namely restaurant, electronics, and furniture. (iii) We propose a multi-modal graph convolutional framework for response generation while explicitly providing aspect information to the decoder to generate aspect-guided responses. (iv) The proposed model for both automatic and human evaluation shows its effectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the resp"
2020.findings-emnlp.210,P19-1646,0,0.0279571,"ual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation. 1 Figure 1: Examples from the Multi-domain Multimodal Dialogue(MDMMD) dataset Introduction Conversational systems have become ubiquitous in our everyday lives. Previous research suggests that the conversational agents need to be more interactive and informative for building engaging systems (Takayama and Arase, 2019; Shukla et al., 2019). ∗ First two authors have contributed equally (Saha et al., 2018) for the fashion domain has established the significance of visual information for effective communication between the user and agent. Inspired by their works, we take a step forward by creating a multi-modal aspect guided response framework for a multi-domain goal-oriented dialogue system. From Figure 1, it can be observed that visual information of the aspects encourages improved communication and informative response generation by the agent with regards to the user queries. In this paper, we propose the task of generating 231"
2020.findings-emnlp.210,W19-4115,0,0.0220576,"formation having both textual and image representation is fed to the decoder and the aspect information for generating aspect guided responses. Quantitative and qualitative analyses show that the proposed methodology outperforms several baselines for the proposed task of aspect-guided response generation. 1 Figure 1: Examples from the Multi-domain Multimodal Dialogue(MDMMD) dataset Introduction Conversational systems have become ubiquitous in our everyday lives. Previous research suggests that the conversational agents need to be more interactive and informative for building engaging systems (Takayama and Arase, 2019; Shukla et al., 2019). ∗ First two authors have contributed equally (Saha et al., 2018) for the fashion domain has established the significance of visual information for effective communication between the user and agent. Inspired by their works, we take a step forward by creating a multi-modal aspect guided response framework for a multi-domain goal-oriented dialogue system. From Figure 1, it can be observed that visual information of the aspects encourages improved communication and informative response generation by the agent with regards to the user queries. In this paper, we propose the"
2020.findings-emnlp.210,P19-1371,0,0.0137205,"human evaluation shows its effectiveness over several baselines. 2 Related Work Uni-modal Dialogue Systems The effectiveness of deep learning has shown significant progress in dialog generation. Deep neural frameworks, as shown in the (Vinyals and Le, 2015; Shang et al., 2015), are very effective in modeling conversations. The hierarchical encoder-decoder system was studied in (Sordoni et al., 2015; Serban et al., 2016, 2017; Xu et al., 2019) to preserve the dependencies among the utterances in dialogue. Recently, memory networks (Madotto et al., 2018; Raghu et al., 2018; Reddy et al., 2019; Tian et al., 2019; Wu, 2019; Chen et al., 2019b; Lin et al., 2019b) have been investigated to capture the contextual information in dialogues for generating responses. In taskoriented dialogues, hierarchical pointer networks (Raghu and Gupta) have been used to generate the responses. With the release of the task-oriented dialog dataset, such as MultiWoz , a few works (Budzianowski and Vuli´c, 2019; Chen et al., 2019a) have emerged that operate in a multi-domain dialogue setting. The meta-learning approach (Mi et al., 2019; Qian and Yu, 2019) has been implemented on the various datasets to improve the domain ad"
2020.icon-main.60,W11-0705,0,0.201269,"Missing"
2020.icon-main.60,D16-1070,0,0.0221504,"ifferent size filters on the word embedding matrix of the sentence. Hi = h1i , h2i , .....hji , ...hL i (2) Where Hi represents the final feature vector for a sentence. Figure 2: Proposed architecture 3.1.3 Attention for text In NLP related tasks, some words in the sentence are more important for the task compared to the other words in the same sentence. To capture this phenomena, attention model for the text has been proven beneficial for many NLP related tasks i.e., text summarization, machine translation (Luong et al., 2015; Bahdanau et al., 2014), textual sentiment analysis(Corpora, 2000; Chen et al., 2016), etc. Attention models calculate the attention score αij which lies in the range of 0 and 1. Attention score is assigned to feature representation of each wij i.e., hji based on its importance, which is calculated as follows are fused with the help of a fully connected layer. The overall architecture of our proposed model is shown in Figure 2. 3.1 Textual features In this section, we discuss the textual features, how they are given as input to our system, and how to apply attention to the features extracted. 3.1.1 Embedding Layer The embedding layer takes the input as a sequence of words pres"
2020.icon-main.60,D14-1181,0,0.0184875,"Missing"
2020.icon-main.60,D15-1166,0,0.0154123,"the n-gram features. Equation 2 shows the CNN output for a sentence after convolving different size filters on the word embedding matrix of the sentence. Hi = h1i , h2i , .....hji , ...hL i (2) Where Hi represents the final feature vector for a sentence. Figure 2: Proposed architecture 3.1.3 Attention for text In NLP related tasks, some words in the sentence are more important for the task compared to the other words in the same sentence. To capture this phenomena, attention model for the text has been proven beneficial for many NLP related tasks i.e., text summarization, machine translation (Luong et al., 2015; Bahdanau et al., 2014), textual sentiment analysis(Corpora, 2000; Chen et al., 2016), etc. Attention models calculate the attention score αij which lies in the range of 0 and 1. Attention score is assigned to feature representation of each wij i.e., hji based on its importance, which is calculated as follows are fused with the help of a fully connected layer. The overall architecture of our proposed model is shown in Figure 2. 3.1 Textual features In this section, we discuss the textual features, how they are given as input to our system, and how to apply attention to the features extracted."
2020.icon-main.60,P05-1015,0,0.390283,"pid growth of users on social media platforms leads to new ways of spreading information. Meme nowadays has become one of the most popular words for social media. A meme is an idea, the way in which a person behaves in response to a particular situation or a manner that spreads from one person to another within a culture. Spreading of memes on social media platforms such as Facebook, Instagram, Reddit, and Twitter is very fast. Sentiment analysis is a growing field of Natural Language Processing (NLP), aiming to identify the polarity of opinion. Sentiment can be positive, negative or neutral (Pang and Lee, 2005). Sentiment analysis has a vast number of applications in real life, including the product’s recommendation to a user based on opinions provided by other users (Pang et al., 2002), in political uses (Bakliwal et al., 2013), etc. Memes play an important role in handling various political battles or public relations on social media platforms. The most common practice in sentiment analysis is finding the sentiment of textual content crawled from Twitter, product reviews, hotel reviews, etc. Existing literature has mostly addressed the problem of sentiment analysis primarily using textual contents"
2020.icon-main.60,W02-1011,0,0.0355117,", the way in which a person behaves in response to a particular situation or a manner that spreads from one person to another within a culture. Spreading of memes on social media platforms such as Facebook, Instagram, Reddit, and Twitter is very fast. Sentiment analysis is a growing field of Natural Language Processing (NLP), aiming to identify the polarity of opinion. Sentiment can be positive, negative or neutral (Pang and Lee, 2005). Sentiment analysis has a vast number of applications in real life, including the product’s recommendation to a user based on opinions provided by other users (Pang et al., 2002), in political uses (Bakliwal et al., 2013), etc. Memes play an important role in handling various political battles or public relations on social media platforms. The most common practice in sentiment analysis is finding the sentiment of textual content crawled from Twitter, product reviews, hotel reviews, etc. Existing literature has mostly addressed the problem of sentiment analysis primarily using textual contents (Xu et al., 2019; Edara et al., 2019; Medhat et al., 2014; . et al., 2020). But with the growing social media, users are expressing their opinions through text and the image. Hen"
2020.icon-main.60,D14-1162,0,0.0835421,"ows are fused with the help of a fully connected layer. The overall architecture of our proposed model is shown in Figure 2. 3.1 Textual features In this section, we discuss the textual features, how they are given as input to our system, and how to apply attention to the features extracted. 3.1.1 Embedding Layer The embedding layer takes the input as a sequence of words present in the sentence. For each word w present in the sentence, a lookup matrix is created to obtain its embedding representation. Lookup matrix can be initialized using pretrained word embedding vectors (Bojanowski et al.; Pennington et al., 2014). In our work, the pre-trained vector representations provided by Glove (Pennington et al., 2014) are used. It captures syntactic and semantic relations among the words. The embedding of each word w is then given as an input to the CNN to learn the text representation. Equation 1 shows the sequence of words present in sentence where wi is ith word present in the sentence and L is length of sentence. Wi = wi1 , wi2 , .....wij , ...wiL (1) 3.1.2 Convolutional Neural Network (CNN) The convolutional neural network automatically learns the features with the help of convolutional filters. Convolutio"
2020.icon-main.62,W14-4012,0,0.084992,"Missing"
2020.icon-main.62,D14-1181,0,0.00577616,"Missing"
2020.icon-main.62,I17-1099,0,0.0228453,"Emotion Corpus, also known as Twitter Emotion Corpus (TEC), was published by (Mohammad and Kiritchenko, 2015), and consists of 21,051 tweets. This resource was created to understand if emotion-word hashtags can successfully be used as emotion labels. Ekman’s basic emotions 461 3 https://data.world/crowdflower/ sentiment-analysis-in-text (Ekman, 1992) have been considered for the annotation process. Tweets were scraped that contained hashtags in the form #emotion corresponding to Ekman’s (Ekman, 1992) 6 basic emotions (like #anger, #disgust). DailyDialogs is a dataset of dialogs published by (Li et al., 2017) spanning over a variety of topics and better structured than any social media data. The SSEC corpus (Schuff et al., 2017) is an annotation of the SemEval 2016 Twitter stance and sentiment corpus (Mohammad et al., 2017) with Plutchik’s emotion labels (Plutchik, 2001). The authors studied the relation between emotion annotation and the other annotation layers like stance and sentiment. The EmoInt dataset published by (Mohammad and Bravo-Marquez, 2017) for evaluation of the WASAA-2017 Shared Task of Emotion Intensity (EmoInt) contains 7,097 tweets annotated with a pair of emotion tag and intensi"
2020.icon-main.62,W17-5205,0,0.0418945,"Missing"
2020.icon-main.62,S18-1001,0,0.0534959,"Missing"
2020.icon-main.62,D14-1162,0,0.0849914,"our severely under-represented classes, namely (disgust, fear, sadness and surprise) and one over-represented class (others). 4 Methodologies We develop various deep learning-based multi-task models for automatic detection of emotion and its intensity. As base learning techniques, we use Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014). We build three separate multi-task models (CNN based, Bi-GRU based and Bi-LSTM based) on top of pre-trained word embedding (GloVe 7 (Pennington et al., 2014)). The embedding layer is initialized with the pre-trained weights and is learned during the training in accordance with our dataset. We employ word attention (Bahdanau et al., 2014) mechanism to focus on the informative words in a document (tweet) and obtain an aggregated representation(document vector) which is passed through two fully-connected layers (100 neurons in each 7 http://nlp.stanford.edu/data/ wordvecs/glove.840B.300d.zip layer) and an output layer (with 7 neurons, one for each class) with Softmax activation. We use the categorical cross-entropy as the loss function. 4.1 Convoluti"
2020.icon-main.62,W17-5203,0,0.0382475,"Missing"
2020.icon-main.62,C16-1287,1,0.892658,"Missing"
2020.icon-main.62,S07-1013,0,0.0863487,"s annotation schemes were introduced to serve the specific purpose for which the corpus is created. (Scherer and Wallbott, 1994) collected questionnaires answered by people with different cultural backgrounds to form The International Survey on Emotion Antecedents and Reactions (ISEAR) dataset. People reported on their emotional events. The dataset contains a total of 7,665 sentences from reports by approximately 3,000 respondents. Sentences are annotated with single labels, chosen from the set of following labels: joy, fear, anger, sadness, disgust, shame, and guilt. The Affective Text task (Strapparava and Mihalcea, 2007) in SemEval 2007 was proposed to focus on the emotion classification of news headlines extracted from news web sites. Given a set of predefined six emotion labels (Paul Ekman’s basic emotions (Ekman, 1992)), classify the titles with the appropriate emotion label and/or with a valence indication (positive/negative). (Aman and Szpakowicz, 2007) published a dataset of blog content consisting of 5,205 sentences from 173 blogs. Each instance is annotated with an emotion label from Ekman’s basic emotions (Ekman, 1992) and also with an intensity score for that emotion. (Alm, 2008) researched the text"
2020.icon-main.62,N16-1174,0,0.0372911,"et al., 2011) and an output layer (with 7 neurons, one for each class) with Softmax activation. 4.4 level). HAtED focuses on each sentence in a tweet individually resulting in sentence vectors which are further attended upon to produce a document vector. The intuition is to focus upon important words in a sentence as well as important sentences in a document (tweet) for a particular emotion. For encoding of the sentences, we leverage Bi-GRU (256 neurons) based word encoder. Without making major changes to the basic architecture of the hierarchical attention framework as in the original work (Yang et al., 2016), we tweaked the last few layers to solve our objective. We pass the document vector through a dense layer (100 neurons with ReLU activation) followed by an output layer (7 neurons with Softmax activation). We use categorical crossentropy loss function for the classification task. Besides HAtED, we also develop two separate Hierarchical Attention-based models considering various sets of emotion classes. They are as follows: Hierarchical Attention Based Deep Neural Framework for Emotion Detection (HAtED) In recent works, Hierarchical attention (Bahdanau et al., 2014) based deep learning systems"
2020.icon-main.62,N16-1000,0,0.21503,"Missing"
2020.icon-main.66,S19-2065,0,0.0343749,"Missing"
2020.icon-main.66,D14-1181,0,0.00453907,"Missing"
2020.icon-main.66,P17-1001,0,0.0294116,"Missing"
2020.icon-main.66,S19-2007,0,0.145705,"speeches that contain insulting or abusive content that could promote violence and social disorders. The Indian government has also urged leading social media sites such as Facebook, Twitter to take necessary action against hate speech, especially those posts that create social outrage. Setting aside legal actions our aim should be to combat these texts by agreeing to a set of standard definitions, guidelines, and practices to remove the content. Recently many automated techniques following supervised learning utilizing deep neural networks have been developed. Recently shared tasks such as (Basile et al., 2019; Mandl et al., 2019; Zampieri et al., 2019) have mainly focused on developing multiple-layer identification of offensive languages. The existing prior research towards this direction mainly focused on singletask learning (STL) where classification task on one data set at a time is solved by training the model in stochastic gradient descent approach. However, training of neural networks relies on a large amount of data, and creating a balanced data set seems to be time-consuming, and tedious. As the number of posts showing aggressive tendencies is very less 4 https://www.inc.com/joseph-steinbe"
2020.icon-main.66,Q17-1010,0,0.0109034,"ed Multi-Task Learning(FS-MTL) 3.2 Embeddings Word embeddings (we ): In our experiments, we utilized the Google pre-trained word2vec vectors trained on 100 billion words to produce 300 dimensions for each word capturing the semantic and syntactic relationship between the words trained using skip-gram by (Mikolov et al., 2013). Character embedding (ce ): The presence of Out-ofVocabulary (OOV) word is a serious problem in a social media text. Embedding for such words in the pre-trained word embedding model is not found, hence losing morphological information. We leverage the skip-gram model by (Bojanowski et al., 2017) which represents each word as a bag of character n-grams. The dimension of each word using character embedding is 300. The final word embedding xe for word x ∈ Xis represented by the following process: xe = we ⊕ ce (1) where (⊕) denotes the concatenation operation and X is the number of unique tokens. The resulting dimension of xe is 600. 3.3 Models (Kim, 2014). The input sequence Si of length l is tokenized to assign a unique integer index to each word wi , that is then mapped to its N dimension real-valued vector. A convolution operation involves a filter f ∈ RhN which is applied to the h w"
2020.icon-main.66,W19-3508,0,0.0284617,"as follows. (i). We propose a deep multi-task learning framework that leverages information from multiple sources. We experiment on five different variations of CNN based single-task learning (STL) and five different variations of CNN based multi-task Learning (MTL) approaches for solving the problem of hate speech classification. (ii). The proposed classification approach can be utilized to obtain hateful or abusive posts to further train any classifier with these data to perform the classification to finer labels. terms to enrich the model with more contextual and knowledge-based features. (Chakrabarty et al., 2019) provided visualization of attention weights and concluded that the model assigned higher attention to potentially abusive terms when employed with contextual information in comparison to self-attention based features. (MacAvaney et al., 2019) utilized BERT, that make use of Transformer (Vaswani et al., 2017), an attention mechanism helping to capture the contextual representation between words and sub-words of a sentence that is utilized to perform the classification task on (de Gibert et al., 2018). (P´erez and Luque, 2019) leveraged BiLSTM with a dense layer on top consuming Elmo vectors (P"
2020.icon-main.66,S19-2085,0,0.0249033,"Missing"
2020.icon-main.66,S19-2096,0,0.041954,"Missing"
2020.icon-main.66,W17-3013,0,0.0608163,"Missing"
2020.icon-main.66,S19-2008,0,0.0435128,"Missing"
2020.icon-main.66,N18-1202,0,0.0529633,") provided visualization of attention weights and concluded that the model assigned higher attention to potentially abusive terms when employed with contextual information in comparison to self-attention based features. (MacAvaney et al., 2019) utilized BERT, that make use of Transformer (Vaswani et al., 2017), an attention mechanism helping to capture the contextual representation between words and sub-words of a sentence that is utilized to perform the classification task on (de Gibert et al., 2018). (P´erez and Luque, 2019) leveraged BiLSTM with a dense layer on top consuming Elmo vectors (Peters et al., 2018), and Bag of words as additional input to do the classification on the data by (Basile et al., 2019). In this paper, we present a multi-task framework that aims at leveraging information contained in multiple related tasks and improve the classification performance of the hate data sets. 3 Methodology 3.1 2 Related Work In recent times, online hate speech detection has attracted the attention of researchers and developers because of its necessity in maintaining social fabrics. In recent times, most of the methods that have emerged are mainly based on classical machine learning and deep learnin"
2020.icon-main.66,S17-2088,0,0.0560817,"Missing"
2020.icon-main.66,N18-2114,0,0.0222677,"ng fully-shared Word-CNN layers to extract features for all the tasks. It takes the view that features of task m can be totally shared by task n and the vice-versa. Figure 1 explains the idea. Here X1 → − D1 , X2 → − D2 , X3 → − D3 .{D1 , D2 and D3 explained in section 4} other tasks by Fijl+1 = l gij = (Wijl · Fijl + blij ) (10) Model 10: The training will remain the same as of model 9 but Di will share the features with S1 , S2 and S3 . Figure 2 explains the idea for 2 task which can be extended to n tasks. Here i ∈ [1,3] 4 4.1 Model 9: Soft Sharing CNN-Word-MTL: This model is motivated by (Xiao et al., 2018) that utilizes the CNN based multitasking paradigm. Every task owns a subnet and shares the features with each other. The embedding layer (EL) in Figure 2 consists of uniques tokens present in all the data sets. Here D1 , D2 and D3 will share feature with each other. All the subnet undergoes a pre-training of the text sequences. Let C be the total collection of n tasks C = {T1 ,T2 .....Tn }. The output of any sequence si at any layer l is the concatenation of the output of the same sequence si from all the other tasks. Task i borrows the features from Task j which is calculated as l gij + Fil"
2020.icon-main.66,S19-2010,0,0.0185333,"ve content that could promote violence and social disorders. The Indian government has also urged leading social media sites such as Facebook, Twitter to take necessary action against hate speech, especially those posts that create social outrage. Setting aside legal actions our aim should be to combat these texts by agreeing to a set of standard definitions, guidelines, and practices to remove the content. Recently many automated techniques following supervised learning utilizing deep neural networks have been developed. Recently shared tasks such as (Basile et al., 2019; Mandl et al., 2019; Zampieri et al., 2019) have mainly focused on developing multiple-layer identification of offensive languages. The existing prior research towards this direction mainly focused on singletask learning (STL) where classification task on one data set at a time is solved by training the model in stochastic gradient descent approach. However, training of neural networks relies on a large amount of data, and creating a balanced data set seems to be time-consuming, and tedious. As the number of posts showing aggressive tendencies is very less 4 https://www.inc.com/joseph-steinberg/germanys-toughnew-social-media-law-punish"
2020.lrec-1.201,W14-4012,0,0.0215111,"Missing"
2020.lrec-1.201,D14-1181,0,0.00246024,"es are highly vulnerable to commit suicide compared to females. These handful number of notes may not be sufficient to derive such conclusions but they can surely give a rough idea of the possible trend. Along with gender, length of notes also varies across the different age groups. Figure 2 depicts the relation among the size of notes, the number of notes and age interval of the deceased. Most lengthy notes belong to the young deceased whose age falls between 11 and 30. Also, among Methodology We develop and train three basic deep learning-based models, viz. Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014) on top of pre-trained word embeddings. Since GloVe 4 (Pennington et al., 2014) embedding model captures syntactic and semantic relations among words, we utilize it to learn our emotional word embeddings. The representation from the CNN/GRU/LSTM network is passed through an attention layer (Yang et al., 2016) that maps the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction."
2020.lrec-1.201,D14-1162,0,0.0866098,"Missing"
2020.lrec-1.201,W08-0616,0,0.323506,"well-being concern and a major cause of death worldwide, can be marked as the death caused by self-directed harmful behaviour with any intent to end one’s life. Moments before committing this horrendous act, usually an individual’s mind is flooded with a range of unpleasant emotions. (Pestian et al., 2008; Pestian et al., 2010; Duch et al., 2008). A suicide note is a significant asset when attempting to evaluate a patient’s risk of attempting suicide repeatedly. The suicide note furnishes us with the firsthand information about that individual’s specific personality status and mind rationale (Pestian and Matykiewicz, 2008). Suicide notes may serve some informative purpose and may have a remedial role in helping the surviving relatives to understand the reason behind suicide. An understanding of the messages contained inside suicide notes could be helpful for suicide aversion programs. Analyzing the inward feelings uncovered in the suicide note may assist us with identifying individuals who conceivably have suicide ideation (Xu et al., 2012), and thus prevent the misery from happening. Modelling the emotions present in such notes may help health experts in surveying suicide hazard, by contrasting the model with"
2020.lrec-1.201,C16-1287,1,0.88981,"Missing"
2020.lrec-1.201,N16-1174,0,0.371987,"ceased whose age falls between 11 and 30. Also, among Methodology We develop and train three basic deep learning-based models, viz. Convolution Neural Network (CNN) (Kim, 2014), Long Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) network (Cho et al., 2014) on top of pre-trained word embeddings. Since GloVe 4 (Pennington et al., 2014) embedding model captures syntactic and semantic relations among words, we utilize it to learn our emotional word embeddings. The representation from the CNN/GRU/LSTM network is passed through an attention layer (Yang et al., 2016) that maps the important and relevant words from the input sentence and assign higher weights to these words, enhancing the accuracy of the output prediction. Finally, 4 http://nlp.stanford.edu/data/wordvecs/ glove.840B.300d.zip 1621 we combine predictions of these models using the following two methods: using majority voting (MV), and using a MultiLayer Perceptron (MLP) network. For comparison purpose, we train four popularly used classical supervised models, namely Multinomial Naive Bayes, Support Vector Machine, Random Forest and Logistic Regression on our curated dataset, and compare with"
2020.lrec-1.273,P16-1072,0,0.0144283,"s model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers used both RNN and CNN to capture local features as well as long term relationships (Cai et al., 2016; Zhang et al., 2018b). Apart from CNN and RNN, some other approaches are also reported in the literature. He et al. (2018) proposed a syntax-aware entity embeddings based on tree-GRU for neural relation classification. Reinforcement learning is used by Feng et al. (2018a) for relation classification from noisy data. Inspired by Generative Adversarial Networks (GANs), Zeng et al. (2018a) proposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised data"
2020.lrec-1.273,P15-1017,0,0.130806,"um, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to u"
2020.lrec-1.273,P17-1038,0,0.012687,"includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conf"
2020.lrec-1.273,N15-1151,0,0.028377,"rial training (Goodfellow et al., 2014) to enhance the robustness of the classifier. CNN also fails to capture long distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities an"
2020.lrec-1.273,P16-2060,0,0.0865811,"xtraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make"
2020.lrec-1.273,D19-1041,0,0.0210277,"explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-resource scenario, especially for the Indic languages. Moreover, event extraction in the disaster domain can be very much helpful as we can mine the vast amount of online news 2242 and extract crucial information from these, and inform to the various stakeholders ranging from the government agencies t"
2020.lrec-1.273,P11-1113,0,0.0353007,"ng one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It incl"
2020.lrec-1.273,P08-1030,0,0.0555903,"in situations of crisis. 2. Related Studies Being one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction"
2020.lrec-1.273,N16-1030,0,0.0293008,"मे भूकंप की खबर है Feature # documents # sentences # Event Trigger words # Argument Trigger words Evaluation Event and Argument Trigger Classification : We formulate both event trigger detection and argument detection as sequence labelling problems. We also argue that they can be solved jointly as both of these tasks are co-related. Thus, we can define the task as : Given a sentence of the form w1 ,w2 ,w3 , . . . , wn , the task is to predict the event and argument labels (li ) for each word (wi ), where li ∈ {I,O,B}4 . We develop two baseline models based on Bi-LSTM+CRF (Huang et al., 2015; Lample et al., 2016) and Bi-LSTM+Softmax. Bi-LSTM (Schuster and Paliwal, 1997) is a very good sentence encoder and CRF (Conditional Random Field) (Lafferty et al., 2001) is very efficient in sequence labeling as it uses state transition matrix to take care of the past and future tags to predict the current tag (Sutton et al., 2012). Sequence labeling problem can 4 The encoding scheme is according to IOB2, where I indicates the word tokens that appear inside the trigger, B denotes the beginning of a trigger and O denotes the outside of an event trigger or argument trigger. The B is used only when two event or argu"
2020.lrec-1.273,P13-1008,0,0.0282259,"long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). So"
2020.lrec-1.273,N19-4019,0,0.0177926,"oposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised dataset. Relation extraction using deep learning techniques in cross-lingual setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-re"
2020.lrec-1.273,P10-1081,0,0.225496,"s. 2. Related Studies Being one of the essential tasks in Natural Language Processing (NLP), Event Extraction has already been studied extensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other f"
2020.lrec-1.273,P17-1004,0,0.0124793,"distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers"
2020.lrec-1.273,P16-1201,0,0.028074,"Missing"
2020.lrec-1.273,P17-1164,0,0.0125344,"shman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conference (TAC). Some of the existing works in Event extraction in disaster domain are reported in (Ta"
2020.lrec-1.273,D18-1156,0,0.0118356,"neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perfo"
2020.lrec-1.273,P16-1105,0,0.0131565,"o capture word-level context information and tensor layer to capture complex connection between the two entities, respectively, on the top of Bi-LSTM. Faruqui and Kumar (2015) proposed a pipe-lined model to develop relation extraction system for any source language. Lin et al. (2017) proposed a multilingual attention-based neural relation extraction (MNRE) model. This model employs mono-lingual attention to select the informative sentences within each language. To take the advantages of pattern consistency and complementarity among languages, the proposed model employs crosslingual attention. Miwa and Bansal (2016) presented a novel end-to-end neural model to extract entities and relations between them. Their proposed model allows joint modeling of both entities and relations using both bidirectional sequential and bidirectional tree-structured LSTM-RNNs. Some researchers used both RNN and CNN to capture local features as well as long term relationships (Cai et al., 2016; Zhang et al., 2018b). Apart from CNN and RNN, some other approaches are also reported in the literature. He et al. (2018) proposed a syntax-aware entity embeddings based on tree-GRU for neural relation classification. Reinforcement lea"
2020.lrec-1.273,P15-2060,0,0.100025,"searchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al."
2020.lrec-1.273,W15-1506,0,0.155737,"searchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al."
2020.lrec-1.273,D16-1085,0,0.0331386,"Missing"
2020.lrec-1.273,N16-1034,0,0.0157544,"se features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) propos"
2020.lrec-1.273,W03-2120,0,0.280326,"Missing"
2020.lrec-1.273,D18-1122,0,0.0180829,"or Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency relationships to perform event detection. Event extraction task has also been assessed in dedicated track in the Text Analysis Conference (TAC). Some of the existing works in Event extraction in disaster domain are reported in (Tanev et al., 2008; Yun, 2011; Klein et al., 2013; Dittrich and Lucas, 2014; Nugent et al., 2017; Burel et al., 2017). However, all the above are mostly in English language. Any significant attempt t"
2020.lrec-1.273,C18-1100,0,0.0124457,"(SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchical layers and dependency embedding to CNN architecture. Recently Ren et al. (2018) proposed a CNN based method with Adversarial training (Goodfellow et al., 2014) to enhance the robustness of the classifier. CNN also fails to capture long distance relationship. Zhang and Wang (2015) proposed a Recurrent Neural Network (RNN) to capture long distance relationships. A SDP based Long Short Term Memory network (SDP-LSTM) is proposed in Xu et al. (2015b) to pick useful information along SDP for different information channel. Zhang et al. (2018a) used the attention layer to capture word-level context information and tensor layer to capture complex connection between the two entiti"
2020.lrec-1.273,D11-1001,0,0.0285231,"xtensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et"
2020.lrec-1.273,W11-1807,0,0.0236237,"xtensively by various researchers across the globe for a long time. However, most of the studies emphasize on sub-tasks like detection and classification of both event and argument triggers. Both feature-based, as well as neural network-based approaches, were tried and tested by the research community. Some of the feature-based approaches have decomposed the entire event extraction task into several sub-tasks and solved them separately (Ji and Grishman, 2008; Liao and Grishman, 2010; Hong et al., 2011). But independent learning of several sub-task leads to error propagation. Some researchers (Riedel and McCallum, 2011a; Riedel and McCallum, 2011b; Li et al., 2013; Venugopal et al., 2014) propose joint event extraction algorithms to deal with this error propagation. Both of the above methods need feature engineering and utilization of the existing NLP tool-kits and resources for doing the same. In contrast, a neural network can learn those features automatically. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et"
2020.lrec-1.273,P15-1061,0,0.0172817,"ated to relation extraction where the relations between a pair of entities are extracted in contrast to our case where relation between event trigger and argument trigger is extracted. Deep Neural Network is being used for relation extraction in recent time. Zeng et al. (2014) proposed to use CNN in relation extraction for the first time where CNN was used for lexical and sentence level feature extraction. In this paper the authors proposed a novel Position Embedding (PE) feature which was very helpful to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irreleva"
2020.lrec-1.273,D19-1030,0,0.0142445,"earning is used by Feng et al. (2018a) for relation classification from noisy data. Inspired by Generative Adversarial Networks (GANs), Zeng et al. (2018a) proposed a GANbased method for distant supervised relation extraction. Reinforcement learning has also been explored in Zeng et al. (2018b) to learn sentence relation extractor with the distant supervised dataset. Relation extraction using deep learning techniques in cross-lingual setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research comm"
2020.lrec-1.273,D14-1090,0,0.0519707,"Missing"
2020.lrec-1.273,D19-1585,0,0.0181788,"l setup has also been explored by the research community. An adversarial multi-lingual neural relation extraction model is proposed by Wang et al. (2018). Recently Subburathinam et al. (2019) investigated a cross-lingual structure transfer method for event and relation extraction using Graph Convolutional Network (GCN). Li et al. (2019) demonstrates a multilingual knowledge extraction system which can perform event extraction, relation extraction, entity discovery, entity linking and coreference. Joint models for event and relation extraction are also proposed by some of the researchers like (Wadden et al., 2019; Han et al., 2019). 3. Motivation and Contribution From the above discussion, it is clear that event extraction as a field is essential and popular among the research community, but this has been mostly carried out for the resource-rich languages like English. However, there has been a very little works in the low-resource scenario, especially for the Indic languages. Moreover, event extraction in the disaster domain can be very much helpful as we can mine the vast amount of online news 2242 and extract crucial information from these, and inform to the various stakeholders ranging from the go"
2020.lrec-1.273,P16-1123,0,0.0357012,"Missing"
2020.lrec-1.273,C18-1099,0,0.0241651,"Missing"
2020.lrec-1.273,D15-1062,0,0.0179022,"to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchic"
2020.lrec-1.273,D15-1206,0,0.0127363,"to achieve high accuracy in classification. PE is also used in (Nguyen and Grishman, 2015b; Santos et al., 2015). Santos et al. (2015) proposed a CNN based relation classifier that performs classification using a Ranking CNN (CR-CNN). Their proposed pairwise ranking loss function makes it easy to reduce the impact of artificial classes. Nguyen and Grishman (2015b) used multiple window sized filters in their CNN architecture compared to single window sized filters as in (Zeng et al., 2014), to capture wider ranges of n-grams. Moreover, their proposed method avoids usage of external resources. Xu et al. (2015a) propose to extract features from the Shortest Dependency Path (SDP) using CNN to avoid irrelevant words. A multi-level attention CNN is proposed by Wang et al. (2016) to detect more subtle cues for relation classification in heterogeneous context. Their proposed method automatically learns which parts are relevant for a given classication. Thus, their proposed method gives best results without any external help. Though CNN is a good feature extractor, it fails to extract syntax as well as hierarchical information of sentence. Based on this observation, Li et al. (2017) introduced hierarchic"
2020.lrec-1.273,N16-1033,0,0.0167622,"ally. Due to this reason, neural based approach gained huge popularity in event extraction task like all the other field of NLP. It includes using a Convolutional Neural Network (CNN) for automatic feature extraction (Nguyen and Grishman, 2015a; Chen et al., 2015; Nguyen and Grishman, 2241 2016). Some authors use Recurrent Neural Network (RNN) (Ghaeini et al., 2016) and the combination of CNN and BiLSTM (Feng et al., 2018b) for Event detection. Like featurebased methods in previous cases, neural-based methods also suffer from error propagation if addressed separately. So (Nguyen et al., 2016; Yang and Mitchell, 2016; Liu et al., 2018b) introduced joint models for the Event Extraction task. To boost the performance further Chen et al. (2017) uses FreeBase and Liu et al. (2016) uses FrameNet to obtain more available data. Liu et al. (2017) proposes to use the annotated argument information explicitly for this task. In recent years (Liu et al., 2018a) make use of a cross language attention model for event detection. Orr et al. (2018) also uses attention mechanism to combine both temporal structure along with syntactic information. (Sha et al., 2018; Nguyen and Grishman, 2018) proposed to use dependency rela"
2020.lrec-1.273,C14-1220,0,0.0608331,"Missing"
2020.lrec-1.514,P18-1073,0,0.0151351,"istory and generic response; Gradient reversal layer is used to learn language invariant features; Polite responses for both Hindi and English are the generated outputs of the proposed model. Embedding Layer: Word embeddings are usually trained through an unsupervised manner on a huge dataset, and then the embeddings are fine-tuned by the supervised training process. For word embedding, we use the pre-trained embedding model, FastText2 for both English and Hindi. The monolingual embeddings for Hindi and English are mapped in the same vector space using linear transformation as illustrated in (Artetxe et al., 2018). With this technique, embeddings for every language exist in the same vector space and maintain the property that words with similar meanings (regardless of language) are close together in the vector space. Hence, the words in English appears close to the words in Hindi in the embedding space. Thus, we train on one or more languages and learn a model that operates on words of a particular language that was not present during training. catenated with the emotional representation of the given utterance to give the final utterance representation fd . The final utterance representation f1 , f2 ,"
2020.lrec-1.514,Q17-1010,0,0.00732333,"nts are first obtained. Each tweet is then divided into the three types of sentences, (i). informative if they have purely information, and no courteous expression in it; (ii). purely courteous utterances; and (iii). hybrid sentence denoting both informative and courteous. • Clustering: The customer care agents of a particular company mainly use expressions and sentences belonging to similar patterns. Hence grouping these similar expressions and sentences before annotation helps in making the annotation process faster. The vector representation of the utterances using the FastText embeddings (Bojanowski et al., 2017) for the Hindi language is used to represent the utterances. We then use the K-means clustering algorithm (Aggarwal and Zhai, 2012) with k = 300 to cluster these sentences. Basically, by clustering, we intend to divide the sentences into groups, where the sentences in a particular group are highly similar to each other in comparison to sentences in other groups. • Annotation: The segmented and clustered sentences are annotated by three annotators proficient in the Hindi language. The annotators were asked to label each sentence into three categories that are courteous, informative and hybrid."
2020.lrec-1.514,D17-1169,0,0.0314411,"states h21 , h22 , ...., h2n˜ . The conversational history is represented by the last hidden state h2n˜ , and is thereby referred to as the conversational context vector c. Contextual Encoder: The context encoder captures the conversational history C, which is a sequence of user utterances u1 , u2 , ....., un , where n is the total number of utterances in a given conversation. Each user utterance un comprises of a sequence of words w1 , w2 , ..., wn′ where n′ is the total number of words in a given utterance, and every word is represented by their pre-trained embeddings. We use the DeepMoji (Felbo et al., 2017) output distribution that is pre-trained on the emoji prediction task to encode the utterances with their corresponding emotional states. A Bi-directional Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) layer is used for encoding the utterances, and their representations are denoted by h1i , h12 , ....h1n , where n denotes the nth word in the utterance. The last hidden state h1n of the BiLSTM denoting the utterance representation is coneti = v T tanh(Wh hi + Ws st + battn ) (1) αt = sof tmax(et ) (2) 2 Generic Response Encoder: The word embedding sequence of the generic res"
2020.lrec-1.514,N19-1091,1,0.176764,"their preferred language by making the responses polite and courteous, eventually leading to user satisfaction and high customer retention for any given brand or company. The ability of such systems to understand the emotions of the users in different languages and responding in accordance with the emotion is a challenging task. Also, politeness is a virtue of humans, and to make a machine understand and behave amicably and courteously is an additional task for such systems. Hence, in this work, we propose a large-scale Hindi dataset for this task and evaluate using the baseline approach of (Golchha et al., 2019) to incorporate politeness in customer care responses belonging to different languages and providing new research directions for showcasing the differences in politeness and courteous behavior across the languages. We summarize the key contributions as follows: (i) We create a large-scale Hindi conversational data, prepared from the actual conversations on Twitter. (ii) We propose a robust response generation model for both Hindi and English languages by modeling the conversational history and the emotional state of the user by learning language invariant representation using adversarial train"
2020.lrec-1.514,W16-3609,0,0.0273755,"t dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversa"
2020.lrec-1.514,N18-2008,0,0.0119683,"g NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018"
2020.lrec-1.514,D16-1127,0,0.0240706,"results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms"
2020.lrec-1.514,N18-1169,0,0.0178495,"otional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018) in an unsupervised fashion. The authors in (Golchha et al., 2019) proposed a reinforced pointer generator network for inducing courteous behavior in customer care responses. Also, there is a recent shift in building systems that are capable of understanding different languages (Li et al., 2018b; Do and Gaspers, 2019; Masumura et al., 2018a), hence making conversational agents robust in their applications. In this work, we propose a novel system that is ca4173 pable of generating polite responses in different languages (in our case, H"
2020.lrec-1.514,W04-1013,0,0.0107642,"set for early stopping and finding the best models for decoding. Language English Hindi Model Seq2Seq Our Model Seq2Seq Our Model 0 16.88 9.87 15.42 10.56 F 1 41.32 42.05 40.54 41.28 2 41.80 48.08 44.04 48.16 0 16.74 13.52 17.23 14.11 CA 1 40.33 39.27 41.63 38.77 2 42.93 47.21 41.14 47.12 -1 24.56 13.24 25.84 14.62 PC 0 48.71 37.19 50.66 38.39 1 26.73 49.57 23.50 46.99 Table 4: Human evaluation results for Fluency, Content Adequacy and Politeness Consistency (All values are in percentages.) Automatic evaluation: In addition to conventional metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and perplexity, we also use two taskspecific metrics such as Emotional Accuracy (EA) and Content preservation (CP) for automatic evaluation as described in (Golchha et al., 2019). Human evaluation: We adopt a human assessment to compare the efficiency of various models to comprehend the quality of the generated polite responses same as (Golchha et al., 2019). For human evaluation, we randomly chose 700 samples from the test set for both the languages. Six human annotators with postgraduate exposure on both Hindi and English languages (three each for a language) were allocated to assess the po"
2020.lrec-1.514,D15-1166,0,0.0317492,"Missing"
2020.lrec-1.514,D18-1064,0,0.257098,"ssist humans and help them in the smallest possible ways as humanly as possible. Natural language generation (NLG) module of every dialogue system is an essential component as it presents the information to the user. To enhance the interactions between human and computers, recently researchers have focused on adapting different styles, emotions and personalities in text generation. Recent research has been inclined to make the system understand different languages giving rise to multi-lingual applications. With the significant focus on making the computer understand different languages as in (Masumura et al., 2018b; Masumura et al., 2018a; Upadhyay et al., 2018), researchers are aiming to make the dialogue systems language invariant as in real-world scenario. Also, for more extensive applications, it is crucial for these systems to be able to converse with humans in their preferred language, thereby increasing the usage and advancement of technology. Providing assistance to the customer through social media channels is attaining high popularity. The centre of our current work focuses on incorporating politeness in customer care responses belonging to different languages. Due to the unavailability of la"
2020.lrec-1.514,C18-1304,0,0.253702,"ssist humans and help them in the smallest possible ways as humanly as possible. Natural language generation (NLG) module of every dialogue system is an essential component as it presents the information to the user. To enhance the interactions between human and computers, recently researchers have focused on adapting different styles, emotions and personalities in text generation. Recent research has been inclined to make the system understand different languages giving rise to multi-lingual applications. With the significant focus on making the computer understand different languages as in (Masumura et al., 2018b; Masumura et al., 2018a; Upadhyay et al., 2018), researchers are aiming to make the dialogue systems language invariant as in real-world scenario. Also, for more extensive applications, it is crucial for these systems to be able to converse with humans in their preferred language, thereby increasing the usage and advancement of technology. Providing assistance to the customer through social media channels is attaining high popularity. The centre of our current work focuses on incorporating politeness in customer care responses belonging to different languages. Due to the unavailability of la"
2020.lrec-1.514,Q18-1027,0,0.089795,"8; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; Niu and Bansal, 2018; Fu et al., 2018) in an unsupervised fashion. The authors in (Golchha et al., 2019) proposed a reinforced pointer generator network for inducing courteous behavior in customer care responses. Also, there is a recent shift in building systems that are capable of understanding different languages (Li et al., 2018b; Do and Gaspers, 2019; Masumura et al., 2018a), hence making conversational agents robust in their applications. In this work, we propose a novel system that is ca4173 pable of generating polite responses in different languages (in our case, Hindi and English) by learning language inv"
2020.lrec-1.514,P02-1040,0,0.109721,"running loss on the validation set for early stopping and finding the best models for decoding. Language English Hindi Model Seq2Seq Our Model Seq2Seq Our Model 0 16.88 9.87 15.42 10.56 F 1 41.32 42.05 40.54 41.28 2 41.80 48.08 44.04 48.16 0 16.74 13.52 17.23 14.11 CA 1 40.33 39.27 41.63 38.77 2 42.93 47.21 41.14 47.12 -1 24.56 13.24 25.84 14.62 PC 0 48.71 37.19 50.66 38.39 1 26.73 49.57 23.50 46.99 Table 4: Human evaluation results for Fluency, Content Adequacy and Politeness Consistency (All values are in percentages.) Automatic evaluation: In addition to conventional metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and perplexity, we also use two taskspecific metrics such as Emotional Accuracy (EA) and Content preservation (CP) for automatic evaluation as described in (Golchha et al., 2019). Human evaluation: We adopt a human assessment to compare the efficiency of various models to comprehend the quality of the generated polite responses same as (Golchha et al., 2019). For human evaluation, we randomly chose 700 samples from the test set for both the languages. Six human annotators with postgraduate exposure on both Hindi and English languages (three each for a language) were allocat"
2020.lrec-1.514,P17-1099,0,0.0184945,"Missing"
2020.lrec-1.514,N18-1186,0,0.0252995,"e dataset description followed by the proposed methodology in Section 4. Experimental details, evaluation metrics and results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG system"
2020.lrec-1.514,P17-1061,0,0.019961,"Section 4. Experimental details, evaluation metrics and results are presented in Section 5 and Section 6, respectively. In Section 7, we present the concluding remarks followed by future direction. 2. Related Work Natural language generation (NLG) module provides a platform to conversational agents through which they can communicate with the users, thereby assisting them in achieving their desired objectives. Natural language generation is one of the core components of every dialogue system (Shen et al., 2018; Vinyals and Le, 2015; Wu et al., 2018; Serban et al., 2017a; Serban et al., 2017b; Zhao et al., 2017; Zhang et al., 2018). The authors in (Li et al., 2016) proposed a reinforcement learning-based approach for generating interesting, diverse and coherent dialogues. While the authors in (Raghu et al., 2018) employed a hierarchical pointer generator memory network for generating responses by handling out-of-vocabulary (OOV) words. In this work, we make the responses more engaging by incorporating politeness in them, thereby differentiating it from the existing NLG systems. Hence, our system can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it i"
2020.lrec-1.514,P18-1104,0,0.0150401,"ystem can add value to these existing NLG systems by making it polite, diverse and interesting. Therefore, it improves its usability and enhances its growth in terms of customer retention. Recently, emotion classification in conversations (Majumder et al., 2018; Herzig et al., 2016) has been an interesting research area, which aims at making the system aware of different human emotions. Specifically, in customer support systems, it is crucial to understand the feelings of the user for providing proper assistance to them as investigated in (Herzig et al., 2016). Generating emotional responses (Zhou and Wang, 2018; Zhou et al., ; Huang et al., 2018) has been addressed in the past to give the systems humanly essence. Unlike the existing emotional response generation systems where emotions are explicitly provided, in our work we model the customers’ emotions through conversational history and provide polite responses by being emotionally aware of the users’ emotional state. Lately, style transfer has been a growing research area with several works done in incorporating specific styles in the output texts which is different from the input texts (Carlson et al., 2017; Li et al., 2018a; Shen et al., 2017; N"
2020.lrec-1.621,C16-1047,1,0.851627,"U layer. Figure 2: The proposed Ensemble architecture embeddings from fastText ((Joulin et al., 2016)). The fastText uses subword information to generate embedding for a word, and hence it is able to handle the out-of-vocabulary (OOV) problem. The embedding of each word is then used as an input to the individual deep learning model to learn the representation of tweet. 4.1.2. Convolutional Neural Network (CNN) CNN architecture has been widely used in variety of NLP task (Kumar and Singh, 2019; Kim, 2014) and it has been successfully applied to solve sentiment classification at various levels (Akhtar et al., 2016). Convolutional neural network (CNN) consists of convolutional layers. Convolutional process on sentence is used to conserve n-gram information of sentence. Convolutional layers are followed by non-linear layer, Relu, followed by the pooling layers. We use 2 convolutional layers. The first convolutioanl layer contains 128 filters of sizes 2, 3, and 4 each, and second convolutional layer contains 128 filters of size 3. It is then followed by max pooling layer, dense layer and output layer. Filters of sizes 2, 3, and 4 correspond that the filter can slide over 2, 3, or 4 words at a time. 4.1.3."
2020.lrec-1.621,baccianella-etal-2010-sentiwordnet,0,0.524625,"Missing"
2020.lrec-1.621,P07-1056,0,0.971504,"blic and to maintain the law and order. The very first step in building such type of intelligent system is the mining of user sentiments. Deep learning has evolved as a popular technique over the years to solve many Natural Language Processing (NLP) problems including sentiment analysis. Annotated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus for sentiment analysis, and then develop a deep neural network based baseline model to tag each twe"
2020.lrec-1.621,D14-1181,0,0.00786971,"the pre-trained word 5049 layers of bidirectional GRU on top of each other with 128 units in each GRU layer. Figure 2: The proposed Ensemble architecture embeddings from fastText ((Joulin et al., 2016)). The fastText uses subword information to generate embedding for a word, and hence it is able to handle the out-of-vocabulary (OOV) problem. The embedding of each word is then used as an input to the individual deep learning model to learn the representation of tweet. 4.1.2. Convolutional Neural Network (CNN) CNN architecture has been widely used in variety of NLP task (Kumar and Singh, 2019; Kim, 2014) and it has been successfully applied to solve sentiment classification at various levels (Akhtar et al., 2016). Convolutional neural network (CNN) consists of convolutional layers. Convolutional process on sentence is used to conserve n-gram information of sentence. Convolutional layers are followed by non-linear layer, Relu, followed by the pooling layers. We use 2 convolutional layers. The first convolutioanl layer contains 128 filters of sizes 2, 3, and 4 each, and second convolutional layer contains 128 filters of size 3. It is then followed by max pooling layer, dense layer and output la"
2020.lrec-1.621,P11-1015,0,0.509579,"sitive, negative, mixed, or other classes. The authors have shown an inter-annotator agreement of 0.655. (Thelwall et al., 2012) created a dataset consisting of 4,424 tweets. Tweets were manually annotated with positive (1 to 5) and negative strength (-1 to -5). (Socher et al., 2013) introduced a Sentiment Treebank (STB) dataset constructed from the movie reviews domain. This dataset contains 215,154 phrases in the parse trees of 11,855 sentences annotated at the fine-grained level. This dataset was annotated with 5 classes, viz., positive, negative, neutral, very positive and very negative. (Maas et al., 2011) introduced a larger IMDB dataset containing 50000 movie reviews for binary classification. Only highly polarized reviews were considered by them. For example, a negative review had score ≤ 4 out of 10 and a positive review had a score ≥ 7 out of 10. (Go et al., 2009) used distant supervision to create Stanford Twitter Sentiment (STS) corpora containing 160,000 tweets. The data was crawled by using positive and negative emoticons from Twitter using Twitter Search API. Tweets with positive emoticons were considered as positive and tweets with negative emoticons were considered as negative. It a"
2020.lrec-1.621,S13-2053,0,0.0832409,"Missing"
2020.lrec-1.621,W16-0429,0,0.168387,"r annotations to make the data suitable for performing experiments. Some examples of the tweets along with their irrelevant categories are shown in Table 2. 3.3. Data Annotation After data extraction and pre-processing, we conduct manual annotation of the dataset. Three annotators with postgraduate level knowledge in English are employed for annotation. Annotators are asked to write the overall polarity of the tweet for 3 classes, viz., neutral, negative and positive, if any opinion expression is found. For annotation, we follow the guidelines used in the SemEval task (Rosenthal et al., 2015; Mohammad, 2016). We provided some tweets to the annotators with gold labels to create understanding of the class labels. Annotators were also instructed to annotate the tweet without being biased towards any specific demographic area, religion, etc. 3.4. Challenges During annotation, we faced the following challenges: 5 • If a writer makes a request to do something positive in the context of a negative situation, then we assumed the sentiment to be positive. For example, we should unite together to remove crime from our country. In this example tweet, the writer is requesting everyone with positive attitude,"
2020.lrec-1.621,P04-1035,0,0.165,"the years have been created for sentiment analysis. But, most of these efforts have been put in the domain of movie reviews, product reviews, hotel reviews, etc. Below we present a survey on the various resources created for sentiment analysis. (Pang et al., 2002) created a sentiment corpora from internet movie database which contains only those reviews where the author rating was expressed either with stars or some numerical value. Based on the rating, sentiment polarity was automatically decided from 2 categories, positive and negative. This contains 752 negative and 1301 positive reviews. (Pang and Lee, 2004) published another movie review polarity dataset for 2 classes, positive and negative. Dataset contains 1000 positive and 1000 negative movie reviews. (Blitzer et al., 2007) created a MultiDomain Sentiment (MDS) Dataset consisting of four different types of product reviews taken from Amazon.com including Books, DVDs, Electronics, and Kitchen appliances. Dataset comprises of 1000 positive and 1000 negative reviews for each domain. (Shamma et al., 2009) constructed Obama-McCain Debate dataset by crawling the first U.S. presidential TV debate tweets in September 2008. They annotated the tweets fo"
2020.lrec-1.621,W02-1011,0,0.0473072,"p to monitor terrorist groups, domestic threats, and crime activities to provide security to public and to maintain the law and order. The very first step in building such type of intelligent system is the mining of user sentiments. Deep learning has evolved as a popular technique over the years to solve many Natural Language Processing (NLP) problems including sentiment analysis. Annotated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus"
2020.lrec-1.621,D14-1162,0,0.0846734,"Missing"
2020.lrec-1.621,S15-2078,0,0.0243974,"bove were performed after annotations to make the data suitable for performing experiments. Some examples of the tweets along with their irrelevant categories are shown in Table 2. 3.3. Data Annotation After data extraction and pre-processing, we conduct manual annotation of the dataset. Three annotators with postgraduate level knowledge in English are employed for annotation. Annotators are asked to write the overall polarity of the tweet for 3 classes, viz., neutral, negative and positive, if any opinion expression is found. For annotation, we follow the guidelines used in the SemEval task (Rosenthal et al., 2015; Mohammad, 2016). We provided some tweets to the annotators with gold labels to create understanding of the class labels. Annotators were also instructed to annotate the tweet without being biased towards any specific demographic area, religion, etc. 3.4. Challenges During annotation, we faced the following challenges: 5 • If a writer makes a request to do something positive in the context of a negative situation, then we assumed the sentiment to be positive. For example, we should unite together to remove crime from our country. In this example tweet, the writer is requesting everyone with p"
2020.lrec-1.621,S17-2088,0,0.189646,"(Go et al., 2009) used distant supervision to create Stanford Twitter Sentiment (STS) corpora containing 160,000 tweets. The data was crawled by using positive and negative emoticons from Twitter using Twitter Search API. Tweets with positive emoticons were considered as positive and tweets with negative emoticons were considered as negative. It also contains a test set which contains 498 tweets, manually annotated for 3 classes, viz., positive, negative, and neutral. Test set was crawled using names of products, companies, and people. SemEval 2017 dataset was constructed as a part of Task 4 (Rosenthal et al., 2017). Dataset was annotated on points of 2 (positive and negative), 3 (positive, negative, and neutral), and 5 (strongly positive, weakly positive, neutral, weakly negative, and strongly negative) scales. All the annotations were performed using CrowdFlower. SemEval 2017 dataset was built by merging all previous year’s SemEval datasets, consisting of 50,333 tweets related to twitter trends Donald Trump, iPhone, etc. Researchers have also put their efforts towards building sentiment corpora for the financial domain. As an example, (O’Hare et al., 2009) created financial blog corpus. They collected"
2020.lrec-1.621,D13-1170,0,0.0191802,"views taken from Amazon.com including Books, DVDs, Electronics, and Kitchen appliances. Dataset comprises of 1000 positive and 1000 negative reviews for each domain. (Shamma et al., 2009) constructed Obama-McCain Debate dataset by crawling the first U.S. presidential TV debate tweets in September 2008. They annotated the tweets for positive, negative, mixed, or other classes. The authors have shown an inter-annotator agreement of 0.655. (Thelwall et al., 2012) created a dataset consisting of 4,424 tweets. Tweets were manually annotated with positive (1 to 5) and negative strength (-1 to -5). (Socher et al., 2013) introduced a Sentiment Treebank (STB) dataset constructed from the movie reviews domain. This dataset contains 215,154 phrases in the parse trees of 11,855 sentences annotated at the fine-grained level. This dataset was annotated with 5 classes, viz., positive, negative, neutral, very positive and very negative. (Maas et al., 2011) introduced a larger IMDB dataset containing 50000 movie reviews for binary classification. Only highly polarized reviews were considered by them. For example, a negative review had score ≤ 4 out of 10 and a positive review had a score ≥ 7 out of 10. (Go et al., 200"
2020.lrec-1.621,takala-etal-2014-gold,0,0.47495,"Missing"
2020.lrec-1.621,L18-1442,1,0.920473,"otated corpora is certainly the foremost requirement. Many annotated corpora are available for sentiment analysis. But most of the prior efforts have been in the domains such as movie, product, and hotel reviews (Pang et al., 2002; Pang and Lee, 1 https://www.internetlivestats.com/twitter-statistics/ 2004; Blitzer et al., 2007). It can help to provide them satisfactory service or recommendation before buying a product etc. Apart from this, a small body of research has also been dedicated to sentiment analysis in the financial domain (Malo et al., 2013; Takala et al., 2014) and medical domain (Yadav et al., 2018). To the best of our knowledge, there is no publicly available multi-domain tweet corpus, dedicated towards sentiment analysis for such pervasive domains. In this paper, we introduce a multi-domain tweet corpus for sentiment analysis, and then develop a deep neural network based baseline model to tag each tweet into three affect classes, namely positive, negative, and neutral. This is the very first attempt towards creating a benchmark setup for sentiment analysis in the above mentioned socially relevant domains. The corpus is manually annotated by three expert annotators. The inter-annotator"
2020.lrec-1.621,S14-2077,0,0.0391117,"Missing"
2020.lrec-1.675,P16-1223,0,0.0488709,"Missing"
2020.lrec-1.675,P17-1171,0,0.0972473,"ls. But both of these works are based on Cloze Style MRC task. In contrast, our task focuses on preparing a dataset for span-of-words prediction (Span Prediction) based MRC model, where the system has to extract span-of-words as answer to a particular question based on the context. We employ articles from Elsevier Computer Science Journals (like ARTINT, COMNET etc.). 1.1. Related Work The problem of document understanding falls in the domain of Natural Language Understanding (NLU), and has a long history. Machine Reading Comprehension (Hermann et al., 2015) and Open Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unans"
2020.lrec-1.675,P17-1152,0,0.19737,"ls. But both of these works are based on Cloze Style MRC task. In contrast, our task focuses on preparing a dataset for span-of-words prediction (Span Prediction) based MRC model, where the system has to extract span-of-words as answer to a particular question based on the context. We employ articles from Elsevier Computer Science Journals (like ARTINT, COMNET etc.). 1.1. Related Work The problem of document understanding falls in the domain of Natural Language Understanding (NLU), and has a long history. Machine Reading Comprehension (Hermann et al., 2015) and Open Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unans"
2020.lrec-1.675,D14-1179,0,0.00935404,"Missing"
2020.lrec-1.675,P18-1078,0,0.0137731,"se MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ questions generated by annotators. RACE: This dataset was created by Lai et al. (2017), and contains nearly 100,000 multiple choice questions and 27,000 passages from standardized tests for Chinese students, who are learning English as a foreign language. The aim of creating this dataset is to test the students0 ability in understanding and reasoning, covering variety of topics. AI2 Reasoning Challenge (ARC): A team (Clark and Gardner, 2018) of Allen Institute for Artificial Intelligence prepared this dataset. It consists of 7,787 grade-school multiple choice (with 4 possible options) science question. ReCoRD: Zhang et al. (2018) represents this MRC dataset that requires commonsense reasoning. It contains 12,000 cloze-style question passage pairs extracted from CNN/Daily Mail news articles. It requires common sense reasoning to answer the queries of this dataset. 1.2. Motivation and Contribution Our understanding and survey reveal that- although there are many benchmark datasets available for questionanswering- but there has not"
2020.lrec-1.675,D19-1600,0,0.019686,"tion data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous datasets and models are controled by questions that can be answered by selecting answers using local contextual similarity or global term frequency. This dataset encourages the deeper understanding of languages. Span Extract Chinese MRC Dataset: Cui et al. (2019) recently proposed a novel dataset to add language diversities in this area as the existing datasets focus on only English language. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. Delta Reading Comprehension Dataset (DRCD): Shao et al. (2018) proposed an open domain traditional Chinese MRC dataset. The main aim of this dataset is to be a standard Chinese MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ question"
2020.lrec-1.675,W18-2605,0,0.019662,"questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chinese MRC dataset proposed by He et al. (2018). The dataset was created with the real application data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous datasets and models are controled by questions that can be answered by selecting answers using local contextual similarity or global term frequency. This dataset encourages the deeper understanding o"
2020.lrec-1.675,P14-1026,0,0.0812576,"Missing"
2020.lrec-1.675,D17-1082,0,0.0393713,"Missing"
2020.lrec-1.675,L18-1439,0,0.0322515,"Missing"
2020.lrec-1.675,D14-1162,0,0.0844653,"Missing"
2020.lrec-1.675,D16-1264,0,0.0730864,"Missing"
2020.lrec-1.675,P18-2124,0,0.0166837,"Domain Question Answering (Chen et al., 2017a) are the two very challenging tasks and fall under the domain of NLU. To encourage this task, over the years research community has come up with publicly available several datasets and methods for benchmarking. We condense a few significant of them, and show in Table 1. We describe these briefly in the following: The Stanford Question Answering Dataset (SQuAD): Rajpurkar et al. (2016) presented the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unanswerable questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chi"
2020.lrec-1.675,D13-1020,0,0.0815989,"Missing"
2020.lrec-1.675,W17-2623,0,0.022775,"the RC dataset having more than 100k questions constructed by the crowdworkers on a set of Wikipedia articles. The second version of SQuAD was released by Rajpurkar et al. (2018) that focuses on unanswerable questions. This version combines the previous version of SQuAD and additionally over 50,000 unanswerable questions are written adversarially by crowdworkers to look into the similar ones. MAchine Reading COmprehension Dataset (MSMARCO): Nguyen et al. (2016) proposed a dataset that comprises of 1 million anonymized questions sampled 5498 from Bing’s search query logs. NewsQA: This dataset (Trischler et al., 2017) consists of more than 100,000 human generated QA pairs. The goal of preparing this was to test the MRC models on reasoning skills. DuReader: This is a Chinese MRC dataset proposed by He et al. (2018). The dataset was created with the real application data from Baidu search and Baidu Zhidao (a community QA website). It comprises of 200,000 questions and 420,000 answers from 1,000,000 documents. In this dataset the answers have additional label like either fact based or opinionative. NarrativeQA: Koˇcisk`y et al. (2018) created NarrativeQA based on summaries of movie scripts and books. Previous"
2020.lrec-1.675,D15-1237,0,0.0381738,"Missing"
2020.lrec-1.675,W17-2603,0,0.0185071,"rea as the existing datasets focus on only English language. The dataset is composed by near 20,000 real questions annotated on Wikipedia paragraphs by human experts. Delta Reading Comprehension Dataset (DRCD): Shao et al. (2018) proposed an open domain traditional Chinese MRC dataset. The main aim of this dataset is to be a standard Chinese MRC dataset, which could be utilised as a source dataset for transfer learning. It comprises of 10,014 paragraphs obtained from 2,108 Wikipedia articles and from there 30,000+ questions generated by annotators. RACE: This dataset was created by Lai et al. (2017), and contains nearly 100,000 multiple choice questions and 27,000 passages from standardized tests for Chinese students, who are learning English as a foreign language. The aim of creating this dataset is to test the students0 ability in understanding and reasoning, covering variety of topics. AI2 Reasoning Challenge (ARC): A team (Clark and Gardner, 2018) of Allen Institute for Artificial Intelligence prepared this dataset. It consists of 7,787 grade-school multiple choice (with 4 possible options) science question. ReCoRD: Zhang et al. (2018) represents this MRC dataset that requires common"
2020.lrec-1.675,P13-1043,0,0.0822359,"Missing"
2020.semeval-1.261,S17-2126,0,0.0409537,"Missing"
2020.semeval-1.261,W17-3007,0,0.0206339,"Missing"
2020.semeval-1.261,W17-3013,0,0.0543949,"Missing"
2020.semeval-1.261,S19-2081,0,0.0284817,"mination of the same to curtail its effect on social media. One promising solution is to develop automated tools for identification of offensive language using various Natural Language Processing (NLP) and Machine Learning (ML) techniques. In the past few years, many shared tasks and seminars were introduced to address this problem and provide relevant annotated data collected from various social media. Some of the related tasks are: ALW1 (related to Abusive Language Identification), TRAC 1 2018 2 related to Aggression Identification (Kumar et al., 2018), GermEval Task 2 3 , HatEval 2019 4 (i Orts, 2019), HASOC 2019 5 and OffensEval 2019 Task 6 6 related to Offensive Language Identification (Zampieri et al., 2019b). Recent works have considered categorizing hate speech problem into sub-classes like abusive, aggressive, or offensive speech. Such categorization of social media posts help law-enforcement agencies with the surveillance of social media. 1.1 Problem definition Similar to OffensEval-2019 shared task, the OffensEval-2020 shared task-organized at SemEval-2020 involves detection of Offensive Language and Target Identification from Tweets in English. OffensEval2020 (Zampieri et al., 202"
2020.semeval-1.261,W18-4401,0,0.0166528,"ene posts, comments, images, etc. and prevent further dissemination of the same to curtail its effect on social media. One promising solution is to develop automated tools for identification of offensive language using various Natural Language Processing (NLP) and Machine Learning (ML) techniques. In the past few years, many shared tasks and seminars were introduced to address this problem and provide relevant annotated data collected from various social media. Some of the related tasks are: ALW1 (related to Abusive Language Identification), TRAC 1 2018 2 related to Aggression Identification (Kumar et al., 2018), GermEval Task 2 3 , HatEval 2019 4 (i Orts, 2019), HASOC 2019 5 and OffensEval 2019 Task 6 6 related to Offensive Language Identification (Zampieri et al., 2019b). Recent works have considered categorizing hate speech problem into sub-classes like abusive, aggressive, or offensive speech. Such categorization of social media posts help law-enforcement agencies with the surveillance of social media. 1.1 Problem definition Similar to OffensEval-2019 shared task, the OffensEval-2020 shared task-organized at SemEval-2020 involves detection of Offensive Language and Target Identification from Twee"
2020.semeval-1.261,malmasi-zampieri-2017-detecting,0,0.0271939,"Missing"
2020.semeval-1.261,W17-3008,0,0.0118492,"f hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentences in Chinese. The GermEval shared task (Schmidt and Wiegand, 2017) was introduced to facilitate research on the offensive language identification in microposts in the German language. 3 Data 3.1 Data Des"
2020.semeval-1.261,D17-1117,0,0.0156697,"et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier for detection of hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentenc"
2020.semeval-1.261,D14-1162,0,0.0837174,"shown under column Undersampled. 3.3 Preprocessing We perform a series of preprocessing of the tweets in the dataset before using them to build our models. We replace smileys (like : −) or :) is replaced by the word Happy) and emojis7 by their meaning. We use 7 https://pypi.org/project/emoji/ 1985 a dictionary of popularly used contractions8 to replace them with their elongated forms. We also perform basic pre-processing steps like URLs and hashtags removal, extra blank spaces removal, conversion to lower case, the omission of non-ASCII characters etc. 4 Methodology We use pre-trained GloVe9 (Pennington et al., 2014) word embeddings to initialize our embedding layer and further fine-tune it on our training data while learning. The output from the embedding layer is passed through a BiGRU (256 units) layer which encodes the input representation to hidden representation. We leverage the effectiveness of Hierarchical attention (HATT) based Document Classification technique (Yang et al., 2016) to attend upon the instances more precisely. The attended vector is passed through two separate task-specific fully connected layers followed by their respective output layers (2 neurons with softmax activation for the"
2020.semeval-1.261,W17-1101,0,0.0118754,"non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier for detection of hate speech. A CNN and GRU based approach was proposed by (Zhang et al., 2018) for hate speech detection. An interesting work on predicting future hostility and its intensity looking at the current situation was studied in (Liu et al., 2018). Though most of the works related to Offensive Language and Hate Speech has been in English, still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments"
2020.semeval-1.261,W17-3003,0,0.0138549,"still there are few works in other languages as well. (Pavlopoulos et al., 2017) worked on a large dataset of Sports Comments in Greek and proposed several approaches to handle user content moderation using neural networks and sophisticated attention mechanism. (Mubarak et al., 2017) introduced a corpus 1984 in Arabic consisting of obscene and offensive user comments and words in social media. (Fiˇser et al., 2017) explored malpractices in social networking sites in Slovenia mainly relating to the legal domain and subsequently introduced a dataset and annotation schema about such practices. (Su et al., 2017) proposed a system to detect and alter obscene and vulgar sentences in Chinese. The GermEval shared task (Schmidt and Wiegand, 2017) was introduced to facilitate research on the offensive language identification in microposts in the German language. 3 Data 3.1 Data Description The OffensEval-2020 dataset (Rosenthal et al., 2020) (Zampieri et al., 2020) follows the similar hierarchical annotation scheme as used in creating Offensive Language Identification Dataset (OLID v1.0) (Zampieri et al., 2019a). The first level of classification is to distinguish between offensive (OFF) and non-offensive"
2020.semeval-1.261,W12-2103,0,0.114814,"Missing"
2020.semeval-1.261,W17-3012,0,0.0121186,"observations and results related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to bui"
2020.semeval-1.261,W16-5618,0,0.0312473,"ults related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general profanity and hate speech. (Wulczyn et al., 2017) introduced the Wikipedia Comments Corpora for building models to evaluate hate speech classifiers. Usage of word n-grams and sentiment lexicons were reported in (Davidson et al., 2017). In a comprehensive survey by (Schmidt and Wiegand, 2017), various linguistic, lexical, sentiment, surface features, etc. were identified that can be useful to build a classifier"
2020.semeval-1.261,N12-1084,0,0.0371778,"In Section 2 we briefly discuss the related work in this area. In Section 3, we describe the dataset and various preprocessing steps on the dataset. We discuss the methodology in Section 4. In Section 5, we present results and give a brief analysis. In Section 6, we give our conclusion along with future works. 2 Related Work For quite some time now, researchers have studied and reported their observations and results related to online misuse of social media platforms ((Razavi et al., 2010); (Warner and Hirschberg, 2012); (Ribeiro et al., 2017)). Misuse may take many forms like cyberbullying (Xu et al., 2012), trolling (Kwok and Wang, 2013) and offensive language (Cheng et al., 2017). (Gamb¨ack and Sikdar, 2017) proposed a range of CNN-based deep neural models for classification of tweets into one of the following categories: sexism, racism, either (sexism or racism) and non-hate. Twitter Hate Speech text includes racism, sexism, both and a non-hate-speech classification system. ((Waseem et al., 2017; Waseem, 2016)) introduced a series of sub-tasks related to cyberbullying, online abuse and hate speech. ((Malmasi and Zampieri, 2017; Malmasi and Zampieri, 2018)) addressed the differences in general"
2020.semeval-1.261,N16-1174,0,0.308473,". We also perform basic pre-processing steps like URLs and hashtags removal, extra blank spaces removal, conversion to lower case, the omission of non-ASCII characters etc. 4 Methodology We use pre-trained GloVe9 (Pennington et al., 2014) word embeddings to initialize our embedding layer and further fine-tune it on our training data while learning. The output from the embedding layer is passed through a BiGRU (256 units) layer which encodes the input representation to hidden representation. We leverage the effectiveness of Hierarchical attention (HATT) based Document Classification technique (Yang et al., 2016) to attend upon the instances more precisely. The attended vector is passed through two separate task-specific fully connected layers followed by their respective output layers (2 neurons with softmax activation for the classification task and 1 neuron with sigmoid activation for the regression task). For sub-task C, we train our model on the classification task only so there is a single output layer with 3 neurons (signifying 3 classes) with softmax activation. Rest of the architecture is similar to the models for sub-task A and B with an exception that the output from the attention layer is"
2020.semeval-1.261,N19-1144,0,0.0232996,"Missing"
2020.semeval-1.261,S19-2010,0,0.0360722,"Missing"
2021.calcs-1.5,P07-2045,0,0.0109553,"obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenizer (Koehn et al., 2007) and truecaser 4 scripts and Indic-nlp-library 5 to tokenize Hindi. We remove the sentences having length greater that 150 tokens and created synthetic code-mixed corpus on the resulting corpus as described earlier. The statistics of data used in the experiments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/clab/fast_align/ 32 Figure 1: An example of alignment between parallel sentence pair and generate"
2021.calcs-1.5,D18-2012,0,0.0239449,"Model Baseline Synthetic CM Test 2.45 10.09 Table 2: BLEU scores of the Baseline model and Synthetic Code-Mixed model on Development and Test sets. Source Experimental Setup We conduct the experiments on Transformer based Encoder-Decoder NMT architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warmup steps with initial learning rate of 2. We use Sentencepiece (Kudo and Richardson, 2018) with joint vocabulary size of 50K. Models are trained with OpenNMT toolkit 6 (Klein et al., 2017) with batch size of 2048 tokens till convergence and checkpoints are created after every 10,000 steps. All the checkpoints that are created during the training are averaged and considered as the best parameters for each model. During inference, beam size is set to 5. 4 Dev 2.55 11.52 Reference Output Source Reference Output Source Reference Output Who is your favorite member from the first avengers? Tumhara favorite member kaun hai first avengers mein se? first avengers se aapka favorite member ko"
2021.calcs-1.5,W18-3817,0,0.0351862,"n roman script, during the synthetic corpus creation, we transliterate the Hindi tokens from Devanagari script to Roman script. Code-Mixing (CM) is a very common phenomenon in various social media contents, product description and reviews, educational domain etc. For better understanding and ease in writing, users 2 Translation of code-mixed data has gained popularity in recent times. Menacer et al. (2019) conducted experiments on translating Arabic-English CM data to pure Arabic and/or to pure English with Statistical Machine Translation (SMT) and Neural Machine Translation (NMT) approaches. Dhar et al. (2018) proposed an MT augmentation pipeline which takes CM sentence and determines the most dominating language and translates the ∗ 1 Related Works Equal contribution Hindi words are romanized 31 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 31–35 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_005 sides except {yk } and {xj } respectively. The obtained candidate token set is further pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token"
2021.calcs-1.5,L18-1548,1,0.792609,"f a specific language pair. We use the implementation2 of fast_align algorithm (Dyer et al., 2013) to obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenizer (Koehn et al., 2007) and truecaser 4 scripts and Indic-nlp-library 5 to tokenize Hindi. We remove the sentences having length greater that 150 tokens and created synthetic code-mixed corpus on the resulting corpus as described earlier. The statistics of data used in the experiments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/cla"
2021.calcs-1.5,N13-1073,0,0.0487045,"de-mixed corpus only and source (English) is kept as it is. The gold corpus provided by organizers is not modified in any way and also kept as it is. System Description In this section, we describe the synthetic parallel corpus creation, dataset and experimental setup of our system. 3.1 Unsupervised Synthetic Code-Mixed Corpus Creation 3.3 We utilize the existing parallel corpus to create synthetic code-mixed data. First we learn word-level alignments between source and target sentences of a given parallel corpus of a specific language pair. We use the implementation2 of fast_align algorithm (Dyer et al., 2013) to obtain the alignment matrix. Let X = {x1 , x2 , ..., xm } be the source sentence and Y = {y1 , y2 , ..., yn } be the target sentence. We consider only those alignment pairs {xj , yk } [for j = (1, ...., m) and k = (1, ...., n)] which are having one-to-one mapping, as candidate tokens. By ‘One-to-one mapping’, we mean that neither {xj } nor {yk } should be aligned to more than one token from their respective counter 2 Romanization of the Hindi text Dataset We consider English-Hindi IIT Bombay (Kunchukuttan et al., 2018) parallel corpus. We tokenize and true-case English using Moses tokenize"
2021.calcs-1.5,D18-1346,0,0.0202374,"ge identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman script, we utilize Python based transliteration tool.3 This convert the Devanagari script to Roman script. We also create another version of the synthetic code-mixed corpus by replacing the two consecutive vowels with single vowel (Belinkov and Bisk, 2018). We call this version of code-mixed corpus as synthet"
2021.calcs-1.5,2020.findings-emnlp.206,1,0.7928,"MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are"
2021.calcs-1.5,P02-1040,0,0.110787,"ments are shown in Table 1. 3 https://github.com/libindic/Transliteration 3https://github.com/mosessmt/mosesdecoder/blob /RELEASE-3.0/scripts/tokenizer/tokenizer.perl 5 https://github.com/anoopkunchukuttan/indic_nlp _library 4 https://github.com/clab/fast_align/ 32 Figure 1: An example of alignment between parallel sentence pair and generated CM sentence. In the CM sentence, the source words that are replaced are shown with red border. Corpus Synthetic CM Synthetic CM + User Patterns Gold Total Train 1,549,115 1,549,115 8,060 3,106,290 Dev 942 942 tains 960 sentences. Our model achieved BLEU (Papineni et al., 2002) score of 10.09. Table 2 shows the BLEU scores obtained from the trained models on Development and Test sets. Table 3 shows some sample translations. Table 1: Data statistics used in the experiment. Synthetic CM: Size of synthetic code-mixed data. Synthetic CM + User Patterns: Size of synthetic codemixed data with addition of user writing patterns. Gold: Size of gold standard parallel corpus provided by organizers. Train, Dev denotes Training and Development set statistics respectively. In the experiments we use only gold standard corpus as development set. 3.4 Model Baseline Synthetic CM Test"
2021.calcs-1.5,P18-1143,0,0.0229817,"while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman scr"
2021.calcs-1.5,P17-4012,0,0.0360608,"e-Mixed model on Development and Test sets. Source Experimental Setup We conduct the experiments on Transformer based Encoder-Decoder NMT architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warmup steps with initial learning rate of 2. We use Sentencepiece (Kudo and Richardson, 2018) with joint vocabulary size of 50K. Models are trained with OpenNMT toolkit 6 (Klein et al., 2017) with batch size of 2048 tokens till convergence and checkpoints are created after every 10,000 steps. All the checkpoints that are created during the training are averaged and considered as the best parameters for each model. During inference, beam size is set to 5. 4 Dev 2.55 11.52 Reference Output Source Reference Output Source Reference Output Who is your favorite member from the first avengers? Tumhara favorite member kaun hai first avengers mein se? first avengers se aapka favorite member kon hai? I think it was a robotic shark, but am not sure. me sochta hoon voh robotic shark thi, but"
2021.calcs-1.5,N19-1044,0,0.0173279,"r pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token xj is replaced with aligned target token yk . The generated code-mixed sentence is in the form: CM = {x1 , x2 , ..., yk , yl , ..., xm }. Figure 1 shows an example of English-Hindi code-mixed sentence generated through this method. remaining words into that language. The resulting sentence will be in one single language and can be translated to other language with the existing MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input"
2021.calcs-1.5,K19-1026,0,0.0175694,"hata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an Encoder-Decoder based model which takes English sentence along with linguistic features as input and generates synthetic code-mixed sentence. Pratapa et al. (2018) explored ‘Equivalence Constraint’ theory to generate the synthetic code-mixed data which is used to improve the performance of Recurrent Neural Network (RNN) based language model. While Winata et al. (2019) proposed a method to generate code-mixed data using a pointer-generator network, Garg et al. (2018) explored SeqGAN for code-mixed data generation. 3 3.2 The task is to generate Hinglish data in which Hindi words are written in Roman script. But in the generated synthetic code-mixed corpus, Hindi words are written in Devanagari script. In order to convert the Devanagari script to Roman script, we utilize Python based transliteration tool.3 This convert the Devanagari script to Roman script. We also create another version of the synthetic code-mixed corpus by replacing the two consecutive vowe"
2021.calcs-1.5,2020.emnlp-main.208,0,0.0291341,"52-056-4_005 sides except {yk } and {xj } respectively. The obtained candidate token set is further pruned by removing the pairs where xj is a stopword. Based on the resulting candidate set, the source token xj is replaced with aligned target token yk . The generated code-mixed sentence is in the form: CM = {x1 , x2 , ..., yk , yl , ..., xm }. Figure 1 shows an example of English-Hindi code-mixed sentence generated through this method. remaining words into that language. The resulting sentence will be in one single language and can be translated to other language with the existing MT systems. Yang et al. (2020) have used code-mixing phenomenon and proposed a pre-training strategy for NMT. Song et al. (2019) augmented the codemixed data with clean data while training the NMT system and reported that this type of data augmentation improves the translation quality of constrained words such as named entities. Singh and Solorio (2017); Masoud et al. (2019); Mahata et al. (2019) also explored various approaches which utilize linguistic resources (such as language identification etc.) to translate the code-mixed data. There have been some efforts for creating codemixed data. Gupta et al. (2020) proposed an"
2021.ecnlp-1.21,N19-1311,0,0.0209521,"ोनरर फोन को बहु त ह अचछ पकड़ देते हैं । (raunded kornar phon ko bahut hee achchhee pakad dete hain) इस क मत में अब तक के सबसे अचछे फोन में से एक । (is keemat mein ab tak ke sabase achchhe phon mein se ek .) ले कन यह ऐपपल है और ऐपपल ऐसा ह होता है (lekin yah aippal hai aur aippal aisa hee hota hai) सबसे पहले मैं थैंक यू िफलपकाटर कहना चाहता हू।ं (sabase pahale main thaink yoo phlipakaart kahana chaahata hoon.) रयर कैमरा इमेज ा लट बहु त अचछ है। (riyar kaimara imej kvaalitee bahut achchhee hai) Table 3: Samples from the generated English-Hindi parallel corpus ing time (Vaibhav et al., 2019; Anastasopoulos et al., 2019). Since it is diﬀicult for the NMT model to see all the noisy variants of a correct token at training time, the model hence treats the noisy tokens as the unseen tokens. Word segmentation is a popular method that deals with the unseen tokens. Byte-pair-encoding (BPE) (Sennrich et al., 2016b) segments the words based on the rare character combinations. In BPE, a word is converted into the subword units based on the fixed learned list of less frequent character combinations. Subword regularization (SR) (Kudo, 2018) was introduced as a more diverse word segmentation method which segments the word"
2021.ecnlp-1.21,W04-3250,0,0.523062,"Missing"
2021.ecnlp-1.21,P18-1007,0,0.172727,"ed English-Hindi parallel corpus ing time (Vaibhav et al., 2019; Anastasopoulos et al., 2019). Since it is diﬀicult for the NMT model to see all the noisy variants of a correct token at training time, the model hence treats the noisy tokens as the unseen tokens. Word segmentation is a popular method that deals with the unseen tokens. Byte-pair-encoding (BPE) (Sennrich et al., 2016b) segments the words based on the rare character combinations. In BPE, a word is converted into the subword units based on the fixed learned list of less frequent character combinations. Subword regularization (SR) (Kudo, 2018) was introduced as a more diverse word segmentation method which segments the words based on a unigram language model. For these segmentation models, it is diﬀicult to capture all the noisy versions at training time. So before segmentation, we use a character based language model that maps the noisy and correct versions of tokens together in a vector space as shown in Figure 1. It helps to replace the noisy token with its correct form before inference. There has not been any significant attempt to translate the product reviews, except the one proposed in (Berard et al., 2019) that addressed th"
2021.ecnlp-1.21,L18-1548,0,0.206318,"grammar and spelling inconsistencies. These sentences have been considered as noisy sentences. Noise handling techniques as discussed in Section 5 are used to train the model to translate the noisy sentences. Gold Corpus Creation by Human Post-editing After pre-processing and filtering, we obtain 16,138 standard English sentences. Instead of translating sentences from scratch, we use an in-house English-Hindi machine translation system developed for the judicial domain. The model is trained for English-Hindi translation using judicial data (English-Hindi), and additional English-Hindi corpus (Kunchukuttan et al., 2018)9 . The sentences generated from this automatic translation are postedited by human experts. The experts are post-graduates in linguistics and have good command in Hindi and English both. The experts read the English sentences and its Hindi translation. They were instructed to make the correction in the sentences, if required. Some • User friendly vocabulary selection at Hindi (target) side. Too many complicated Hindi words which are not in much use should be avoided. Transliteration of an English word can also be used in the Hindi side because in India, people generally use Hinglish (mix of H"
2021.ecnlp-1.21,D19-5617,0,0.0222988,"bword regularization (SR) (Kudo, 2018) was introduced as a more diverse word segmentation method which segments the words based on a unigram language model. For these segmentation models, it is diﬀicult to capture all the noisy versions at training time. So before segmentation, we use a character based language model that maps the noisy and correct versions of tokens together in a vector space as shown in Figure 1. It helps to replace the noisy token with its correct form before inference. There has not been any significant attempt to translate the product reviews, except the one proposed in (Berard et al., 2019) that addressed the translation of English to French. In contrast, we develop product review translation system for English-Hindi. English and Hindi are morphologically and syntactically distant languages, which pose more challenges for machine translation. Further, Hindi is a resource-poor language for which we do not have suﬀicient resources and tools, even for the generic domain. 3 3.1 3.2 Pre-processing We remove the emojis from the English sentence by providing their unicode range using regular expressions. Any character having repetition of more than 2 times is trimmed and then checked f"
2021.ecnlp-1.21,D18-1050,0,0.0224022,"ction 5 presents the approaches of training the character language model, word vector generation model, and handling of noisy source input tokens at inference time. Section 6 presents the details regarding the dataset used and the experimental setup. Results and analysis of our approach are discussed in Section 7. Finally, Section 8 concludes the work with future research directions. 2 Related Work Machine translation with noisy text is, itself, a very challenging task. Noisy tokens (misspelled words) pose great challenges to develop the Neural Machine Translation (NMT) models (c.f. Table 2) (Michel and Neubig, 2018). In the literature, there are a few existing works that focus on handling the noisy text by increasing the robustness of the translation model. An MTNT (machine translation of noisy text) test-bed was introduced in (Michel and Neubig, 2018) that discussed the challenges of noisy contents. It has been also observed that even small noise in the input sentence can degrade the translation quality of the NMT model significantly (Belinkov and Bisk, 2018; Karpukhin et al., 2019). To improve the robustness of the translation model, they introduced synthetic errors like character swapping, replacement"
2021.ecnlp-1.21,P16-1009,0,0.208053,"ts) from the ecommerce websites. After pre-processing (ref. Section 3.2) and filtering (ref. Section 3.3), we translate the English sentences into Hindi language using our in-house English-Hindi translation system2 . The generated Hindi output sentences are given to the professionals who are experts in Hindi and English languages. The language experts post-edit the Hindi output as per the guidelines (ref. Section 3.5) provided to them. In addition, we also crawl monolingual Hindi sentences (ref. Section 3.6) from electronics gadgets’ description websites. These sentences are back-translated3 (Sennrich et al., 2016a) using the Hindi-toEnglish translation model trained over the post-edited parallel corpus. Neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017) is the dominant translation technology nowadays, and adapting this to the noisy text is very crucial due to the phenomenal growth in social media. Since NMT models learn from a fixed number of source and target vocabulary during training, any noisy word during the inference becomes an out-of-vocabulary (OOV) token because it does not belong to the NMT model’s training vocabulary. It is not possible to train an NMT model with"
2021.ecnlp-1.21,P16-1162,0,0.346694,"ts) from the ecommerce websites. After pre-processing (ref. Section 3.2) and filtering (ref. Section 3.3), we translate the English sentences into Hindi language using our in-house English-Hindi translation system2 . The generated Hindi output sentences are given to the professionals who are experts in Hindi and English languages. The language experts post-edit the Hindi output as per the guidelines (ref. Section 3.5) provided to them. In addition, we also crawl monolingual Hindi sentences (ref. Section 3.6) from electronics gadgets’ description websites. These sentences are back-translated3 (Sennrich et al., 2016a) using the Hindi-toEnglish translation model trained over the post-edited parallel corpus. Neural machine translation (NMT) (Bahdanau et al., 2015; Vaswani et al., 2017) is the dominant translation technology nowadays, and adapting this to the noisy text is very crucial due to the phenomenal growth in social media. Since NMT models learn from a fixed number of source and target vocabulary during training, any noisy word during the inference becomes an out-of-vocabulary (OOV) token because it does not belong to the NMT model’s training vocabulary. It is not possible to train an NMT model with"
2021.ecnlp-1.21,N19-1326,0,0.049224,"Missing"
2021.ecnlp-1.21,N19-1190,0,0.101675,"(corrected) राउंडड े कोनरर फोन को बहु त ह अचछ पकड़ देते हैं । (raunded kornar phon ko bahut hee achchhee pakad dete hain) इस क मत में अब तक के सबसे अचछे फोन में से एक । (is keemat mein ab tak ke sabase achchhe phon mein se ek .) ले कन यह ऐपपल है और ऐपपल ऐसा ह होता है (lekin yah aippal hai aur aippal aisa hee hota hai) सबसे पहले मैं थैंक यू िफलपकाटर कहना चाहता हू।ं (sabase pahale main thaink yoo phlipakaart kahana chaahata hoon.) रयर कैमरा इमेज ा लट बहु त अचछ है। (riyar kaimara imej kvaalitee bahut achchhee hai) Table 3: Samples from the generated English-Hindi parallel corpus ing time (Vaibhav et al., 2019; Anastasopoulos et al., 2019). Since it is diﬀicult for the NMT model to see all the noisy variants of a correct token at training time, the model hence treats the noisy tokens as the unseen tokens. Word segmentation is a popular method that deals with the unseen tokens. Byte-pair-encoding (BPE) (Sennrich et al., 2016b) segments the words based on the rare character combinations. In BPE, a word is converted into the subword units based on the fixed learned list of less frequent character combinations. Subword regularization (SR) (Kudo, 2018) was introduced as a more diverse word segmentation"
2021.ecnlp-1.21,W18-1820,0,0.144069,"+BT’ model that yields 37.79 BLEU points. 5 Handling Noisy Tokens In this section, we describe the methodology used in our work. Figure 2 presents the overall process of our proposed method. It consists of various steps like character language model (LM) training, word vector (embedding) generation, and finally noisy token replacement at inference time. Section 5.1 and Section 5.2 describe the steps in details. 5.1 Training Character LM and Word Vector Generation We train an English–to–Hindi baseline model using the human corrected corpus as mentioned in Table 4. We use the Sockeye framework (Hieber et al., 2018) for training the Transformer neural network based NMT. We splitted the words into subwords (Sennrich et al., 2016b) using BPE technique. We perform 4,000 BPE merge operations. Our model contains 6-6 encoder-decoder layers, 512 hidden size and word embedding size, learning rate as 0.0002 and min batch size as 3800 tokens. We used early stopping over the validation set. After training over the human corrected corA word consists of a sequence of characters. Each character is represented as a one-hot vector and a sequence of such vectors is passed through two different Long Short Term Memory (LST"
2021.ecnlp-1.21,D19-5506,0,0.0187543,"sy tokens (misspelled words) pose great challenges to develop the Neural Machine Translation (NMT) models (c.f. Table 2) (Michel and Neubig, 2018). In the literature, there are a few existing works that focus on handling the noisy text by increasing the robustness of the translation model. An MTNT (machine translation of noisy text) test-bed was introduced in (Michel and Neubig, 2018) that discussed the challenges of noisy contents. It has been also observed that even small noise in the input sentence can degrade the translation quality of the NMT model significantly (Belinkov and Bisk, 2018; Karpukhin et al., 2019). To improve the robustness of the translation model, they introduced synthetic errors like character swapping, replacement and drop in the corpus. Synthetic noise using back-translated corpus was also inserted in the original corpus to introduce the NMT model with noise at train175 Sr. English Sentence (crawled) 1. rounded corners make griping the phone very well . 2. one of the best phone ever at this price . 3. but this is Apple and Apple is like that only 4. At first I want to say Thank u flipkart. 5. Rear camera image quality is very good. Hindi Sentence (corrected) राउंडड े कोनरर फोन को"
2021.findings-emnlp.151,D16-1203,0,0.0669094,"Missing"
2021.findings-emnlp.151,D18-1024,0,0.0246943,"proposed single student model that can correctly 1 https://www.iitp.ac.in/~ai-nlp-ml/ resources.html predict the answers to the questions of the various language combinations (on eleven (11) different language-vision setups) including code-mixed setups over state-of-the-art pretrained language-vision models. 2 Related Work Multilingual and Code-Mixing: There is a recent trend in developing methods and resources for various NLP applications involving multilingual and code-mixed languages. Some of the works include question-answering (Raghavi et al., 2015; Gupta et al., 2018b), word embedding (Chen and Cardie, 2018; Lample et al., 2018; Pratapa et al., 2018b), code-mixed text generation (Pratapa et al., 2018a; Gonen and Goldberg, 2019; Gupta et al., 2020a), code-mixed language modelling (Winata et al., 2018; Gonen and Goldberg, 2019), and other NLP tasks (Gupta et al., 2018a, 2016a,b, 2017). Visual Question Answering: In the literature, various VQA datasets (Silberman et al., 2012; Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017) have been created to encourage multi-disciplinary research. The popular frameworks for VQA explore attention mechanisms to learn the joint representation of image and"
2021.findings-emnlp.151,K18-1012,1,0.830338,"e demonstrate the effectiveness of our proposed single student model that can correctly 1 https://www.iitp.ac.in/~ai-nlp-ml/ resources.html predict the answers to the questions of the various language combinations (on eleven (11) different language-vision setups) including code-mixed setups over state-of-the-art pretrained language-vision models. 2 Related Work Multilingual and Code-Mixing: There is a recent trend in developing methods and resources for various NLP applications involving multilingual and code-mixed languages. Some of the works include question-answering (Raghavi et al., 2015; Gupta et al., 2018b), word embedding (Chen and Cardie, 2018; Lample et al., 2018; Pratapa et al., 2018b), code-mixed text generation (Pratapa et al., 2018a; Gonen and Goldberg, 2019; Gupta et al., 2020a), code-mixed language modelling (Winata et al., 2018; Gonen and Goldberg, 2019), and other NLP tasks (Gupta et al., 2018a, 2016a,b, 2017). Visual Question Answering: In the literature, various VQA datasets (Silberman et al., 2012; Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017) have been created to encourage multi-disciplinary research. The popular frameworks for VQA explore attention mechanisms to lea"
2021.findings-emnlp.151,2020.aacl-main.90,1,0.921403,"various language combinations (on eleven (11) different language-vision setups) including code-mixed setups over state-of-the-art pretrained language-vision models. 2 Related Work Multilingual and Code-Mixing: There is a recent trend in developing methods and resources for various NLP applications involving multilingual and code-mixed languages. Some of the works include question-answering (Raghavi et al., 2015; Gupta et al., 2018b), word embedding (Chen and Cardie, 2018; Lample et al., 2018; Pratapa et al., 2018b), code-mixed text generation (Pratapa et al., 2018a; Gonen and Goldberg, 2019; Gupta et al., 2020a), code-mixed language modelling (Winata et al., 2018; Gonen and Goldberg, 2019), and other NLP tasks (Gupta et al., 2018a, 2016a,b, 2017). Visual Question Answering: In the literature, various VQA datasets (Silberman et al., 2012; Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017) have been created to encourage multi-disciplinary research. The popular frameworks for VQA explore attention mechanisms to learn the joint representation of image and question (Fukui et al., 2016; Kim et al., 2017; Yu et al., 2017; Kim et al., 2018). Recently, with the success of Transformer (Vaswani et al.,"
2021.findings-emnlp.151,W18-3207,0,0.0280637,"ent language-vision setups) including code-mixed setups over state-of-the-art pretrained language-vision models. 2 Related Work Multilingual and Code-Mixing: There is a recent trend in developing methods and resources for various NLP applications involving multilingual and code-mixed languages. Some of the works include question-answering (Raghavi et al., 2015; Gupta et al., 2018b), word embedding (Chen and Cardie, 2018; Lample et al., 2018; Pratapa et al., 2018b), code-mixed text generation (Pratapa et al., 2018a; Gonen and Goldberg, 2019; Gupta et al., 2020a), code-mixed language modelling (Winata et al., 2018; Gonen and Goldberg, 2019), and other NLP tasks (Gupta et al., 2018a, 2016a,b, 2017). Visual Question Answering: In the literature, various VQA datasets (Silberman et al., 2012; Gao et al., 2015; Antol et al., 2015; Goyal et al., 2017) have been created to encourage multi-disciplinary research. The popular frameworks for VQA explore attention mechanisms to learn the joint representation of image and question (Fukui et al., 2016; Kim et al., 2017; Yu et al., 2017; Kim et al., 2018). Recently, with the success of Transformer (Vaswani et al., 2017), Tan and Bansal (2019) proposed cross-modality"
2021.inlg-1.39,N18-2008,0,0.0902917,"ction IV, we describe the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) design"
2021.inlg-1.39,W17-5534,0,0.0151217,"having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The au"
2021.inlg-1.39,N19-1374,0,0.0361565,"Missing"
2021.inlg-1.39,P17-1059,0,0.0344962,"Missing"
2021.inlg-1.39,P18-3020,0,0.0161891,"l setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic s"
2021.inlg-1.39,D18-1071,0,0.0175659,"e the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized re"
2021.inlg-1.39,P16-1094,0,0.174332,"employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016"
2021.inlg-1.39,D16-1127,0,0.242056,"employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016"
2021.inlg-1.39,I17-1099,0,0.0171313,"e used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with co"
2021.inlg-1.39,W04-1013,0,0.024961,"to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when the gradient norm is over 5. To reduce data sparsity, all the numbers and names are replaced with &lt;number&gt; and &lt;person&gt;. Automatic Evaluation Metrics: In order to assess the model at the emotional and grammatical level, we present the results using the traditional automatic metrics. Perplexity(Chen et al., 1998) is stated to test our proposed framework at the content level. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the ability of the generated response for capturing the correct information. BLEU measures the n-grams overlap between the generated response and the gold response, and has become a standard measure for comparing task-oriented dialog systems. It is used to measure the content preservation in the generated responses. We report Distinct-1 and Distinct-2 metrics that measure the distinct n-grams in the generated responses and are scaled with respect to the total number of generated tokens to avoid repetitive and boring responses (Li et al., 2016b). To measure the emotional content in"
2021.inlg-1.39,D15-1166,0,0.0231398,"he last hidden representation hi|Pm |(i.e. the representation at the EOS token) as the persona representation of the given speaker. Therefore, the final persona representation of the utterance Pm is: hpi = hi|Pm |+ p0i (5) Emotion controlled Decoder: To generate the next textual response with the given emotion information we employ a RNN decoder as shown in Figure 1. We employ GRU for generating the response in a sequential manner based on the context hidden representation from both the transformers, and the words decoded previously. We use the input feeding decoding along with the attention (Luong et al., 2015) mechanism for enhancing the performance of the model. Using the decoder state hdec d,t as the query vector, we apply self-attention on the hidden representation of the utterance-level encoder. The decoder state, persona information and the context vector are concatenated and used to calculate a final distribution of the probability 356 over the output tokens. the basic hierarchical encoder-decoder framework. (vii) Trans: Basic transformer network without persona, sentiment and emotion information. (viii) Trans + E + P: The transformer encoders along with persona encoder and emotion informatio"
2021.inlg-1.39,P19-1542,0,0.0155032,"18, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The authors in (Madotto et al., 2019) used a meta-learning framework to include persona information in the generated responses. Similarly, the authors in (Yavuz et al., 2019) employed a hierarchical pointer network for generating personabased responses. The authors in (Song et al., 2019) used persona information to generate diverse responses by employing conditional variational autoencoder. Our present work differs from these existing works (that made use of the PersonaChat dataset) in a sense that we intend to use the persona information while g"
2021.inlg-1.39,D18-1298,0,0.0439632,"Missing"
2021.inlg-1.39,P02-1040,0,0.109339,"as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when the gradient norm is over 5. To reduce data sparsity, all the numbers and names are replaced with &lt;number&gt; and &lt;person&gt;. Automatic Evaluation Metrics: In order to assess the model at the emotional and grammatical level, we present the results using the traditional automatic metrics. Perplexity(Chen et al., 1998) is stated to test our proposed framework at the content level. We also report the results using the standard metrics like BLEU-4 (Papineni et al., 2002) and Rouge-L (Lin, 2004) to measure the ability of the generated response for capturing the correct information. BLEU measures the n-grams overlap between the generated response and the gold response, and has become a standard measure for comparing task-oriented dialog systems. It is used to measure the content preservation in the generated responses. We report Distinct-1 and Distinct-2 metrics that measure the distinct n-grams in the generated responses and are scaled with respect to the total number of generated tokens to avoid repetitive and boring responses (Li et al., 2016b). To measure t"
2021.inlg-1.39,P19-1050,0,0.0181672,"wd workers who were instructed to play the part of a given persona. In over 10,981 dialogues, this dataset comprises of 164,356 utterances and has a collection of 1,155 personas, each consisting of at least four personality texts. There are 1,016 dialogues in the testing set and 200 never before seen personas. As the dataset is not labeled with emotions, we use the emotion annotated version of the dataset used in (Firdaus et al., 2020). Dataset Preparation: As sentiment and emotions are highly co-related we annotate the PersonaChat dataset using the emotion information in a similar manner as (Poria et al., 2019). As emotions such as excited, grateful, joyful, caring, hopeful, faithful, impressed have a positive undertone hence we automatically label the utterances having these emotion labels as positive sentiment. Similarly for emotions such as angry, sad, annoyed, disgusted, terrified, furious, disappointed, jealous has a negative undertone hence are labelled as negative sentiment. For the other emotion labels such as surprise, proud, nostalgic, guilty, confident, prepared, sentimental that can either be positive, neutral or negative depending on the utterance and the context we resort to manual ann"
2021.inlg-1.39,P19-1534,0,0.0136906,"g with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed to generate responses with a specific emotion (Song et al., 2020). Emotional embedding, along with affective sampling and regularizer, was employed to generate the affect driven dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence fr"
2021.inlg-1.39,2020.acl-main.7,0,0.0236516,"ersona and sentiment while generating emotional responses compared to the existing baselines. The rest of the paper is structured as follows. In Section II, we present a brief survey of the related work. In Section III, we explain the proposed methodology. In Section IV, we describe the details of the datasets that we used and annotated. The experimental setup, along with the evaluation metrics, is reported in Section V. In Section VI, we present the results along with the necessary analysis. Finally, we conclude in Section VII with future work. 2 auto-encoders for personalized generation in (Wu et al., 2020). As personalization has been considered in responses, we intend to take a step ahead by inculcating the emotions in accordance to the emotion of the user and the dialogue history. Lately, emotional text generation has gained immense popularity (Huang et al., 2018; Li and Sun, 2018; Lin et al., 2019; Li et al., 2017; Ghosh et al., 2017; Kezar, 2018; Rashkin et al., 2019; Zhou and Wang, 2017). In (Zhou et al., 2018), an emotional chatting machine (ECM) was proposed that was built upon seq2seq framework for generating emotional responses. Recently, a lexiconbased attention framework was employed"
2021.inlg-1.39,N18-1186,0,0.0231067,"dialogues in (Colombo et al., 2019). Lately, authors in (Firdaus et al., 2020) designed personalized response generation framework with controllable emotions using basic sequence-to-sequence framework. Our present research differs from these existing works as we propose a novel framework using a generative adversarial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to"
2021.inlg-1.39,D18-1428,0,0.0243644,"arial network to generate responses in an empathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Ma"
2021.inlg-1.39,D17-1065,0,0.0607417,"Missing"
2021.inlg-1.39,W19-5917,0,0.0176674,", 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song et al., 2019, 2020). The authors in (Madotto et al., 2019) used a meta-learning framework to include persona information in the generated responses. Similarly, the authors in (Yavuz et al., 2019) employed a hierarchical pointer network for generating personabased responses. The authors in (Song et al., 2019) used persona information to generate diverse responses by employing conditional variational autoencoder. Our present work differs from these existing works (that made use of the PersonaChat dataset) in a sense that we intend to use the persona information while generating emotional"
2021.inlg-1.39,P18-1205,0,0.228837,"nts and decreasing breakdowns in conversations (Martinovski and Traum, 2003). Moreover, these agents should also have the capability to generate personalized responses conforming to the personal interests and unique needs of different users while presenting a consistent personality to gain the user’s trust and confidence. Hence, the primary motivation of our current work lies in generating responses that are engaging, emotionally appropriate, and also integrates the personal interests of the user. Lately, researchers have started focusing on incorporating personality information on chit-chat (Zhang et al., 2018) and goal-oriented (Joshi et al., 2017; Luo et al., 2019) conversational systems. Due to the lack of persona data sets, the authors created a PersonaChat dataset in (Zhang et al., 2018), where the individual personality data is represented in a few texts for open-domain chit-chat dialogue systems. We present an example from the dataset in Table 1, from which it is obvious that the speakers are able to retain the persona knowledge when communicating with each other. This helps to make the dialogue engaging and also makes it easier to build trust and credibility with the users (Shum et al., 2018"
2021.inlg-1.39,P19-1366,0,0.0163002,"mpathetic manner, having a consistent persona. Related Work In complete applications, such as dialogue systems, natural language generation (NLG) has become increasingly essential (Vinyals and Le, 2015; Li et al., 2016b; Serban et al., 2017; Wu et al., 2018) and also in many other natural language interfaces. The generation of responses provides the means by which a conversational agent can communicate with its user to assist users in achieving their desired goals. Recently, generative adversarial networks have been exploited for dialogue generation (Xu et al., 2018, 2017; Zhang et al., 2019; Zhu et al., 2019; Bruni and Fernandez, 2017) for a better generation of responses. Persona information is an essential part of generating responses. Earlier works on persona-based conversational models (Li et al., 2016a) incorporated speakers’ embeddings to infuse persona information in the responses. To incorporate persona in chit-chat models, the authors in (Zhang et al., 2018; Mazar´e et al., 2018) introduced a PersonaChat dataset that includes personal information of the speakers. This dataset has been extensively used to build persona-based dialogue systems (Madotto et al., 2019; Yavuz et al., 2019; Song"
2021.mtsummit-research.13,I17-1051,0,0.0211228,"ing with sentiment reward only; (4). MTbleu : AC finetuning with content reward only; (5). MTac : AC fine-tuning with both the rewards. Finally, har for brevity we choose the best performing AC-reward model for the proposed curriculum-based learning. 4.2 Hyper-parameters Setting In all our experiments, we use an NMT system based on Luong et al. (2015), using a single layer bi-directional RNN for the encoder. All the encoder-decoder parameters are uniformly initialized in the range of [-0.1,0.1]. The sizes of embedding and hidden layers are set to 256 and 512, respectively. The Adam optimizer (Abdalla and Hirst, 2017) with β1 = 0.9, β2 = 0.99 is used and the gradient vector is clipped to magnitude 5. We set the dropout to 0.2 and use the input feeding with learning rate (lr) and batch size (bs) set to 1e − 3 and 64. We first perform supervised pre-training of the NMT using the parallel corpora from Table 1: (A), and select the best model parameters according to the perplexity on the development set (c.f. Table 1: (A)). We refer the actor- thus obtained- as MTLL , that acts as a trained policy in the RL training (refer to the upper left side of Figure 1). Then, we keep the actor fixed and warm-up the critic"
2021.mtsummit-research.13,N18-1053,1,0.83651,"translation. (iii). Additionally, we utilize the idea of CL during the RL fine-tuning of the pre-trained model and try to learn from easy to hard data, where hard corresponds to the instances with lower harmonic reward. To the best of our knowledge, this is the first work in NMT that studies CL in the ambit of RL fine-tuning. 2 Related Work The use of translation-based solution for cross-lingual sentiment classification is successfully leveraged in the literature (Wu et al., 2021; Tebbifakhr et al., 2020; Araújo et al., 2020; Poncelas et al., 2020b; Fei and Li, 2020; Tebbifakhr et al., 2019; Akhtar et al., 2018; Barnes et al., 2016; Balahur and Turchi, 2012; Kanayama et al., 2004) which suggest an inspiring use-case of the MT system, and brings motivation for this piece of work. Given the context of this work, we look at the pieces of works that address the preservation of sentiment in the automatic translation. In one of the early works, Chen and Zhu (2014) used a lexicon-based consistency approach to design a list of sentiment-based features and used it to rank the candidates of t-table in a Phrase based MT system. Lohar et al. (2017, 2018) prepared the positive, negative and neutral sentiment-spe"
2021.mtsummit-research.13,W12-3709,0,0.0156336,"ize the idea of CL during the RL fine-tuning of the pre-trained model and try to learn from easy to hard data, where hard corresponds to the instances with lower harmonic reward. To the best of our knowledge, this is the first work in NMT that studies CL in the ambit of RL fine-tuning. 2 Related Work The use of translation-based solution for cross-lingual sentiment classification is successfully leveraged in the literature (Wu et al., 2021; Tebbifakhr et al., 2020; Araújo et al., 2020; Poncelas et al., 2020b; Fei and Li, 2020; Tebbifakhr et al., 2019; Akhtar et al., 2018; Barnes et al., 2016; Balahur and Turchi, 2012; Kanayama et al., 2004) which suggest an inspiring use-case of the MT system, and brings motivation for this piece of work. Given the context of this work, we look at the pieces of works that address the preservation of sentiment in the automatic translation. In one of the early works, Chen and Zhu (2014) used a lexicon-based consistency approach to design a list of sentiment-based features and used it to rank the candidates of t-table in a Phrase based MT system. Lohar et al. (2017, 2018) prepared the positive, negative and neutral sentiment-specific translation systems to ensure the cross-l"
2021.mtsummit-research.13,C16-1152,0,0.0850646,"icant improvement over a full-fledged supervised baseline for the machine translation and sentiment classification tasks. 1 Introduction Product and/or service reviews available in the e-commerce portals are predominantly in the English language, and hence a large number of population can not understand these. Machine Translation (MT) system can play a crucial role in bridging this gap by translating the usergenerated contents, and directly displaying them, or making these available for the downstream Natural Language Processing (NLP) tasks e.g. sentiment classification1 (Araújo et al., 2020; Barnes et al., 2016; Mohammad et al., 2016; Kanayama et al., 2004). However, numerous studies (Poncelas et al., 2020a; Afli et al., 2017; Mohammad et al., 2016; Sennrich et al., 2016a) have found a significant loss of sentiment during the automatic translation of the source text. The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin. For example, a noisy review contains several peculiarities and informal structures, such as shortening of words (e.g. “awesome” as “awsm”), acronyms (e.g. “Oh My God” as “OMG”), phone"
2021.mtsummit-research.13,D19-5617,0,0.0187094,"gical properties, i.e. English to Hindi (henceforth, En–Hi) and French–English (henceforth, Fr–En). We use the following supervised datasets for the pre-training and validation of LL-based NMT in En–Hi and Fr–En tasks, (i). For En–Hi task, we use a newly created domain-specific parallel corpus (see section 4.1) whose sources were selected from an e-commerce site. This corpus is released as a part of this work. Statistics of the dataset is shown in Table 1 : (A), row(ii). (ii). For Fr–En task, we concatenate a recently released domain-specific parallel corpus, namely Foursquare (4SQ) corpus 7 (Berard et al., 2019) with first 60K sentences from OpenSubtitles 8 corpus to simulate a low-resource setting. The basic statistics of this dataset are shown in Table 1 : (A), row(i). For RL fine-tuning of the LL-based NMT(s), we use the corresponding RL datasets from Table 1: (B). In each task, the RL trainset sentences are a subset of human translated sentences drawn from the supervised training samples and additionally annotated with respect to sentiment. For En–Hi task, these sentences are randomly sampled from the supervised training corpus (c.f. Table 1: (A), row(ii)), and for Fr–En we use 4SQ-HT dataset (c."
2021.mtsummit-research.13,E14-1064,0,0.0104734,"translation-based solution for cross-lingual sentiment classification is successfully leveraged in the literature (Wu et al., 2021; Tebbifakhr et al., 2020; Araújo et al., 2020; Poncelas et al., 2020b; Fei and Li, 2020; Tebbifakhr et al., 2019; Akhtar et al., 2018; Barnes et al., 2016; Balahur and Turchi, 2012; Kanayama et al., 2004) which suggest an inspiring use-case of the MT system, and brings motivation for this piece of work. Given the context of this work, we look at the pieces of works that address the preservation of sentiment in the automatic translation. In one of the early works, Chen and Zhu (2014) used a lexicon-based consistency approach to design a list of sentiment-based features and used it to rank the candidates of t-table in a Phrase based MT system. Lohar et al. (2017, 2018) prepared the positive, negative and neutral sentiment-specific translation systems to ensure the cross-lingual sentiment consistency. Recently, Tebbifakhr et al. (2019) proposed Machine-Oriented (MO) Reinforce, a policybased method to pursue a machine-oriented objective2 in a sentiment classification task unlike the traditional human-oriented objective 3 . It gives a new perspective for a use-case of the MT"
2021.mtsummit-research.13,N19-1423,0,0.0106692,"predicted sentiment distribution (e.g. [1, 0, 0] and [0.2, 0.1, 0.7] in Figure 1 evaluates to scalar value 0.2) taken from the softmax layer of the target language classifier to ensure sentiment preservation, also referred 6 Although shown like this, it only means true reward corresponding to a given source sentence and the corresponding sampled action, not as a function. Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 154 as R2 . To simulate the target language classifier, we fine-tune the pre-trained BERT model (Devlin et al., 2019). The tuned classifier (preparation steps discussed in Section 4.1) is used to obtain the reward R2 as in Equation (5). R2 = P(s)gold • P(tˆ)bert (5) and, (iii). Harmonic mean of (i) and (ii) as a reward, also referred to as R3 to ensure the preservation of both sentiment and semantic during the translation, as in Equation (6). R3 = (1 + β 2 ) (2 · R1 · R2 ) (β 2 · R1 ) + R2 (6) where β is the harmonic weight which is set to 0.5. 3.3 Curriculum Construction The core of CL is (i). to design an evaluation metric for difficulty, and (ii). to provide the model with easy samples first before the ha"
2021.mtsummit-research.13,2020.acl-main.510,0,0.0348418,"ntent; and (b). the source sentiment during translation. (iii). Additionally, we utilize the idea of CL during the RL fine-tuning of the pre-trained model and try to learn from easy to hard data, where hard corresponds to the instances with lower harmonic reward. To the best of our knowledge, this is the first work in NMT that studies CL in the ambit of RL fine-tuning. 2 Related Work The use of translation-based solution for cross-lingual sentiment classification is successfully leveraged in the literature (Wu et al., 2021; Tebbifakhr et al., 2020; Araújo et al., 2020; Poncelas et al., 2020b; Fei and Li, 2020; Tebbifakhr et al., 2019; Akhtar et al., 2018; Barnes et al., 2016; Balahur and Turchi, 2012; Kanayama et al., 2004) which suggest an inspiring use-case of the MT system, and brings motivation for this piece of work. Given the context of this work, we look at the pieces of works that address the preservation of sentiment in the automatic translation. In one of the early works, Chen and Zhu (2014) used a lexicon-based consistency approach to design a list of sentiment-based features and used it to rank the candidates of t-table in a Phrase based MT system. Lohar et al. (2017, 2018) prepared th"
2021.mtsummit-research.13,C04-1071,0,0.240714,"ised baseline for the machine translation and sentiment classification tasks. 1 Introduction Product and/or service reviews available in the e-commerce portals are predominantly in the English language, and hence a large number of population can not understand these. Machine Translation (MT) system can play a crucial role in bridging this gap by translating the usergenerated contents, and directly displaying them, or making these available for the downstream Natural Language Processing (NLP) tasks e.g. sentiment classification1 (Araújo et al., 2020; Barnes et al., 2016; Mohammad et al., 2016; Kanayama et al., 2004). However, numerous studies (Poncelas et al., 2020a; Afli et al., 2017; Mohammad et al., 2016; Sennrich et al., 2016a) have found a significant loss of sentiment during the automatic translation of the source text. The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin. For example, a noisy review contains several peculiarities and informal structures, such as shortening of words (e.g. “awesome” as “awsm”), acronyms (e.g. “Oh My God” as “OMG”), phonetic substitution of numbers (e.g. “before” as “"
2021.mtsummit-research.13,W04-3250,0,0.307273,"and row (vi)., we can see that in both Fr–En and En–Hi task, merely learning in an easy-to-hard fashion brings the highest improvement in BLEU scores over the supervised baselines, i.e. +0.24, +0.31 point for the Fr–En and En–Hi tasks, respectively. F1 scores are also improved by +0.07 and +0.08 point, respectively. All these improvement are statistically significant12 . Furthermore, we also observe 11 Please note unlike Tebbifakhr et al. (2019) “out-of-domain” MT-adaption approach ours’ LL-based MT is trained using in-domain data. 12 To test significance, we use bootstrap resampling method (Koehn, 2004) for BLEU and student’s t-test for sentiment Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 159 that the CL-based fine-tuning observes a faster convergence over the vanilla approach. 5.2 Error Analysis for English–Hindi task Although our proposed method outperforms the LL-based baseline in the sentiment classification task, we also observe several failure cases. To investigate this, we observe the sentimentconflicting cases, i.e. selected those samples from ours’ model where there is an observed disagreement betwe"
2021.mtsummit-research.13,D15-1166,0,0.0509441,"fine-tuning frameworks, i.e. (1). REINFORCE, and (2). Machine-Oriented Reinforce. Additionally, we also conduct the ablation study to better analyse the utility of harmonic reward in the task through our vanilla AC method ac as follows: (3). MTac bert : AC fine-tuning with sentiment reward only; (4). MTbleu : AC finetuning with content reward only; (5). MTac : AC fine-tuning with both the rewards. Finally, har for brevity we choose the best performing AC-reward model for the proposed curriculum-based learning. 4.2 Hyper-parameters Setting In all our experiments, we use an NMT system based on Luong et al. (2015), using a single layer bi-directional RNN for the encoder. All the encoder-decoder parameters are uniformly initialized in the range of [-0.1,0.1]. The sizes of embedding and hidden layers are set to 256 and 512, respectively. The Adam optimizer (Abdalla and Hirst, 2017) with β1 = 0.9, β2 = 0.99 is used and the gradient vector is clipped to magnitude 5. We set the dropout to 0.2 and use the input feeding with learning rate (lr) and batch size (bs) set to 1e − 3 and 64. We first perform supervised pre-training of the NMT using the parallel corpora from Table 1: (A), and select the best model pa"
2021.mtsummit-research.13,D17-1153,0,0.140151,"one w.r.t the highest rewarding sample. Although they achieved a performance boost in the sentiment classification task, they had to greatly compromise with the translation quality. In contrast to Tebbifakhr et al. (2019), we focus on performing a task-specific customisation of a pre-trained MT system via a harmonic reward based deep reinforcement framework that uses an AC method in conjunction with the CL. The adapted NMT system, thus obtained, is expected to produce a more accurate (high-quality) translation as well as improve the performance of a sentiment analyser. Bahdanau et al. (2017); Nguyen et al. (2017), unlike us, used the popular AC method, and focused only on preserving the semantics (translation quality) of a text. Additionally, we develop a CL based strategy to guide the training. Recently, Zhao et al. (2020) also studied AC method in the context of NMT. However, they used this method to learn the curriculum for re-selecting influential data samples from the existing training set that can further improve the performance (translation quality) of a pre-trained NMT system. 3 Methodology Firstly, we perform the pre-training of a NMT model until the convergence using the standard log-likelih"
2021.mtsummit-research.13,2020.amta-research.7,0,0.379665,"ment classification tasks. 1 Introduction Product and/or service reviews available in the e-commerce portals are predominantly in the English language, and hence a large number of population can not understand these. Machine Translation (MT) system can play a crucial role in bridging this gap by translating the usergenerated contents, and directly displaying them, or making these available for the downstream Natural Language Processing (NLP) tasks e.g. sentiment classification1 (Araújo et al., 2020; Barnes et al., 2016; Mohammad et al., 2016; Kanayama et al., 2004). However, numerous studies (Poncelas et al., 2020a; Afli et al., 2017; Mohammad et al., 2016; Sennrich et al., 2016a) have found a significant loss of sentiment during the automatic translation of the source text. The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin. For example, a noisy review contains several peculiarities and informal structures, such as shortening of words (e.g. “awesome” as “awsm”), acronyms (e.g. “Oh My God” as “OMG”), phonetic substitution of numbers (e.g. “before” as “b4”), emphasis on characters to define extremity o"
2021.mtsummit-research.13,N16-1005,0,0.164807,"eviews available in the e-commerce portals are predominantly in the English language, and hence a large number of population can not understand these. Machine Translation (MT) system can play a crucial role in bridging this gap by translating the usergenerated contents, and directly displaying them, or making these available for the downstream Natural Language Processing (NLP) tasks e.g. sentiment classification1 (Araújo et al., 2020; Barnes et al., 2016; Mohammad et al., 2016; Kanayama et al., 2004). However, numerous studies (Poncelas et al., 2020a; Afli et al., 2017; Mohammad et al., 2016; Sennrich et al., 2016a) have found a significant loss of sentiment during the automatic translation of the source text. The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin. For example, a noisy review contains several peculiarities and informal structures, such as shortening of words (e.g. “awesome” as “awsm”), acronyms (e.g. “Oh My God” as “OMG”), phonetic substitution of numbers (e.g. “before” as “b4”), emphasis on characters to define extremity of the emotion (e.g. “good” as “gooooooood”), spelling mistakes, et"
2021.mtsummit-research.13,P16-1162,0,0.116625,"eviews available in the e-commerce portals are predominantly in the English language, and hence a large number of population can not understand these. Machine Translation (MT) system can play a crucial role in bridging this gap by translating the usergenerated contents, and directly displaying them, or making these available for the downstream Natural Language Processing (NLP) tasks e.g. sentiment classification1 (Araújo et al., 2020; Barnes et al., 2016; Mohammad et al., 2016; Kanayama et al., 2004). However, numerous studies (Poncelas et al., 2020a; Afli et al., 2017; Mohammad et al., 2016; Sennrich et al., 2016a) have found a significant loss of sentiment during the automatic translation of the source text. The susceptibility to sentiment loss aggravates when the MT system is translating a noisy review that lacks a legitimate language structure at the origin. For example, a noisy review contains several peculiarities and informal structures, such as shortening of words (e.g. “awesome” as “awsm”), acronyms (e.g. “Oh My God” as “OMG”), phonetic substitution of numbers (e.g. “before” as “b4”), emphasis on characters to define extremity of the emotion (e.g. “good” as “gooooooood”), spelling mistakes, et"
2021.mtsummit-research.13,D19-1140,0,0.0869425,"or-critic (AC) reinforcement learning framework in the ambit of curriculum learning (CL) to alleviate the issue of sentiment loss while improving the quality of translation in a cross-lingual setup. The idea of actor-critic is to have two neural networks, viz. (i). an actor (i.e. a pre-trained NMT) that takes an action (policy-based), and (ii). a critic that observes how good the action taken is and provides feedback (value-based). This feedback acts as a guiding signal to train the actor. Further, to better utilize the data, we also integrate curriculum learning into our framework. Recently, Tebbifakhr et al. (2019) demonstrated that an MT system (actor) can be customised to produce a controlled translation that essentially improves the performance of a cross-lingual (binary) sentiment classifier. They achieved this task-specific customisation of a “generic-MT” system via a policy-based method that optimizes a task-specific metric, i.e. F 1 score (see Section 2). However, this often miserably fails to encode the semantics of the source sentence. Recent studies (Xu et al., 2018) demonstrated that the non-opinionated semantic content improves the quality of a sentiment classifier. Accordingly, the transfer"
2021.mtsummit-research.13,2020.eamt-1.25,0,0.113503,", Volume 1: MT Research Track Page 151 the non-opinionated semantic content; and (b). the source sentiment during translation. (iii). Additionally, we utilize the idea of CL during the RL fine-tuning of the pre-trained model and try to learn from easy to hard data, where hard corresponds to the instances with lower harmonic reward. To the best of our knowledge, this is the first work in NMT that studies CL in the ambit of RL fine-tuning. 2 Related Work The use of translation-based solution for cross-lingual sentiment classification is successfully leveraged in the literature (Wu et al., 2021; Tebbifakhr et al., 2020; Araújo et al., 2020; Poncelas et al., 2020b; Fei and Li, 2020; Tebbifakhr et al., 2019; Akhtar et al., 2018; Barnes et al., 2016; Balahur and Turchi, 2012; Kanayama et al., 2004) which suggest an inspiring use-case of the MT system, and brings motivation for this piece of work. Given the context of this work, we look at the pieces of works that address the preservation of sentiment in the automatic translation. In one of the early works, Chen and Zhu (2014) used a lexicon-based consistency approach to design a list of sentiment-based features and used it to rank the candidates of t-table in"
2021.mtsummit-research.13,D18-1397,0,0.0229707,"s of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 158 5 Results and Analysis We first present the results of fine-tuning the pre-trained MT through different RL-based methods, i.e. (i). REINFORCE (ii). MO Reinforce, and (iii). vanilla AC (ours) in Section 5.1. Further, to better analyse the utility of harmonic reward (R3 ) in sentiment and content preservation task over the previously studied rewards (i.e. SBLEU: R1 or BERT: R2 ) in the context of NMT (Tebbifakhr et al., 2020, 2019; Nguyen et al., 2017; Bahdanau et al., 2017; Wu et al., 2018; Ranzato et al., 2016), we additionally present the fine-tuning results of the vanilla AC method with the following two types of rewards: (i). only content, i.e. R1 and (ii). only sentiment, i.e. R2 as a reward. We choose the best performing reward model (i.e. R3 ) among the AC-based NMT(s). At last, we discuss the results in the context of our curriculum-based AC framework. To evaluate the translation quality we record the BLEU score of the RL testset when translated from the relevant models. To validate our claim that the translations obtained by our proposed MT system can further improve t"
2021.mtsummit-research.13,P18-1090,0,0.0131158,"rain the actor. Further, to better utilize the data, we also integrate curriculum learning into our framework. Recently, Tebbifakhr et al. (2019) demonstrated that an MT system (actor) can be customised to produce a controlled translation that essentially improves the performance of a cross-lingual (binary) sentiment classifier. They achieved this task-specific customisation of a “generic-MT” system via a policy-based method that optimizes a task-specific metric, i.e. F 1 score (see Section 2). However, this often miserably fails to encode the semantics of the source sentence. Recent studies (Xu et al., 2018) demonstrated that the non-opinionated semantic content improves the quality of a sentiment classifier. Accordingly, the transfer of such information from the source to the target can be pivotal for the quality of the sentiment classifier in a cross-lingual setup. Towards this, we investigate the optimization of a harmonic-score-based reward function in our proposed RL-based framework that ensures to preserve both sentiment and semantics. This function operates by taking a weighted harmonic mean of two rewards: (i). content preservation score measured through Sentence-level BLEU or SBLEU; and"
2021.mtsummit-research.2,P10-1088,0,0.619744,"learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, interactive paradigm of SMT was introduced in Gonz´alez-Rubio et al. (2012); Gonz´alez-Rubio and Casacuberta (2014). Later, the NMT became more prominent and efficient in the interactive paradigm of MT (Knowles and Koehn, 2016; Peris et al., 2017). Peris and Casacuberta (2018) explored the application of active learning and IMT on the NMT model. They performed the experiments over the attention based encoder-decoder NMT model (Bah"
2021.mtsummit-research.2,E14-1042,0,0.264435,"ments using this proposed INMT model. 2 Related Work In a case, where an MT model is not providing high quality translation due to low resource or out-of-domain scenarios, it could be beneficial to update the model with new samples while preserving the previous knowledge too. There has been some works which deal with the large input data streams but generally adopt the incremental learning approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and"
2021.mtsummit-research.2,E12-1025,0,0.867306,"Missing"
2021.mtsummit-research.2,N09-1047,0,0.53437,"17) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, interactive paradigm of SMT was introduced in Gonz´alez-Rubio et al. (2012); Gonz´alez-Rubio and Casacuberta (2014). Later, the NMT became more prominent and efficient in the interactive paradigm of MT (Knowles and Koehn, 2016; Peris et al., 2017). Peris and Casacuberta (2018) explored the application of active learning and IMT on the NMT model. They performed the experiments over the attention"
2021.mtsummit-research.2,P19-3020,0,0.178652,"Missing"
2021.mtsummit-research.2,2020.amta-research.9,0,0.319239,"ation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments were based on the Transformer NMT model Vaswani et al. (2017). We used 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up steps. We used the BPE (Sennrich et al., 2016) with a vocabulary size of 40K. Models were trained with OpenNMT toolkit4 (Klein et al., 2020) with batch size of 2,048 tokens till convergence and checkpoints were created after every 10,000 steps. During inference, beam size is set to 5. We measured BLEU (calculated with multi-bleu.pl script) (Papineni et al., 2002) of the trained models on the test sets. 7 Results and Analysis We evaluate the impact of the proposed sampling techniques for active learning in NMT in two different ways. Firstly, we test whether the proposed techniques help the NMT model to improve its translation performance in terms of the BLEU score. Secondly, in order to see whether the proposed techniques are able"
2021.mtsummit-research.2,2016.amta-researchers.9,0,0.620653,"in automatic translation workflows by employing an iterative collaborative strategy with its two most important Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 10 Figure 1: A pipeline showing the flow of data through sampling module, model updation through active learning. components, the human agent and the MT engine. As of today, NMT models (Bahdanau et al., 2015; Vaswani et al., 2017) represent state-of-the-art in MT research. This has led researchers to test interactive-predictive protocol on NMT too. Papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003). So use of interactive NMT (INMT) for output sample correction can significantly reduce the overall translation time and active learning strategy can use human corrected samples for adapting the underlying NMT model so that in future, the model does not repeat previous errors and improves the translation quality. The contributions of our current work are stated as follows: • We propose term based (NEC) and quality based (QE and Sim) sampling techniques tha"
2021.mtsummit-research.2,W04-3250,0,0.203057,"Missing"
2021.mtsummit-research.2,2005.mtsummit-papers.11,0,0.309802,"-th target word. Note that, 1 the fraction |x| is equivalent to the mean of the attention weights of the word yi . Finally, The kurtosis values for all the target words are used to obtain the attention distraction score. 5 Dataset We carried out experiments on three language pairs using three benchmark datasets. Table 2 shows the statistics of training, development and test sets used for our experiments. In order to measure performance of the proposed sampling techniques, we use different domain datasets for training and testing. For German-English and Spanish-English, we use Europarl corpus (Koehn, 2005) for training and News-Commentary (NC) corpus for testing. This gives us a clear indication whether the translation models trained over Europarl corpus are able to adapt over the sampled examples from NC corpus using active learning. Similarly, for English-Hindi translation, we use the IITB corpus (Kunchukuttan et al., 2018) for training which is a combination of sentences from government sites, ted talks, administration books etc. As for evaluation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments w"
2021.mtsummit-research.2,N03-1017,0,0.210173,"al Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 10 Figure 1: A pipeline showing the flow of data through sampling module, model updation through active learning. components, the human agent and the MT engine. As of today, NMT models (Bahdanau et al., 2015; Vaswani et al., 2017) represent state-of-the-art in MT research. This has led researchers to test interactive-predictive protocol on NMT too. Papers (Knowles and Koehn, 2016; Peris et al., 2017) that pursued this line of research suggest that NMT is superior than phrase-based statistical MT (Koehn et al., 2003). So use of interactive NMT (INMT) for output sample correction can significantly reduce the overall translation time and active learning strategy can use human corrected samples for adapting the underlying NMT model so that in future, the model does not repeat previous errors and improves the translation quality. The contributions of our current work are stated as follows: • We propose term based (NEC) and quality based (QE and Sim) sampling techniques that provide us with the ideal source samples which are first post-edited using interactive NMT (INMT) and then used to update the Transformer"
2021.mtsummit-research.2,L18-1548,1,0.831213,". Table 2 shows the statistics of training, development and test sets used for our experiments. In order to measure performance of the proposed sampling techniques, we use different domain datasets for training and testing. For German-English and Spanish-English, we use Europarl corpus (Koehn, 2005) for training and News-Commentary (NC) corpus for testing. This gives us a clear indication whether the translation models trained over Europarl corpus are able to adapt over the sampled examples from NC corpus using active learning. Similarly, for English-Hindi translation, we use the IITB corpus (Kunchukuttan et al., 2018) for training which is a combination of sentences from government sites, ted talks, administration books etc. As for evaluation, we use the ILCI corpus (Jha, 2010) which is a combination of sentences from the health and tourism domain. 6 Experimental Setup Our experiments were based on the Transformer NMT model Vaswani et al. (2017). We used 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up"
2021.mtsummit-research.2,N10-1062,0,0.430942,"n terms of token replacements using this proposed INMT model. 2 Related Work In a case, where an MT model is not providing high quality translation due to low resource or out-of-domain scenarios, it could be beneficial to update the model with new samples while preserving the previous knowledge too. There has been some works which deal with the large input data streams but generally adopt the incremental learning approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model le"
2021.mtsummit-research.2,2020.eamt-1.11,0,0.181939,"riteria. For every input sentence, this tool takes two inputs which are source sentence and translation of the source sentence generated by the initial NMT model and gives us the estimated HTER score. For a test sentence Si in S where (1 ≤ i ≤ |S|) (|S |= number of sentences in S), quality estimation (QE) pre-trained model takes Si and its generated translation Ti , and returns the corresponding HTER score HT ERi . 4.3 Sentence Similarity (SS) Here, we calculate the similarity between the source sentence and its round trip translation (source-to-target and again target-to-source translation) (Moon et al., 2020). We explore the similarity based sampling criteria since the quality of the round trip translation depends on the two intermediate translations i.e. forward translation (source-to-target) and back-translation (target-to-source). In case of a weak NMT model (i.e. MT system that does not generate high quality translations; e.g. say in low resource scenario or translating out-of-domain data), it is unlikely that a generated round-trip translation would be closer to the source sentence. As for the RTT setup, we had to train forward- and back-translation models. In this case, a low similarity scor"
2021.mtsummit-research.2,W04-3225,0,0.804296,"approaches (e.g. updating the model as the labelled data become available) rather than the active learning approach (where labelled data stream is not guaranteed). In the literature (Levenberg et al., 2010; Denkowski et al., 2014), authors used incremental learning to update the translation model but these were with respect to the statistical machine translation (SMT) model. Turchi et al. (2017) applied incremental learning over the NMT model where they used the human post-edited data to update the initially trained models which make it very costly and time consuming due to human-edited data. Nepveu et al. (2004); Ortiz-Mart´ınez (2016) used an interactive paradigm for updating the SMT model on the iteratively corrected outputs. As for active learning, it has also been well adopted for model learning. The unbounded and unlabelled large data streams is well suited to the objective of active learning (Olsson, 2009; Settles, 2009). This unbounded data stream scenario was explored by Haffari et al. (2009); Bloodgood and Callison-Burch (2010), where a pool of data was edited and the SMT model was updated using this data. Gonz´alez-Rubio et al. (2011) used the stream data to update the SMT model. Further, i"
2021.mtsummit-research.2,J16-1004,0,0.692805,"Missing"
2021.mtsummit-research.2,P02-1040,0,0.109148,"6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes were set to 512, dropout rate was set to 0.1. Feed-forward layer consists of 2,048 cells. Adam optimizer (Kingma and Ba, 2015) was used for training with 8,000 warm up steps. We used the BPE (Sennrich et al., 2016) with a vocabulary size of 40K. Models were trained with OpenNMT toolkit4 (Klein et al., 2020) with batch size of 2,048 tokens till convergence and checkpoints were created after every 10,000 steps. During inference, beam size is set to 5. We measured BLEU (calculated with multi-bleu.pl script) (Papineni et al., 2002) of the trained models on the test sets. 7 Results and Analysis We evaluate the impact of the proposed sampling techniques for active learning in NMT in two different ways. Firstly, we test whether the proposed techniques help the NMT model to improve its translation performance in terms of the BLEU score. Secondly, in order to see whether the proposed techniques are able to reduce the human efforts (number of token correction required) in correcting the hypothesis, we compare the performance of the proposed 4 https://opennmt.net/ Proceedings of the 18th Biennial Machine Translation Summit Vir"
2021.mtsummit-research.2,2021.mtsummit-research.2,1,0.0530913,"Missing"
2021.mtsummit-research.2,D19-1410,0,0.0225896,"e may be different from the original source sentence but semantically similar to it, which is not captured by surface level metrics such as BLEU. In fact, we need information about the semantics of both source and back translation. ‘Similarity based on sentence embedding’ (Simemb ) as the name itself suggests, this sampling technique uses a cosine similarity measure based on sentence embeddings. For every input sentence, two embeddings are generated: 1) embedding of the source sentence and 2) embedding of the RTTed sentence of the source sentence. These embeddings are generated using S-BERT 1 Reimers and Gurevych (2019). Sentences having the least similarity scores in the block are sampled and supervised by the user. 4.3.2 Similarity based on Edit distance between sentences (Simf uzzy ) This similarity is a surface level similarity method and it does not take into account the semantics of the source and back translated sentences. In this sampling technique the similarity measure/score is based on the ‘levenshtein-distance’ between the source sentence and the round-trip translation of the source sentence. For every test sentence the similarity score (Simf uzzy ) between the sentence and round-trip translation"
2021.mtsummit-research.2,P16-1162,0,0.474635,"een the source sentence and the round-trip translation of the source sentence. For every test sentence the similarity score (Simf uzzy ) between the sentence and round-trip translation is calculated using ‘fuzzywuzzy’ toolkit2 which is based on the levenshtein-distance and generates a score between 0-100 (0 and 100 are the lowest and highest similarity level). The sentences having the least score in the block are considered for supervision. 4.4 Named Entity Counting (NEC) The NMT model suffers with the vocabulary restriction problem due to the limitation over the decoder side vocabulary size (Sennrich et al., 2016). Named entities (NEs) are open vocabularies and it is not possible for the NMT model to have all the NEs in the decoder vocabulary. Therefore, we considered presence of NEs as one of the sampling criteria. In other words, we took inability of the NMT model to translate the NEs perfectly into account for sampling. We count the NE tokens in each source sample of the incoming inference data and the sentences having the most number of NE tokens in the block are considered as “difficult to translate” by the NMT model, and hence filtered for supervision. We use Spacy3 named entity recognizer (NER)"
2021.mtsummit-research.20,D19-5617,0,0.0145381,") showed that small noise in the input text can reduce the quality of translation. To improve the robustness of the translation model they introduced synthetic errors like character swapping, deletion and insertion in the corpus. Vaibhav et al. (2019) also inserted synthetic noises and back-translated noise in the original corpus. Apart from the spelling distortion, to make the model immune to the grammatical errors, Anastasopoulos et al. (2019) augmented training data with the grammatical errors. They focused on articles, prepositions, subject-verb agreements etc. Considering the challenges, Berard et al. (2019) analyzed the performance of NMT model over a small French-English corpus of restaurant reviews. Unlike this, we do not inject any random noise, rather we introduce an attention guided noise augmentation (AttnNoise) technique to insert the synthetic noise at the source (English) side. To address the second challenge related to the availability of training data, we make use of the data augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model. Fadaee et al. (2017) replaced the common words by rare words to provide better evide"
2021.mtsummit-research.20,W15-3001,0,0.0110752,"original parallel corpus for final source–to–target training. 6 Experiment Setup Our translation model is based on the Transformer architecture Vaswani et al. (2017). We use the Sockeye toolkit9 Hieber et al. (2018) for our experiments. Table 2 gives the size of the training samples for different systems. We also experiment our proposed method on the IIT Bombay English-Hindi parallel corpus Kunchukuttan et al. (2018). To perform experiments for the English–to–French translation, we use a part (for true resource-poor setting) of the UN-corpus Ziemski et al. (2016) for training and newstest2015 Bojar et al. (2015) as the test set. We also perform experiment for English-German translation and test over the IWSLT 2017 testset10 . The tokens of the training, test and validation sets are segmented into subword units Sennrich et al. (2016) by applying 4,000 BPE merge operations at the source and target sides. Our training set-up details are given below: No. of layers at the encoder and decoder sides: 6 each; 8-head attention; Hidden layer size: 512; Embedding vector size: 512; Learning rate: 0.0002; Minimum batch size: 4800 tokens; early stopping is used to terminate the training. 7 Results and Analysis Fro"
2021.mtsummit-research.20,N19-1423,0,0.0122296,"a augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model. Fadaee et al. (2017) replaced the common words by rare words to provide better evidence and contexts for the rare words. Gao et al. (2019) introduced a soft contextual augmentation method where a word’s embedding is replaced by a weighted average of its similar words. Kobayashi (2018) used a bi-directional language model to predict the replacement by using the sentence context. Wu et al. (2019) used the BERT (Bidirectional Encoder Representations from Transformers) Devlin et al. (2019) model to predict the randomly masked word. Inspired by Wu et al. (2019), we mask the noun and adjective words in the source sentence and predict the appropriate nouns and adjectives as substitutes based on the sentence context. We introduce a phrase replacement based data augmentation technique (PhrRep) to replace the whole syntactic noun phrase (multiple words in a single attempt) with other diverse but contextually similar noun phrases. 3 Parallel Corpus Creation In this section we describe the steps followed for parallel corpus creation and the necessary statistics. 3.1 Crawling reviews an"
2021.mtsummit-research.20,N19-1326,0,0.0553573,"Missing"
2021.mtsummit-research.20,P17-2090,0,0.38882,"tions, subject-verb agreements etc. Considering the challenges, Berard et al. (2019) analyzed the performance of NMT model over a small French-English corpus of restaurant reviews. Unlike this, we do not inject any random noise, rather we introduce an attention guided noise augmentation (AttnNoise) technique to insert the synthetic noise at the source (English) side. To address the second challenge related to the availability of training data, we make use of the data augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model. Fadaee et al. (2017) replaced the common words by rare words to provide better evidence and contexts for the rare words. Gao et al. (2019) introduced a soft contextual augmentation method where a word’s embedding is replaced by a weighted average of its similar words. Kobayashi (2018) used a bi-directional language model to predict the replacement by using the sentence context. Wu et al. (2019) used the BERT (Bidirectional Encoder Representations from Transformers) Devlin et al. (2019) model to predict the randomly masked word. Inspired by Wu et al. (2019), we mask the noun and adjective words in the source sente"
2021.mtsummit-research.20,P19-1555,0,0.0710652,"l over a small French-English corpus of restaurant reviews. Unlike this, we do not inject any random noise, rather we introduce an attention guided noise augmentation (AttnNoise) technique to insert the synthetic noise at the source (English) side. To address the second challenge related to the availability of training data, we make use of the data augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model. Fadaee et al. (2017) replaced the common words by rare words to provide better evidence and contexts for the rare words. Gao et al. (2019) introduced a soft contextual augmentation method where a word’s embedding is replaced by a weighted average of its similar words. Kobayashi (2018) used a bi-directional language model to predict the replacement by using the sentence context. Wu et al. (2019) used the BERT (Bidirectional Encoder Representations from Transformers) Devlin et al. (2019) model to predict the randomly masked word. Inspired by Wu et al. (2019), we mask the noun and adjective words in the source sentence and predict the appropriate nouns and adjectives as substitutes based on the sentence context. We introduce a phra"
2021.mtsummit-research.20,W18-1820,0,0.0552014,"Missing"
2021.mtsummit-research.20,D19-5506,0,0.0199248,"parallel corpus; and (ii). noisy sentences in product and/or service reviews. Machine translation with noisy text is, itself, a very challenging task. The typical noises that pose challenges for machine translation include improper grammatical structures, misspellings, punctuation, emojis etc (c.f. Section 3.1) (Michel and Neubig, 2018). In the literature, there are a few works concerning the noise in the text and to increase the robustness of the translation model. Michel and Neubig (2018) presented a noisy dataset and discussed the challenges of noisy contents. Belinkov and Bisk (2018) and Karpukhin et al. (2019) showed that small noise in the input text can reduce the quality of translation. To improve the robustness of the translation model they introduced synthetic errors like character swapping, deletion and insertion in the corpus. Vaibhav et al. (2019) also inserted synthetic noises and back-translated noise in the original corpus. Apart from the spelling distortion, to make the model immune to the grammatical errors, Anastasopoulos et al. (2019) augmented training data with the grammatical errors. They focused on articles, prepositions, subject-verb agreements etc. Considering the challenges, B"
2021.mtsummit-research.20,N18-2072,0,0.022302,"noise augmentation (AttnNoise) technique to insert the synthetic noise at the source (English) side. To address the second challenge related to the availability of training data, we make use of the data augmentation techniques to increase the training samples and noise handling techniques to increase the robustness of the model. Fadaee et al. (2017) replaced the common words by rare words to provide better evidence and contexts for the rare words. Gao et al. (2019) introduced a soft contextual augmentation method where a word’s embedding is replaced by a weighted average of its similar words. Kobayashi (2018) used a bi-directional language model to predict the replacement by using the sentence context. Wu et al. (2019) used the BERT (Bidirectional Encoder Representations from Transformers) Devlin et al. (2019) model to predict the randomly masked word. Inspired by Wu et al. (2019), we mask the noun and adjective words in the source sentence and predict the appropriate nouns and adjectives as substitutes based on the sentence context. We introduce a phrase replacement based data augmentation technique (PhrRep) to replace the whole syntactic noun phrase (multiple words in a single attempt) with othe"
2021.mtsummit-research.20,L18-1548,0,0.241756,"CDA PhrRep There are many offers for this smartphone There are many provides for this smartphone There are many applications/designs/models for this smartphone There are multiple features in my new smartphone Table 3: Samples generated using WDA, CDA and PhrRep approaches. tively. The gold standard corpus, and the parallel corpus created synthetically is made available5 . We also crawl the Hindi sentences and back-translate them into English. We build a Hindi–to–English NMT model to back-translate the crawled Hindi sentences. We use the IIT Bombay Hindi-English general domain parallel corpus Kunchukuttan et al. (2018) to train a Hindi–to–English NMT model, and then fine-tune it over the human post-edited review domain parallel corpus. The fine-tuned Hindi–to–English NMT model is used to back-translate the crawled monolingual Hindi sentences into English. These back-translated (BT) English-Hindi synthetic parallel sentences are augmented with the human post-edited parallel sentences and referred to as ‘Base+BT’, shown in Table 2. 4 Data Augmentation We further enrich the training corpus (in low-resource language) following the data augmentation techniques as discussed below. 4.1 Word Embedding based Data Au"
2021.mtsummit-research.20,W02-0109,0,0.0466176,"t complexity, we choose noun and adjective words as the replacement candidates because: • Hindi is morphologically richer than English. One English verb token may be aligned to more than one Hindi tokens. But nouns and adjectives are most likely to generate only one Hindi token. For example: translation of word ‘started (verb)’ (1 token) can be ‘शुरू कर दया’ ‘shuroo kar diya’ (3 tokens) or ‘शुरू कया’ ‘shuroo kiya’ (2 tokens). Here, we see that for the word ‘started’, more than one translations possible with different token lengths. To select the noun and adjectives for replacement, we use NLTKLoper and Bird (2002) Part-of-Speech (PoS) tagger for the English sentences. A word2vec skip-gram model6 Mikolov et al. (2013) is trained using the WMT14 monolingual English dataset and English sentences from the gold corpus. Now for all the noun and adjective words, we find the most similar words using our trained word2vec model. The words having the cosine similarity more than 0.75 will be considered as the substitutes. A mapping dictionary is created with the triplet consisting of the ‘original English word’, ‘its replacement English word’ and ‘Hindi translation of the replacement word’. Now using the mapping d"
2021.mtsummit-research.20,D18-1050,0,0.0183916,"o make the NMT model robust towards noisy inputs (c.f. Section 5.1). We show that AttnNoise method significantly outperforms the random noise injection (RndNoise) techniques. 2 Related Work There are two main challenges for translating the product reviews, viz. (i). nonavailability of parallel corpus; and (ii). noisy sentences in product and/or service reviews. Machine translation with noisy text is, itself, a very challenging task. The typical noises that pose challenges for machine translation include improper grammatical structures, misspellings, punctuation, emojis etc (c.f. Section 3.1) (Michel and Neubig, 2018). In the literature, there are a few works concerning the noise in the text and to increase the robustness of the translation model. Michel and Neubig (2018) presented a noisy dataset and discussed the challenges of noisy contents. Belinkov and Bisk (2018) and Karpukhin et al. (2019) showed that small noise in the input text can reduce the quality of translation. To improve the robustness of the translation model they introduced synthetic errors like character swapping, deletion and insertion in the corpus. Vaibhav et al. (2019) also inserted synthetic noises and back-translated noise in the o"
2021.mtsummit-research.20,J03-1002,0,0.0142136,"ing the WMT14 monolingual English dataset and English sentences from the gold corpus. Now for all the noun and adjective words, we find the most similar words using our trained word2vec model. The words having the cosine similarity more than 0.75 will be considered as the substitutes. A mapping dictionary is created with the triplet consisting of the ‘original English word’, ‘its replacement English word’ and ‘Hindi translation of the replacement word’. Now using the mapping dictionary, the tokens in the original corpus are replaced. Source-target word alignment information using GIZA++ tool (Och and Ney, 2003) is used to replace the aligned Hindi tokens in the Hindi side. But WDA does not guarantee to replace the original word with a similar context word as shown by an example in Table 3. 5 https://www.iitp.ac.in/~ai-nlp-ml/resources/data/review-corpus.zip 6 https://code.google.com/archive/p/word2vec/ Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 247 Figure 1: Synthetic sample generation using phrase replacement (PhrRep) 4.2 Context Aware Data Augmentation (CDA) Wu et al. (2019) used a BERT based method which predicts"
2021.mtsummit-research.20,D19-1410,0,0.0145244,"ace a noun phrase (NP) with its semantically similar noun phrase (NP). To extract NP from the English sentences, we use the Stanford parser7 and obtain the corresponding constituency trees. To reduce the complexity in alignment mapping and trivial replacements, we filter out very large (&gt;8 tokens) and very short (&lt;3 tokens) NPs. Here, we refer to the replacements of very small NPs as trivial replacements since most likely they are already part of larger NPs, and get replaced when larger NPs are replaced. To find the similarity among phrase embeddings, we use a BERT based sentence-transformer8 Reimers and Gurevych (2019). For an original phrase Poi , its similar phrase Psi is: Psi = Pj , [i = (1,....,n) and j = (1,....,n)] (1) 7 https://nlp.stanford.edu/software/lex-parser.shtml 8 https://github.com/UKPLab/sentence-transformers Proceedings of the 18th Biennial Machine Translation Summit Virtual USA, August 16 - 20, 2021, Volume 1: MT Research Track Page 248 Pj = arg min d(hi , hj ) (2) j n is the number of NPs. h is the hidden representation of the phrases. d represents the Euclidean distance between the two vectors. Equation 2 returns the index j of a phrase having minimum Euclidean distance d with the phras"
2021.mtsummit-research.20,P16-1162,0,0.0427389,"r experiments. Table 2 gives the size of the training samples for different systems. We also experiment our proposed method on the IIT Bombay English-Hindi parallel corpus Kunchukuttan et al. (2018). To perform experiments for the English–to–French translation, we use a part (for true resource-poor setting) of the UN-corpus Ziemski et al. (2016) for training and newstest2015 Bojar et al. (2015) as the test set. We also perform experiment for English-German translation and test over the IWSLT 2017 testset10 . The tokens of the training, test and validation sets are segmented into subword units Sennrich et al. (2016) by applying 4,000 BPE merge operations at the source and target sides. Our training set-up details are given below: No. of layers at the encoder and decoder sides: 6 each; 8-head attention; Hidden layer size: 512; Embedding vector size: 512; Learning rate: 0.0002; Minimum batch size: 4800 tokens; early stopping is used to terminate the training. 7 Results and Analysis From Table 5, we can see significant BLEU score improvement over the baseline using various data and noise augmentation techniques. Using human translated and backtranslated corpus, we train the Base+BT model which yields the BL"
2021.mtsummit-research.20,N19-1190,0,0.3095,"res, misspellings, punctuation, emojis etc (c.f. Section 3.1) (Michel and Neubig, 2018). In the literature, there are a few works concerning the noise in the text and to increase the robustness of the translation model. Michel and Neubig (2018) presented a noisy dataset and discussed the challenges of noisy contents. Belinkov and Bisk (2018) and Karpukhin et al. (2019) showed that small noise in the input text can reduce the quality of translation. To improve the robustness of the translation model they introduced synthetic errors like character swapping, deletion and insertion in the corpus. Vaibhav et al. (2019) also inserted synthetic noises and back-translated noise in the original corpus. Apart from the spelling distortion, to make the model immune to the grammatical errors, Anastasopoulos et al. (2019) augmented training data with the grammatical errors. They focused on articles, prepositions, subject-verb agreements etc. Considering the challenges, Berard et al. (2019) analyzed the performance of NMT model over a small French-English corpus of restaurant reviews. Unlike this, we do not inject any random noise, rather we introduce an attention guided noise augmentation (AttnNoise) technique to in"
2021.mtsummit-research.20,L16-1561,0,0.0135469,"lly, this noisy parallel corpus is augmented to the original parallel corpus for final source–to–target training. 6 Experiment Setup Our translation model is based on the Transformer architecture Vaswani et al. (2017). We use the Sockeye toolkit9 Hieber et al. (2018) for our experiments. Table 2 gives the size of the training samples for different systems. We also experiment our proposed method on the IIT Bombay English-Hindi parallel corpus Kunchukuttan et al. (2018). To perform experiments for the English–to–French translation, we use a part (for true resource-poor setting) of the UN-corpus Ziemski et al. (2016) for training and newstest2015 Bojar et al. (2015) as the test set. We also perform experiment for English-German translation and test over the IWSLT 2017 testset10 . The tokens of the training, test and validation sets are segmented into subword units Sennrich et al. (2016) by applying 4,000 BPE merge operations at the source and target sides. Our training set-up details are given below: No. of layers at the encoder and decoder sides: 6 each; 8-head attention; Hidden layer size: 512; Embedding vector size: 512; Learning rate: 0.0002; Minimum batch size: 4800 tokens; early stopping is used to"
2021.wat-1.18,D17-1105,0,0.0175465,"features as additional inputs to encoder and decoder. (Delbrouck and Dupont, 2017) used attention mechanism on visual inputs for the source hidden states. (Lin et al., 2020) used Dynamic Context-guided Capsule Network (Sabour et al., 2017) (DCCN) for iterative extraction of related visual features. Multimodal Machine Translation (MMT) for English-Hindi has not been well explored yet. (Dutta Chowdhury et al., 2018) used synthetic data for training. Furthermore they used multi-modal, attention-based MMT which incorporate visual features into different parts of both the encoder and the decoder (Calixto and Liu, 2017). (Sanayai Meetei et al., 2019) used a Recurrent Neural Network (RNN) based approach achieving BLEU score of 28.45 on Evaluation set and 12.58 on Challenge set. (Laskar et al., 2020) exploited monolingual data for better translation. Recent works tried to focus on developing unsupervised model for multi-modal NMT. Su et al. (2018) demonstrated an unsupervised method based on the language translation cycle consistency loss conditional on the image. This is done to learn the bidirectional multi-modal translation simultaneously. Moreover, Su et al. (2021) showed that jointly learning text-image i"
2021.wat-1.18,P17-1175,0,0.0491666,"Missing"
2021.wat-1.18,W16-2360,0,0.0155564,"ibution https://www.ethnologue.com/guides/ethnologue200 161 Proceedings of the 8th Workshop on Asian Translation, pages 161–165 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics several state-of-the-art features. The following sections describe our processes in greater details. 2 Related Works There have been many attempts to use information other than the source for better translation. Uni-modal systems include document-level NMT (Wang et al., 2017), sentence-level NMT with contextual information (Gain et al., 2021), etc. Among multimodal systems, (Huang et al., 2016) used an object detection system and extracted local and global image features. Thereafter, they used those image features as additional inputs to encoder and decoder. (Delbrouck and Dupont, 2017) used attention mechanism on visual inputs for the source hidden states. (Lin et al., 2020) used Dynamic Context-guided Capsule Network (Sabour et al., 2017) (DCCN) for iterative extraction of related visual features. Multimodal Machine Translation (MMT) for English-Hindi has not been well explored yet. (Dutta Chowdhury et al., 2018) used synthetic data for training. Furthermore they used multi-modal,"
2021.wat-1.18,P17-4012,0,0.0291776,"help of the image. An example of multimodal dataset is given in Figure 1. 3.2 Pre-processing For text data, we lowercase all the utterances. Then, we jointly learn byte-pair-encoding (Sennrich et al., 2016) combining both source and target with a vocabulary of 10,000. We treat the images by cropping a specified rectangular portions. This operation is used to discard the portions that do not contribute much to the translation performance. After we get those cropped-out images, we use the pre-trained VGG19-bn (Simonyan and Zisserman, 2015) to obtain the image representations. We use OpenNMT-py (Klein et al., 2017) framework to perform this step. 3.3 Training We use OpenNMT-py (Klein et al., 2017) for our NMT systems. We use Bidirectional RNN encoder and doubly attentive RNN decoder (Calixto et al., 2017) for our experiments. We train our system in two ways viz. With pre-training, and Without pre-training.: 1. With pre-training We pre-train one of our models on HindEnCorp dataset. This step does not use any visual features as the dataset used for pre-training is devoid of any visual We use Hindi Visual Genome 1.1 dataset (Parida et al., 2019)(Nakazawa et al., 2020)(Nakazawa et al., 2021) for our experim"
2021.wat-1.18,W18-3405,0,0.0173498,"textual information (Gain et al., 2021), etc. Among multimodal systems, (Huang et al., 2016) used an object detection system and extracted local and global image features. Thereafter, they used those image features as additional inputs to encoder and decoder. (Delbrouck and Dupont, 2017) used attention mechanism on visual inputs for the source hidden states. (Lin et al., 2020) used Dynamic Context-guided Capsule Network (Sabour et al., 2017) (DCCN) for iterative extraction of related visual features. Multimodal Machine Translation (MMT) for English-Hindi has not been well explored yet. (Dutta Chowdhury et al., 2018) used synthetic data for training. Furthermore they used multi-modal, attention-based MMT which incorporate visual features into different parts of both the encoder and the decoder (Calixto and Liu, 2017). (Sanayai Meetei et al., 2019) used a Recurrent Neural Network (RNN) based approach achieving BLEU score of 28.45 on Evaluation set and 12.58 on Challenge set. (Laskar et al., 2020) exploited monolingual data for better translation. Recent works tried to focus on developing unsupervised model for multi-modal NMT. Su et al. (2018) demonstrated an unsupervised method based on the language trans"
2021.wat-1.18,D19-5224,0,0.0199812,"encoder and decoder. (Delbrouck and Dupont, 2017) used attention mechanism on visual inputs for the source hidden states. (Lin et al., 2020) used Dynamic Context-guided Capsule Network (Sabour et al., 2017) (DCCN) for iterative extraction of related visual features. Multimodal Machine Translation (MMT) for English-Hindi has not been well explored yet. (Dutta Chowdhury et al., 2018) used synthetic data for training. Furthermore they used multi-modal, attention-based MMT which incorporate visual features into different parts of both the encoder and the decoder (Calixto and Liu, 2017). (Sanayai Meetei et al., 2019) used a Recurrent Neural Network (RNN) based approach achieving BLEU score of 28.45 on Evaluation set and 12.58 on Challenge set. (Laskar et al., 2020) exploited monolingual data for better translation. Recent works tried to focus on developing unsupervised model for multi-modal NMT. Su et al. (2018) demonstrated an unsupervised method based on the language translation cycle consistency loss conditional on the image. This is done to learn the bidirectional multi-modal translation simultaneously. Moreover, Su et al. (2021) showed that jointly learning text-image interaction instead of modeling"
2021.wat-1.18,P16-1162,0,0.0210544,"modal dataset pairs along with the associated images. Furthermore, we use HindEnCorp dataset for pre-training containing 273K English-Hindi sentence pairs without images. Statistics of the datasets are shown in Table 1. Multimodal dataset consists of an image along with a description of certain rectangular portion of the image. We are given the coordinates of the portion. We aim to translate the description with help of the image. An example of multimodal dataset is given in Figure 1. 3.2 Pre-processing For text data, we lowercase all the utterances. Then, we jointly learn byte-pair-encoding (Sennrich et al., 2016) combining both source and target with a vocabulary of 10,000. We treat the images by cropping a specified rectangular portions. This operation is used to discard the portions that do not contribute much to the translation performance. After we get those cropped-out images, we use the pre-trained VGG19-bn (Simonyan and Zisserman, 2015) to obtain the image representations. We use OpenNMT-py (Klein et al., 2017) framework to perform this step. 3.3 Training We use OpenNMT-py (Klein et al., 2017) for our NMT systems. We use Bidirectional RNN encoder and doubly attentive RNN decoder (Calixto et al."
2021.wat-1.18,D17-1301,0,0.018005,"trics including BLEU, RIBES, AMFM. In subsequent modifications, we aim to develop our models incorporating 1 Equal contribution https://www.ethnologue.com/guides/ethnologue200 161 Proceedings of the 8th Workshop on Asian Translation, pages 161–165 Bangkok, Thailand (online), August 5-6, 2021. ©2021 Association for Computational Linguistics several state-of-the-art features. The following sections describe our processes in greater details. 2 Related Works There have been many attempts to use information other than the source for better translation. Uni-modal systems include document-level NMT (Wang et al., 2017), sentence-level NMT with contextual information (Gain et al., 2021), etc. Among multimodal systems, (Huang et al., 2016) used an object detection system and extracted local and global image features. Thereafter, they used those image features as additional inputs to encoder and decoder. (Delbrouck and Dupont, 2017) used attention mechanism on visual inputs for the source hidden states. (Lin et al., 2020) used Dynamic Context-guided Capsule Network (Sabour et al., 2017) (DCCN) for iterative extraction of related visual features. Multimodal Machine Translation (MMT) for English-Hindi has not be"
2021.wat-1.29,N19-1388,0,0.0761137,"paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively"
2021.wat-1.29,2020.findings-emnlp.223,0,0.140666,"between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indi"
2021.wat-1.29,W19-5308,0,0.0178398,"a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingual 117,757 125,647 156,605 79,433 82,026 120,362 103,876 90,916 91,324 111,749 109,480 Table 1: Language wise training set sizes in terms of number of sentences. Parallel: Parallel corpus"
2021.wat-1.29,2020.coling-tutorials.3,0,0.0360101,"Missing"
2021.wat-1.29,D17-1158,0,0.0179712,"nnada (KN) Malayalam (ML) Marathi (MR) Odia (OR) Punjabi (PA) Tamil (TA) Telugu (TE) English (EN) Improving the quality of NMT models with monolingual data is a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingual 117,757 125,647 156,605"
2021.wat-1.29,N16-1101,0,0.0544852,"3.79 respectively. 1 Introduction In this paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each oth"
2021.wat-1.29,2020.wmt-1.66,0,0.17654,"submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when comp"
2021.wat-1.29,D10-1092,0,0.20704,"as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indic data. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Multilingual Machine Translation enabled the ability to deploy a single model for multiple language pairs without training multiple models. Dong et al. (2015) proposes a multi-task learning framework to translate one source languag"
2021.wat-1.29,P17-4012,0,0.0421798,"idden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as described in Section 3.2. After creating synthetic parallel corpora, baseline models are further trained for another 200,000 steps 8 on combined synthetic and clean parallel corpora (see Table 2). Finally, all checkpoints that are created by the model using the combined corpora are averaged 9 and consi"
2021.wat-1.29,P15-1166,0,0.227793,"of 8.51, 6.25 and 3.79 respectively. 1 Introduction In this paper, we describe our submission to the MultiIndicMT shared task at the 8th Workshop on Asian Translation 1 (WAT 2021) (Nakazawa et al., 2021). The objective of this shared task is to build Machine Translation (MT) models between 10 Indic languages (Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu) and English. We submit two Multilingual Neural Machine Translation models (MNMT): one for XX → EN and one for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are"
2021.wat-1.29,P18-1007,0,0.0229244,"training set after merging clean corpus with synthetic back-translated corpus. XX → EN: Indic-toEnglish model. EN → XX: English-to-Indic model. trained on the Transformer architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as des"
2021.wat-1.29,D18-2012,0,0.0203765,"corpus with synthetic back-translated corpus. XX → EN: Indic-toEnglish model. EN → XX: English-to-Indic model. trained on the Transformer architecture (Vaswani et al., 2017). We use 6 layered Encoder-Decoder stacks with 8 attention heads. Embedding size and hidden sizes are set to 512, dropout rate is set to 0.1. Feed-forward layer consists of 2048 cells. Adam optimizer (Kingma and Ba, 2015) is used for training with 8,000 warm up steps with initial learning rate of 2. We split the training data of baseline models into subwords with the unigram language model (Kudo, 2018) using SentencePiece (Kudo and Richardson, 2018) implementation. We create two subword vocabularies, one for English and one for all romanized Indic data 6 . The size of English subword vocabulary is 60K and of Indic languages is 100K, for both the models. We use OpenNMT toolkit (Klein et al., 2017)7 to train our models with batch size of 2048 tokens. Models are evaluated on development sets after every 10,000 steps and checkpoints are created. The baseline models are trained for 100,000 steps and the last checkpoint is used to create a synthetic corpus with the backtranslation approach as described in Section 3.2. After creating synthetic"
2021.wat-1.29,I17-2050,0,0.0153507,"e XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The model"
2021.wat-1.29,P02-1040,0,0.110046,"e suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works on multilingual NMT and romanized NMT. In section 3, we describe the systems submitted along with preprocessing and romanization of Indic data. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Multilingual Machine Translation enabled the ability to deploy a single model for multiple language pairs without training multiple models. Dong et al. (2015) proposes a multi-task learning framework t"
2021.wat-1.29,Y18-3012,1,0.881082,"tion Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic data romanized. The models are evaluated using the BLEU (Papineni et al., 2002), RIBES (Isozaki et al., 2010) and AMFM (Banchs et al., 2015) metrics. The paper is organized as follows. In section 2, we briefly mention some notable works"
2021.wat-1.29,P16-1009,0,0.206728,"training time. Aharoni et al. (2019) show that multilingual NMT models are capable of handling large number of language pairs. Freitag and Firat (2020) proposes that the use of multi-way alignment information will improve the translation quality of language pairs for which training data is scarce in multilingual settings. 3 This section describes datasets, preprocessing and experimental setup of our models. 3.1 In this work, we follow Johnson et al. (2017) method to train multilingual NMT models. We romanize Indic data and use it to train our models. We also follow back-translation approach (Sennrich et al., 2016) to create synthetic parallel data. We report the results of the models which are trained on combined synthetic and clean parallel corpus. Datasets We use MultiIndicMT parallel corpus 2 consisting of following languages: Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu and English. It contains the parallel corpora for 10 Indic languages which are translated into English. We also use PMI monolingual corpus 3 to generate synthetic data with back-translation (Sennrich et al., 2016) approach. Table 1 shows the data sizes of corpora used in the experiments. Develo"
2021.wat-1.29,D16-1160,0,0.0194489,"ati (GU) Hindi (HI) Kannada (KN) Malayalam (ML) Marathi (MR) Odia (OR) Punjabi (PA) Tamil (TA) Telugu (TE) English (EN) Improving the quality of NMT models with monolingual data is a common approach nowadays, especially in low resource settings. Backtranslation Sennrich et al. (2016) is an effective approach to make use of target monolingual data. In this approach, with the help of existing target-tosource MT system target is translated into source and resulting synthetic parallel corpus is combined with clean corpus and used to train source-totarget NMT system. Multi-task learning framework (Zhang and Zong, 2016; Domhan and Hieber, 2017) is another way to utilize monolingual data to improve the performance of NMT. Recent studies (Du and Way, 2017; Gheini and May, 2019; Briakou and Carpuat, 2019) show that the romanization will improve the performance of NMT system. However these approaches apply romanization at source side only. Amrhein and Sennrich (2020) showed that romanization can be applied on the target side also followed by an additional, learned deromanization step. System Description Parallel 1,341,284 518,015 3,069,725 396,865 1,142,053 621,481 252,160 518,508 1,354,247 457,453 - Monolingua"
2021.wat-1.29,D16-1163,0,0.0198568,"ne for EN → XX (here XX denotes a set of all 10 Indic languages). Multilingual Machine Translation (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017; Aharoni et al., 2019; Freitag and Firat, 2020) has gained ∗ 1 Equal contribution Our Team ID: IITP-MT popularity in recent times due to the ability to train a single model which is capable of translating between multiple language pairs. The main benefit of multilingual model is transfer learning. When a low resource language pair is trained together with a high resource pair, the translation quality of a low resource pair may improve (Zoph et al., 2016; Nguyen and Chiang, 2017). This method of training is more suitable for Indic languages as they are similar to each other (Dabre et al., 2017, 2020) and relatively under-resourced when compared with European languages (Sen et al., 2018). Romanization is the process of converting characters that are written in various scripts into Latin script. Amrhein and Sennrich (2020) showed that in a transfer learning setting, romanization improves the transfer between related languages that use different scripts. We train two MNMT models, which translate between Indic languages and English with all Indic"
C12-1151,M98-1028,0,0.0148658,"Missing"
C12-1151,W09-3539,1,0.927656,"nce and the maintenance costs could be quite steep. Literature shows that most of the works carried out in this direction cover mostly English and European languages. There are also significant amount of works in some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. People speak in 22 different official languages that are derived from almost all the dominant linguistic families. Some works on NER for Indian languages can be found in (Ekbal and Saha, 2011; Ekbal and Bandyopadhyay, 2008; Ekbal et al., 2007; Ekbal and Bandyopadhyay, 2009b, 2007; Ekbal et al., 2008; Li and McCallum, 2004; Patel et al., 2009; Srikanth and Murthy, 2008; Shishtla et al., 2008; Vijayakrishna and Sobha, 2008). However, the works related to NER in Indian languages are still in the nascent stages due to the potential facts such as (Ekbal and Saha, 2011): • Unlike English and most of the European languages, Indian languages lack capitalization information, which plays a very important role in NE identification. • Indian person names are more diverse compared to the other languages and a lot of these words can be found in the dictionary with specific m"
C12-1151,I08-2077,1,0.819781,"quite steep. Literature shows that most of the works carried out in this direction cover mostly English and European languages. There are also significant amount of works in some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. People speak in 22 different official languages that are derived from almost all the dominant linguistic families. Some works on NER for Indian languages can be found in (Ekbal and Saha, 2011; Ekbal and Bandyopadhyay, 2008; Ekbal et al., 2007; Ekbal and Bandyopadhyay, 2009b, 2007; Ekbal et al., 2008; Li and McCallum, 2004; Patel et al., 2009; Srikanth and Murthy, 2008; Shishtla et al., 2008; Vijayakrishna and Sobha, 2008). However, the works related to NER in Indian languages are still in the nascent stages due to the potential facts such as (Ekbal and Saha, 2011): • Unlike English and most of the European languages, Indian languages lack capitalization information, which plays a very important role in NE identification. • Indian person names are more diverse compared to the other languages and a lot of these words can be found in the dictionary with specific meanings. • Indian languages"
C12-1151,M98-1019,0,0.0592266,"ew expressions are constantly being invented. The problem of NER was actually formulated in Message Understanding Conferences (MUCs) [MUC6; MUC7] (Chinchor, 1995, 1998). The issues of correct identification of NEs were specifically addressed and benchmarked by the developers of information extraction system, such as the GATE system (Cunningham, 2002). The existing approaches for NER can be grouped into three main categories, namely rule-based, machine learning based and hybrid approach. Majority of the research focussed on machine learning (ML) approaches (Bikel et al., 1999; Borthwick, 1999; Sekine, 1998; Lafferty et al., 2001a; Yamada et al., 2001) because these are easily trainable, adaptable to different domains and languages as well as their maintenance are also being less expensive. In contrast, rule-based approaches lack the ability of dealing with the problems of robustness and portability. Each new source of text requires significant updates to the rules to maintain optimal performance and the maintenance costs could be quite steep. Literature shows that most of the works carried out in this direction cover mostly English and European languages. There are also significant amount of wo"
C12-1151,I08-5010,0,0.0173918,"mostly English and European languages. There are also significant amount of works in some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. People speak in 22 different official languages that are derived from almost all the dominant linguistic families. Some works on NER for Indian languages can be found in (Ekbal and Saha, 2011; Ekbal and Bandyopadhyay, 2008; Ekbal et al., 2007; Ekbal and Bandyopadhyay, 2009b, 2007; Ekbal et al., 2008; Li and McCallum, 2004; Patel et al., 2009; Srikanth and Murthy, 2008; Shishtla et al., 2008; Vijayakrishna and Sobha, 2008). However, the works related to NER in Indian languages are still in the nascent stages due to the potential facts such as (Ekbal and Saha, 2011): • Unlike English and most of the European languages, Indian languages lack capitalization information, which plays a very important role in NE identification. • Indian person names are more diverse compared to the other languages and a lot of these words can be found in the dictionary with specific meanings. • Indian languages are highly inflectional language providing one of the richest and most challenging sets of l"
C12-1151,I08-5007,0,0.0238583,"ut in this direction cover mostly English and European languages. There are also significant amount of works in some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. People speak in 22 different official languages that are derived from almost all the dominant linguistic families. Some works on NER for Indian languages can be found in (Ekbal and Saha, 2011; Ekbal and Bandyopadhyay, 2008; Ekbal et al., 2007; Ekbal and Bandyopadhyay, 2009b, 2007; Ekbal et al., 2008; Li and McCallum, 2004; Patel et al., 2009; Srikanth and Murthy, 2008; Shishtla et al., 2008; Vijayakrishna and Sobha, 2008). However, the works related to NER in Indian languages are still in the nascent stages due to the potential facts such as (Ekbal and Saha, 2011): • Unlike English and most of the European languages, Indian languages lack capitalization information, which plays a very important role in NE identification. • Indian person names are more diverse compared to the other languages and a lot of these words can be found in the dictionary with specific meanings. • Indian languages are highly inflectional language providing one of the richest and mos"
C12-1151,I08-5009,0,0.0260791,"pean languages. There are also significant amount of works in some of the Asian languages like Chinese, Japanese and Korean. India is a multilingual country with great linguistic and cultural diversities. People speak in 22 different official languages that are derived from almost all the dominant linguistic families. Some works on NER for Indian languages can be found in (Ekbal and Saha, 2011; Ekbal and Bandyopadhyay, 2008; Ekbal et al., 2007; Ekbal and Bandyopadhyay, 2009b, 2007; Ekbal et al., 2008; Li and McCallum, 2004; Patel et al., 2009; Srikanth and Murthy, 2008; Shishtla et al., 2008; Vijayakrishna and Sobha, 2008). However, the works related to NER in Indian languages are still in the nascent stages due to the potential facts such as (Ekbal and Saha, 2011): • Unlike English and most of the European languages, Indian languages lack capitalization information, which plays a very important role in NE identification. • Indian person names are more diverse compared to the other languages and a lot of these words can be found in the dictionary with specific meanings. • Indian languages are highly inflectional language providing one of the richest and most challenging sets of linguistic and statistical featur"
C12-1151,M98-1020,0,\N,Missing
C16-1047,L16-1429,1,0.902536,"Missing"
C16-1047,bakliwal-etal-2012-hindi,0,0.0468051,"ling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark datasets and other resources/tools in Indian languages. In our work, we focus on sentiment analysis in Hindi, the official language of India and the fourth most spoken language all over in the world. We make use of benchmark datasets released as part of a shared task on sentiment analysis in Indian languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have created a dataset for aspe"
C16-1047,C12-2008,1,0.309392,"etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark datasets and other resources/tools in Indian languages. In our work, we focus on sentiment analysis in Hindi, the official language of India and the fourth most spoken language all over in the world. We make use of benchmark datasets released as part of a shared task on sentiment analysis in Indian languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have cre"
C16-1047,C14-1008,0,0.0230143,"2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In this work, we do not use the tra"
C16-1047,P97-1023,0,0.0611487,"Missing"
C16-1047,C04-1200,0,0.0321456,"00,000 tweets per minute2 . At the same time more than 26K user reviews are posted on Yelp, an online user review portal. This tremendous amount of semi-structured data poses a great challenge in its efficient processing for any specific purpose. Sentiment analysis for web generated content e.g. tweets and online reviews, is a cumbersome problem mainly due to its unstructured and noisy nature (e.g. gr8, g8 etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the sc"
C16-1047,D14-1181,0,0.00304412,"dings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In thi"
C16-1047,S13-2053,0,0.0329837,"nute2 . At the same time more than 26K user reviews are posted on Yelp, an online user review portal. This tremendous amount of semi-structured data poses a great challenge in its efficient processing for any specific purpose. Sentiment analysis for web generated content e.g. tweets and online reviews, is a cumbersome problem mainly due to its unstructured and noisy nature (e.g. gr8, g8 etc. for great) and spelling and grammatical mistakes. Considering the challenges as mentioned above, authors have proposed their sentiment analyzers for Twitter data and/or online reviews (Kim and Hovy, 2004; Mohammad et al., 2013a; Gupta et al., 2015). However, most of the works have been done on the resource-rich languages such as English. India is a multi-lingual country with great linguistic and cultural diversities. There are 22 officially spoken languages. However, there have not been enough research works that address sentiment analysis involving Indian languages, except few such as (Balamurali et al., 2012; Bakliwal et al., 2012; Kumar et al., 2015). However, these existing works do not address the fine-grained sentiment analysis at the aspect level. The prime reason behind this is the scarcity of benchmark dat"
C16-1047,S14-2004,0,0.151622,"languages (SAIL) for Twitter (Patra et al., 2015). Recently, we (Akhtar et al., 2016) have created a dataset for aspect based sentiment analysis This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 1 https://www.domo.com/learn/data-never-sleeps-2 2 http://aci.info/2014/07/12/the-data-explosion-in-2014-minute-by-minute-infographic/ License details: http:// 482 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 482–493, Osaka, Japan, December 11-17 2016. (ABSA) (Pontiki et al., 2014) in Hindi. For sentence-level sentiment analysis we annotate these same set of reviews. Here, we evaluate our proposed approach for both coarse-grained (sentence based) and fine-grained (aspect based) sentiment analysis. Our proposed method is based on deep learning, which has shown its premise in various NLP problems including sentiment analysis. Authors worldwide have proposed many variants of its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activ"
C16-1047,D15-1303,0,0.0122283,"its architecture (Kim, 2014; dos Santos and Gatti, 2014), which have shown success for solving problems in varying domains. Most of these works employ traditional technique of using softmax as an activation function on top of a typical convolutional neural network (CNN). However, in our work we learn sentiment embedded vectors using CNN pipeline and perform final classification using a strong classifier, Support Vector Machine (SVM) (Vapnik, 1995). Replacing softmax layer with some stronger classifier might be useful as shown in very few research, such as computer vision (Tang, 2013) and NLP (Poria et al., 2015). In this work, we do not use the traditional pipeline of CNN (c.f. Section 2.1) for sentiment classification. Rather, we learn sentiment features through CNN, which we call as ‘sentiment-embedded vector’. Parallely, a multi-objective optimization (MOO) based framework using Genetic Algorithm (GA) (Deb et al., 2002) is employed to derive optimized features for the respective optimization functions. In the final step, we augment the sentiment-embedded vector with the optimized feature set to form ‘sentiment augmented optimized vector’. This vector is used as the feature for sentiment classifica"
C16-1047,S15-2078,0,0.0382749,"astic) and online product reviews (sentence-level and aspect-level), across two different languages viz. Hindi and English for sentence-level as well as aspect-level sentiment analysis. Experiments show that the proposed hybrid deep learning architecture is highly efficient for sentiment analysis in multiple domains for Hindi. To the best of our knowledge, this is the very first attempt of using such a hybrid deep learning model for sentiment analysis, especially in less-resource languages. For English, we use the benchmark dataset of SemEval-2015 shared task on sentiment analysis in Twitter (Rosenthal et al., 2015) and SemEval-2014 shared task on aspect based sentiment analysis (Pontiki et al., 2014). 2 Methodology Logistic regression (LR) (or Softmax regression for multi-class classification) and SVM are two algorithms that often produce comparable results. However, SVM has an edge over LR if the data is not linearly separable, i.e. SVM with non-linear kernel performs better than LR (Pochet and Suykens, 2006). Also, LR focuses on maximizing the likelihood and is prone to over-fitting. However, SVM finds a linear hyperplane by projecting input data into higher dimension and generalizes well. We incorpor"
C16-1047,P06-1134,0,0.08803,"Missing"
C18-1042,D14-1181,0,0.00359966,"the most semantically similar question is identified by comparing the unseen question with the existing set of questions. The question, which is closest to the unseen question can be retrieved as a possible semantically similar question. Thus, accurate semantic question matching can significantly improve a QA system. In the recent past, several deep learning based models such as recurrent neural networks (RNNs), convolution neural network (CNN), gated recurrent units (GRUs) etc. have been explored to obtain representation at the word (Mikolov et al., 2013; Pennington et al., 2014), sentence (Kim, 2014) and paragraph (Zhang et al., 2017) level. In the proposed semantic question matching framework, we use attention based neural network models to generate question vectors. We create a hierarchical taxonomy by considering different types and subtypes in such a way that questions having similar answers belong to the same taxonomy class. We propose and train a deep learning based question classifier network to classify the taxonomy classes. The taxonomy information is helpful in taking a decision on semantic similarity between them. For example, the questions ‘How do scientists work?’ and ‘Where"
C18-1042,N16-1153,0,0.112881,"Missing"
C18-1042,P11-1143,0,0.0115573,"researchers in very recent times (M`arquez et al., 2015; Nakov et al., 2016). It solves the problem of question starvation in cQA forums by providing a semantically similar question which has already been answered. In literature, there have been attempts to address the problem of finding the most similar match to a given question, for e.g. Burke et al. (1997) and Mlynarczyk and Lytinen (2005). Wang et al. (2009) have presented syntactic tree based matching for finding semantically similar questions. ‘Similar question retrieval’ has been modeled using various techniques such as topic modeling (Li and Manandhar, 2011), knowledge graph representation (Zhou et al., 2013) and machine translation (Jeon et al., 2005). Semantic kernel based similarity methods for QA have also been proposed in (Filice et al., 2016; Croce et al., 2017; Croce et al., 2011). Answer selection in QA forums is similar to the question similarity task. In recent times, researchers have been investigating DL-based models for answer selection (Wang and Nyberg, 2015; Severyn and Moschitti, 2015; Feng et al., 2015). Most of the existing works either focus on better representations for questions or linguistic information associated with the q"
C18-1042,C02-1150,0,0.348075,"Missing"
C18-1042,S15-2047,0,0.060731,"Missing"
C18-1042,D14-1162,0,0.083547,"bypassed. For each unseen question, the most semantically similar question is identified by comparing the unseen question with the existing set of questions. The question, which is closest to the unseen question can be retrieved as a possible semantically similar question. Thus, accurate semantic question matching can significantly improve a QA system. In the recent past, several deep learning based models such as recurrent neural networks (RNNs), convolution neural network (CNN), gated recurrent units (GRUs) etc. have been explored to obtain representation at the word (Mikolov et al., 2013; Pennington et al., 2014), sentence (Kim, 2014) and paragraph (Zhang et al., 2017) level. In the proposed semantic question matching framework, we use attention based neural network models to generate question vectors. We create a hierarchical taxonomy by considering different types and subtypes in such a way that questions having similar answers belong to the same taxonomy class. We propose and train a deep learning based question classifier network to classify the taxonomy classes. The taxonomy information is helpful in taking a decision on semantic similarity between them. For example, the questions ‘How do scienti"
C18-1042,P15-2116,0,0.0707142,"Missing"
C18-1237,D15-1075,0,0.0270996,"s are relevant to a context. We reserve Temporality to be explored in a later work. 5.1 Embedding and Sentence Encoder The task of Novelty Detection requires high-level understanding and reasoning about semantic relationships within texts. Textual Entailment or Natural Language Inference is one such task which exhibits such complex semantic interactions. Following from (Conneau et al., 2017) we therefore employ a sentence encoder based on a bi-directional Long Short Term Memory (LSTM) architecture with max pooling, trained on the large-scale Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). SNLI entries supposedly captures rich semantic associations between the text pairs (entailment/inference relationships between premise and hypothesis). The output of our sentence encoder is 2 https://www.uni-weimar.de/en/media/chairs/computer-science-department/webis/data/corpus http://www.iitp.ac.in/ ai-nlp-ml/resources.html 4 We call our document vector relative, as we desire to encode the relative new information of a target document w.r.t. its relevant source document(s) 3 2805 Figure 1: RDV-CNN framework for Novelty Detection. Generic SNLI Training (Conneau et al., 2017). The sentence e"
C18-1237,J81-4005,0,0.700949,"Missing"
C18-1237,D17-1070,0,0.268875,"m has already seen the designated source documents for a particular event, it is to judge whether an incoming on-topic document is novel or not. The semantic nature of the dataset makes it an ideal candidate for our experiments. 5 Proposed Model Having an acceptable benchmark dataset avaialable at our end, instead of handcrafting the similarity/divergence based features,we try to learn feature representations from a target document with respect to the source document(s) using a Convolutional Neural Network (CNN). Our proposed model is based on a recent sentence embedding paradigm proposed by (Conneau et al., 2017). We leverage their idea and create a representation of the relevant target document relative to the designated source document(s) and call it as the Relative Document Vector (RDV)4 . We then train a Convolutional Neural Network (CNN) with the RDV of the target documents in the similar line of (Kim, 2014), and finally classify a document as novel or non-novel with respect to its source documents (Figure 1). Although there are document embedding models available, our method is specifically tailored to address the relativity and diversity criteria which is fundamental to the definition of novelt"
C18-1237,L18-1559,1,0.92083,"new information to be labeled as novel with respect to a set of source documents. The source document set could be seen as the memory of the reader which stores known information. Document-level novelty detection is a crucial problem and finds application in diverse domains of language processing such as information retrieval, document summarization, predicting impact of scholarly articles, etc. It is a complex problem that comprehends lexical, syntactic, semantic and pragmatic levels of texts in conjunction with certain characteristics like relevance, diversity, relativity, and temporality (Ghosal et al., 2018). Literature methods for novelty detection based on lexical similarity, divergence features, and information retrieval measures, although proved effective for sentence level but could not address the document-level comprehension needs. Our understanding of the problem led us to an assertion that a deep neural network might be able to extract the text subtleties involved in understanding a document’s novelty. To the best of our knowledge this is the very first attempt to address document-level novelty detection without the involvement of any hand-crafted features. Our approach achieves signific"
C18-1237,D14-1181,0,0.0266799,"f handcrafting the similarity/divergence based features,we try to learn feature representations from a target document with respect to the source document(s) using a Convolutional Neural Network (CNN). Our proposed model is based on a recent sentence embedding paradigm proposed by (Conneau et al., 2017). We leverage their idea and create a representation of the relevant target document relative to the designated source document(s) and call it as the Relative Document Vector (RDV)4 . We then train a Convolutional Neural Network (CNN) with the RDV of the target documents in the similar line of (Kim, 2014), and finally classify a document as novel or non-novel with respect to its source documents (Figure 1). Although there are document embedding models available, our method is specifically tailored to address the relativity and diversity criteria which is fundamental to the definition of novelty. Here T1 is the target document whose state of novelty is to be determined against the source document(s) S1 , S2 , ...SM i.e. to say the objective is to automatically figure out whether T1 is novel or not once the machine has already seen/scanned S1 , S2 , ...SM . Our model assumes that the documents a"
C18-1237,N13-1090,0,0.0143391,"s of the concatenation of the two sentence embeddings corresponding to tk and sij , their absolute element-wise difference, and their element-wise product (Mou et al., 2016). The first heuristic follows the most standard procedure of the Siamese architecture, while the latter two are certain measures of ”similarity” or ”closeness”. Thus, the Relative Sentence Vector (RSV) corresponding to a target sentence tk is represented as : RSVk = [ak , bij , |ak − bij |, ak ∗ bij ] where comma (,) refers to the column vector concatenation. This representation is inspired from the word embedding studies (Mikolov et al., 2013) where the linear offset of vectors is seen to capture semantic relationships between the two words. (Mou et al., 2016) successfully leveraged this idea for modeling sentence-pair relationships which we alleviate to model documents. Thus for each target sentence tk we compute the RSV and aggregate them to form the Relative Document Vector (RDV) of target document Tj with respect to the source documents(s) Si . Aggregation is realized as a slot filling task to shape the document matrix8 or RDV of dimension N X 4D where N is the number of sentences in a target document (padded when necessary) an"
C18-1237,P16-2022,0,0.0141123,"n the target document with one of the source sentences. bij → ak where ak has the max. cosine similarity with bij . 5.3 Aggregator module This module aggregates the mappings produced in the comparator module to generate a document matrix. The mapping of a target sentence tk to its closest source sentence sij is rendered by constructing a feature vector that captures the relation between the source and the target. This feature vector consists of the concatenation of the two sentence embeddings corresponding to tk and sij , their absolute element-wise difference, and their element-wise product (Mou et al., 2016). The first heuristic follows the most standard procedure of the Siamese architecture, while the latter two are certain measures of ”similarity” or ”closeness”. Thus, the Relative Sentence Vector (RSV) corresponding to a target sentence tk is represented as : RSVk = [ak , bij , |ak − bij |, ak ∗ bij ] where comma (,) refers to the column vector concatenation. This representation is inspired from the word embedding studies (Mikolov et al., 2013) where the linear offset of vectors is seen to capture semantic relationships between the two words. (Mou et al., 2016) successfully leveraged this idea"
C18-1237,H05-1090,0,0.029064,"was to highlight the relevant sentences that contain novel information, given a topic and an ordered list of relevant documents. Some interesting works in TREC were based on the sets of terms (Zhang et al., 2003a; Zhang et al., 2003b), term translations (Collins-Thompson et al., 2002), Principal Component Analysis (PCA) vectors (Ru et al., 2004), Support Vector Machine (SVM) classification (Tomiyama et al., 2004) etc. Similar works relied on named entities (Gabrilovich et al., 2004; Li and Croft, 2005; Zhang and Tsai, 2009), language models (Zhang et al., 2002; Allan et al., 2003), contexts (Schiffman and McKeown, 2005), etc. Next came the novelty sub tracks of Recognizing Textual Entailment-Text Analytics Conference (RTE-TAC) 6 and 7 (Bentivogli et al., 2011) where Textual Entailment was viewed as one close neighbor to sentence level novelty detection. At the document level an interesting work was carried out by (Yang et al., 2002) via topical classification of on-line document streams and then detecting novelty of documents in each topic exploiting the named entities. Another work by (Zhang et al., 2002) viewed novelty as an opposite characteristic to redundancy and proposed a set of five redundancy measur"
C18-1237,H05-1014,0,0.0664507,"he Topic Detection and Tracking (TDT) (Wayne, 1997) evaluation campaigns where the concern was to detect new events or First Story Detection (FSD) with respect to online news streams. Techniques mostly involved grouping the news stories into clusters and then measuring the belongingness of an incoming story to any of the clusters based on some preset similarity threshold. Some notable contributions from TDT are (Allan et al., 1998; Yang et al., 2002; Allan et al., 2000; Yang et al., 1998). The task gained prominence in the novelty tracks of Text Retrieval Conferences (TREC) from 2002 to 2004 (Soboroff and Harman, 2005; Harman, 2002; Soboroff and Harman, 2003; Clarke et al., 2004) although the focus was sentence-level novelty detection. The goal of these tracks was to highlight the relevant sentences that contain novel information, given a topic and an ordered list of relevant documents. Some interesting works in TREC were based on the sets of terms (Zhang et al., 2003a; Zhang et al., 2003b), term translations (Collins-Thompson et al., 2002), Principal Component Analysis (PCA) vectors (Ru et al., 2004), Support Vector Machine (SVM) classification (Tomiyama et al., 2004) etc. Similar works relied on named en"
D17-1057,S17-2089,0,0.0197875,"Missing"
D17-1057,C16-1047,1,0.0808105,"current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging the syntactic and semantic richness of the two distributed word representation through a stacked denoising autoencoder; and c) build a state-of-the-art model for sentiment analysis in financial domain. 2 Figure 1: MLP based ensemble architecture. A. Convolution Neural Network (CNN): Literature suggests that CNN architecture had been successfully applied for sentiment analysis at various level (Kim, 2014; Akhtar et al., 2016; Singhal and Bhattacharyya, 2016). Most of these works involve classification tasks, however, we adopt CNN architecture for solving the regression problem. Our proposed system employs a convolution layer followed by a max pool layer, 2 fully connected layers and an output layer. We use 100 different filters while sliding over 2, 3 and 4 words at a time. We employ all these filters in parallel. Proposed Methodology We propose a Multi-Layer Perceptron based ensemble approach to leverage the goodness of various supervised systems. We develop three deep neural network architecture based models, v"
D17-1057,D14-1181,0,0.0031271,"ons of the current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging the syntactic and semantic richness of the two distributed word representation through a stacked denoising autoencoder; and c) build a state-of-the-art model for sentiment analysis in financial domain. 2 Figure 1: MLP based ensemble architecture. A. Convolution Neural Network (CNN): Literature suggests that CNN architecture had been successfully applied for sentiment analysis at various level (Kim, 2014; Akhtar et al., 2016; Singhal and Bhattacharyya, 2016). Most of these works involve classification tasks, however, we adopt CNN architecture for solving the regression problem. Our proposed system employs a convolution layer followed by a max pool layer, 2 fully connected layers and an output layer. We use 100 different filters while sliding over 2, 3 and 4 words at a time. We employ all these filters in parallel. Proposed Methodology We propose a Multi-Layer Perceptron based ensemble approach to leverage the goodness of various supervised systems. We develop three deep neural network archite"
D17-1057,W14-4012,0,0.04212,"Missing"
D17-1057,S17-2152,0,0.0291858,"Missing"
D17-1057,S17-2138,0,0.0483805,"WE-GLV CNN C3 FWE-W2V CNN C4 FWE-GLV CNN C5 DAWE CNN Long short term memory (LSTM) L1 PWE-W2V LSTM L2 PWE-GLV LSTM L3 FWE-W2V LSTM L4 FWE-GLV LSTM L5 DAWE LSTM Gated Recurrent Unit (GRU) G1 PWE-W2V GRU G2 PWE-GLV GRU G3 FWE-W2V GRU G4 FWE-GLV GRU G5 DAWE GRU Feature - SVR F1 Tf-idf + Lexicon + Vader F2 Tf-idf + Lexicon + Vader + PWE-W2V F3 Tf-idf + Lexicon + Vader + PWE-GLV F4 Tf-idf + Lexicon + Vader + FWE-W2V F5 Tf-idf + Lexicon + Vader + FWE-GLV F6 Tf-idf + Lexicon + Vader + DAWE Ensemble E1 C4 + L3 + G5 + F6 (MLP) E2 C1 + L5 + G1 + F6 (MLP) systems (ECNU (Lan et al., 2017) and Fortia-FBK (Mansar et al., 2017)) which were the best performing systems at SemEval-2017 shared task 5. ECNU reported to have obtained cosine similarity of 0.777 in microblog as compared to 0.797 cosine similarity of our proposed system, whereas, for news headlines Fortia-FBK reported cosine similarity of 0.745. ECNU employed several regressors on top of optimized feature set obtained through hill climbing algorithm. For the final prediction, authors averaged the predictions of different regressors. Fortia-FBK trained a CNN with the assistance of sentiment lexicons for predicting the sentiment score. It should be noted that"
D17-1057,S13-2053,0,0.0104051,"which can efficiently learn long-term dependencies. A key difference of GRU with LSTM is that, GRU’s 541 recurrent state is completely exposed at each time step in contrast to LSTM’s recurrent state which controls its recurrent state. Thus, comparably GRUs have lesser parameters to learn and training is computationally efficient. We use two GRU layers on top of each other having 100 neurons in each. This was followed by 2 fully connected layers and an output layer. cons. These are NRC (Hashtag Context, Hashtag Sentiment, Sentiment140, Sentiment140 Context) lexicons (Kiritchenko et al., 2014; Mohammad et al., 2013) which associate a positive or negative score to a token. Following features are extracted for each of these: i) positive, negative and net count. ii) maximum of positive and negative scores. iii) sum of positive, negative and net scores. - Vader Sentiment: Vader sentiment (Gilbert, 2014) score is a rule-based method that generates a compound sentiment score for each sentence between -1 (extreme negative) and +1 (extreme positive). It also produces ratio of positive, negative and neutral tokens in the sentence. We obtain score and ratio of each instance in the datasets and use as feature for t"
D17-1057,D14-1162,0,0.0937113,"re 1. and implicit sentiment in the financial text. An application of multiple regression model was developed by (Oliveira et al., 2013). In this paper, we propose a novel Multi-Layer Perceptron (MLP) based ensemble technique for fine-grained sentiment analysis. It combines the outputs of four systems, one is feature-driven supervised model and the rest three are deep learning based. We further propose to develop an enhanced word representation by learning through a stacked denoising autoencoder network (Vincent et al., 2010) using word embeddings of Word2Vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) models. For evaluation purpose we use datasets of SemEval-2017 ‘Fine-Grained Sentiment Analysis on Financial Microblogs and News’ shared task (Keith Cortis and Davis, 2017). The dataset comprises of financial short texts for two domains i.e. microblog messages and news headlines. Comparisons with the state-of-the-art models show that our system produces better performance. The main contributions of the current work are summarized as follows: a) we effectively combine competing systems to work as a team via MLP based ensemble learning; b) develop an enhanced word representation by leveraging t"
D17-1057,C16-1287,1,0.898554,"Missing"
D17-1057,H05-1044,0,0.0279265,"ting the sentiment score in the continuous range of -1 to +1. - Word Tf-Idf: Term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to reflect how important a word is to a document in a corpus. We consider tf-idf weighted counts of continuous sequences of ngrams (n=2,3,4,5) at a time. - Lexicon Features: Sentiment lexicons are widely utilized resources in the field of sentiment analysis. Its application and effectiveness in sentiment prediction task had been widely studied. We employ two lexicons i.e. Bing Liu opinion lexicon (Ding et al., 2008) and MPQA (Wilson et al., 2005) subjectivity lexicon for news headlines domain. First we compile a comprehensive list of positive and negative words form these lexicons and then extract the following lexicon driven features. Agreement Score : This score indicates the polarity of the sentence i.e. whether the sentence takes a polar or neutral stance. If the agreement score is 1 then it implies that the instance is of having either high positive or negative sentiment whereas, a 0 agreement score indicates a mixed or disharmony in the positive and negative sentiment implying the sentence is not polar (Rao and Srivastava, 2012)"
D18-1377,P18-1087,0,0.190777,"his was first proposed by Wang et al. (2016) where they appended aspect embeddings with the each word embeddings of the sentence to generate aspect-aware sentence representation. This representation was further fed to an attention layer followed by softmax for final classification. More recently, Ma et al. (2017) proposed a model where both context and aspect representations interact with each other’s attention mechanism to generate the overall representation. Tay et al. (2017) proposed word-aspect associations using circular correlation as an improvement over Wang et al. (2016)’s work. Also, Li et al. (2018) used transformer networks for target-oriented sentiment classification. ABSA has also been researched from a question-answering perspective where deep memory networks have played a major role (Tang et al., 2016b; Li et al., 2017). However, unlike our proposed method, none of these methods have tried to model the inter-aspect relations. 3 Method In this section, we formalize the task and present our method. 3.1 Problem Definition Input We are given a sentence S = [w1 , w2 , . . . , wL ], where wi are the words and L is the maximum number of words in a sentence. Also, the given aspect-terms for"
D18-1377,D15-1166,0,0.0168066,"Missing"
D18-1377,D14-1162,0,0.0812171,"between our model and the literature is the consideration of the neighboring aspects in a sentence with the target aspect. We assume that our inter-aspect relation modeling (IARM) architecture1 models the relation between the target aspect and surrounding aspects, while filtering out irrelevant information. Fig. 1 depicts our model. 1 Implementation available on http://github. com/senticnet/IARM 3403 3.2.1 Overview Our IARM model can be summarized with the following steps: Input Representation We replace the words in the input sentences and aspect-terms with pre-trained Glove word embeddings (Pennington et al., 2014). For multi-worded aspect-terms, we take the mean of constituent word embeddings as aspect representation. Aspect-Aware Sentence Representation Following Wang et al. (2016), all the words in a sentence are concatenated with the given aspect representation. These modified sequence of words are fed to a gated recurrent unit (GRU)2 for context propagation, followed by an attention layer to obtain the aspect-aware sentence representation; we obtain for all the aspects in a sentence. Inter-Aspect Dependency Modeling We employ memory network (Sukhbaatar et al., 2015) to model the dependency of the t"
D18-1377,P17-2023,0,0.0210742,"a product is also very important to them. Companies allocate their resources to research, development, and marketing based on these factors. Aspect-based sentiment analysis (ABSA) caters to these needs. Users tend to express their opinion on different aspects of a given product. For example, the sentence “Everything is so easy to use, Mac software is just so much simpler than Microsoft software.” expresses sentiment behind three aspects: “use”, “Mac software”, and “Microsoft software” to be positive, positive, and negative respectively. This leads to two tasks to be solved: aspect extraction (Shu et al., 2017) and aspect sentiment polarity detection (Wang et al., 2016). In this paper, we In sentences containing multiple aspects, the main challenge an Aspect-Based-SentimentAnalysis (ABSA) classifier faces is to correctly connect an aspect to the corresponding sentimentbearing phrase (typically adjective). Let us consider this sentence “Coffee is a better deal than overpriced cosi sandwiches”. Here, we find two aspects: “coffee” and “cosi sandwiches”. It is clear in this sentence that the sentiment of “coffee” is expressed by the sentimentally charged word “better”; on the other hand, “overpriced” ca"
D18-1377,C16-1311,0,0.305142,"fed to an attention layer followed by softmax for final classification. More recently, Ma et al. (2017) proposed a model where both context and aspect representations interact with each other’s attention mechanism to generate the overall representation. Tay et al. (2017) proposed word-aspect associations using circular correlation as an improvement over Wang et al. (2016)’s work. Also, Li et al. (2018) used transformer networks for target-oriented sentiment classification. ABSA has also been researched from a question-answering perspective where deep memory networks have played a major role (Tang et al., 2016b; Li et al., 2017). However, unlike our proposed method, none of these methods have tried to model the inter-aspect relations. 3 Method In this section, we formalize the task and present our method. 3.1 Problem Definition Input We are given a sentence S = [w1 , w2 , . . . , wL ], where wi are the words and L is the maximum number of words in a sentence. Also, the given aspect-terms for sentence S are A1 , A2 , . . . , AM , where Ai = [wk , . . . , wk+m−1 ], 1 ≤ k ≤ L, 0 < m ≤ L − k + 1, and M is the maximum number of aspects in a sentence. Output Sentiment polarity (1 for positive, 0 for nega"
D18-1377,D16-1021,0,0.442397,"fed to an attention layer followed by softmax for final classification. More recently, Ma et al. (2017) proposed a model where both context and aspect representations interact with each other’s attention mechanism to generate the overall representation. Tay et al. (2017) proposed word-aspect associations using circular correlation as an improvement over Wang et al. (2016)’s work. Also, Li et al. (2018) used transformer networks for target-oriented sentiment classification. ABSA has also been researched from a question-answering perspective where deep memory networks have played a major role (Tang et al., 2016b; Li et al., 2017). However, unlike our proposed method, none of these methods have tried to model the inter-aspect relations. 3 Method In this section, we formalize the task and present our method. 3.1 Problem Definition Input We are given a sentence S = [w1 , w2 , . . . , wL ], where wi are the words and L is the maximum number of words in a sentence. Also, the given aspect-terms for sentence S are A1 , A2 , . . . , AM , where Ai = [wk , . . . , wk+m−1 ], 1 ≤ k ≤ L, 0 < m ≤ L − k + 1, and M is the maximum number of aspects in a sentence. Output Sentiment polarity (1 for positive, 0 for nega"
D18-1377,D16-1058,0,0.626981,"e their resources to research, development, and marketing based on these factors. Aspect-based sentiment analysis (ABSA) caters to these needs. Users tend to express their opinion on different aspects of a given product. For example, the sentence “Everything is so easy to use, Mac software is just so much simpler than Microsoft software.” expresses sentiment behind three aspects: “use”, “Mac software”, and “Microsoft software” to be positive, positive, and negative respectively. This leads to two tasks to be solved: aspect extraction (Shu et al., 2017) and aspect sentiment polarity detection (Wang et al., 2016). In this paper, we In sentences containing multiple aspects, the main challenge an Aspect-Based-SentimentAnalysis (ABSA) classifier faces is to correctly connect an aspect to the corresponding sentimentbearing phrase (typically adjective). Let us consider this sentence “Coffee is a better deal than overpriced cosi sandwiches”. Here, we find two aspects: “coffee” and “cosi sandwiches”. It is clear in this sentence that the sentiment of “coffee” is expressed by the sentimentally charged word “better”; on the other hand, “overpriced” carries the sentiment of “cosi sandwiches”. The aim of the ABS"
D19-1566,W18-3301,0,0.0474043,"ical correlation analysis approach (En-SLDCCA) to learn the multi-modal shared feature representation. Tzirakis et al. (2017) introduced a Long Short Term Memory (LSTM) based end-to-end multi-modal emotion recognition system in which convolutional neural network (CNN) and a deep residual network are used to capture the emotional content for various styles of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Network (TFN) to capture the inter-modality and intra-modality dynamics between the multi-modalities (i.e., text, visual, and acoustic). These works did not take contextual information into account. Poria et al. (2017b) i"
D19-1566,W12-3701,0,0.026074,"distinct features of the input modalities, i.e., text, acoustic and visual; (2) We employ a Context-aware Attention Module (CAM) that identifies and assigns the weights to the neighboring utterances based on their contributing features. It exploits the interactive representations of pairwise modalities to learn the attention weights, and (3) We present new state-of-the-arts for five benchmark datasets for both sentiment and emotion predictions. 2 Related Work Different reviews in (Arevalo et al., 2017; Poria et al., 2016, 2017b; Ghosal et al., 2018; Morency et al., 2011a; Zadeh et al., 2018a; Mihalcea, 2012; Lee et al., 2018; Tsai et al., 2018) suggest that multi-modal sentiment and emotion analysis are relatively new areas as compared to uni-modal analysis. Feature selection (fusion) is a challenging and important task for any multi-modal analysis. Poria et al. (2016) proposed a multi-kernel learning based feature selection method for multimodal sentiment and emotion recognition. A convolutional deep belief network (CDBN) is proposed in (Ranganathan et al., 2016) to learn salient multi-modal features of low-intensity expressions of emotions, whereas Lee et al. (2018) introduced a convolutional"
D19-1566,W14-4012,0,0.0408248,"Missing"
D19-1566,P13-1096,0,0.381682,"Missing"
D19-1566,D18-1382,1,0.554739,"e (IIM) that aims to learn the interaction among the diverse and distinct features of the input modalities, i.e., text, acoustic and visual; (2) We employ a Context-aware Attention Module (CAM) that identifies and assigns the weights to the neighboring utterances based on their contributing features. It exploits the interactive representations of pairwise modalities to learn the attention weights, and (3) We present new state-of-the-arts for five benchmark datasets for both sentiment and emotion predictions. 2 Related Work Different reviews in (Arevalo et al., 2017; Poria et al., 2016, 2017b; Ghosal et al., 2018; Morency et al., 2011a; Zadeh et al., 2018a; Mihalcea, 2012; Lee et al., 2018; Tsai et al., 2018) suggest that multi-modal sentiment and emotion analysis are relatively new areas as compared to uni-modal analysis. Feature selection (fusion) is a challenging and important task for any multi-modal analysis. Poria et al. (2016) proposed a multi-kernel learning based feature selection method for multimodal sentiment and emotion recognition. A convolutional deep belief network (CDBN) is proposed in (Ranganathan et al., 2016) to learn salient multi-modal features of low-intensity expressions of emo"
D19-1566,P17-1081,0,0.0333009,"nce in (Patwardhan, 2017). Similar work on feature-level fusion based on self- attention mechanism is reported in (Hazarika et al., 2018). Fu et al. (2017) introduced an enhanced sparse local discriminative canonical correlation analysis approach (En-SLDCCA) to learn the multi-modal shared feature representation. Tzirakis et al. (2017) introduced a Long Short Term Memory (LSTM) based end-to-end multi-modal emotion recognition system in which convolutional neural network (CNN) and a deep residual network are used to capture the emotional content for various styles of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Netw"
D19-1566,W18-3304,0,0.0298946,"Missing"
D19-1566,P18-1208,0,0.317135,"Missing"
D19-1566,P17-1142,0,0.0295202,"fine the following setups for our experiments. • Two-class (pos and neg) classification: MOSEI, MOSI, ICT-MMMO, and MOUD. • Three-class (pos, neu, and neg) classification: YouTube. • Five-class (strong pos, weak pos, neu, weak neg, and strong neg) classification: MOSEI. • Seven-class (strong pos, moderate pos, weak pos, neu, weak neg, moderate neg, and strong neg) classification: MOSEI and MOSI. • Intensity prediction: MOSEI and MOSI. 4.3 Experiments We implement our proposed model on the Pythonbased Keras deep learning library. As the evaluation metric, we employ accuracy (weighted accuracy (Tong et al., 2017)) and F1-score for the classification problems, while for the intensity prediction task, we compute Pearson correlation scores and mean-absolute-error (MAE). We evaluate our proposed CIA model on five benchmark datasets i.e., MOUD, MOSI, YouTube, ICT-MMMO, and MOSEI. For all the datasets, we perform grid search to find the optimal hyperparameters (c.f. Table 4). Though we push for a generic hyper-parameter configuration for all datasets, in some cases, a different choice of the parameter has a significant effect. Therefore, we choose different parameters for different datasets for our experime"
D19-1566,D17-1115,0,0.0196845,"of speaking, robust features. Poria et al. (2017a) presented a literature survey on various affect dimensions e.g., sentiment analysis, emotion analysis, etc., for the multi-modal analysis. A multi-modal fusion-based approach is proposed in (Blanchard et al., 2018) for sentiment classification. The author used exclusively high-level fusion of visual and acoustic features to classify the sentiment. Zadeh et al. (2016) presented the multi-modal dictionary-based technique to capture the interaction between spoken words and facial expression better when expressing the sentiment. In another work, Zadeh et al. (2017) proposed a Tensor Fusion Network (TFN) to capture the inter-modality and intra-modality dynamics between the multi-modalities (i.e., text, visual, and acoustic). These works did not take contextual information into account. Poria et al. (2017b) introduced an Long Short Term Memory (LSTM) based framework for sentiment classification which uses contextual information to capture interrelationships between the utterances. In another work, Poria et al. (2017c) proposed a user opinion based framework to combine all the multi-modal inputs (i.e., visual, acoustic, and textual) by applying a multi-ker"
E17-1109,grouin-2014-biomedical,0,0.0252279,"utperform the dictionary based methods. Some of the recent works in BNER includes the unsupervised model as proposed in (Zhang and Elhadad, 2013), and the system based on CRF (Li et al., 2015a). A two-phase approach based on semi-Markov CRF is proposed in (Yang and Zhou, 2014). In the first phase boundaries of entities are identified while in the second phase semantic labeling is performed to label the detected entities. A CRF based system has been proposed by (Tang et al., 2015), where in the first step boundaries of NEs are identified and in the second step appropriate labels are assigned. (Grouin, 2014) performed experiments on the i2b2/VA-2010 challenge dataset to detect bacteria and biotopes names. They developed a model based on CRFs. An unsupervised approach is proposed in (Han et al., 2016) that made use of clustering based active learning. They have used Shared Nearest Neighbor (SNN) clustering technique. The work reported in (Li et al., 2015a), authors have proposed a parallel CRF algorithm (MapReduce CRF) which provides a mechanism to minimise the time taken for CRF learning. They showed that the proposed approach outperforms other traditional models in terms of time and efficiency."
E17-1109,W04-1219,0,0.045718,"ation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account t"
E17-1109,W02-0301,0,0.116279,"to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these fe"
E17-1109,W04-1213,0,0.0899452,"m. The challenges as of these kinds are the primary causes behind the low accuracies of the systems developed for entity extraction in biomedi1159 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 1159–1170, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics cal text. The research challenges have been addressed in the literature including in some sharedtask challenges, such as JNLPBA (Joint Workshop on Natural Language Processing in Biomedicine and its Applications) in 2004 (Kim et al., 2004) and BioCreative (Critical Assessment for Information Extraction in Biology Challenge) II GM (gene mention) subtask in 2007 (Smith et al., 2008). Over the years several benchmark corpora have been created that do not conform to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised"
E17-1109,I05-1057,0,0.0287701,"er domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account to the problem of data sparsity. In the recent past, there has been huge interest in using large unlabeled corpus to generate word representation feat"
E17-1109,W16-5102,0,0.0181422,"ed function that can efficiently solve the full NER task. The work proposed in (Tohidi et al., 2014) aims to improve the performance of entity extraction using statistical character-based syntax similarity (SCSS) algorithm. This algorithm computes the similarity between the identified candidate entities and a known set of well-known NEs. This set of NEs is created by extracting the most frequently occurring NEs in the GENIA V3.0 corpus. In recent times deep learning based approaches such as Recurrent Neural Network and Bi-directional LSTM have also used for entity extraction(Li et al., 2015b; Limsopatham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) et"
E17-1109,N04-1043,0,0.0608219,"portant role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene expression (Ding and Peng, 2005), face recognition (Seal et al., 2015) and signal processing (Ala"
E17-1109,W04-1221,0,0.0914003,"ed for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Language System) (Bodenreider, 2004). However, these features heavenly account to the problem of data sparsity. In the recent past, there has been huge interest in using large unlabeled corpus to generate word"
E17-1109,P13-1045,0,0.0532197,"t al., 2015b; Limsopatham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene"
E17-1109,P10-1040,0,0.079586,"tham and Collier, 2016). It is well known that relevant features play an important role for building a high accurate system. In our work, in addition to the standard features we also use the features extracted from the word embedding model. Bengio et al.(Bengio et al., 2003) have proposed a neural network based model for vector representation of words. Distributed representation (also known as word embedding) of a word has been used to improve the performance of various NLP tasks like Part-of-Speech (POS) tagging, NER in news-wire domain (Collobert et al., 2011), parsing (Socher et al., 2013; Turian et al., 2010) etc. Word cluster has been used used by Miller et al.(Miller et al., 2004) to boost the performance of a NER system. Tang et al. (Tang et al., 2012; Tang et al., 2013) have reported that performance of biomedical entity extraction can be improved when word representation is used as a feature to CRF and SVM classifiers. Here we propose a PSO based feature selection technique that determines the most relevant features from a full word embedding set, and use this subset as feature for classifier’s training. Feature selection has been widely used for many tasks such as gene expression (Ding and P"
E17-1109,W04-1216,0,0.055746,"orpora have been created that do not conform to the uniform annotation guidelines. Therefore the system, developed by targeting a specific domain, often fails to show reasonable accuracy when it is evaluated for some other domains. In our work we attempt to build a system for entity extraction that performs well across various biomedical corpora. Popular existing system mostly rely on rule-based system or supervised machine learning technique to automatically extract entities. They looked upon this problem as in terms of sequence labeling and used algorithm such as hidden markov models (HMM) (Zhao, 2004), support vector machines (SVM) (Kazama et al., 2002; GuoDong and Jian, 2004), maximum entropy Markov model (MEMM) (Finkel et al., 2005) and conditional random fields (CRF) (Ekbal et al., 2013; Settles, 2004; Kim et al., 2005). These supervised learning models is fully dependent on the features that we use for training. Some of the popular features used in the existing studies include linguistic features such as morphological, syntactic and semantic information of words and domain-specific features from biomedical ontologies such as BioThesaurus (Liu et al., 2006) and UMLS (Unified Medical Lan"
ekbal-saha-2010-maximum,M98-1014,0,\N,Missing
ekbal-saha-2010-maximum,M98-1021,0,\N,Missing
ekbal-saha-2010-maximum,I08-5008,1,\N,Missing
ekbal-saha-2010-maximum,M98-1019,0,\N,Missing
ekbal-saha-2010-maximum,E99-1001,0,\N,Missing
ekbal-saha-2010-maximum,A00-1034,0,\N,Missing
ekbal-saha-2010-maximum,W09-3539,1,\N,Missing
ekbal-saha-2010-maximum,W03-0419,0,\N,Missing
ekbal-saha-2010-maximum,M98-1012,0,\N,Missing
I08-2077,W03-0430,0,0.0760769,"Missing"
I08-2077,N03-1028,0,0.0307936,"=1 k which as in HMMs, can be obtained efficiently by dynamic programming. To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequences: L∧ = N X log(P∧ (s(i) |o(i) )) − i=1 X λ2 k k 2σ 2 NE tag PER , where {< o(i) , s(i) >} is the labeled training data. The second sum corresponds to a zero-mean, σ 2 -variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set parameters λ to maximize the penalized log-likelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is significantly more efficient, and which results in only minor changes in accuracy due to changes in λ. When applying CRFs to the NER problem, an observation sequence is a token of a sentence or document of text and the state sequence is its corresponding label sequence. While CRFs generally can use real-valued functions, in our experiments maximum of the features are binary valued. A feature function fk (st−1 , st , o, t) has a value of 0 for most cases and is only set to be 1, when st−1 , st are certain states and the observation has certain properties. We have"
I08-5006,W03-2201,0,0.133497,"Missing"
I08-5006,W97-0312,0,0.0825691,"Missing"
I08-5006,N03-1028,0,0.349029,"features for named entity recognition in Bengali, Hindi, Telugu, Oriya and Urdu. 2 Conditional Random Fields Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which corresponds to conditionally trained probabilistic finite state automata. Being conditionally trained, these CRFs can easily incorporate a large number of arbitrary, nonindependent features while still having efficient procedures for non-greedy finite-state inference and training. CRFs have shown success in various sequence modeling tasks including noun phrase segmentation (Sha and Pereira, 2003) and table extraction (Pinto et al., 2003). CRFs are used to calculate the conditional probability of values on designated output nodes given values on other designated input nodes. The conditional probability of a state sequence S s1, s 2, , sT given an observation sequence O o1, o 2, ....., oT ) is calculated as: T 1 exp( kfk (st 1, st , o, t )), where Zo t 1 k fk ( st 1, st , o, t ) is a feature function whose weight P ( s |o) k is to be learned via training. The values of the feature functions may range between ..... , but typically they are binary. To make all conditional probabilities su"
I08-5006,W03-0430,0,0.0522082,"is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are HMM (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy http://ltrc.iiit.ac.in/ner-ssea-08 33 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 33–40, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing (New York University’s MENE in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine 1998), SRA’s system in (Bennet, 1997) and Conditional Random Fields (CRFs) (Lafferty et al., 2001; McCallum and Li, 2003). There is no concept of capitalization in Indian languages (ILs) like English and this fact makes the NER task more difficult and challenging in ILs. There has been very little work in the area of NER in Indian languages. In Indian languages particularly in Bengali, the work in NER can be found in (Ekbal and Bandyopadhyay, 2007a) and (Ekbal and Bandyopadhyay, 2007b). These two systems are based on the pattern directed shallow parsing approach. An HMM-based NER in Bengali can be found in (Ekbal et al., 2007c). Other than Bengali, the work on NER can be found in (Li and McCallum, 2004) for Hind"
I08-5006,M98-1019,0,\N,Missing
I08-5006,M98-1020,0,\N,Missing
I08-5008,N01-1025,0,0.0107121,"t it is trainable and adoptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Languag"
I08-5008,W00-0730,0,0.0137702,"Missing"
I08-5008,N03-1002,0,0.0160218,"roaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing little work in the area of NER in Indian languages. In Indian languages, particularly in Bengali, the works in NER can be found in (Ekbal and Bandyopadhyay,"
I08-5008,W03-2201,0,0.00846914,"classification of NEs are very crucial and pose a very big challenge to the NLP researchers. The level of ambiguity in NER makes it difficult to attain human performance NER has drawn more and more attention from the NE tasks (Chinchor 95; Chinchor 98) in Message Understanding Conferences (MUCs) [MUC6; MUC7]. The problem of correct identification of NEs is specifically addressed and benchmarked by the developers of Information Extraction System, such as the GATE system (Cunningham, 2001). NER also finds application in question-answering systems (Maldovan et al., 2002) and machine translation (Babych and Hartley, 2003). The current trend in NER is to use the machinelearning approach, which is more attractive in that it is trainable and adoptable and the maintenance of a machine-learning system is much cheaper than that of a rule-based one. The representative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was pro"
I08-5008,W02-2029,0,0.0424697,"presentative machine-learning approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), Maximum Entropy (New York University’s MEME in (Borthwick, 1999)), Decision Tree (New York University’s system in (Sekine, 1998) and Conditional Random Fields (CRFs) (Lafferty et al., 2001). Support Vector Machines (SVMs) based NER system was proposed by Yamada et al. (2002) for Japanese. His system is an extension of Kudo’s chunking system (Kudo and Matsumoto, 2001) that gave the best performance at CoNLL-2000 shared tasks. The other SVM-based NER systems can be found in (Takeuchi and Collier, 2002) and (Asahara and Matsumoto, 2003). Named entity identification in Indian languages in general and particularly in Bengali is difficult and challenging. In English, the NE always appears with capitalized letter but there is no concept of capitalization in Bengali. There has been a very 51 Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 51–58, c Hyderabad, India, January 2008. 2008 Asian Federation of Natural Language Processing little work in the area of NER in Indian languages. In Indian languages, particularly in Bengali, the works in NER can be f"
I08-5008,M98-1019,0,\N,Missing
I08-5008,M98-1020,0,\N,Missing
I08-7001,bertagna-etal-2004-content,0,0.0157974,"use is not in the familiar territory of computational linguistics. The web walked into the ACL meetings started in 1999. The use of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-ssea-08 3 1 http://www.ilc.cnr.it/Eagles96/home.html The 6th"
I08-7001,P06-2035,0,0.0275213,"of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-ssea-08 3 1 http://www.ilc.cnr.it/Eagles96/home.html The 6th Workshop on Asian Languae Resources, 2008 tion of the corpus has been manually annotated with the twelve NE tags as part of the I"
I08-7001,bel-etal-2000-simple,0,0.0119807,"ge of research activities. The web is anarchic, and its use is not in the familiar territory of computational linguistics. The web walked into the ACL meetings started in 1999. The use of the web as a corpus for teaching and research on language technology has been proposed a number of times (Rundel, 2000; Fletcher, 2001; Robb, 2003; Fletcher, 2003). There is a long history of creating a standard for western language resources. The human language technology (HLT) society in Europe has been particularly zealous for the standardization, making a series of attempts such as EAGLES3, PROLE/SIMPLE (Lenci et al., 2000), ISLE/MILE (Calzolari et al., 2003; Bertagna et al., 2004) and more recently multilingual lexical database generation from parallel texts in 20 European languages (Giguet and Luquet, 2006). On the other hand, in spite of having great linguistic and cultural diversities, Asian language resources have received much less attention than their western counterparts. A new project (Takenobou et al., 2006) has been started to create a common standard for Asian language resources. They have extended an existing description framework, the sourceforge.net/project/nlp-sanchay http://ltrc.iiit.ac.in/ner-s"
I08-7001,C02-1025,0,\N,Missing
I08-7001,P06-2106,0,\N,Missing
I11-1011,D08-1031,0,0.0143919,"001100101000011011100101101101111001100 11111001100001000011110100101111101110001101 01000100100101011001000010111100101100001000 10100101101011100011111110010100100010010011 00101111101110101001100000010101001011001001 00011000101110100010000010011000100110000100 76.8 76.7 74.6 72.2 71.4 74.0† 71.4 71.7 64.6 63.6 66.5 66.2 63.8 63.4 71.5 71.8† 67.1 67.6 65.2 70.3† 72.3 72.1 59.7 59.6 59.7† 59.1 60.0 61.2 74.5 74.9† 70.1† 69.1 70.3 73.1† 73.6 74.4 58.4 58.8 54.7† 55.6† 58.1 58.4 gos (2008). The results clearly show that although even larger sets of features have been proposed (Uryupina, 2007; Bengtson and Roth, 2008), the set of features already included in BART is sufficient to achieve results well above the state of the art on the dataset we used. larger datasets and larger sets of features learning a model becomes slower and requires much more memory. This suggests that automatic feature selection may be essential not just to improve performance but also to be able to train a model—i.e., that an efficient coreference resolution system should combine rich linguistic feature sets with automatic feature selection mechanisms. The results in Table 2 also confirm the intuition that, contrary to what is sugge"
I11-1011,W09-2411,0,0.0934369,"Missing"
I11-1011,doddington-etal-2004-automatic,0,0.0770207,"Missing"
I11-1011,J01-4004,0,0.640849,"(MOO) for coreference, that suggests a family of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features"
I11-1011,S10-1020,1,0.825903,"ts) as the optimization functions). Perhaps the most interesting result of this work is the finding that by working in such a multi-metric space it is possible to find solutions that are better with respect to an individual metric than when trying to optimize for that metric alone—which arguably suggests that indeed both families of metrics capture some fundamental intuition about anaphora, and taking into account both intuitions we avoid local optima. 1 Introduction In anaphora resolution,1 as in other HLT tasks, optimization to a metric is essential to achieve good performance (Hoste, 2005; Uryupina, 2010). However, many evaluation metrics have been proposed for anaphora resolution, each capturing what seems to be a key intuition about the task: from MUC (Vilain et al., 1995) to B3 (Bagga and 1 We use the term ’anaphora resolution’ to refer to the task perhaps most commonly referred to as ’coreference resolution,’ which many including us find a misnomer. For the purposes of the present paper the two terms could be seen as interchangeable. The structure of the paper is as follows. We first review the literature on using genetic algorithms for both single function and multi function opti93 Procee"
I11-1011,P08-4003,1,0.897342,"the system proposed by Soon et al. (2001), commonly used as baseline and relying only on very shallow information. Our reimplementation of the Soon et al. model uses only a subset of features: those marked with an asterisk in Table 1. We also provide in Table 2 typical state-of-the-art figures on the ACE-02 dataset, as presented in an overview by Poon and Domin5 Methods 2 5.1 The choice of the best model and the best machine learner, along with its parameters, is the main direction of our future work. 3 http://sourceforge.net/projects/ carafe The BART System For our experiments, we use BART (Versley et al., 2008), a modular toolkit for anaphora reso96 Table 1: Features used by BART: each feature describes a pair of mentions {Mi , Mj }, i &lt; j, where Mi is a candidate antecedent and Mj is a candidate anaphor MentionType* MentionType Ante Salient MentionType Ante Extra MentionType Ana MentionType2 MentionType Salience FirstSecondPerson PronounLeftRight PronounWordForm SemClassValue BothLocation GenderAgree* NumberAgree* AnimacyAgree* Alias* BetterNames Appositive* Appositive2 Coordination HeadPartOfSpeech SynPos Attributes Relations StringMatch* NonPro StringMatch Pro StringMatch NE StringMatch HeadMatch"
I11-1011,W03-2604,0,0.0420636,"amily of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpo"
I11-1011,P04-1018,0,0.0209417,"to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then proceeded to manually select a subset of fe"
I11-1011,H05-1004,0,0.0543821,"coreference resolution system (i.e., BART) with only these N features. 3. This coreference system is evaluated on the development data. The recall, precision and F-measure values of three metrics are calculated. In case of single objective optimization (SOO), the objective function corresponding to a particular chromosome is the F-measure value of a single metric. This objective function is optimized using the search capability of GA. For MOO, the objective functions corresponding to a particular chromosome are FM U C (for the MUC metric), Fφ3 (for CEAF using the φ3 entity alignment function (Luo, 2005)) and Fφ4 (for CEAF using the φ4 entity alignment function). These three objective functions are simultaneously optimized using the search capability of NSGA-II. 4.3 Genetic Operators In case of SOO, a single point crossover operation is used with a user defined crossover probability, µc . A mutation operator is applied to each entry of the chromosome with a mutation probability, µm , where the entry is randomly replaced by either 0 or 1. In this approach, the processes of fitness computation, selection, crossover, and mutation are executed for a maximum number of generations. The best string"
I11-1011,H05-1068,0,0.69205,"C, CEAF or BLANC ). The results on the SEMEVAL-10 dataset clearly show that existing metrics of coreference rely on different intuitions and therefore a system, optimized for a particular metric, might show inferior results for the other ones. For example, the reported BLANC difference between the runs optimized for BLANC and CEAF is around 10 percentage points. This highlights the importance of the multiobjective optimization (MOO) for coreference, that suggests a family of systems, showing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs."
I11-1011,P02-1014,0,0.158918,"resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then proceeded to manually select a subset of features that did yield better results for the MUC-6/7 datasets. A much larger scale and very systematic effort of manual feature selection over the same dataset was carried out by Uryupina (2007), who evaluated over 600 features. The first systematic attempt at automatic optimization of anaphora resolution we are aware of was carried out by Hoste (2005), who i"
I11-1011,D08-1068,0,0.0358973,"Missing"
I11-1011,M95-1005,0,0.335391,"solutions that are better with respect to an individual metric than when trying to optimize for that metric alone—which arguably suggests that indeed both families of metrics capture some fundamental intuition about anaphora, and taking into account both intuitions we avoid local optima. 1 Introduction In anaphora resolution,1 as in other HLT tasks, optimization to a metric is essential to achieve good performance (Hoste, 2005; Uryupina, 2010). However, many evaluation metrics have been proposed for anaphora resolution, each capturing what seems to be a key intuition about the task: from MUC (Vilain et al., 1995) to B3 (Bagga and 1 We use the term ’anaphora resolution’ to refer to the task perhaps most commonly referred to as ’coreference resolution,’ which many including us find a misnomer. For the purposes of the present paper the two terms could be seen as interchangeable. The structure of the paper is as follows. We first review the literature on using genetic algorithms for both single function and multi function opti93 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 93–101, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP mization. Next, we di"
I11-1011,I05-1063,0,0.0178361,"howing reliable performance according to all the desired metrics. A form of MOO was applied to coreference by Munson et al. (2005). Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. 2 Background: Optimizing for Anaphora Resolution A great number of statistical approaches to anaphora resolution have been proposed in the past ten years. These approaches differ with respect to their underlying models (e.g., mention pair model (Soon et al., 2001) vs. tournament model (Iida et al., 2003; Yang et al., 2005), vs. entity-model (Luo et al., 2004)), machine learners (e.g., decision trees vs. maximum entropy vs. SVMs vs. TiMBL) and their parameters, and with respect to feature sets used. There have been, however, only few attempts at explicit optimization of these aspects, and in those few cases, optimization tends to be done by hand. An early step in this direction was the work by Ng and Cardie (2002), who developed a rich feature set including 53 features, but reported no significant improvement over their baseline when all these features were used with the MUC 6 and MUC 7 corpora. They then procee"
I11-1011,C10-1147,0,0.0142034,"imization of anaphora resolution we are aware of was carried out by Hoste (2005), who investigated the possibility of using genetic algorithms for automatic optimization of both feature selection and of learning parameters, also considering two different machine learners, TiMBL and Ripper. Her results suggest that such techniques yield improvements on the MUC-6/7 datasets. Recasens and Hovy (2009) carried out an investigation of feature selection for Spanish using the ANCORA corpus. These approaches focused on a single metric only; the one proposal simultaneously to consider multiple metrics, Zhao and Ng (2010) still optimized for each metric individually. The effect of optimization on anaphora resolution was dramatically demonstrated by Uryupina’s contribution to SEMEVAL 2010 Multilingual 3 Optimization with Genetic Algorithms In this section, we review optimization techniques using genetic algorithms (GAs) (Goldberg, 1989). We first discuss single objective optimization, that can optimize according to a single objective function, and then multi-objective optimization (MOO), that can optimize more than one objective function, in particular, a popular MOO technique named Non-dominated Sorting Geneti"
I11-1011,versley-etal-2008-bart-modular,1,\N,Missing
I11-1011,S10-1001,1,\N,Missing
I13-1099,S10-1021,1,0.847757,"ing mention detection models: 1. First Model: In our first model we consider each noun phrase(NP) as a possible candidate of mention. Results of this model are shown in Table 1. 2 Brief Description of BART System Architecture 2. Second Model: In our second model we consider each Named Entity (NE) or pronoun (PRP) as a mention and its results are shown in Table 1. Our starting point of anaphora resolution system is the toolkit from (Versley et al., 2008), originally conceived as a modularized version of previous efforts from (Ponzetto and Strube, 2006; Poesio and Kabadjov, 2004; Versley, 2006; Broscheit et al., 2010). BART’s final aim is to bring together stateof-the-art approaches, including syntax-based and semantic features. The state-of-the-art anaphora resolution system, BART has five main components: preprocessing pipeline, mention factory, feature extraction module, decoder and encoder. In addition, an independent language plugin module handles all the language specific information and is accessible from any component. Each module can be accessed independently and thus adjusted to leverage the system’s performance on a particular language or domain. The preprocessing pipeline converts an input docu"
I13-1099,P04-1018,0,0.0659193,"Missing"
I13-1099,H05-1004,0,0.0388236,"ve mentioned feature combinations. Instances are created following (Soon et al., 2001). We generate a positive training instance from each pair of adjacent coreferent markables. Negative instances are created by pairing the anaphor with any markable occurring between the anaphor and the antecedent. During testing, we perform a closest first clustering of instances deemed coreferent by the classifier. Each text is processed from left to right: each In order to evaluate the anaphora resolution system we use different scorers such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, October 2011). We experiment with the different mention detectors for anaphora resolution. Table 4 shows the MUC recall, precision and F-measure values of the system trained using the training data and evaluated using the development data. Experiments were carried out on a high performance computing facility with the following configuration: Dell machine, 216 cores, 2.66 GHZ Intel Xeon processors, 4 GB RAM/core, and 10 TB storage. 7 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow parser.php 819 Mentions NP NE/PRP PER/PRP CRF Classifier recall 52.5"
I13-1099,P02-1014,0,0.290061,"Missing"
I13-1099,P10-1142,0,0.0522234,"Missing"
I13-1099,poesio-kabadjov-2004-general,1,0.899434,"utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; Ng, 2010; Poesio et al., 2010) are mainy in non-Indian lang"
I13-1099,N06-1025,0,0.147589,"ineering, IIT Patna, India, {utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; Ng, 2010; Poesio et al., 2010)"
I13-1099,W12-4501,1,0.883955,"Missing"
I13-1099,J01-4004,0,0.89698,"ot first person then the corresponding feature is also set to high. The feature also behaves in a similar way if the pair (REj , REi ) appears outside the quotation. Preprocessing and Markable Extraction For the anaphora resolution system, mentions are identified from the datasets based on the gold annotations. These are treated as the markables. Thereafter we convert the markables to the data format used by BART, namely MMAX2s standoff XML format. 3.2 Features for anaphora resolution We view coreference resolution as a binary classification problem. We use the learning framework proposed by (Soon et al., 2001) as a baseline. Each classification instance consists of two markables, i.e. an anaphor and its potential antecedent. Instances are modelled as feature vectors and are used to train a binary classifier. The classifier has to decide, given the features, whether the anaphor and the candidate antecedent are coreferent or not. To improve the performance we define some features specific to the language. Given BART’s flexible architecture, we explore the contribution of some features implemented in BART for co-reference resolution in Bengali. Given a potential antecedent REi and a anaphor REj , we c"
I13-1099,P08-4003,1,0.925741,"Computer Science and Engineering, IIT Patna, India, {utpal.sikdar,asif,sriparna}@iitp.ac.in 2 University of Trento, Center for Mind/Brain Sciences, uryupina@unitn.it 3 University of Essex, Language and Computation Group, poesio@essex.ac.uk Abstract Most of these works on supervised machine learning co-reference resolution have been developed for English (Soon et al., 2001; Ng and Cardie, 2002; Yang et al., 2003; Luo et al., 2004), due to the availability of large corpora such as ACE (Walker et al., 2006) and OntoNotes (Weischedel et al., 2008). BART, the Beautiful Anaphora Resolution Toolkit (Versley et al., 2008), (Ponzetto and Strube, 2006), (Poesio and Kabadjov, 2004), is the resultant of the project titled ”Exploiting Lexical and Encyclopedic Resources For Entity Disambiguation” carried out at the Johns Hopkins Summer Workshop 2007. It can handle all the preprocessing tasks to perform automatic coreference resolution. A variety of machine learning approaches are used in BART; it mainly uses several machine learning toolkits, including WEKA, MaxEnt and Support Vector Machine (SVM). Literature shows the significant amount of works in the area of anaphora resolution. But these (Pradhan et al., 2012; N"
I13-1099,M95-1005,0,0.157159,"sion tree learning algorithm (Quinlan, 1993), with the above mentioned feature combinations. Instances are created following (Soon et al., 2001). We generate a positive training instance from each pair of adjacent coreferent markables. Negative instances are created by pairing the anaphor with any markable occurring between the anaphor and the antecedent. During testing, we perform a closest first clustering of instances deemed coreferent by the classifier. Each text is processed from left to right: each In order to evaluate the anaphora resolution system we use different scorers such as MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and BLANC (Recasens and Hovy, October 2011). We experiment with the different mention detectors for anaphora resolution. Table 4 shows the MUC recall, precision and F-measure values of the system trained using the training data and evaluated using the development data. Experiments were carried out on a high performance computing facility with the following configuration: Dell machine, 216 cores, 2.66 GHZ Intel Xeon processors, 4 GB RAM/core, and 10 TB storage. 7 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow parser.php 819 Menti"
I13-1099,P03-1023,0,0.0856957,"Missing"
I17-4031,D14-1181,0,0.0269852,"Tasks, pages 184–193, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP fective in solving the problem. Hence it is challenging to build a generic model that could perform reasonably well acoross different domains and languages. In recent times, the emergence of deep learning methods have inspired researchers to develop solutions that do not require careful feature engineering. Deep Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN) are two very popular deep learning techniques that have been successfully used in solving many sentence and document classification (Kim, 2014; Xiao and Cho, 2016) problems. We aim at developing a generic model that can be used across different languages and platforms for customer feedback analysis. The remainder of our paper is structured as follows: Section 2 offers the related literature survey for customer feedback analysis, where we discuss about the existing approaches. Section 3 describes our two proposed approaches, one based on CNN and the other based on amalgamation of RNN with CNN. Section 4 provides the detailed information about the data set used in the experiment and the experimental setup. Results, analysis and discus"
I17-4031,D14-1179,0,0.006605,"Missing"
I17-4031,W02-1011,0,0.0191959,"Missing"
I17-4031,D14-1162,0,0.0806309,"ack sentences to the same length is useful because it allows us to efficiently batch our data while training. Let a feedback sentence F consisting of ‘n’ words be the input to our model such that x = [x1 , x2 , . . . xn ] where xi is the ith word in the feedback sentence. Each token xi ∈ F is represented by its distributed representation pi ∈ Rk which is the kdimensional word vector. The distributed representation p is looked up into the word embedding matrix W which is initialized either by a random process or by some pre-trained word embeddings like Word2Vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014). We then concatenate (row wise) the distributed representation pi for every ith token in the feedback F and build the feedback sentence representation matrix. The feedback sentence representation matrix p1:n can be represented as: for customer feedback analysis. 3 Network Architecture for Feedback Classification In this section we describe our proposed neural network architecture for feedback classification. We propose two variants, the first one is convolution operation inspired CNN and the second one is the amalgamation of CNN with RNN. 3.1 Feedback classification using CNN In this model a"
I17-4031,D13-1170,0,0.00497594,"Missing"
K18-1012,2010.jeptalnrecital-court.36,0,0.0222723,"al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है"
K18-1012,P17-1055,0,0.0247585,"there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?) (Transliteration: Seattle mai baseball dal ka naam kya hai?) • Q3 :Seattle mein baseball team ka naam kya hai? (Trans: What is the name of the baseball team in Seattle?) All the three questions are same but are asked in English, Hindi and the code-mixed English-Hindi languages. It can be s"
K18-1012,W14-3902,0,0.033183,"ider the following three questions: Related Work Code-mixing refers to the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al."
K18-1012,P06-2025,1,0.666144,"Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion of SQuAD (Rajpurkar et al., 2016) dataset. We translate the English 10 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iii"
K18-1012,D14-1159,0,0.0134896,"et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?) (Transliteration: Seattle mai baseball dal ka naam kya hai?) • Q3 :Seattle mein baseball team ka naam kya hai? (Trans: What is the name of the baseball team in Seattle?) All the three questions are same but are asked in English, Hindi and the code-mixed English-Hindi languages. It can be seen that Q2 and Q3 are similar and share many false cognates (Moss, 1992) [(Seattle, िसएटल), (naam, नाम"
K18-1012,L18-1278,1,0.770245,"ode-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network"
K18-1012,bojar-etal-2014-hindencorp,0,0.19087,"di-English code mixing (CM) dataset. The CMI score of the system generated codemixed questions is 37.22. 5.2 5.3 Experimental Setup for CMQG The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser10 . The Polyglot11 Named Entity Recognizer (NER) (Al-Rfou et al., 2015) is used for named entity recognition. The lexical translation set is obtained by the lexical translation table generated as an intermediate output of Statistical Machine Translation (SMT) training by Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion o"
K18-1012,L18-1440,1,0.895377,"Missing"
K18-1012,W16-6331,1,0.816392,"Missing"
K18-1012,C18-1042,1,0.934699,"ode-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network"
K18-1012,N10-1086,0,0.0460939,"n et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बे"
K18-1012,P02-1040,0,0.10238,"11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq"
K18-1012,R15-1033,0,0.0175163,"the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a)"
K18-1012,D16-1264,0,0.278424,"egate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We evaluate Datasets (CMQA) (1) CM-SQuAD: We generate the CMQA dataset from the portion of SQuAD (Rajpurkar et al., 2016) dataset. We translate the English 10 http://ltrc.iiit.ac.in/showfile.php? filename=downloads/shallow_parser.php 11 http://polyglot.readthedocs.io/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architectur"
K18-1012,P07-2045,0,0.00479329,"HinglishQue dataset is more complex and challenging in comparison to the other Hindi-English code mixing (CM) dataset. The CMI score of the system generated codemixed questions is 37.22. 5.2 5.3 Experimental Setup for CMQG The tokenization and PoS tagging are performed using the publicly available Hindi Shallow Parser10 . The Polyglot11 Named Entity Recognizer (NER) (Al-Rfou et al., 2015) is used for named entity recognition. The lexical translation set is obtained by the lexical translation table generated as an intermediate output of Statistical Machine Translation (SMT) training by Moses (Koehn et al., 2007) on publicly available12 English-Hindi (EN-HI) parallel corpus (Bojar et al., 2014). We aggregate the output probability p(e|h) and inverse probability p(h|e) along with their associated words in both English (e) and Hindi (h) languages. We choose a threshold (5) to filter out the least probable translations. The co-occurrence weight (Dice Co-efficient) is calculated on the available13 n-gram dataset consisting of unique 2, 86, 358 bigrams and 3, 33, 333 unigrams. For Devanagari (Hindi) to Roman (English) transliteration, we use the transliteration system14 based on Ekbal et al. (2006). We eva"
K18-1012,D16-1121,0,0.0792189,"Missing"
K18-1012,C02-1150,0,0.275618,"4.4 Answer-type Focused Answer Extraction The answer-type of a question provides the clues to detect the correct answer from the passage. Consider a code-mixed question Q: Kaun sa Portuguese player, Spanish club Real Madrid ke liye as a forward player khelta hai? (Trans: Which Portuguese player plays as a forward for Spanish club Real Madrid?.) The answer-type of the question Q is ‘person’. Even though the network has the capacity to capture this information up to a certain degree, it would be better if the model takes into account this information in advance while selecting the answer span. Li and Roth (2002) proposed a hierarchical question classification based on the answer-type of a question. Based on the coarse and fine classes of Li and Roth (2002), we train two separate answer-type detection networks on the Text REtrieval Conference (TREC) question classification dataset4 . First, we translate5 5952 TREC English questions into Hindi and thereafter transform the Hindi questions into the code-mixed questions by using our proposed CMQG algorithm. We train the answer-type detection network with code-mixed questions and their associated labels using the technique as discussed in (Gupta et al., 20"
K18-1012,W04-1013,0,0.0112582,"/en/latest/ NamedEntityRecognition.html 12 http://ufal.mff.cuni.cz/hindencorp 13 http://norvig.com/ngrams/ 14 https://github.com/libindic/indic-trans 6 The question formulators are the undergraduate and postgraduate students having good proficiencies in English and Hindi. 7 http://fire.irsi.res.in/fire/2015/home 8 http://ltrc.iiit.ac.in/icon2015/ 9 Please note that these two datasets are not related to QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq network. We evaluate"
K18-1012,E17-3017,0,0.0344471,"QA 124 Figure 2: Proposed CMQA model architecture. The green color column denotes the character embeddings. the performance of CMQG in terms of accuracy, BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) score. 5.4 5.5 Baselines 5.5.1 Baselines (CMQG) We portray the problem of code-mixed question generation with respect to sequence to sequence learning where the input sequence comprises of Hindi question and the output sequence is the codemixed EN-HI question. A seq2seq with attention (Sutskever et al., 2014; Bahdanau et al., 2014) network is trained using the default parameters of Nematus (Sennrich et al., 2017). The training dataset of the pair of Hindi translated question and codemixed questions from CM-SQuAD dataset (c.f. Section 5.2) is used for training the seq2seq network. We evaluate the network on the manually created CMQG dataset (c.f. Section 5.1). Experimental Setup for CMQA CMQA datasets contain the words both in Roman script and English. For English, we use the fastText (Bojanowski et al., 2016) word embedding of dimension 300. We use the Hindi sentences from Bojar et al. (2014), and then transliterate it into the Roman script. These sentences are used to train the word embeddings of dim"
K18-1012,P16-1056,0,0.0215941,"(Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball"
K18-1012,D16-1013,0,0.0661766,"Missing"
K18-1012,D14-1105,0,0.0332379,"de-mixing refers to the mixing of more than one language in the same sentence. Creating resources and tools capable of handling code-mixed languages is more challenging in comparison to the traditional language processing activities that are concerned with only one language. In recent times, researchers have started investigating methods for creating tools and resources for various Natural Language Processing (NLP) applications involving code-mixed languages. Some of the applications include language identification (Chittaranjan et al., 2014; Barman et al., 2014), partof-speech (PoS) tagging (Vyas et al., 2014; Jamatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 201"
K18-1012,D17-1090,0,0.0426014,"Missing"
K18-1012,P17-1018,0,0.372409,"amatia et al., 2015; Gupta et al., 2017), question classification (Raghavi et al., 2015), entity extraction (Gupta et al., 2018a, 2016b), sentiment analysis (Rudra et al., 2016; Gupta et al., 2016a) etc. Developing QA system in a code-mixed scenario is, itself, very novel in the sense that there have not been very significant attempts towards this direction, except the few such as (Chandu et al., 2017). Our literature survey shows that the existing methods of question generation (general) include both rules Heilman and Smith (2010); Ali et al. (2010) and machine learning (Serban et al., 2016; Wang et al., 2017a) techniques. A joint model of question generation and answering based on sequenceto-sequence neural network model is proposed in (Wang et al., 2017a). In recent times, there have been several studies on deep learning based reading comprehension/ QA (Hermann et al., 2015; Cui et al., 2017; Shen et al., 2017; Wang et al., 2017b; Gupta et al., 2018c; Wang and Jiang, 2016; Berant et al., 2014; Maitra et al., 2018; Cheng et al., 2016; Trischler • Q1 : What is the name of the baseball team in Seattle? • Q2 : िसएटल म बेसबॉल दल का नाम ा है? (Trans: What is the name of the baseball team in Seattle?)"
L16-1429,bakliwal-etal-2012-hindi,0,0.042434,"ned level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to th"
L16-1429,D11-1100,1,0.867467,"Missing"
L16-1429,C12-2008,1,0.673646,"Missing"
L16-1429,S14-2135,0,0.0453619,"Missing"
L16-1429,W10-3207,0,0.0309648,"of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to the non-availability of benchmark datasets and the scarcity of various ot"
L16-1429,P97-1023,0,0.142173,"Neg-negative, Neu-Neutral and Con-Conflict text analytic problems. The classifier is trained with the following features: 1. Target aspect term and local context: Sentiment bearing words usually occur closer to the target aspect term. We extract target term along with its preceding and following few tokens, and use as features for training. For the proposed method we fix context window size to 5. 2. Word Bigrams: Pair of two consecutive tokens in the local context are used as features to capture the cooccurrence behavior of the tokens. 3. Semantic Orientation (SO): Semantic Orientation (SO) (Hatzivassiloglou and McKeown, 1997) is a measure of association of a token towards positive or negative sentiments and can be defined as: SO(t) = P M I(t, posRev) − P M I(t, negRev) (2) where P M I(t, posRev) stands for point-wise mutual information of a token t towards positive sentiment reviews. 4. Experiments and Evaluation As a base learning algorithm we make use of Conditional Random Field (CRF) and Support Vector Machine (SVM) for the aspect term extraction and sentiment classification tasks respectively. We use CRF++ 4 and TinySVM 5 based packages for our experiments. 4 5 http://taku910.github.io/crfpp/ http://chasen.org"
L16-1429,W13-4306,0,0.0121323,"ect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages mainly discuss the problems of sentiment analysis at the coarse-grained level with the aims of classifying sentiments either at the sentence or document level. In this work we describe our research on aspect based sentiment analysis in Hindi. Hindi is the national language in India, and ranks 5th in the world in terms of speaker population. As we already mentioned, the bottleneck for performing sentiment analysis involving Hindi is again due to the non-availability of"
L16-1429,S14-2004,0,0.141815,"Missing"
L16-1429,S14-2038,0,0.0110963,"raction focuses on identifying various terms that denote aspects, and the second step, i.e. sentiment classification deals with classifying the sentiments with respect to the aspect. A review sentence, therefore, may contain more than one aspect term and the sentiment associated with each. Such a fine-grained analysis provides greater insight to the sentiments expressed in the written reviews. In recent times, there have been a growing trend for sentiment analysis at the more fine-grained level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das"
L16-1429,S14-2036,0,0.00618632,"rms that denote aspects, and the second step, i.e. sentiment classification deals with classifying the sentiments with respect to the aspect. A review sentence, therefore, may contain more than one aspect term and the sentiment associated with each. Such a fine-grained analysis provides greater insight to the sentiments expressed in the written reviews. In recent times, there have been a growing trend for sentiment analysis at the more fine-grained level, i.e. for aspect based sentiment analysis (ABSA). Some of the recent systems that have emerged are (Toh and Wang, 2014; Chernyshevich, 2014; Wagner et al., 2014; Castellucci et al., 2014; Gupta et al., 2015). However, almost all these research are related to some specific languages, especially the English. Sentiment analysis in Indian languages are still largely unexplored due to the non-availability of various resources and tools such as annotated corpora, lexicons, Part-ofSpeech (PoS) tagger etc. Existing works (Joshi et al., 2010; Balamurali et al., 2012; Balamurali et al., 2011; Bakliwal et al., 2012; Mittal et al., 2013; Sharma et al., 2014; Das and Bandyopadhyay, 2010b; Das and Bandyopadhyay, 2010a; Das et al., 2012) involving Indian languages"
L16-1595,E14-4002,1,0.6516,"014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. Linking cognitive features to word senses: Several attempts have been made in both computational linguistics and cognitive science that build resources linking words with several cognitive features such as abstractness-correctness (Coltheart, 1981), sentiment (Esuli and Sebastiani, 2005a), imageability (Coltheart, 1981), and colors (Özbal et al., 2011). There are two prior approaches that attempt to understand the underlying temporal orientation of word senses. In (Hasanuzzaman et al., 2014; Dias et al., 2014), authors developed TempoWordNet (TWn), an extension of WordNet, where each synset is augmented with its temporal connotation (past, present, future, or atemporal). It mainly relies on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. To the best of our knowledge, this is the first attempt to associate temporality to word senses in Hindi. 3. Task Definition and Challenges WordNet, the task is to identify temporal information associated with each of the syn"
L16-1595,S07-1014,0,0.0178227,"ier, in general, performs remarkably well while we deal only with two classes, namely temporal and atemporal. However, results are not up to the mark while we attempt to perform classification with all the five classes (four temporal and one atemporal classes). Too stringent gold standard test set which hardly represents the training set might be one of the possible reasons for this. Training set does not cover the instances of all types of temporal classes. The use of more temporallyrich corpus such as newspaper text, narratives, etc. may be more useful as shown in the TempEval shared tasks (Verhagen et al., 2007). 6.3.1. Error Analysis We perform error analysis both from the quantitative and qualitative perspectives. One step classification: Confusion matrix showing the possible errors of the one-step classification framework is shown in Table 13. It shows that there are many false negatives, i.e. many temporal instances are 3757 Bigram representation precision recall F-measure Past 1.0 0.25 0.4 Present 1.0 0.3 0.461 Future 0.334 0.05 0.086 Neutral 0.303 1.0 0.465 Avg 0.659 0.4 0.497 Table 12: Results of second step of two-step classification framework: Bigram model on gold standard test set experimen"
L18-1049,N15-1132,1,0.92024,"spectively. We find inter-annotator multi-rater kappa agreement (Fleiss, 1971) of 0.80. We develop two models based on rules and supervised machine learning. 3.1. Rule-based Approach We define a set of generic rules which we apply for determining the temporal sense of any sentence for both Twitter and Newswire text. We apply the same set of rules for the following two cases: (i). Temporal sense of each word sense in the sentence is detected using our temporal resource. The most suitable sense of each word in the sentence is determined using an unsupervised Most Frequent Sense (MFS) algorithm (Bhingardive et al., 2015). (ii). We identify the tense of each word in a sentence using a Hindi Morphological Analyzer.7 Verbs with the tense information (past, present or future) are used for developing the rulebased system. We depict the rules in Algorithm 2. 7 Algorithm 2 Basic Steps of Rule-based Approach. 1: If majority words in a sentence belong to a particular temporal/tense category t then label it as t. 2: If the words in the sentence are equally distributed among the three classes then 2.1. Label the sentence as present if the classes are only past and present; 2.2. Label the sentence as future if the classe"
L18-1049,E14-4002,0,0.0175601,"poral word senses. The process initiates learning with a set of seed instances for each class, and then iteratively expands it following various expansion strategies. The temporal resource, Tempo-Hindi-WordNet that we build will definitely be an effective resource for the efficient temporal information access in the resource-poor languages like Hindi which is one of the widely spoken languages worldwide and one of the official languages in India. We show how this resource can be utilized for sentencelevel temporal tagging. Our present study is inspired from the prior works (Dias et al., 2014; Hasanuzzaman et al., 2014), where the authors attempted to annotate each synset of English WordNet with four temporal dimensions, namely past, present, future and atemporal. Our work differs from these existing works in terms of the following points: (i). present work attempts to build a temporal resource that can facilitate temporal information access in Hindi; (ii). new expansion strategies including word-embedding based techniques are proposed; and (iii). two approaches (i.e. rule-based and machine learning-based) for sentence-level temporality detection are developed. The present work also differs from an earlier w"
L18-1049,S13-2001,0,0.0353591,"यवःथा म भारत के लए अवसर ा है? (DijiTala arthavyavasthA meM bhArata ke lie avasara kyA hai?What are the opportunities for India in the Digital Economy?)”, “अशोक का इितहास (ashoka kA itihAsa- History of Ashoka)” need future and past related information, respectively. Here, tense related information does not help but the implicit temporal keywords ‘current’, ‘opportunities’ and ‘history’ help in finding the temporal information of the respective queries. 1.1. Motivation and Problem Definition Most of the earlier studies, for example, TempEval tasks (Verhagen et al., 2009; Verhagen et al., 2010; UzZaman et al., 2013) in the computational linguistics, have concentrated on identifying the temporal expressions, event expressions and various relations among these. These studies tried to address the temporal aspects of information with the help of linguistic constructs such as the presence of temporal expressions like before, now, after etc., document creation time (DCT), or explicit time expressions. Let us consider the following two example sentences: Sentence-I: You should live in the present. Sentence-II: She gave him a nice present. When these two sentences are subjected as input to the SUTime tagger,2 we"
L18-1049,bojar-etal-2014-hindencorp,0,0.0129835,"tic relations such as hypernym and hyponym detect connotative temporality as hypernym is a generalization and hyponym is a specialization of the synset. For example, “ वराम_काल (virAma kAla-rest period)” is the kind of “काल (kAla-period)”. Here, “काल (kAla-period)” is the hypernym and “ वराम_काल (virAma kAla- rest period)” is a hyponym. Both of these indicate temporality. As we encode both hyponyms and hypernyms, one’s presence ensures other’s inclusion through expansion. We use Word2vec tool (Mikolov et al., 2013) for generating word embedding vectors. The model is trained on Bojar’s corpus (Bojar et al., 2014) of around 44 million Hindi sentences for the training of Word2Vec using Skip-gram model with the dimension set to 200 and window size set to 7. For each content word of the synset, hyponyms, hypernyms and their glosses, we extract the corresponding vector of 200 dimension. All these vectors are averaged over to create a ‘prototype vector’. If there are m content words then the prototype vector is generated as shown in the Equation 1. ∑m i=1 W E(wi ) (1) m where, m is the number of content words in the glosses of synset, synonyms, hypernyms and hyponyms; WE(wi ) is the word embedding vector of"
L18-1049,L16-1595,1,0.886829,"Missing"
L18-1278,A97-1029,0,0.642534,"also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for entity extraction involving Indian languages was introduced in FIRE-2016 shared 1762 task (Rao and Devi, 2016). Some of the challenges for entity extraction in a code-mixed environment are as follows: • The dataset contains tweets uttera"
L18-1278,D14-1179,0,0.00987688,"Missing"
L18-1278,C96-1079,0,0.431751,"nt code-mixing in Indian languages. Hence, Indians are multilingual by adaptation and necessity, and frequently change and mix languages while writing in social media platforms. These pose additional difficulties in building automated tools for social media analytics. Named Entity Recognition (NER) is a primary task in information extraction. It aims at identifying the names of entities and classifying them into some predefined categories such as people, location, organization and product. This task can also be thought of as a two-step process, viz. entity detection and entity classification (Grishman and Sundheim, 1996). There are a significantly large body of works existing in Indian languages, but these are mostly related to the domains such as newswire. Nowadays, information extraction in microblogs has become an active research topic (Cano Basave et al., 2013), following the early experiments which showed this genre to be extremely challenging for the state-of-theart algorithms (Derczynski et al., 2015; Bontcheva et al., 2014). The shortness of micro-blogs makes them hard to interpret. The social media text normally carries less discourse information per document, and threaded structure is fragmented acr"
L18-1278,P11-1038,0,0.0284999,"rom this, short text (tweets) also exhibits more language variations, tend to be less grammatical than the longer posts, contains unorthodox capitalization, and makes use of frequent abbreviations, hashtags and emoticons. These information also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for"
L18-1278,C02-1054,0,0.165258,"d on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts involving Indian language have been reported in FIRE-2015 workshop (Rao et al., ). In recent times, a benchmark setup for entity extraction involving Indian languages was introduced in FIRE-2016 shared 1762 task (Rao and Devi, 2016). Some of the challenges for entity extraction in a code-mixed environment are as follows: • The dataset contains tweets utterance in code mixed as well as in mono-lingual text. • Introduction of a diverse set of entities, not limited to the only tra"
L18-1278,D14-1162,0,0.0823198,"Missing"
L18-1278,D11-1141,0,0.0676835,"carries less discourse information per document, and threaded structure is fragmented across multiple documents. Apart from this, short text (tweets) also exhibits more language variations, tend to be less grammatical than the longer posts, contains unorthodox capitalization, and makes use of frequent abbreviations, hashtags and emoticons. These information also participates to decide the intention or meaning of a short text. To combat these problems, researcher has focused on microblog-specific information extraction algorithms, e.g. NER on Twitter data using Conditional Random Field (CRF) (Ritter et al., 2011) or hybrid methods (Van Erp et al., 2013). Particular attention is given to microtext normalization (Han and Baldwin, 2011), as a way of removing some of the linguistic noise prior to Part-of-Speech (PoS) tagging and NER. Several Machine Learning (ML) techniques have already been applied for the NER tasks such as Hidden Markov Model(HMM) (Bikel et al., 1997), Maximum Entropy (Borthwick, 1999; Kumar and Bhattacharyya, 2006), Support vector Machine (SVM) (Isozaki and Kazawa, 2002), Conditional Random Field (CRF) (Li and McCallum, 2003) etc. Few systems for entity extraction in social media texts"
L18-1440,D14-1179,0,0.00754919,"Missing"
L18-1440,P05-1045,0,0.0525458,"d between the query and candidate answers. ∑ VEC(ti ) × tf-idfti VEC(X) = ti ∈X (3) number of look-ups where X is query q or candidate answer sentence S, VEC(ti ) is the word vector of word ti . number of lookups represents the number of words in the question for which pre-trained word embeddings8 are available. Candidate Answer Extraction This depends on the output of question classification. For factoid question, the coarse class and finer class guide this stage to extract the appropriate entities from the candidate passage(s). We tag the candidate passage with Stanford named entity tagger (Finkel et al., 2005). We utilize the coarse class and finer class of a question to extract the suitable candidate answers. For a descriptive question, candidate answers are extracted by segmenting the relevant passage. 4.5. 2. Proximity score (PS): It calculates the length of the shortest span that covers the query contained in the candidate answer sentence. This is again normalized in the same way. Passage Retrieval The candidate passage that contains the answer(s) to the given question(s) are extracted in this stage. We exploit the Lucene’s text retrieval functionality to retrieve passage. It retrieves and rank"
L18-1440,H01-1069,0,0.304621,"Missing"
L18-1440,D14-1181,0,0.0133982,"plying that only candidate answers that are dates need to be considered. With the recent developments in deep learning, neural network models have shown promise for QA. Deep neural network being perform exceptionally well in other NLP problem. Inspired by the success of deep neural network we adapt neural network architecture to develop our question classification model. Our question classification model is based on CNN and RNN. The model comprises of Question embedding layer, Convolution layer, Recurrent layer, Softmax classification layer. Our question classification model is inspired from (Kim, 2014) and (Xiao and Cho, 2016). The input to the model is an English question. Now we describe each component of the model: Question (English): Why did Alexander marched back in 325 BC? Question (Hindi): अले जडर 325 ईसा पूव म चला गया? Answer (English): After Alexander’s last major victory in India as his forces refused to go any further. They were too tired to carry on with the Alexander’s expedition and wanted to return home. Moreover, the might of Magadhan Empire (the Nanda Rulers) also dissuaded them. Alexander marched back in 325 BC after making necessary administrative arrangement for the conq"
L18-1440,C02-1150,0,0.071864,"ge Source Preparation In this step, an information source (articles) from which answers are to be derived was set-up. We translate Hindi questions and articles into English by Google Translate5 . The complete English articles are indexed at passage level using inverted indexing mechanism. We use the Lucene6 implementation of inverted indexing. 4.2. Question Processing: The question processing (QP) step is responsible for analyzing and understanding the questions posed to the QA system. We perform question classification with the question 2 Tourism (EN):www.india.com/travel classes proposed by Li and Roth (2002). Question class proTourism (HI): https://hindi.nativeplanet.com Diseases (EN, HI): https://simple.wikipedia.org/wiki/List_of_diseases, vides us the semantic constraint on the sought-after answer. We propose a deep learning based question classification rest of the domains are curated from http://www.jagranjosh.com/ 3 The annotators are equally proficient in both the languages Total of 7120 questions (English+Hindi) for which the answer exists in either of two language documents. 4 2779 5 6 https://translate.google.com https://lucene.apache.org/ Domains Tourism History Diseases Geography Econo"
L18-1440,W04-1013,0,0.02059,"Missing"
L18-1440,forner-etal-2010-evaluating,0,0.0235479,"s about the difficulty levels associated. For better understanding and thorough analysis of various answer types, similar to Rajpurkar et al. (2016) and Trischler et al. (2016), we categorize the answers of factoid questions into 8 entities and phrases. Statistics of the answer types for English and Hindi QA pairs are provided in Table 7. An example of QA pairs formulated from a comparable articles is given in Table 1. Some examples of short descriptive QA pair from our dataset are given in Table 5. The direct comparison of our dataset with the Cross-Language Evaluation Forum (CLEF) datasets (Pamela et al., 2010) is not possible because we have created question answers pair in both language (MQA) in contrast the CLEF dataset have the question and answer pair in the different languages. However, we have shown the comparison in various terms as shown in Table 6. 4. Evaluation: Proposed Approach We develop a translation based approach for multilingual QA. As English is a resource-rich language, we translate Hindi question and articles into English. Our proposed model comprises of Knowledge Source Preparation, Question Processing and Answer Extraction, We describe the details of each component in the foll"
L18-1440,P02-1040,0,0.107844,"Missing"
L18-1440,D16-1264,0,0.0874729,"Missing"
L18-1440,W06-1908,0,0.0166324,"mplemented the web based Hindi question answer. In this work the question and answer deal with only Hindi language, if the answer was not presented in Hindi document then it was retrieved from Google. (Sekine and Grishman, 2003) proposed a question answering system for Hindi and English. The questions were created in Hindi language and the answers retrieved from Hindi newspaper in the Hindi language. These answers were then converted into the English language. In this work, an English Hindi bilingual dictionary was used to find top 20 Hindi articles which were used to find candidate answers. (Reddy and Bandyopadhyay, 2006) proposed question answering system in the Telugu language. The system was dialogue based and railway specific domain. The architecture was based on the keyword approach. The query analyzer generates the tokens and keywords. From tokens, SQL statements were generated. Using SQL query the answer was retrieved from the database. (Reddy and Bandyopadhyay, 2006) develop the question answer system in English and Punjabi language. In this work a pattern and matching algorithm was introduced to retrieve the most relevant appropriate answer from multiple sets of answers for a given question. 3. Resour"
L18-1440,N03-1033,0,0.124428,"te gate, reset get and new memory content, respectively. ci is the convolution output at time t.The final output of recurrent layer h is obtained as the concatenation of the last hidden state of forward and backward hidden states. • Softmax classification layer: Finally, the fixeddimensional vector h is fed into the softmax classification layer to compute the predictive probabilities for all the question classes (coarse or fine). 4.2.2. Query Formulation In order to form the query, we remove all the stop word, punctuation symbol from the question. We tag the question with Stanford PoS tagger (Toutanova et al., 2003). Then we concatenate all the noun, verb and adjective in the same order in which it appears in the question. 4.3. 3. N-Gram coverage score (NS): We compute the ngram coverage till n = 4. Finally, the n-gram score between a query (q) and a candidate answer sentence (S) is calculated based on the following formula. ∑ ng ∈S N GCoverage(q, S, n) = ∑ n Countcommon (ngn ) Countquery (ngn ) (1) n ∑ N GCoverage(q, S, i) ∑n N GScore(q, S) = (2) i=1 i i=1 ngn ∈q 4. Semantic Similarity Score (SS) : Query and candidate answer are represented using the semantic vectors. Cosine similarity is then computed"
L18-1442,D14-1181,0,0.00338329,"ng is fed as the input to the convolutional layer where filter F ∈ Rm×k is convoluted to the context window xi:i+m−1 of h words for each blog-post as follows. In this section we have presented the approach developed for extracting sentiments of users’ posts in medical blogs. 5.1. Network for Identifying Severity Level In this section we propose a method based on CNN that exploits sentiments from health forums (or, medical blogs) in augmentation layer. As presented in Figure-1, the proposed model has four different components which are similar to the conventional CNN components as proposed by (Kim, 2014). The first layer represents the input layer which takes a complete blog post in the form of vector representation (word embedding) and outputs a probability corresponding to the classification types. We use max-pooling over the whole blog post to obtain global features through all the filters. This pooled feature is fed into the fully connected neural network. In the output layer, we use the softmax classifier to automatically classify the post into three out(1) ci = f (F.xi:i+m−1 + b) (2) where f is non-linear function4 and b is a bias term. The feature map f is generated by applying given f"
L18-1442,W10-1915,0,0.0176194,"onducted by Biyani et al. (2013) used online cancer community user data to determine the polarity. They have adapted supervised machine learning techniques using hand-crafted features, which cover both domain-dependent as well as domain-independent features. They identified sentiments on two discourse functions such as expressive and persuasive. A supervised machine learning model (multi-nominal naive Bayes) is developed using frequency-based features. • Adverse Drug Relation: For medical domains, social media texts (corresponding to medical forums) have been utilized in the works such as DS (Leaman et al., 2010; Nikfarjam and Gonzalez, 2011; Liu and Chen, 2013), MedHelp (Yang et al., 2012) and PatientsLikeMe (Wicks et al., 2011). Non-medical social media forums like Twitter (Nikfarjam et al., 2015) have been exploited to capture adverse drug effect. With the availability of the extensive Adverse Drug Reaction (ADR) lexicons such as Side Effect Resource (SIDER)2 (Kuhn et al., 2010), Coding Symbols for a Thesaurus of Adverse Reaction Terms (COSTART), Consumer Health Vocabulary (CHV) (Zeng-Treitler et al., 2008) and Medical Dictionary for Regulatory Activities (MedDRA) (Mozzicato, 2009), some prominent"
L18-1442,R11-1019,0,0.020062,"Missing"
L18-1442,R13-1083,0,0.0439844,"Missing"
L18-1559,P14-5010,0,0.00376227,"of few news sources : www.ndtv.com, indianexpress.com, timesofindia.indiatimes.com, indiatoday.intoday.in, thehindu.com, news18.com, firstpost.com, dnaindia.com, deccanchronicle.com, financialexpress.com, business-standard.com, sify.com, newskerala.com, mid-day.com, thedailystar.net, theweek.in, tribuneindia.com 3543 id, event id, event name, category, Document Level Annotation (DLA), number of words and sentences. We develop a semi-automatic meta file generator interface where attribute values are automatically captured from the hierarchically organized data (See Figure 2). Stanford CoreNLP (Manning et al., 2014) integrated with our interface gave us the field values for sentence and word count. We asked our annotators to provide their judgments for the DLA attribute based on the guidelines specified in the next section. 5.6. Annotation Three annotators with post-graduate level knowledge in English were involved in labeling the TAP-DLND 1.0 target documents. Having read the source document(s) we asked the annotators to annotate an incoming on-event document as non-novel or novel solely based on the information coverage in the source documents. The annotation guidelines were simple: 1. To annotate a do"
L18-1559,H05-1014,0,0.33244,"lan, 2002) evaluation campaigns where the concern was to detect new event from online news streams. Although the intention was to detect the first story or reporting of a new event from a series of news stories, the notion of novelty detection from texts came into light for the research community. Some notable approaches for New Event Detection with the TDT corpus are by (Allan et al., 1998; Yang et al., 2002; Stokes and Carthy, 2001; Franz et al., 2001; Yang et al., 1998; Allan et al., 2000; Brants et al., 2003). However, the Novelty track in Text Retrieval and Evaluation Conferences (TREC) (Soboroff and Harman, 2005) were the first to explicitly explore the concept of Novelty Detection from texts. Under the paradigm of information retrieval, given a query, the TREC experiments were designed to retrieve relevant and novel sentences from a given collection. Some notable approaches for sentence level novelty detection from the TREC exercises are by (Allan et al., 2003; Kwee et al., 2009; Li and Croft, 2005; Zhang et al., 2003; Collins-Thompson et al., 2002; Gabrilovich et al., 2004; Ru et al., 2004). Textual Entailment based sentence level novelty mining was explored in the novelty subtask of Recognizing Tex"
L18-1559,H01-1030,0,0.15739,"texts has been carried out at three levels : event level, sentence level and document level. Research in novelty mining could be traced back to the Topic Detection and Tracking (TDT) (Allan, 2002) evaluation campaigns where the concern was to detect new event from online news streams. Although the intention was to detect the first story or reporting of a new event from a series of news stories, the notion of novelty detection from texts came into light for the research community. Some notable approaches for New Event Detection with the TDT corpus are by (Allan et al., 1998; Yang et al., 2002; Stokes and Carthy, 2001; Franz et al., 2001; Yang et al., 1998; Allan et al., 2000; Brants et al., 2003). However, the Novelty track in Text Retrieval and Evaluation Conferences (TREC) (Soboroff and Harman, 2005) were the first to explicitly explore the concept of Novelty Detection from texts. Under the paradigm of information retrieval, given a query, the TREC experiments were designed to retrieve relevant and novel sentences from a given collection. Some notable approaches for sentence level novelty detection from the TREC exercises are by (Allan et al., 2003; Kwee et al., 2009; Li and Croft, 2005; Zhang et al., 2"
N18-1053,C16-1047,1,0.712217,"of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in resource-poor scenario due to the absence of sufficient amount of corpus. In this work, we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM)"
N18-1053,P97-1023,0,0.380911,"tation of one word (जाता है |jAtA hai) is missing in Hindi embedding we can still find its representation in English through its translation i.e. ‘goes’. Bilingual embedding also helps in addressing the spelling variation cases. For e.g. two differently spelled words in Hindi such as ‘कि बनेशन |kambineshana’ and ‘कंबीनेशन |kaMbIneshana’ translate to an English word ‘combination’. We repeat the above process for English-French language pair to obtain two (English and French) word2vec models. We also released computed bilingual word embeddings for the research commu3. Semantic Orientation (SO) (Hatzivassiloglou and McKeown, 1997): Semantic orientation defines the association of a word w.r.t. its positivity and negativity. Semantic orientation (SO) of a word is the difference of point-wise mutual information of a word w in positive and negative reviews. We calculate the SO score of each word in the context window of size ±5 and take the cumulative SO score as the feature value. 3.3 Cross-lingual and Multi-lingual Setups We evaluate our proposed approach for two setups i.e. multi-lingual and cross-lingual setups. In multi-lingual setup, the proposed model is trained and evaluated on datasets of the same language i.e. Hi"
N18-1053,baccianella-etal-2010-sentiwordnet,0,0.0354141,"Missing"
N18-1053,bakliwal-etal-2012-hindi,0,0.0120045,"econd problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation"
N18-1053,W16-4307,0,0.0151257,"h as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has s"
N18-1053,C12-2008,1,0.91976,"lassification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The"
N18-1053,C04-1200,0,0.319706,"i.e. conflict) of the sentence ignoring critical information such as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-t"
N18-1053,C16-1152,0,0.113331,"hammad et al., 2016). 2. Approach: System (Akhtar et al., 2016a) defines classical feature driven approach while the system (Barnes et al., 2016) utilized bilingual word embeddings as feature values to train a Support Vector Machine (SVM) classifier. Rest of the systems (Akhtar et al., 2016b; Singhal and Bhattacharyya, 2016) (including the proposed one) are based on deep neural network architecture. However, the techniques employed are very much different. Akhtar et al. (2016b) is a CNN-SVM based system with the assistance of multi-objective optimized features, while Singhal and Bhattacharyya (2016) is a CNN based system that translate the source language texts into target language text (English) for training and evaluation. In comparison, our proposed method employ LSTM to solve the data sparsity problem in both multi-lingual as well as crosslingual setups. 6. Hand-crafted Features: The proposed system employs much richer set of lexicon based features than that of (Singhal and Bhattacharyya, 2016). Also, we do not augment polar words in the training instances as done in (Singhal and Bhattacharyya, 2016), rather we use sentiment scores of these lexicons as features themselves in the trai"
N18-1053,P07-2045,0,0.00578476,"ar et al., 2016b) employed mono-lingual word embeddings for training and evaluation. 3.1 Bilingual Word Embedding We employ bilingual word embeddings (Luong et al., 2015) trained on a parallel English-Hindi (and English-French) corpus. We generate a parallel corpus for Amazon product review datasets2 (consisting of approx. 7.2M sentences) using an in-house product review domain based English→Hindi (English→French) Statistical Machine Translation (SMT) system (English→Hindi: 39.5 BLEU score and English→French: 37.9 BLEU score). We employ widely used and standard machine translation tool Moses (Koehn et al., 2007) to train the phrasebased SMT system. The alignment information are obtained from the mosesdecoder (Koehn et al., 2007) during translation of the reviews. The parallel corpus along with the alignment information are used to train two (English and Hindi) 5. Data Sparsity: The system of (Akhtar et al., 2016b) does not address the problem of data sparsity, while our proposed system tries to minimize the effect of data sparsity. Our proposed system tackles the data sparsity problem by replacing the OOV word with its translated form which usually happens to be its closest neighbor in the shared vec"
N18-1053,W15-1521,0,0.0713966,"Missing"
N18-1053,P02-1053,0,0.0262187,"ll sentiment (i.e. conflict) of the sentence ignoring critical information such as food and service qualities. However, ABSA will first identify all the aspects in the text (i.e. food and service) and then associate positive with food and negative with service. Identification of aspect terms is also known as aspect term extraction or opinion target extraction. In this work, we focus on the second problem i.e. aspect level sentiment classification. Literature survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more c"
N18-1053,P05-1015,0,0.171748,"ource-poor scenario due to the absence of sufficient amount of corpus. In this work, we propose to minimize the effect of data sparsity by leveraging bilingual word embeddings learned through a parallel corpus. We train and evaluate Long Short Term Memory (LSTM) based architecture for aspect level sentiment classification. The neural network architecture is further assisted by the handcrafted features for the prediction. We show the efficacy of the proposed model against stateof-the-art methods in two experimental setups i.e. multi-lingual and cross-lingual. 1 Introduction Sentiment analysis (Pang and Lee, 2005) tries to automatically extract the subjective information from a user written textual content and classifies it into one of the predefined set of classes, e.g. positive, negative, neutral or conflict. Sentiment analysis performed on coarser level (i.e. document or sentence level) does not provide enough information for a user who is critical of finer details such as battery life of a laptop or service of a restaurant etc. Aspect level sentiment analysis (ABSA) (Pontiki et al., 2014) serves such a purpose, which first identifies the features (or aspects) mentioned in the text and then classifi"
N18-1053,P06-1134,0,0.145071,"Missing"
N18-1053,D14-1162,0,0.0853515,"Missing"
N18-1053,P16-1133,0,0.0362521,"Missing"
N18-1053,S14-2004,0,0.377995,"e-art methods in two experimental setups i.e. multi-lingual and cross-lingual. 1 Introduction Sentiment analysis (Pang and Lee, 2005) tries to automatically extract the subjective information from a user written textual content and classifies it into one of the predefined set of classes, e.g. positive, negative, neutral or conflict. Sentiment analysis performed on coarser level (i.e. document or sentence level) does not provide enough information for a user who is critical of finer details such as battery life of a laptop or service of a restaurant etc. Aspect level sentiment analysis (ABSA) (Pontiki et al., 2014) serves such a purpose, which first identifies the features (or aspects) mentioned in the text and then classifies it into one of the target classes. For example, the following review is for a restaurant where the writer shares her/his experience. Though s/he likes the food but certainly not happy with the service. One of the best food we had in a while but the service was very disappointing. 2 Motivation and Problem Definition Indian languages are resource-constrained in nature as there is a lack of ready availability of different qualitative lexical resources and tools. In a supervised machi"
N18-1053,C16-1287,1,0.919113,"survey suggests a wide range of research on sentiment analysis (at the document or sentence level) is being carried out in recent years (Turney, 2002; Kim and Hovy, 2004; Jagtap and Pawar, 2013; Poria et al., 2016; Kaljahi and Foster, 2016; Gupta et al., 2015). However, most of these researches are focused on resource-rich language like English. Like many other Natural Language Processing (NLP) problems, research on sentiment analysis involving Indian languages (e.g. Hindi, Bengali etc.) are very limited (Joshi et al., 2010; Bakliwal et al., 2012; Kumar et al., 2015; Balamurali et al., 2012; Singhal and Bhattacharyya, 2016). Due to the scarcity of various qualitative resources and/or tools in such languages, the problems have become more challenging and non-trivial to solve. The research on ABSA involving Indian languages has started only very recently, for e.g. (Akhtar et al., 2016a,b). Efficient word representations play an important role in solving various problems related to Natural Language Processing (NLP), data mining, text mining etc. The issue of data sparsity poses a great challenge in creating efficient word representation model for solving the underlying problem. The problem is more intensified in re"
N18-1061,P11-2008,0,0.0727691,"Missing"
N18-1061,P06-4018,0,0.0411422,"tional LSTMs (Schuster and Paliwal, 1997) train two LSTMs, instead of one, on the input sequence. The first on the input sequence and the second on a reversed copy of the input sequence. It is designed to capture information of the sequential dataset and maintain the contextual features from the past and the future. This can provide an additional context to the network and result in faster and even fuller learning on the problem without keeping the redundant context information. 3.2 Sentiment View of Temporal Orientation We use an existing sentiment classifier available with the NLTK toolkit (Bird, 2006) to classify the user-level tweets into positive, negative or neutral.3 Sentiment is added at the fine-grained level of the temporal orientation. Given the tweets of a user, the sentiment view of temporal orientation of that user is defined by the following equation: orientations,t (user) = |tweetss/t (user)| (1) |tweetst (user)| where, (t ∈ { past, present, or future}), and (s ∈ { positive, negative, or neutral}), in equation (1). Here, we first classify each user’s tweet into the past, present or future temporal category. Then for each temporal category, we find the percentage of each sentim"
N18-1061,chang-manning-2012-sutime,0,0.0350988,"Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and normalize time mentions in the texts. Although interesting results have been reported (UzZaman et al., 2013), but the coverage is limited to the finite number of rules they implement. The time perspective and its importance in various social science and psychological studies is well established in literature. It plays a fundamental role in our interpersonal relation influenced by cognitive process (Zimbardo and Boyd, 2015). • We introduce the sentiment dimensions in the human temporal orientation to infer the social media users’ psycho-demographic attributes on a large-scale."
N18-1061,D14-1121,0,0.0818986,"Missing"
N18-1061,P14-5010,0,0.00412872,"ucation, intelligence, optimism, and relationship using a linear regression (LR) classifier (Neter et al., 1996). 4 the filtering method, we filter out the tweets which do not contain a verb. The verb part-of-speech tag is determined using the CMU tweet-tagger (Gimpel et al., 2011). In the second pass of the filtering method, we removed the tweets having tense as past from the tweets of the present and future events. The CMU tweet-tagger does not provide verbs in different sub-categories. For this reason, we also retrieve the Part-of-Speech (PoS) tag information from the Standford PoS-tagger (Manning et al., 2014) for all the tweets to get the subcategories of verb (i.e. VB, VBD, VBG, VBN, VBP, VBZ). We observed that although Standford PoS-tagger assigned the required verb subcategories, it also incorrectly tagged some nonverbs as verbs. This is the reason why we considered only those verbs for sub-categorization which were identified (as verbs) by the CMU tweettagger. We varied the training set starting from 3K (equally distributed) to 30K and observed that the accuracy on the gold standard test set did not improve after 27K training instances. Few example tweets with the trending topics are depicted"
N18-1061,N15-1044,0,0.361498,"e current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware information access systems. In parallel, new trends have emerged in the context of the human temporal orientation (Schwartz et al., 2013; Sap et al., 2014; Park et al., 2015; Schwartz et al., 2015; Park et al., 2017). The underlying idea is to understand how the past, present, and future emphasis in the text may affect people’s finances, health, and happiness. For that purpose, the temporal classifiers are built to detect the overall temporal dimension of a given sentence. For instance, the following sentence “can’t wait to get a pint tonight” would be tagged as future. In summary, most of the temporal text processing applications have been mainly relying on the rule-based time taggers, for e.g. HeidelTime (Str¨otgen and Gertz, 2015) or SUTime (Chang and Manning, 2012) to identify and"
N18-1061,D14-1162,0,0.0809289,"Missing"
N18-1061,D15-1063,0,0.0446812,"Missing"
N18-1061,S13-2001,0,0.0975751,"ibutes, namely age, eduction, relationship, intelligence, and optimism. Our contributions are summarised as below: • We define a way to find a novel association between the sentiment view of temporal orientation and the different psychodemographic factors of the tweet users. 2 Related Background The temporal study has recently received an increased attention in several application domains of Natural Language Processing (NLP) and Information Retrieval (IR). The introduction of the TempEval task (Verhagen et al., 2009) and the subsequent challenges i.e. TempEval-2 and -3 (Verhagen et al., 2010; UzZaman et al., 2013) in the Semantic Evaluation workshop series have clearly established the importance of time in dealing with the different NLP tasks. Alonso et al. (2011) reviewed the current research trends and presented a number of interesting applications along with the open problems. The shared task like the NTCIR-11 Temporalia task (Joho et al., 2014) further pushed this idea and proposed to distinguish whether a given query is related to past, recency, future or atemporal. It is the first such challenge, which is organized to provide a common platform for designing and analyzing the time-aware informatio"
N18-2044,R13-1003,0,0.0169051,"online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific feature space, lea"
N18-2044,P17-1001,0,0.0828406,"Missing"
N18-2044,W14-5907,0,0.0288789,"timent analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific feature space, leading to feature redundancy. Adversarial learning (Goodfellow et al., 2014) is the process of learning a model to correctly classify both unmodified data and adversarial data through the regularization m"
N18-2044,W16-0312,0,0.0781823,"ects (medical condition and medication). The texts in bold indicates the sentiment word. uments (nurse letters, radiology reports, and discharge summaries). They also studied users self reported drug reviews on blogs (WebMD, DrugRating) to asses the possible medical sentiments. Majority of the current research in medical sentiment analysis are focused on understanding the mental health disorder, mainly depression. Several shared tasks (Losada et al., 2017; Hollingshead et al., 2017) have also been organized to study the patient health-related opinions on social media. The challenge defined in Milne et al. (2016) aims to automatically classify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies"
N18-2044,W16-0303,0,0.0193889,"ogs (WebMD, DrugRating) to asses the possible medical sentiments. Majority of the current research in medical sentiment analysis are focused on understanding the mental health disorder, mainly depression. Several shared tasks (Losada et al., 2017; Hollingshead et al., 2017) have also been organized to study the patient health-related opinions on social media. The challenge defined in Milne et al. (2016) aims to automatically classify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical se"
N18-2044,R11-1019,0,0.0205476,"sify the user posts from an online mental health forum into four different categories (crisis/red/amber/green) according to how urgently the post needs the attention. Shickel et al. (2016) introduced the notion of applying sentiment analysis to the mental health domain by defining new polarity classification scheme. They split the traditional ‘neutral’ class into both a dual polarity sentiment (both positive and negative) and a ‘neither positive nor negative’ sentiment class. Some of the other prominent works in the opinion mining in medical setting, includes studies by (Bobicev et al., 2012; Sokolova and Bobicev, 2011; Ali et al., 2013). In the study conducted by (Pestian et al., 2012), authors analyzed the emotions and sentiment of suicide notes. The other study in medical sentiment analysis includes the work of Bobicev et al. (2014), where they analyzed sequences of sentiments (encouragement, gratitude, confusion, facts, and endorsement) in In Vitro Fertilization (IVF) medical forum. In terms of methods, majority of the work utilizes machine learning technique (SVM, naive Bayes, logistic regression) by exploiting features such as features, while certain common features could also lie in the task specific"
N18-2044,R13-1082,0,0.0138959,"Deng (2015) provides the quantitative assessment of sentiment across the clinical narrative and social media sources. Towards this, they created a domain-specific corpus from MIMIC II database containing clinical doc272 Figure 2: Architecture of proposed methodology 3.1 bigram, trigram, parts of speech. Also, there has been predominant use of general sentiment lexicon, however their analysis shows that it does not help in capturing the medical sentiment. More domain specific knowledge is also embedded using medical knowledge graph such as UMLS to identify the medical condition and treatment (Sokolova et al., 2013). 3 Let us assume that a blog-text P having k sentences and word sequence w = {w1 , w2 , . . . wl } be given. The embedding layer is used to find out the vector representation xi ∈ Rd×V from a d dimensional pre-trained word embedding of vocabulary V . Each word wi ∈ w will be represented by its respective word embedding xi . The hidden units hl learned at the last time step (l) of sequence are considered as the encoding of the medical blog, P . The representations hl generated from the Eq 1 are fed to a fully connected softmax layer to generate the probability distribution over the given class"
N19-1034,D18-1382,1,0.377607,"Missing"
N19-1034,P17-1142,0,0.0269576,"Missing"
N19-1034,D17-1115,1,0.922987,"Missing"
N19-1034,P05-1015,0,0.450583,"Missing"
N19-1034,D14-1162,0,0.0825707,"rning (STL) and Multi-task (MTL) learning frameworks for the proposed approach. T: Text, V: Visual, A: Acoustic. Weighted accuracy as a metric is chosen due to unbalanced samples across various emotions and it is also in line with the other existing works (Zadeh et al., 2018c). 5.2 Parameters Bi-GRU Dense layer Activations Optimizer Output Feature extraction We use the CMU-Multi-modal Data SDK1 for downloading and feature extraction. The dataset was pre-tokenized and a feature vector was provided for each word in an utterance. The textual, visual and acoustic features were extracted by GloVe (Pennington et al., 2014), Facets2 & CovaRep (Degottex et al., 2014), respectively. Thereafter, we compute the average of word-level features to obtain the utterance-level features. 5.3 Loss Threshold Batch Epochs Values 2×200 neurons, dropout=0.3 100 neurons, dropout=0.3 ReLu Adam (lr=0.001) Softmax (Sent) & Sigmoid (Emo) Categorical cross-entropy (Sent) Binary cross-entropy (Emo) 0.4 (F1) & 0.2 (W-Acc) for multi-label 16 50 Table 5: Model configurations Experiments We evaluate our proposed approach on the datasets of CMU-MOSEI. We use the Python based Keras library for the implementation. We compute F1score and accu"
N19-1034,P18-1208,1,0.904332,"Missing"
N19-1091,N18-2008,0,0.188843,"ent to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer ha"
N19-1091,D15-1075,0,0.0358108,"Missing"
N19-1091,D16-1127,0,0.0648497,"er is structured as follows: In section 2, we discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work dif"
N19-1091,D17-1070,0,0.0416797,"Missing"
N19-1091,D17-1230,0,0.0346807,"m these previous works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long"
N19-1091,N18-1169,0,0.058027,"encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific application of customer care. 3 rd = [ed · h1N d ] The second hierarchical layer Bi-LSTM encodes the utterance representations r1 , r2 , . . . , rD as hidden states h21 , h22 , . . . , h2D . The last hidden state h2D is the representativ"
N19-1091,D17-1090,0,0.0194825,"works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogue generation in a wa"
N19-1091,W04-1013,0,0.0169882,"Missing"
N19-1091,D15-1166,0,0.0166178,"tences which do not contain any information/ suggestions, and are purely non-informative. These may include personalized greetings and expression of appreciation, apology, empathy, assurance, or enthusiasm. Example: Sorry to hear about the trouble! (ii) Informative sentences without courteous expressions: These sentences contain the actual content of the tweet and are generally assertions, instructions, imperatives or suggestions. Example: Simply visit url name to see availability in that area! (13) Baselines We develop the following models: 1. Model-1: This is a Seq2Seq model with attention (Luong et al., 2015) and decoder conditioned on the conversational context vector c (without concatenating emotional embedding i.e. instead of Eq. 2, rd = h1N d ) 2. Model-2: This model is developed using Model-1 along with the copying mechanism of Pointer Generator Network. 3. Model-3: This model is developed using Model-2 along with emotional embeddings in the conversational context vector as in E.g., 2. # Conversation # Utterances Train 140203 179034 Valid 20032 25642 Test 40065 51238 Table 2: Dataset Statistics 4 Dataset In this section we describe the details of the dataset that we create for our experiments"
N19-1091,D17-1169,0,0.064795,". This attention distribution helps to identify the relevant encoder states necessary for the current decoding step. The representation of the encoder for this time step is an attention weighted sum of its states, called the context vector: X h∗t = αit hi (5) Conversational History Representation The conversation history C is a sequence of utterances (u1 , u2 , . . . , uD ) and each utterance ud is a sequence of words w1 , w2 , . . . , wN which are represented by their embeddings. For encoding the emotional states associated with these utterances, we use the output distribution from DeepMoji (Felbo et al., 2017) which is pre-trained on the emoji prediction task. Let the utterance ud be a sequence of sentences s1 , s2 , . . . , sN , where the nth sentence has an emotional embedding en,d . Then the emotional representation of the utterance is: n Decoder states and Attention calculation At the decoder time step t, the decoder LSTM state st is used to calculate the attention distribution over the encoder states αt : Methodology ed [i] = max en,d [i] Encoder states Another single layer unidirectional LSTM network encodes the generic response word embedding sequence to obtain the encoder hidden states hi ."
N19-1091,Q18-1027,0,0.0543472,"sappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific application of customer care. 3 rd = [ed · h1N d ] The second hierarchical layer Bi-LSTM encodes the utterance representations r1 , r2 , . . . , rD as hidden states h21 , h22 , . . . , h2D . The last hidden state h2D is the representative of the conversational"
N19-1091,P02-1040,0,0.104526,"Missing"
N19-1091,W16-3609,0,0.126114,"he dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specif"
N19-1091,N18-1012,0,0.0253443,"hile generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In (Rao and Tetreault, 2018), a dataset has been introduced for formality style transfer. Unsupervised text style transfer has encouraged in transforming a given text without parallel data (Shen et al., 2017; Carlson et al., 2017; Fu et al., 2018; Li et al., 2018; Niu and Bansal, 2018). Overall our system is novel as it is motivated by the need for inducing specific behavior and style in an existing NLG systems (neural, or template-based) as a means of post editing, by simultaneously being emotionally and contextually consistent. We have successfully demonstrated this behavior through empirical analysis for a specific ap"
N19-1091,W17-3541,0,0.0184401,"ogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the demand of the situation. Style transfer has been an emerging field in natural language processing (NLP). A couple of works have been done in changing the style of an input text and designing the output text according to some particular styles. In"
N19-1091,E17-1036,0,0.0248809,"discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diverse replies by capturing discourse-level information in the encoder. Our work differentiates from these previous works in dialogu"
N19-1091,P17-1061,0,0.117345,"Missing"
N19-1091,P18-1104,0,0.0807128,"tion. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) encoder. ness, disappointment) and traits (like friendliness, apologetic, thankfulness, empathy), as is the de"
N19-1091,P17-1099,0,0.0292979,"t vector c. st = LST M (st−1 , Wp [wemb (yt−1 ), h∗t−1 , c] + ˜b) (6) where, Wp and ˜b are the trainable parameters. (1) 3.4 The first bi-directional layer over any utterance ud yields the hidden states h11d , h12d , . . . , h1N d , where N is the word length of the utterance. The final representation of any utterance rd is given by the concatenation of the emotional representation as well as the last hidden state of the Bi-directional Output distribution calculation To aid the copying of words from the generic response while generating the courteous response, we use the mechanism similar to (See et al., 2017). For the pointer generator network, the model computes two distributions, one over the 853 Figure 1: Architectural Diagram of the Proposed Model. Inputs to the model: Conversation History (left), Generic Response (centre) Output: Courteous Response (right). The Conversation History is encoded by hierarchical BiLSTM to a Conversational Context vector c. The encoder encodes the Generic Response into hidden states hi . Response tokens are decoded one at a time. Attention αi , and vocabulary distributions (pvocab ) are computed, and combined using pgen to produce output distribution. Sampling it"
N19-1091,W17-2629,0,0.0171487,"works in dialogue generation in a way that we embellish the appropriate response content with courteous phrases and sentences, according to the conversation. Hence, our system is an accompaniment to any standalone natural language generation system to enhance its acceptability, usefulness and user-friendliness. Emotion classification and analysis (Herzig et al., 2016) in customer support dialogue is important for better understanding of the customer and to provide better customer support. Lately, a number of works have been done on controlled text generation (Hu et al., 2017; Li et al., 2017; Subramanian et al., 2017; Fedus et al., 2018; Peng et al., 2018) in order to generate responses with desired attributes. Emotion aware text generation (Zhou and Wang, 2018; Zhou et al., 2018; Huang et al., 2018) have gained popularity as it generates responses depending on a specific emotion. Previous works in conditioned text generation have worked on inducing specific biases and behaviors (Herzig et al., 2017) while generation (like emotion, style, and personality trait). Our work is different in the sense that it can encompass different emotional states (like joy, excitement, sad852 Long Short Term Memory (Bi-LSTM"
N19-1091,N18-1186,0,0.156431,"pecific metrics, both automatic and human evaluation based. The rest of the paper is structured as follows: In section 2, we discuss the related works. In Section 3 we explain the proposed methodology followed by the dataset description in section 4. Experimental details, evaluation metrics and results are presented in section 5 and 6 respectively. In section 7, we present the concluding remarks followed by future directions. Related Work Natural language generation (NLG) module has been gaining importance in wide applications such as dialogue systems (Vinyals and Le, 2015; Shen et al., 2018; Wu et al., 2018; Serban et al., 2017a; Raghu et al., 2018; Zhang et al., 2018; Li et al., 2016), question answering systems (Reddy et al., 2017; Duan et al., 2017), and many other natural language interfaces. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent is able to communicate with its user. In (Serban et al., 2017b), the authors have proposed an hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al., 2017), that generates diver"
P06-2025,C00-1056,0,0.32554,"Missing"
P06-2025,W98-1005,0,0.218658,"large name collections like census data, electoral roll and railway reservation information must be available to multilingual citizens of the country in their vernacular. In the present work, the various proposed models have been evaluated on a training corpus of person names. A hybrid neural network and knowledge-based system to generate multiple English spellings for Arabic personal names is described in (Arbabi et al., 1994). (Knight and Graehl, 1998) developed a phoneme-based statistical model using finite state transducer that implements transformation rules to do back-transliteration. (Stalls and Knight, 1998) adapted this approach for back transliteration from Arabic to English for English names. A spelling-based model is described in (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c) that directly maps English letter sequences into Arabic letter sequences with associated probability that are trained on a small English/Arabic name list without the need for English pronunciations. The phonetics-based and spelling-based models have been linearly combined into a single transliteration model in (Al-Onaizan and Knight, 2002b) for transliteration of Arabic named entities into English. Several"
P06-2025,W03-1508,0,0.320963,"Missing"
P06-2025,P02-1051,0,0.0843851,"Missing"
P06-2025,2005.mtsummit-papers.37,0,0.0691749,"Missing"
P06-2025,2005.mtsummit-papers.36,0,\N,Missing
P06-2025,P04-1021,0,\N,Missing
P06-2025,W02-0505,0,\N,Missing
P06-2025,W05-0700,0,\N,Missing
P17-2104,N15-2023,0,0.0306458,"on, a linear Support Vector Machine (lSVM)1 is used. In particular, we trained three binary classifiers (one per class)2 using one-vs.-rest, and label a tweet with the class that assigned the highest score. In the second step, we pass tweets through these two optimized components to detect their temporal orientation. Related Work Existing message-/sentence-level temporal classification methods generally fall into two categories: (1) rule-based methods, and (2) supervised machine-learning methods. Rule-based methods mainly rely on manually designed classification rules for each temporal class (Nie et al., 2015). Despite their effectiveness, this kind of method requires substantial efforts in rule design. Most research on machine learning-based sentence temFigure 1: Proposed learning architecture. 1 Trained using the Weka implementation of LIBSVM with linear kernels (polynomial kernels yielded worse performance). 2 Multi-class classification yielded worse performance. 660 The choice of CNN for feature extraction is motivated by: of the CNN model are learned by passing multiple filters over word vectors and then applying the max-over-time pooling operation to generate features which are used in a full"
P17-2104,D15-1303,0,0.0129706,"lar to Kim (2014), we use dropout (Hinton et al., 2012) to regularize the change of parameters by randomly setting some weights to zero that prevents overfitting. • CNNs have been successfully used as feature extractors in various computer vision tasks and achieved better results compared to handcrafted features. Research has shown that CNN feature maps can be used with SVM to yield classification results that outperform the original CNN (Athiwaratkun et al., 2015) 3.2 • Superior accuracies have also been achieved by following a similar line of research in the context of NLP tasks (Kim, 2014; Poria et al., 2015). Income Predictor Model Similar to Preot¸iuc-Pietro et al. (2015), we formulate the income prediction task as regression using user-level temporal orientation as features. First, the tweet temporal orientation classifier is used to label whether a tweet focuses on past, present, or future. Afterwards, at user-level, we produce three categories of temporal orientation (three separate variables summing to one), defined simply as the proportion of a user’s total tweets (tweets(user)all ) classified in the given temporal category (c ∈ { past, present, or future}), as in (1): Convolutional Neural"
P17-2104,D14-1181,0,0.00435547,"Missing"
P17-2104,D14-1121,0,0.0323303,"se it to build a predictive model of income. Our analysis uncovers a correlation between future temporal orientation and income. Finally, we measure the predictive power of future temporal orientation on income by performing regression. 1 Introduction User-generated content in social media such as Twitter has enabled the study of author profiling on an unprecedented scale. Author profiling in social media aims at inferring various attributes of the user from the text that they have written. Most of the prior studies in this field have focused on age, gender prediction (Marquardt et al., 2014; Sap et al., 2014), psychological well-being (Dodds et al., 2011; Choudhury et al., 2013), and a host of other behavioural, psychological and medical phenomena (Kosinski et al., 2013). However, there has been a lack of work looking at the socioeconomic characteristics of Twitter users. In this paper, we focus on automatic estimation of Twitter users’ income from their Twitter language. An income predictor of social media users can be useful for both social science research and a range of 659 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 659–665 c V"
P17-2104,N15-1044,0,0.265038,"income predictor of social media users can be useful for both social science research and a range of 659 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 659–665 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2104 poral classification has revolved around feature engineering for better classification performance. Different kinds of features have been explored such as bag-of-words, time expressions, part-ofspeech tags, and temporal class-specific lexicons (Schwartz et al., 2015). Temporal class specific lexicon creation and feature engineering also cost a lot of human efforts. In addition, creation of a large-scale training data set for supervised machine-learning approaches is also very laborious. ally selected eighty (80) high-precision seed terms (and automatically extracted similar terms) representing past, present, and future to train the CNN. For example, tweets exclusively containing past (resp. present and future) seed terms were marked with weak labels past (resp. present and future). We used the tweet-level temporal classifier to automatically classify a la"
P19-1106,D18-2029,0,0.0282081,"Missing"
P19-1106,N18-1149,0,0.161049,"Missing"
P19-1297,D13-1176,0,0.0553978,"nguage participating in training into an interlingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder’s ability to generate interlingual representation. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has become a dominant paradigm for machine translation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently beco"
P19-1297,W17-3204,0,0.0203715,"any-to-many fashion leveraging encoder’s ability to generate interlingual representation. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has become a dominant paradigm for machine translation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et"
P19-1297,P02-1040,0,0.107848,"directions using 4 languages (English, French, German and Spanish) to perform translation in 12 directions. We take En3083 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3083–3089 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics glish as the anchor language and map three nonEnglish languages’ embeddings into the English embedding space. We train the network to denoise all the four languages and back-translate between English and non-English languages. We evaluate on newstest13 and newstest14 using BLEU (Papineni et al., 2002) score. We find that the multilingual model outperforms the bilingual models by up to 1.48 BLEU points. We also find that the network learns to translate between the nonEnglish (French, German and Spanish) language pairs as well even though it does not explicitly see these pairs during training. To translate between a non-English language pair, no modification to the network is required at inference time. We also evaluate the performance of the non-English language pairs and achieve a maximum BLEU score of 13.92. 3 Background In this section, we briefly describe the basic unsupervised NMT mode"
P19-1297,P16-1009,0,0.0874549,"parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et al., 2018) or two (Artetxe et al., 2018) decoders. In supervised NMT settings, combining multiple languages to jointly train an NMT system has been found to be successful in improving the performance (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). However, to the best of our knowledge, this is the very first attempt which aims at combining multiple languages in an unsupervised NMT training. To translate between many language"
P19-1297,P18-1005,0,0.0470953,"anslation achieving state-of-the-art results on publicly available benchmark datasets. An effective NMT system requires supervision of a huge amount of high-quality parallel data which is not easily available for many language pairs. In absence of such huge amount of parallel data, NMT systems tend to perform poorly (Koehn and Knowles, 2017). However, NMT without using any parallel data such as bilingual translations, bilingual dictionary or comparable translations, has recently become reality and opened up exciting opportunities for future research (Lample et al., 2018; Artetxe et al., 2018; Yang et al., 2018). It completely eliminates the need of any kind of parallel data and depends heavily on cross-lingual embeddings and iterative back-translations (Sennrich et al., 2016) between the source and target language using monolingual corpora. On the architectural point of view, the approaches combine one encoder and one (Lample et al., 2018) or two (Artetxe et al., 2018) decoders. In supervised NMT settings, combining multiple languages to jointly train an NMT system has been found to be successful in improving the performance (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). However, to"
P19-1516,P16-2058,0,0.0435355,"Missing"
P19-1516,P15-1033,0,0.012886,"2016). Let us the consider the input sequence to this layer is E = {e1 , e2 , . . . , en }. A convolution operation is performed over the zero-padded sequence E p . Similar to the character embedding, a set of k filter of size m are applied to the sequence. We obtain convoluted features ct at given time t for t = 1, 2, . . . , n. ct = relu(F [et− m−1 . . . et . . . et+ m−1 ]) 2 (3) 2 Then, we generate the feature vectors C 0 = [c01 , c02 . . . c0n ], by applying max pooling on C. Inspired by the success of stacked attentive RNN in solving other NLP tasks (Wu et al., 2016; Graves et al., 2013; Dyer et al., 2015; Prakash et al., 2016), we use the stacked GRU to encode the input text. The stacked GRU is an extension to GRU model that has multiple hidden GRU layers. The purpose of using multiple GRUs layers is to learn more sophisticated conditional distributions from the data (Bahdanau et al., 2015). In this work, we employ vertical stacking strategy where the output of the previous layer of GRU is fed to the highway layer and corresponding output is passed as input to the next layer of GRU. Let the number of layers in stacked GRU is L then the GRU computes the hidden state for each layer l ∈ L as fol"
P19-1516,C16-1084,0,0.12138,"es such as social media, both EMR and medical case reports offer several advantages of having complete records of patients’ medical history, treatment, conditions and the possible risk factors, and is also not restricted to the patients experiencing ADRs (Harpaz et al., 2012b). Recently, a study conducted by (Sarker and Gonzalez, 2015) utilized the data from MEDLINE case reports and Twitter. They proposed several textual features and investigated how the combination of different datasets would increase the performance of identifying ADRs. With the advancement of the neural network technique, (Huynh et al., 2016) investigated multiple neural network (NN) frameworks for ADR classification on both medical case reports and Twitter dataset. (ii) Social Media: Social media offers a very rich and viable source of information for identifying potential ADRs in a real-time. Leaman et al. (2010) conducted very first study utilizing user comments from their social media post. In total, the dataset contains 6, 890 user comments. The research shows that user comments are highly beneficial in uncovering the ADRs. Further works (Gurulingappa et al., 2012b; Benton et al., 2011; Harpaz et al., 2012a) utilized the lexi"
P19-1516,W10-1915,0,0.257382,"al Drug Administration’s Adverse Event Reporting System (FAERS) (Li et al., 2014). These systems are often under-reported, biased and delayed. To overcome the limitation of a passive reporting system, active methods to ADR monitoring continuously explores frequently updated ADR data sources (Behrman et al., 2011). The quantity and near-instantaneous nature of social media provide potential opportunities for real-time monitoring of Adverse Drug Reaction (ADR). The fact that this data is up-to-date and is generated by patients overcomes the weaknesses of traditional ADR surveillance techniques (Leaman et al., 2010). Thus, social media could complement traditional information sources for more effective pharmacovigilance studies, as well as potentially serve as an early warning system for unknown ADR, which may be important for a clinical decision. Additionally, the high statistically significant correlation (p &lt; 0.001, ρ = 0.75) between FAERS and ADRs (extracted through Twitter data) shows that Twitter is a viable pharmacovigilance data source (Freifeld et al., 2014). With the enormous amount of data generated every day, it is desirable to have an automated ADR extraction system that can ease the work of"
P19-1516,P17-1001,0,0.0174851,"order to capture the common features along the task, we utilize the above feature extractor framework which serves as a Generator model and the feed forward neural network as a Discriminator. 3.4 Task Discriminator Layer Our feature extractor layer is generating two types of features, shared and task-specific. Ideally both feature spaces should be mutually exclusive. To ensure that task-specific features of given task do not exist in the shared space, we exploit the concept of adversarial training (Goodfellow et al., 2014) into shared feature space. We follow the same method as introduced by (Liu et al., 2017) to make the shared feature space uncontaminated by the task-specific features. For achieving the aforementioned strategy, a Task Discriminator D is used to map the attention prioritized shared feature to estimate the task of its origin. In our case, Task Discriminator is a fully connected layer using a softmax layer to produce the probability distribution of the shared features belonging to any task. The shared feature extractor (c.f. 3.3) works as Generator (G) to generate shared features. The shared feature extractor is made to work in an adversarial way, preventing the discriminator from p"
P19-1516,C16-1275,0,0.0526184,"Missing"
P19-1516,E17-1014,0,0.0289778,"included in lexicons. With the emergence of annotated data, several research works have employed supervised machine learning techniques such as Support Vector Machine (SVM) (Sarker and Gonzalez, 2015), Conditional Random Field (CRF) (Nikfarjam et al., 2015) and Random Forest (Zhang et al., 2016). In recent years with the introduction of deep learning techniques, most of the studies utilize deep learning model to predict ADRs. Lee et al. (2017) developed semi-supervised deep learning model on the Twitter corpus. In particular, they used the Convolution Neural Network (CNN) for classification. Stanovsky et al. (2017) used the Recurrent Neural Network integrated with knowledge graph embedding on the CADEC corpus. Their study shows that this integration can make the model more accurate. Tutubalina and Nikolenko (2017) explored the combination of CRF and Recurrent Neural Network (RNN). Their 5236 results show that CRF can assist RNN model in capturing the context well. The most relevant work to this study is the work conducted by Chowdhury et al. (2018). They learned jointly for three tasks: binary classification, ADR labeling, and indication labeling using RNN-attentioncoverage model. 3 Methodology With our"
P19-1516,1983.tc-1.13,0,0.193168,"Missing"
P19-1540,W18-6514,0,0.260802,"from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form of knowledge 5439 bases using hierarchical encoder-decoder model. Our work is different from these existing works on MMD dataset in the sense that we incorporate position and attribute aware attention mechanism for capturing ordered information and minute details such as colour, style etc. from the image representations for more accurate response generation. Our method, unlike the previous works, make use of the MFB technique for better information fusion across different modalities. The approach that we propose to capture and i"
P19-1540,W18-5709,0,0.462084,"from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form of knowledge 5439 bases using hierarchical encoder-decoder model. Our work is different from these existing works on MMD dataset in the sense that we incorporate position and attribute aware attention mechanism for capturing ordered information and minute details such as colour, style etc. from the image representations for more accurate response generation. Our method, unlike the previous works, make use of the MFB technique for better information fusion across different modalities. The approach that we propose to capture and i"
P19-1540,P19-1648,0,0.140656,"urse-level information in the encoder. Our current work differentiates from these existing works in dialogue systems in a way that we generate the appropriate responses by capturing information from both the text and image, conditioned on the conversational history. 2.2 Multimodal Dialogue Systems With the recent shift in interdisciplinary research, dialogue systems combining different modalities (text, images, video) have been investigated for creating robust conversational agents. Dialogue generation combining information from text and images (Das et al., 2017a,b; Mostafazadeh et al., 2017; Gan et al., 2019; De Vries et al., 2017) has been successful in bridging the gap between vision and language. Our work differs from these as the conversation in Multimodal Dialogue (MMD) dataset (Saha et al., 2018) deals with multiple images and the growth in conversation is dependent on both image and text as opposed to a conversation with a single image. Lately, with the release of DSTC7 dataset, video and textual modalities have been explored in (Lin et al., 2019; Le et al., 2019). Prior works on MMD dataset reported in (Agarwal et al., 2018b,a; Liao et al., 2018) have captured the information in the form"
P19-1540,W18-5712,0,0.202564,"Missing"
P19-1540,W07-0734,0,0.0236998,"s is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with respect to the following three metrics: 1. Fluency (F): The ge"
P19-1540,N16-1014,0,0.0249704,"osition and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al.; Shen et al., 2018) that generate diverse replies by capturing discourse-level information in the encoder. Our current work differentiates from these existing works in dialogue sy"
P19-1540,P16-1094,0,0.0176738,"osition and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decoder model for capturing the dependencies in the utterances of a dialogue. Conditional auto-encoders have been employed in (Zhao et al.; Shen et al., 2018) that generate diverse replies by capturing discourse-level information in the encoder. Our current work differentiates from these existing works in dialogue sy"
P19-1540,W04-1013,0,0.0245465,"size for all the layers is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with respect to the following"
P19-1540,D15-1166,0,0.072908,"A pre-trained VGG-19 model (Simonyan and Zisserman, 2014) is used to extract image features for all the images in a given dialogue turn. The concatenation of single image features is given as input to a single linear layer to obtain a global image context representation. In this section we firstly define the problem and then present the details of the proposed method. 3.1 (1) (7) Decoder: In the decoding stage, the decoder is another GRU that generates words sequentially conditioned on the final hidden state of the context GRU and the previously decoded words. Attention mechanism similar to (Luong et al., 2015) is incorporated to enhance the performance of the decoder GRU. The attention layer is applied to the hidden state of context encoder using decoder state dt as the query vector. The concatenation of the context vector and the decoder state is used to compute a final probability distribution over the output tokens. hd,t = GRUd (yk,t−1 , hd,t−1 ) (8) 5440 αt,m = sof tmax(hTc,m W hd,t ) (10) Attribute-aware Attention: To focus on different attributes of the image mentioned in the text, we employ attribute-aware attention. ˜ t = tanh(W˜ [hd,t ; ct ]) h h (11) αa = sof tmax(Wa T HU ), Ua = αa HU T"
P19-1540,I17-1047,0,0.378945,"jointly the first authors the information to the user is the primary objective of every response generation module. One of the running goals of AI is to bring language and vision together in building robust dialogue systems. Advances in visual question answering (VQA) (Kim et al., 2016; Xiong et al., 2016; Ben-Younes et al., 2017), and image captioning (Anderson et al., 2018; Chen et al., 2018) have ensured interdisciplinary research in natural language processing (NLP) and computer vision. Recently, several works in dialogue systems incorporating both vision and language (Das et al., 2017a; Mostafazadeh et al., 2017) have shown promising research directions. Goal oriented dialogue systems are majorly based on textual data (unimodal source). With increasing demands in the domains like retail, travel, entertainment, conversational agents that can converse by combining different modalities is an essential requirement for building the robust systems. Knowledge from different modalities carries complementary information about the various aspects of a product, event or activity of interest. By combining information from different modalities to learn better representation is crucial for creating robust dialogue"
P19-1540,P02-1040,0,0.103687,"rot and Bengio, 2010). The hidden size for all the layers is 512. We employ AMSGrad (Reddi et al., 2019) as the optimizer for model training to mitigate the slow convergence issues. We use uniform label smoothing with  = 0.1 and perform gradient clipping when gradient norm is over 5. For image representation, 5442 1 https://pytorch.org/ Description State-of -the-arts FC6(4096 dimension) layer representation of the VGG-19 (Simonyan and Zisserman, 2014), pretrained on ImageNet is used. Baseline Models 5.2 Automatic Evaluation For evaluating the model we report the standard metrics like BLEU-4 (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) employing the evaluation scripts made available by (Sharma et al., 2017). 5.3 Human Evaluation To understand the quality of responses, we adopt human evaluation to compare the performance of different models. We randomly sample 700 responses from the test set for human evaluation. Given an utterance, image along with the conversation history were presented to three human annotators, with post-graduate level of exposure. They were asked to measure the correctness and relevance of the responses generated by the different models with resp"
P19-1540,D11-1054,0,0.0484056,"rall system architecture for text generation; Right image is the baseline encoder model Context Encoder MFB Concatenation Position Concatenation Attribute aware image representation Attribute VGG Direction from left to right Position aware image representation VGG PE1 PE2 Figure 3: Proposed Multimodal Encoder with Position and Attribute aware Attention with MFB fusion several works carried out on data-driven textual response generation. To help the users achieve their desired goals, response generation provides the medium through which a conversational agent can communicate with its user. In (Ritter et al., 2011), the authors used social media data for response generation following the machine translation approach. The effectiveness of deep learning has shown remarkable improvement in dialogue generation. Deep neural models have been quite beneficial for modelling conversations in (Vinyals and Le, 2015; Li et al., 2016a,b; Shang et al., 2015). A context-sensitive neural language model was proposed in (Sordoni et al., 2015), where the model chooses the most probable response given the textual conversational history. In (Serban et al., 2015, 2017), the authors have proposed a hierarchical encoder-decode"
P19-1540,P15-1152,0,0.175666,"Missing"
P19-1540,N15-1020,0,0.0604202,"Missing"
P19-1540,P17-1061,0,0.0317097,"Missing"
P19-1540,P17-1018,0,0.0262307,"knowledge of every image with respect to its position is necessary so that the agent can capture the information and fulfill the objective of the customer. The lack of position information of the images in the baseline MHRED model causes quite a few errors in focusing on the right image. To alleviate this issue, we fuse position embedding of every image with the corresponding image features. The position of every image is represented by position embedding P Ei , where, P E = [P E1 , ..., P En0 ]. This information is concatenated to the corresponding image features. To compute self attention (Wang et al., 2017) we represent textual features as HU = [hU,1 , ...., hU,n ]. αp = sof tmax(Wp T HU ), Up = αp HU T (13) βa = sof tmax(Ua T Wa0 HI ), Ia = βa HI T (16) where, Wa T and Wa0 are trainable parameters. Finally, in our proposed model, as shown in Figure 3, we incorporate position-aware and attributeaware attention mechanisms to provide focused information conditioned on the text utterance. We concatenate Ua and Up vectors for the final utterance representations Uf , Ia and Ip vectors as the final image representation If . The output of the context encoder hc along with If and Uf serves as input to t"
R11-1084,S10-1077,1,0.890585,"Missing"
R11-1084,P98-1013,0,0.106407,"A subcategorization frame is a statement of what types of syntactic arguments a verb (or an adjective) takes, such as objects, infinitives, that-clauses, participial clauses, and subcategorized prepositional phrases (Manning et al. 1993). VerbNet (Kipper-Schuler et al, 2005) is the largest online verb lexicon with explicitly stated syntactic and semantic information based on Levin’s verb classification (Levin et al 1993). It is a hierarchical domain-independent, broadcoverage verb lexicon with mappings to other lexical resources such as WordNet (Miller et al, 1990), XTAG (2001) and FrameNet (Baker et al, 1998). We use VerbNet throughout this experiment for identifying the event actors. The existing syntax for each event verb is extracted from VerbNet and a separate rule based argument structure acquisition system is developed in the present task for identifying the event actor. The acquired argument structures are compared against the extracted VerbNet frame syntaxes. If the acquired argument structure matches with any of the extracted frame syntaxes, the event actor corresponding to each event verb is tagged with the actor information in the appropriate slot in the sentence. Syntax Acquisition fro"
R11-1084,de-marneffe-etal-2006-generating,0,\N,Missing
R11-1084,C98-1013,0,\N,Missing
R11-1084,P93-1032,0,\N,Missing
R11-1084,J02-3001,0,\N,Missing
R11-1084,N04-1030,0,\N,Missing
S10-1077,P07-2044,0,0.0796423,"Missing"
S10-1077,W06-0110,0,0.0606228,"Missing"
S10-1077,N03-1028,0,0.194286,"Missing"
S10-1077,S07-1014,0,0.117204,"Missing"
S10-1077,P06-1095,0,\N,Missing
S13-2011,setzer-gaizauskas-2000-annotating,0,0.148104,"Missing"
S13-2011,J02-3001,0,0.303093,"Missing"
S13-2011,N04-1030,0,0.130323,"Missing"
S13-2011,P11-2061,0,0.149426,"Missing"
S13-2011,S07-1014,0,\N,Missing
S13-2011,S10-1010,0,\N,Missing
S14-2053,H05-1043,0,0.282011,"Missing"
S14-2053,P02-1053,0,0.0350136,"Missing"
S14-2053,P06-1134,0,0.0300997,"classify the sentiments or opinions into semantic classes such as positive, negative, and neutral. We develop a Random Forest classifier for this task. In this particular task one more class conflict is introduced. It is assigned if the sentiment can either be positive or negative. For classification we make use of some of the features such as local context, PoS, Chunk, prefix and suffix etc., as defined in the previous Subsection. Some other problem-specific features that we implement for sentiment classification are defined as below: • MPQA feature: We make use of MPQA subjectivity lexicon (Wiebe and Mihalcea, 2006) that contains sentiment bearing words as feature in our classifier. This list was prepared semi-automatically from the corpora of MPQA7 and Movie Review dataset8 . A feature is defined that takes the values as follows: 1 for positive; -1 for negative; 0 for neutral and 2 for those words that do not appear in the list. 9 • Function words: A list of function words is 7 8 http://www2.fs.u-bunkyo.ac.jp/ gilner/wordlists.html www.cs.waikato.ac.nz/ml/weka/ 11 B, I and O denote the beginning, intermediate and outside tokens 10 http://cs.pitt.edu/mpqa/ http://cs.cornell.edu/People/pabo/movie-review-d"
S14-2053,H05-2017,0,\N,Missing
S14-2053,N10-1122,0,\N,Missing
S14-2054,C10-2005,0,0.0349188,"ov et al., 2013). Text also contains lots of misspellings, slang, out-of-vocabulary words, URLs, and genre-specific terminology and abbreviations, e.g., RT for reTweet and #hashtags. The kind of these specific features pose great challenges for building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. These aspects also introduce complexities to build the state-of-theart data mining systems. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., 2011; Pak and Paroubek, 2010; Kouloumpis et al., 2011). Recent studies show the interests in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Mandel et al., 2012). Another aspect of social media data, such as twitter messages, is that they include rich information about the individuals involved in the communication. For e.g., twitter maintains information about who follows whom. ReTweets (reshares of a Tweet) and tags inside of Tweets provide"
S14-2054,W12-2104,0,0.0409458,"required for efficient processing of texts. These aspects also introduce complexities to build the state-of-theart data mining systems. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., 2011; Pak and Paroubek, 2010; Kouloumpis et al., 2011). Recent studies show the interests in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Mandel et al., 2012). Another aspect of social media data, such as twitter messages, is that they include rich information about the individuals involved in the communication. For e.g., twitter maintains information about who follows whom. ReTweets (reshares of a Tweet) and tags inside of Tweets provide discourse information (Nakov et al., 2013). Efficient modelling of such information is crucial in the sense that it provides a mean to empirically study the social interactions where opinion is conveyed. Several corpora with detailed opinion and sentiment annotation have been made freely available, In this paper w"
S14-2054,S13-2053,0,0.017991,"tive repeated characters (e.g., happppppppy, hurrrrrey etc.). In such cases, the words are normalized to contain only upto two repeated characters. This helps to capture the words having similar structures. 6. Negated contexts: A negated word can affect the polarity of the target word. A negated segment is defined as a sequence of tokens that starts with a negation word (e..g, no, couldn’t etc.) and ends with a punctuation marks (e.g.,,,., :, ;, !, ?). All the words following the negation word are suffixed with NEGATIVE, and the polarity features are also converted with NEGATIVE in line with (Mohammad et al., 2013). 3 Experimental Results and Analysis The SemEval-2014 shared task datasets are based on SemEval-2013 competition datasets. It covers a range of topics, including a mixture of entities, products and events. Keywords and Twitter hashtags were used to identify messages relevant to the selected topic. The selected test sets were taken from the five different domains. We perform experiment with the python based NLTK toolki2 . We 2 precision 72.02 76.86 7.69 52.19 gspred positive negative neutral positive 502 160 35 negative 50 196 9 neutral 3 9 1 Table 3: Confusion matrix for A. Here, gs: Gold st"
S14-2054,S13-2052,0,0.142161,"arajan and Asif Ekbal Department of Computer Science and Engineering Indian Institute of Technology Patna, India {raja.cs10,asif}@iitp.ac.in Abstract nities to automatically study public opinion. Dealing with these informal text genres presents new challenges for data mining and language processing techniques beyond those encountered when working with more traditional text genres such as newswire. Tweets and SMS messages are short in length, usually a sentence or a headline rather than a document. These texts are very informal in nature and contains creative spellings and punctuation symbols (Nakov et al., 2013). Text also contains lots of misspellings, slang, out-of-vocabulary words, URLs, and genre-specific terminology and abbreviations, e.g., RT for reTweet and #hashtags. The kind of these specific features pose great challenges for building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. These aspects also introduce complexities to build the state-of-theart data mining systems. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng"
S14-2054,S10-1097,0,0.0252952,"misspellings, slang, out-of-vocabulary words, URLs, and genre-specific terminology and abbreviations, e.g., RT for reTweet and #hashtags. The kind of these specific features pose great challenges for building various lexical and syntactic resources and/or tools, which are required for efficient processing of texts. These aspects also introduce complexities to build the state-of-theart data mining systems. In recent times, there has been a huge interest to mine and understand the opinions and sentiments that people are communicating in social media (Barbosa and Feng, 2010; Bifet et al., 2011; Pak and Paroubek, 2010; Kouloumpis et al., 2011). Recent studies show the interests in sentiment analysis of Tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salathe and Khandelwal, 2011) and disaster management (Mandel et al., 2012). Another aspect of social media data, such as twitter messages, is that they include rich information about the individuals involved in the communication. For e.g., twitter maintains information about who follows whom. ReTweets (reshares of a Tweet) and tags inside of Tweets provide discourse information (Nakov et al., 2013)."
S14-2057,S10-1097,0,0.0641823,"Missing"
S14-2057,W11-0705,0,0.123889,"Missing"
S14-2057,C10-2005,0,0.132843,"Missing"
S14-2057,W12-2104,0,0.0945614,"Missing"
S14-2057,S13-2053,0,0.0290537,"Missing"
S14-2057,S13-2052,0,0.0398233,"Missing"
S15-2100,N13-1039,0,0.0712167,"Missing"
S15-2100,baccianella-etal-2010-sentiwordnet,0,0.016245,"ords and their associations with eight emotions (anger, fear, anticipation, trust, surprise, sadness, joy and disgust) and two sentiments (negative and positive). We categorize joy, surprise, trust and anticipation as positive emotions and the rest as negative emotions. Based on the categorization, we compute the number of tokens with positive score, number of tokens with negative score and number of tokens with neutral score as the features. • SentiWordNet Feature: We compute the average positive score (posScore) and negative score (negScore) for each word in the tweet using SentiWordNet3.0 (Baccianella et al., 2010). For a given tweet we define two features that denote the number of words which have posScore greater than negScore, and number of word with negScore greater than posScore. in positive context than in negative context. Similarly all the tokens which belong to the negative sub-cluster occur more in negative context than in the positive context. The categorization of positive and negative sub-cluster is done based on the number of times the token occurs in positive and negative contexts. A feature vector of length 2000 is defined, each bit of which takes a value denoting the number of times the"
S15-2100,S15-2078,0,0.0287962,"Missing"
S15-2100,S14-2009,0,0.0148801,"ions. This has urged the scientific community to extract the substantive information from these texts. The proliferation of microblogging sites like Twitter which boasts of user’s comments on everything trending in real time opens The technical study of public sentiment has been a subject of trending research and a significant amount of extensive work is being carried out in the domain. Sentiment Analysis has been handled at the various levels of granularity. Early research works (Pang and Lee, 2004) focussed on the document level classification with further studies at message and term level (Rosenthal et al., 2014). Twitter has also been investigated for its possible applications in the fields of commerce (Jansen et al., 2009; Bollen et al., 2011), elections (O’Connor et al., 2010; Tumasjan et al., 2010), disaster management (Nagy and Stamberger, 2012; Terpstra et al., 2012) etc. using varied approaches and different experimental setups. Semantic Evaluation tasks (Nakov et al., 2013; Rosen601 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 601–607, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics thal et al., 2014) continue to"
S15-2100,S13-2052,0,0.160131,"e domain. Sentiment Analysis has been handled at the various levels of granularity. Early research works (Pang and Lee, 2004) focussed on the document level classification with further studies at message and term level (Rosenthal et al., 2014). Twitter has also been investigated for its possible applications in the fields of commerce (Jansen et al., 2009; Bollen et al., 2011), elections (O’Connor et al., 2010; Tumasjan et al., 2010), disaster management (Nagy and Stamberger, 2012; Terpstra et al., 2012) etc. using varied approaches and different experimental setups. Semantic Evaluation tasks (Nakov et al., 2013; Rosen601 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 601–607, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics thal et al., 2014) continue to pitch in with the newer systems for the sentiment classification of tweets. 2 Proposed Approach In this section, we describe the supervised learning system that we develop for the first two subtasks, namely A and B. The first section would focus on Tasks A and B and later section would describe the method that was adopted for Task E. 2.1 Preprocessing We normalize all URLs"
S15-2100,P11-2008,0,0.0203046,"ry valued feature is implemented as contextual feature for Task A. Based on the results obtained on the development set, two words on each side of the targeted segment are taken into consideration. For Task B, all ngrams of size upto three are extracted. • Character n-Grams: For each token in the target text in the tweet, all the character n-grams of prefix and suffix of lengths of two and three characters are extracted. This feature is implemented only for the term level task. • Part of Speech (PoS) Information: For both the subtasks, we label each token in the tweet with CMU ARK PoS tagger (Gimpel et al., 2011). The number of each of the PoS tags is kept as feature. 2. Group-II: Semantic Features: To take into account the semantics of the text present in the tweet/targeted segment, we use Lexicon and SentiWordNet based features. • Lexicon Features: We use lexicons such as NRC Hashtag 2 , Sentiment 140 3 , Bing Liu (Liu et al., 2005) and NRC Emotion Lexicons (Mohammad and Turney, 2013) to implement various features. The implementation of features for these tasks is based on the number of tokens associated with positive and negative sentiment using NRC Hashtag, Sentiment 140 and Bing Liu lexicon. For"
S15-2100,S13-2053,0,0.0280389,"the number of occurrences of the username and URLs. The feature is defined for the term level task. • Inverted Segment: An inverted segment is defined as the part of the tweet which occurs after an inverting word (i.e. the tokens that denote the negative context) such as doesn’t, isn’t, can’t, etc. until a punctuation. The polarity of the words occurring in the inverted segment is reversed, i.e. a token with positive or negative sentiment is converted to the token bearing negative or positive sentiment, respectively. The intensity values of the tokens are adopted from the NRC Hashtag lexicon (Mohammad et al., 2013) and Sentiment140 lexicon (Mohammad et al., 2013) which are used to construct the feature vector. The feature vector contains several pieces of information that denote the number of inverted segments in the tweet, sum of intensities of all the words that appear in the inverted segments in the tweet, etc. • Tweet Clusters: We use the CMU Twitter Word Clusters (Owoputi et al., 2013) to generate the clusters of words that appear either in the context of positive or negative sentiment. All the tokens which belong to the positive sub-cluster occur more 603 2.3 Method for Determining the Strength Ou"
S15-2100,P04-1035,0,0.0555587,"s. The huge abundance of inexpensive data, rich in applications, can prove handy for public and corporate institutions. This has urged the scientific community to extract the substantive information from these texts. The proliferation of microblogging sites like Twitter which boasts of user’s comments on everything trending in real time opens The technical study of public sentiment has been a subject of trending research and a significant amount of extensive work is being carried out in the domain. Sentiment Analysis has been handled at the various levels of granularity. Early research works (Pang and Lee, 2004) focussed on the document level classification with further studies at message and term level (Rosenthal et al., 2014). Twitter has also been investigated for its possible applications in the fields of commerce (Jansen et al., 2009; Bollen et al., 2011), elections (O’Connor et al., 2010; Tumasjan et al., 2010), disaster management (Nagy and Stamberger, 2012; Terpstra et al., 2012) etc. using varied approaches and different experimental setups. Semantic Evaluation tasks (Nakov et al., 2013; Rosen601 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 601–6"
S16-1174,esuli-sebastiani-2006-sentiwordnet,0,0.0269477,"is defined by considering whether the current token is present in dependency relations ‘nsubj’, ‘dep’, ‘amod’, ‘nmod’ and ‘dobj’ or not. • Character N-grams: We use all substrings up to length 5 of the current token as features. • Orthographic feature: This feature checks whether the current token starts with the capitalized letter or not. • DT features: We use the top 5 DT expansions of current token as the features. • Expansion Score: OTEs have opinion around them. Opinions are regularly lexicalized with words found in sentiment lexicons. We calculate sentiment score based on SentiWordNet5 (Esuli and Sebastiani, 2006) in English language. For Non-English language, we use our induced lexicons. We calculate sentiment score by considering the window size of 10 (preceding 5 and following 5 tokens of the target one). 5 http://sentiwordnet.isti.cnr.it/ 1131 We additionally extract the following features only for English language. • Chunk information: To identify the boundaries of multi-word OTEs, we use chunk information of the current token as the features. • Lemma: Lemmatization trims the inflectional forms and derivationally related forms of a token to a common base form. • WordNet: We use top 4 noun synsets"
S16-1174,W14-5117,1,0.852868,"Missing"
S16-1174,P97-1023,0,0.0961654,"Entries Positive Negative Neutral 2005 4789 12953 4120 3314 5923 8496 2992 9338 10339 5993 18308 7636 2175 1737 7869 12480 4306 3217 8849 7697 2945 1900 2515 1382 6547 1838 1916 4467 9077 1447 Table 2: Expansion statistics for induced lexicons. Common entries denote the number of words which are present both in the seed lexicon and the induced lexicon. of more than 50 in the background corpus (English8 , French9 , Spanish10 , Dutch11 , Russian12 , Arabic13 ). Finally, we compute the normalized positive, negative and neutral score for each word similar to (Kumar et al., 2015), and inspired by (Hatzivassiloglou and McKeown, 1997). The core assumption is that words tend to be semantically more similar to words of same sentiment. Hence, words appearing more in the expansions of positive (negative/neutral) words get assigned a higher positive (negative/neutral) sentiment score, Here, in difference to (Kumar et al., 2015), we compute normalized positive, negative and neutral scores rather than assigning one of the polarity class to the words. It should be noted that the volume of induced lexicon depends on two factors: (i) number of words in the seed lexicon that have expansions and (ii) pruning threshold for obtaining th"
S16-1174,R15-2003,1,0.756371,"ther than English, we use the universal parser2 for tokenization and parsing. Since we deal with the OTE as a sequence labelling problem, it is necessary to identify the boundary of OT properly. We follow the standard BIO notation, where ‘BASP’, ‘I-ASP’ and ‘O’ represent the beginning, intermediate and outside tokens of a multi-word OTE respectively. e.g. In, ‘Chow (B-ASP) fun (I-ASP) was (O) very (O) dry (O) . (O)’, ’Chow Fun’ is the OTE. 2.2 Features for Aspect Category Detection • Domain Dependency Graph: We use the aspects list produced by Domain Dependency Graph (DDG) for each domain by (Kohail, 2015). The idea is to detect topics underlying a mixed-domain dataset, aggregate individual dependency relations between domain-specific content words, weigh them with tf-idf and produce a DDG by selecting the highest-ranked words and their dependency relations. Since the domains are already given, no topic modeling is required. However, only one domain was provided for French and Spanish, we used ex1 nlp.stanford.edu/software/corenlp.shtml 2 http://www.undl.org/unlsys/uparser/UP. htm 1130 Token drinks price fresh laptop toshiba touchpad DT Expansion beers, wines, coffee, liquids, beverage prices,"
S16-1174,N15-1078,0,0.0106109,"he current token with Stanford CoreNLP tool, and use the NERsequence labels in BIO-scheme as features. 2.4 Features for Sentiment Polarity Classification • Lexical Acquisition: We use lexical expansion for inducing sentiment words based on distributional hypothesis. We observe that for rare words, unseen instances and limited coverage of available lexicons, the distributional expansion can provide a useful backoff technique, also cf. (Govind et al., 2014). For all languages, we construct a polarity lexicon using an external corpus and seed sentiment lexicon. For seed lexicons, we use English (Salameh et al., 2015) and Arabic (Salameh et al., 2015) versions of Bing Liu’s lexicon (Hu and Liu, 2004) for English and Arabic respectively, VU sentiment lexicon6 for French, Dutch and Spanish, a lexicon by (Panchenko, 2014) for Russian, and SentiTurkNet (Dehkharghani et al., 2015) and NRC Emotion for Turkish7 . For inducing a lexicon, we obtain the top 100 DT expansion of each word in the seed lexicon. Next we accept candidate terms that a) occur in the expansions of at least 10 seed terms, b) have a corpus frequency 6 https://github.com/opener-project/ VU-sentiment-lexicon 7 http://saifmohammad.com/WebPages/ N"
S16-1174,S14-2038,0,0.123164,"ation of the current, preceding two and following two tokens as the features. • Head Word and its PoS: We use the head word of the noun phrase and PoS information of the head word. • Prefix and Suffix: We use prefix and suffix of length up to four characters. • Frequent Aspect Term: We build a list of frequently occurring OTEs from the training set. An OTE is considered to be frequent if it appears at least four times in the training corpus. We define a binary feature for the presence or absence of extracted OTEs. • Dependency Relations: In English language, features are defined in line with (Toh and Wang, 2014). For other languages, feature is defined by considering whether the current token is present in dependency relations ‘nsubj’, ‘dep’, ‘amod’, ‘nmod’ and ‘dobj’ or not. • Character N-grams: We use all substrings up to length 5 of the current token as features. • Orthographic feature: This feature checks whether the current token starts with the capitalized letter or not. • DT features: We use the top 5 DT expansions of current token as the features. • Expansion Score: OTEs have opinion around them. Opinions are regularly lexicalized with words found in sentiment lexicons. We calculate sentiment"
S16-1174,S14-2077,0,0.0153565,"are provided in Table 2. We compute the sum of positive, negative and 8 https://snap.stanford.edu/data/ web-Amazon.html 9 http://wacky.sslmit.unibo.it/doku.php? id=corpora 10 http://corporafromtheweb.org/escow14/ 11 http://corporafromtheweb.org/nlcow14/ 12 lib.ruc.ecebooks 13 http://corpora2.informatik.uni-leipzig. de 1132 neutral scores of tokens using induced lexicon for that language as features. In addition, scores as given in the seed lexicon are also used as features. For English, we also computed these features from different lexicons: AFINN (Nielsen, 2011), NRC Hashtag, Sentiment 140 (Zhu et al., 2014), NRC Emotion (Mohammad and Turney, 2013) and Bing Liu (Hu and Liu, 2004). • Word N-gram: All unigrams and bigrams tokens are extracted from the training set are used as a binary feature, where 1 and 0 indicates the presence and absence of n-grams in the review. • Entity-Attribute Pair: We use E#A pair as a binary feature for sentiment classification. 3 Datasets, Experimental Results and Discussions For feature selection and hyperparameter tuning, we perform five-fold cross-validation on the training set. For Slot 1 and Slot 3, we use supervised classification using Support Vector Machine (SVM"
S17-2009,P05-1045,0,0.0125919,"inding the focus of the question and comment is important in measuring if the comment specifically covers the aspects of the question. We extract keywords from the texts using the RAKE keyword extraction algorithm (Rose et al., 2010), and derive features from the keyword match between question and comment. We also use the relative importance of common keywords as feature values. In case of factoid questions, or especially in subtask B, Named Entity Recognition becomes an important tool for computing the relevance of a text. We extract named entities using the Stanford Named Entity Recognizer (Finkel et al., 2005) and classify words into seven entity categories including PERSON, LOCATION, ORGANIZATION, DATE, MONEY, PERCENT, and TIME. We compute if both question and comment have named entities, and if these belong to the same The algorithm to construct this dynamic graph is given in Algorithm 1. We simultaneously construct two graphs, a user graph and a dialogue graph. Initially, the user graph has the question node and the dialogue graph is empty. We add new users to the graphs according to the timestamp of their occurrence in the thread. For each new comment, we add edges to each previous user and the"
S17-2009,S16-1137,0,0.0215787,"Missing"
S17-2009,P15-2113,0,0.0524853,"Missing"
S17-2009,S15-2048,0,0.0400066,"Missing"
S17-2009,S17-2003,0,0.0764522,"Missing"
S17-2009,S16-1172,0,0.0219561,"Missing"
S17-2009,S15-2036,0,0.134421,"Missing"
S17-2009,S16-1135,0,0.0366377,"Missing"
S17-2087,C00-1044,0,0.485514,"Missing"
S17-2087,P16-2064,0,0.0642479,"(Castillo et al., 2011; Derczynski and Bontcheva, 2014; Qazvinian et al., 2011). Rumours are the statement which cannot be verified for its correctness. These rumours may confuse people with the unverified information and drive them in poor decision making. In many organizations(political, administration etc.), detection and support for rumour invites great interest from the concerned authorities. Recently, researchers across the globe have started addressing the challenges related to rumours. A time sequence classification technique has been proposed for detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in treestructured conversations for classifying stance. A 2 System Overview We adopted a supervised classification approach for both the tasks. We use Decision Tree (DT), Naive Bayes (NB) and Support Vector Machine 497 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 497–501, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Tweet conversation thread Src: Very good on #Putin coup by @CoalsonR: Three Scenarios For A Succession In Russia http://t.co/fotdqxD"
S17-2087,H05-1044,0,0.0810695,"Missing"
S17-2087,C16-1230,0,0.0121576,"Derczynski and Bontcheva, 2014; Qazvinian et al., 2011). Rumours are the statement which cannot be verified for its correctness. These rumours may confuse people with the unverified information and drive them in poor decision making. In many organizations(political, administration etc.), detection and support for rumour invites great interest from the concerned authorities. Recently, researchers across the globe have started addressing the challenges related to rumours. A time sequence classification technique has been proposed for detecting the stance against a rumor (Lukasik et al., 2016). Zubiaga et al. (2016) used sequence of label transitions in treestructured conversations for classifying stance. A 2 System Overview We adopted a supervised classification approach for both the tasks. We use Decision Tree (DT), Naive Bayes (NB) and Support Vector Machine 497 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 497–501, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Tweet conversation thread Src: Very good on #Putin coup by @CoalsonR: Three Scenarios For A Succession In Russia http://t.co/fotdqxDfEV Rep1: @andersostlun"
S17-2087,D11-1147,0,\N,Missing
S17-2153,baccianella-etal-2010-sentiwordnet,0,0.145786,"Missing"
S17-2153,P13-2005,0,0.0331544,"mic system of a country. Therefore, a reliable and prompt delivery of information plays an important role in the financial market. Up until the last decade printed/television news were the major source of stock marketrelated information. However, with the introduction of micro-blogging websites (e.g. Twitter etc.) the trend has been shifted. The rise of Twitter and StockTwits has given the people and organizations an opportunity to vent out their feelings and views. This information can be used by an individual or an organization to make an informed prediction related to any company or stock (Si et al., 2013). This opens a new avenue for sentiment analysis in the financial domain of microblogs and news. News headlines are a short piece of text describing the nature of an article. Due to space constraints, headlines normally follow a compact writing style, known as headlinese, which limits 894 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 894–898, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Track 1 Message: Score: Span: Cashtag: Track 2 Message: Score: Company: Microblogs Putting on a little $F short, prevail"
S17-2153,S17-2089,0,0.0852026,"Missing"
S17-2153,P97-1023,0,0.632546,"Missing"
S17-2153,J09-3003,0,0.44437,"Missing"
S17-2153,S13-2053,0,0.0274224,"lysis (Khanarian and Alwarez-Melis, 2012). Moreover, each tweet can have reference to multiple company names (or stock symbols) and the expressed sentiment can be different towards different companies. Hence, there is a need to perform fine-grained sentiment analysis wherein, generally, a context is used to decide the relevant portion of a tweet for a particular company. Another inherent challenge with the microblog and news data is the use of short languages, hashtag, emoticons and embedded URL. Special attention should be given to these as they can provide some important hidden information (Mohammad et al., 2013). Example - #bullishMarket and #increasingProfit can reflect positive sentiment. These are some of the major challenges associated with fine-grained sentiment analysis of microblogging and news data. The SemEval-2017 task 5 (Fine-Grained Sentiment Analysis on Financial Microblogs and News) has two tracks (Cortis et al., 2017). For both the tracks, the overall aim was to assign a sentiment score to a cashtag/company over a continuous range of -1 (very negative/bearish) to 1 (very positive/bullish). First track involves finding a sentiment score towards a given ‘cashtag’ (stock symbol preceded b"
S17-2154,H05-1044,0,0.0475796,"c features. We trained a multilayer perceptron on top of the following features. • Character ngrams: tf-idf weighted counts of continuous sequences of 2, 3, and 4 characters; • Word ngrams: tf-idf weighted counts of continuous sequences of 1, 2, 3, and 4 words; • POS-tag: parts of speech tags of each token in the text; • Lexicons: – Following set of features are used for each of the four lexicons: Opinion Lexicon (Liu et al., 2005), Loughran and McDonald Sentiment Word Lists (Loughran and McDonald, 2011), MPQA Lexicon [+1.0 for strong positive, +0.5 for weak positive, similarly for negative] (Wilson et al., 2005) and Harvard’s General Inquirer (Stone et al., 1962): ∗ positive count: number of positive tokens in a tweet/title. Convolutional Neural Network (CNN) Convolutional neural network consists of one or more convolution and pooling layers followed by one or more dense layers. Our system uses 2 convolution layers followed by a max pool layer, 2 dense layers and an output layer. Size of convolution filters dictates the hidden features to be ex1 Multilayer Perceptron (MLP) - Vector Averaging Model Concatenation of word vectors for generating sentence embeddings often face the curse of highdimensional"
S17-2154,S13-2053,0,0.0241879,"Here we discuss second stage of the our proposed system. We merge predicted sentiment scores of all four models (CNN, LSTM, Vector Averaging, Feature Driven) to create a new feature vector, and then fed it into a multilayer perceptron (MLP) network for training. Figure 1 shows, an overall schema of the proposed approach. ∗ negative count: number of negative tokens in a tweet/title. ∗ net count: positive count - negative count in tweet/title. – In addition we use four NRC Lexicons: Hashtag Context, Hashtag Sentiment, Sentiment140, Sentiment140 Context (Svetlana Kiritchenko and Mohammad, 2014; Mohammad et al., 2013) for the microblogs messages. Following set of features are extracted for each of them: ∗ positive count, negative count and net count. ∗ sum of positive scores, negative scores and all scores. ∗ maximum of positive and negative scores. Figure 1: Ensembling Network Structure • Pointwise Mutual Information (PMI): We calculate a sentiment score for each term in our training corpus to get the association of each term with positive as well as negative sentiment. 3 3.1 PMI is calculated as follows:f req(w, pos) ∗ N f req(w) ∗ f req(pos) In the above equation f req(w, pos) is the frequency of word w"
S17-2154,D14-1162,0,0.0818328,"e curse of highdimensionality. In an attempt to get a constant low-dimensional feature vector we employ vector averaging technique for producing sentence vector. We perform an element wise averaging of the word vectors in a tokenized tweet/headline. We then use the sentence embeddings to train a 3layered neural network for the prediction. Word embeddings are generally helpful in many natural processing tasks due to it’s excellence in capturing hidden semantic structures. For word embeddings we used two pre-trained embedding models: GloVe1 and Word2Vec2 . For microblogs messages we used GloVe (Pennington et al., 2014) and Word2Vec (Godin et al., 2015) twitter model trained on 2 billion and 400 million tweets respectively. For news headline we used GloVe common crawl model trained on 802 billion words and Word2Vec Google News model (Mikolov et al., 2013). We experimented with 200, 300 and 400 dimension vectors and observed that 200 & 300 dimension vectors are the near-optimal case for microblogs messages and news headlines respectively. We have used concatenation of word embeddings to form sentence embeddings. 2.2 Long Short Term Memory (LSTM) http://nlp.stanford.edu/projects/glove/ https://code.google.com/"
S19-2105,W18-4411,0,0.406543,"Missing"
S19-2105,D14-1162,0,0.0846091,"Huang, 2017) discusses the Bi-LSTM with attention mechanism with learning components context improved the classifier performance. Figure 1: Sub Task A:CNN-BiLSTM-Attention (Founta et al., 2018) studied different forms of abusive behaviour and made public annotated corpus of 80K Tweets categorized into 8 labels like Hate, aggressive, cyber bullying, normal , Spam. 3 3.1 Methodology Task A:CNN-BiLSTM-Attention In this model first we converted all the words to their unique index. Then all the unique index in the sentences were mapped to their real valued vectors of Dimensions 100 using Glove by (Pennington et al., 2014) from Embedding Matrix. Convolution layers is used to extract useful information by convolving i words at a time using learnable kernel of size i*h where i = [2,3,4] and h is of size equal to the dimensions. The element wise dot product is performed to get the feature map f1 . N numbers of filters are used to get feature map = [f1 ,f2 ...fn ]. Pooling reduces the size of representation by selecting max value from each feature map which is then passed to the BiLSTM layer with 100 hidden units. The sentence level representation is then passed to activation layers to capture the important keyword"
S19-2105,W17-3010,0,0.0281948,"Missing"
S19-2105,W17-1101,0,0.0212488,"This task consist of classifying between offensive and not offensive comments Task B: The Offensive language was further needs to be classified into Targeted(TIN) and UnTargeted(UNT). Task C: The targeted offensive needs to be further classified into Individual(IND), Group(GRP) and Other(OTH). 2 Related Work (Nockleby, 2000) defined hate speech as any communication that demean any person or any group on the basis of race, color, gender, ethnicity, sexual orientation, and nationality. (Kowalski et al., 2014) defined cyber aggression as using digital media to intentionally harm another person. (Schmidt and Wiegand, 2017) presents a survey on the existing research in this field and different set of features used in machine learning and Deep learning were discussed. (Silva et al., 2016) proposed and validated sentence structure to detect hate speech and also used this to construct hate speech datasets. They also provided the characteristics study to identify the main targets of hate speech in Twitter and Whisper. They designed two rules i.e I<intensity><user intent><hate Target> and <one word> people ex:”black people”,”maxican” people. (Waseem, 2016) examined the performance of classification based on training"
S19-2105,W16-5618,0,0.0324016,"tal media to intentionally harm another person. (Schmidt and Wiegand, 2017) presents a survey on the existing research in this field and different set of features used in machine learning and Deep learning were discussed. (Silva et al., 2016) proposed and validated sentence structure to detect hate speech and also used this to construct hate speech datasets. They also provided the characteristics study to identify the main targets of hate speech in Twitter and Whisper. They designed two rules i.e I<intensity><user intent><hate Target> and <one word> people ex:”black people”,”maxican” people. (Waseem, 2016) examined the performance of classification based on training performed on amateur and expert annotations. (Ross et al., 2017) concluded that hate speech requires significantly better definitions and Introduction Due to the exponential rise in the usage of internet user generated content in the form of blogs, posts, comments etc. have been increased manifold. Some users also using this platform to target any individual or any particular group on social media on the basis of certain attributes, sharing different views. Many studies have been conducted on offensive language, hate speech, cyberbu"
S19-2105,W17-3013,0,0.039401,"Missing"
S19-2105,gao-huang-2017-detecting,0,0.0431432,"Missing"
S19-2105,W17-3012,0,0.0938893,"Missing"
S19-2105,N12-1084,0,0.415163,"Missing"
S19-2105,W18-4401,0,0.078276,"Missing"
S19-2105,malmasi-zampieri-2017-detecting,0,0.130909,"Missing"
U13-1008,U11-1012,1,0.414703,"Missing"
U13-1008,J13-3008,0,0.0718542,"Missing"
U13-1008,U04-1000,0,\N,Missing
W09-3517,P06-2025,1,0.893972,"Missing"
W09-3517,2003.mtsummit-papers.17,0,0.504975,"ering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard run and two nonstan"
W09-3517,C00-1056,0,0.396199,"Missing"
W09-3517,P04-1021,0,0.304488,"ji Bandyopadhyay Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009"
W09-3517,2005.mtsummit-papers.36,0,0.226978,"Missing"
W09-3517,I08-1009,0,0.162852,"Missing"
W09-3517,W03-1508,0,0.0702944,"Computer Science and Engineering Department Jadavpur University, Kolkata-700032, India amitava.research@gmail.com, asif.ekbal@gmail.com, tapabratamondal@gmail.com, sivaji_cse_ju@yahoo.com different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of NEs is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). Abstract This paper reports about our work in the NEWS 2009 Machine Transliteration Shared Task held as part of ACL-IJCNLP 2009. We submitted one standard"
W09-3517,P02-1051,0,0.119126,"Missing"
W09-3517,W09-3501,0,\N,Missing
W09-3517,W09-3502,0,\N,Missing
W09-3517,J98-4003,0,\N,Missing
W09-3539,I08-1045,0,0.0446319,"such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portion of this corpus containing 200K wordforms has been manually annotated with the four NE tags namely, Person, Location, Organization and Miscellaneous. We have also used the NE annotated data of 1"
W09-3539,I08-2077,1,0.57992,"uired measure. 6. Although Indian languages have a very old and rich literary history, technological developments are of recent origin. 7. Web sources for name lists are available in English, but such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portio"
W09-3539,I08-5008,1,0.620587,"Indian languages have a very old and rich literary history, technological developments are of recent origin. 7. Web sources for name lists are available in English, but such lists are not available in Bengali. This necessitates the use of transliteration for creating such lists. A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are used for handling the unknown words. More recently, the works in the area of Bengali NER can be found in Ekbal et al. (2008a), and Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Saha et al. (2008) with a hybrid feature set based ME approach. Various works of NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. 2 Named Entity Recognition in Bengali We have used a Bengali news corpus (Ekbal and Bandyopadhyay, 2008c), developed from the web-archive of a widely read Bengali newspaper for NER. A portion of this corpus containing 200K word"
W09-3539,W03-0425,0,0.144415,"Missing"
W10-2411,I08-1009,0,0.0692551,"Missing"
W10-2411,W09-3517,1,0.31321,"ithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tamil Non-Standard Run-1: TNSR1). Abstract This paper reports about our work in the NEWS 2010 Shared Task on Transliteration Generation held"
W10-2411,W03-1508,0,0.211651,"niversity of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hind"
W10-2411,P02-1051,0,0.0210547,"e alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliterati"
W10-2411,P06-2025,1,0.798718,"uage Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Run: HSR), Kannada (Kannada Standard Run: KSR) and Tamil (Tamil Standard Run: TSR) were submitted. Two non-standard runs for English to Hindi (Hindi Non-Standard Run 1 & 2: HNSR1 & HNSR2) and Bengali (Bengali NonStandard Run 1 & 2: BNSR1 & BNSR1) transliteration were submitted. Only one non-standard run were submitted for Kannada (Kannada NonStandard Run-1: KNSR1) and Tamil (Tam"
W10-2411,2003.mtsummit-papers.17,0,0.0288078,"Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Bengali Standard Run: BSR), Hindi (Hindi Standard Ru"
W10-2411,C00-1056,0,0.0909754,"Missing"
W10-2411,P04-1021,0,0.131127,"al Linguistics4 University of Heidelberg Im Neuenheimer Feld 325 69120 Heidelberg, Germany ekbal@cl.uni-heidelberg.de of alphabets is trivial: the word is left as it is. However, for languages those use different alphabet sets the names must be transliterated or rendered in the target language alphabets. Transliteration of words is necessary in many applications, such as machine translation, corpus alignment, cross-language Information Retrieval, information extraction and automatic lexicon acquisition. In the literature, a number of transliteration algorithms are available involving English (Li et al., 2004; Vigra and Khudanpur, 2003; Goto et al., 2003), European languages (Marino et al., 2005) and some of the Asian languages, namely Chinese (Li et al., 2004; Vigra and Khudanpur, 2003), Japanese (Goto et al., 2003; Knight and Graehl, 1998), Korean (Jung et al., 2000) and Arabic (Al-Onaizan and Knight, 2002a; Al-Onaizan and Knight, 2002c). Recently, some works have been initiated involving Indian languages (Ekbal et al., 2006; Ekbal et al., 2007; Surana and Singh, 2008). The detailed report of our participation in NEWS 2009 could be found in (Das et al., 2009). One standard run for Bengali (Benga"
W10-2411,2005.mtsummit-papers.36,0,0.0629612,"Missing"
W10-2411,W10-2402,0,\N,Missing
W10-2411,P97-1017,0,\N,Missing
W10-2415,W03-0430,0,0.00801887,"ld standard dataset for FG-NERC. This dataset is used to benchmark methods for classifying NEs at various levels of fine-grainedness using classical NERC techniques and global contextual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93"
W10-2415,P08-1003,0,0.039537,"Missing"
W10-2415,P06-1102,0,0.0194731,"Missing"
W10-2415,U06-1013,0,0.0287566,"ty Recognition and Classification Asif Ekbal, Eva Sourjikova, Anette Frank and Simone Paolo Ponzetto Department of Computational Linguistics Heidelberg University, Germany {ekbal,sourjikova,frank,ponzetto}@cl.uni-heidelberg.de Abstract addressed the problem of recognizing and categorizing fine-grained NE classes (such as biologist, composer, or athlete) in an open-domain setting. Fine-grained NERC is expected to be beneficial for a wide spectrum of applications, including Information Retrieval (Mandl and WomserHacker, 2005), Information Extraction (Pas¸ca et al., 2006a) or Question-Answering (Pizzato et al., 2006). However, manually compiling widecoverage gazetteers for fine-grained NE classes is time-consuming and error-prone. Also, without an extrinsic evaluation, it is difficult to define a priori which classes are relevant for a particular domain or task. Finally, prior research in FG-NERC is difficult to evaluate, due to the diversity of NE classes and datasets used. Accordingly, in the interest of a general approach, we address the challenge of capturing a broad range of NE classes at various levels of conceptual granularity. By turning FG-NERC into a widely applicable task, applications are free"
W10-2415,E06-1003,0,0.0545995,"Missing"
W10-2415,W02-2024,0,0.0209751,"tual information inspired from Word Sense Disambiguation approaches. Our results indicate high difficulty of the task and provide a ‘strong’ baseline for future research. 1 Introduction Named Entity Recognition and Classification (cf. Nadeau and Sekine (2007)) is a well-established NLP task relevant for nearly all semantic processing and information access applications. NERC has been investigated using supervised (McCallum and Li, 2003), unsupervised (Etzioni et al., 2005) and semi-supervised (Pas¸ca et al., 2006b) learning methods. It has been investigated in multilingual settings (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) and special domains, e.g. biomedicine (Ananiadou et al., 2004). The classical NERC task is confined to coarsegrained named entity (NE) classes established in the MUC (MUC-7, 1998) or CoNLL (Tjong Kim Sang, 2002) competitions, typically P ERS, L OC, O RG, M ISC. While most recent work concentrates on feature engineering and robust statistical models for various domains, few researchers 93 Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 93–101, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics 2 Related work fu"
W10-2415,W03-0420,0,0.059724,"nces for a given target class c, we add all instances labeled with the hyponyms of c to I. All other instances (not in that subtree) are labeled as being Outside (O-) a NE. This approach ensures that, for each node, the dataset contains two classes (NE and O) only, and implicitly ‘propagates’ the instances up the tree. As a result, non-leaf nodes that did not have any instance in the original dataset become populated. Also, the classification of classes at higher levels is based on larger datasets. NERC using a MaxEnt tagger Our baseline system is modeled following a Maximum Entropy approach (Bender et al., 2003, inter alia). The MaxEnt model produces a probability for each class label t (the NE tag) of a classification instance, conditioned on its context of occurrence h. This probability is calculated by:   n X 1 λj fj (h, t) (1) exp  P (t|h) = Z(h) j=1 where fj (h, t) is the j-th feature with associated weight λj and Z(h) is a normalization constant to ensure a proper probability distribution.3 Given a word wi to be classified as Beginning, Inside or Outside (IOB) of a NE, we extract as features: 1. Context words. The words occurring within i+2 the context window wi−2 = wi−2 . . . wi+2 . 2. Wo"
W10-2415,E06-1002,0,0.10149,"Missing"
W10-2415,C02-1130,0,0.578916,"Missing"
W10-2415,W03-0425,0,0.0098922,"ure frequency cutoff. The best configuration is found by optimizing the F1 measure on the development data with various feature representations. The chosen features are: 1, 2 (with n = 3), 4, 5, 6, 7 and 8. Evaluation on the test set is performed blindly, using this feature set. The results are presented in Table 2. The MaxEnt labeler achieves performance comparable with the CoNLL-2003 task participants, ranking 12th among the 16 systems participating in the task, with a 2 point margin off the F1 of the most similar system of Bender et al. (2003) and 7 points below the best-performing system (Florian et al., 2003). The former used a relatively complex set of features and different gazetteers extracted from unannotated data. The latter combined four diverse classifiers, namely a robust linear classifier, maximum entropy, transformationbased learning and a hidden Markov model. They used different feature sets, unannotated data and an additional NE tagger. In comparison, our NERC system is simpler and based on a small set of features that can be easily obtained for many languages. Besides, it does not make use of any external resources and still shows state-of-the-art performance on the overall data. 6 6."
W10-2415,D07-1026,0,0.0336034,"Missing"
W10-2415,C08-1034,0,0.0579714,"Missing"
W10-2415,W09-1125,0,0.580182,"Missing"
W10-2415,W00-0730,0,0.110963,"sly associates proper names with their corresponding semantic class. Mapping to the WordNet person domain. In order to perform a hierarchical classification of people, we need a taxonomy for the domain at hand. We achieve this by mapping the extracted class labels to WordNet synsets. In our setting, we map against all synsets found under person#n#1, Pattern-based extraction of NE-concept pairs. NEs are often introduced by so-called appositional structures as in (1), which overtly express which semantic class (here, painter) the NE (Kandinsky) belongs to. Appositions involving 1 We use YamCha (Kudo and Matsumoto, 2000) to perform phrase chunking. 95 Level #C 1 1 2 29 3 57 4 63 5 37 6 18 7 6 8 2 all 213 which are direct hypernyms of at least one instance in WordNet (CW N pers+Inst ).2 Since our goal is to map class labels to synsets (i.e. our future NE classes), we check each class label candidate against all synonyms contained in the synset. At this point we have to deal with two cases: two extracted class label candidates (synonyms such as doctor, physician) will map to a single synset, while ambiguous class labels (e.g. director) can be mapped to more than one synset. In the latter case, we heuristically"
W10-2415,W02-1006,0,0.0271859,"our baseline MaxEnt tagger is very local, including at most the two preceding and succeeding words. Hence, the classifier is not able to capture informative contextual clues in a larger context. Previous work has related FG-NERC to WSD approaches (Alfonseca and Manandhar, 2002). Accordingly, we investigate two context-sensitive approaches inspired from WSD proposals, which consider a more global context for classification. We first define a new feature set to induce a new MaxEnt model (MaxEnt-B) which only uses lexical features from a larger context window, as used in standard supervised WSD (Lee and Ng, 2002): 1. PoS context. The part-of-speech occurring within the context window posi+3 i−3 = posi−3 . . . posi+3 . 2. Local collocation. Local collocations Cnm surrounding wi . We use C−2,−1 and C1,2 . 3. Content words in surrounding context. We i+3 consider all unigrams in contexts wi−3 = wi−3 . . . wi+3 of wi (crossing sentence boundaries) for the entire training data. We convert tokens to lower case, remove stopwords, numbers and punctuation symbols. We define a feature vector of length 10 using the 10 most frequent content words. Given a classification instance, the feature corresponding to token"
W10-2415,W00-0726,0,\N,Missing
W11-0904,baccianella-etal-2010-sentiwordnet,0,0.0590986,"Missing"
W11-0904,banea-etal-2008-bootstrapping,0,0.0707488,"Missing"
W11-0904,S07-1094,0,0.0622787,"Missing"
W11-0904,J02-3001,0,0.0596275,"Missing"
W11-0904,S07-1067,0,0.045748,"Missing"
W11-0904,S10-1077,1,0.88507,"Missing"
W11-0904,strapparava-valitutti-2004-wordnet,0,0.169951,"Missing"
W11-0904,stoyanov-cardie-2008-annotating,0,0.18126,"o annotate emotional content. Our motivation is that though events and sentiments are closely coupled with each other from social, psychological and commercial perspectives, very little attention has been given about their detection and analysis. To the best of our knowledge, only a few tasks have been attempted (Fukuhara et al., 2007) (Das et al., 2010). 21 Sometimes, the opinion topics are not necessarily spatially coherent as there may be two opinions in the same sentence on different topics, as well as opinions that are on the same topic separated by opinions that do not share that topic (Stoyanov and Cardie 2008). The authors have established their hypothesis by applying the coreference technique. Similarly, we have adopted the co-reference technique based on basic rhetoric components for identifying the association between event and sentiment expressions. In addition to that, we have also employed the lexical equivalence approach for identifying their association. 3 Event Identification In this work, we propose a hybrid approach for event identification from the text under the TempEval-2010 framework. We use Conditional Random Field (CRF) as the underlying machine learning algorithm. We observe that"
W11-0904,de-marneffe-etal-2006-generating,0,\N,Missing
W11-0904,P00-1010,0,\N,Missing
W11-0904,W00-1308,0,\N,Missing
W11-0904,S07-1013,0,\N,Missing
W11-0904,S10-1063,0,\N,Missing
W11-0904,N04-1030,0,\N,Missing
W11-1908,P05-1045,0,0.0191921,"Missing"
W11-1908,H05-1004,0,0.507946,"asif@iitp.ac.in, massimo.poesio@unitn.it Abstract Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multiobjective function Optimization (MOO) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously. 1 Introduction Many evaluation metrics have been proposed for anaphora resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Doddington et al., 2000; Luo, 2005; Recasens and Hovy, 2011). Each of these metrics seems to capture some genuine intuition about the the task, so that, unlike in other areas of HLT, none has really taken over. This makes it difficult to compare systems, as dramatically demonstrated by the results of the Coreference Task at SEMEVAL 2010 (Recasens et al., 2010). It was therefore wise of the CONLL organizers to use a basket of metrics to assess performance instead of a single one. This situation suggests using methods to optimize systems according to more than one metric at once. And as it happens, techniques for doing just that"
W11-1908,H05-1068,0,0.0196597,"tomatic optimization of both feature selection and of learning parameters, also considering 61 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61–65, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics two different machine learners, TimBL and Ripper. Her results suggest that such techniques yield improvements on the MUC-6/7 datasets. Recasens and Hovy (2009) carried out an investigation of feature selection for Spanish using the ANCORA corpus. A form of multi-objective optimization was applied to coreference by Munson et al. (2005). Munson et al. (2005) did not propose to train models so as to simultaneously optimize according to multiple metrics; instead, they used ensemble selection to learn to choose among previously trained models the best model for each example. Their general conclusion was negative, stating that “ensemble selection seems too unreliable for use in NLP”, but they did see some improvements for coreference. Genetic algorithms are known to be more effective for solving MOO than classical methods such as weighted metrics, goal programming (Deb, 2001), because of their population-based nature. A particul"
W11-1908,P02-1014,0,0.480729,"inguistically-rich system for coreference resolution might benefit a lot from feature selection. In particular, we have investigated Non-Dominated Sorting Genetic Algorithm II (Deb et al., 2002) for multiobjective optimization. In subsequent work, we plan to expand the optimization technique to consider also learning parameters optimization, classifier selection, and learning model selection. 4 Results 4.1 Acknowledgments Development set Table 1 compares the performance level obtained using all the features with that of loose reimplementations of the systems proposed by Soon et al. (2001) and Ng and Cardie (2002), commonly used as baselines. Our reimplementation of the Ng & Cardie model uses only a subset of features. The results in Table 1 show that our system with a rich feature set does not outperform simpler baselines (and, in fact, yields poorer results). A similar trend has been observed by Ng and Cardie (2002), where the improvement was only possible after manual feature selection. The last line of Table 1 shows the performance level of the best chromosome found through the MOO technique. As it can be seen, it outperforms all the baselines according to all the measures, leading to an improvemen"
W11-1908,W11-1901,0,0.150736,"Missing"
W11-1908,W09-2411,0,0.0655299,"Missing"
W11-1908,I11-1011,1,0.674013,"one. This situation suggests using methods to optimize systems according to more than one metric at once. And as it happens, techniques for doing just that have been developed in the area of Genetic Algorithms—so-called multi-objective optimization techniques (MOO) (Deb, 2001). The key idea of our submission is to use MOO techniques to optimize our anaphora resolution system according to three metrics simultaneously: the MUC scorer (a member of what one might call the ’link-based’ cluster of metrics) and the two CEAF metrics (representative of the ’entity-based’ cluster). In a previous study (Saha et al., 2011), we show that our MOO -based approach yields more robust results than single-objective optimization. We test two types of optimization: feature selection and architecture–whether to learn a single model for all types of anaphors, or to learn separate models for pronouns and for other nominals. We also discuss how the default mention extraction techniques of the system we used for this submission, BART (Versley et al., 2008), were modified to handle the all-mention annotation in the OntoNotes corpus. In this paper, we first briefly provide some background on optimization for anaphora resolutio"
W11-1908,J01-4004,0,0.885984,"d split classifiers. We considered 42 features, including 7 classifying mention type, 8 for string matching of different subparts and different levels of exactness, 2 for aliasing, 4 for agreement, 12 for syntactic information including also binding constraints, 3 encoding salience, 1 encoding patterns extracted from the Web, 3 for proximity, and 2 for 1st and 2nd person pronouns. Again because of time considerations, we used decision trees as implemented in Weka as our classification model instead of maximum-entropy or SVMs. Finally, we used a simple mention-pair model without ranking as in (Soon et al., 2001). 3.2 Mention detection BART supports several solutions to the mention detection (MD) task. The users can input precomputed mentions, thus, experimenting with gold boundaries or system boundaries computed by external modules (e.g., CARAFE). BART also has a built-in mention extraction module, computing boundaries heuristically from the output of a parser. For the CoNLL shared task, we use the BART internal MD module, as it corresponds better to the mention detection guidelines of the OntoNotes dataset. We have further adjusted this module to improve the MD accuracy. The process of mention detec"
W11-1908,P08-4003,1,0.950061,"MUC scorer (a member of what one might call the ’link-based’ cluster of metrics) and the two CEAF metrics (representative of the ’entity-based’ cluster). In a previous study (Saha et al., 2011), we show that our MOO -based approach yields more robust results than single-objective optimization. We test two types of optimization: feature selection and architecture–whether to learn a single model for all types of anaphors, or to learn separate models for pronouns and for other nominals. We also discuss how the default mention extraction techniques of the system we used for this submission, BART (Versley et al., 2008), were modified to handle the all-mention annotation in the OntoNotes corpus. In this paper, we first briefly provide some background on optimization for anaphora resolution, on genetic algorithms, and on the method for multiobjective optimization we used, Non-Dominated Sorting Genetic Algorithm II (Deb et al., 2002). After that we discuss our experiments, and present our results. 2 Background 2.1 Optimization for Anaphora Resolution There have only been few attempts at optimization for anaphora resolution, and with a few exceptions, this was done by hand. The first systematic attempt at autom"
W11-1908,M95-1005,0,0.476534,"y Patna ∗ University of Essex uryupina@gmail.com, sriparna@iitp.ac.in, asif@iitp.ac.in, massimo.poesio@unitn.it Abstract Because there is no generally accepted metric for measuring the performance of anaphora resolution systems, a combination of metrics was proposed to evaluate submissions to the 2011 CONLL Shared Task (Pradhan et al., 2011). We investigate therefore Multiobjective function Optimization (MOO) techniques based on Genetic Algorithms to optimize models according to multiple metrics simultaneously. 1 Introduction Many evaluation metrics have been proposed for anaphora resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Doddington et al., 2000; Luo, 2005; Recasens and Hovy, 2011). Each of these metrics seems to capture some genuine intuition about the the task, so that, unlike in other areas of HLT, none has really taken over. This makes it difficult to compare systems, as dramatically demonstrated by the results of the Coreference Task at SEMEVAL 2010 (Recasens et al., 2010). It was therefore wise of the CONLL organizers to use a basket of metrics to assess performance instead of a single one. This situation suggests using methods to optimize systems according to more than one metr"
W11-1908,S10-1001,1,\N,Missing
W11-1908,doddington-etal-2004-automatic,0,\N,Missing
W13-4305,P91-1022,0,0.475773,"Missing"
W13-4305,P93-1002,0,0.0841543,"correctness of the alignment of the rule based system. The remainder of the paper is organized as follows. Next section briefly elaborates the related work. The proposed system is described in Section 3. Section 4 states the tools and resources used for the various experiments. Section 5 includes the results obtained, together with some analysis. Section 6 concludes and provides avenues for further work. 2 Related Works The works related to alignment are mostly developed for machine translation task. Some works in sentence alignment can be found in (Brown, 1991) and (Gale and Church, 1993). (Chen, 1993) developed a method which was slower but more accurate than the sentencelength based Brown and Gale algorithm. (Wu, 1994) used an approach which was adopted from Gale and Church‘s method for Chinese. They used a small corpus-specific bilingual lexicon to improve alignment accuracy in texts containing multiple sentences of similar length. (Melamed 1996, 1997) also proposed a method based on word correspondences. (Plamondon, 1998) developed a two-pass approach, in which a method similar to the one proposed by Melamed identifies points of correspondence in the text that constrain a second-pass se"
W13-4305,P07-2045,0,0.00635855,"Missing"
W13-4305,R11-1084,1,0.828413,"Missing"
W13-4305,W04-3248,0,0.0288507,"words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huang et al. (2003). 3 3.2 It has been observed from the detailed text analysis that almost all events are associated with some actors (―anything having existence (living or nonliving)”), either active or passive. More generally, event actions are associated with persons or organizations and sometimes with l"
W13-4305,W96-0201,0,0.202806,"Missing"
W13-4305,C94-2178,0,0.220366,"Missing"
W13-4305,P97-1039,0,0.121292,"Missing"
W13-4305,moore-2002-fast,0,0.042022,"rate than the sentencelength based Brown and Gale algorithm. (Wu, 1994) used an approach which was adopted from Gale and Church‘s method for Chinese. They used a small corpus-specific bilingual lexicon to improve alignment accuracy in texts containing multiple sentences of similar length. (Melamed 1996, 1997) also proposed a method based on word correspondences. (Plamondon, 1998) developed a two-pass approach, in which a method similar to the one proposed by Melamed identifies points of correspondence in the text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical"
W13-4305,E03-1035,0,0.0393444,"e in the text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huan"
W13-4305,J02-3001,0,0.248147,"Missing"
W13-4305,P03-1021,0,0.0329635,"Missing"
W13-4305,W03-1502,0,0.0389762,"he text that constrain a second-pass search based on the statistical translation model. (Moore, 2002) developed a hybrid sentence-alignment method using sentence length-based and word-correspondencebased models. This model is fast, very accurate, and requires that the corpus be separated into words and sentences. In the hybrid model, they used the sentence pairs that are assigned the highest probability of alignment to train a modified version of IBM Translation Model 1 (Brown, 1993). (Fung, 1994) presented K-vec, an alternative alignment strategy, that starts by estimating the lexicon. Moore (2003) used capitalization cues for identifying NEs on the English side and then applied statistical techniques to decide which portion of the target language corresponds to the specified English NE. A Maximum Entropy model based approach for English—Chinese NE alignment has been proposed in Feng et al. (2004) which significantly outperforms IBM Model 4 and HMM. A method for 1 http://ltrc.iiit.ac.in/showfile.php?filename=downloads/shallow _parser.php 37 automatically extracting NE translingual equivalences between Chinese and English based on multi-feature cost minimization has been proposed in Huan"
W13-4305,N03-1017,0,0.010592,"Missing"
W13-4305,J93-1004,0,\N,Missing
W13-4305,de-marneffe-etal-2006-generating,0,\N,Missing
W13-4305,W09-1104,0,\N,Missing
W13-4305,J93-2003,0,\N,Missing
W13-4305,W09-3539,1,\N,Missing
W13-4305,W04-3250,0,\N,Missing
W14-5117,J92-4003,0,0.486538,"chnique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008), and the datasets reported in (Biemann et al., 2007b) for Hindi and German. These datasets were used for unsupervised lexical acquisition. A Distributional Thesaurus (DT) is an automatically computed resource that relates words according to their similarity. A"
W14-5117,D10-1056,0,0.0151201,"ed on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical representations and showed performance gains in knowledge-based word sense disambiguation (WSD), the expansion technique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008),"
W14-5117,E03-1009,0,0.0503129,"uding NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to build a tagging model and hence highly suitable for the resource poor languages. There have been various approaches to unsupervised PoS induction. One such approach, reported in (Brown et al., 1992) is based on the class based n-gram models. In (Clark, 2003) distributional and morphological information is used for PoS induction. We use the unsupervised PoS tagging system of (Biemann, 2009) because of its availability as an open source software. We use web-based corpus of 34 million tokens for Bengali (Ekbal and Bandyopadhyay, 2008), and the datasets reported in (Biemann et al., 2007b) for Hindi and German. These datasets were used for unsupervised lexical acquisition. A Distributional Thesaurus (DT) is an automatically computed resource that relates words according to their similarity. A DT contains, for every sufficiently frequent word, the most"
W14-5117,P98-2127,0,0.369085,"sion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical representations and showed performance gains in knowledge-based word sense disambiguation (WSD), the expansion technique can also be used in other text processing applications including NERC: especially for rare words and unseen instances, lexical expansion can provide a useful back-off technique as it performs a generalization of the training and test data. 2 Technical Background Unlike supervised techniques, unsupervised PoS tagging (Christodoulopoulos et al., 2010) techniques require no pre-existing manually tagged corpus to buil"
W14-5117,C12-1109,1,0.853279,"problems including NERC (Ekbal and Saha, 2011a; Sofianopoulos and Tambouratzis, 2010). 1.2 Unsupervised Lexical Acquisition One of the major problems in applying machine learning algorithms for solving information extraction problems is the availability of large annotated corpora. We explore possibilities arisD S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 107–112, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) ing from the use of unsupervised part-of-speech (PoS) induction (Biemann, 2009) and lexical expansion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller"
W14-5117,D13-1089,1,0.903679,"fianopoulos and Tambouratzis, 2010). 1.2 Unsupervised Lexical Acquisition One of the major problems in applying machine learning algorithms for solving information extraction problems is the availability of large annotated corpora. We explore possibilities arisD S Sharma, R Sangal and J D Pawar. Proc. of the 11th Intl. Conference on Natural Language Processing, pages 107–112, c Goa, India. December 2014. 2014 NLP Association of India (NLPAI) ing from the use of unsupervised part-of-speech (PoS) induction (Biemann, 2009) and lexical expansion (Miller et al., 2012) with distributional thesauri (Riedl and Biemann, 2013). Unsupervised PoS induction is a technique that induces lexical-syntactic categories through the statistical analysis of large, raw text corpora. As shown in (Biemann et al., 2007a), using these induced categories as features results in improved accuracies for a variety of NLP tasks, including NERC. Lexical expansion (Miller et al., 2012) is also an unsupervised technique that needs a large corpus for the induction, and is based on the computation of a distributional thesaurus (DT), see (Riedl and Biemann, 2013; Lin, 1998). While (Miller et al., 2012) used a DT for expanding lexical represent"
W14-5130,P02-1053,0,0.00825665,"di et al., 2013). 3 Technical Background In this section we familiarize the reader with the two pivotal approaches used for determing the trustworthiness of customer reviews on e– Commerce websites. We take the help of detailed literature available in the area of Factor Analysis (Escofier and Pag`es, 1994), (Abdi et al., 2013), Logistic Regression (Sharma, 1995) and Support Vector Machine (Cortes and Vapnik, 1995). 198 3.1 Sentiment Analysis Sentiment conveys humans emotions or opinions in a given piece of text. Sentiment Analysis as pointed out by Pang et al. (Pang and Lee, 2007) and Turney (Turney, 2002) is an attempt to identify the subjectivity or sentiment polarity of given piece of text. This is done by leveraging Natural Language Processing (NLP) techniques and trying to model a computation algorithm to identify the same automatically for a given piece of input text (Pang and Lee, 2007; Turney, 2002). Pang et al. (Pang and Lee, 2007) points out that the term “Sentiment Analysis” is more popular amongst the NLP community. “Opinion Mining” also conveys aggregating the subjectivity associated with the item (product) features being discussed in the text (Pang and Lee, 2007). For example, in"
W15-4308,I08-5008,1,0.831507,"Missing"
W15-4308,P11-1037,0,0.353567,"Missing"
W15-4308,D09-1026,0,0.530629,"Missing"
W15-4308,D11-1141,0,0.63386,"Missing"
W15-4308,C12-1151,1,0.507451,"Missing"
W15-4316,P12-1109,0,0.0644602,"evels. The goal of normalization is twofold, i.e. a) identification of candidates for normalization and b) converting the candidate wordforms to the normalized form. Unlike the general well-formatted corpus, like newswire, it does not always contain noisy text. Its main sources are normally those platforms on which users have complete freedom to express themselves. Therefore, user generated tweets are one of the major sources of noisy texts. In the last couples of years researchers across worldwide are actively working for the normalization of noisy contents of twitter (Han and Baldwin, 2011; Liu et al., 2012; Wang and Ng, 2013; Porta and Sancho, 2013; Chrupala, 2014). In (Han and Baldwin, 2011), a linear Support Vector Machine (SVM) classifier was trained for detecting ill-formed words, and then performed normalization based on morphophonemic similarity. Application of edit operations and recurrent neural embedding can be found in (Chrupala, 2014) for text normalization. Their method learns sequence of edit operations using conditional random field (CRF). In another work, (Liu et al., 2012) investigated the human perspectives of enhanced letter transformation, visual priming and the phonetic simi"
W15-4316,N13-1050,0,0.115982,"normalization is twofold, i.e. a) identification of candidates for normalization and b) converting the candidate wordforms to the normalized form. Unlike the general well-formatted corpus, like newswire, it does not always contain noisy text. Its main sources are normally those platforms on which users have complete freedom to express themselves. Therefore, user generated tweets are one of the major sources of noisy texts. In the last couples of years researchers across worldwide are actively working for the normalization of noisy contents of twitter (Han and Baldwin, 2011; Liu et al., 2012; Wang and Ng, 2013; Porta and Sancho, 2013; Chrupala, 2014). In (Han and Baldwin, 2011), a linear Support Vector Machine (SVM) classifier was trained for detecting ill-formed words, and then performed normalization based on morphophonemic similarity. Application of edit operations and recurrent neural embedding can be found in (Chrupala, 2014) for text normalization. Their method learns sequence of edit operations using conditional random field (CRF). In another work, (Liu et al., 2012) investigated the human perspectives of enhanced letter transformation, visual priming and the phonetic similarity for the text"
W15-4316,P14-2111,0,0.0521132,"cation of candidates for normalization and b) converting the candidate wordforms to the normalized form. Unlike the general well-formatted corpus, like newswire, it does not always contain noisy text. Its main sources are normally those platforms on which users have complete freedom to express themselves. Therefore, user generated tweets are one of the major sources of noisy texts. In the last couples of years researchers across worldwide are actively working for the normalization of noisy contents of twitter (Han and Baldwin, 2011; Liu et al., 2012; Wang and Ng, 2013; Porta and Sancho, 2013; Chrupala, 2014). In (Han and Baldwin, 2011), a linear Support Vector Machine (SVM) classifier was trained for detecting ill-formed words, and then performed normalization based on morphophonemic similarity. Application of edit operations and recurrent neural embedding can be found in (Chrupala, 2014) for text normalization. Their method learns sequence of edit operations using conditional random field (CRF). In another work, (Liu et al., 2012) investigated the human perspectives of enhanced letter transformation, visual priming and the phonetic similarity for the text normalization. The use of beam search de"
W15-4316,P11-1038,0,0.0752089,"achieve good accuracy levels. The goal of normalization is twofold, i.e. a) identification of candidates for normalization and b) converting the candidate wordforms to the normalized form. Unlike the general well-formatted corpus, like newswire, it does not always contain noisy text. Its main sources are normally those platforms on which users have complete freedom to express themselves. Therefore, user generated tweets are one of the major sources of noisy texts. In the last couples of years researchers across worldwide are actively working for the normalization of noisy contents of twitter (Han and Baldwin, 2011; Liu et al., 2012; Wang and Ng, 2013; Porta and Sancho, 2013; Chrupala, 2014). In (Han and Baldwin, 2011), a linear Support Vector Machine (SVM) classifier was trained for detecting ill-formed words, and then performed normalization based on morphophonemic similarity. Application of edit operations and recurrent neural embedding can be found in (Chrupala, 2014) for text normalization. Their method learns sequence of edit operations using conditional random field (CRF). In another work, (Liu et al., 2012) investigated the human perspectives of enhanced letter transformation, visual priming and"
W15-4316,P11-1037,0,0.098535,"Missing"
W15-5942,J84-3009,0,0.627107,"Missing"
W15-5942,W02-1011,0,0.0163214,"Missing"
W15-5942,C10-2028,0,0.098462,"Missing"
W15-5942,S14-2035,0,0.0534921,"Missing"
W15-5942,P11-2008,0,0.113972,"Missing"
W15-5942,J11-2001,0,0.0694507,"Missing"
W15-5942,R13-1054,0,0.0694485,"Missing"
W15-5942,S14-2111,0,0.0344045,"Missing"
W15-5942,S13-2053,0,0.0523014,"Missing"
W15-5942,S14-2033,0,0.0382886,"Missing"
W15-5942,S14-2077,0,0.0263551,"Missing"
W15-5942,W11-0705,0,\N,Missing
W15-5942,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W15-5942,S13-2078,0,\N,Missing
W16-4205,U13-1008,1,0.885225,"123], [5523] [1080], [8545] [5523], [3321], [6434] [8545], [3321], [6434], [6755] titioning. However, in order to determine a proper partitioning, optimizing a single cluster validity index is not always sufficient, especially in the situation when we deal with text documents having clusters of different shapes and sizes. The concept of multi-objective optimization (MOO) can be brought into consideration where we need to optimize several objective functions at the same time. The advantage of MOO is that we can generate clusters by optimizing several cluster validity indices. Inspired by this, Ekbal et al. (2013) proposed a MOO-based approach for clustering medical documents for EBM by using the search capability of a simulated annealing based approach, AMOSA (Archived MultiObjective Simulated Annealing based technique) (Bandyopadhyay et al., 2008). However, it has been shown that for some benchmark datasets, AMOSA performs slowly compared to a popular genetic algorithm based MOO technique, NSGA-II (Non-dominated Sorting Genetic Algorithm-II) (Bandyopadhyay et al., 2008). Therefore, an alternative MOO-based approach is needed in order to verify whether we can improve the run-time complexity of AMOSA."
W16-4205,U11-1012,1,0.896402,"Missing"
W16-4206,J81-4005,0,0.760879,"Missing"
W16-4206,D14-1181,0,0.00265422,"the rules. This incurs cost and time as the appropriate set of features or rules can be framed only after analyzing the full records. The advent of deep learning algorithms has facilitated to introduce a new framework where we do not require handcrafted features or rules. These models have the abilities to learn automatically the relevant features by performing composition over the words represented in the form of vectors known as word embedding. In recent times, deep neural network architecture has shown promise for solving various NLP tasks such as text classification (Socher et al., 2013; Kim, 2014), language modeling (Mikolov et al., 2010), question answering (Weston et al., 2015), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) etc. In this paper, we propose a novel system (DI-RNN) based on deep learning for patient data de-identification (PDI). We formulate the task as a sequence labeling problem and develop a technique based on Recurrent Neural Network (RNN) (Mikolov et al., 2010). RNN, unlike other techniques, does not require features to be explicitly generated for classifier’s training or testing. Instead it learns features by itsel"
W16-4206,H05-1026,0,0.0901288,". The underlying idea is that similar words appear in close vicinity of each other. The vector corresponding to each input word wi is produced whose dimensionality is set at the time of learning the neural language model from the given unsupervised corpus. This representation provides the continuousspace representation for each word. Usually, training of the word embedding is done in an unsupervised manner using external natural language text like Wikipedia, news article, bio-medical literature etc. The architecture can be varied by adopting various architectures like shallow neural networks (Schwenk and Gauvain, 2005), RNN (Mikolov et al., 2010; Mikolov et al., 2011), SENNA (Collobert et al., 2011), 34 word2vec (Mikolov et al., 2013) etc. We use three different procedures to learn word embeddings like random number initialization, RNN’s word embedding and continuous bag-of-words (CBOW). For random word embedding we initialize the vector of dimension 100 in the range −0.25 to +0.25. In order to evaluate the impact of RNN we use the word embedding as provided by RNNLM 4 of dimension 80 which is trained on Broadcast news corpus. In addition to this we also use word embedding model trained by CBOW technique as"
W16-4206,D13-1170,0,0.0313007,"minent feature set or the rules. This incurs cost and time as the appropriate set of features or rules can be framed only after analyzing the full records. The advent of deep learning algorithms has facilitated to introduce a new framework where we do not require handcrafted features or rules. These models have the abilities to learn automatically the relevant features by performing composition over the words represented in the form of vectors known as word embedding. In recent times, deep neural network architecture has shown promise for solving various NLP tasks such as text classification (Socher et al., 2013; Kim, 2014), language modeling (Mikolov et al., 2010), question answering (Weston et al., 2015), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) etc. In this paper, we propose a novel system (DI-RNN) based on deep learning for patient data de-identification (PDI). We formulate the task as a sequence labeling problem and develop a technique based on Recurrent Neural Network (RNN) (Mikolov et al., 2010). RNN, unlike other techniques, does not require features to be explicitly generated for classifier’s training or testing. Instead it learns featu"
W16-4206,W00-1308,0,0.0526202,"ication task. Here R,P and F denotes Recall, Precision and F-score respectively. 2. Bag-of-word feature: This feature includes uni-grams, bi-grams, tri-grams of the target token. We use window size of [-2, 2] with respect to the target token. Here, n-gram is referred as the continuous sequence of n items. An n-gram generated having sizes of 1, 2, 3 are known as an uni-gram, bi-gram and tri-gram, respectively. 3. Part-of-Speech (PoS) Information: The PoS information of current word, previous two words and the next two words are used as features. We obtain PoS of words from the Stanford tagger (Toutanova and Manning, 2000). 4. Chunk Information: The chunk information is an important feature to identify the PHI termboundary. We use chunk information obtained from openNLP5 . 5. Combined POS-token and Chunk-token Feature: This feature is generated by the combination of other token features like PoS, Chunk within the context window of [-1, 1]. This is represented as [w0 p−1 , w0 p0 , w0 p1 ] where w0 represents the target word, and p−1 , p0 and p1 represent the previous, current and the next POS or Chunk tags, respectively. We build our model by incorporating the above features. We use the CRF implementation6 of CR"
W16-4622,P05-1033,0,0.66211,"language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2005; Chiang, 2007) manage to perform efficiently as long as sufficient parallel text are available. Our submitted system is based on hierarchical SMT, performance of which is improved by performing reordering in the source side and augmenting English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two systems for English to Hindi (En-Hi) translation, viz"
W16-4622,P05-1066,0,0.0672619,"ght of a rule tells the decoder how probable the rule is. Decoder implements a parsing algorithm which is inspired by monolingual syntactic chart parsing along with a beam search to find the best target sentence. 2.3 Reordering One of the major difficulties of machine translation lies in handling the structural differences between the language pair. Translation from one language to another becomes more challenging when the language pair follows different word order. For example, English language follows subject-verb-object (SVO) whereas Hindi follows subject-object-verb (SOV) order. Research (Collins et al., 2005; Ramanathan et al., 2008) has shown that syntactic reordering of the sourceside to conform the syntax of the target-side alleviates the structural divergence and improves the translation quality significantly. Though the PBSMT has an independent reordering model which reorders the phrases but it has limited potential to model the word-order differences between different languages (Collins et al., 2005). We perform syntactic reordering of the source sentences in the preprocessing phase in which every English sentence is modified in such a way that its word order is almost similar to the word o"
W16-4622,W11-2123,0,0.0213498,"365 22164816 49,394 57,037 10,656 10174 844,925,569 Table 1: Statistics of data set 2.6 Preprocessing We begin with a preprocessing of raw data, which includes tokenization, true-casting, removing long sentences as well as sentences with a length mismatch exceeding certain ratio. Training and development sets were already tokenized. For tokenizing English sentences we use tokenizer.perl5 script and for Hindi sentences we use indic_NLP_Library6 . 2.7 Training For all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierar"
W16-4622,N03-1017,0,0.394895,"or English-Hindi language pair. This year English-Hindi language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT (Koehn et al., 2003) and hierarchical phrase-based SMT (Chiang, 2005; Chiang, 2007) manage to perform efficiently as long as sufficient parallel text are available. Our submitted system is based on hierarchical SMT, performance of which is improved by performing reordering in the source side and augmenting English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two syste"
W16-4622,P07-2045,0,0.0927874,"English-Hindi bilingual dictionary. The rest of the paper is organized as follows. Section 2 describes the various methods that we use. Section 3 presents the details of datasets, experimental setup, results and analysis. Finally, Section 4 concludes the paper. 2 Method For WAT-2016, we have submitted two systems for English to Hindi (En-Hi) translation, viz. one without adding any external data to the training corpus and the other by augmenting bilingual dictionary in training. Both systems are reordered in the source side. As a baseline model we develop a phrase-based SMT model using Moses (Koehn et al., 2007). We perform several experiments with the hierarchical SMT in order to study the effectiveness of reordering and bilingual dictionary augmentation. These were done to improve syntactic order and alignment with linguistic knowledge. 2.1 Phrase-based Machine Translation Phrase-based statistical machine translation (PBSMT) (Koehn et al., 2003) is the most popular approach among all other approaches to machine translation and it has became benchmark for machine translation systems in academia as well as in industry. A phrase-based SMT consists 216 Proceedings of the 3rd Workshop on Asian Translati"
W16-4622,W16-4601,0,0.0230641,"ical phrase-based SMT for English to Hindi language pair. We perform reordering and augment bilingual dictionary to improve the performance. As a baseline we use a phrase-based SMT model. The MT models are fine-tuned on the development set, and the best configurations are used to report the evaluation on the test set. Experiments show the BLEU of 13.71 on the benchmark test data. This is better compared to the official baseline BLEU score of 10.79. 1 Introduction In this paper, we describe the system that we develop as part of our participation in the Workshop on Asian Translation (WAT) 2016 (Nakazawa et al., 2016) for English-Hindi language pair. This year English-Hindi language pair is adopted for translation task for the first time in WAT. Apart from that, the said language pair was introduced in WMT 14 (Bojar et al., 2014). Our system is based on Statistical Machine Translation (SMT) approach. The shared task organizers provide English-Hindi parallel corpus for training and tuning and monolingual corpus for building language model. Literature shows that there exists many SMT based appraoches for differnt language pairs and domains. Linguistic-knowledge independent techniques such as phrase-based SMT"
W16-4622,J03-1002,0,0.00927902,"or all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the trained system was tuned with Minimum Error Rate Training (MERT) (Och, 2003) to learn the weights of different parameters of the model. 3 Results and Analysis We bui"
W16-4622,P03-1021,0,0.0455331,"learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the trained system was tuned with Minimum Error Rate Training (MERT) (Och, 2003) to learn the weights of different parameters of the model. 3 Results and Analysis We build the following systems using Moses7 . 1. Phrase-based model (Phr) 2. Phrase-based model after reordering the source side (PhrRe) 3. Hierarchical phrase-based model (Hie) 4. Hierarchical phrase-based model after reordering the source side. We build two variations of this model: one (HieRe) without adding any external resources to the train set and another (HieReDict) with adding bilingual dictionary to the train set. 5 https://github.com/moses-smt/mosesdecoder/blob/RELEASE-3.0/scripts/tokenizer/tokenizer."
W16-4622,P02-1040,0,0.0974368,"velopment sets were already tokenized. For tokenizing English sentences we use tokenizer.perl5 script and for Hindi sentences we use indic_NLP_Library6 . 2.7 Training For all the systems we train, we build n-gram (n=4) language model with modified KneserNey smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). We build two separate language models, one using the monolingual Hindi corpus and another merging the Hindi training set with the monolingual corpus. In our experiment, as we find language model built using only monolingual Hindi corpus produces better results in terms of BLEU (Papineni et al., 2002) score therefore, we decide to use the former language model. For learning the word alignments from the parallel training corpus, we used GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. We build several MT systems using Moses based on two models, namely phrase-based SMT and hierarchical phrase-based SMT. For building phrase-based systems, we use msdbidirectional-fe as reordering model, set distortion limit to 6. For other parameters of Moses, default values were used. For building hierarchical phrase-based systems we use default values of the parameters of Moses. Finally, the t"
W16-4622,W13-2807,0,0.0298254,"y English sentence is modified in such a way that its word order is almost similar to the word order of the Hindi sentence. For example, English: The president of America visited India in June. Reordered: America of the president June in India visited. Hindi: अमे रका के रा पित ने जून म भारत क याऽा क । (amerikA ke rAShTrapati ne jUna meM bhArata kI yAtrA kI .) For source-side reordering we use the rule-based preordering tool1 , which takes parsed English sentence as input and generates sentence whose word order is similar to that of Hindi. This reordering is based on the approach developed by (Patel et al., 2013) which is an extension of an earlier work reported in (Ramanathan et al., 2008). For parsing source side English sentences, we use Stanford parser2 . 2.4 Augmenting Bilingual Dictionary Bilingual dictionaries are always useful in SMT as it improves the word-alignment which is the heart of every SMT. In addition to reordering the source corpus, we add a English-Hindi bilingual dictionary to improve our MT system. We show our proposed model in Figure 1. We use Moses (Koehn et al., 2007), an open source toolkit for training different systems. We start training with Phrase-based SMT as a baseline"
W16-4622,I08-1067,1,0.777796,"e decoder how probable the rule is. Decoder implements a parsing algorithm which is inspired by monolingual syntactic chart parsing along with a beam search to find the best target sentence. 2.3 Reordering One of the major difficulties of machine translation lies in handling the structural differences between the language pair. Translation from one language to another becomes more challenging when the language pair follows different word order. For example, English language follows subject-verb-object (SVO) whereas Hindi follows subject-object-verb (SOV) order. Research (Collins et al., 2005; Ramanathan et al., 2008) has shown that syntactic reordering of the sourceside to conform the syntax of the target-side alleviates the structural divergence and improves the translation quality significantly. Though the PBSMT has an independent reordering model which reorders the phrases but it has limited potential to model the word-order differences between different languages (Collins et al., 2005). We perform syntactic reordering of the source sentences in the preprocessing phase in which every English sentence is modified in such a way that its word order is almost similar to the word order of the Hindi sentence"
W16-4622,vilar-etal-2006-error,0,0.0944589,"Missing"
W16-6303,2010.amta-papers.6,0,0.0388479,"d combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for"
W16-6303,W05-0808,0,0.0333302,"an be developed using the strengths of both SMT and RBMT. In this paper, we develop a hybrid model to exploit the benefits of disambiguation, linguistic rules, and structural issues. Knowledge of coupling is very useful to build hybrid model of machine translation. There are different types of coupling, viz. serial coupling and Parallel coupling. In serial coupling, SMT and RBMT are processed one after another in sequence. In parallel coupling, models are processed in parallel to build a hybrid model. In Indian languages, few hybrid models have been proposed as in(Dwivedi and Sukhadeve, 2010; Aswani and Gaizauskas, 2005). The rest of the paper is structured as follows. We present a brief review of the existing works in Section 2. Motivations and various characteristic features have been discussed in Section 3. We describe our proposed method in Section 4. Experiential setup and results are discussed in Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in transl"
W16-6303,J90-2002,0,0.76531,"have been proposed as in(Dwivedi and Sukhadeve, 2010; Aswani and Gaizauskas, 2005). The rest of the paper is structured as follows. We present a brief review of the existing works in Section 2. Motivations and various characteristic features have been discussed in Section 3. We describe our proposed method in Section 4. Experiential setup and results are discussed in Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for"
W16-6303,D07-1007,0,0.0500122,"and endi−1 are the starting positions of the translation of ith phrase and end position of the (i − 1)th phrase of e in f. In the above equation, it is well defined that most probable phrases present in training corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system gr"
W16-6303,P16-2007,0,0.0234208,"bability P (f |e) is modeled as, P (f1−I |e−I 1 )= I ∏ i=1 ϕ(f¯i |e¯i )d(starti −endi−1 −1) ϕ is phrase translation probability and d(.) is distortion probability. starti −endi–1 −1, which is the argument of d(.) is a function of i, whereas starti and endi−1 are the starting positions of the translation of ith phrase and end position of the (i − 1)th phrase of e in f. In the above equation, it is well defined that most probable phrases present in training corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target"
W16-6303,2008.tc-1.2,0,0.632026,"ss in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for Combining SMT and RBMT In this work we propose a hybrid architecture for translating English documents into Hindi. Both of these languages are very popular. English is an internation"
W16-6303,N03-1017,0,0.186143,"main. The data is often mixed, comprising of very short sentences (even the phrases) and the long sentences. To the best of our knowledge, for such a domain, there is no work involving Indian languages.Below we describe SMT and RBMT very briefly. 1.1 Statistical Machine Translation (SMT) Statistical machine translation (SMT) systems are considered to be good at capturing knowledge of the domain from a large amount of parallel data. This has robustness in resolving ambiguities and other related issues. SMT provides good translation output based on statistics and maximum likelihood expectation (Koehn et al., 2003a): ebest = argmaxe P (e|f ) = argmaxe [P (f |e)PLM (e)] where f and e are the source and target languages, respectively. PLM (e) and P (f |e) are the language and translation model, respectively. The best output translation is denoted D S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural Language Processing, pages 10–19, c Varanasi, India. December 2016. 2016 NLP Association of India (NLPAI) by ebest . Language model corresponds to the n-gram probability. The translation probability P (f |e) is modeled as, P (f1−I |e−I 1 )= I ∏ i=1 ϕ(f¯i |e¯i )d(starti −endi−1 −1)"
W16-6303,W10-3707,0,0.023742,"tput. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pre-processing, morp"
W16-6303,P02-1040,0,0.105342,"Missing"
W16-6303,W09-3528,0,0.0243521,"as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pr"
W16-6303,I08-1067,1,0.908514,"ng corpora will be chosen as the translated output. This could be useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysi"
W16-6303,P09-1090,1,0.83307,"e useful in handling ambiguity at the translation level. The work reported in (Dakwale and Monz, 2016) focuses on improving the performance of a SMT system. Along with the translation model authors allow the reestimation of reordering models to improve accuracy of translated sentences. The authors in their work reported in (Carpuat and Wu, 2007) show how word sense disambiguation helps to improve the performance of a SMT system. Literature shows that there are few systems available for English-Indian language machine translation (Ramanathan et al., 2008; Rama and Gali, 2009; Pal et al., 2010; Ramanathan et al., 2009). 1.2 Rule-based Machine Translation (RBMT) Rule-based system generates target sentence with the help of linguistic knowledge. Hence, there is a high chance that translated sentence is grammatically well-formed.There are several steps required to build linguistic rules for translation. Robustness of a rule-based system greatly depends on the quality of rules devised. A set of sound rules ensures to build a good accurate system. Generally, the steps can be divided into three sub parts: 1. Analysis 2. Transfer 3. Generation Analysis step consists of pre-processing, morphological analysis, chunki"
W16-6303,W07-0728,0,0.0325171,"-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In this paper, we combine both SMT and RBMT in order to exploit advantages of both the translation strategies. 3 Necessity for Combining SMT and RBMT In this work we propose a hybrid architecture for translating English doc"
W16-6303,2003.mtsummit-systems.15,0,0.10308,"ally, generation step consists of genderization, vibhakti computation, TAM computation, agreement computing, word generator and sentence generator. The agreement computing can be accomplished with three sub steps: intra-chunk, inter-chunk and default agreement computing. In (Dave et al., 2001) authors have proposed an inter-lingua based English–Hindi machine translation system. In (Poornima et al., 2011), authors have described how to simplify English to Hindi translation using a rule-based approach. AnglaHindi is one of the very popular English-Hindi rule-based translation tools proposed in (Sinha and Jain, 2003). Multilingual machine aided translation for English to Indian languages has been developed in (Sinha et al., 1995). Apertium is an open source rule-based machine translation tool proposed in (Forcada et al., 2011). Rule-based approach for machine translation has been proposed with respect to Indian language (Dwivedi and Sukhadeve, 2010). 1.3 Hybrid Machine Translation A hybrid model of machine translation can be developed using the strengths of both SMT and RBMT. In this paper, we develop a hybrid model to exploit the benefits of disambiguation, linguistic rules, and structural issues. Knowle"
W16-6303,2009.mtsummit-posters.21,0,0.0275789,"Section 5. Finally, we conclude in Section 6. 2 Related work In rule-based MT, various linguistic rules are defined and combined in (Arnold, 1994). Statistical machine translation models have resulted from the word-based models (Brown et al., 1990). This has become so popular because of its robustness in translation only with the parallel corpora. As both of this approaches have their own advantages and disadvantages, there is a trend nowadays to build a hybrid model by combining both SMT and RBMT (Costa-Jussa and Fonollosa, 2015). Various architectures of hybrid model have been compared in (Thurmair, 2009). Among the various existing architectures, serial coupling and parallel coupling are the most popular (Ahsan et al., 2010).Rule-based approach along with post-processed SMT outputs are described in (Simard et al., 2007). A review for hybrid MT is available in (Xuan et al., 2012). In (Eisele et al., 2008), authors proposed an architecture to build a hybrid machine translation engine by following a parallel coupling method. They merged phrase tables of general training data of SMT and the output of RBMT. However, they did not consider the source and target language ordering characteristics. In"
W16-6308,N06-1020,0,0.0628442,"am Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates the word inhibits. Figure 3: Depen"
W16-6308,W09-1401,0,0.417369,"Missing"
W16-6308,W11-1806,0,0.0210969,"5. Bi-gram and Tri-gram Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates"
W16-6308,W11-1801,0,0.353092,"Missing"
W16-6308,W13-2015,0,0.0321345,"Missing"
W16-6308,W11-1807,0,0.360884,"Missing"
W16-6308,W11-1808,0,0.263057,"5. Bi-gram and Tri-gram Features: We use the character bi-gram and tri-gram sequences extracted from a token as features. For example, for the token “upregulation”, the bigram features will be up pr re eg gu ul la at ti io on and tri-gram features will be upr pre reg egu gul ula lat ati tio ion . 6. Dependency Path Features: There are are some trigger words which ca not be detected using context features or b-gram or tri-gram features. So we depend on dependency relations inside sentence. Dependency features are extracted from dependency graph generated by dependency parser(David McClosky and Manning, 2011; David McClosky and Johnson, 2006) . Figure 3 shows the dependency graph for the sentence “BMP-6 inhibits growth of mature human B cells; in59 duction of Smad phosphorylation and upregulation of Id1”, generated by the CharniakMcCloskey parser (David McClosky and Johnson, 2006). In the graph, an edge label represents the dependency relation between two nodes. Each node in the graph is labelled by a number which represents a word appearing in that position (0-based index) of the sentence. For example, node labelled with number 0 indicates the word BMP-6 and node labelled with number 1 indicates"
W16-6308,W09-1313,0,0.0700811,"Missing"
W16-6311,P98-1041,0,0.081381,"pta et al., 2012). For example, the word khusboo (”fragrance”) can be written in Roman script using different variations such as kushboo, khusbu, khushbu and so on. This type of problem is termed as a non-trivial term matching problem for search engines with the aim to match the native-script or Roman-transliterated query with the documents in multiple scripts after considering the spelling variations. Many single (native) script queries and documents with spelling variations have been studied (French et al., 1997; Zobel and Dart, 1996) as well as transliteration of named entities (NE) in IR (Collier et al., 1998; Wang et al., 2009). It is important for every IR engine to present users with information that are most relevant to the users’ needs. While searching, user has an idea of what s/he wants, but in many instances, due to the variations in query formulations, retrieved results could greatly vary. As a result, understanding the nature of information need behind the queries issued by Web users has become an imD S Sharma, R Sangal and A K Singh. Proc. of the 13th Intl. Conference on Natural Language Processing, pages 81–89, c Varanasi, India. December 2016. 2016 NLP Association of India (NLPAI) por"
W16-6311,gupta-etal-2012-mining,0,0.03166,"as well as Roman scripts, and these should also be matched to the documents written in both the scripts. Transliteration (Lopez, 2008) is the process of phonetically describing the words of a given language using a non-native script. For both the web documents and intended search queries to retrieve those documents, transliteration, especially into Roman script, is generally used. Since no standard ways of spelling any word into a non-native script exist, transliterated contents offer extensive spelling variations; typically, we can transliterate a native term into Roman script in many ways (Gupta et al., 2012). For example, the word khusboo (”fragrance”) can be written in Roman script using different variations such as kushboo, khusbu, khushbu and so on. This type of problem is termed as a non-trivial term matching problem for search engines with the aim to match the native-script or Roman-transliterated query with the documents in multiple scripts after considering the spelling variations. Many single (native) script queries and documents with spelling variations have been studied (French et al., 1997; Zobel and Dart, 1996) as well as transliteration of named entities (NE) in IR (Collier et al., 1"
W16-6325,D14-1181,0,0.00470024,"Missing"
W16-6325,H05-1026,0,0.0222709,"and syntactic variations of words (Mikolov et al., 2013). The vector initially can be generated randomly or can be pre-trained from the large unlabeled corpus in an unsupervised fashion using external resources such as Wikipedia, news article, bio-medical literature etc. Word embedding is learned through sampling word cooccurrence distribution. These techniques are useful to identify similar words which appear in close vicinity in vector space. There are several ways of generating the word-vectors using different architectures such as word2vec (Mikolov et al., 2013), shallow neural networks (Schwenk and Gauvain, 2005), RNN (Mikolov et al., 2010; Mikolov et al., 2011) etc. We learn our word embedding through three different ways such as random number initialization, RNN’s word embedding and continuous bag-of-words (CBOW) based models. In case Sentence Named Entity De-identified Sentence Discussed O Discussed this O this case O case with O with Dr. O Dr. John Doe B-DOCTOR I-DOCTOR XYZ DOCTOR for O for Mr. O Mr. Ness B-PATIENT XYZ PATIENT Table 1: Sample sentence (sequence of words with the corresponding labels using BIO notation) and its corresponding de-identified sentence of random number initialization, w"
W16-6325,W16-4206,1,0.679779,"al., 2013) as well as named entity recognition (Collobert et al., 2011; Lample et al., 2016). Motivated by the success of deep learning techniques, in this paper, we have adopted in particular Recurrent Neural Network (RNN) (Mikolov et al., 2010) architecture to capture PHI terms. RNN has shown advantages over other machine learning 189 and rule based techniques. RNN unlike other techniques does not require features explicitly developed for the classifier learning. The virtue of system learning by itself makes the system adaptable and scalable. This work is an extension of our previous work (Shweta et al., 2016) where we identified only 7 PHI category (Patient, Doctor, Hospital, Location, Date, Age, ID) irrespective of subcategories using only i2b2-2014 training dataset. The current work provide comprehensive experimentation on i2b2-2014 challenge dataset to deidentify 7 categories and 25 subcategories. We have formulated this task as the sequence labeling problem and developed the baseline model using a supervised machine learning technique. Conditional random field (CRF) (Lafferty et al., 2001) along with a set of handcrafted features are used to build the base classifier. In the current study, we"
W16-6325,N16-1030,0,0.0290698,"rned by other hyperparameters which are initialized randomly or can be pre-trained on large unlabeled corpus. Pretraining is much beneficial in improving performance as it effectively captures the linguistic variations and patterns. Recently, there has been significant success of deep learning techniques in solving various natural language processing tasks such as text classification (Kim, 2014), language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2014), spoken language understanding (Mesnil et al., 2013) as well as named entity recognition (Collobert et al., 2011; Lample et al., 2016). Motivated by the success of deep learning techniques, in this paper, we have adopted in particular Recurrent Neural Network (RNN) (Mikolov et al., 2010) architecture to capture PHI terms. RNN has shown advantages over other machine learning 189 and rule based techniques. RNN unlike other techniques does not require features explicitly developed for the classifier learning. The virtue of system learning by itself makes the system adaptable and scalable. This work is an extension of our previous work (Shweta et al., 2016) where we identified only 7 PHI category (Patient, Doctor, Hospital, Loca"
W16-6325,W00-1308,0,0.0482235,"Missing"
W16-6331,W13-3520,0,0.0297224,"news data. A comment is finally represented by a vector composed of the word vectors of the individual tokens. The vector is generated as follows: P ti ∈Comment(T ) Reps(ti ) Reps(comment) = number Of Lookups (5) Here, Reps(t) is the token representation obtained by Google news word embedding and number of lookups is equal to the number of tokens from the comments present in the word embedding model. Representation of Hindi comments: For Hindi we build our own word embedding model. For training we use the data sets obtained from the Hindi Wikipedia and some other sources (Joshi. et al., 2010; Al-Rfou et al., 2013) including all the comments that we crawled. We use skip-gram representation (Mikolov et al., 2013) for the training of word2vec tool. Further, we use Eq-5 to obtain the representations of Hindi comments. We set dimension to 200 and window size as 5. After we represent the comments in terms of vectors, we develop two baselines to compare with our proposed approach. 7 253 https://code.google.com/archive/p/word2vec/ 6.2 Baseline-1 The hypothesis behind this baseline being the fact that, if two comments have the same sentiments (positive or negative) then the similarity between them would be high"
W16-6331,dey-fung-2014-hindi,0,0.0283545,"ics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong Kong (Li, 2000) indicated that linguistic constructions predominantly trigger code mixing. The work reported in (Hidayat, 2012) showed that Facebook users tend to mainly use inter-sentential switching over intra-sentential. A code-mixed speech corpus of English-Hindi on student interviews is presented in (Dey and Fung, 2014). It shows analysis and motivations of code mixing, and discusses in what grammatical contexts code mixing occurs (Dey and Fung, 2014) . To the best of our knowledge we do not see the use of any deep learning that addresses the problem of sentiment analysis in a code-mixed environment. In our current work we discuss the scope for text analysis based on deep learning architecture on government data / citizen views which can very well frame a new concept of better e-governance. 3 Resource Creation We design a web-crawler to crawl user comments from mygov.in portal. We consider the comments writt"
W16-6331,C14-1008,0,0.0128698,"1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixed is very common that demands special attention. Significant characteristics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong Kong (Li, 2000) indicated tha"
W16-6331,W15-3904,0,0.0636274,"Missing"
W16-6331,P06-2025,1,0.421909,"on the classes assigned at the token-level we classify the sentence based on the majority voting. The sentence is classified to belong to that particular class which appears most in the sentence. Mathematically, it can be defined as follows: S = {x|x ∈ lang(t), ∀t ∈ T } Lang(comments) = argmax(f (s)) s∈S (1) abundant of data sources from Hindi Wikipedia. We back-transliterate the roman script into Devanagari script. A transliteration system takes as input a character string in the source language and generates a character string in the target language as output. The transliteration algorithm (Ekbal et al., 2006) that we used here can be conceptualized as two-levels of decoding: (a) segmenting source and target language strings into transliteration units (TUs); and (b). defining appropriate mapping between the source and target TUs by resolving different combinations of alignments and unit mappings. The TU is defined based on a regular expression. For a given token belonging to ‘non-native’ script X6 written in English Y as the observed channel output, we have to find out the most likely English transliteration Y that maximizes P (Y |X). The Indic word(Hindi) is divided into TUs that have the pattern"
W16-6331,P14-1062,0,0.0604684,"ent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social medi"
W16-6331,D14-1181,0,0.0152011,"Missing"
W16-6331,D15-1280,0,0.0181688,"alysis are discussed in Section-9. Finally, we conclude in Section-10. 2 Related Works Nowadays deep learning models are being used to solve various natural language processing (NLP) problems. Usually, the input to any deep learning based model is the word representation. Some of the commonly used word representation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic"
W16-6331,P11-1015,0,0.0531074,"3) for the training of word2vec tool. Further, we use Eq-5 to obtain the representations of Hindi comments. We set dimension to 200 and window size as 5. After we represent the comments in terms of vectors, we develop two baselines to compare with our proposed approach. 7 253 https://code.google.com/archive/p/word2vec/ 6.2 Baseline-1 The hypothesis behind this baseline being the fact that, if two comments have the same sentiments (positive or negative) then the similarity between them would be higher than any other comments having different sentiments. We use the IMDB movie reviews data sets (Maas et al., 2011) containing 2K positive and 2K negative reviews for English and Hindi movie reviews, and tourism data (Joshi. et al., 2010) to compare the opinions represented in our Hindi comments. This algorithm takes as input a comment and the source documents (i.e. datasets that we collected). Each comment and all the documents belonging to a particular set (positive set: containing all the positive comments and negative set: containing all the negative comments) are represented as vectors following the representation techniques that we discussed in Subsection-6.1. We compute the cosine similarity of a gi"
W16-6331,D14-1162,0,0.0767908,"ails of our proposed convolutional network model to identify opinion from comments. The experimental setup along with the details of external data are described in Section-8. The obtained results, key observations and error analysis are discussed in Section-9. Finally, we conclude in Section-10. 2 Related Works Nowadays deep learning models are being used to solve various natural language processing (NLP) problems. Usually, the input to any deep learning based model is the word representation. Some of the commonly used word representation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convoluti"
W16-6331,P13-1045,0,0.0118334,"presentation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learnin"
W16-6331,D13-1170,0,0.00292371,"presentation techniques are word2vec (Mikolov et al., 2013), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learnin"
W16-6331,S14-2033,0,0.0163851,"rk(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixed is very common that demands special attention. Significant characteristics of code mixing have been pointed out in some of the works such as (Milroy and Muysken, 1995; Alex, 2007; Auer, 2013). In a multi-lingual country like India, code-mixing poses a big challenge to handle the contents in social media. Chinese-English code mixing in Macao (San, 2009) and Hong"
W16-6331,P10-1040,0,0.0171598,"3), Glove (Pennington et al., 2014), Neural language model (Mikolov et al., 2010), etc. Distributed representation of a word is one of the popularly used models (Hinton, 1984; Rumelhart et al., 1988). Similarly recurrent neural network (Liu et al., 2015) has been used for modeling sentence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment cla"
W16-6331,P14-2105,0,0.0313506,"ence and documents. The numeric vectors, used to represent words are called word embedding. Word embedding has shown promising results in variety of the NLP applications, such as named entity recognition (NER) (dos Santos et al., 2015), sentiment analysis (Socher et al., 2013b) and parsing (Socher et al., 2013a; Turian et al., 2010). The convolutional neural network(CNN) (LeCun et al., 1998) was originally proposed for computer vision. The success of CNN has been seen in few of the NLP applications such as sentence modeling (Kalchbrenner et al., 2014), semantic parsing for question answering (Yih et al., 2014), query retrieval in web search (Shen et al., 2014), sentence classification (Kim, 2014; Socher et al., 2013b) etc. Collobert (Collobert et al., 2011) has also claimed the effectiveness of CNN in traditional NLP task such as PoS tagging, NER etc. Deep learning based architectures have shown success for sentiment classification of tweets, such as (Tang et al., 2014; dos Santos and Gatti, 2014). The domain adaption for large scale sentiment classification has been handled through deep learning model (Glorot et al., 2011). In social media contents, code-mixing where more than one language is mixe"
W17-5229,W15-4319,0,0.158286,"Missing"
W17-5229,C16-1047,1,0.842663,"ion. Upon completion of training, the output of the top most hidden layer is used as sentence embedding. The trained sentence embeddings represent the relevant semantic and syntactic features of the tweets. Next, optimized feature set, as obtained by PSO, is concatenated with sentence embeddings for training a SVR model. The idea of cascading SVR with LSTM was motivated by the • Lexicon based Features: For each tweet we extract the following lexicon based features: – Polar word count: Count of positive and negative words using the MPQA subjectivity lexicon (Wiebe and Mihal214 recent works of (Akhtar et al., 2016; Wang et al., 2016). utilizing GloVe common crawl embeddings. The resultant network produces average Pearson score of merely 0.1877. We observe that a good percentage of tokens (mostly noisy) were missing in the embeddings - thus poses challenge to the network during the learning phase. Subsequently, we try to minimize the effect of noisy tokens by utilizing GloVe Twitter embeddings. Though, the network obtains improved average Pearson score at 0.1921, improvement is not significant. On analysis we find similar issues with Twitter embeddings. To address the problem of data sparsity we employ"
W17-5229,E17-1109,1,0.80842,"ormance. However, finding the relevant set of features is cumbersome and time-consuming task. Motivated by this we employ a Particle Swarm Optimization (PSO) based feature selection technique for selecting a subset of features from a feature pool. By utilizing the reduced and pruned feature set for training and evaluation, resultant system often performs considerably well. At the same time complexity of the system also reduces as it requires fewer parameters to learn. Literature survey shows successful application of PSO for various tasks and/or domains (Lin et al., 2008; Akhtar et al., 2017; Yadav et al., 2017). 212 Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 212–218 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics 2 System Description • Frequent noisy term: We compile a dictionary of frequently used slang terms and abbreviations along with its normal form that are commonly in practice in the Twitter domain. Every token in a tweet is searched in this dictionary. If a match is found then it is replaced with the normal form. The list was compiled utilizing the datasets of WNUT2015 sha"
W17-5229,S13-2053,0,0.0333875,"tion intensity. All these features are fed to the PSO to generate the optimized feature set. – • VADER Sentiment: VADER (Gilbert, 2014) stands for Valence Aware Dictionary and Sentiment Reasoner. It is a rule-based sentiment analysis technique designed to work with contents on social media. For every input tweet, it provides positive, negative, neutral and compound sentiment score. We use these four values as features. 2.4 cea, 2006) and Bing Liu lexicon (Ding et al., 2008). Aggregate polarity scores: Positive and negative scores are obtained from each of the following lexicons: Sentiment140 (Mohammad et al., 2013), AFINN (Nielsen, 2011) and Sentiwordnet (Baccianella et al., 2010). It is calculated by aggregating the positive and negative word scores provided by each lexicon. Aggregate polarity scores (Hashtags): Aggregate of positive and negative scores of the hashtags in a tweet is calculated from NRC Hashtag Sentiment lexicon (Mohammad et al., 2013). Emotion word count: Count of the number of words matching each emotion from NRC Word-Emotion Association Lexicon (Mohammad and Turney, 2013). Aggregate emotion score: Sum of emotion associations of the words present in NRC-10 Expanded lexicon (Bravo-Marq"
W17-5229,W17-5205,0,0.0590887,"Missing"
W17-5229,D14-1162,0,0.0855275,"o dense layers. Hidden layer of the LSTMs consists of 100 neurons whereas the dense layers contain 100 and 50 neurons, respectively. • Elongation: User tends to express their state of emotion by elongating a valid word e.g. ‘jooooy’, ‘goooodd’ etc. In this step, all such elongated words are identified and converted into valid words by removing the consecutive characters. For example ‘jooyyyy’ and ‘jooooy’ are converted to ‘joy’. 2.2.1 Word Embeddings Word embedding (or word vector) is a distributed representation of words that contains syntactic and semantic information (Mikolov et al., 2013; Pennington et al., 2014). For this task, we use GloVe (Pennington et al., 2014) pre-trained word embedding trained on common crawl corpus. Each token in the tweet is represented by 300 dimension word vector. The choice of common crawl word embeddings for Twitter datasets is because of the normalization steps (Section 2.1). We observe that the application of normalization has a positive effect on the overall performance of the system. • Verb present participle: In Twitter domain, it is observed that user tends to omit the character ‘i’ or ‘g’ in words ending with ‘ing’. For example, ‘going’ is written as ‘goin’ or ‘go"
W17-5229,D16-1059,0,0.029088,"of training, the output of the top most hidden layer is used as sentence embedding. The trained sentence embeddings represent the relevant semantic and syntactic features of the tweets. Next, optimized feature set, as obtained by PSO, is concatenated with sentence embeddings for training a SVR model. The idea of cascading SVR with LSTM was motivated by the • Lexicon based Features: For each tweet we extract the following lexicon based features: – Polar word count: Count of positive and negative words using the MPQA subjectivity lexicon (Wiebe and Mihal214 recent works of (Akhtar et al., 2016; Wang et al., 2016). utilizing GloVe common crawl embeddings. The resultant network produces average Pearson score of merely 0.1877. We observe that a good percentage of tokens (mostly noisy) were missing in the embeddings - thus poses challenge to the network during the learning phase. Subsequently, we try to minimize the effect of noisy tokens by utilizing GloVe Twitter embeddings. Though, the network obtains improved average Pearson score at 0.1921, improvement is not significant. On analysis we find similar issues with Twitter embeddings. To address the problem of data sparsity we employ a series of heuristi"
W17-7517,W17-1322,0,0.027019,"1 indicates exactly opposite, 1 indicates exactly same, and 0 indicates the independence. It is to be assumed that higher the similarity score obtained more is the chance that the pair of text snippets become textually entailed, so it could be a good predictor of TE. 2. Jaccard Similarity: Jaccard similarity (Jaccard, 1901) is a set based similarity metric. It is defined as follows: Jaccard(A, B) = |A ∩ B|/|A ∪ B| 1 http://www.cs.waikato.ac.nz/ml/weka/ (2) 134 where A and B represent two sets of documents. A similar pair is expected to share more words and hence the entailment relation holds (Almarwani and Diab, 2017). Following this intuition we make use of set based similarity metric in our work. This is very well established similarity metric and measure the similarity between the two finite sets. 3. Dice Similarity: Dice Similarity (Dice., 1945) is also a vector based similarity metric. It’s value lies within the range of 0 to 1. It can be calculated using the following formula. Dice(A, B) = 2|A ∩ B|/(|A |+ |B|) (3) Here, A and B represent the first and second set of documents, respectively. The mathematical derivation of this measure is same as the derivation of F-measure, where precision and recall b"
W17-7517,N10-3003,0,0.0774551,"Missing"
W17-7517,H05-1090,0,0.0865571,"Missing"
W17-7529,D14-1181,0,0.00437347,"Missing"
W18-4408,I08-5008,1,0.743146,"classification problems. A convolution layer of n × m kernel size is used (where m-size of word embedding) to look at n-grams of word at a time and then a Max-pooling layer selects the largest from the convoluted inputs. In our system the convolutional layer is constructed with filter size 64 and ReLU as activation function. We capture the bi-gram features from this layer by taking kernel size as 2 × EmbeddingSize. In Max-pooling layer we have used pool size of 1 × 2. • Ensemble: Classifier ensemble3 aims at combining the predictions of different classifiers. Ensembles (Florian et al., 2003; Ekbal and Bandyopadhyay, 2008) are often seen to be much more accurate than the individual classifiers that make them up. In the system being discussed output of the max-pooling layer of CNN classifier goes to a flatten layer and then to a dense layer with ReLU activation. On the other hand output of SVM is passed through a dense layer with ReLU activation. Output tensor form both CNN and SVM are then concatenated and passed through a dense layer and a Dropout layer with hyperparameter of 0.5. This is finally inputted to the output layer with Softmax as an activation function. We use Adam as an optimizer and Categorical Cr"
W18-4408,W03-0425,0,0.148871,"cessfully used in text classification problems. A convolution layer of n × m kernel size is used (where m-size of word embedding) to look at n-grams of word at a time and then a Max-pooling layer selects the largest from the convoluted inputs. In our system the convolutional layer is constructed with filter size 64 and ReLU as activation function. We capture the bi-gram features from this layer by taking kernel size as 2 × EmbeddingSize. In Max-pooling layer we have used pool size of 1 × 2. • Ensemble: Classifier ensemble3 aims at combining the predictions of different classifiers. Ensembles (Florian et al., 2003; Ekbal and Bandyopadhyay, 2008) are often seen to be much more accurate than the individual classifiers that make them up. In the system being discussed output of the max-pooling layer of CNN classifier goes to a flatten layer and then to a dense layer with ReLU activation. On the other hand output of SVM is passed through a dense layer with ReLU activation. Output tensor form both CNN and SVM are then concatenated and passed through a dense layer and a Dropout layer with hyperparameter of 0.5. This is finally inputted to the output layer with Softmax as an activation function. We use Adam as"
W18-4408,gao-huang-2017-detecting,0,0.0640674,"Missing"
W18-4408,D14-1162,0,0.0818714,"rnalism sold media Which one is the best example of #Bhagwa terrorism. Which one is the best example of bhagwa terrorism goons and #presstitutes goons and presstitutes दोन शह द के #शव के साथ बबरता दोन शह द के शव के साथ बबरता Table 2: Preprocessing example Word Embedding In order to fit textual data into Neural Network we need vectorization of texts. This provides useful evidence in capturing semantic property of a word. For Hindi we use the pre-trained model of Wikipedia (Bojanowski et al., 2016). Each word is represented as a vector of 300 dimension. For English, we use the pre-trained Glove(Pennington et al., 2014) vectors. After preprocessing the data, the English dataset has an average sentence length of 20 words with maximum sentence length of 50 words, while the Hindi dataset has an average sentence length of 21 words with maximum sentence length of 50 words 1 . We consider maximum length of a sentence to be 50-words. We use padding with zeros if the sentence length is less than 50, and prune from the last if length of the sentence is greater than 50. 4.3 Features To train any statistical machine learning model we need a set of features to be extracted from the dataset. The features that we use to t"
W19-0413,L16-1429,1,0.942898,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,C16-1047,1,0.934723,"re survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-o"
W19-0413,S16-1049,0,0.04135,"Missing"
W19-0413,baccianella-etal-2010-sentiwordnet,0,0.0144947,"* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et"
W19-0413,C12-2008,1,0.824775,"n detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA"
W19-0413,W13-5006,1,0.768299,"positive sentiment of the sentence. “It was used to be a horrible place to eat but not any more.” In contrast to A4, architecture A3 does not rely on the sequence information of the extracted features and allows the network to learn on its own. We use 300 dimension Word2Vec (Mikolov et al., 2013) word embeddings for the experiments. Each Bi-LSTM layer contains 100 neurons while two dense layers contain 100 and 50 neurons, respectively. Features As additional features, we extract the following information for each token in an instance. – Aspect term extraction: Distributional thesaurus (DT)1 (Biemann and Riedl, 2013) defines the lexicon expansion of a token based on a similar context. It is usually very effective for the handling of unseen text. If a token in the test set never appears in the training set, it becomes a non-trivial task for the classifier to make a correct prediction. By employing DT feature, the classifier can additionally utilize lexical expansion of the current token for mapping with the training set, thus minimize the chance of unseen text. For each token, we use its top 3 DT expansions as features. Language English Spanish French Dutch German Hindi #sent. 2,000 2,070 1,733 1,711 19,43"
W19-0413,S16-1044,0,0.270428,"m identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) network (Graves et al"
W19-0413,S16-1054,0,0.0389318,"Missing"
W19-0413,P14-2063,0,0.0690449,"Missing"
W19-0413,D18-1382,1,0.797108,"Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achi"
W19-0413,S15-2080,0,0.0298934,"ral language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & D"
W19-0413,W06-0301,0,0.0146414,"evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Condition"
W19-0413,S14-2076,0,0.167189,"2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags"
W19-0413,S16-1174,1,0.942977,"n a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and refer to the same span of text. Motivation and Contribution A survey of the literature for ABSA suggests a number of works for different languages (Kumar et al., 2016; Brun et al., 2016; C ¸ etin et al., 2016). Although the reported performance for these works are good, they usually suffer in handling the language diversity, i.e., the systems that reported state-of-theart performance for one language typically do not work well for the other languages. The unavailability of such a generic system motivates us to build a language-agnostic model for aspect based sentiment analysis. We propose a generic deep neural network architecture that handles the language divergence to a great extent. Our model is based on Bidirectional Long Short-Term Memory (Bi-LSTM) ne"
W19-0413,S13-2053,0,0.0243541,"ing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for SVM and CRF. They achieved comparable performance for Spanish, French & Dutch. System XRCE (Brun"
W19-0413,P05-1015,0,0.48177,"Missing"
W19-0413,W02-1011,0,0.029084,"or aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez"
W19-0413,S16-1002,0,0.0636458,"Missing"
W19-0413,S14-2004,0,0.525133,"timent towards food and service are positive and negative, respectively. Such analysis offers finegrained information to a user or an organization who seeks users opinion towards any specific entity. For example, based on the users’ feedback, an individual can draw a general perception about the specific attribute or aspect of a product or service, and he/she can make an informed decision about the product or service under observation. Similarly, an organization can utilize the feedback to refine its product/service or to take a decision in the business model. Aspect-based sentiment analysis (Pontiki et al., 2014, 2016) has two subproblems at its core, i.e., aspect term identification (or opinion target extraction) and aspect sentiment classification. Given a text, aspect term identification task aims to find the boundaries of all the aspect terms present in the text, whereas aspect sentiment classification task classifies each of these identified aspect terms into one of the predefined sentiment classes (e.g., positive, negative, neutral etc.). A sentence may contain any number of aspect terms or no aspect term at all. The terms ‘aspect term‘ and ‘opinion target‘ are often used interchangeably and re"
W19-0413,S16-1045,0,0.0591637,"Missing"
W19-0413,P02-1053,0,0.0202289,"es (one each for aspect extraction and aspect classification) for the training and evaluation; and c) we provide the new state-of-the-art performance for two problems of ABSA across six different languages. Rest of the paper is organized as follows: In Section 2, we present the literature survey. The proposed methodology has been discussed in detail in Section 3. In Section 4, we furnished experimental results and provided the necessary analysis. Finally, we conclude in Section 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA,"
W19-0413,waltinger-2010-germanpolarityclues,0,0.0321484,"9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspect based sentiment analysis (Pontiki et al., 2016) (Task 5), which contain user reviews across multiple languages. The datasets of English, Spanish, French and Dutch are related to the reviews of consumer electronics a"
W19-0413,H05-1044,0,0.101206,"65.6 65.7 64.2 23.1 22.0 22.4 24.0* 50.0 49.3 50.4 53.5* Aspect Classification (Acc) A1 A2 A3 A4 82.4 82.7 82.1 83.4 86.4 86.3 86.1 87.1* 75.0 75.3* 75.2 74.3 80.9 80.7 81.9* 81.4 86.7 87.2* 86.6 87.2* 64.5 66.3 65.8 66.9* Table 3: Comparison of various models for aspect extraction and aspect classification on test dataset. A1, A2, A3 & A4 refers to four architectures depicted in Figure 1. *Statistically significant (T-test) w.r.t. other architectures (p-values&lt; 0.05). + Significant w.r.t. A4. lexicons of English language (Bing Liu opinion lexicon, Ding et al. 2008; MPQA subjectivity lexicon, Wilson et al. 2005; SentiWordNet, Baccianella et al. 2010; and Vader sentiment, Hutto and Gilbert 2014) through the application of Google Translator. For German, we additionally use GermanPolarityClues lexical resource (Waltinger, 2010). The final list contains 2757, 2164, 3271, 1615, 17627 and 11874 positive words for English, Spanish, Dutch, French, German and Hindi, respectively. Similarly, there are 5112, 1735, 5834, 3038, 19962 and 2225 negative words in the list. 4 Experiments, Results and Analysis 4.1 Datasets We evaluate our proposed approach on the benchmark datasets of SemEval-2016 shared task on aspe"
W19-0413,P18-1208,0,0.0149591,"n 5. 2 Related Works Sentiment analysis is a well-studied problem of natural language processing for English language (Turney, 2002; Pang et al., 2002, 2005; Pang and Lee, 2008; Jagtap and Pawar, 2013; Kim and Hovy, 2006). However, in recent times, researchers have focused on various extensions of sentiment analysis, e.g., aspect based sentiment analysis (Pontiki et al., 2014; Kiritchenko et al., 2014; Akhtar et al., 2016), multi-lingual sentiment analysis (Balamurali et al., 2012; Mishra et al., 2017; Brun et al., 2016; Kumar et al., 2016), multi-modal sentiment analysis (Poria et al., 2017; Zadeh et al., 2018; Ghosal et al., 2018), sentiment analysis in Twitter (Ghosh et al., 2015; Mohammad et al., 2013) etc. For ABSA, System GTI (Alvarez-L´opez et al., 2016) used a Support Vector Machine (SVM) and Conditional Random Field (CRF) based approach for aspect extraction and sentiment classification, respectively. They used language-dependent features like lemmas and Part-of-Speech (PoS) tags to achieve the state-of-the-art score for aspect extraction in Spanish. IIT-TUDA (Kumar et al., 2016) also used a number of hand-crafted features like character n-grams, dependency relations, prefix and suffix for"
W19-5056,P03-1003,0,0.156562,"Missing"
W19-5056,P06-1114,0,0.146209,"Missing"
W19-5056,D18-1187,0,0.0480616,"Missing"
W19-5056,N19-1423,0,\N,Missing
W19-5056,W19-5039,0,\N,Missing
W19-5346,W16-2342,0,0.0118923,"ata is tokenized using moses tokenizer, and truecased. For tokenizing Gujarati data, we use indic nlp library8 . After tokeninzation and truecasing, we subword (Sennrich et al., 2015) all original data. We apply 10,000 BPE merge operations over English and Gujarati data independently. For back-translation of monolingual data, two PBSMT models English→Gujarati and Gujarati→English are trained over original available parallel subworded corpora. 4-gram lan8 5 Results The official automatic evaluation uses the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), CharactTER (Wang et al., 2016). The official scores are shown in the Table 2. Phrase-base SMT (PBSMT) obtains BLEU scores of 5.2 and 7.3 for English→Gujarati and Gujarati→Englsih, respectively. Whereas, baseline NMT (Transformer) obtains lower BLEU scores of 4.0 and 5.5 for the same directions. Though, SMT systems outperforms baseline NMT systems trained using small amount of original parallel data only. We observe from the Table 2 that Transformer with synthetic (Transformer + https://github.com/anoopkunchukuttan/indic nlp library 409 82.7 80.3 82.4 76.3 43.3 −0.119 −0.129 −0.132 −0.400 −1.769 Ave. 64.8 61.7 59.4 60.8 59."
W19-5346,W17-3204,0,0.0428257,"rtment of Computer Science and Engineering Indian Institute of Technology Patna {sukanta.pcs15,kamal.pcs17,asif,pb}@iitp.ac.in Abstract NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (Koehn and Knowles, 2017). However, back-translation (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard ph"
W19-5346,N03-1017,0,0.417658,"ion (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NMT systems are based on Transformer (Vaswani et al., 2017) architecture. Experiments show that NMT systems achieve BLEU (Papineni et al., 2002) scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data. Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Guj"
W19-5346,P03-1021,0,0.0514261,"different SMT and NMT based systems. Synth: Synthetic data Apart from these parallel data, we use monolingual English (news crawl) and Gujarati (Wikipedia dumps) sentences for synthetic parallel data creation. After training two models i.e. English→Gujarati and Gujarati→English using the parallel data mentioned in Table 1, English and Gujarati monolingual sentences are back translated respectively. 4.2 guage model is trained using KenLM (Heafield, 2011). For word alignment, we use GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. Model is tuned with Minimum Error Rate Training (Och, 2003). After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English→Gujarati and Gujarati→English PBSMT model, respectively. We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati→English and English→Gujarati translation directions. Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction. We use the following hyperparameters values of Sockeye toolkit: 6 layers in both encoder and decoder, word embedding size of 512, hidden size of 5"
W19-5346,J03-1002,0,0.0435229,"haractTER 0.987 1.005 0.919 0.782 0.884 0.763 0.883 0.905 0.828 0.817 0.859 0.774 Table 2: BLEU scores of different SMT and NMT based systems. Synth: Synthetic data Apart from these parallel data, we use monolingual English (news crawl) and Gujarati (Wikipedia dumps) sentences for synthetic parallel data creation. After training two models i.e. English→Gujarati and Gujarati→English using the parallel data mentioned in Table 1, English and Gujarati monolingual sentences are back translated respectively. 4.2 guage model is trained using KenLM (Heafield, 2011). For word alignment, we use GIZA++ (Och and Ney, 2003) with grow-diag-final-and heuristics. Model is tuned with Minimum Error Rate Training (Och, 2003). After these two models are trained, monolingual subworded data from both English and Gujarati are back-translated using English→Gujarati and Gujarati→English PBSMT model, respectively. We merge the back translated data with original parallel data to have larger parallel corpora for Gujarati→English and English→Gujarati translation directions. Finally, with the augmented parallel corpora, we train one Transformer based NMT model for each direction. We use the following hyperparameters values of So"
W19-5346,P02-1040,0,0.105073,"th English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NMT systems are based on Transformer (Vaswani et al., 2017) architecture. Experiments show that NMT systems achieve BLEU (Papineni et al., 2002) scores of 10.4 and 8.1 for Gujarati→English and English→Gujarati, respectively, outperforming the baseline SMT systems even in the absence of enough-sized parallel data. Rest of the paper is arranged in following manner: Section 2 gives brief introduction of the Transformer architecture that we used for NMT training, Section 3 describes the task, Section 4 describes the submitted systems, Section 5 gives various evaluation scores for English-Gujarati translation pair, and finally, Section 6 concludes the work. We describe our submission to WMT 2019 News translation shared task for GujaratiEng"
W19-5346,P16-1009,0,0.0331945,"titute of Technology Patna {sukanta.pcs15,kamal.pcs17,asif,pb}@iitp.ac.in Abstract NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) has recently become dominant paradigm for machine translation (MT) achieving state-ofthe-art on standard benchmark data sets for many language pairs. As opposed to SMT, NMT systems are trained in an end-to-end manner. Training an effective NMT requires a huge amount of high-quality parallel corpus and in absence of that, an NMT system tends to perform poorly (Koehn and Knowles, 2017). However, back-translation (Sennrich et al., 2016) has been shown to improve NMT systems in such a situation. In this work, we train a SMT system and an NMT system for both English→Gujarati and Gujarati→English using the original training data. SMT systems are also used to generate synthetic parallel corpora through back-translation of monolingual data from English news crawl and Gujarati Wikipedia dumps. These corpora along with the original training corpora are used to improve the baseline NMT systems. All the SMT and NMT systems are trained at subword level. Our SMT systems are standard phrase-based SMT systems (Koehn et al., 2003), and NM"
W19-5346,2006.amta-papers.25,0,0.0274195,"using it for experiment. English data is tokenized using moses tokenizer, and truecased. For tokenizing Gujarati data, we use indic nlp library8 . After tokeninzation and truecasing, we subword (Sennrich et al., 2015) all original data. We apply 10,000 BPE merge operations over English and Gujarati data independently. For back-translation of monolingual data, two PBSMT models English→Gujarati and Gujarati→English are trained over original available parallel subworded corpora. 4-gram lan8 5 Results The official automatic evaluation uses the following metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), CharactTER (Wang et al., 2016). The official scores are shown in the Table 2. Phrase-base SMT (PBSMT) obtains BLEU scores of 5.2 and 7.3 for English→Gujarati and Gujarati→Englsih, respectively. Whereas, baseline NMT (Transformer) obtains lower BLEU scores of 4.0 and 5.5 for the same directions. Though, SMT systems outperforms baseline NMT systems trained using small amount of original parallel data only. We observe from the Table 2 that Transformer with synthetic (Transformer + https://github.com/anoopkunchukuttan/indic nlp library 409 82.7 80.3 82.4 76.3 43.3 −0.119 −0.129 −0.132 −0.400 −1."
W19-5440,P07-2045,0,0.0209917,"Missing"
W19-5440,D18-1549,0,0.0129773,"extracted from each of those two parallel corpora. The quality of the scoring method is judged based on the quality of the neural machine translation (NMT) and statistical machine translation (SMT) systems trained on these smaller corpora. We participated in both language pair: Nepali-English and Sinhala-English. Building machine translation (MT) systems, specifically NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) systems, require supervision of huge amount of high-quality parallel training data. Though recently emerged unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it takes time, money and expert translators. Though we can have parallel data compiled from online sources, it is not reliable as it is often very noisy and poor in quality. It has been found that MT systems are sens"
W19-5440,N03-1017,0,0.0635676,"Missing"
W19-5440,W14-4012,0,0.0715786,"Missing"
W19-5440,D19-1632,0,0.032813,"Missing"
W19-5440,L18-1275,0,0.0522698,"Missing"
W19-5440,W11-2123,0,0.0385153,"Missing"
W19-5440,P02-1040,0,0.109937,"Missing"
W19-5440,D13-1176,0,0.0153023,"are asked to submit score for each sentence in each of these two parallel corpora (Nepali-English and SinhalaEnglish). Based on the scores, two smaller sets of parallel sentences that amount to 1 million and 5 millions are extracted from each of those two parallel corpora. The quality of the scoring method is judged based on the quality of the neural machine translation (NMT) and statistical machine translation (SMT) systems trained on these smaller corpora. We participated in both language pair: Nepali-English and Sinhala-English. Building machine translation (MT) systems, specifically NMT (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) systems, require supervision of huge amount of high-quality parallel training data. Though recently emerged unsupervised NMT (Artetxe et al., 2018; Lample et al., 2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it"
W19-5440,W18-6319,0,0.121023,"ool (Koehn et al., 2007). For training the SMT system we keep the following settings: growdiag-final-and heuristics for word alignment, msdbidirectional-fe for reordering model, and 5gram language model with modified Kneser-Ney smoothing (Kneser and Ney, 1995) using KenLM (Heafield, 2011). The BLEU6 (Papineni et al., 2002) scores for these SMT systems are 3.7 and 4.6 for Nepali-English and Sinhala-English, respectively. 5 Results 4 https://bitbucket.org/anoopk/indic_ nlp_library 5 https://github.com/moses-smt/ mosesdecoder/blob/RELEASE-3.0/scripts/ tokenizer/tokenizer.perl 6 We use sacreBLEU (Post, 2018). 291 Scoring Scheme Arithmetic Mean Geometric Mean Arithmetic Mean Geometric Mean 1 million SMT NMT test devtest test devtest Nepali-English 3.84 3.64 5.48 5.94 3.89 3.57 5.28 5.57 Sinhala-English 3.07 3.63 3.16 3.70 3.03 3.52 3.01 3.36 5 million SMT NMT test devtest test devtest 4.34 4.27 4.03 4.01 1.29 1.32 1.25 1.25 4.44 4.42 5.12 5.17 3.87 4.28 4.54 5.08 Table 3: Official BLEU scores for 1-million and 5-million sub-sampled sets. Scoring Scheme 1 million Nepali-English Arithmetic Mean 56,868 Geometric Mean 53,821 Sinhala-English Arithmetic Mean 70,114 Geometric Mean 67,888 we observe that"
W19-5440,W18-2709,0,0.066798,"2018) has shown promising results on related language pairs, it does not work for distant language pairs like Nepali-English and Sinhala-English (Guzm´an et al., 2019). Also, a vast majority of languages in the world fall in the category of low-resource languages as they have too little, if any, parallel data. However, getting parallel training data is not easy as it takes time, money and expert translators. Though we can have parallel data compiled from online sources, it is not reliable as it is often very noisy and poor in quality. It has been found that MT systems are sensitive to noise (Khayrallah and Koehn, 2018). This necessitates to filter out noisy sentences from a large pool of parallel parallel sentences. Parallel corpus filtering task of WMT 2019 focuses on two new low-resource languages pairs: In this paper, we describe the IIT Patna’s submission to WMT 2019 shared task on parallel corpus filtering. This shared task asks the participants to develop methods for scoring each parallel sentence from a given noisy parallel corpus. Quality of the scoring method is judged based on the quality of SMT and NMT systems trained on smaller set of high-quality parallel sentences sub-sampled from the original"
W19-5440,W19-5404,0,0.0400032,"Missing"
Y08-1016,W02-2007,0,0.064774,"Missing"
Y08-1016,I08-2077,1,0.673545,"iterary history, technological developments are of recent origin.  Web sources for name lists are available in English, but such lists are not available in Bengali forcing the use of transliteration. A pattern directed shallow parsing approach for NER in Bengali is reported in Ekbal and Bandyopadhyay (2007a). A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are kept for handling the unknown words. More recently, the related works in this area can be found in Ekbal et al. (2008a), Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Cucerzan and Yarowsky (1999) with a language independent method. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL)2. 2. Named Entity Recognition in Bengali Bengali is the seventh popular language in the world, second in India and the national language of Bangladesh. We have u"
Y08-1016,I08-5008,1,0.912233,"ological developments are of recent origin.  Web sources for name lists are available in English, but such lists are not available in Bengali forcing the use of transliteration. A pattern directed shallow parsing approach for NER in Bengali is reported in Ekbal and Bandyopadhyay (2007a). A HMM based NER system for Bengali has been reported in Ekbal et al. (2007b), where additional contextual information has been considered during emission probabilities and NE suffixes are kept for handling the unknown words. More recently, the related works in this area can be found in Ekbal et al. (2008a), Ekbal and Bandyopadhyay (2008b) with the CRF, and SVM approach, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF and Cucerzan and Yarowsky (1999) with a language independent method. As part of the IJCNLP-08 NER shared task, various works of NER in Indian languages using various approaches can be found in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL)2. 2. Named Entity Recognition in Bengali Bengali is the seventh popular language in the world, second in India and the national language of Bangladesh. We have used a Bengali news corpus (Ekbal"
Y08-1016,W03-0425,0,0.0387153,"The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), ME (New York University’s MENE in (Borthwick, 1999)), CRFs (Lafferty et al., 2001) and SVM (Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in (Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting. Named Entity (NE) identification in Indian languages"
Y08-1016,W03-0431,0,0.0609327,"Missing"
Y08-1016,W03-0433,0,0.0325944,"ummarization etc. The current trend in NER is to use the machine-learning (ML) approach, which is more attractive in that it is trainable and adoptable and the maintenance of a ML based system is much cheaper than that of a rule-based one. The representative ML approaches used in NER are Hidden Markov Model (HMM) (BBN’s IdentiFinder in (Bikel, 1999)), ME (New York University’s MENE in (Borthwick, 1999)), CRFs (Lafferty et al., 2001) and SVM (Yamada et al., 2002). The process of stacking and voting method for combining strong classifiers like boosting, SVM and TBL, on NER task can be found in (Wu et al., 2003). Florian et al. (2003) tested different methods for combining the results of four systems and found that robust risk minimization worked best. The work reported in this paper differs from the existing works in the sense that here, we have conducted a number of experiments to improve the performance of the classifiers with the lexical context patterns, which are generated in a semi-automatic way from an unlabeled corpus of 3 million wordforms, and used several post-processing techniques to improve the performance of each classifier before applying weighted voting. Named Entity (NE) identificat"
Y09-1014,A00-1031,0,0.0849849,"Missing"
Y09-1014,A92-1018,0,0.533676,"Missing"
Y09-1014,I08-5008,1,0.912091,"ed with the assumption that the words belonging to the same POS classes contain some common suffix/prefix. This feature works effectively for the highly inflective Indian languages like Bengali. 3. POS Information: POS information of the previous word(s) can play a crucial role in deciding the POS tag of the current word. This is the only dynamic feature in the experiment. 4. Named Entity (NE) Information: The NE information of the current and/or the surrounding word(s) does have an important role in the overall accuracy of the POS tagger. In order to use this feature, a SVM based NER system (Ekbal and Bandyopadhyay, 2008b) has been used. The NE tag(s) of the current and/or the surrounding word(s) have been used as the features in the ME/CRF/SVM based POS tagging models. The NE information has been included into the system in order to reduce the rate of errors that we faced in our earlier experiments for HMM based POS tagging (Ekbal et al., 2007). The confusion matrix of the HMM based POS tagger showed that most of the errors were concerned with NNP (Proper noun) vs. NN (Common noun). 5. Lexicon Feature: A lexicon (Ekbal and Bandyopadhyay, 2008c) in Bengali has been used to improve the performance of the POS t"
Y09-1014,N01-1025,0,0.148917,"Missing"
Y09-1014,W02-2018,0,0.023462,"arly in Bengali, has started to appear very recently as there was neither any standard POS tagset nor any available tagged corpus just one/two years ago. In this work, we have developed POS taggers for Bengali using ME, CRF and SVM frameworks. These POS taggers have been combined together into a final system with the help of weighted voting techniques. We have used the C++ based ME package3 for building the ME based POS tagger. A number of POS tagging models have been built that are differentiated from each other by the features, which are included in the model. The system uses L-BFGS method (Malouf, 2002) to build the ME model, which is guaranteed to converge to a solution in this kind of problem. The sequential classification approach like ME can handle many correlated features but it suffers from the label bias problem. Careful feature selection is very essential in the ME framework. In contrast, CRF (Lafferty et al., 2001) is a sequential modeling framework that has all the advantages of ME and also solves the problem of label bias in a principled way. Moreover, CRFs bring together the best of generative and classification models. We have used the OpenNLP C++ based CRF++ package (http://crf"
Y09-1014,J94-2001,0,0.309132,"Missing"
Y09-1014,N03-1028,0,0.0524283,"kind of problem. The sequential classification approach like ME can handle many correlated features but it suffers from the label bias problem. Careful feature selection is very essential in the ME framework. In contrast, CRF (Lafferty et al., 2001) is a sequential modeling framework that has all the advantages of ME and also solves the problem of label bias in a principled way. Moreover, CRFs bring together the best of generative and classification models. We have used the OpenNLP C++ based CRF++ package (http://crfpp.sourceforge.net). For parameter estimation, the system uses L-BFGS method (Sha and Pereira, 2003) to build the CRF model, which is guaranteed to converge to a solution in this kind of problem. SVM (Vapnik, 1995) achieves high generalization even with training data of a very high dimension. Further, by introducing the Kernel function, SVMs handle non-linear feature spaces, and carry out training considering combinations of more than one feature. We have used the YamCha toolkit (http://chasen-org/~taku/software/yamcha) for training and TinySVM-0.07 (http://cl.aist-nara.ac.jp/~taku-ku/software/TinySVM) classifier for classification. A number of experiments have been carried out with the diff"
Y09-1014,W96-0213,0,\N,Missing
Y09-2045,I08-2077,1,0.534144,"relatively free word order language. Thus NEs can appear in subject and object positions making the NER task more difficult compared to others. 5 Manipuri is a resource-constrained language. Annotated corpus, name dictionaries, sophisticated morphological analyzers, POS taggers etc. are not yet available. In Indian language context, a HMM based NER system for Bengali has been reported in Ekbal et al. (2007), where additional contextual information has been considered for emission probabilities and NE suffixes are used for handling the unknown words. Other works in Bengali NER can be found in Ekbal et al. (2008), and Ekbal and Bandyopadhyay (2008) with the CRF, and SVM approaches, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF. Various works on NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. In this paper, Manipuri NER systems have been developed using an active learning technique as well as SVM. We collected the data from http://www.thesangaiexpress.com/ , a popular Manipuri newspaper. Initially, a baseline system has been developed based on an"
Y09-2045,I08-5008,1,0.60592,"er language. Thus NEs can appear in subject and object positions making the NER task more difficult compared to others. 5 Manipuri is a resource-constrained language. Annotated corpus, name dictionaries, sophisticated morphological analyzers, POS taggers etc. are not yet available. In Indian language context, a HMM based NER system for Bengali has been reported in Ekbal et al. (2007), where additional contextual information has been considered for emission probabilities and NE suffixes are used for handling the unknown words. Other works in Bengali NER can be found in Ekbal et al. (2008), and Ekbal and Bandyopadhyay (2008) with the CRF, and SVM approaches, respectively. Other than Bengali, the works on Hindi can be found in Li and McCallum (2004) with CRF. Various works on NER involving Indian languages are reported in IJCNLP-08 NER Shared Task on South and South East Asian Languages (NERSSEAL) 1 using various techniques. In this paper, Manipuri NER systems have been developed using an active learning technique as well as SVM. We collected the data from http://www.thesangaiexpress.com/ , a popular Manipuri newspaper. Initially, a baseline system has been developed based on an active learning technique that gene"
Y09-2045,D08-1017,0,0.0639195,"Missing"
Y09-2045,M98-1018,0,\N,Missing
Y09-2045,A97-1029,0,\N,Missing
Y10-1015,W09-3539,1,0.799203,"erge only very recently. Named Entity (NE) identification in Indian languages in general and Bengali in particular is more difficult and challenging compared to others due to facts such as: (i). missing of capitalization information, (ii). appearance of NEs in the dictionary with some other specific meanings, (iii). free word order nature of the languages, (iv). resource-constrained environment, i.e. non-availability of corpora, annotated corpora, name dictionaries, good morphological analyzers, part of speech (POS) taggers etc. Some of the recent works related to Bengali NER can be found in (Ekbal and Bandyopadhyay, 2009b; Ekbal and Bandyopadhyay, 2009a; Ekbal and Bandyopadhyay, 2008b). Other works related to Indian language NER are reported in the proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages (NERSSEAL)1. 1 http://ltrc.iiit.ac.in/ner-ssea-08 115 116 Regular Papers The concept of combining classifiers is a very emerging topic in the area of machine learning. The primary goal of classifier ensemble 2 is to improve the performance of the individual classifiers. These classifiers could be based on a variety of classification methodologies, and could achieve different rate"
Y10-1015,W03-0419,0,0.0456897,"Missing"
Y10-1019,W03-2201,0,0.0175481,"Missing"
Y10-1019,W09-3539,1,0.806064,"works related to NER in Indian languages have started to emerge only very recently. Named Entity (NE) identification in Indian languages is more difficult and challenging compared to others due to the lack of capitalization information, appearance of NEs in the dictionary as common nouns, relatively free word order nature of the languages, resource-constrained environment, i.e., non-availability of corpus, annotated corpus, name dictionaries, morphological analyzers, part of speech (POS) taggers etc. Some of the works related to Indian languages can be found in (Ekbal and Bandyopadhyay, 2007; Ekbal and Bandyopadhyay, 2009a; Ekbal and Bandyopadhyay, 2009b) for Bengali, in (Li and McCallum, 2004) for Hindi and in (Shishtla et al., 2008) for Telugu. The performance of any classification technique depends on the features of data sets. Feature selection, also known as variable selection, feature reduction, attribute selection or variable subset selection, is the technique, commonly used in machine learning, of selecting a subset of relevant features for building robust learning models. In a machine learning approach, feature selection is an optimization problem that involves choosing an appropriate feature subset."
Y10-1019,nobata-etal-2002-summarization,0,0.0724756,"Missing"
Y10-1019,I08-1045,0,0.0381548,"Missing"
Y10-1019,I08-5010,0,0.0486624,"Missing"
Y10-1019,I08-5007,0,0.0643303,"Missing"
Y10-1019,W03-0419,0,0.0589383,"Missing"
Y10-1051,S07-1014,0,0.0629916,"Missing"
Y10-1051,P07-2044,0,0.0443036,"Missing"
Y10-1051,N03-1028,0,0.0550466,", as in HMMs, can be obtained efficiently by dynamic programming. To train a CRF, the objective function to be maximized is the penalized log-likelihood of the state sequences given the observation sequence: N K i =1 k =1 L ∧ = ∑ log( P ∧ ( s (i ) |o (i ) )) −∑ λ 2σ 2 k 2 , where, { &lt; o ( i ) , s (i ) &gt; } is the labeled training data. The second sum corresponds to a zero-mean, σ 2 -variance Gaussian prior over parameters, which facilitates optimization by making the likelihood surface strictly convex. Here, we set parameters λ to maximize the penalized loglikelihood using Limited-memory BFGS (Sha and Pereira, 2003), a quasi-Newton method that is significantly more efficient, and results in only minor changes in accuracy due to changes in σ. CRFs generally can use real-valued functions but it is often required to incorporate the binary valued features. A feature function fk ( st − 1, st , o, t ) has a value of 0 for most cases and is only set to 1, when st − 1, st are certain states and the observation has certain properties. We have used the C++ based CRF++ package 1 , a simple, customizable, and open source implementation of CRF for segmenting /labeling sequential data. 3.2 Temporal Features Used for C"
Y18-3012,W14-4012,0,0.137348,"Missing"
Y18-3012,P15-1166,0,0.18267,"rs. Sharing of parameters between low-resource and high-resource language pairs helps low-resource pairs to learn better model compared to model trained separately. However, it has been seen that training multiple languages together sometimes degrades the performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1"
Y18-3012,N16-1101,0,0.110114,"Missing"
Y18-3012,E17-3017,0,0.0284114,"1,500 2,000 5,248 4,150 5,273 7,417 2,114 1,633 2,126 Table 2: Original vocabulary size, number of BPE merge and final vocabulary size after applying BPE for each training data pair. We decided the BPE merge values without any rigorous exploration. 3.3 Experimental Setup We train 2 multilingual models namely XX→EN (Indic languages to English) and EN→XX (Englsih to Indic languages) and 14 bilingual models (7 for Indic languages to English, and 7 for English to Indic languages). All of these models are based on Transformer (Vaswani et al., 2017) network. For training the models, we use Sockeye (Hieber et al., 2017), a toolkit for NMT. Each token in training, development and test sets are split in subword units in preprocessing stage. Along with that an additional token5 indicating which Indic language a sentence pair belong to is added at the beginning of every source6 sentence. Then parallel data of all pairs are appended in one parallel corpus with Indic languages in one side and English on other side, for training a single multilingual model for each of EN→XX and XX→EN directions. These tokens are added with development and test sets too and likewise, development sets are also appended in a single de"
Y18-3012,D13-1176,0,0.0249306,"l translation model by means of sharing parameters with high-resource languages is a common practice to improve the performance of low-resource language pairs. Sharing of parameters between low-resource and high-resource language pairs helps low-resource pairs to learn better model compared to model trained separately. However, it has been seen that training multiple languages together sometimes degrades the performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train s"
Y18-3012,P02-1040,0,0.102539,"l., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 model as a baseline for each translation direction involving English. We evaluate the multilingual models against the bilingual models using BLEU (Papineni et al., 2002) metric. We found that multilingual NMT is better than bilingual models for all XX→EN directions, however for EN→XX directions, multilingual NMT performs better than bilingual NMT for low-resource language pairs only. In the next section, we briefly mention some notable multilingual NMT works. We describe our submitted systems in section 3 which includes description on datasets, preprocessing, experimental setup. Results are described in section 4. Finally, the work is concluded in section 5. 2 Related Works Dong et al. (2015) implemented a system with oneto-many mapping of languages. They tra"
Y18-3012,P16-1162,0,0.410155,"performance of some language pairs compared to a separate single bilingual model as languages may have different linguistic properties. Recent success of end-to-end bilingual NMT systems (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015) quickly gave the rise of multilingual NMT in various ways (Dong et al., 2015; Firat et al., 2016; Johnson et al., 2017). Most of the existing multilingual NMT involve non-Indic languages and are based on attentional encoder-decoder approach. We use the Transformer architecture (Vaswani et al., 2017) with subword (Sennrich et al., 2016) as basic translation unit. We develop two multilingual translation models: one is for XX→EN (7 Indic languages to English) and another is for EN→XX (English to 7 Indic languages). We also train separate bilingual 1003 32nd Pacific Asia Conference on Language, Information and Computation The 5th Workshop on Asian Translation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 - WAT 2018 model as a baseline for each translation direction involving English. We evaluate the multilingual models against the bilingual models using BLEU (Papineni et al., 2002) metric. We found that m"
