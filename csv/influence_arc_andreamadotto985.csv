2020.emnlp-main.273,D19-5602,0,0.0844866,"Missing"
2020.emnlp-main.273,D18-1547,0,0.426427,"des the Lev for updating the previous dialogue state; then, the updated state is used to search the external knowledge base; and finally, a response decoder decodes response by conditioning on the dialogue context and knowledge base match result. MinTL is easy to set up by using different pretrained seq2seq backbones. We conduct extensive experiments on both DST and end-to-end dialogue response generation tasks with two pre-trained seq2seq models, such as T5 (Raffel et al., 2019) and BART (Lewis et al., 2019). The experimental result on a large-scale task-oriented dialogue benchmark MultiWOZ (Budzianowski et al., 2018; Eric et al., 2019) suggests that our proposed method significantly improves SOTA performance in both the full data and simulated low resource setting. Our contributions are summarized as follows: • We propose the MinTL framework that efficiently leverages pre-trained language models for task-oriented dialogue without any ad hoc module. • We propose the novel Lev for efficiently tracking the dialogue state with the minimal length of generation, which greatly reduces the inference latency. • We instantiate our framework with two different pre-trained backbones, and both of them improve the SOT"
2020.emnlp-main.273,P19-1360,0,0.0321159,"o simplify the system design and reduce human supervision, several end-to-end trainable systems have been proposed (Bordes et al., 2016; Wen et al., 2017; Lei et al., 2018; Neelakantan et al., 2019; Eric and Manning, 2017; Eric et al., 2017; Madotto et al., 2018). These methods have been shown to achieve promising results in single-domain tasks. However, the recently proposed multi-domain taskoriented dialogue datasets (Budzianowski et al., 2018; Eric et al., 2019) bring new challenges for multi-domain dialogue state tracking and response generation. Several follow up works (Wu et al., 2019a; Chen et al., 2019; Budzianowski and Vuli´c, 3392 Figure 1: Dialogue state tracking with Lev. The model first generates Lev, then updates the dialogue state with new generated slot-values. The updating operations are insertion (blue), deletion (red), and substitution (green). 2019; Mehri et al., 2019; Madotto et al., 2020b) improved on the initial baselines with various methodologies. Zhang et al. (2019b) proposed the domain aware multi-decoder network and augmented the system act labels by leveraging the user act annotation, achieving the SOTA results in MultiWoz. However, the aforementioned works rely on task"
2020.findings-emnlp.215,P19-1470,0,0.0290641,"action. This enables models to learn a better dialogue policy via interaction (Asri et al., 2016; Li et al., 2017; Wu et al., 2019c; Peng et al., 2018), and it is especially useful in scenarios in where few or no data is available (Liu and Lane, 2017; Liu et al., 2017; Shah et al., 2018; Kreyssig et al., 2018; Li et al., 2020). In our work, instead, we use all the possible user goal queries to generate dialogues directly, instead of creating a reinforcement learning loop to train the model. Language Models as Knowledge Bases has been used for encoding common sense knowledge into transformers (Bosselut et al., 2019; Liu et al., 2019a; Xiong et al., 2019; Wang et al., 2020, 2019). (Guan et al., 2020) improved story generation by training a Language Model with knowledge triples converted into sentences using predefined templates (Levy et al., 2017). Differently, we extract templates from real data, and we aim to store the KB into the models parameters to be able to extract knowledge directly, instead of improving common sense generation. Moreover, several studies tried to extract (Petroni et al., 2019; Kassner and Sch¨utze, 2019; Petroni et al., 2020) or use (Roberts et al., 2020) large pre-trained models"
2020.findings-emnlp.215,D18-1547,0,0.27377,", 2017; Eric and Manning, 2017; Madotto et al., 2018; Reddy et al., 2019; Wu et al., 2019b). These systems re2372 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2372–2394 c November 16 - 20, 2020. 2020 Association for Computational Linguistics quire at least the DST annotation for generating the API calls or to select the gold KB. Moreover, even with the most advanced transformer architecture (Kitaev et al., 2020; Lample et al., 2019; Child et al., 2019), end-to-end models struggle when the input becomes too large (Neelakantan et al., 2019). For example, in MWOZ (Budzianowski et al., 2018), there are 22K entities just for one of the domains. Interested readers can refer to Appendix C for an overview of different task-oriented methodologies. On the other hand, Petroni et al. (2019) discovered a simple yet effective way to query factual knowledge from BERT (Devlin et al., 2019). Later on, Roberts et al. (2020) fine-tuned a pre-trained language model, T5 (Raffel et al., 2019), on just question-answers pairs, without letting the model access any external context or knowledge. These results suggest that the actual knowledge is stored in the model parameters. However, in task-oriente"
2020.findings-emnlp.219,D18-1431,0,0.0889668,"20 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is not properly set (See et al., 2019). On the other hand, (Dathathri et al., 2019) incu"
2020.findings-emnlp.219,D19-1165,0,0.0578978,"Missing"
2020.findings-emnlp.219,N19-1423,0,0.0299653,"se using GPT2 (Radford et al., 2019).PP For diversity, we use the distinct n-grams (Li et al., 2016a) (normalized by the length of theAD text) across all the responses generated by a given method. For evaluating the attribute consistency, we train external classifiers using no-overlapping data with the attribute model. For sentiments, we use AMAZON-5 (McAuley and Leskovec, 2013) product reviews. For topics, we use the test-set data of AG-NEWS (Zhang et al., 2015) because we could not find another topic classification dataset with the same classes. For each dataset, we trained a separate BERT (Devlin et al., 2019) (base) classifier with a simple classification head. Table 2 in Appendix B, summarizes the dataset statistics and the performance of the trained scorer. 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 55.4 85.1 76.0 71.6 61.8 HM DG WD PP model comparison and attribute, which amount to 40 a74.0 total of69.0 420061.5 human annotations. Further details are provided in Appendix C. 20 5 1.0 Human Eval. is the most effective way for evaluating open-domain chat-bots. In this paper, we evaluate two aspects from the generated response: Humanness and Attribute Consistency. The first is used for evaluating the fluency"
2020.findings-emnlp.219,W17-4912,0,0.147474,"repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Fina"
2020.findings-emnlp.219,P18-5002,0,0.0341462,"Missing"
2020.findings-emnlp.219,P17-4008,0,0.498709,", pages 2422–2433 c November 16 - 20, 2020. 2020 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is not properly set (See et al., 2019). On the o"
2020.findings-emnlp.219,P18-1152,0,0.291094,"or Computational Linguistics: EMNLP 2020, pages 2422–2433 c November 16 - 20, 2020. 2020 Association for Computational Linguistics with human annotators in the loop to generate contuining text with positive sentiment. Both of these approaches require learning/fine-tuning all of the models’ parameters, and new desired attributes cannot be easily incorporated into the generation once the models have been trained. Other approaches that do not alter the language model, but modify the decoding procedure for controlled generation include 1) re-weighting the output distribution using discriminators (Holtzman et al., 2018) or bag of words (Ghazvininejad et al., 2017; See et al., 2019; Baheti et al., 2018), and 2) perturbing the models activation with an attribute model (PPLM) (Dathathri et al., 2019). These approaches, instead, are plug-and-play methods in that they can be used on top of any existing pre-trained language model. These methods, do not modify or train the parameters of the original models and they can achieve comparable performance to finetuning methods (Dathathri et al., 2019). Weighted decoding is generally difficult to tune because it can easily generate unrelated responses when the weight is n"
2020.findings-emnlp.219,I17-1099,0,0.155669,"of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In thi"
2020.findings-emnlp.219,D16-1140,0,0.0204114,"y of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subr"
2020.findings-emnlp.219,2020.findings-emnlp.41,1,0.835521,"Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn multiple language generation tasks (Lin et al., 2020) without fine-tuning the original models’ parameters. Concurrently to our work, Smith et al. (2020) compare the performance and tradeoffs of three existing controllable language generation methods on 200 possible styles. 3 Methodology A dialogue consists of one or more alternating turns between two speakers. We define the dialogue history at turn t as Dt = {U1 , S1 , . . . , Ut } where Ut is the user utterance and St is the system response. For simplicity, we overload Dt to denote the concatenation of sequences across turns with a special token separating the turns. In this paper, we model the"
2020.findings-emnlp.219,D16-1230,0,0.0777179,"Missing"
2020.findings-emnlp.219,P19-1542,1,0.841346,"neration viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) con"
2020.findings-emnlp.219,D19-1205,0,0.0152924,", ot+1 , Ht+1 = LM(xt , Ht + ∆Ht ). At the beginning of the generation, ∆Ht is initialized to zero and it is updated using the gradients from the attribute model. Following Dathathri et al. 2424 Dataset Task #C SST-5 (Socher et al., 2013) Daily Dialogue (Li et al., 2017) AG NEWS (Zhang et al., 2015) Sentiment Act Topic 5 4 4 Samples Train Test 318,582 2210 92,650 10,295 120,000 7,600 Train 77.68 80.58 90.68 F1-Score Test SotA 47.01 55.50† 80.00 86.10‡ 90.65 95.44§ Table 2: Attribute dataset statistics and performance. State-of-the-Art (SotA) results are taken from † (Munikar et al., 2019), ‡ (Kumar et al., 2019), and § (Yang et al., 2019). (2019), we rewrite the attribute model p(a|X) as p(a|Ht + ∆Ht ) and we define the gradient update for ∆Ht as ∇∆Ht log p(a|Ht + ∆Ht ) k∇∆Ht log p(a|Ht + ∆Ht )kγ (3) where α is the step size, and γ is the scaling coefficient for the normalization term. Equation 3 is repeated p times depending on how strongly we want the response to be conditioned to the attribute. We study the effect of the step-size α and the number of iterations p on the generated text in detail in e t = Ht + ∆Ht Section 6. Subsequently, the new H is computed and a new token is generated using e t"
2020.findings-emnlp.219,N16-1014,0,0.0312371,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,D18-1255,0,0.0248346,"thods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend th"
2020.findings-emnlp.219,P16-1094,0,0.0341909,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,D16-1127,0,0.0468992,"taskspecific parameters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininej"
2020.findings-emnlp.219,P02-1040,0,0.111307,"Missing"
2020.findings-emnlp.219,2020.findings-emnlp.17,0,0.0275868,"(Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019"
2020.findings-emnlp.219,N18-1202,0,0.0480166,"omputation at decoding time, while also does not require any fine-tuning of a large language model. We demonstrate, through extensive automatic and human evaluation, a high degree of control over the generated conversational responses with regard to multiple desired attributes, while being fluent.1 1 HUMAN 2 DGPT NEGATIVE POSITIVE QUESTION BUSINESS SCIENCE & TECH SPORTS HUMAN 1 Table 1: Example of controllable response generation. DGPT is the DialoGPT (Zhang et al., 2019) response, while the others are generated using different attribute models. Introduction Large pre-trained language models (Peters et al., 2018; Radford et al., 2019; Raffel et al., 2019) have greatly improved the state-of-the-art in many downstream tasks. These language models are trained using the simple log-likelihood objective over large amounts of unlabeled data (e.g., Wikipedia articles). This approach results in large powerful language models that produce coherent text and can be ∗ Equal Contribution Work done primarily at the Caltech. 1 Code available at https://github.com/ andreamad8/PPCM † Hmmmm. I really want to try bull riding. Do you have any interest in that? I’d love to try! Can we schedule something for next week? Yes"
2020.findings-emnlp.219,P19-1534,0,0.0260474,"ics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in controlling models trained on personachat. See et al. (2019) concluded that controlling specificity, relatedness, and repetition increase human-engagement, motivating us to extend the controllabitly to styles and topics. In this paper, we focus on t"
2020.findings-emnlp.219,N19-1170,0,0.347773,"u guys again in a couple of weeks :D What’s it like? Do you ride on a regular basis or just for fun? Yea sure i can wait until after work. I am off at the end of my shift in the middle east. Sure thing! I’ll PM you my email. We can set up a dropbox or something and just message people together if you’d like. Ok how about Wednesday at 7pm central? I’ll be there with a buddy of mine who is a bull rider. Sure! What does your Saturday look like? used to perform unconditional language generation. However, they provide little control over the text generated. In the context of conversational models, See et al. (2019) show that being able to control the response generation can have a significant impact on the quality of conversations. However, controlled generation from these large conversational models remains a challenge, and is particularly more difficult in the absence of annotated conversational datasets. For large language models, controlled generation has recently received increased attention. In CTRL (Keskar et al., 2019), the language model is trained to generate based on a control code presented to the model at the start of the context. In Ziegler et al. (2019), GPT-2 (Radford et al., 2019) is fi"
2020.findings-emnlp.219,2020.findings-emnlp.291,0,0.0348626,"pics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn multiple language gene"
2020.findings-emnlp.219,D13-1170,0,0.00500261,"oglikelihood of the attribute a under the conditional attribute model p(a|X) and ii) ensuring high loglikelihood of the generated text under the unmodified conversational language model p(X). The gradient updates are restricted to Ht so to preserve the original model parameters. Let ∆Ht be the update to Ht to shift the generated text towards possesing the desired attribute a i.e., ot+1 , Ht+1 = LM(xt , Ht + ∆Ht ). At the beginning of the generation, ∆Ht is initialized to zero and it is updated using the gradients from the attribute model. Following Dathathri et al. 2424 Dataset Task #C SST-5 (Socher et al., 2013) Daily Dialogue (Li et al., 2017) AG NEWS (Zhang et al., 2015) Sentiment Act Topic 5 4 4 Samples Train Test 318,582 2210 92,650 10,295 120,000 7,600 Train 77.68 80.58 90.68 F1-Score Test SotA 47.01 55.50† 80.00 86.10‡ 90.65 95.44§ Table 2: Attribute dataset statistics and performance. State-of-the-Art (SotA) results are taken from † (Munikar et al., 2019), ‡ (Kumar et al., 2019), and § (Yang et al., 2019). (2019), we rewrite the attribute model p(a|X) as p(a|Ht + ∆Ht ) and we define the gradient update for ∆Ht as ∇∆Ht log p(a|Ht + ∆Ht ) k∇∆Ht log p(a|Ht + ∆Ht )kγ (3) where α is the step size,"
2020.findings-emnlp.219,W18-6321,0,0.15583,"the response to be conditioned to the attribute. We study the effect of the step-size α and the number of iterations p on the generated text in detail in e t = Ht + ∆Ht Section 6. Subsequently, the new H is computed and a new token is generated using e t ). The described optimizaoet+1 , Ht+1 = LM(st , H tion process is repeated for every token in the generated sequence. As aforementioned, to ensure fluency we also take a step towards minimizing the Kullback–Leibler (KL) regularization between the perturbed and the original distribution. In addition, we also use the Post-norm Geometric Fusion (Stahlberg et al., 2018; Dathathri et al., 2019) for avoiding adversarial generation (Szegedy et al., 2013). ∆Ht ← ∆Ht + α Attribute Models In PPLM the authors propose two attribute models, such as bag-of-words and discriminators. In this paper, we focus on the latter, since discriminators based attribute models do not require human selected keywords. The discriminator is a linear classifier f trained on an annotated dataset with sentence and label pairs as (x, y) – note that these sentences do not necessarily need to be conversational responses, as in our case. For each sentence x of length t, we compute the set of"
2020.findings-emnlp.219,P18-1205,0,0.0233022,"meters per style/topic, to make the controllable response generation viable for online systems. • we run a comprehensive automatic and human evaluation to show that plug-and-play methods can control the generate responses in term of style and topics, without losing fluency. • we carry out a thorough qualitative analysis on the difficulty of steering conversational models, highlighting current limitations and possible solutions. 2 Related work Open-domain conversational models Generating human-like responses involves overcoming a variety of challenges such as personalization (Li et al., 2016b; Zhang et al., 2018; Dinan et al., 2019; Wolf et al., 2019b; Madotto et al., 2019), knowledge grounding (Dinan et al., 2018; Gopalakrishnan et al., 2019; Ghazvininejad et al., 2018; Moghe et al., 2018; Wu et al., 2020), emotions (Li et al., 2017; Rashkin et al., 2019; Zhou et al., 2018; Fan et al., 2020; Li et al., 2020), diversity (Li et al., 2016a,c; Ghandeharioun et al., 2019; Serban et al., 2017; Gao et al., 2018) and so on. In terms of controlled dialogue generation, See et al. (2019) stud2423 ied of conditional generative models (Kikuchi et al., 2016) and weighted decoding (Ghazvininejad et al., 2017) in c"
2020.findings-emnlp.219,2020.emnlp-main.698,0,0.0695298,"itly to styles and topics. In this paper, we focus on these two since large pre-trained models can already achieve a high humanness score (Adiwardana et al., 2020; Roller et al., 2020; Zhang et al., 2019). Controlled Text Generation Recent methods for controlled generation include fine-tuning models using supervised learning (Peng et al., 2020; Subramani et al., 2019), reinforcement learning (Ziegler et al., 2019), adversarial training (Yu et al., 2017), by pre-training models with control codes (Keskar et al., 2019; Ficler and Goldberg, 2017; Chan et al., 2020), and other various approaches (Zhang et al., 2020b; Sheng et al., 2020; Carbone and Sarti, 2020). Alternatively, weight decoding using both bag-of-words (Holtzman et al., 2018; Ghazvininejad et al., 2017; Baheti et al., 2018; See et al., 2019) and discriminators (Holtzman et al., 2018; Krause et al., 2020), does not require any fine-tuning. Similarly, Dathathri et al. (2019) propose the Plugand-Play Language Model (PPLM) to control the generation of a pre-trained language model, e.g., GPT2 (Radford et al., 2019), both in terms of style and topic of the generated text. Finally, residual adapters (Houlsby et al., 2019) has been used to learn m"
2020.findings-emnlp.41,D18-1443,0,0.0481438,"Missing"
2020.findings-emnlp.41,P19-1285,0,0.0456936,"Missing"
2020.findings-emnlp.41,W14-3348,0,0.0197859,"ks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in each task to trade-off the performance with the parameter efficiency. Specifically,"
2020.findings-emnlp.41,N19-1423,0,0.0456527,"t downstream tasks. We adapt the design of the feedforward Transformer sub-layer following Bapna and Firat (2019). To elaborate, the adapter block consists of 1) a layer normalization (Ba et al., 2016) for an efficient adaptation and 2) a following autoencoder (Hinton and Zemel, 1994), with a residual connection. Formally, given the hidden repLM Head Adapter Layers CQA NLG SUM NMT DLG N× GPT-2 Layer Positional Word + Embedding Embedding Segment Embedding Task Embedding 2019; Peters et al., 2018) have shown to be very effective in language generation, whereas, bidirectional pre-trained models (Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019) significantly improve the performance of several down-stream classification tasks. Fine-tuning large pre-trained models has shown positive results in dialogue tasks (Wolf et al., 2019b; Budzianowski and Vuli´c, 2019) and other language generation tasks (Dong et al., 2019). However, all of the previous works only consider fine-tuning on each generation task individually, which requires a separate model for each task. In this work, we use only a single model, for multiple generation tasks. Residual adapters, derived from residual networks (He et al., 2016),"
2020.findings-emnlp.41,D16-1139,0,0.0271,"construct a set of task-specific segment embeddings. For example, in multi-turn dialogue, we alternate between System and User embeddings to help the model to capture the hierarchical structure of dialogues. Figure 1 shows the task embedding for each task, and more details are available in Appendix A2. 442 Knowledge Distillation In tasks with a large distributional shift from the original pre-trained language model (e.g., Machine Translation), we expect a larger performance gap between VLM and full fine-tuning. To cope with this issue, we propose to use sentence-level knowledge distillation (Kim and Rush, 2016), to help the task-specific parameters to better adapt to the task. Specifically, Param. GPT-2 Finetune w/o Pre-Train w/o Task Emb. LM Head VLM MT VLM w/o Task Emb. Reference SOTA 5× 5× 5× 2.55× 1.13× 1.13× 1.13× - Persona (DLG) ppl. ↓ BLEU ↑ 13.13 2.17 37.77 0.99 13.24 0.00 17.58 1.34 13.15 0.84 14.06 1.99 14.31 0.00 38.08¶ † 17.51 - NMT BLEU ↑ 25.45 16.52 0.61 12.05 22.49 24.19* 0.95 29.2§ 35.2‡ SUM ROUGE 2 ↑ 18.1 17.0 15.0 15.8 17.7 18.0* 15.0 17.20 ¶¶ 21.53 §§ CoQA F1 ↑ 67.7 15.1 35.2 47.0 69.3 66.2 32.2 45.4†† 82.5k NLG BLEU ↑ AVG ↑ 66.4 57.77 60.5 53.51 53.1 47.25 65.2 55.25 65.6 57.08 6"
2020.findings-emnlp.41,P04-1077,0,0.0172099,"ring multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in each task to trade-off the performance with the"
2020.findings-emnlp.41,2021.ccl-1.108,0,0.0875374,"Missing"
2020.findings-emnlp.41,P19-1542,1,0.903181,"Missing"
2020.findings-emnlp.41,K16-1028,0,0.033511,"of a task (e.g., Machine Translation). Then we replace the gold target (e.g., gold translation) in the training set with the greedy decoded output from the full fine-tuned model. Finally, the new constructed training set is used to fine-tune the student VLM. 4 4.1 Experiments Datasets & Evaluation Metrics We conduct our experiment on five diverse datasets covering multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More"
2020.findings-emnlp.41,P02-1040,0,0.107492,"duct our experiment on five diverse datasets covering multiple generation tasks: PersonaChat (Zhang et al., 2018; Dinan et al., 2019) for chit-chat based dialogue (DLG), IWSLT (Cettolo et al., 2016) German-English neural machine translation (NMT), CNN/Daily-Mail (Hermann et al., 2015; Nallapati et al., 2016) for text-summarization (SUM), CoQA (Reddy et al., 2019) for generative conversational question answering (CQA), and E2E NLG-challenge (Duˇsek et al., 2019) for taskoriented natural language generation (NLG). We use a large variety of evaluation metrics, such as perplexity, F1 score, BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), NIST (Lin and Och, 2004), METEOR (Denkowski and Lavie, 2014) and CiDER (Vedantam et al., 2015). Each task uses the appropriate measure, as reported in Table 1, where in NLG we report the normalized average score of multiple metrics, as in Duˇsek et al. (2019). More information about task description and the metrics used in each task are reported in Appendix A2. 4.2 Implementation and model comparison We implement VLM based on GPT-2-small (124M) (Wolf et al., 2019a), and experiment with varying adapter bottleneck dimensions in {10, 50, 100, 300} and pick the best one in eac"
2020.findings-emnlp.41,N18-1202,0,0.0925107,"meters in different colours. Residual Adapters These are trainable modules which steer the pre-trained model to different downstream tasks. We adapt the design of the feedforward Transformer sub-layer following Bapna and Firat (2019). To elaborate, the adapter block consists of 1) a layer normalization (Ba et al., 2016) for an efficient adaptation and 2) a following autoencoder (Hinton and Zemel, 1994), with a residual connection. Formally, given the hidden repLM Head Adapter Layers CQA NLG SUM NMT DLG N× GPT-2 Layer Positional Word + Embedding Embedding Segment Embedding Task Embedding 2019; Peters et al., 2018) have shown to be very effective in language generation, whereas, bidirectional pre-trained models (Devlin et al., 2019; Liu et al., 2019; Sanh et al., 2019) significantly improve the performance of several down-stream classification tasks. Fine-tuning large pre-trained models has shown positive results in dialogue tasks (Wolf et al., 2019b; Budzianowski and Vuli´c, 2019) and other language generation tasks (Dong et al., 2019). However, all of the previous works only consider fine-tuning on each generation task individually, which requires a separate model for each task. In this work, we use o"
2020.findings-emnlp.41,W04-1013,0,\N,Missing
2020.findings-emnlp.41,P17-1099,0,\N,Missing
2020.findings-emnlp.41,D19-5602,0,\N,Missing
2020.findings-emnlp.41,P16-1162,0,\N,Missing
2020.findings-emnlp.41,P18-1205,0,\N,Missing
2020.findings-emnlp.41,Q19-1016,0,\N,Missing
2020.findings-emnlp.41,W19-5917,0,\N,Missing
2020.findings-emnlp.41,D19-1165,0,\N,Missing
2020.lrec-1.73,P15-1034,0,0.0888508,"y masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by semicolons. • PG is one of the best gen"
2020.lrec-1.73,P11-1062,0,0.0282575,"et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would"
2020.lrec-1.73,P17-1152,0,0.0256402,"Missing"
2020.lrec-1.73,D13-1114,0,0.0202194,"ution. Lastly, the conversations in the Persona-Chat dataset are not collected naturally, with most of the users tending to ignore what the other said and just talking about themselves. Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007"
2020.lrec-1.73,C16-1279,0,0.013158,"Therefore, it is hard to evaluate whether “understanding your partner” helps agents speak properly. Also, since there is no publicly available data with the same user continually talking to a system, it is hard to evaluate the lifelong setting. 7. Related Work User Attributes Inference Most previous work has treated user attribute inference from social media as a classification task, such as gender prediction (Ciot et al., 2013), age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al"
2020.lrec-1.73,Q17-1024,0,0.0174945,"Missing"
2020.lrec-1.73,P14-1016,0,0.051709,"ue systems) of such extracted user attributes, and point out current limitations to cast light on future work. Keywords: Dialogue Systems, Personalization, Information Extraction, Natural Language Processing 1. Introduction User attributes are explicit representations of a person’s identity and characteristics in a structured format. They provide a rich repository of personal information for better user understanding in many applications. High-quality user attributes are, however, hard to obtain since the information in social networks such as Facebook and Twitter is often sparsely populated (Li et al., 2014). Therefore, exploiting unstructured data sources to obtain structured user attributes is a challenging research direction. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either g"
2020.lrec-1.73,P18-1136,1,0.841085,"f words in the utterance and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query ve"
2020.lrec-1.73,P19-1542,1,0.82919,"der systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we discuss potential downstream applications and point out current limitations to provide suggestions for futur"
2020.lrec-1.73,D12-1048,0,0.0185552,"reot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas."
2020.lrec-1.73,D18-1298,0,0.106348,"e is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utterance. Meanwhile, not"
2020.lrec-1.73,D16-1147,0,0.0139355,"to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attr"
2020.lrec-1.73,P02-1040,0,0.106403,"Missing"
2020.lrec-1.73,D14-1162,0,0.0918919,"Missing"
2020.lrec-1.73,P15-1169,0,0.0659842,"Missing"
2020.lrec-1.73,P19-1363,0,0.0211327,"Missing"
2020.lrec-1.73,P17-1099,0,0.0432375,"d character embeddings (100) (Hashimoto et al., 2016). The λ to weight two losses is set to be 0.5. A greedy search decoding strategy is used for our entity generator since the generated phrases are usually short. In addition, to increase model generalization and simulate an out-of-vocabulary setting, a word dropout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models"
2020.lrec-1.73,N18-1081,0,0.0831064,"opout is applied to the input by randomly masking a small number of input source tokens into unknown tokens. 4.2. Baselines We compare our model with the following implemented baselines: the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014), the pointer-generator (PG) model (See et al., 2017), and the key-value memory networks (KVMN) (Miller et al., 2016). Meanwhile, existing OpenIE models, which parse sentences and tag parts of them as output, could be an alternative. We compare our model with two state-of-the-art open information extraction (OpenIE) pre-trained models, S-OpenIE (Stanovsky et al., 2018) and LLS-OpenIE (Angeli et al., 2015). Seq2Seq, PG, and KVMN are used for internal comparison, where all the models are trained from scratch using 584 the distant supervision data. S-OpenIE and LLS-OpenIE, on the other hand, are used for external comparison, where these two models are trained on several OpenIE datasets and evaluated on the attribute extraction task. We briefly introduce the baselines: • Seq2Seq is the most common baseline for sequence generation. We use GRUs as a base model to encode a sequence of words and decode a sequence that concatenates (subject, predicate, object) by se"
2020.lrec-1.73,D18-1157,0,0.011451,"tributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches"
2020.lrec-1.73,P16-1123,0,0.0464071,"Missing"
2020.lrec-1.73,W18-5713,0,0.0202127,"ly introduce these datasets and discuss some of their limitations. Persona-Chat This is a multi-turn chit-chat corpus with annotation of the participants’ personal profiles (e.g., preferences about food, movies). It is collected by asking two crowd-workers to talk to each other freely but conditioned on their artificial personas, which are established by four to six persona sentences. An example from the dataset is provided in Table 2. In total there are 1155 personas with over 5,000 persona sentences, and 162,064 utterances over 10,907 dialogues. Most of the related works using this dataset (Weston et al., 2018; Semih Yavuz, 2018; Wolf et al., 2019; Dinan et al., 2019) focus on adapting systems to a given persona, i.e., learning to generate responses that are consistent with the persona. 1 The code is released at https://github.com/ jasonwu0731/GettingToKnowYou Although the dataset contains pre-defined personas and the corresponding conversations, it cannot be applied directly to the attribute extraction task for the following two reasons: 1) The mapping between utterances and the persona is missing. Which persona sentence is related to which utterance remains unknown. 2) All the personas are writte"
2020.lrec-1.73,P10-1013,0,0.0605996,"age prediction (Rao et al., 2010; Alekseev and Nikolenko, 2016), occupation (Preot¸iuc-Pietro et al., 2015), and political polarity (Pennacchiotti and Popescu, 2011; Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the pr"
2020.lrec-1.73,P19-1078,1,0.921683,"ce and dhdd is the hidden size of the GRU. The last hidden state henc is represented l as the final encoded vector, which will be used to query the predicate classifier and initialize the entity generator. 2 PyTorch version in github.com/huggingface/ pytorch-pretrained-BERT Predicate Classifier We use a multi-hop (K = 3 hops) end-to-end memory network (MN) (Sukhbaatar et al., 2015) as our predicate classifier because we believe its reasoning ability can benefit predicates prediction, as shown in question answering and dialogue tasks (Bordes et al., 2016; Wu et al., 2018; Madotto et al., 2018; Wu et al., 2019b). We assign the memory in the MN as all the predicate words R = {r1 , . . . , rJ }, where J is the total number of possible predicates. The predicate classifier is queried by the encoded vector henc l , and the memory attention at each hop k is computed as αk = Sof tmax(C k (P )q k ) ∈ RJ , (1) where C k and q k are the embedding matrix and query vector at hop k, respectively. Here, αk is a soft memory selector that decides the memory relevance with respect to the query vector q k . The model reads out the memory ok as X ok = αik C k+1 (ri ) ∈ Rdhdd . (2) i Then the query vector is updated f"
2020.lrec-1.73,D15-1206,0,0.0120966,"Johnson and Goldwasser, 2016). (Li et al., 2014) propose to extract three user attributes (spouse, education, and job) from Twitter using weak supervision. (Bastian et al., 2014) present a largescale topic extraction pipeline, which includes constructing a folksonomy of skills and expertise on LinkedIn. Information Extraction Closed and open form information extraction are important and well studied NLP tasks (Banko et al., 2007; Wu and Weld, 2010; Berant et al., 2011; Fader et al., 2014). Both rule-based (Mausam et al., 2012; Del Corro and Gemulla, 2013) and learningbased (Zeng et al., 2014; Xu et al., 2015; Angeli et al., 2015; Wang et al., 2016; Stanovsky et al., 2018; Vashishth et al., 2018) methods have been proposed by the research community. However, most approaches are only able to handle information by tagging/parsing part of the input source. Additionally, our work is also related to the dialogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burk"
2020.lrec-1.73,K18-1053,0,0.0150253,"alogue state tracking tasks for task-oriented dialogue systems (Wu et al., 2019a). Personalized Systems Recommender systems predict the preference a user would give to an item, which is utilized in a variety of areas. Content-based filtering (Pazzani and Billsus, 2007), knowledge-based filtering (Burke, 2000) and collaborative filtering (Sarwar et al., 1998) are the most common approaches for recommender systems. For dialogue applications, (Lucas et al., 2009) and (Joshi et al., 2017) focus on letting the agent be aware of the human pre-defined profile and so adjust the dialogue accordingly. (Zemlyanskiy and Sha, 2018) define a mutual information discovery score to re-rank system generating responses. (Madotto et al., 2019) uses meta-learning to fast adapt to unseen persona scenarios. 8. Conclusion We utilize conversational data to extract user attributes for better user understanding. Due to lacking a labeled dataset, we apply distant supervision with a natural language inference model to train our proposed two-stage attribute extractor. Our model surpasses several retrieval and generation baselines on human evaluation, and is different from existing open information extraction approaches. In the end, we d"
2020.lrec-1.73,C14-1220,0,0.08916,"Missing"
2020.lrec-1.73,P18-1205,0,0.166608,"ion. Meanwhile, there is an increasing reliance on dialogue agents to assist, inform, and entertain humans, for example, keeping the elderly company and providing customer service. Conversational data between users and systems is informative and abundant, and most of the existing deep learning approaches are trained on these large crowd-sourced corpora or scraped conversations. These models, given the current dialogue context (e.g., few previous turns), are focused on either generating good responses (Serban et al., 2015), or incorporating “system attributes” to generate consistent responses (Zhang et al., 2018; Mazare et al., 2018). However, the whole dialogue history of the same person is ignored, implying that these systems are not gradually getting to know their users by extracting user information through conversations. In this paper, we demonstrate that it is feasible to automatically extract user attributes from dialogues. Given a user utterance, our goal is to predict user information that can be represented as a (Subject, Predicate, Object) triplet format, which is available for any downstream application. For example, in Table 1, (I, live in, Florida) is extracted from the second user utte"
2021.calcs-1.20,W18-3219,0,0.032719,"Missing"
2021.calcs-1.20,E14-1049,0,0.0464071,"Missing"
2021.calcs-1.20,2020.lrec-1.223,0,0.241113,"Experiments In this section, we describe the details of the datasets we use and how the models are trained. 3.1 Datasets datasets since the multilingual models are trained with data using that form. Table 1 shows the number of tokens of each language for each dataset. We classify the language with more tokens as the ML and the other as the EL. We replace user hashtags and mentions with <USR&gt;, emoji with <EMOJI&gt;, and URL with <URL&gt; for models that use word-embeddings, similar to Winata et al. (2019a). We evaluate our model with the micro F1 score for NER and accuracy for POS tagging, following Aguilar et al. (2020a). #L1 #L2 ML EL 13,860 163,824 - 11,391 402,923 - HIN ENG MSA ENG SPA EA 12,589 178,135 9,882 92,517 HIN SPA ENG ENG NER HIN-ENG SPA-ENG MSA-EA† POS HIN-ENG SPA-ENG Table 1: Dataset statistics are taken from Aguilar et al. (2020a). We define L1 and L2 as the languages found in the dataset. For example, in HIN-ENG, L1 is HIN and L2 is ENG. † We define MSA as ML and EA as EL. #L1 represents the number of tokens in the first language and #L2 represents the number of tokens in the second language. 3.2 Experimental Setup We describe our experimental details for each model. 3.2.1 Scratch We train"
2021.calcs-1.20,L18-1550,0,0.244155,"rio. 2 Representation Models In this section, we describe multilingual models that we explore in the context of code-switching. Figure 1 shows the architectures for a word embeddings model, a multilingual language model, and the multilingual meta-embeddings (MME), and HME models. 2.1 Word Embeddings 2.1.1 FastText In general, code-switching text contains a primary language the matrix language (ML)) as well as a secondary language (the embedded language (EL)). To represent code-switching text, a straightforward idea is to train the model with the word embeddings of the ML and EL from FastText (Grave et al., 2018). Code-switching text has many noisy tokens and sometimes mixed words in the ML and EL that produce a “new word”, which leads to a high number of out-of-vocabulary (OOV) tokens. To solve this issue, we utilize subword-level embeddings from FastText (Grave et al., 2018) to generate the representations for these OOV tokens. We conduct experiments on two variants of applying the word embeddings to the code-switching tasks: FastText (ML) and FastText (EL), which utilize the word embeddings of ML and EL, respectively. 2.1.2 MUSE To leverage the information from the embeddings of both the ML and EL,"
2021.calcs-1.20,W17-4419,0,0.0329303,"Missing"
2021.calcs-1.20,2020.findings-emnlp.206,0,0.0374655,"Missing"
2021.calcs-1.20,2021.findings-emnlp.141,0,0.0409602,"Missing"
2021.calcs-1.20,N18-2031,0,0.0507836,"Missing"
2021.calcs-1.20,2020.acl-main.747,0,0.189471,"word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domain and compare son speaks more than one language in a conver- them with the existing works. Here, we would like to answer the following research question, “Whic"
2021.calcs-1.20,N19-1423,0,0.283999,"orm languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domain and compare son speaks more than one language in a conver- them with the existing works. Here, we would like to answer the following research question, “Which sation, and its usage is prevalent in multilingual models are effective in representing code-swi"
2021.calcs-1.20,L18-1473,0,0.0910692,"Missing"
2021.calcs-1.20,P14-1006,0,0.0144022,"Missing"
2021.calcs-1.20,D18-1330,0,0.0359848,"Missing"
2021.calcs-1.20,2020.acl-main.329,0,0.0877389,"ch as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. While more advanced multilingual language Learning representation for code-switching has bemodels (Conneau et al., 2020) than multilincome a crucial area of research to support a greater gual BERT (Devlin et al., 2019) have been provariety of language speakers in natural language posed, their effectiveness is still unknown in codeprocessing (NLP) applications, such as dialogue system and natural language understanding (NLU). switching tasks. Thus, we investigate their effecCode-switching is a phenomenon in which a per- tiveness in the code-switching domai"
2021.calcs-1.20,D18-1176,0,0.0479456,"Missing"
2021.calcs-1.20,W15-1521,0,0.0284237,"Missing"
2021.calcs-1.20,L18-1008,0,0.0405835,"en size of 768. This setting is important to measure the effectiveness of pre-trained multilingual models. We start the training with a learning rate of 1e-4 and an early stop of 10 epochs. We evaluate our models on five downstream tasks in the LinCE Benchmark (Aguilar et al., 2020a). We choose three named entity recognition (NER) tasks, Hindi-English (HIN-ENG) (Singh et al., 2018a), Spanish-English (SPA-ENG) (Aguilar et al., 2018) 3.2.2 Word Embeddings and Modern Standard Arabic (MSA-EA) (Aguilar We use FastText embeddings (Grave et al., 2018; et al., 2018), and two part-of-speech (POS) tag- Mikolov et al., 2018) to train our transformer modging tasks, Hindi-English (HIN-ENG) (Singh et al., els. The model consists of a 4-layer transformer 2018b) and Spanish-English (SPA-ENG) (Soto encoder with four heads and a hidden size of 200. and Hirschberg, 2017). We apply Roman-to- We train a transformer followed by a Conditional Devanagari transliteration on the Hindi-English Random Field (CRF) layer (Lafferty et al., 2001). 145 The model is trained by starting with a learning rate of 0.1 with a batch size of 32 and an early stop of 10 epochs. We also train our model with only ML and EL embeddings. We freeze al"
2021.calcs-1.20,W17-0212,0,0.0545924,"Missing"
2021.calcs-1.20,D14-1162,0,0.0948378,"Missing"
2021.calcs-1.20,P19-1493,0,0.0219007,"l language models (Devlin et al., 2019; Conneau et al., 2020) possess the ability to produce aligned multilingual representations for semantically similar words and sentences, which brings them advantages to cope with codemixed multilingual text. 2.2.1 Multilingual BERT Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual version of the BERT model, is pretrained on Wikipedia text across 104 languages with a model size of 110M parameters. It has been shown to possess a surprising multilingual ability and to outperform existing strong models on multiple zero-shot cross-lingual tasks (Pires et al., 2019; Wu and Dredze, 2019). Given its strengths in handling multilingual text, we leverage it for code-switching tasks. 2.2.2 XLM-RoBERTa XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is a multilingual language model that is pre-trained on 100 languages using more than two terabytes of filtered CommonCrawl data. Thanks to the largescale training corpora and enormous model size (XLM-RBASE and XLM-RLARGE have 270M and 550M parameters, respectively), XLM-R is shown to have a better multilingual ability than mBERT, and it can significantly outperform mBERT on a variety of cross-lingual benchmarks. Theref"
2021.calcs-1.20,P18-1143,0,0.0451186,"Missing"
2021.calcs-1.20,D18-1344,0,0.239691,"Missing"
2021.calcs-1.20,P18-3008,0,0.0857479,"Missing"
2021.calcs-1.20,W18-3503,0,0.115728,"in transformer-based models without any pretraining by following the mBERT model structure, and the parameters are randomly initialized, including the subword embeddings. We train transformer models with four and six layers with a hidden size of 768. This setting is important to measure the effectiveness of pre-trained multilingual models. We start the training with a learning rate of 1e-4 and an early stop of 10 epochs. We evaluate our models on five downstream tasks in the LinCE Benchmark (Aguilar et al., 2020a). We choose three named entity recognition (NER) tasks, Hindi-English (HIN-ENG) (Singh et al., 2018a), Spanish-English (SPA-ENG) (Aguilar et al., 2018) 3.2.2 Word Embeddings and Modern Standard Arabic (MSA-EA) (Aguilar We use FastText embeddings (Grave et al., 2018; et al., 2018), and two part-of-speech (POS) tag- Mikolov et al., 2018) to train our transformer modging tasks, Hindi-English (HIN-ENG) (Singh et al., els. The model consists of a 4-layer transformer 2018b) and Spanish-English (SPA-ENG) (Soto encoder with four heads and a hidden size of 200. and Hirschberg, 2017). We apply Roman-to- We train a transformer followed by a Conditional Devanagari transliteration on the Hindi-English R"
2021.calcs-1.20,W18-3220,0,0.0742304,"ions have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This"
2021.calcs-1.20,W18-5446,0,0.0696945,"Missing"
2021.calcs-1.20,W18-3221,0,0.0159302,"in multilingual models are effective in representing code-switching communities. Yet, despite the enormous number of text, and why?."" studies in multilingual NLP, only very few focus on code-switching. Recently, contextualized language In this paper, we evaluate the representation models, such as mBERT (Devlin et al., 2019) and quality of monolingual and bilingual word embedXLM-R (Conneau et al., 2020) have achieved state- dings, multilingual meta-embeddings, and multiof-the-art results on monolingual and cross-lingual lingual language models on five downstream tasks tasks in NLU benchmarks (Wang et al., 2018a; Hu on named entity recognition (NER) and part-ofet al., 2020; Wilie et al., 2020; Liu et al., 2020; Lin speech tagging (POS) in Hindi-English, Spanishet al., 2020). However, the effectiveness of these English, and Modern Standard Arabic-Egyptian. multilingual language models on code-switching We study the effectiveness of each model by contasks remains unknown. sidering three criteria: performance, speed, and the 142 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 142–153 June 11, 2021. ©2021 Association for Computational Linguistics https:/"
2021.calcs-1.20,2020.aacl-main.85,1,0.792195,"Yet, despite the enormous number of text, and why?."" studies in multilingual NLP, only very few focus on code-switching. Recently, contextualized language In this paper, we evaluate the representation models, such as mBERT (Devlin et al., 2019) and quality of monolingual and bilingual word embedXLM-R (Conneau et al., 2020) have achieved state- dings, multilingual meta-embeddings, and multiof-the-art results on monolingual and cross-lingual lingual language models on five downstream tasks tasks in NLU benchmarks (Wang et al., 2018a; Hu on named entity recognition (NER) and part-ofet al., 2020; Wilie et al., 2020; Liu et al., 2020; Lin speech tagging (POS) in Hindi-English, Spanishet al., 2020). However, the effectiveness of these English, and Modern Standard Arabic-Egyptian. multilingual language models on code-switching We study the effectiveness of each model by contasks remains unknown. sidering three criteria: performance, speed, and the 142 Proceedings of the Fifth Workshop on Computational Approaches to Linguistic Code-Switching, pages 142–153 June 11, 2021. ©2021 Association for Computational Linguistics https://doi.org/10.26615/978-954-452-056-4_020 Figure 1: Model architectures for code-swit"
2021.calcs-1.20,2020.acl-main.348,1,0.858692,"Missing"
2021.calcs-1.20,W19-4320,1,0.786001,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,D19-1077,0,0.0244203,"evlin et al., 2019; Conneau et al., 2020) possess the ability to produce aligned multilingual representations for semantically similar words and sentences, which brings them advantages to cope with codemixed multilingual text. 2.2.1 Multilingual BERT Multilingual BERT (mBERT) (Devlin et al., 2019), a multilingual version of the BERT model, is pretrained on Wikipedia text across 104 languages with a model size of 110M parameters. It has been shown to possess a surprising multilingual ability and to outperform existing strong models on multiple zero-shot cross-lingual tasks (Pires et al., 2019; Wu and Dredze, 2019). Given its strengths in handling multilingual text, we leverage it for code-switching tasks. 2.2.2 XLM-RoBERTa XLM-RoBERTa (XLM-R) (Conneau et al., 2020) is a multilingual language model that is pre-trained on 100 languages using more than two terabytes of filtered CommonCrawl data. Thanks to the largescale training corpora and enormous model size (XLM-RBASE and XLM-RLARGE have 270M and 550M parameters, respectively), XLM-R is shown to have a better multilingual ability than mBERT, and it can significantly outperform mBERT on a variety of cross-lingual benchmarks. Therefore, we also investiga"
2021.calcs-1.20,P16-1128,0,0.0494507,"Missing"
2021.calcs-1.20,D19-1360,1,0.812612,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,S19-2021,1,0.639343,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,W18-3207,1,0.940907,"al Models Effective in Code-Switching? Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology giwinata@connect.ust.hk Abstract Several approaches have been explored in code-switching representation learning in NLU. Multilingual language models have shown deCharacter-level representations have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dicti"
2021.calcs-1.20,K19-1026,1,0.59921,"., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dictionary and it is language-dependent. rameters to measure their practicality. We conIn another line of research, meta-embeddings duct experiments in three language pairs on have been used in code-switching by combinnamed entity recognition and part-of-speech ing multiple word embeddings from different lantagging and compare them with existing methods, such as using bilingual embeddings and guages (Winata et al., 2019a,b). This method multilingual meta-embeddings. Our findings shows the effectiveness of mixing word representasuggest that pre-trained multilingual models tions in closely related languages to form languagedo not necessarily guarantee high-quality repagnostic representations, and is considered very resentations on code-switching, while using effective in Spanish-English code-switched named meta-embeddings achieves similar results with entity recognition tasks, and significantly outpersignificantly fewer parameters. forming mBERT (Khanuja et al., 2020) with fewer 1 Introduction parameters. Whil"
2021.calcs-1.20,W18-3214,1,0.934916,"al Models Effective in Code-Switching? Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto, Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology giwinata@connect.ust.hk Abstract Several approaches have been explored in code-switching representation learning in NLU. Multilingual language models have shown deCharacter-level representations have been utilized cent performance in multilingual and crossto address the out-of-vocabulary issue in codelingual natural language understanding tasks. switched text (Winata et al., 2018c; Wang et al., However, the power of these multilingual mod2018b), while external handcrafted resources such els in code-switching tasks has not been fully explored. In this paper, we study the effectiveas gazetteers list are usually used to mitigate ness of multilingual language models to underthe low-resource issue in code-switching (Aguilar stand their capability and adaptability to the et al., 2017; Trivedi et al., 2018); however, this mixed-language setting by considering the inapproach is very limited because it relies on the ference speed, performance, and number of pasize of the dicti"
2021.dialdoc-1.6,2020.acl-main.703,0,0.0176165,"ng rate # of ckpt Testdev Phase Inference with Knowledge Evidence. During the testdev and test phase, we leverage the predictions from the KI process as the knowledge evidence components for the dialogue queries. The model generates responses based on a concatenation of the knowledge evidence and the dialogue context. Response Generation To obtain natural and relevant responses, we take advantage of the evidence to the query identified from § 3.1 and focusing on paraphrasing the corresponding knowledge sentences based on the dialogue context. We leverage the large pre-trained model BARTlarge (Lewis et al., 2020). The process of training and inference can be summarized as three steps: Post-processing To avoid serious information loss in the generations compared to the knowledge evidence for the OOD data samples, we compare the lengths of the knowledge evidence and the responses (denoted as Lkn and Lresp ). The generated response will be replaced by the raw knowledge evidence as the final output if Lresp ≤ αLkn , where α is set as 0.4. Pre-training on WoW dataset. We first pretrain the BART model on the WoW dataset for better initialization because of its similarity with the RG task. In the training pr"
2021.dialdoc-1.6,2020.emnlp-main.275,0,0.0405936,"nses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data samples in the test phase, where the corresponding documents are unseen in the training set. In Table 3, without post-processing on the predictions, the model performance consistently drops to a certain extent, which indicates that postprocessing i"
2021.dialdoc-1.6,2021.ccl-1.108,0,0.0263752,"Missing"
2021.dialdoc-1.6,D19-1129,1,0.882377,"Missing"
2021.dialdoc-1.6,2020.findings-emnlp.215,1,0.833504,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,2020.findings-emnlp.219,1,0.784817,"yxucb,eishii,giwinata}@connect.ust.hk, pascale@ece.ust.hk Abstract To tackle this problem, we leverage the pretrained language models from Liu et al. (2019a) and Lewis et al. (2020) and explore data augmentation methods with several training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the"
2021.dialdoc-1.6,W18-6319,0,0.0144952,"everal training techniques so as to avoid over-fitting to the DialDoc datasets and to teach the model the general pattern of the task. Ensemble and post-processing are conducted to further improve the model performance. Experimental results show that data augmentation is a simple but effective approach for knowledge identification in information-seeking dialogue systems (Madotto et al., 2020a), while bringing improvement to response generation at the same time. In the DialDoc21 competition, our system achieved 74.95 of F1 score and 60.74 of Exact Match in subtask 1, and 37.72 SacreBLEU score (Post, 2018) in subtask 21 . Information-seeking dialogue systems, including knowledge identification and response generation, aim to respond to users with fluent, coherent, and informative responses based on users’ needs, which. To tackle this challenge, we utilize data augmentation methods and several training techniques with the pre-trained language models to learn a general pattern of the task and thus achieve promising performance. In DialDoc21 competition, our system achieved 74.95 F1 score and 60.74 Exact Match score in subtask 1, and 37.72 SacreBLEU score in subtask 2. Empirical analysis is provid"
2021.dialdoc-1.6,P17-1147,0,0.0183514,"QA dataset and CQA datasets using MTL method. For better readability, we summarize the model settings in Table 1. We also explore more combinations of the experimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue datase"
2021.dialdoc-1.6,Q19-1016,0,0.062586,"l settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology We utilize a series of data-augmentation approaches to enable the model to obtain better representations on both dialogue context and document context and learn a general pattern of the task with less domain bias. Namely, we have a tw"
2021.dialdoc-1.6,D19-5827,1,0.846457,"erimental settings, such as other combinations of the datasets and other pre-trained language models. However, those fail to bring the improvements as much as those we mentioned above. FT Table 1: The combinations of the experimental settings for the KI subtask. Two-stage training consists of two stages: pre-training (PT) and fine-tuning (FT). which is not included in the evaluation. Among them, SearchQA (Dunn et al., 2017) and TriviaQA (Joshi et al., 2017) differ from the others by the data resource and have the least generalization ability compared to the other four datasets as reported in (Su et al., 2019). In this shared task, we consider two settings when leveraging the MRQA dataset: MRQA and MRQAsmall which excludes SearchQA and TriviaQA. Conversational QA (CQA) datasets We also introduce three CQA datasets, CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), and DoQA (Campos et al., 2020), in the shared task because of their similar settings to the KI process. Wizard-of-Wikipedia (WoW) is a commonlyused knowledge-grounded dialogue dataset (Dinan et al., 2018). It aims at providing content-full responses to user utterances based on Wikipedia documents. 3 Knowledge Identification Methodology"
2021.dialdoc-1.6,2020.emnlp-main.226,1,0.655395,"ly the question but also the previous conversation turns. Various datasets have been introduced in recent years, and many of them restrict answers to be extraction of a span from the reference document, while the others allow free-form responses (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020). In addition to the works to enrich the contents of open-domain conversations by controllable generation (Lin et al., 2020; Madotto et al., 2020b), the knowledge grounded dialogue task aims to offer more informative conversation by leveraging an external knowledge source (Dinan et al., 2018; Xu et al., 2020). Relevant knowledge selection is the key to improving the whole system, and very recently, latent variable models have been attracting more attention for this purpose (Lian et al., 2019; Liu et al., 2019b; Kim et al., 2020; Chen et al., 2020; Xu et al., 2021). Results and Discussion Results The results are shown in Table 3 and Table 4. For both subtasks, we observe gaps between the testdev phase and the test phase. For some of the models in subtask 1, multiple random seeds are applied in the training process. The performance gap may result from the domain difference of the partial data sample"
2021.emnlp-main.168,2020.emnlp-main.492,1,0.845618,"Missing"
2021.emnlp-main.168,P19-1080,0,0.0612007,"Missing"
2021.emnlp-main.168,W18-6539,0,0.0420246,"Missing"
2021.emnlp-main.168,W19-4103,1,0.811699,"Conversation within a dialogue can be thought of et al., 2019). This suggests that this inherent lack as an exchange of utterances between two speakers. of controllability may be remedied by leveraging Each utterance is not independent of one another external oracle knowledge. However, existing apbut is instead grounded within a larger dialogue proaches to knowledge grounding often suffer from context known to both parties (Jurafsky and Mar- a source-reference divergence problem whereby the tin, 2018; Sordoni et al., 2015; Serban et al., 2016; reference contains additional factual information Dziri et al., 2019). Indeed, if a response to an utter- and simply training on the reference is insufficient ance fails to be faithful to some given knowledge— to guarantee faithfulness (Wiseman et al., 2017; i.e. by producing false information—it is uninfor- Parikh et al., 2020; Tian et al., 2019). Consequently, mative and runs the risk of jeopardizing the entire ensuring the faithfulness of knowledge grounded enterprise of conversation. More precisely, this dialogue systems—via precise alignment of the means that in addition to being fluent, topical, and source and reference—remains an open challenge. ∗ Corres"
2021.emnlp-main.168,D13-1160,0,0.175343,"Missing"
2021.emnlp-main.168,N19-1423,0,0.00682988,"e each contextual representation mi ∈ Mc with randomly initialized values. We highlight our findings in Table 3 where NPH- W / O MLM performs worse than NPH across all models. Investigating further in Table 4, we observe that performance without MLM degrades substantially (e.g. ↓ 26 Hits@1) when using pre-trained GPT2 embeddings as entity memory and similarly for CompGCN embeddings. These findings suggest that MLM facilitates the learning of rich masked representations that are useful in downstream applications, a fact which is in line with other works that leverage MLM (Roberts et al., 2020; Devlin et al., 2019; Joshi et al., 2020). To judge the impact of the critic, we mask out all entity mentions as opposed to only masking out potential hallucinated ones during refinement. In Table 3, we find that NPH- W / O CRITIC performs the worst in every metric compared to all baselines which underlines that simply masking all entities—hallucinated or otherwise—in a response is not a productive strategy for effective refinement. Model GPT2-KG GPT2-KG (+ NPH) AdapterBot AdapterBot (+ NPH) GPT2+KE GPT2+KE (+ NPH) Hallucination 97.5 ± 0.6 56.5 ± 1.2 95.5 ± 0.8 59.0 ± 0.5 97.0 ± 0.2 58.5 ± 0.6 Fluency 92.5 ± 1.6"
2021.emnlp-main.168,P19-1483,0,0.0550908,"Missing"
2021.emnlp-main.168,2020.acl-main.454,0,0.0207701,"0.001). NPH uses GPT2 emb. for the KG-Entity Memory. to measure the quality of the LM. Similarly, we use retrieval metrics like Hits@k, Mean Rank (MR), and Mean Reciprocal Rank (MRR) to evaluate the Entity Mention Retriever. Precise implementation details can be found in §D. Hallucination Metrics. We consider 3 different hallucination metrics M1-M3 that provide a multifaceted measure of performance. Appendix §E outlines these metrics in detail. Succinctly, M1. BLEU (Papineni et al., 2002) M2. Hallucination Critic which we reapply to the refined response. For M3. we repurpose the FeQA measure (Durmus et al., 2020)—known to be an effective faithfulness measure in text summarization—by considering our document as the concatenation of D and all Gc1 triples while the summary is the response x ¯n+1 . Negative Candidates. We consider two different negative sampling strategies in order to compute LNCE : SANS (Ahrabian et al., 2020) and In-batchnegatives. SANS selects hard negatives by leveraging the graph structure and selecting negative samples from a context entity’s k-hop subgraph (e.g. Gc1 ). Meanwhile, In-batch-negatives considers the ground truth triple of each sample within a batch as a negative candid"
2021.emnlp-main.168,D18-1407,0,0.0658171,"Missing"
2021.emnlp-main.168,P17-1021,0,0.0502038,"Missing"
2021.emnlp-main.168,2020.acl-main.450,0,0.0487209,"Missing"
2021.emnlp-main.168,2020.coling-main.463,0,0.0727765,"Missing"
2021.emnlp-main.168,P14-1090,0,0.0318045,"Missing"
2021.emnlp-main.168,P14-2089,0,0.0401002,"Missing"
2021.emnlp-main.168,2020.acl-demos.30,0,0.0952759,"Missing"
2021.emnlp-main.590,2020.acl-main.573,0,0.0378034,"observe that by storing only a few samples per task (10-50) the model still greatly suffers from catastrophic forgetting, where with around 500 samples, which is equivalent to a total of 18,500 samples in our setting, the performance is closer to that of the multitask baseline (i.e., a possible upper bound). Similar observations are shown for the other two tasks in Figure 8, 9, and 10 in the Appendix. a fine-tuning step) during inference. Finally, continual learning has been used for sentence encoding (Liu et al., 2019), composition language learning (Li et al., 2019c) and relation learning (Han et al., 2020). However, these methods are specific to particular applications not generalizable to ToDs. 6 Related Work Continual learning methods are usually developed and benchmarked on computer visions tasks. Interested readers may refer to Mundt et al. (2020); Parisi et al. (2019); De Lange et al. (2019) for an overview of the existing approaches, and to Section 2.2 for more details on the three main CL approaches studied in this paper. Continual learning has also been studied in the Long Life Learning (LLL) scenario, where a learner continuously accumulates knowledge and makes use of it in the future"
2021.emnlp-main.622,D18-1547,0,0.416429,"of a list of slot-value pairs. Training a tion model for non-categorical slots and a classifiDST model often requires extensive annotated di- cation model for categorical slots, which hinders alogue data. These data are often collected via a the knowledge sharing from the different types of Wizard-of-Oz (Woz) (Kelley, 1984) setting, where QA datasets. Furthermore, unanswerable questions two workers converse with each other and anno- are not considered during their QA training phase. tate the dialogue states of each utterance (Wen Therefore, in a zero-shot DST setting, the model et al., 2017; Budzianowski et al., 2018; Moon et al., proposed by Gao et al. (2020) is not able to han2020), or with a Machines Talking To Machines dle “none” value slots (e.g., unmentioned slots) ∗ Work done during internship at Facebook that present in the dialogue state. 7890 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7890–7900 c November 7–11, 2021. 2021 Association for Computational Linguistics Figure 1: A high-level representation of the cross-task transfer for zero-shot DST (best viewed in color). During the QA training phase (top figure), the unified generative model (T5) i"
2021.emnlp-main.622,2020.acl-main.12,0,0.0330909,"Missing"
2021.emnlp-main.622,N19-1423,0,0.062187,"Missing"
2021.emnlp-main.622,Q19-1026,0,0.012715,"uncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi dom"
2021.emnlp-main.622,D17-1082,0,0.0876989,"re 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains"
2021.emnlp-main.622,P19-1546,0,0.0155263,"no “none” values are considered as active slots. Then the model gener2 https://github.com/jasonwu0731/ trade-dst 3 https://github.com/google-research/ google-research/tree/master/schema_ guided_dst 4 Source code is available in https://github.com/ facebookresearch/Zero-Shot-DST 7893 Model Joint Goal Accuracy Hotel Restaurant Taxi Train Average 20.06 22.46 22.60 31.25 14.20 16.28 19.80 22.72 12.59 13.56 16.50 26.28 59.21 59.27 59.50 61.87 22.39 22.76 22.50 36.72 25.69 26.87 28.18 35.77 56.81 53.90 56.81 63.22 49.57 56.06 Attraction TRADE† (Wu et al., 2019) MA-DST† (Kumar et al., 2020) SUMBT‡ (Lee et al., 2019) TransferQA (Ours) w/ Oracle Slot Gate SGD-baseline JGA AGA TransferQA JGA AGA Unseen Buses* Messaging* Payment* Trains* Alarm* 9.7 10.2 11.5 13.6 57.7 50.9 20.0 34.8 63.5 1.8 15.9 13.3 24.7 17.4 58.3 63.6 37.9 60.7 64.9 81.7 Seen Table 2: Zero-shot results on MultiWoz 2.1 (Eric et al., 2020). Results marked with † and ‡ are from Kumar et al. (2020) and Campagna et al. (2020). We also report the averaged zero shot joint goal accuracy among five domains. Note that this averaged per-domain accuracy is not comparable to the JGA in full shot setting. RentalCars Music RideSharing Media Homes Restau"
2021.emnlp-main.622,P18-1133,0,0.024592,"Predicted Values hotel-area hotel-pricerange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learn"
2021.emnlp-main.622,2021.acl-long.353,0,0.0363165,"Missing"
2021.emnlp-main.622,2021.naacl-main.448,1,0.866589,"Missing"
2021.emnlp-main.622,2020.emnlp-main.273,1,0.761706,"e range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective solution. Wu et al. (2019) adapt a copy mechanism for transferring prior k"
2021.emnlp-main.622,2021.ccl-1.108,0,0.0535518,"Missing"
2021.emnlp-main.622,P18-2124,0,0.020354,"omain unmentioned slots often appear in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level an"
2021.emnlp-main.622,D16-1264,0,0.0515644,"datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the training set. 1 Note that original MRQA-2019 dataset use SQuAD (Rajpurkar et al., 2016), here we also add the unanswerable questions from SQuAD2.0. Dataset Type Train Dev SQuAD2.0 NewsQA TriviaQA SearchQA HotpotQA NaturalQA extractive extractive extractive extractive extractive extractive 130,319 74,160 61,688 117,384 72,928 104,071 11,873 4,212 7,785 16,980 5,904 12,836 multiple-choice multiple-choice 87,866 6,116 4,887 2,040 RACE DREAM Table 1: Datasets used in the QA pre-training. Statistics of extractive datasets (except SQuAD2.0) are taken from MRQA-2019 (Fisch et al., 2019), and that of multiple-choice datasets are from RACE (Lai et al., 2017) and DREAM (Sun et al., 2019)."
2021.emnlp-main.622,N18-2074,0,0.0549602,"Missing"
2021.emnlp-main.622,Q19-1014,0,0.141767,"passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training and testing. In SGD, the test set has 18 domains, and 5 of the domains are not presented in the trai"
2021.emnlp-main.622,W17-2623,0,0.0269985,"in the middle of conversations, where some of the in-domain slots have not yet mentioned by the user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In Mul"
2021.emnlp-main.622,D19-1221,0,0.0383259,"Missing"
2021.emnlp-main.622,E17-1042,0,0.0744009,"Missing"
2021.emnlp-main.622,2020.emnlp-main.66,0,0.0179673,"erange what is the area of the hotel that the user wants? what is the price range of the hotel or guesthouse that the user wants? east cheap none none Table 6: Two typical errors of TransferQA zeroshot in MultiWoz 2.1. The first (top example) is predicting the values that not confirmed by the user yet, and the second (bottom example) is missing the values of implicit mentioned domain. Dialogue State Tracking is an essential yet challenging task in conversational AI research (Williams and Young, 2007; Williams et al., 2014). Recent state-of-the-art models (Lei et al., 2018; Zhang et al., 2020; Wu et al., 2020; Peng et al., 2020; Zhang et al., 2019; Kim et al., 2019; Lin et al., 2020; Chen et al., 2020; Heck et al., 2020; Mehri et al., 2020; Hosseini-Asl et al., 2020; Yu et al., 2020; Li et al., 2020; Madotto et al., 2020) trained with extensive annotated dialogue data have shown promising performance in complex multi-domain conversations (Budzianowski et al., 2018; Eric et al., 2020). However, collecting large amounts of data for every dialogue domain is often costly and inefficient. To reduce the expense of data acquisition, zero-shot (few-shot) transfer learning has been proposed as an effective"
2021.emnlp-main.622,P19-1078,1,0.83341,"nsuming manual annotations, while M2M requires exhaustive hand-crafted rules for covering various dialogue scenarios. In industrial applications, virtual assistants are required to add new services (domains) frequently based on user’s needs, but collecting extensive data for every new domain is costly and inefficient. Therefore, performing zero-shot prediction of dialogue states is becoming increasingly important since it does not require the expense of data acquisition. There are mainly two lines of work in the zero-shot transfer learning problem. The first is cross-domain transfer learning (Wu et al., 2019; Kumar et al., 2020; Rastogi et al., 2020; Lin et al., 2021a), where the models are first trained on several domains, then zero-shot to new domains. However, these methods rely on a considerable amount 1 Introduction of DST data to cover a broad range of slot types, Virtual assistants are designed to help users per- and it is still challenging for the models to handle form daily activities, such as travel planning, on- new slot types in the unseen domain. The second line shopping and restaurant booking. Dialogue line of work leverages machine reading question state tracking (DST), as an essen"
2021.emnlp-main.622,D18-1259,0,0.0255177,"user. We simulate such scenario by truncating the context passage from the first sentence that contains the answer span. As illustrated in Figure 3, given a question and a passage from a QA training set, we first truncate the passage according to the answer span annotation, then we pair the question and the truncated passage as an unanswerable sample. 3 Experiments 3.1 Datasets QA datasets. For the QA training, we use six extractive QA datasets such as SQuAD2.0 (Rajpurkar et al., 2018) 1 , NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), Natural Question (Kwiatkowski et al., 2019) from MRQA-2019 (Fisch et al., 2019), and two multichoice datasets such as RACE (Lai et al., 2017) and DREAM (Sun et al., 2019). The main train/dev statistics are reported in Table 1. DST datasets. The evaluation is conducted on two multi-domain task-oriented dialogue benchmark, MultiWoz (Budzianowski et al., 2018; Eric et al., 2020) and Schema-Guided-Dialogue (SGD) (Rastogi et al., 2020). Both datasets provide turn-level annotations of dialogue states. In MultiWoz, we follow the pre-processing and evaluation setup from Wu et al. (2019), where resta"
2021.emnlp-main.622,2020.nlp4convai-1.13,0,0.0601404,"Missing"
2021.mrl-1.1,Q19-1038,0,0.0200543,"on the few-shot learning, where the source language and target language are German and English, respectively. and the performance increases as the models are given more samples. We conjecture that pre-trained models may be able to adapt to languages that are similar to English. However, for many language tasks, it is difficult to collect a large supervised training dataset as language experts (e.g., linguists or native speakers) are required to annotate the data. Another line of work is to apply cross-lingual transfer on English with the same task as the target languages (Ponti et al., 2018; Artetxe and Schwenk, 2019; Liu et al., 2019b; Lauscher et al., 2020; Liu et al., 2020b, 2021c; Chen et al., 2021). However, such methods still need to apply a fine-tuning step to update the model for fast adaptation, which can be challenging for large pre-trained models – some models require substantial memory capacity – since the models have to be trained on highperforming machines. Different from the aforementioned method, in-context learning using an LM does not allow any parameter updates. Thus, the process does not need to compute and store the gradients for backward propagation. In this work, we investigate the"
2021.mrl-1.1,2020.emnlp-main.697,0,0.0750431,"Missing"
2021.mrl-1.1,2021.acl-long.295,0,0.0505251,"Missing"
2021.mrl-1.1,2021.repl4nlp-1.4,0,0.0340213,"ero-shot experiments on the MTOP and MultiNLU datasets. 7 Figure 3: The results on German (de) MTOP dataset with GPT models. Figure 4: The results on English (en) MTOP dataset with GPT models. Figure 5: The results on Spanish (es) MTOP dataset with GPT models. Figure 6: The results on French (fr) MTOP dataset with GPT models. Figure 7: The results on English (en) multilingual NLU dataset with GPT models. Figure 8: The results on Spanish (es) multilingual NLU dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in"
2021.mrl-1.1,2020.acl-main.747,0,0.267705,"oken limit. We utilize an NVIDIA Tesla V100 16GB GPU to run the inference so that the model is ensured to fit in a single GPU, and we use 16-bit precision. Model Name Model details We run experiments on a variety of publicly available models:3 four sizes of GPT-2 models (0.1B, 0.3B, 0.8B and 1.6B), three sizes of GPTNEO models (1.3B, 2.7B, and 6B), and two sizes of T5 models (0.8B and 3B). Table 3 shows the details of each pre-trained model. Baselines We use the same sets of few-shot samples for the baselines. We run fine-tuning on the pre-trained models mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and also compare our models with the zero-shot cross-task models using pre-trained models XLM-R, fine-tuned on XNLI (Conneau et al., 2018), and BART, finenparams nlayers nhidden nffn GPT-2 GPT-2MEDIUM GPT-2LARGE GPT-2XL GPTNEO GPTNEO GPTNEO-J 0.1B 0.3B 0.8B 1.6B 1.3B 2.7B 6B 12 24 36 48 24 32 28 768 768 1,280 1,600 2,048 2,560 4096 16,384 T5LARGE T53B 0.8B 3B 24 24 1,024 1,024 4,096 16,384 Table 3: Model architecture. 4 The XLM-R model fine-tuned with XNLI data can be accessed at https://huggingface.co/joeddav/ xlm-roberta-large-xnli. The BART model finetuned with MNLI data can be accessed a"
2021.mrl-1.1,P18-1031,0,0.0238954,"h GPT models. Figure 8: The results on Spanish (es) multilingual NLU dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffe"
2021.mrl-1.1,2020.emnlp-main.363,0,0.0420681,"Missing"
2021.mrl-1.1,D18-1269,0,0.0228577,"-bit precision. Model Name Model details We run experiments on a variety of publicly available models:3 four sizes of GPT-2 models (0.1B, 0.3B, 0.8B and 1.6B), three sizes of GPTNEO models (1.3B, 2.7B, and 6B), and two sizes of T5 models (0.8B and 3B). Table 3 shows the details of each pre-trained model. Baselines We use the same sets of few-shot samples for the baselines. We run fine-tuning on the pre-trained models mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and also compare our models with the zero-shot cross-task models using pre-trained models XLM-R, fine-tuned on XNLI (Conneau et al., 2018), and BART, finenparams nlayers nhidden nffn GPT-2 GPT-2MEDIUM GPT-2LARGE GPT-2XL GPTNEO GPTNEO GPTNEO-J 0.1B 0.3B 0.8B 1.6B 1.3B 2.7B 6B 12 24 36 48 24 32 28 768 768 1,280 1,600 2,048 2,560 4096 16,384 T5LARGE T53B 0.8B 3B 24 24 1,024 1,024 4,096 16,384 Table 3: Model architecture. 4 The XLM-R model fine-tuned with XNLI data can be accessed at https://huggingface.co/joeddav/ xlm-roberta-large-xnli. The BART model finetuned with MNLI data can be accessed at https:// huggingface.co/facebook/bart-large-mnli 3 The models except GPTNEO-J are taken from https://huggingface.co/. The GPTNEO-J model i"
2021.mrl-1.1,2020.lrec-1.302,0,0.072522,"rms as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such a"
2021.mrl-1.1,2020.acl-main.703,0,0.225711,"on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GPU or TPU resources. Fewshot learning can be seen as a one-for-all plugand-play computational model that can be applied to various natural la"
2021.mrl-1.1,N19-1423,0,0.561013,"l., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GPU or TPU resources. F"
2021.mrl-1.1,2021.ccl-1.108,0,0.0558726,"Missing"
2021.mrl-1.1,2021.eacl-main.257,0,0.0383577,"Missing"
2021.mrl-1.1,D19-1129,1,0.876253,"Missing"
2021.mrl-1.1,2021.acl-long.353,0,0.0468921,"Missing"
2021.mrl-1.1,P19-1301,0,0.0260588,"2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has l"
2021.mrl-1.1,2021.naacl-main.448,1,0.812595,"Missing"
2021.mrl-1.1,2021.repl4nlp-1.13,1,0.918187,"the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 202"
2021.mrl-1.1,2020.findings-emnlp.215,1,0.907069,"han random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster"
2021.mrl-1.1,2020.emnlp-main.273,1,0.676661,"T5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investi"
2021.mrl-1.1,2020.tacl-1.47,0,0.287468,"text, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models and translation models. 1 Figure 1: Accuracy vs. model size on English-Spanish MNLU dataset. Cross-lingual in-context learning with LMs (i.e., context with few English examples tested on Spanish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Ng"
2021.mrl-1.1,2020.findings-emnlp.92,0,0.0343562,"Missing"
2021.mrl-1.1,W96-0200,0,0.743872,"Missing"
2021.mrl-1.1,2021.naacl-main.185,0,0.261269,"Missing"
2021.mrl-1.1,N19-1380,0,0.379572,"al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et"
2021.mrl-1.1,N18-1202,0,0.449595,"t al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility of conducting few-shot learning, that is, learning a new task using a small number of examples without any further training or gradient computation. Few-shot learning alleviates the cost for extensive labeled data, which is beneficial since collecting high-quality labeled data is resource-intensive and expensive. It also reduces the cost for model fine-tuning, which requires tremendous GP"
2021.mrl-1.1,2020.emnlp-main.346,0,0.0355019,"al., 2020b; Madotto et al., 2020b; Zhao et al., 2021; Schick and Schütze, 2021; Lin et al., 2021a). In this approach, we select the appropriate prompts to trigger the LMs to behave so that they can predict the desired output (Liu et al., 2021b). However, the prompts have to be engineered to allow the LM to generate a text appropriate to solve the task. Learning to calibrate the few-shot results is also essential to reduce the model’s performance variance (Zhao et al., 2021), and the selection criteria in choosing the prompts are also important (Perez et al., 2021). In another stream of work, Shin et al. (2020); Li and Liang (2021) proposed an automated method to create prompts for a diverse set of tasks by gradient-based tuning instead of manually searching for a good prompt. Using such a method, may allow us to find an optimal prompt easier, it is very difficult to discover the optimal prompts for complicated natural language processing tasks, such as semantic parsing (Liu et al., 2021b). Ablation Study To further understand how much data we need for the in-context learning, we conduct experiments with different numbers of few-shot samples, including zero-shot experiments on the MTOP and MultiNLU"
2021.mrl-1.1,D19-1250,0,0.07384,"Missing"
2021.mrl-1.1,2020.aacl-main.85,1,0.749893,"ish sentences) performs as well as models trained in cross-lingual setting (Liu et al., 2020b) and translation baselines. The idea of few-shot learning is also relevant to address the low-resource issue in non-English languages. Few-shot learning has been applied to NLP tasks (Brown et al., 2020; Madotto et al., 2020b; Lu et al., 2021; Perez et al., 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English"
2021.mrl-1.1,2020.emnlp-main.617,0,0.0549595,"Missing"
2021.mrl-1.1,D18-1026,0,0.0444186,"Missing"
2021.mrl-1.1,N18-1101,0,0.0485174,"± 3.82 68.89 ± 2.51 63.33 ± 7.14 38.97 ± 14.80 80.12 ± 3.95 61.51 ± 1.63 63.10 ± 4.46 86.60 ± 2.40 mBERT 0.2B XLM-RBASE 0.3B 88.57 ± 3.14 87.95 ± 1.39 25.21 ± 2.31 27.47 ± 11.90 16.54 ± 5.54 13.8 ± 6.50 84.88 ± 1.59 77.06 ± 3.16 87.87 ± 3.29 74.85 ± 1.53 Zero-shot Cross-Task Prediction 43.41 53.37 36.06 51.67 Few-shot Learning (K-shot) Fine-tuning (40-shot) 41.44 ± 5.59 37.03 ± 5.11 33.82 ± 10.08 27.16 ± 5.51 Table 2: Zero-shot and few-shot results in the monolingual setting. The SOTA results are taken from † Li et al. (2021), ‡ Qin et al. (2019), and ∗ Schuster et al. (2019). tuned on MNLI (Williams et al., 2018);4 a random baseline; and state-of-the-art results reported on each dataset. For the finetuning, we use a learning rate of 5e-5 with a decay of 0.9 for every epoch, and a batch size of 32. We apply an early stopping after 5 epochs without any improvement on the validation set. with the English context (en→XX). In the few-shot in-context learning, we use k-way-few-shot classification, taking k samples. For each model, we take k ∈ [0, 5, K], where K ≤ 40 is the largest number of few-shot samples that can be passed to the model as input and is divisible by 10 without exceeding the maximum input t"
2021.mrl-1.1,D19-1214,0,0.0249661,"± 1.87 50.77 ± 4.41 62.71 ± 6.30 66.97 ± 1.35 50.70 ± 2.47 55.91 ± 3.82 68.89 ± 2.51 63.33 ± 7.14 38.97 ± 14.80 80.12 ± 3.95 61.51 ± 1.63 63.10 ± 4.46 86.60 ± 2.40 mBERT 0.2B XLM-RBASE 0.3B 88.57 ± 3.14 87.95 ± 1.39 25.21 ± 2.31 27.47 ± 11.90 16.54 ± 5.54 13.8 ± 6.50 84.88 ± 1.59 77.06 ± 3.16 87.87 ± 3.29 74.85 ± 1.53 Zero-shot Cross-Task Prediction 43.41 53.37 36.06 51.67 Few-shot Learning (K-shot) Fine-tuning (40-shot) 41.44 ± 5.59 37.03 ± 5.11 33.82 ± 10.08 27.16 ± 5.51 Table 2: Zero-shot and few-shot results in the monolingual setting. The SOTA results are taken from † Li et al. (2021), ‡ Qin et al. (2019), and ∗ Schuster et al. (2019). tuned on MNLI (Williams et al., 2018);4 a random baseline; and state-of-the-art results reported on each dataset. For the finetuning, we use a learning rate of 5e-5 with a decay of 0.9 for every epoch, and a batch size of 32. We apply an early stopping after 5 epochs without any improvement on the validation set. with the English context (en→XX). In the few-shot in-context learning, we use k-way-few-shot classification, taking k samples. For each model, we take k ∈ [0, 5, K], where K ≤ 40 is the largest number of few-shot samples that can be passed to the model"
2021.mrl-1.1,2021.calcs-1.20,1,0.721866,"T (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different langua"
2021.mrl-1.1,K19-1026,1,0.830093,", 2021; Liu et al., 2021a,b; Cahyawijaya et al., 2021a). Common approaches to solve the low-resource issue are to pretrain models with self-supervised learning using unlabelled monolingual text data collected from various resources available online (Wilie et al., 2020; Le et al., 2020; Martin et al., 2020; Eddine et al., 2020; Nguyen and Nguyen, 2020; Scheible et al., 2020; Bhattacharjee et al., 2021; Lee et al., 2020; Cahyawijaya et al., 2021b; Park et al., 2021) and then apply pre-training on the source language and fine-tune on the target languages (Schuster et al., 2019; Lin et al., 2019; Winata et al., 2019, 2021; Pfeiffer et al., 2020; Zheng et al., 2021; Lin et al., 2021b). Conversely, the few-shot learning does not need any training from the source and target languages. Figure 1 shows how it is possible to utilize pre-trained models on non-English languages, such as Spanish, as the performance is not random, Introduction The progress in language model (LM) pretraining (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019a; Brown et al., 2020; Liu et al., 2020a; Lewis et al., 2020; Raffel et al., 2020; Gao et al., 2020a) has led to the possibility"
2021.mrl-1.1,2020.emnlp-main.409,0,0.0235976,"2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investigate further the applicability of this method"
2021.mrl-1.1,2020.emnlp-main.410,0,0.0336126,"Missing"
2021.mrl-1.1,2021.naacl-main.41,0,0.0260138,"U dataset with GPT models. 6.2 Pre-trained Language Models et al., 2019), XLM (Conneau and Lample, 2019), and XLM-R (Conneau et al., 2020; Goyal et al., 2021), decoder-only models, such as GPT models (Radford et al., 2019; Brown et al., 2020) and encoder-decoder models, such as T5 (Raffel et al., 2020), BART (Lewis et al., 2020), and their mulRecent advances in pre-trained LMs have been focused on building pre-trained encoders, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019a), ELMO (Peters et al., 2018), ULMFiT (Howard and Ruder, 2018), ELECTRA (Clark 8 tilingual versions, mT5 (Xue et al., 2021) and mBART (Liu et al., 2020a). Pre-trained encoders have been used to improve the contextualized representations of multilingual systems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a;"
2021.mrl-1.1,2020.emnlp-main.272,0,0.0328713,"ems in various NLP tasks, for example, dialogue systems (Liu et al., 2020b, 2021d; Li et al., 2021), code-switching sequence labeling (Aguilar et al., 2020; Winata et al., 2021; Winata, 2021), and multilingual speech recognition (Datta et al., 2020; Winata et al., 2020). Meanwhile, the pre-trained encoder-decoder models, have been used for various sequence generation tasks, such as summarization (Raffel et al., 2020), conversational agents (Lin et al., 2020b,a; Madotto et al., 2020a; Wu and Xiong, 2020; Hosseini-Asl et al., 2020; Lin et al., 2021b), and knowledge grounding (Chen et al., 2020; Zhao et al., 2020). 7 work is our initial attempt to show the effectiveness of in-context learning in the multilingual and crosslingual setting. It covers four different languages and explores the possibility of conducting efficient inference on low-resource tasks. We find that LMs can predict samples correctly, significantly better than the random prediction, in cross-lingual tasks with no training examples of the target languages. We would like to investigate further the applicability of this method to other tasks and languages in future work. Acknowledgment We want to thank Bryan Wilie and Samuel Cahyawijaya"
2021.naacl-main.158,W18-5513,0,0.0509333,"Missing"
2021.naacl-main.158,N19-1423,0,0.0121606,"d “Refute”/“Not Enough Info” is mapped into Unsupported to match our task setting. Note that to balance the dataset, we obtain half the data from “Refute” and the other half from “Not Enough Info”. Note that the gold evidence is included in the dataset released by Thorne et al. 5.2 Models Ours We consider one unidirectional LM and one masked LM for our proposed perplexity-based methodology. • PPLGPT2-B – Our single-parameter classifier based on perplexity from GPT2-base (Radford et al., 2019) (unidirectional LM) • PPLBERT-B – Our single-parameter classifier based on perplexity from BERT-base (Devlin et al., 2019) (Masked LM) Baselines We finetune various pre-trained Transformer-based (Vaswani et al., 2017) models to build our baseline classifiers, which is a common approach used to achieve many state-of-the-art results in the literature. • Major Class – A simple majority classifier which always assigns the majority class of the training set to all samples. We provide this for reference because some of our dataset classes are imbalanced. 5.3 Experimental Setup Few-Shot Data Setup Given ND as the size of the dataset D, we do an n-shot experiment with n samples from D as a “validation set” for our perple"
2021.naacl-main.158,2020.acl-main.398,0,0.0228133,"Missing"
2021.naacl-main.158,karadzhov-etal-2017-fully,0,0.0584555,"Missing"
2021.naacl-main.158,2020.fever-1.5,1,0.779876,"knowledge from its training corpus, there are a few limitations to solely relying on the pre-trained weights. First, we cannot easily check and guarantee whether the LM has already seen the evidence that is required for verification, and the LM would definitely not have seen the evidence related to newly emerging events after the LM pretraining. For instance, the event of COVID-19 emerged after the release of the GPT2 pre-trained model. Second, although LMs have shown surprising ability in memorizing some knowledge, they are not perfect, as pointed out by previous works (Poerner et al., 2019; Lee et al., 2020). Therefore, we propose to incorporate evidence into the perplexity calculation by using it as a prefix of the claim. There are two popular kinds of LMs: i) unidirectional LMs that are trained with the conventional next token prediction task, and ii) masked LMs that are trained with the masked token prediction token, resulting in a bidirectional LM. We briefly describe how to obtain the evidence-conditioned perplexity for both types of LM: Unidirectional Language Model Perplexity For a unidirectional LM, first we concatenate the evidence and claim to obtain the input to the LM: X = {xe0 , . ."
2021.naacl-main.158,D18-1003,0,0.0341158,"Missing"
2021.naacl-main.158,2021.ccl-1.108,0,0.0703809,"Missing"
2021.naacl-main.158,P19-1244,0,0.0330818,"Missing"
2021.naacl-main.158,D19-1250,0,0.161028,"Going further, Brown et al. illustrated the impressive potential of LMs as strong zero-shot and fewshot learners across translation, commonsense reasoning and natural language inference (NLI). However, little or no exploration has been made on fewshot learning in the fact-checking domain, which is a timely and important task in which data-scarcity is particularly problematic. Previous works have proposed different ways of leveraging LMs to conduct zero- or few-shot learning. One common approach is to query the LM for the missing token (i.e., “answer”) for the zeroshot question-answering task (Petroni et al., 2019; 1 Introduction Brown et al., 2020) by transforming questions into Few-shot learning is being actively explored to a form of statement. Another approach is to adopt overcome the heavy dependence on large-scale la- an in-context learning approach where the input beled data that serves as a crucial bottleneck to context of the LM is carefully crafted to control machine learning models. Recently, researchers the output. For example, a natural language task have explored few-shot learning that leverages the instruction (e.g., “Translate English to French:”) or powerful transfer learning ability o"
2021.naacl-main.158,2020.emnlp-main.437,0,0.0376932,"Missing"
2021.naacl-main.158,2020.acl-main.240,0,0.0531823,"Missing"
2021.naacl-main.158,N18-1074,0,0.0224053,"Missing"
2021.naacl-main.158,N19-1230,0,0.0223888,"Missing"
2021.naacl-main.158,W18-5515,0,0.0431353,"Missing"
2021.naacl-main.158,2020.acl-main.549,0,0.031193,"Missing"
2021.naacl-main.158,P19-1085,0,0.0247678,"Missing"
2021.naacl-main.448,D18-1547,0,0.438485,"y incorporate a pre-trained 5640 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5640–5648 June 6–11, 2021. ©2021 Association for Computational Linguistics Slot Type Figure 2: Slot description examples. seq2seq model (e.g., T5 (Raffel et al., 2020)) without any task-specific modification. To further enhance the model’s cross-domain transferability, we propose Slot Type Informed Descriptions that capture the shared information of different slots. Experimental results on the MultiWOZ benchmark (Budzianowski et al., 2018) suggest that 1) our model achieves significantly higher joint goal accuracy compared to existing results in zero-shot cross domain DST; 2) models using the proposed slot description formulation substantially outperform those using other slot description variants. Our contributions are summarized as the following: • We propose a simple yet novel generative DST model based on T5 that significantly improves existing zero-shot cross-domain DST results; • We investigate the effectiveness of different slot description formulations. To the best of our knowledge, this is the first work that comprehen"
2021.naacl-main.448,N18-2074,0,0.036191,"wledge transfer among different slots. We construct a template for each slot type that follows &quot;[slot type] of [slot] of the [domain]&quot;. We denote such a slot description as Slot Type. More details are available in Appendix A.1. 4 4.1 vi = Seq2seq(Ct , si ). (1) The learning objective of this generation process is minimizing the negative log-likelihood of vi given Ct and si , that is, L=− n X log p(vi |Ct , si ), (2) i where n is the number of slots to be tracked. We initialize the model parameters with T5 (Raffel et al., 2020), an encoder-decoder Transformer with relative position embeddings (Shaw et al., 2018) pre-trained on a massive amount of English text. We denote our model as T5DST. To incorporate slot descriptions into T5DST, we replace the slot name with its corresponding slot description as the model input. 3.2 Slot Type Informed Descriptions Experiments Dataset and Evaluation We evaluate the proposed method on the MultiWOZ 2.0 dataset (Budzianowski et al., 2018), which has 7 domains. We use the pre-processing and evaluation setup from Wu et al. (2019), where restaurant, train, attraction, hotel, and taxi domains are used for training, as the test set only contains these 5 domains. In the z"
2021.naacl-main.448,2020.emnlp-main.66,0,0.0402704,"et al. (2019, 2020) formulated DST as a question answering problem by casting a slot name into questions. However, these works did not show the effectiveness of slot descriptions, by comparing the performance of models with and without them. There is no study on how to construct slot descriptions. In this paper, we aim to fill this research gap by providing an empirical study on the different slot description formulations. Dialogue State Tracking has been of broad interest to the dialogue research community (Williams and Young, 2007; Williams et al., 2014; Heck et al., 2020; Liu et al., 2020; Wu et al., 2020; Madotto et al., 2020). Current 3 Methodology state-of-the-art models (Chen et al., 2020; Lin et al., 2020; Heck et al., 2020; Hosseini-Asl et al., 3.1 T5DST 2020; Ye et al., 2021; Li et al., 2020) trained with extensive annotated data have been shown The design of our model follows the basis of genpromising performance in complex multi-domain erative question answering models. As illustrated conversations (Budzianowski et al., 2018). How- in Figure 1, given a dialogue history which conever, collecting large amounts of data for every sists of an alternating set of utterances from two 5641 Mod"
2021.naacl-main.448,P19-1078,1,0.899837,"tate tracking (DST) is an essential component of task-oriented dialogue systems that tracks users’ requirements over multi-turn conversations. A popular formulation of the dialogue state is in the form of a list of slot-value pairs. In DST, tracking unseen slots in a new domain, a.k.a. zeroshot domain adaptation, is a significant challenge, ∗ Work done during internship at Facebook since the model has never seen in-domain training samples. There are two main lines of work to tackle this problem. The first proposes domain transferable models using copy mechanisms or ontology graph information (Wu et al., 2019; Zhou and Small, 2019). A limitation of such models is that they may not fully leverage pre-trained language models due to the specialized model architecture. The second line of work uses slot-descriptions as input to the model to facilitate the slot understanding (Rastogi et al., 2020). However, the provided slot descriptions are collected by crowd sourced human annotators and might be inconsistent among different domains. In general, the optimal approach for constructing slot descriptions in zero-shot settings remains unexplored. In this work, we tackle the challenge of zeroshot cross-domai"
2021.naacl-main.448,2020.nlp4convai-1.13,0,0.263341,"Missing"
2021.nlp4convai-1.10,W18-3219,0,0.060773,"Missing"
2021.nlp4convai-1.10,N19-1253,0,0.0286069,"es in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused on classification and sequence labeling, while inCompared to collecting new persona sentences st"
2021.nlp4convai-1.10,P15-1039,0,0.0629361,"Missing"
2021.nlp4convai-1.10,D18-1038,0,0.0133005,"ong languages and circum- speaker annotators with at least a bachelor degree and a fluent level of English and asked them to vents the requirement of extensive training data in revise the machine-translated dialogues and pertarget languages (Wisniewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and"
2021.nlp4convai-1.10,P19-4007,0,0.0356572,"glish dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused on classification and sequence labeling, while inCompared to collecting new persona sentences stead, Chi et al. (2019) proposed to pre-train both and dialogues"
2021.nlp4convai-1.10,D18-1269,0,0.147764,"odels, especially in low resources languages, translation-pipeline models using both autoare only available using costly APIs. matic and human evaluation. Experimental reIn this paper, we analyze two possible sults show that the multilingual trained models workarounds to alleviate the aforementioned chaloutperform the translation-pipeline and that lenges. The first is to build a cross-lingual transthey are on par with the monolingual models, ferable system by aligning cross-lingual represenwith the advantage of having a single model across multiple languages. On the other hand, tations, as in Conneau et al. (2018), in which the the state-of-the-art cross-lingual trained modsystem is trained on one language and zero-shot els achieve inferior performance to the other to another language. The second is to learn a mulmodels, showing that cross-lingual conversatilingual system directly from noisy multilingual tion modeling is a challenging task. We hope data (e.g., translated data), thus getting rid of the that our dataset and baselines 1 will accelerate translation system dependence at inference time. research in multilingual dialogue systems. To evaluate the aforementioned solutions, we propose a dataset"
2021.nlp4convai-1.10,P19-1346,0,0.0277735,"ed as follows: • We present the first multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the pe"
2021.nlp4convai-1.10,P18-5002,0,0.0289758,"Missing"
2021.nlp4convai-1.10,D17-1302,0,0.0223462,"niewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), and dependency to translate the English dialogues. They are also parsing (Ahmad et al., 2019). Meanwhile, Lample and Conneau (2019) and Conneau et al. (2019) pro- allowed to customize dialogues and persona sentences. The annotated dialogues can deviate from posed pre-trained cross-lingual language models to original translation while retain persona and conalign multiple language representations, achieving state-of-the-art results in many cross-lingual classi- versation consistency. The full annotation instructions are reported in Appendix A. fication tasks. The aforementioned tasks focused o"
2021.nlp4convai-1.10,P16-1094,0,0.17271,"ns to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona-consistent dialogue (Zhang et al., 2018). Several works have improved on the initial baselines with various methodologies, especially using large pre"
2021.nlp4convai-1.10,D16-1127,0,0.211223,"ns to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona-consistent dialogue (Zhang et al., 2018). Several works have improved on the initial baselines with various methodologies, especially using large pre"
2021.nlp4convai-1.10,D16-1230,0,0.07703,"Missing"
2021.nlp4convai-1.10,P19-1227,0,0.0209796,"et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The proposed XPersona dataset is an extension of the persona-chat"
2021.nlp4convai-1.10,2020.tacl-1.47,0,0.0179185,"e number of dialogues (#Dial.) and utterances (#Utt.) of the validation and test set in six languages. Edit distance per dialogue (Edit) and BLEU score are computed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none"
2021.nlp4convai-1.10,P19-1081,0,0.0202563,"multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions, a chit-chat model is"
2021.nlp4convai-1.10,Q17-1022,0,0.0700587,"Missing"
2021.nlp4convai-1.10,P17-1135,0,0.022254,"Missing"
2021.nlp4convai-1.10,P17-1178,0,0.0121823,"7), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The"
2021.nlp4convai-1.10,P02-1040,0,0.111605,"layer remain frozen. This retains the decoders’ ability to generate multilingual output while still being able to learn new tasks using only the target language. 5 5.1 Experiments Evaluation Metrics Evaluating open-domain chit-chat models is challenging, especially in multiple languages and at the dialogue-level. Hence, we evaluate our models using both automatic and human evaluation. In both cases, human-annotated dialogues are used, which show the importance of the provided dataset. Automatic For each language, we evaluate responses generated by the models using perplexity (ppl.) and BLEU (Papineni et al., 2002) with reference to the human-annotated responses. Although these automatic measures are not perfect (Liu et al., 2016), they help to roughly estimate the performance of different models under the same test set. More recently, Adiwardana et al. (2020) has shown the correlation between perplexity and human judgment in open-domain chit-chat models. such evaluation, a human interacts with the systems for several turns, and then they assign a score from 1 to 5 based on three questions (Zhang et al., 2018) about fluency, engagingness, and consistency. This evaluation is both expensive to conduct and"
2021.nlp4convai-1.10,P19-1493,0,0.152553,"mputed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation ta"
2021.nlp4convai-1.10,Q19-1016,0,0.0257738,"e present the first multilingual non-goaloriented dialogue benchmark for evaluating multilingual generative chatbots. • We provide both cross-lingual and multilingual baselines and discuss their limitations to inspire future research. • We show the potential of multilingual systems to understand the mixed language dialogue context and generate coherent responses. 2 Related Work this paper, we focus on the latter, for which, in recent years, several tasks and datasets have been proposed to ground the conversation on knowledge (Dinan et al., 2019b; Gopalakrishnan et al., 2019; Fan et al., 2019; Reddy et al., 2019; Moon et al., 2019) such as Wiki-Articles, Reddit-Post, and CNN-Article. In this work, we focus on personalized dialogue agents where the dialogues are grounded on persona information. Li et al. (2016a) was the first to introduce a persona-grounded dialogue dataset for improving response consistency. Later on, Zhang et al. (2018) and Dinan et al. (2019a) introduced Persona-chat, a multi-turn conversational dataset, where two speakers are paired, and a persona description (4–5 sentences) is randomly assigned to each of them. By conditioning the response generation on the persona descriptions,"
2021.nlp4convai-1.10,N19-1380,0,0.0197831,"t al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et al., 2019). However, none of these datasets include the multilingual chit-chat task. the encoder and decoder of a sequence-to-sequence model (XNLG) to conduct cross-lingual generation tasks, namely, question generation and abstractive summarization. The latter is the closest to our task since it focuses on language generation; however cross-lingual dialogue generation has not yet been explored. 3 Data Collection The proposed XPersona dataset is an extension of the persona-chat dataset (Zhang et al., 2018; Dinan et al., 2019a). Specifically, we extend ConvAI2 (Dinan et al., 2019a) to six languages: Chine"
2021.nlp4convai-1.10,P18-1205,0,0.375821,"tter human-machine interaction. multilingual conversational benchmarks is essenExisting personalized dialogue agents rely tial, yet challenging since it is costly to perform on properly designed conversational datasets, human annotation of data in all languages. which are mostly monolingual (e.g., English), which greatly limits the usage of conversaA possible solution is to use translation systems tional agents in other languages. In this pabefore and after the model inference. This comes per, we propose a multi-lingual extension of with three major problems: 1) amplification of Persona-Chat (Zhang et al., 2018), namely translation errors since the current dialogue sysXPersona. Our dataset includes persona contems are far from perfect, especially with noisy versations in six different languages other than input; 2) the three-stage pipeline system is signifEnglish for evaluating multilingual personalicantly slower in terms of inference speed; and 3) ized agents. We experiment with both mulhigh translation costs since the current state-of-thetilingual and cross-lingual trained baselines, and evaluate them against monolingual and art models, especially in low resources languages, translation-pipeline mo"
2021.nlp4convai-1.10,N19-1170,0,0.0567634,"Missing"
2021.nlp4convai-1.10,N16-1156,0,0.0599374,"Missing"
2021.nlp4convai-1.10,P17-1061,0,0.0187814,"sonaChat training set and early stop based on the perplexity on target language validation set. 5.3 Results and Discussion 5.3.1 Quantitative Analysis Table 3 compares monolingual, multilingual, and cross-lingual models in terms of BLEU and perplexity in the human-translated test set. On both evaluation matrices, the causal decoder models outperform the encoder-decoder models. We observe that the encoder-decoder model tends to overlook dialogue context and generate digressive responses. (Generated samples are available in Appendix D) We hypothesize that this is because the one-tomany problem (Zhao et al., 2017) in open-domain conversation weakens the relation between encoder and decoder; thus the well pre-trained decoder (Bert) easily converges to a local optimum, and learns to ignore the dialogue context from the encoder and generate the response in an unconditional language model way. We leave the investigation of this problem to future work. On the other hand, M-CausalBert achieves a comparable or slightly better performance compared to CausalBert, which suggests that M-CausalBert leverages the data from other languages. As expected, we observe a significant gap between the cross-lingual model an"
2021.nlp4convai-1.10,D14-1187,0,0.0275078,"et of ConvAI2 is hidden, we split the original validation set into a new validation set and test sets. Then, we firstly automatically translate the training, validation, and test set using APIs (PapaGo for Korean, Google Translate for other languages). For each language, we hired native Cross-lingual Cross-lingual adaptation learns the inter-connections among languages and circum- speaker annotators with at least a bachelor degree and a fluent level of English and asked them to vents the requirement of extensive training data in revise the machine-translated dialogues and pertarget languages (Wisniewski et al., 2014; Zhang et al., 2016). Cross-lingual transfer learning meth- sona sentences in the validation set and test set according to original English dialogues. The main ods have been applied to multiple NLP tasks, such as named entity recognition (Ni et al., 2017), di- goal of human annotation is to ensure the revised alogue state tracking (Chen et al., 2018), part-of- conversations are coherent and fluent in target language despite the cultural discrepancy in different speech tagging (Wisniewski et al., 2014; Zhang languages. Therefore, annotators are not restricted et al., 2016; Kim et al., 2017), a"
2021.nlp4convai-1.10,2021.naacl-main.41,0,0.0238846,"cted dataset. We report the number of dialogues (#Dial.) and utterances (#Utt.) of the validation and test set in six languages. Edit distance per dialogue (Edit) and BLEU score are computed to show the difference between the human-annotated dataset and auto-translated dataset (Training set is reported in Appendix A). The BLEU score also reflects the quality of machine translated dialogues. lation (Johnson et al., 2017), and multilingual automatic speech recognition (Toshniwal et al., 2018). Multilingual deep contextualized model, such as Multilingual BERT (M-BERT) (Devlin et al., 2018), MT5 (Xue et al., 2021), MBART (Liu et al., 2020) have been commonly used to represent multiple languages and elevate the performance in many NLP applications, such as classification tasks (Pires et al., 2019), textual entailment, named entity recognition (K et al., 2020), and natural language understanding. Multilingual datasets have also been created for a number of NLP tasks, such as named entity recognition or linking (Pan et al., 2017; Aguilar et al., 2018), question answering (Liu et al., 2019; Lewis et al., 2019), dialogue state tracking (Mrkši´c et al., 2017), and natural language understanding (Schuster et"
2021.repl4nlp-1.8,D14-1187,0,0.0601937,"Missing"
2021.repl4nlp-1.8,D19-1077,0,0.326085,"c. 40 37.72 35.68 34.12 30 26.12 20 15.12 10 0 es → en fr → en it → en el → en Figure 1: Masked language model and cross-lingual sentence retrieval results before and after fine-tuning mBERT to the English part-of-speech tagging task. Introduction Recently, multilingual language models (Devlin et al., 2019; Conneau and Lample, 2019), pretrained on extensive monolingual or bilingual resources across numerous languages, have been shown to enjoy surprising cross-lingual adaptation abilities, and fine-tuning them to downstream crosslingual tasks has achieved promising results (Pires et al., 2019; Wu and Dredze, 2019). Taking this further, better pre-trained language models have been proposed to improve the cross-lingual performance, such as using larger amounts of pre-trained data with larger pre-trained models (Conneau et al., 2019; Liang et al., 2020), and utilizing more tasks in the pre-training stage (Huang et al., 2019). However, we observe that multilingual BERT (mBERT) (Devlin et al., 2019), a pre-trained language model, forgets the masked language model (MLM) task that has been learned and partially loses the cross-lingual ability (from a cross-lingual sentence retrieval (XSR)1 experiment) after b"
2021.sigdial-1.57,S19-2184,1,0.883467,"Missing"
2021.sigdial-1.57,W19-3655,1,0.702072,"dence analysis of various chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.1 1 Figure 1: Illustration of responses from different chatbots in a political conversation. Abortion law is a topic that often leads to divisive political debates. Introduction With the rise of end-to-end open-domain chatbots, it is increasingly important to ensure their responsible and safe behavior. Chatbot safety has been studied from various aspects including sexism and racism (Lee et al., 2019b; Liu et al., 2020; Xu et al., 2020). However, political prudence of chatbot is an under explored angle. Ensuring responsible behavior when discussing politics deserves more attention, because a hyper-partisan chatbot could be off-putting to the user. Recently, Xu et al. (2020) conducted comprehensive exploration of safety protocols for chatbots. However, political prudence remains an open discussion because a “topic avoidance” strategy – providing canned responses such as the “I’m sorry, I’m not sure what to say. Thank you for sharing and talking to me though”– is adopted for political topic"
2021.sigdial-1.57,2020.coling-main.390,0,0.0265091,"arious chatbots and discuss their behavior from multiple angles through our automatic metric and human evaluation metrics. The testsets and codebase are released to promote research in this area.1 1 Figure 1: Illustration of responses from different chatbots in a political conversation. Abortion law is a topic that often leads to divisive political debates. Introduction With the rise of end-to-end open-domain chatbots, it is increasingly important to ensure their responsible and safe behavior. Chatbot safety has been studied from various aspects including sexism and racism (Lee et al., 2019b; Liu et al., 2020; Xu et al., 2020). However, political prudence of chatbot is an under explored angle. Ensuring responsible behavior when discussing politics deserves more attention, because a hyper-partisan chatbot could be off-putting to the user. Recently, Xu et al. (2020) conducted comprehensive exploration of safety protocols for chatbots. However, political prudence remains an open discussion because a “topic avoidance” strategy – providing canned responses such as the “I’m sorry, I’m not sure what to say. Thank you for sharing and talking to me though”– is adopted for political topics and other sensiti"
2021.sigdial-1.57,2021.ccl-1.108,0,0.0951879,"Missing"
2021.sigdial-1.57,P19-1534,0,0.0230278,"atbot by Lin et al. (2020) fine-tuned on empathetic dialogue (a) Offensiveness vs. Hyper- (b) Slantedness vs. Hyperpartisan in Scenario B partisan in Scenario B Figure 2: Plots of offensiveness and slantedness scores against hyper-partisanship score in Scenario B. No correlation is shown in (a) for offensive vs. hyperpartisan, while in (b), higher slantedness score chatbots tend to have a higher hyper-partisanship score. The chatbot names are written using their abbreviations (DGPT: DialoGPT; EB: EmpatheticBot; PC: PersonaChat; AWiki: AdapterWiki; BB: Blenderbot; BB+Fact: Blenderbot+Fact). by Rashkin et al. (2019); and c) PersonaChat – a personalized chatbot backboned by DialoGPT and finetuned on the Persona dataset by Zhang et al. (2018). The KG chatbots includes d) AdapterWiki – a Wikipedia adapter of AdapterBot (Madotto et al., 2021) trained on Dinan et al. (2018); e) Blenderbot – a publicly available multi-skill chatbot (blenderbot400M-distill) (Roller et al., 2020); f) Blenderbot+Fact – our proposed naive yet safe and neutral chatbot which has a safety layer specialized for political discussion. This chatbot is back-boned by Blenderbot with a safety layer that detects whether the context is politi"
2021.sigdial-1.57,N18-1101,0,0.0177417,"Missing"
2021.sigdial-1.57,P18-1205,0,0.0307416,"enario B partisan in Scenario B Figure 2: Plots of offensiveness and slantedness scores against hyper-partisanship score in Scenario B. No correlation is shown in (a) for offensive vs. hyperpartisan, while in (b), higher slantedness score chatbots tend to have a higher hyper-partisanship score. The chatbot names are written using their abbreviations (DGPT: DialoGPT; EB: EmpatheticBot; PC: PersonaChat; AWiki: AdapterWiki; BB: Blenderbot; BB+Fact: Blenderbot+Fact). by Rashkin et al. (2019); and c) PersonaChat – a personalized chatbot backboned by DialoGPT and finetuned on the Persona dataset by Zhang et al. (2018). The KG chatbots includes d) AdapterWiki – a Wikipedia adapter of AdapterBot (Madotto et al., 2021) trained on Dinan et al. (2018); e) Blenderbot – a publicly available multi-skill chatbot (blenderbot400M-distill) (Roller et al., 2020); f) Blenderbot+Fact – our proposed naive yet safe and neutral chatbot which has a safety layer specialized for political discussion. This chatbot is back-boned by Blenderbot with a safety layer that detects whether the context is political or not using a dialogue context classifier by Xu et al. (2020). When the context is detected as “politics” class, Blenderbo"
D19-1012,S19-2005,0,0.0266457,"l., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task models on it. 3 Mixture of Empathetic Listeners The dialogue context is an alternating set of utter"
D19-1012,P18-5002,0,0.0254446,"Missing"
D19-1012,P19-1358,0,0.0329288,"erban et al., 2016; Vinyals and Le, 2015; Wolf et al., 2019). A recent trend is to produce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et"
D19-1012,D16-1230,0,0.213158,"Missing"
D19-1012,P19-1542,1,0.807543,"es by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017)"
D19-1012,P18-1136,1,0.896748,"Missing"
D19-1012,D18-1298,0,0.414457,"rsation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rashkin et al., 2018) about how an empathetic person would respond to the stressful situation the Speaker has been through. However, despite the importance of empathy and emotional understanding in human conversations, it is still very challenging to train a dialogue agent able to recognize and respond with the correct emotion. So far, to solve the problem of empathetic dialogue response generation, which is to understand the user emot"
D19-1012,S19-2184,1,0.802043,"follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empathetic dialogues and train multi-task mode"
D19-1012,D16-1147,0,0.0241154,"for each token. i where T RSDec refers to the i-th listener, including the shared one. Conceptually, we expect that 0 , to be the output from the shared listener, T RSDec a general representation which can help the model to capture the dialogue context. On the other hand, we expect that each empathetic listener learns how to respond to a particular emotion. To model this behavior, we assign different weights to each empathetic listener according to the user emotion distribution, while assigning a fixed weight of 1 to the shared listener. To elaborate, we construct a Key-Value Memory Network (Miller et al., 2016) and represent each memory slot as a vector pair (ki , Vi ), where ki ∈ Rdmodel denotes the key vector and Vi is from Equation 4. Then, the encoder informed query q is used to address the key vectors k by performing a dot product followed by a Softmax function. Thus, we have: tracker. We first flatten all dialogue turns in C, and map each token into its vectorial representation using the context embedding E C . Then the encoder encodes the context sequence into a context representation. We add a query token QRY at the beginning of each input sequence as in BERT (Devlin et al., 2018), to comput"
D19-1012,N16-1014,0,0.269283,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,P02-1040,0,0.104181,"Missing"
D19-1012,P16-1094,0,0.266273,"y less focus is emotional understanding and empathy (Rashkin et al., 2018; Dinan et al., 2019; Wolf et al., 2019). Intuitively, ordinary social conversations between two humans are often about their daily lives that revolve around happy or sad experiences. In such scenarios, people generally tend to respond in a way that acknowledges the feelings of their conversational partners. Introduction Neural network approaches for conversation models have shown to be successful in scalable training and generating fluent and relevant responses (Vinyals and Le, 2015). However, it has been pointed out by Li et al. (2016a,b,c); Wu et al. (2018b) that only using Maximum Likelihood Estimation as the objective function tends to lead to generic and repetitive responses like “I am sorry”. Furthermore, many others have shown that the incorporation of additional inductive bias leads to a more engaging chatbot, such as understanding commonsense (Dinan et al., 2018), or modeling consistent persona (Li et al., 2016b; Zhang et al., 2018a; Mazare et al., 2018a). Meanwhile, another important aspect of an engaging human conversation that received relaTable 1 shows an conversation from the empathetic-dialogues dataset (Rash"
D19-1012,D14-1162,0,0.0820006,"l MoEL vs TRS MoEL vs Multi-TRS 4 4.1 p(r1:t |C, r0:t−1 ) = softmax(O&gt; W ) (9) where O ∈ Rdmodel ×t is the output of meta listener and p(r1:t |C, r0:t−1 ) is a distribution over the vocabulary for the next tokens. We then use a standard maximum likelihood estimator (MLE) to optimize the response prediction: L2 = − log p (St |C) 4.2 Experiment Dataset Training We train our model using Adam optimizer (Kingma and Ba, 2014) and varied the learning rate during training following (Vaswani et al., 2017). The weight of both losses α and β are set to 1 for simplicity. We use pre-trained Glove vectors (Pennington et al., 2014) to initialize the word embedding and we share it across the encoder and the decoder. The rest of the parameters are randomly initialized. In the early training stage, emotion tracker randomly assign weights to the listeners, and may send noisy gradient flow back to the wrong listeners, which can make the model convergence harder. To stabilize the learning process, we replace the distribution p of the listeners with the or(10) Lastly, all the parameters are jointly trained endto-end to optimize the listener selection and response generation by minimizing the weightedsum of two losses: L = αL1"
D19-1012,N19-1126,0,0.045197,"Missing"
D19-1012,W18-5713,0,0.0125101,"019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative mo"
D19-1012,S19-2021,1,0.875815,"Missing"
D19-1012,W18-6243,1,0.763174,"istent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dialogues, Hu et al. (2017); Wang and Wan (2018); Zhou and Wang (2018) successfully introduce a framework of controlling the sentiment and emotion of the generated response, while (Zhou and Wang, 2018) also introduces a new Twitter conversation dataset and propose to distantly supervised the generative model with emojis. Meanwhile, (Lubis et al., 2018; Rashkin et al., 2018) also introduce new datasets for empa"
D19-1012,K18-1053,0,0.0147919,"roduce personalized responses by conditioning the generation on a persona profile to make the response more consistent through the dialogue (Li et al., 2016b). In particular, PersonaChat (Zhang et al., 2018b; Kulikov et al., 2018) dataset was created, and then extended in ConvAI 2 challenge (Dinan et al., 2019), to show that by adding persona information as input to the model, the produced responses elicit more consistent personas. Based on such, several follow-up work has been presented (Mazare et al., 2018b; Hancock et al., 2019; Joshi et al., 2017; Kulikov et al., 2018; Yavuz et al., 2018; Zemlyanskiy and Sha, 2018; Madotto et al., 2019). However, such personalized dialogue agents focus only on modeling a consistent persona and often neglect the feelings of their conversation partners. Another line of work combines retrieval and generation to promote the response diversity (Cai et al., 2018; Weston et al., 2018; Wu et al., 2018b). However, only fewer works focus on emotion (Winata et al., 2017, 2019; Xu et al., 2018; Fan et al., 2018a,c,b; Lee et al., 2019) and empathy in the context of dialogues systems (Bertero et al., 2016; Chatterjee et al., 2019a,b; Shin et al., 2019). For generating emotional dial"
D19-1012,P18-1205,0,0.0948744,"Missing"
D19-1012,P18-1104,0,0.0734934,"lly, our analysis demonstrates that not only MoEL effectively attends to the right listener, but also each listener learns how to properly react to its corresponding emotion, hence allowing a more interpretable generative process. of work. The first is a multi-task approach that jointly trains a model to predict the current emotional state of the user and generate an appropriate response based on the state (Lubis et al., 2018; Rashkin et al., 2018). Instead, the second line of work focuses on conditioning the response generation to a certain fixed emotion (Hu et al., 2017; Wang and Wan, 2018; Zhou and Wang, 2018; Zhou et al., 2018). Both cases have succeeded in generating empathetic and emotional responses, but have neglected some crucial points in empathetic dialogue response generation. 1) The first assumes that by understanding the emotion, the model implicitly learns how to respond appropriately. However, without any additional inductive bias, a single decoder learning to respond for all emotions will not only lose interpretability in the generation process, but will also promote more generic responses. 2) The second assumes that the emotion to condition the generation on is given as input, but w"
D19-1012,D16-1127,0,\N,Missing
D19-1012,D16-1110,1,\N,Missing
D19-1012,W19-5917,0,\N,Missing
D19-1129,D16-1250,0,0.0328322,"mains (weather, alarm, and reminder) and translate them using bilingual lexicons. We refine the embeddings by leveraging the framework proposed in Artetxe et al. (2017). Let X and Z be the aligned cross-lingual word embeddings between two languages. Xi∗ and Zj∗ are the embeddings for the ith source word and j th target word. We denote a binary dictionary matrix D: Dij = 1 if the ith source language word is aligned with the j th target language word and Dij = 0 otherwise. The goal is to find the optimal mapping matrix W∗ by minimizing: W∗ = arg min W X Dij ||Xi∗ W − Zj∗ ||2 . (1) i,j Following Artetxe et al. (2016), with orthogonal constraints, mean centering, and length normaliza1298 1 The embeddings are available in https://fasttext.cc tion, we can maximize the following instead: W∗ = arg max Tr(XWZT DT ). (2) W We iteratively optimize Equation 2 until distances between domain-related seed words are closer than a certain threshold after refinement. Figure 1 illustrates better alignment for domainrelated words after refinement. 3.2 Gaussian Noise Injection To cope with the noise in alignments, we inject Gaussian noise to English embeddings, so the trained model will be more robust to variance. This is"
D19-1129,P17-1042,0,0.119995,"the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student frame"
D19-1129,Q17-1010,0,0.0433367,"r, forecast, temperature, rain, hot, cold, remind, forget, alarm, cancel, tomorrow, which are related to the three dialogue domains (weather, alarm, and reminder). We translate them by leveraging bilingual dictionaries2 . The corresponding translations in Spanish and Thai are clima, pron´ostico, temperatura, lluvia, caliente, fr´ıo, recordar, olvidar, alarma, cancelar, ma˜nana and อากาศ, พยากรณ, อุณหภูมิ, ฝน, รอน, หนาว, 2 https://github.com/facebookresearch/MUSE 4.4 Evaluation We implement and evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will"
D19-1129,D18-1038,0,0.197164,"Cortana) as a virtual agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskorient"
D19-1129,D17-1169,0,0.0396427,"but not negligible differences across languages. Instead, using latent variables will allow us to model the distribution that captures the variance of semantically similar sentences across different languages. The whole training process is defined as follows: [h1 ...ht ...hT ] = BiLSTM(e∗ ), (3) T X exp(mt ) mt = ht wa , at = PT , v= at ht , exp(m ) j j=1 t=1 (4)     S I µt µ = WrS ht , = WrI v, log σtS )2 log σ I )2 (5) ztS ∼ qtS (z|ht ), z I ∼ q I (z|v), (6) pSt (st |ztS ) = Softmax(WgS ztS ), (7) I I p (I|z ) = Softmax(WgI z I ), (8) where attention vector (v) is obtained by following Felbo et al. (2017) and wa is the weight {S,I} matrix for the attention layer, W{r,g} are trainable parameters, superscripts S and I refer to slot prediction and intent detection respectively, subscript “r” refers to “recognition” for obtaining the LI = Ez I [log pI (I|z I )], LSt = EztS [log pSt (st |ztS )], LS = T X LSt , (9) (10) (11) t=1 hence, the final objective function to minimize is, L = LS + LI . (12) The model prediction is not deterministic since the latent variables ztS and z I are sampled from the Gaussian distributions. Therefore, in the inference time, we use the true mean µSt and µI to replace z"
D19-1129,N18-1032,0,0.0164652,"d achieve state-of-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to"
D19-1129,D18-1330,0,0.0233275,"Missing"
D19-1129,S19-2184,1,0.762048,"f-the-art results in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lin"
D19-1129,W19-5327,1,0.850579,"in zero-shot adaptation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-represen"
D19-1129,N19-1380,0,0.291983,"al agent to tend to the needs of the users. However, these agents have mostly been trained with the monolingual dataset that is often expensive to build or acquire. In order to cope with the scarcity of low-resource language dialogue data, we are motivated to look into cross-lingual dialogue systems which can adapt with very little or no training data in the target language. This task of zero-shot adaptation of dialogue systems to different languages is relatively new and has not been explored thoroughly enough yet. The main approach of previous work (Upadhyay et al., 2018; Chen et al., 2018; Schuster et al., 2019) in this task is using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen"
D19-1129,W19-4320,1,0.821524,"of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-switching name entity recognition by combining multiple monolingual word embeddings. Chen et al. (2018) proposed a teacher-student framework leveraging bilingual data for crosslingual transfer learning in dialogue state trackOur model consists of a refined cross-lingual embedding layer followed by a BiLSTM (Hochreiter and Schmidhuber, 1997) which parameterizes the Latent Variable Model, as illustrated in Figure 2. We jointly train our model to predict both slots and user intents. We denote w = [w1 , . . . , wT ] as th"
D19-1129,W18-6243,1,0.833712,"ation of English to Spanish and Thai for the natural language understanding task (i.e., the intent prediction and slot filling) on the dataset proposed by Schuster et al. (2019), even though we use much less external resources (i.e., ∼10 seed word-pairs) while others utilize a large amount of bilingual corpus. We further visualize the learned latent variables to confirm that same-meaning words and sentences have similar distributions. 2 Methodology wt Related Work Cross-lingual transfer learning which acts as one of the low-resource topics (Gu et al., 2018; Lee et al., 2019; Liu et al., 2019; Xu et al., 2018) has attracted more and more people recently, followed by the rapid development of cross-lingual word embeddings. Artetxe et al. (2017) proposed a self-learning framework and utilized a small size of word dictionary to learn the mapping between source and target word embeddings. Conneau et al. (2018) leveraged adversarial training to learn a linear mapping from a source to a target space without using parallel data. Joulin et al. (2018) utilized Relaxed CSLS loss to optimize this mapping problem. Winata et al. (2019) introduced a method to leverage cross-lingual meta-representations for code-s"
D19-1129,W18-3023,0,0.0445921,"nd evaluate the following models: Zero-shot SLU Upadhyay et al. (2018) used cross-lingual embeddings (Bojanowski et al., 2017) to do zero-shot transfer learning. Conditional Random Fields (CRF) We reproduce the baseline model in Schuster et al. (2019), and also add embedding noise, cross-lingual refinement, and delexicalization. Latent Variable Model (LVM) - Ours We replace the CRF module with latent variables and also apply it to intent prediction. Besides, we directly compare with the baseline models illustrated in Schuster et al. (2019): Multi. CoVe w/ auto They combined Multilingual CoVe (Yu et al., 2018) with an auto-encoder objective and then used the trained encoder with the CRF model. 1300 noche เ น What will the weather be like this evening weather อากาศ clima Cancel tuesday alarm clock evening English Spanish Thai English Spanish Thai Figure 3: Visualization of latent variables on words (left) and sentences (right). Left: We choose “weather-climaอากาศ” and “evening-noche- เยน” from parallel sentences. English: “What will the weather be like this evening”, Spanish: “C´omo ser´a el clima esta noche”, Thai: “ตอน เยน นี อากาศ จะ เปน อยางไร”. Right: We choose two English sentences and sh"
D19-1129,W18-5001,0,0.039756,"gual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these methods allows us to build a t"
D19-1129,P18-1101,0,0.0456593,"Missing"
D19-1129,P17-1061,0,0.0291931,"s using aligned cross-lingual word embeddings between source and target languages. However, this method suffers from imperfect alignments between the source and target language embeddings. This can be attributed not only to the noise in aligning two different embeddings, but also to the inherent discrepancies in different languages such as Thai and English which come from entirely different roots. To address such variance in the alignment, we turn to probabilistic modeling with latent variables as it has been successfully used in several recent taskoriented dialogue systems (Wen et al., 2017; Zhao et al., 2017, 2018; Zhao and Eskenazi, 2018; Le et al., 2018). However, we notice that naively using latent variables does not help the model improve much in slot filling and intent prediction. We hypothesize that the variance of the cross-lingual word embeddings is too large for the model to learn any meaningful latent variables. Hence, we propose to first refine the cross-lingual embeddings with ∼10 seed word-pairs related to the dialogue domains. We then add Gaussian noise (Zheng et al., 2016) to further compensate the imperfect alignment of cross-lingual embeddings. As a result, a combination of these"
D19-1303,N18-1150,0,0.0156643,"t applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summarie"
D19-1303,D17-1169,0,0.0359551,"ansfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As"
D19-1303,P16-1154,0,0.023977,"that our 6 https://www.figure-eight.com/ Table 2: Generated Chinese headlines from different models. Our model (Pointer-Gen+ARL-SEN) sensationalized the headline with the phrase “In Serious Trouble!”. model produces relevant headlines and we leave the sensationalism for human evaluation. Note that we only compare our models to commonly used strong summarization baselines, to validate that our implementation achieves comparable performance to existing work. In our implementation, Pointer-Gen achieves a 34.51 RG-1 score, 22.21 RG-2 score, and 31.68 RG-L score, which is similar to the results of Gu et al. (2016). PointerGen+ARL-SEN, although optimized for the sensationalism reward, achieves similar performance to our Pointer-Gen baseline, which means that Pointer-Gen+ARL-SEN still keeps its summarization ability. An example of headlines generated from different models in Table 2 shows that Pointer-Gen and Pointer-Gen+RL-ROUGE learns to summarize the main point of the article: “The Nikon D600 camera is reported to have black spots when taking photos”. Pointer-Gen+RL-SEN 3070 Model Pointer-Gen Pointer-Gen-Pos Pointer-Gen+Same-FT Pointer-Gen+Pos-FT Pointer-Gen+RL-ROUGE Pointer-Gen+RL-SEN Pointer-Gen+ARL"
D19-1303,S18-1039,1,0.900112,"Missing"
D19-1303,D15-1229,0,0.198555,"ARL loss function becomes: LARL-SEN = (1 − αsen (y ∗ )) LRL + αsen (y ∗ ) LMLE (15) If αsen (y ∗ ) is high, meaning the training headline is sensational, our loss function encourages our model to imitate the sample more using the MLE training. If αsen (y ∗ ) is low, our loss function replies on RL training to improve the sensationalism. Note that the weight αsen (y ∗ ) is different from our sensationalism reward αsen (y s ) and we call the loss function Auto-tuned Reinforcement Learning, because the ratio between MLE and RL are well “tuned” towards different samples. 3.3 Dataset We use LCSTS (Hu et al., 2015) as our dataset to train the summarization model. The dataset is collected from the Chinese microblogging website Sina Weibo. It contains over 2 million Chinese short texts with corresponding headlines given by the author of each text. The dataset is split into 2,400,591 samples for training, 10,666 samples for validation and 725 samples for testing. We tokenize each sentence with Jieba 4 and a vocabulary size of 50000 is saved. 3068 4 https://github.com/fxsjy/jieba 3.5 Figure 2: The probability density function (pdf) of predicted sensationalism score in log scale. Low sensationalism score has"
D19-1303,N18-2102,0,0.0208419,"attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu"
D19-1303,D18-1207,0,0.0344404,"Missing"
D19-1303,S19-2184,1,0.872022,"Missing"
D19-1303,D15-1166,0,0.0385727,"Pointer-Gen Headline Generator We choose Pointer Generator (Pointer-Gen) (See et al., 2017), a widely used summarization model, as our headline generator for its ability to copy words from the input article. It takes a news article as input and generates a headline. Firstly, the tokens of each article, {x1 , x2 , x3 , · · · , xM }, are fed into the encoder one-by-one and the encoder generates a sequence of hidden states hi . For each decoding step t, the decoder receives the embedding for each token of a headline yt as input and updates its hidden states st . An attention mechanism following Luong et al. (2015) is used: Figure 1: The loss function of Auto-tuned Reinforcement Learning is a weighted sum of LRL and LMLE , where the weight is decided by our sensationalism scorer. eti = v T tanh(Wh hi + Ws st + battn ) t t a = softmax(e ) X h∗t = ati hi (2) (3) i 0.50 averaged F1 score. This confirms that the predicted sensationalism score can partially capture the sensationalism of headlines. On the other hand, a more natural choice is to take headlines with few comments as negative examples. Thus, we train another baseline classifier on a crawled balanced sensationalism corpus of 84k headlines where th"
D19-1303,K16-1028,0,0.0832857,"Missing"
D19-1303,Q18-1027,0,0.0194518,"se extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et"
D19-1303,P18-1080,0,0.170636,"eadline, we collect a sensationalism dataset and then train a sensationalism scorer. For the sensationalism dataset collection, we choose headlines with many comments from popular online websites as positive samples. For the negative samples, we propose to use the generated headlines from a sentence summarization model. Intuitively, the summarization model, which is trained to preserve the semantic meaning, will lose the sensationalization ability and thus the generated negative samples will be less sensational than the original one, similar to the obfuscation of style after back-translation (Prabhumoye et al., 2018). For example, an original headline like “一 趟 挣10万？铁总增开申通、顺丰专列” (One trip to earn 100 thousand? China Railway opens new 3 https://github.com/HLTCHKUST/ sensational_headline Shentong and Shunfeng special lines) will become “中铁总将增开京广两列快递专列” (China Railway opens two special lines for express) from the baseline model, which loses the sensational phrases of “一 趟 挣10万 ？” (One trip to earn 100 thousand?) . We then train the sensationalism scorer by classifying sensational and nonsensational headlines using a one-layer CNN with a binary cross entropy loss Lsen . Firstly, 1-D convolution is used to ext"
D19-1303,P18-2025,0,0.0678051,"Missing"
D19-1303,D15-1044,0,0.0549155,"will hurt the performance, both in sensationalism and fluency. After manually checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summar"
D19-1303,P17-1099,0,0.0703767,"ur work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al."
D19-1303,P17-1108,0,0.0603074,"Missing"
D19-1303,P18-1090,0,0.108045,"Missing"
D19-1303,W18-6243,1,0.839564,"ion task and achieved better performance. Niu and Bansal (2018) tackles the problem of polite generation with politeness reward. Our work is different in that we propose a novel function to balance RL and MLE. Our task is also related to text style transfer. Implicit methods (Shen et al., 2017b; Fu et al., 2018; Prabhumoye et al., 2018) transfer the styles by separating sentence representations into content and style, for example using backtranslation(Prabhumoye et al., 2018). However, these methods cannot guarantee the content consistency between the original sentence and transferred output (Xu et al., 2018a). Explicit methods (Zhang et al., 2018b; Xu et al., 2018a) transfer the style by directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word"
D19-1303,D18-1088,0,0.0168206,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,D18-1138,0,0.025251,"arse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achieved better performan"
D19-1303,P18-1061,0,0.0231592,"et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2018b; Zhang et al., 2018a) use extractive methods to directly select sentences from articles. However, none of these work considered the sensationalism of generated outputs. RL is also gaining popularity as it can directly optimize non-differentiable metrics (Pasunuru and Bansal, 2018; Venkatraman et al., 2015; Xu and Fung, 2019). Paulus et al. (2018) proposed an intra-decoder model and combined RL and MLE to deal with summaries with bad qualities. RL has also been explored with generative adversarial networks (GANs) (Yu et al., 2017). Liu et al. (2018) applied GANs on summarization task and achi"
D19-1303,P17-1101,0,0.018251,"ally checking the outputs, we observe that our model is able to generate sensational headlines using diverse sensationalization strategies. These strategies include, but are not limited to, creating a curiosity gap, asking questions, highlighting numbers, being emotional and emphasizing the user. Examples can be found in Table 4. 5 Related Work Our work is related to summarization tasks. An encoder-decoder model was first applied to two sentence-level abstractive summarization tasks on the DUC-2004 and Gigaword datasets (Rush et al., 2015). This model was later extended by selective encoding (Zhou et al., 2017), a coarse to fine approach (Tan et al., 2017b), minimum risk training (Shen et al., 2017a), and topic-aware models (Wang et al., 2018). As long summaries were recognized as important, the CNN/Daily Mail dataset was used in Nallapati et al. (2016). Graph-based attention (Tan et al., 2017a), pointer-generator with coverage loss (See et al., 2017) are further developed to improve the generated summaries. Celikyilmaz et al. (2018) proposed deep communicating agents for representing a long document 3072 for abstractive summarization. In addition, many papers (Nallapati et al., 2017; Zhou et al., 2"
D19-1303,S19-2021,1,0.844097,"directly identifying style related keywords and modifying them. However, sensationalism is not always restricted to keywords, but the full sentence. By leveraging small human labeled English dataset, clickbait detection has been well investigated (Chakraborty et al., 2016; Shu et al., 2018; Potthast et al., 2018). However, these human labeled dataset are not available for other languages, such as Chinese. Modeling sensationalism is also related to modeling emotion. Emotion has been well investigated in both word level(Tang et al., 2016; Xu et al., 2018b) and sentence level(Felbo et al., 2017; Winata et al., 2019, 2018; Park et al., 2018; Lee et al., 2019). It has also been considered an important factor in engaging interactive systems(Lin et al., 2019b; Winata et al., 2017; Zhou et al., 2018a). Although we observe that sensational headlines contain emotion, it is still not clear which emotion and how emotions will influence the sensationalism. 6 Conclusion and Future Work In this paper, we propose a model that generates sensational headlines without labeled data using Reinforcement Learning. Firstly, we propose a distant supervision strategy to train the sensationalism scorer. As a result, we achieve"
K19-1026,N13-1073,0,0.139357,"constraint. We propose to generate synthetic code-switching data by the following steps: chinese Permissible switching 这个 其实 是 belonged to 简体 中⽂ 这个 其实 是 belonged to simpliﬁed chinese Impermissible switching this belonged to simpliﬁed chinese 是 其实 Figure 2: Example of equivalence constraint (Li and Fung, 2012). Solid lines show the alignment between the matrix language (top) and the embedded language (bottom). The dotted lines denote impermissible switching. We use a beam search to select the N -best codeswitching sentences. 2.2 1. Align the L1 sentences Q and L2 sentences E using fast_align2 (Dyer et al., 2013). We use the mapping from the L1 sentences to the L2 sentences. Equivalence Constraint Studies on the EC (Poplack, 1980, 2013) show that code-switching only occurs where it does not violate the syntactic rules of either language. An example of a English-Mandarin mixed-language sentence generation is shown in Figure 2, where EC theory does not allow the word “其实&quot; to come after “是&quot; in Chinese, or the word “is&quot; to come after “actually&quot;. Pratapa et al. (2018) apply the EC in English-Spanish language modeling with a strong assumption. We are working with English and Mandarin, which have distinctive"
K19-1026,D18-1346,0,0.404003,"The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using pointergenerator networks (Pointer-Gen) (See et al., 2017) to learn code-switching constraints from a limited source of code-switching data and leverage their translations in both languages. IntuTraining"
K19-1026,C12-1102,1,0.941147,"they have word alignments with an inverted order. Building a language model (LM) and an automatic speech recognition (ASR) system that can handle intra-sentential code-switching is known to be a difficult research challenge. The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data g"
K19-1026,D15-1166,0,0.0548825,"oder. We use a bidirectional long short-term memory (LSTM), which, produces hidden state ht in each step t. The decoder is a unidirectional LSTM receiving the word embedding of the previous word. For each decoding step, a generation probability pgen ∈ [0,1] is calculated, which weights the probability of generating words from the vocabulary, and copying words from the source text. pgen is a soft gating probability to decide whether to generate the next token from the decoder or to copy the word from the input instead. The attention distribution at is a standard attention with general scoring (Luong et al., 2015). It considers all encoder hidden states to derive the context vector. The vocabulary distribution Pvoc (w) is calculated by concatenating the decoder state st and the context vector h∗t : • We propose a language-agnostic method to generate code-switching sentences using a pointer-generator network (See et al., 2017) that learns when to switch and copy words from parallel sentences, without using external word alignments or constituency parsers. By using the generated data in the language model training, we achieve the state-of-theart performance in perplexity and also improve the end-to-end A"
K19-1026,P14-5010,0,0.00251229,"tural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of the results, we release the translated data).4 Then, we use them to generate 270,531 new pieces of code-switching"
K19-1026,P13-2037,0,0.402545,"gnate words and to avoid confusion. Also, English adverbs such as “then&quot; and “so&quot; are phrase or sentence connectors between two language phrases for intra-sentential and intersentential code-switching. On the other hand, Chinese transitional words such as the measure word “个&quot; or associative word “的&quot; are frequently used as inter-lingual word associations. 6 extended recurrent neural networks (RNNs) by adding POS information to the input layer and a factorized output layer with a language identifier. The factorized RNNs were also combined with an n-gram backoff model using linear interpolation (Adel et al., 2013b), and syntactic and semantic features were added to them (Adel et al., 2015). Baheti et al. (2017) adapted an effective curriculum learning by training a network with monolingual corpora of two languages, and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et"
K19-1026,W17-7509,0,0.117319,"entence connectors between two language phrases for intra-sentential and intersentential code-switching. On the other hand, Chinese transitional words such as the measure word “个&quot; or associative word “的&quot; are frequently used as inter-lingual word associations. 6 extended recurrent neural networks (RNNs) by adding POS information to the input layer and a factorized output layer with a language identifier. The factorized RNNs were also combined with an n-gram backoff model using linear interpolation (Adel et al., 2013b), and syntactic and semantic features were added to them (Adel et al., 2015). Baheti et al. (2017) adapted an effective curriculum learning by training a network with monolingual corpora of two languages, and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et al., 2017) for generating new mixed-language sequences. Winata et al. (2018b) leveraged character r"
K19-1026,P18-1143,0,0.344093,"nments with an inverted order. Building a language model (LM) and an automatic speech recognition (ASR) system that can handle intra-sentential code-switching is known to be a difficult research challenge. The main reason lies in the unpredictability of code-switching points in an utterance and data scarcity. Creating a large-scale code-switching dataset is also very expensive. Therefore, code-switching data generation methods to augment existing datasets are a useful workaround. Existing methods that apply equivalence constraint theory to generate code-switching sentences (Li and Fung, 2012; Pratapa et al., 2018) may suffer performance issues as they receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using po"
K19-1026,E17-2025,0,0.0971241,"Missing"
K19-1026,P17-1099,0,0.314318,"receive erroneous results from the word aligner and the partof-speech (POS) tagger. Thus, this approach is not reliable and effective. Recently, Garg et al. (2018) proposed a SeqGAN-based model to generate code-switching sentences. Indeed, the model learns how to generate new synthetic sentences. However, the distribution of the generated sentences is very different from real code-switching data, which leads to underperforming results. To overcome the challenges in the existing works, we introduce a neural-based codeswitching data generator model using pointergenerator networks (Pointer-Gen) (See et al., 2017) to learn code-switching constraints from a limited source of code-switching data and leverage their translations in both languages. IntuTraining code-switched language models is difficult due to lack of data and complexity in the grammatical structure. Linguistic constraint theories have been used for decades to generate artificial code-switching sentences to cope with this issue. However, this require external word alignments or constituency parsers that create erroneous results on distant languages. We propose a sequence-to-sequence model using a copy mechanism to generate code-switching da"
K19-1026,W19-4320,1,0.822278,"and subsequently trained on code-switched data. A further investigation of EC and curriculum learning showed an improvement in English-Spanish language modeling (Pratapa et al., 2018), and a multitask learning approach was introduced to train the syntax representation of languages by constraining the language generator (Winata et al., 2018a). Garg et al. (2018) proposed to use SeqGAN (Yu et al., 2017) for generating new mixed-language sequences. Winata et al. (2018b) leveraged character representations to address out-of-vocabulary words in the code-switching named entity recognition. Finally, Winata et al. (2019) proposed a method to represent code-switching sentence using language-agnostic meta-representations. Related Work Code-switching language modeling research has been focused on building a model that handles mixed-language sentences and on generating synthetic data to solve the data scarcity issue. The first statistical approach using a linguistic theory was introduced by Li and Fung (2012), who adapted the EC on monolingual sentence pairs during the decoding step of an ASR system. Ying and Fung (2014) implemented a functional-head constraint lattice parser with a weighted finite-state transduc"
K19-1026,W18-3207,1,0.929414,"(Y ) as the probability of the sentence. We incorporate language model probability plm (Y ) to select more natural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of th"
K19-1026,W18-3214,1,0.896391,"(Y ) as the probability of the sentence. We incorporate language model probability plm (Y ) to select more natural code-switching sequences from generation candidates. A word count is added to avoid generating very short sentences. P (Y ) is calculated as follows: p P (Y ) = αPtrans (Y |X) + βplm (Y ) + γ wc(Y ) (3) where α is the parameter to control the decoding probability from the probability of characters from the decoder Ptrans (Y |X), β is the parameter to control the language model probability plm (Y ), and γ is the parameter to control the effect of the word count wc(Y ). 4 4.1 from Winata et al. (2018a). The details are depicted in Table 1. We tokenize words using the Stanford NLP toolkit (Manning et al., 2014). For monolingual speech datasets, we use HKUST (Liu et al., 2006), comprising spontaneous Mandarin Chinese telephone speech recordings, and Common Voice, an open-accented English dataset collected by Mozilla.3 We split Chinese words into characters to avoid word boundary issues, similarly to Garg et al. (2018). We generate L1 sentences and L2 sentences by translating the training set of SEAME Phase II into English and Chinese using the Google NMT system (To enable reproduction of th"
K19-1026,D14-1098,1,0.846911,"s to address out-of-vocabulary words in the code-switching named entity recognition. Finally, Winata et al. (2019) proposed a method to represent code-switching sentence using language-agnostic meta-representations. Related Work Code-switching language modeling research has been focused on building a model that handles mixed-language sentences and on generating synthetic data to solve the data scarcity issue. The first statistical approach using a linguistic theory was introduced by Li and Fung (2012), who adapted the EC on monolingual sentence pairs during the decoding step of an ASR system. Ying and Fung (2014) implemented a functional-head constraint lattice parser with a weighted finite-state transducer to reduce the search space on a codeswitching ASR system. Then, Adel et al. (2013a) 7 Conclusion We propose a novel method for generating synthetic code-switching sentences using Pointer-Gen by learning how to copy words from parallel cor278 pora. Our model can learn code-switching points by attending to input words and aligning the parallel words, without requiring any word alignments or constituency parsers. More importantly, it can be effectively used for languages that are syntactically differe"
P18-1136,P16-1154,0,0.0453427,"itectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). However, Mem2Seq differs from these models as such: it uses multi1475"
P18-1136,P16-1014,0,0.463055,"n Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffer from two main problems: 1) They struggle to effectively incorporate external KB information into the RNN hidden states (Sukhbaatar et al., 2015), 1468 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long"
P18-1136,P17-1019,0,0.0132652,"used in task-oriented dialog systems (Zhao et al., 2017). These architectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). H"
P18-1136,W14-4337,0,0.106322,"r turns Avg. Sys turns Avg. KB results Avg. Sys words Max. Sys words Pointer Ratio Vocabulary Train dialogs Val dialogs Test dialogs 1 4 6 0 6.3 9 .23 2 6.5 9.5 0 6.2 9 .53 3 4 5 6.4 3.5 12.9 9.9 3.5 18.4 24 7 23.7 7.2 5.7 6.5 9 8 9 .46 .19 .60 3747 1000 1000 1000 + 1000 OOV DSTC2 6.7 9.3 39.5 10.2 29 .46 1229 1618 500 1117 In-Car 2.6 2.6 66.1 8.6 87 .42 1601 2425 302 304 3.2 Table 2: Dataset statistics for 3 different datasets. 3 3.1 Experimental Setup Dataset We use three public multi-turn task-oriented dialog datasets to evaluate our model: the bAbI dialog (Bordes and Weston, 2017), DSTC2 (Henderson et al., 2014) and In-Car Assistant (Eric et al., 2017). The train/validation/test sets of these three datasets are split in advance by the providers. The dataset statistics are reported in Table 2. The bAbI dialog includes five end-to-end dialog learning tasks in the restaurant domain, which are simulated dialog data. Task 1 to 4 are about API calls, refining API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. There are two test sets for each task: one follows the same distribution as the training set and the other has out-of-vocabulary (OO"
P18-1136,N16-1014,0,0.0364496,"g are correct, which can be considered as the task-completion rate. Note that Bordes and Weston (2017) tests their model by selecting the system response from predefined response candidates, that is, their system solves a multi-class classification task. Since Mem2Seq generates each token individually, evaluating with this metric is much more challenging for our model. BLEU: It is a measure commonly used for machine translation systems (Papineni et al., 2002), but it has also been used in evaluating dialog systems (Eric and Manning, 2017; Zhao et al., 2017) and chat-bots (Ritter et al., 2011; Li et al., 2016). Moreover, BLEU score is a relevant measure in task-oriented dialog as there is not a large variance between the generated answers, unlike open domain generation (Liu et al., 2016). Hence, we include BLEU score in our evaluation (i.e. using Moses multi-bleu.perl script). Entity F1: We micro-average over the entire set of system responses and compare the entities in plain text. The entities in each gold system response are selected by a predefined entity list. This metric evaluates the ability to generate relevant entities from the provided KBs and to capture the semantics of the dialog (Eric"
P18-1136,D16-1230,0,0.0609271,"Missing"
P18-1136,E17-1001,0,0.239676,"t F1 scores reported in Eric et al. (2017) uses the entities in their canonicalized forms, which are not calculated based on real entity value. Since the datasets are not designed for slot-tracking, we report entity F1 rather than the slot-tracking accuracy as in (Wen et al., 2017; Zhao et al., 2017). 4 BLEU 13.5 6.6 13.2 8.4 9.3 8.3 11.6 12.6 9.9 Experimental Results We mainly compare Mem2Seq with hop 1,3,6 with several existing models: query-reduction networks (QRN, Seo et al. (2017)), end-toend memory networks (MemNN, Sukhbaatar et al. (2015)), and gated end-to-end memory networks (GMemNN, Liu and Perez (2017)). We also implemented the following baseline models: standard sequence-to-sequence (Seq2Seq) models with and without attention (Luong et al., 2015), and pointer to unknown (Ptr-Unk, Gulcehre et al. (2016)). Note that the results we listed in Table 3 and Table 4 for QRN are different from the original paper, because based on their released code, 3 we discovered that the per-response accuracy was not correctly computed. bAbI Dialog: In Table 3, we follow Bordes 3 We simply modified the evaluation part and reported the results. (https://github.com/uwnlp/qrn) and Weston (2017) to compare the perf"
P18-1136,D15-1166,0,0.117279,"ficial state labels. End-to-End Memory Networks (Bordes and Weston, 2017; Sukhbaatar et al., 2015), and its variants (Liu and Perez, 2017; Wu et al., 2017, 2018) have also shown good results in such tasks. In each of these architectures, the output is produced by generating a sequence of tokens, or by selecting a set of predefined utterances. Sequence-to-sequence (Seq2Seq) models have also been used in task-oriented dialog systems (Zhao et al., 2017). These architectures have better language modeling ability, but they do not work well in KB retrieval. Even with sophisticated attention models (Luong et al., 2015; Bahdanau et al., 2015), Seq2Seq fails to map the correct entities to the generated input. To alleviate this problem, copy augmented Seq2Seq models Eric and Manning (2017), were used. These models outperform utterance selection methods by copying relevant information directly from the KBs. Copy mechanisms has also been used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work"
P18-1136,D16-1147,0,0.0263522,"o control a soft gate in a language modeling task. With this method, the model does not need to learn a gating function separately as in Gulcehre et al. (2016), and is not constrained by a soft gate function as in See et al. (2017). 2.3 Memory Content We store word-level content X in the memory module. Similar to Bordes and Weston (2017), we add temporal information and speaker information in each token of X to capture the sequential dependencies. For example, “hello t1 $u” means “hello” at time step 1 spoken by a user. On the other hand, to store B, the KB information, we follow the works of Miller et al. (2016); Eric et al. (2017) that use a (subject, relation, object) representation. For example, we represent the information of The Westin in Table 1: (The Westin, Distance, 5 miles). Thus, we sum word embeddings of the subject, relation, and object to obtain each KB memory representation. During decoding stage, the object part is used as the generated word for Pptr . For instance, when the KB tuple (The Westin, Distance, 5 miles) is pointed, our model copies “5 miles” as an output word. Notice that only a specific section of the KB, relevant to a specific dialog, is loaded into the memory. 1470 Task"
P18-1136,P02-1040,0,0.10299,"uracy: A generative response is correct only if it is exactly the same as the gold response. A dialog is correct only if every generated responses of the dialog are correct, which can be considered as the task-completion rate. Note that Bordes and Weston (2017) tests their model by selecting the system response from predefined response candidates, that is, their system solves a multi-class classification task. Since Mem2Seq generates each token individually, evaluating with this metric is much more challenging for our model. BLEU: It is a measure commonly used for machine translation systems (Papineni et al., 2002), but it has also been used in evaluating dialog systems (Eric and Manning, 2017; Zhao et al., 2017) and chat-bots (Ritter et al., 2011; Li et al., 2016). Moreover, BLEU score is a relevant measure in task-oriented dialog as there is not a large variance between the generated answers, unlike open domain generation (Liu et al., 2016). Hence, we include BLEU score in our evaluation (i.e. using Moses multi-bleu.perl script). Entity F1: We micro-average over the entire set of system responses and compare the entities in plain text. The entities in each gold system response are selected by a predef"
P18-1136,D11-1054,0,0.0993531,"Missing"
P18-1136,P17-1099,0,0.163623,"the Pptr is trained to produce the sentinel token $, as shown in Equation 1. Once the sentinel is chosen, our model generates the token from Pvocab , otherwise, it takes the memory content using the Pptr distribution. Basically, the sentinel token is used as a hard gate to control which distribution to use at each time step. A similar approach has been used in (Merity et al., 2017) to control a soft gate in a language modeling task. With this method, the model does not need to learn a gating function separately as in Gulcehre et al. (2016), and is not constrained by a soft gate function as in See et al. (2017). 2.3 Memory Content We store word-level content X in the memory module. Similar to Bordes and Weston (2017), we add temporal information and speaker information in each token of X to capture the sequential dependencies. For example, “hello t1 $u” means “hello” at time step 1 spoken by a user. On the other hand, to store B, the KB information, we follow the works of Miller et al. (2016); Eric et al. (2017) that use a (subject, relation, object) representation. For example, we represent the information of The Westin in Table 1: (The Westin, Distance, 5 miles). Thus, we sum word embeddings of th"
P18-1136,D16-1027,0,0.0207541,"used in question answering tasks (Dehghani et al., 2017; He et al., 2017), neural machine translation (Gulcehre et al., 2016; Gu et al., 2016), language modeling (Merity et al., 2017), and summarization (See et al., 2017). Less related to dialog systems, but related to our work, are the memory based decoders and the nonrecurrent generative models: 1) Mem2Seq query generation phase used to access our memories can be seen as the memory controller used in Memory Augmented Neural Networks (MANN) (Graves et al., 2014, 2016). Similarly, memory encoders have been used in neural machine translation (Wang et al., 2016), and meta-learning application (Kaiser et al., 2017). However, Mem2Seq differs from these models as such: it uses multi1475 hop attention in combination with copy mechanism, whereas other models use a single matrix representation. 2) non-recurrent generative models (Vaswani et al., 2017), which only rely on selfattention mechanism, are related to the multi-hop attention mechanism used in MemNN. 7 Conclusion In this work, we present an end-to-end trainable Memory-to-Sequence model for task-oriented dialog systems. Mem2Seq combines the multi-hop attention mechanism in end-to-end memory networks"
P18-1136,E17-1042,0,0.412796,"away and PAD is no traffic Palo Alto Cafe is 4 miles away at 436 Alger Drive Palo Alto is located at 436 Alger Dr. Table 1: Example of generated responses for the In-Car Assistant on the navigation domain. Introduction ∗ DRIVER Seq2Seq +Attn Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffe"
P18-1136,P17-1062,0,0.0548383,"ntities and recurrent patterns. Thus, we believe BLEU score still can be considered as a relevant measure. In future works, several methods could be applied (e.g. Reinforcement Learning (Ranzato et al., 2016), Beam Search (Wiseman and Rush, 2016)) to improve both responses relevance and entity F1 score. However, we preferred to keep our model as simple as possible in order to show that it works well even without advanced training methods. 6 Related Works End-to-end task-oriented dialog systems train a single model directly on text transcripts of dialogs (Wen et al., 2017; Serban et al., 2016; Williams et al., 2017; Zhao et al., 2017; Seo et al., 2017; Serban et al., 2017). Here, RNNs play an important role due to their ability to create a latent representation, avoiding the need for artificial state labels. End-to-End Memory Networks (Bordes and Weston, 2017; Sukhbaatar et al., 2015), and its variants (Liu and Perez, 2017; Wu et al., 2017, 2018) have also shown good results in such tasks. In each of these architectures, the output is produced by generating a sequence of tokens, or by selecting a set of predefined utterances. Sequence-to-sequence (Seq2Seq) models have also been used in task-oriented dia"
P18-1136,D16-1137,0,0.0296642,"Missing"
P18-1136,W17-5505,0,0.460644,"o traffic Palo Alto Cafe is 4 miles away at 436 Alger Drive Palo Alto is located at 436 Alger Dr. Table 1: Example of generated responses for the In-Car Assistant on the navigation domain. Introduction ∗ DRIVER Seq2Seq +Attn Ptr-Unk Distance 5 miles 4 miles 5 miles 4 miles 6 miles 6 miles 2 miles techniques, modeling the dependencies between modules is complex and the KB interpretation requires human effort. Recently, end-to-end approaches for dialog modeling, which use recurrent neural networks (RNN) encoder-decoder models, have shown promising results (Serban et al., 2016; Wen et al., 2017; Zhao et al., 2017). Since they can directly map plain text dialog history to the output responses, and the dialog states are latent, there is no need for hand-crafted state labels. Moreover, attention-based copy mechanism (Gulcehre et al., 2016; Eric and Manning, 2017) have been recently introduced to copy words directly from the input sources to the output responses. Using such mechanism, even when unknown tokens appear in the dialog history, the models are still able to produce correct and relevant entities. However, although the above mentioned approaches were successful, they still suffer from two main prob"
P19-1078,D18-1398,0,0.0366563,"y. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limit"
P19-1078,P16-1014,0,0.0299372,"ded dialogue history is represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal"
P19-1078,K16-1002,0,0.113926,"Missing"
P19-1078,W14-4337,0,0.761555,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,D18-1547,0,0.333872,"For example, as shown in Fig. 1, (slot, value) pairs such as (price, cheap) and (area, centre) are extracted from the conversation. Accurate DST performance is crucial for ∗ Work partially done while the first author was an intern at Salesforce Research. 808 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808–819 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics of possible values. Therefore, many of the previous works that are based on neural classification models may not be applicable in real scenario. Budzianowski et al. (2018) recently introduced a multi-domain dialogue dataset (MultiWOZ), which adds new challenges in DST due to its mixed-domain conversations. As shown in Fig. 1, a user can start a conversation by asking to reserve a restaurant, then requests information regarding an attraction nearby, and finally asks to book a taxi. In this case, the DST model has to determine the corresponding domain, slot and value at each turn of dialogue, which contains a large number of combinations in the ontology, i.e., 30 (domain, slot) pairs and over 4,500 possible slot values in total. Another challenge in the multidoma"
P19-1078,W14-4340,0,0.678325,"lid arrows on the left are the single-turn mapping, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST)"
P19-1078,N18-2115,0,0.0286875,"th only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications"
P19-1078,P18-5002,0,0.0582567,"Missing"
P19-1078,D18-1299,0,0.504676,"Missing"
P19-1078,P18-1133,0,0.202571,"eatures and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-specific local modules to learn slot features, which has proved to successfully improve tracking of rare slot values. Lei et al. (2018) use a Seq2Seq model to generate belief spans and the delexicalized response at the same time. Ren et al. (2018) propose StateNet that generates a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approac"
P19-1078,P18-1136,1,0.873258,"represented as Ht = enc |Xt |×dhdd , where d [henc hdd is the 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y l"
P19-1078,P17-1099,0,0.030768,"he 1 , . . . , h|Xt |] ∈ R hidden size. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns. Therefore, we use the recent dialogue history of length l as the utterance encoder input, rather than the current utterance only. 2.2 State Generator To generate slot values using text from the input source, a copy mechanism is required. There are three common ways to perform copying, i.e., index-based copy (Vinyals et al., 2015), hardgated copy (Gulcehre et al., 2016; Madotto et al., 2018; Wu et al., 2019) and soft-gated copy (See et al., 2017; McCann et al., 2018). The indexbased mechanism is not suitable for DST task because the exact word(s) of the true slot value are not always found in the utterance. The hard-gate copy mechanism usually needs additional supervihistory pute the history attention Pjk dialogue history Ht : over the encoded vocab = Softmax(E · (hdec )&gt; ) ∈ R|V |, Pjk jk history &gt; |Xt |. = Softmax(Ht · (hdec Pjk jk ) ) ∈ R (1) final is the weightedThe final output distribution Pjk 810 For the latter, another cross-entropy loss Lv befinal and the true words Y label is used. We tween Pjk j define Lv as sum of two dis"
P19-1078,D16-1022,0,0.0865702,"Missing"
P19-1078,P17-1163,0,0.574186,"Missing"
P19-1078,D17-1314,0,0.0174056,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,P17-2023,0,0.0220702,"earning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are diffic"
P19-1078,D14-1162,1,0.106752,"five domains. The numbers in the last three rows indicate the number of dialogues for train, validation and test sets. Fi (Θi − ΘS,i )2 , (8) MinimizeΘ L(Θ) Hotel price, type, parking, stay, day, people, area, stars, internet, name 3381 416 394 4.2 Training Details Multi-domain Joint Training The model is trained end-to-end using the Adam optimizer (Kingma and Ba, 2015) with a batch size of 32. The learning rate annealing is in the range of [0.001, 0.0001] with a dropout ratio of 0.2. Both α and β in Eq (7) are set to one. All the embeddings are initialized by concatenating Glove embeddings (Pennington et al., 2014) and character embeddings (Hashimoto et al., 2016), where the dimension is 400 for each vocabulary word. A greedy search decoding strategy is used for our state generator since the generated slot values are usually short in length. In addition, to in(9) where L(Θ, K) is the loss value of the K stored samples. Lopez-Paz et al. (2017) show how to solve the optimization problem in Eq (9) with quadratic programming if the loss of the stored samples increases. 812 crease model generalization and simulate an outof-vocabulary setting, a word dropout is utilized with the utterance encoder by randomly"
P19-1078,P18-2069,0,0.230289,"tes a dialogue history representation and compares the distances between this representation and value vectors in the candidate set. Xu and Hu (2018) use the index-based pointer network for different slots, and show the ability to point to unknown values. However, many of them require a predefined domain ontology, and the models were only evaluated on single-domain setting (DSTC2). For multi-domain DST, Rastogi et al. (2017) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on delexicalization to extract the features. Ramadan et al. (2018) propose a model to jointly track domain and the dialogue states using multiple bi-LSTM. They utilize semantic similarity between utterances and the ontology terms and allow the information to be shared across domains. For a more general overview, readers may refer to the neural dialogue review paper from Gao et al. (2018). 7 Conclusion We introduce a transferable dialogue state generator for multi-domain dialogue state tracking, which learns to track states without any predefined domain ontology. TRADE shares all of its parameters across multiple domains and achieves stateof-the-art joint goa"
P19-1078,W13-4067,0,0.279858,"al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share"
P19-1078,E17-1042,0,0.323621,"Missing"
P19-1078,W14-4339,0,0.101165,", 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representation learning to leverage semantic information from word embeddings to and resolve lexical/morphological ambiguity. However, parameters are not shared across slots. On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters betwe"
P19-1078,P17-1062,0,0.0575854,"ple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opinion mining, Shu et al. (2017a) for document classification, and Lee (2017) for hybrid code networks (Williams et al., 2017). Dialogue State Tracking Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states (Williams and Young, 2007; Thomson and Young, 2010; Wang and Lemon, 2013; Williams, 2014), or to jointly learn speech understanding (Henderson et al., 2014b; Zilka and Jurcicek, 2015; Wen et al., 2017). One drawback is that they rely on hand-crafted features and complex domain-specific lexicons (besides the ontology), and are difficult to extend and scale to new domains. Mrkˇsi´c et al. (2017) use distributional representatio"
P19-1078,P18-1134,0,0.500512,"for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogue systems, such as restaurant reservation or ticket booking. The goal of DST is to extract user goals/intentions expressed d"
P19-1078,N18-1109,0,0.0241526,"type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al., 2017; Lopez-Paz et al., 2017; Rusu et al., 2016; Fernando et al., 2017; Lee et al., 2017), especially in image recognition tasks (Aljundi et al., 2017; Rannen et al., 2017). The applications within NLP has been comparatively limited, e.g., Shu et al. (2016, 2017b) for opin"
P19-1078,W18-5001,0,0.0369032,"nd, number-related slots such as arrive by, people, and stay usually have the lowest error rates. We also find that the type slot of hotel domain has a high error rate, even if it is an easy task with only two possible values in the ontology. The reason is that labels of the (hotel, type) pair are sometimes missing in the dataset, 815 6 Related Work intention classifiers (Chen et al., 2016), slotfilling (Bapna et al., 2017), and dialogue policy (Gaˇsi´c and Young, 2014). For language generation, Johnson et al. (2017) propose single encoder-decoder models for zero-shot machine translation, and Zhao and Eskenazi (2018) propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing (Huang et al., 2018), machine translation (Gu et al., 2018), and text classification (Yu et al., 2018) with meta-learning approaches (Schmidhuber, 1987; Finn et al., 2017). These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community (Kirkpatrick et al."
P19-1078,P18-1135,1,0.917453,"ng, and the dot arrows on the right are multi-turn mapping. The state tracker needs to track slot values mentioned by the user for all the slots in all the domains. appropriate dialogue management, where user intention determines the next system action and/or the content to query from the databases. Traditionally, state tracking approaches are based on the assumption that ontology is defined in advance, where all slots and their values are known. Having a predefined ontology can simplify DST into a classification problem and improve performance (Henderson et al., 2014b; Mrkˇsi´c et al., 2017; Zhong et al., 2018). However, there are two major drawbacks to this approach: 1) A full ontology is hard to obtain in advance (Xu and Hu, 2018). In the industry, databases are usually exposed through an external API only, which is owned and maintained by others. It is not feasible to gain access to enumerate all the possible values for each slot. 2) Even if a full ontology exists, the number of possible slot values could be large and variable. For example, a restaurant name or a train departure time can contain a large number Introduction Dialogue state tracking (DST) is a core component in task-oriented dialogu"
P19-1542,P17-1152,0,0.0492102,"Missing"
P19-1542,P18-5002,0,0.0782059,"Missing"
P19-1542,D18-1398,0,0.0579393,"lf. Recently, several meta-learning models has been proposed for solving few-shot image classification (Ravi −0.05 0 1 3 5 10 K-shot Figure 2: k-shot results for different settings. Consistency of PAML grows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al."
P19-1542,P16-1094,0,0.200529,"ows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works conditioned their response on the persona description, instead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel"
P19-1542,D16-1230,0,0.112712,"Missing"
P19-1542,P18-1136,1,0.862632,"tead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel meta-learning setting for personalizing dialogue agents without conditioning the model response to the persona description. This is especially useful since obtaining such persona description requires human effort. Moreover, we show that a dialogue agent trained with meta-learning achieves a more consistent dialogue by both of automatic measures and human evaluation. In future works, we plan to apply meta-learning to comment genera5457 tion (Lin et al., 2019) and task-oriented dialogues systems (Madotto et al., 2018; Wu et al., 2019, 2017, 2018; Reddy et al., 2018). Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. 2019. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415. 6 Po-Sen Huang, Chenglong Wang, Rishabh Singh, Wen-tau Yih, and Xiaodong He. 2018. Natural language to structured query generation via metalearning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 732–738. Association for Computational Ling"
P19-1542,D18-1298,0,0.149968,"of them. For example, “I am an old man” and “I like to play football” are one of the possible persona descriptions provided to the speaker. By conditioning the response generation on the persona descriptions, a chit-chat model is able to produce a more persona consistent dialogue (Zhang et al., 2018). However, it is difficult to capture a persona just by using few sentences, and collecting a nonsynthetic set of persona descriptions from a real human-human conversation, e.g., Reddit, is challenging as well since it requires hand-crafted fea† These two authors contributed equally. ture designs (Mazare et al., 2018). In light of this, we propose to leverage a set of dialogues done by the same persona directly, instead of using its persona descriptions, to generate a more consistent response. We consider learning different personas as different tasks via meta-learning algorithms, which is fundamentally different from optimizing the model to represent all the personas. A high-level intuition of the difference between these two approaches is shown in Figure 1. We aim to learn a persona-independent model that is able to quickly adapt to a new persona given the dialogues. We formulate this task as a few-shot"
P19-1542,P02-1040,0,0.103242,"Missing"
P19-1542,D14-1162,0,0.0812706,"Missing"
P19-1542,N18-1109,0,0.0238206,"been proposed for solving few-shot image classification (Ravi −0.05 0 1 3 5 10 K-shot Figure 2: k-shot results for different settings. Consistency of PAML grows linearly with respect to k. and Larochelle, 2016; Vinyals et al., 2016; Finn et al., 2017; Mishra et al., 2017; Santoro et al., 2016), optimization (Andrychowicz et al., 2016) and reinforcement learning (Finn et al., 2017). Meta-learning for NLP application is less common, and it has been applied in semantic parsing task (Huang et al., 2018), machine translation for low resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works c"
P19-1542,K18-1053,0,0.167015,"w resource language (Gu et al., 2018), and for text classification (Yu et al., 2018). To the best of our knowledge, this is the first attempt in adapting meta-learning to personalized dialogue learning. Personalized Dialogue Li et al. (2016) was the first to propose a persona based dialogue models for improving response consistency. Zhang et al. (2018) introduced Persona-chat, which was further extended in ConvAI2 (2019). Several works improved on the initial baselines with various methodologies (Kulikov et al., 2018; Yavuz et al.; Hancock et al., 2019; Lucas et al., 2009; Joshi et al., 2017; Zemlyanskiy and Sha, 2018; Gao et al., 2018). However, all of these previous works conditioned their response on the persona description, instead of using the dialogues produced by the persona. 5 Conclusion In this paper, we present a novel meta-learning setting for personalizing dialogue agents without conditioning the model response to the persona description. This is especially useful since obtaining such persona description requires human effort. Moreover, we show that a dialogue agent trained with meta-learning achieves a more consistent dialogue by both of automatic measures and human evaluation. In future works"
P19-1542,P18-1205,0,0.638362,"man designed persona descriptions to improve dialogue consistency. Collecting such descriptions from existing dialogues is expensive and requires hand-crafted feature designs. In this paper, we propose to extend Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) to personalized dialogue learning without using any persona descriptions. Our model learns to quickly adapt to new personas by leveraging only a few dialogue samples collected from the same user, which is fundamentally different from conditioning the response on the persona descriptions. Empirical results on Persona-chat dataset (Zhang et al., 2018) indicate that our solution outperforms non-metalearning baselines using automatic evaluation metrics, and in terms of human-evaluated fluency and consistency. 1 ⋯ ⋯ Figure 1: The difference between finetuning from a) joint training on all personas and b) meta-learning persona. The solid line represents the optimization path of the initial parameters and dashed line the fine-tuning path. Meta-learned initial parameters can faster adapt to a new persona. Introduction There is a growing interest in learning personalized chit-chat dialogue agents for making chatbots more consistent. Recently, a m"
S17-2048,S16-1128,0,0.0251377,"Figure 1 shows an example of a question and a comment which are parsed accordingly. We applied standard preprocessing to question and comment body, so as to achieve better performance during syntactic parsing and a better alignment of our vocabulary to the GloVe one. Each question also includes the subject of the topic. Preprocessing included the following steps: 2.1 Additional Features After that, we generated several features, representing both metadata and some properties of the couple Question-Comment. These features have been commonly used in literature, both with Neural Networks (as in (Mohtarami et al., 2016)), linear or SVM models as in (Mihaylova et al., 2016), in order to include additional and potentially relevant information not easily conveyed through semantic representations. In our case, they are used as additional input beside the RNN output. Features can be grouped as follows: • Portions of text that include HTML tags and special sequences were removed or substituted with simpler strings. • Using a set of regular expressions, we replaced URLs, nicknames, email addresses with a placeholder for each category. • Too long repetitions of characters inside tokens were replaced by a single char"
S17-2048,D07-1119,1,0.824106,"Missing"
S17-2048,S17-2003,0,0.0144491,"ers. For these reasons we present a model which tries to tackle these problems. The subtasks we have worked on can be described as follows: A) Question-Comment Similarity - Given a question q and 10 comments c1 , . . . , c10 , rank such comments from the most relevant to the least one with respect to q, and assign to each one a label which can be ""Good"" or ""Bad"". B) Question-Question Similarity - Given a question q and a set of 10 related questions q1 , . . . , q10 , rank the 10 questions from ""Relevant"" to ""Irrelevant"", according to q. A more detailed description of the task can be found in (Nakov et al., 2017). Our work has been inspired by studies regarding embedding spaces. Indeed, in (Hsu et al., 2016) 299 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 299–304, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics P ROOT SUBJ is   there NMOD NMOD any  place  SUBJ I OBJ  VC can OPRD  find   scented NMOD massage  oils LOC  in PMOD  qatar  ? P PMOD PRD  SUBJ It is  PMOD right  behind PMOD  kahraba LOC  in  the NMOD  national  area  . Figure 1: Dependency parsing of two sentences taken from a questio"
S17-2048,W14-4012,0,0.0815634,"Missing"
S17-2048,D14-1162,0,0.082242,"softmax activation, thus we have: exp(yi ) oi = Pn j=0 exp(yj ) Where n is the length of the vector y. The latter provides a distribution over two classes: ’Good’ at 301 and ’Bad/Partially Useful’ for subtask A, ’PerfectMatch/Relevant’ and ’Irrelevant’ for subtask B. To obtain the final ranking we took the probability of a given input to be labeled as the positive class. The entire network is trained with back-propagation using a cross-entropy loss function. Figure 2 shows the conceptual schema of the model. 4 ended up to over-fit. Two different types of embeddings have been evaluated: GloVe (Pennington et al., 2014), which are trained using Wikipedia, and embeddings trained directly with questions and answers extracted from the Qatar Living forum (Mihaylov and Nakov, 2016). However, in our model both embeddings worked well, thus with the latter we did not obtained any particular improvements. To encode the RNN input into a single embedding we compare three different approaches: SUM (which sums all the triples given as input), LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014). Finally, the neural network model was implemented using Keras (Chollet, 2015), which provides an efficient and ea"
S19-2021,D16-1110,1,0.844814,"Missing"
S19-2021,S19-2005,0,0.0485243,"Missing"
S19-2021,P15-1162,0,0.0559911,"Missing"
S19-2021,S18-1019,0,0.0419734,"Missing"
S19-2021,D17-1054,0,0.0171696,"ji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained in end-to-end models. We further explore a GP for faster hyperparameter search. Our experiments show that hierarchical architectures give significant impro"
S19-2021,W16-6208,0,0.0375742,"Missing"
S19-2021,S18-1039,1,0.819812,"Missing"
S19-2021,D17-1169,0,0.268995,"ecifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical"
S19-2021,N18-1202,0,0.275659,"tep is to be able to detect emotion with a hierarchical structure. To the best of our knowledge, this task of extracting emotional knowledge in a hierarchical setting has not yet been extensively explored in the literature. Therefore, in this paper, we investigate this problem in depth with several strong hierarchical baselines and by using a large variety of pre-trained word embeddings. 2 Methodology In this task, we focus on two main approaches: 1) feature-based and 2) end-to-end. The former compares several well-known pre-trained embeddings, including GloVe (Pennington et al., 2014), ELMo (Peters et al., 2018), and BERT (Devlin et al., 2018), as well as emotional embeddings. We combine these pre-trained features with a simple Logistic Regression (LR) and XGBoost (Chen and Guestrin, 2016) model as the classifier to compare their effectiveness. The latter approach is to train Emotion a) b) Softmax T2 c) Emotion Voting Softmax Encoder (E) T1 Emotion T3 E E E M1 T1 T2 T3 T1 T2 T3 ... Mn−1 Mn T1 T2 T3 T1 T2 T3 Figure 1: a) Flat model; b) Hierarchical model; c) Voting scheme a model fully end-to-end with back-propagation. We mainly compare the performances of flat models and hierarchical models, which al"
S19-2021,P17-4021,1,0.890635,"Missing"
S19-2021,D13-1170,0,0.00394712,"to be empathetic, specifically by contextual emotion detection in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion"
S19-2021,J11-2001,0,0.00541946,"t ways. Word-level emotional representations, inspired from word embeddings, learn a vector for each word, and have shown effectiveness in different emotion related tasks, such as sentiment classification (Tang et al., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et"
S19-2021,D17-1052,0,0.0309174,"l., 2016), emotion classification (Xu et al., 2018), and emotion intensity prediction (Park et al., 2018). Sentence-level emotional representations, such as DeepMoji (Felbo et al., 2017a), train a biLSTM model to encode the whole sentence to predict the corresponding emoji of the sentence. The learned model achieves stateof-the-art results on eight datasets. Sentiment lexicons from Taboada et al. (2011) show that word lexicons annotated with sentiment/emotion labels are effective in sentiment classification. This method is further developed using both supervised and unsupervised approaches in Wang and Xia (2017). Also, other models, such as a deep averaging network (Iyyer et al., 2015), attention-based network (Winata et al., 2018), and memory network (Dou, 2017), have been investigated to improve the classification performance. Practically, the application of emotion classification has been investigated on interactive dialogue systems (Bertero et al., 2016; Winata et al., 2017; Siddique et al., 2017; Fung et al., 2018). 5 Conclusion In this paper, we compare different pre-trained word embedding features by using Logistic Regression and XGBoost along with flat and hierarchical architectures trained i"
S19-2021,W18-6243,1,0.925306,"in text. Given a textual dialogue with two turns of context, the system has to classify the emotion of the next utterance into one of the following emotion classes: Happy, Sad, Angry, or Others. The training dataset contains *Equal contribution. 15K records for emotion classes, and contains 15K records not belonging to any of the aforementioned emotion classes. The most naive first step would be to recognize emotion from a given flattened sequence, which has been researched extensively despite the very abstract nature of emotion (Socher et al., 2013; Felbo et al., 2017a; McCann et al., 2017; Xu et al., 2018; Chatterjee et al., 2019a). However, these flat models do not work very well on dialogue data as we have to merely concatenate the turns and flatten the hierarchical information. Not only does the sequence get too long, but the hierarchy between sentences will also be destroyed (Hsu and Ku, 2018; Kim et al., 2018). We believe that the natural flow of emotion exists in dialogue, and using such hierarchical information will allow us to predict the last utterance’s emotion better. Naturally, the next step is to be able to detect emotion with a hierarchical structure. To the best of our knowledge"
S19-2021,D14-1162,0,\N,Missing
S19-2021,W18-3505,0,\N,Missing
W18-3207,P13-2037,0,0.238927,". We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NL"
W18-3207,W17-4419,0,0.0427497,"去 check. (I want to go) check. (2) 我 不 懂 要 怎么 讲 一 个 小时 seriously I didn’t have so much things to say (I don’t understand how to speak for an hour) seriously I didn’t have so much things to say 62 Proceedings of The Third Workshop on Computational Approaches to Code-Switching, pages 62–67 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics mains (Collobert et al., 2011; Luong et al., 2016; Hashimoto et al., 2016). They presented a joint many-task model to handle multiple NLP tasks and share parameters with growing depth in a single end-to-end model. A work by Aguilar et al. (2017) showed the potential of combining POS tagging with Named-Entity Recognition task. Language modeling using only word lexicons is not adequate to learn the complexity of codeswitching patterns, especially in a low resource setting. Learning at the same time syntactic features such as POS tag and language identifier allows to have a shared grammatical information that constraint the next word prediction. Due to this reason, we propose a multi-task learning framework for code-switching language modeling task which is able to leverage syntactic features such as language and POS tag. The main contr"
W18-3207,N16-1024,0,0.0293915,"012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NLP tasks using multi-task learning have been recently used in many do3.2 Model Description Figure 1 illustrates our multi-task learning extension to recurrent language model. In this multitask learning setting, the tasks are language modeling and POS tagging. The POS tagging task shares the POS tag vector and the hidden states to LM task, but it doe"
W18-3207,E17-2025,0,0.0424242,"icial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity (Press and Wolf, 2017). Learning multiple NLP tasks using multi-task learning have been recently used in many do3.2 Model Description Figure 1 illustrates our multi-task learning extension to recurrent language model. In this multitask learning setting, the tasks are language modeling and POS tagging. The POS tagging task shares the POS tag vector and the hidden states to LM task, but it does not receive any information from the other loss. Let wt be the word lexicon in the document and pt be the POS tag of the corresponding wt at index t. They are mapped into embedding matrices to get their d-dimensional vector p"
W18-3207,N03-1033,0,0.0452277,"sity, 2015). 2 3 Methodology This section shows how to build the features and how to train our multi-task learning language model. Multi-task learning consists of two NLP tasks: Language modeling and POS sequence tagging. 3.1 Feature Representation In the model, word lexicons and syntactic features are used as input. Word Lexicons: Sentences are encoded as 1hot vectors and our vocabulary is built from training data. Syntactic Features: For each language island, phrase within the same language, we extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolat"
W18-3207,C12-1102,1,0.875453,"extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dye"
W18-3207,I05-3005,0,0.0144798,"Technological University, 2015). 2 3 Methodology This section shows how to build the features and how to train our multi-task learning language model. Multi-task learning consists of two NLP tasks: Language modeling and POS sequence tagging. 3.1 Feature Representation In the model, word lexicons and syntactic features are used as input. Word Lexicons: Sentences are encoded as 1hot vectors and our vocabulary is built from training data. Syntactic Features: For each language island, phrase within the same language, we extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which"
W18-3207,P14-5010,0,0.00643434,"INEN 4,801 CDZH 20,158 VBEN 4,703 Ltotal = pLlm + (1 − p)Lpt where p is the weight of the loss in the training. 3.3 Experimental Setup In this section, we present the experimental setting for this task Corpus: SEAME (South East Asia MandarinEnglish), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations (Nanyang Technological University, 2015). Our dataset (LDC2015S04) is the most updated version of the Linguistic Data Consortium (LDC) Preprocessing: First, we tokenized English and Chinese word using Stanford NLP toolkit (Manning et al., 2014). Second, all hesitations and punctuations were removed except apostrophe, for examples: “let’s” and “it’s”. Table 1 and Table 2 show the statistics of SEAME Phase I and II corpora. Table 3 shows the most common trigger POS tag for Phase II corpus. 64 Training: The baseline model was trained using RNNLM (Mikolov et al., 2011)1 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization (Srivastava et al., 2014) was applied to the word embedding vector"
W18-3207,D14-1098,1,0.862172,"eratively using Chinese and English Penn Tree Bank Parser (Tseng et al., 2005; Toutanova et al., 2003). There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages. Related Work The earliest language modeling research on codeswitching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data (Li and Fung, 2012; Ying and Fung, 2014). Vu et al. (2012) built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial codeswitching text. Adel et al. (2013a,b) introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. Adel et al. (2015) explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, Dyer et al. (2016) propos"
W18-3214,W17-4419,0,0.0775262,"al Network (CNN) was used in NER task as word decoder by Collobert et al. (2011) and a few years later, Huang et al. (2015) introduced Bidirectional Long-Short Term Memory (BiLSTM) (Sundermeyer et al., 2012). Character-level features were explored by using neural architecture and replaced hand-crafted features (Dyer et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Limsopatham and Collier, 2016). Lample et al. (2016) also showed Conditional Random Field (CRF) (Lafferty et al., 2001) decoders to improve the results and used Stack memory-based LSTMs for their work in sequence chunking. Aguilar et al. (2017) proposed multi-task learning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 20"
W18-3214,W17-2630,0,0.0131081,"ning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 2016; Lample et al., 2016; Shen et al., 2017) using monolingual corpus. However, learning from code-switching tweets data is very challenging due to several reasons: (1) words may have different semantics in different context and language, for instance, the word “cola” can be associated with product or “queue” in Spanish (2) data from social media are noisy, with many inconsistencies such as spelling mistakes, repetitions, and informalities which eventually points to Out-ofVocabulary (OOV) words issue (3) entities may appear in different language other than the matrix language. For example “todos los Domingos en Westland Mall” where “Wes"
W18-3214,Q16-1026,0,0.0689048,"Missing"
W18-3214,P15-1033,0,0.0535152,"Missing"
W18-3214,L18-1550,0,0.0684472,"Missing"
W18-3214,N16-1030,0,0.0318593,"level features used in our model. Word Representation: Words are encoded into continuous representation. The vocabulary is built from training data. The Twitter data are very noisy, there are many spelling mistakes, irregular ways to use a word and repeating characters. 111 3.3 Model Description output In this section, we describe our model architecture and hyper-parameters setting. Bilingual Char-RNN: This is one of the approaches to learn character-level embeddings without needing of any lexical hand-crafted features. We use an RNN for representing the word with character-level information (Lample et al., 2016). Figure 1 shows the model architecture. The inputs are characters extracted from a word and every character is embedded with d dimension vector. Then, we use it as the input for a Bidirectional LSTM as character encoder, wherein every time step, a character is input to the network. Consider at as the hidden states for word t. BiLSTM decoder a7 a6 a5 a4 a3 a2 a1 a1 a2 a3 a4 a5 a6 a7 a8 b a I-LOC c1 c2 c3 c4 c5 c6 h6 h5 h4 h3 h2 h1 h1 h2 h3 h4 h5 h6 Char lang2 Bilingual Char-RNN Figure 2: Main architecture → − ← − ct = ht ⊕ ht where ⊕ denotes the concatenation operator. Dropout is applied to th"
W18-3214,W16-3920,0,0.0567317,"Missing"
W18-3214,D15-1176,0,0.0345107,"yer et al., 2015; Lample et al., 2016; Chiu and Nichols, 2016; Limsopatham and Collier, 2016). Lample et al. (2016) also showed Conditional Random Field (CRF) (Lafferty et al., 2001) decoders to improve the results and used Stack memory-based LSTMs for their work in sequence chunking. Aguilar et al. (2017) proposed multi-task learning by combining Part-of-Speech tagging task with NER and using gazetteers to provide language-specific knowledge. Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER (Lample et al., 2016), POS tagging, and language modeling (Ling et al., 2015). Introduction Named Entity Recognition (NER) predicts which word tokens refer to location, people, organization, time, and other entities from a word sequence. Deep neural network models have successfully achieved the state-of-the-art performance in NER tasks (Cohen; Chiu and Nichols, 2016; Lample et al., 2016; Shen et al., 2017) using monolingual corpus. However, learning from code-switching tweets data is very challenging due to several reasons: (1) words may have different semantics in different context and language, for instance, the word “cola” can be associated with product or “queue” i"
W18-6243,P15-1166,0,0.036932,"rious methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than te"
W18-6243,D17-1268,0,0.0190674,"eveloped for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we propose Emo2Vec to represent emotion with vectors using a multi-task training framework. Six affect-related tasks are utilized, including emotion/sentiment analysis, sarcasm classification, stress detection, abusive language classification, insult detection, and personality recognition. We empirically show how Emo2Vec leverages multi-task training to learn a generalized emotion representation. In addition, Emo2Vec outperforms existing affect-related embeddings on more than ten different datasets. By"
W18-6243,D17-1054,0,0.0408305,", more attention needs to be paid to words. ing (Peng et al., 2017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et"
W18-6243,P17-1186,0,0.0314113,"Missing"
W18-6243,uryupina-etal-2014-sentube,0,0.0275266,"ed is illustrated in Figure 2. Firstly, 1-D convolution is used to extract n293 personality recognition are included. The reason why we include many datasets is to 1) leverage different aspects of words emotion knowledge, which may not be present in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Suppleme"
W18-6243,D17-1052,0,0.0637837,"-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which boosts the performance of affectrelated tasks. Multi-task training has achieved great success in various natural language tasks, such as machine translation (Dong et al., 2015; Malaviya et al., 2017), multilingual tasks (Duong et al., 2015; Gillick et al., 2016), semantic parsIn this paper, we prop"
W18-6243,D14-1162,0,0.0828235,"sive language classification, insult detection, and personality recognition. Our evaluation of Emo2Vec shows that it outperforms existing affect-related representations, such as Sentiment-Specific Word Embedding and DeepMoji embeddings with much smaller training corpora. When concatenated with GloVe, Emo2Vec achieves competitive performances to state-of-the-art results on several tasks using a simple logistic regression classifier. 1 Introduction Recent work on word representation has been focusing on embedding syntactic and semantic information into fixed-sized vectors (Mikolov et al., 2013; Pennington et al., 2014) based on the distributional hypothesis, and have proven to be useful in many natural language tasks (Collobert et al., 2011). However, despite the rising popularity regarding the use of word embeddings, they often fail to capture the emotional semantics the words convey. For example, the GloVe vector captures the semantic meaning of “headache”, as it is closer to words of ill symptoms like “fever” and “toothache”, but misses the emotional association that the word carries. The word “headache” in the sentence “You are giving me a headache” does not really mean that the speaker will get a heada"
W18-6243,D16-1058,0,0.0571363,"Missing"
W18-6243,W16-5618,0,0.0119463,"ts of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model,"
W18-6243,W13-1603,0,0.02274,"nt in single domain dataset; 2) create a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model a"
W18-6243,N16-2013,0,0.0172717,"stic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alone. Parameters of T and CNN are randomly initialized and Adam is used for optimization. Best parameter settings are tuned on the validation set. For the best model, we use the batch size o"
W18-6243,D13-1170,0,0.00939142,"ation Baselines: We use 50-dimension Sentimentspecific Word Embedding (SSWE) (Tang et al., 2016) as our baseline, which is an embedding model trained with 10 millions of tweets by encoding both semantic and sentiment information into vectors. Also, lots of work about the detection/classification in sentiment analysis implicitly encodes emotion inside the word vectors. For example, Felbo et al. (2017) trains a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the Small datasets For sentiment, we include 8 datasets. (1,2) SSTfine and SST-binary (Socher et al., 2013) (3) OpeNER (Agerri et al., 2013) (4,5) tube auto 2 http://hci.epfl.ch/sharing-emotion-lexicons-anddata#emo-hash-data 294 model SSWE DeepMoji embedding CNN embedding Emo2Vec SS-T 0.815 0.788 0.803 0.801 SS-Y 0.835 0.841 0.862 0.859 SS-binary 0.698 0.751 0.734 0.812 SS-fine 0.365 0.369 0.369 0.416 OpeNER 0.701 0.754 0.713 0.744 tube auto 0.620 0.628 0.605 0.629 tube tablet 0.654 0.675 0.667 0.688 SemEval 0.629 0.676 0.622 0.638 average 0.665 0.685 0.672 0.698 Table 1: Comparison between different emotion representations on sentiment datasets, all results are reported with accuracy. The best res"
W18-6243,P14-2070,0,0.0448146,"ate a more general embedding emotional space that can generalize well across different tasks and domains. To avoid over-fitting, L2 regularization penalty is added from the weights of all logistic regression classifiers ϕi for i ∈ [1, n]. Hence, we jointly optimize the following loss function: L(MΦ ) = and tube tablet (Uryupina et al., 2014) (6) SemEval (Hltcoe, 2013) (7,8) SS-Twitter and SSYoutube (Thelwall et al., 2010). For emotion tasks, we include 4 datasets, (1) ISEAR (Wallbott and Scherer, 1986) (2) WASSA (Mohammad and Bravo-Marquez, 2017) (3) Olympic Sintsova et al. (2013) (4) SE0714 (Staiano and Guerini, 2014). We further include 6 other affect-related datasets. (1,2) SCv1-GEN and SCv2-GEN for sarcasm detection, (3) Stress (Winata et al., 2018), (4) Abusive (Waseem, 2016; Waseem and Hovy, 2016). (5) Personality (Pennebaker and King, 1999) (6) Insult. The detailed statistics can be found in Table 4 and Table 5 in Supplemental Material. ∑ 1∑ Lj + λ ∥LRϕj ∥2 n n n j=1 j=1 Where Lj is the negative log likelihood (NLL) between yˆj and y j , and λ an hyper-parameter for the regularization terms. 3 3.2 Pre-training Emo2Vec Emo2Vec embedding matrix and the CNN model are pre-trained using hashtag corpus alo"
W18-6243,D17-1056,0,0.0363301,"by Multi-task Training Peng Xu, Andrea Madotto, Chien-Sheng Wu, Ji Ho Park and Pascale Fung Center for Artificial Intelligence Research (CAiRE) The Hong Kong University of Science and Technology, Clear Water Bay [pxuab,eeandreamad,cwuak,jhpark,pascale]@ust.hk Abstract and syntactic contextual information in a vector space. This work demonstrates the effectiveness of incorporating sentiment labels in a wordlevel information for sentiment-related tasks compared to other word embeddings. However, they only focus on binary labels, which weakens their generalization ability on other affect tasks. Yu et al. (2017) instead uses emotion lexicons to tune the vector space, which gives them better results. Nevertheless, this method requires human-labeled lexicons and cannot scale to large amounts of data. Felbo et al. (2017) achieves good results on affect tasks by training a two-layer bidirectional Long Short-Term Memory (bi-LSTM) model, named DeepMoji, to predict emoji of the input document using a huge dataset of 1.2 billions of tweets. However, collecting billions of tweets is expensive and time consuming for researchers. Furthermore, most works in sentiment and emotion analysis have focused solely on a"
W18-6243,D17-1115,0,0.0312324,"017). Hashimoto et al. (2017) jointly learns POS tagging, chunking, dependency parsing, semantic relatedness, and textual entailment by considering linguistic hierarchy and achieves state-of-the-results on five datasets. For sentiment analysis, Balikas et al. (2017) jointly trains ternary and fine-grained classification with a recurrent neural network and achieves new stateof-the-art results. 5 6 Conclusion and Future Work Related work For sentiment analysis, numerous classification models (Kalchbrenner et al.; Iyyer et al., 2015; Dou, 2017) have been explored. Multi-modal sentiment analysis (Zadeh et al., 2017; Poria et al., 2017) extends text-based model to the combination of visual, acoustic and language, which achieves better results than the single modality. Various methods are developed for automatic constructions of sentiment lexicons using both supervised and unsupervised way (Wang and Xia, 2017). Aspect-based sentiment (Chen et al., 2017; Wang et al., 2016) is also a hot topic where researchers care more about the sentiment towards a certain target. Transfer learning from the large corpus is also investigated by Felbo et al. (2017) to train a large model on a huge emoji tweet corpus, which"
W19-5508,P19-1542,1,0.776258,"Missing"
W19-5508,S19-2021,1,0.86315,"Missing"
