C14-1057,J07-4004,0,0.0116036,"account when retrieving graphics and that doing so results in a model with better performance than a baseline model that relies on matching query words with words in the graphic. 1 Introduction Infographics are non-pictorial graphics such as bar charts and line graphs. When such graphics appear in popular media, they generally have a high-level message that they are intended to convey. For example, the graphic in Figure 1 ostensibly conveys the message that Toyota has the highest profit among the automobile companies listed. Thus infographics are a form of language since, according to Clark (Clark and Curran, 2007), language is any deliberate signal that is intended to convey a message. Although much research has addressed the retrieval of documents, very little attention has been given to the retrieval of infographics. Yet research has shown that the content of an infographic is often not included in the article’s text (Carberry et al., 2006). Thus infographics are an important knowledge source that should be accessible to users of a digital library. Techniques that have been effective for document or image retrieval are inadequate for the retrieval of infographics. Current search engines employ strate"
C90-2048,J86-3001,0,\N,Missing
C98-2191,C94-1042,0,0.0416783,"Missing"
H86-1018,P84-1045,0,0.0201705,"ic with some necessary contextual information, in particular, attribute salience ratings. Finally, an example of how perspective information and the similarity metric can be used to give reasonable responses to misconceptions involving object properties is given. 2. O b j e c t Similarity As was shown above, in order to respond effectively to property misconceptions, we must have a method for determining object similarity. Object similarity has previously been shown to be important in tasks such as organizing explanations [6], offering cooperative responses to pragmatically ill-formed queries [2], and identifying metaphors [9]. In the above systems the similarity of two objects is based on the distance between the objects in the generalization hierarchy. One problem with this approach is that it is context invariant.* T h a t is, there is no way for contextual information to affect similarity judgments. However, Tversky [8] proposes a measure of object similarity based on common and disjoint features/properties of the objects involved, which enables contextual *See [5] for additional problems and discussion of this point. 193 information to be taken into account. Tversky&apos;s similarity"
H86-1018,J84-1001,0,0.0541745,"al information, in particular, attribute salience ratings. Finally, an example of how perspective information and the similarity metric can be used to give reasonable responses to misconceptions involving object properties is given. 2. O b j e c t Similarity As was shown above, in order to respond effectively to property misconceptions, we must have a method for determining object similarity. Object similarity has previously been shown to be important in tasks such as organizing explanations [6], offering cooperative responses to pragmatically ill-formed queries [2], and identifying metaphors [9]. In the above systems the similarity of two objects is based on the distance between the objects in the generalization hierarchy. One problem with this approach is that it is context invariant.* T h a t is, there is no way for contextual information to affect similarity judgments. However, Tversky [8] proposes a measure of object similarity based on common and disjoint features/properties of the objects involved, which enables contextual *See [5] for additional problems and discussion of this point. 193 information to be taken into account. Tversky&apos;s similarity rating for two objects a and b,"
J02-4004,W97-0703,0,0.974817,"f previous lexical chains algorithms. 1. Introduction The overall motivation for the research presented in this article is the development of a computationally efficient system to create summaries automatically. Summarization has been viewed as a two-step process. The first step is the extraction of important concepts from the source text by building an intermediate representation of some sort. The second step uses this intermediate representation to generate a summary (Sparck Jones 1993). In the research presented here, we concentrate on the first step of the summarization process and follow Barzilay and Elhadad (1997) in employing lexical chains to extract important concepts from a document. We present a linear-time algorithm for lexical chain computation and offer an evaluation that indicates that such chains are a promising avenue of study as an intermediate representation in the summarization process. Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. Attempts to determine the benefit of this proposal have been faced with a number of difficulties. First, previous methods for computing lexical chains have either been manual (Morris and Hirst 199"
J02-4004,P99-1071,0,0.119177,"Missing"
J02-4004,A00-2024,0,0.012848,"ch has begun to look at the difficult problem of generating a summary text from an intermediate representation. Hybrid approaches such as extracting phrases instead of sentences and recombining these phrases into salient text have been proposed (Barzilay, McKeown, and Elhadad 1999). Other recent work looks at summarization as a process of revision; in this work, the source text is revised until a summary of the desired length is achieved (Mani, Gates, and Bloedorn 1999). Additionally, some research has explored cutting and pasting segments of text from the full document to generate a summary (Jing and McKeown 2000). It is our intention to use lexical chains as part of the input to a more classical text generation algorithm to produce new text that captures the concepts from the extracted chains. The lexical chains identify noun (or argument) concepts for the 495 Computational Linguistics Volume 28, Number 4 summary. We are examining ways for predicates to be identified and are concentrating on situations in which strong lexical chains intersect in the text. 6. Conclusions In this article, we have outlined an efficient, linear-time algorithm for computing lexical chains as an intermediate representation"
J02-4004,P99-1072,0,0.0110856,"Missing"
J02-4004,J91-1002,0,0.959866,"y and Elhadad (1997) in employing lexical chains to extract important concepts from a document. We present a linear-time algorithm for lexical chain computation and offer an evaluation that indicates that such chains are a promising avenue of study as an intermediate representation in the summarization process. Barzilay and Elhadad (1997) proposed lexical chains as an intermediate step in the text summarization process. Attempts to determine the benefit of this proposal have been faced with a number of difficulties. First, previous methods for computing lexical chains have either been manual (Morris and Hirst 1991) or automated, but with exponential efficiency (Hirst and St.-Onge 1997; Barzilay and Elhadad 1997). Because of this, computing lexical chains for documents of any reasonable size has been impossible. We present here an algorithm for computing lexical chains that is linear in space and time. This algorithm makes the computation of lexical chains computationally feasible even for large documents. ∗ Department of Computer and Information Sciences, Newark, DE 19711. E-mail: silber@udel.edu † Department of Computer and Information Sciences, Newark, DE 19711. E-mail: mccoy@mail.eecis. udel.edu c 20"
J02-4004,W00-1438,1,0.720189,"s the worst-case values may not reflect the actual performance of our application. In addition, the synsets with many parent-child relations tend to represent extremely general concepts such as “thing” and “object.” These synsets will most likely not appear very often in a document. Whereas in the worst case these constants are quite large, in the average case they are reasonable. This algorithm is O(n) in the number of nouns within the source document. Considering the size of most documents, the linear nature of this algorithm makes it usable for generalized summarization of large documents (Silber and McCoy 2000). For example, in a test, our algorithm calculated a lexical chain interpretation of a 40,000-word document in 11 seconds on a Sparc Ultra 10 Creator. It was impossible to compute lexical chains for such a document under previous implementations because of computational complexity. Thus documents tested by Barzilay and Elhadad were significantly smaller in size. Our method affords a considerable speedup for these smaller documents. For instance, a document that takes 300 seconds using Barzilay and Elhadad’s method takes only 4 seconds using ours (Silber and McCoy 2000). 4. Evaluation Design Ou"
J12-3004,W98-1435,0,0.0678968,"our future work. 2. Background 2.1 Related Work There has been a growing interest in language systems that generate textual summaries of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally referred to as data-to-text systems, is to enable efﬁcient processing of large volumes of numeric data by supporting traditional visualisation modalities and to reduce the effort spent by human experts on analyzing the data. Various examples of datato-text systems in the literature include systems that summarize weather forecast data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich 1983), and georeferenced data (Turner, Sripada, and Reiter 2009). One of the most successful data-to-text generation research efforts is the SumTime project, which uses pattern recognition techniques to generate textual summaries of automatically generated time-series data in order to convey the signiﬁcant and interesting events (such as spikes and oscillations) that a domain expert would recognize by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003) and SumTime-Turbine (Yu et al. 2007) systems were designed to summarize weather forecast dat"
J12-3004,P05-1007,0,0.0422219,"Missing"
J12-3004,W98-1501,0,0.0516016,"portant. We hypothesize that providing alternative access to what the graphic looks like is not enough and that the user should be provided with the message and knowledge that one would gain from viewing the graphic. We argue that the textual summaries generated by our approach could be associated with graphics as ALT texts so that individuals with sight impairments would be provided with the high-level content of graphics while reading electronic documents via screen readers. 2.2.2 Document Summarization. Research has extensively investigated various techniques for single (Hovy and Lin 1996; Baldwin and Morton 1998) and multi-document summarization (Goldstein et al. 2000; Schiffman, Nenkova, and McKeown 2002). The summary should provide the topic and an overview of the summarized documents by identifying the important and interesting aspects of these documents. Document summarizers generally evaluate and extract items of information from documents according to their relevance to a particular request (such as a request for a person or an event) and address discourse related issues such as removing redundancies (Radev et al. 2004) and ordering sentences (Barzilay, Elhadad, and McKeown 2002) in order to mak"
J12-3004,N06-1046,0,0.0679969,"Missing"
J12-3004,W08-1127,0,0.0925435,"Missing"
J12-3004,P87-1022,0,0.481365,"Missing"
J12-3004,W00-0405,0,0.124524,"o what the graphic looks like is not enough and that the user should be provided with the message and knowledge that one would gain from viewing the graphic. We argue that the textual summaries generated by our approach could be associated with graphics as ALT texts so that individuals with sight impairments would be provided with the high-level content of graphics while reading electronic documents via screen readers. 2.2.2 Document Summarization. Research has extensively investigated various techniques for single (Hovy and Lin 1996; Baldwin and Morton 1998) and multi-document summarization (Goldstein et al. 2000; Schiffman, Nenkova, and McKeown 2002). The summary should provide the topic and an overview of the summarized documents by identifying the important and interesting aspects of these documents. Document summarizers generally evaluate and extract items of information from documents according to their relevance to a particular request (such as a request for a person or an event) and address discourse related issues such as removing redundancies (Radev et al. 2004) and ordering sentences (Barzilay, Elhadad, and McKeown 2002) in order to make the summary more coherent. It is widely accepted that"
J12-3004,W11-2703,1,0.887556,"Missing"
J12-3004,J86-3001,0,0.637976,"Missing"
J12-3004,J95-2003,0,0.153317,"Missing"
J12-3004,P88-1020,0,0.237425,"Missing"
J12-3004,J09-1003,0,0.0326888,"Missing"
J12-3004,J04-4001,0,0.0628968,"Missing"
J12-3004,J03-1003,0,0.0347988,"Missing"
J12-3004,P83-1022,0,0.657399,"2.1 Related Work There has been a growing interest in language systems that generate textual summaries of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally referred to as data-to-text systems, is to enable efﬁcient processing of large volumes of numeric data by supporting traditional visualisation modalities and to reduce the effort spent by human experts on analyzing the data. Various examples of datato-text systems in the literature include systems that summarize weather forecast data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich 1983), and georeferenced data (Turner, Sripada, and Reiter 2009). One of the most successful data-to-text generation research efforts is the SumTime project, which uses pattern recognition techniques to generate textual summaries of automatically generated time-series data in order to convey the signiﬁcant and interesting events (such as spikes and oscillations) that a domain expert would recognize by analyzing the data. The SumTime-Mousam (Somayajulu, Reiter, and Davy 2003) and SumTime-Turbine (Yu et al. 2007) systems were designed to summarize weather forecast data and the data from gas turbine e"
J12-3004,A97-1039,0,0.151601,"Missing"
J12-3004,J97-1004,0,0.0314434,"Missing"
J12-3004,C96-2123,0,0.126574,"phic is important. We hypothesize that providing alternative access to what the graphic looks like is not enough and that the user should be provided with the message and knowledge that one would gain from viewing the graphic. We argue that the textual summaries generated by our approach could be associated with graphics as ALT texts so that individuals with sight impairments would be provided with the high-level content of graphics while reading electronic documents via screen readers. 2.2.2 Document Summarization. Research has extensively investigated various techniques for single (Hovy and Lin 1996; Baldwin and Morton 1998) and multi-document summarization (Goldstein et al. 2000; Schiffman, Nenkova, and McKeown 2002). The summary should provide the topic and an overview of the summarized documents by identifying the important and interesting aspects of these documents. Document summarizers generally evaluate and extract items of information from documents according to their relevance to a particular request (such as a request for a person or an event) and address discourse related issues such as removing redundancies (Radev et al. 2004) and ordering sentences (Barzilay, Elhadad, and McK"
J12-3004,A97-1041,0,0.198943,"Missing"
J12-3004,W98-1411,0,0.0675056,"Missing"
J12-3004,J93-4004,0,0.464877,"Missing"
J12-3004,N03-2024,0,0.0917821,"Missing"
J12-3004,J88-3006,0,0.496121,"Missing"
J12-3004,W07-2315,0,0.136716,"content in natural language. Particular attention is devoted to our methodology for generating referring expressions for certain graphical elements such as a descriptor of what is being measured in the graphic. Section 8 presents a user study that was conducted to evaluate the effectiveness of the generated summaries for the purposes of this research by measuring readers’ comprehension. Section 9 concludes the article and outlines our future work. 2. Background 2.1 Related Work There has been a growing interest in language systems that generate textual summaries of non-linguistic input data (Reiter 2007). The overall goal of these systems, generally referred to as data-to-text systems, is to enable efﬁcient processing of large volumes of numeric data by supporting traditional visualisation modalities and to reduce the effort spent by human experts on analyzing the data. Various examples of datato-text systems in the literature include systems that summarize weather forecast data (Goldberg, Driedger, and Kittredge 1994; Coch 1998), stock market data (Kukich 1983), and georeferenced data (Turner, Sripada, and Reiter 2009). One of the most successful data-to-text generation research efforts is t"
J12-3004,P04-1011,0,0.0741627,"Missing"
J12-3004,W98-1419,0,0.0654086,"Missing"
J12-3004,J94-2006,1,0.474029,"Missing"
J12-3004,W09-0607,0,0.0295757,"Missing"
J12-3004,W09-0629,0,\N,Missing
J12-3004,X98-1026,0,\N,Missing
J88-3005,P84-1045,0,0.0627195,"Missing"
J88-3005,J84-1001,0,0.0645579,"pe) and a characterization of the relevant knowledge pool (again, a simple test) a response schema is chosen for the response. The response schema dictates the textual 7 HIGHLIGHTINGAND OBJECT SIMILARITY We claim that in order for the above strategy for correcting misconceptions to work, the similarity metric that is used to assess object similarity must be affected by the preceding discourse. To date, most AI systems do not assess object similarity in a way that is context dependent. Several systems that do assess object similarity (Rumelhart and Abrahamson 1973, McKeown 1982, Carberry 1984, Weiner 1984) use a metric based on distance in some space. Most often, this space is the generalization hierarchy. Basically, two objects that have a common immediate superordinate (i.e., are siblings in the hierarchy) are seen as very similar, while objects whose lowest common ancestor is several levels up in the hierarchy are seen as quite different. One problem with this metric arises when objects can be classified in more than one way and there are several lowest common ancestors of the objects being compared. A decision must be made about which of these lowest common ancestors should be considered si"
J88-3005,P83-1007,0,\N,Missing
J94-2006,P87-1022,0,0.958238,"Missing"
J94-2006,H86-1012,0,0.155602,"Missing"
J94-2006,J86-3001,0,0.641414,"Linguistics Volume 20, Number 2 Subsequent to Sidner&apos;s work, Grosz, Joshi, and Weinstein (1983) introduced centering to account for the same phenomena addressed by Sidner&apos;s algorithm. 1 Centering attempted to simplify processing by keeping fewer data structures than Sidner&apos;s framework did. In particular, the centering literature claims that, rather than two foci, only one focus is needed, termed the backward-lookingcenter (Cb). Pronoun resolution within the centering framework is largely based on an ordering of preferred focus (centering) moves. Other research on discourse (e.g., Grosz 1981; Grosz and Sidner 1986; Reichman 1978) has studied another phenomenon, the globalfocus of discourse. The term global focus generally refers to the entity or set of entities that are relevant to or salient in the overall discourse; the identification of global focus typically interacts with the identification of discourse segments. Global focus and discourse segmentation are distinct from the phenomenon of local focusing that is addressed in this paper. However, we should point out that the centering literature has noted that centering ""... is intended to operate within a [discourse[ segment"" (Walker 1989, p. 253)."
J94-2006,P83-1007,0,0.750807,"Missing"
J94-2006,P86-1031,0,0.740954,"that native speakers of English preferred in an informal poll. 305 Computational Linguistics Volume 20, Number 2 Table 1 Centering transitions. Cb(Un)=Cp(Un) Cb(Un)¢Cp(Un) Cb(Un)=Cb(Un-1) Cb(Un)#Cb(Un-1) Continue Smooth-Shift Retain (Rough-)Shift No-Cb: No element in Un realizes an element of Cf(Un-l). 3. Centering In this section, we give a brief introduction to the centering framework. This framework was introduced by Grosz, Joshi, and Weinstein (1983), it has been discussed a n d / o r expanded on in several other works, including Brennan, Friedman, and Pollard (1987), Walker (1989, 1993), Kameyama (1986, 1993), Walker, Iida, and Cote (1992), Brennan (1993), Kameyama, Passonneau, and Poesio (1993), Linson (1993), and Hoffman and Turan (1993). 3.1 Computing Centers For each utterance, centering computes the following (Grosz, Joshi, and Weinstein 1983; Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1992): the backward-looking center (Cb), which is intended to capture that item which ties the current sentence in with the previous sentence in the discourse, and a list of forward-looking centers (Cf), or elements that can potentially be the Cb of the next sentence. For English, the Cf"
J94-2006,P93-1010,0,0.362012,"Missing"
J94-2006,P92-1035,1,0.925545,"problem that precludes undertaking a corpora analysis: although complex sentences are prevalent in written English, neither framework has explicitly specified how to handle complex sentences. 13 In fact, some previous research handled complex sentences in an inconsistent manner (Sidner 1979; Linson 1993). Other research made claims that were not accurate because it did not acknowledge the need to specify how to process complex sentences. In particular, centering literature (Grosz, Joshi, and Weinstein 1983) questioned Sidner&apos;s use of an Actor Focus using text that involved complex sentences. Suri (1992) argued that the reason Sidner&apos;s algorithms could not handle that text was not because they used two foci, but because a method for processing of complex sentences was not speci~ed. The central question is whether one should process a complex sentence as multiple sentences or as a single sentence. For instance, should the clauses of a complex sentence be processed in linear order, updating the focusing (e.g., RAFT/RAPR or centering) data structures as if one were processing a linear sequence of simple sentences? Or, is some other order more appropriate? More specifically, 1) How do you resolve"
J94-2006,P89-1031,0,0.865484,"Grosz and Sidner 1986; Reichman 1978) has studied another phenomenon, the globalfocus of discourse. The term global focus generally refers to the entity or set of entities that are relevant to or salient in the overall discourse; the identification of global focus typically interacts with the identification of discourse segments. Global focus and discourse segmentation are distinct from the phenomenon of local focusing that is addressed in this paper. However, we should point out that the centering literature has noted that centering ""... is intended to operate within a [discourse[ segment"" (Walker 1989, p. 253). In our work on RAFT/RAPR we do not restrict the domain of the algorithms to within a discourse segment. Given that multiple frameworks for focus tracking and pronoun resolution have emerged, we would like to do a comparison to see how the frameworks are the same and how they differ. Previous assessments and comparisons of local focusing frameworks have relied on comparing how frameworks process a small number of constructed discourses, but this kind of comparison is inadequate. Instead the question that must be answered is which framework performs best on naturally occurring text. H"
J94-2006,J94-2003,0,\N,Missing
J99-2001,P87-1022,0,0.87577,"Missing"
J99-2001,H86-1012,0,0.105085,"Missing"
J99-2001,P83-1007,0,0.0354717,"Missing"
J99-2001,J95-2003,0,0.404193,"Missing"
J99-2001,P93-1010,0,0.0267801,"Missing"
J99-2001,J94-4002,0,0.879994,"person, object, property, or concept that a sentence is most centrally about within the discourse context in which it occurs. The appropriate movement and marking of local focus, and the appropriate choice of the form of a noun phrase (NP) based on local focus information, are considered to contribute to the local coherence exhibited by discourse (Sidner [1979], Grosz, Joshi, and Weinstein [1983, 1995], Carter [1987], and others). In addition, local focus information is one source of information that is used by readers and hearers for interpreting pronouns. Some researchers (e.g., Hobbs 1978; Lappin and Leass 1994) have proposed pronoun resolution algorithms that do not involve focus tracking. However, our view is that local focus tracking and pronoun resolution are mutually dependent processes. The local focus information influences pronoun resolution, and pronoun resolution, in turn, influences updating focus information. Therefore, the tracking of local focus is crucial for the interpretation of pronouns. * 1807 Park 270 Drive, Suite 350 St. Louis, MO 63146. E-mail: suri@nexen.com t Department of Computer and Information Sciences, University of Delaware, Newark DE 19716. E-mail: mccoy@cis.udel.edu De"
J99-2001,J88-2004,0,0.0633061,"Missing"
J99-2001,P86-1004,0,0.0575903,"Missing"
J99-2001,P96-1057,0,0.0263608,"ritten English, most other local focusing research (focusing: Sidner [1979] and Carter [1987]; centering: Grosz, Joshi, and Weinstein [1983, 1995], Brennan, Friedman, and Pollard [1987], Walker [1989, 1993], Kameyama [1986] 2, Walker, Iida, and Cote [1994], Brennan [1998], Kameyama, Passonneau, and Poesio [1993], Linson [1993] and Hoffman [1998]; and PUNDIT: Dahl [1986], Palmer et al. [1986], and Dahl and Ball [1990]) did not explicitly a n d / o r adequately address how to process complex sentences. Thus, there is a need to extend focusing algorithms. An exception to this rule is the work of Strube (1996) (which applies functionalinformation-structure-based criteria on a per-clause basis), Kameyama (1998), and Strube (1998). Kameyama&apos;s focus was on intrasentential anaphora and she attempted to define an &quot;utterance&quot; in the face of syntactic complexity. Strube, in very recent work (Strube 1998), handles arbitrary sentence complexity. Still, neither considers how particular types of complexity might affect a broad range of focusing factors. Notice that there are a number of ways that a given type of complex sentence might be handled by a focusing algorithm. For instance, consider processing a com"
J99-2001,P98-2204,0,0.842686,"nd Weinstein [1983, 1995], Brennan, Friedman, and Pollard [1987], Walker [1989, 1993], Kameyama [1986] 2, Walker, Iida, and Cote [1994], Brennan [1998], Kameyama, Passonneau, and Poesio [1993], Linson [1993] and Hoffman [1998]; and PUNDIT: Dahl [1986], Palmer et al. [1986], and Dahl and Ball [1990]) did not explicitly a n d / o r adequately address how to process complex sentences. Thus, there is a need to extend focusing algorithms. An exception to this rule is the work of Strube (1996) (which applies functionalinformation-structure-based criteria on a per-clause basis), Kameyama (1998), and Strube (1998). Kameyama&apos;s focus was on intrasentential anaphora and she attempted to define an &quot;utterance&quot; in the face of syntactic complexity. Strube, in very recent work (Strube 1998), handles arbitrary sentence complexity. Still, neither considers how particular types of complexity might affect a broad range of focusing factors. Notice that there are a number of ways that a given type of complex sentence might be handled by a focusing algorithm. For instance, consider processing a complex sentence of the form &quot;SX because SY,&quot; where SX and SY each consist of a single clause. One might imagine processing"
J99-2001,J94-2006,1,0.860768,"Missing"
J99-2001,P89-1031,0,0.511927,"the focusing factors discussed in the previous section. On the other hand, pronoun resolution algorithms dependent on the kind of focusing information discussed in the previous section are affected by complex sentence structures. Thus we must face the challenge of determining how the focusing algorithm (and pronoun resolution algorithms) should act in the face of the complexity introduced by a type of complex sentence. Other researchers have attempted to make use of corpus analyses in focusing research (though they did not attempt to extend the various algorithms to handle complex sentences). Walker (1989) performed a corpus analysis on written and spoken English to compare centering with Hobbs&apos;s algorithm (Hobbs 1976) in terms of their accuracy and coverage for finding the cospecifiers of pronouns. Walker also performed a corpus analysis (on spoken English) to investigate how centering should process a sentence beginning with the word Now, which she assumes (frequently) marks a new discourse segment (Walker 1993). Linson (1993) analyzed a corpus of spoken data to investigate focus transition patterns. (Previous work on centering assumed a particular priority on focus transition possibilities.)"
J99-2001,J94-2003,0,0.493472,"Missing"
J99-2001,H86-1011,0,\N,Missing
J99-2001,P86-1031,0,\N,Missing
J99-2001,C98-2199,0,\N,Missing
P01-1051,J96-2004,0,0.0109094,"correspondence with the system grammar) 3. Divide samples into the levels of acquisition they represent 4. Statistically analyze errors within each level and compare to the magnitude of occurrence at other levels 5. Analyze resulting findings to determine a progression of competence In (Michaud et al., 2001) we discuss the initial steps we took in this process, including the development of a list of error codes documented by a coding manual, the verification of our manual and coding scheme by testing inter-coder reliability in a subset of the corpus (where we achieved a Kappa agreement score (Carletta, 1996) of )2 , and the subsequent tagging of the entire corpus. Once the corpus was annotated with the errors each sentence contained, we obtained expert evaluations of overall proficiency levels performed by ESL instructors using the national Test of Written English (TWE) ratings3 . The initial analysis we go on to describe in (Michaud et al., 2001) confirmed that clustering algorithms looking at the relative magnitude of different errors grouped the samples in a manner which corresponded to where they appeared in the spectrum of proficiency represented by the corpus. The next step, the results of"
P01-1051,W99-0408,1,0.766927,"(Dulay and Burt, 1974; Dulay and Burt, 1975), and some research has focused on developing a general account of acquisition across a broad range of morphosyntactic structures (cf. (Pienemann and H˚akansson, 1999)). In this work, we explore how our second language instruction system, ICICLE, has generated the need for modeling such an account, and we discuss the results of a corpus analysis we have undertaken to fulfill that need. 1.1 ICICLE: an Overview ICICLE (Interactive Computer Identification and Correction of Language Errors) is an intelligent tutoring system currently under development (Michaud and McCoy, 1999; Michaud et al., 2000; Michaud et al., 2001). Its primary function is to tutor deaf students on their written English. Essential to performing that function is the ability to correctly analyze user-generated language errors and produce tutorial feedback to student performance which is both correct and tailored to the student’s language competence. Our target learners are native or near-native users of American Sign Language (ASL), a distinct language from English (cf. (Baker and Cokely, 1980)), so we view the acquisition of skills in written English as the acquisition of a second language for"
P01-1051,P98-2196,1,0.826276,"Model representing the user’s grammatical competence in written English. 1.2 A Need for Modeling L2A Status What currently exists of the ICICLE system is a prototype application implemented in a graphical interface connected to a text parser that uses a wide-coverage English grammar augmented by “mal-rules” capturing typical errors made by our learner population. It can recognize and label many grammatical errors, delivering “canned” one- or two-sentence explanations of each error on request. The user can then make changes and resubmit the piece for additional analysis. We have discussed in (Schneider and McCoy, 1998) the performance of our parser and mal-ruleaugmented grammar and the unique challenges “She is teach piano on Tuesdays.” Beginner: Inappropriate use of auxiliary and verb morphology problems. “She teaches piano on Tuesdays.” Intermediate: Missing appropriate +ing morphology. “She is teaching piano on Tuesdays.” Advanced: Botched attempt at passive formation. “She is taught piano on Tuesdays.” Figure 2: Possible interpretations of nongrammatical user text. faced when attempting to cover non-grammatical input from this population. In its current form, when the parser obtains more than one possib"
P01-1051,C98-2191,1,\N,Missing
P08-2066,W97-0504,0,0.0593891,"ication (AAC) device. AAC devices seek to address the dual problem of speech and motor impairment by attempting to optimize text input. Even still, communication rates with AAC devices are often below 10 words per minute (Newell et al., 1998), compared to the common 130-200 words per minute speech rate of speaking people. Word prediction addresses these issues by reducing the number of keystrokes required to produce a message, which has been shown to improve communication rate (Trnka et al., 2007). The reduction in keystrokes also translates into a lower degree of fatigue from typing all day (Carlberger et al., 1997). Word prediction systems present multiple completions of the current word to the user. Systems generate a list of W predictions on the basis of the word being typed and a language model. The vocabulary is filtered to match the prefix of the current word and the language model ranks the words according to their likelihood. In the case that no letters of the current word have been entered, the language model is the sole factor in generating predictions. Systems often use a touchscreen or function/number keys to select any of the predicted words. Because the goal of word prediction systems is to"
P08-2066,W97-0506,0,0.0640991,"Missing"
P08-2066,wandmacher-antoine-2006-training,0,0.0527482,"Missing"
P08-2066,N07-2044,1,\N,Missing
P82-1029,P82-1028,0,\N,Missing
P86-1016,P84-1076,0,0.054882,"Missing"
P86-1016,P83-1007,0,\N,Missing
P92-1007,H86-1019,0,0.0156023,"operations necessary to build the string from its elementary components. The corresponding syntactic structure is then generated by doing Mumble-86 (McDonald & Pustejovsky, 1985; Meteer et al., 1987) is a sentence generator based on TAG that is able to take more than just the logical form representation into account. Mumble-86 is one of the foremost sentence generation systems and it (or its predecessors) has been used as the sentence generation components of a number of natural language generation projects (e.g., (McDonald, 1983; McCoy, 1989; Conklin & McDonald, 1982; Woolf& McDonald, 1984; Rubinoff, 1986)). After briefly describing the methodology in Mumble-86, we will point out some problematic aspects of its design. We will then describe our architecture which is based on interfacing TAG with a rich functional theory provided by functional systemic grammar (Halliday, 1970; Halliday, 1985; Fawcett, 1980; Hudson, 1981). 2 We pay particular attention to those aspects which distinguish our generator from Mumble-86. 2 Mumble-86 Mumble-86 generates from a specification of what is to be said in the form of an ""L-Spec"" 1This work is supported ill part by Grant #H133E80015 from the National hlstitute"
P92-1007,C88-2121,0,0.0886646,"Missing"
P92-1007,J90-1004,0,0.0650462,"Missing"
P92-1007,W90-0101,1,0.897423,"Missing"
P92-1007,P85-1012,0,\N,Missing
P92-1007,P82-1030,0,\N,Missing
P98-2196,C94-1042,0,0.0168773,"me, and none of the values in Det will be allowed to be in the agreement values for 1202 the NP and the DP. This will allow the rule to fire precisely when there are no possible ways to unify the values between the Det and the NP, i.e. none of the a g r values for the Det will be allowed in the variable ?a. Thus, this rule will only fire for ungrammatical constructions. 4 Grammar Coverage/User Interface The ICICLE grammar is a broad-coverage grammar designed to parse a wide variety of both grammatical sentences and sentences containing errors. It is built around the COMLEX Syntax 2.2 lexicon (Grishman et al., 1994), which contains approximately 38,000 different syntactic head words. We have a simple set of rules that allows for inflection, thereby doubling the number of noun forms, while giving us three to four times as many verb forms as there are heads. Thus we can handle approximately 40,000 noun forms, 8,000 adjectives, and well over 15,000 verb forms. In addition, unknown words coming into the system are assumed to be proper nouns, thus expanding the number of words handled even further. The grammar itself contains approximately 25 different adjectival subcategorizations, including subcategorizatio"
W00-1438,W97-0703,0,0.747284,"or generating natural text summaries from large documents, several issues must be examined in detail. First, an analysis of the quality of the intermediate representation for use in generation must be examined. Second, a detailed examination of the processes which link the intermediate representation to a potential final summary must be undertaken. The system presented here provides a useful first step towards these ends. By developing a robust and efficient tool to generate these intermediate representations, we can both evaluate the representation Also using WordNet in their implenmntation. Barzilay and Elhadad (1997) dealt with some of tile limitations in Hirst and St-Onge&apos;s algorithm by examining every possible lexical chain which could be computed, not just those possible at a given point in the text. That is to say, while Hirst and St.Onge would compute the chain in which a word should be placed when a word was first encountered, Barzilay and Elhadad computed ever:,&apos; possible chain a word could become a member of when the word was encountered, and later determined the best interpretation. 268 2 2.1 A Linear T i m e A l g o r i t h m for Computing Lexical Chains Overview Our research on lexical chains a"
W00-1438,P94-1002,0,0.0780542,"word instance is added, its contribution, which is dependent on the scoring metrics used, is added to the &quot;metachain&quot; score. The contribution is then stored within Intra Intra Adjacent Pgrph. Segment Segment Other Same 1 1 1 1 Synonym Hypernym Hyponym Sibling 1 I 1 1 1 1 1 0 0 0 0 0 &quot;O &quot; 0 0 0 Table 1: D y n a m i c Scoring Metrics Set to Mimic B+E&apos;s Algorithm the word itself. These scores are dynamic and can b e set ~ a s e d ,on:segmentation information, dista.nce, and t y p e of relation. Currently, segmentation is accomplished prior to using our algorithm by executing Hearst&apos;s text tiler (Hearst, 1994). The sentence numbers of each segm e n t b o u n d a r y are stored for use by our algorithm. These sentence numbers are used in conjunction with relation t y p e as keys into a table of potential scores. Table 1 denotes sample metrics tuned to simulate the s y s t e m devised by Barzilay and E l h a d a d (1997). A t this point, the collection of &quot;meta-chains&quot; contalns all possible interpretations of the source document. T h e problem is t h a t in our final represent a t i o n , each word instance can exist in only one chain. To figure out which chain is the correct one, each word is examin"
W00-1438,J91-1002,0,0.715352,"ontrolled automatically, or manually based on length or percentage of compression. While Early methods using word frequency counts did still under development, the system provides useful not consider the relations between similar words. summaries which compare well in information conFinding the aboutness of a document requires findtent to human generated summaries. Additionally, ing these relations. How these relations occur within the system provides a robust test bed for future suma document is referred to as cohesion (Halliday mary generation research. and Hasan, 1976). First introduced by Morris and Hirst (1991), lexical chains represent lexical cohe1 Introduction sion among related terms within a corpus. These relations can be recognized by identifying arbitrary Automatic text summarization has long been viewed size sets of words which are semantically related (i.e., as a two-step process. First, an intermediate reprehave a sense flow). These lexical chains provide an sentation of the summary must be created. Second, interesting method for summarization because their a natural language representation of the summary recognition is easy within the source text and vast must be generated using the inter"
W00-1438,W00-1438,1,0.0512347,"red chains correspond to the important concepts in the original document. These i m p o r t a n t concepts can be used to generate a s u m m a r y from the source text. Barzilay and E l h a d a d use the notion of strong chains (i.e., chains whose scores are in excess of two s t a n d a r d deviations above the mean of all scores) to d e t e r m i n e which chains to include in a summary. O u r system can use this method, as well as several other methods including percentage compression and number of sentences. For a more detailed description of our algorithm please consult our previous work (Silber and McCoy, 2000). 2.4 Runtime Analysis In this analysis, we will not consider the computational complexity of p a r t of speech tagging, as that is not the focus of this research. Also, because the size 269 C1 C2 Ca C4 C5 C6 =No. of senses =Parent/child isa relations =No. of nouns in WordNet =No. of synsets in WordNet =No. of siblings =Chains word can belong to Worst Case 30 ,45147 94474 66025 397 45474 Average Case 2 t4 94474 66025 39 55 Table 2: Constants from WordNet and structure of WordNet does not change from exe c u t i o n to execution of.aJae.algorit, hm, we shall take these aspects of WordNet to be"
W03-1601,1993.tmi-1.14,0,0.70528,"propriate realizations be produced. Di erent placement of argument realizations Sometimes di erent synonyms, like the verbs enjoy and please, place argument realizations di erently with respect to the head, as illustrated in (2a-2b). (2a) Amy enjoyed the interaction. (2b) The interaction pleased Amy. To handle this variety, a uniform generation methodology should not assume a xed mapping between thematic and syntactic roles but let each lexical item determine the placement of argument realizations. Generation systems that use such a xed mapping must override it for the divergent cases (e.g., (Dorr, 1993)). Words with overlapping meaning There are often cases of di erent words that realize different but overlapping semantic pieces. The easiest way to see this is in what has been termed incorporation, where a word not only realizes a predicate but also one or more of its arguments. Di erent words may incorporate di erent arguments or none at all, which may lead to paraphrases, as illustrated in (3a-3c). (3a) Charles ew across the ocean. (3b) Charles crossed the ocean by plane. (3c) Charles went across the ocean by plane. Notice that the verb y realizes not only going but also the mode of transp"
W03-1601,J97-2001,0,0.424341,"various ways using di erent lexical and syntactic means. These di erent realizations, called paraphrases, vary considerably in appropriateness based on pragmatic factors and communicative goals. If a generator is to come up with the most appropriate realization, it must be capable of generating all paraphrases that realize the input semantics. Even if it makes choices on pragmatic grounds during generation and produces a single realization, the ability to generate them all must still exist. Variety of lexical and grammatical forms of expression pose challenges to a generator ((Stede, 1999); (Elhadad et al., 1997); (Nicolov et al., 1995)). In this paper, we discuss the generation of single-sentence paraphrases realizing the same semantics in a uniform fashion using a simple sentence generation architecture. In order to handle the various ways of realizing meaning in a simple manner, we believe that the generation architecture should not be aware of the variety and not have any special mechanisms to handle the di erent types of realizations1 . Instead, we want all lexical and grammatical variety to follow automatically from the variety of the elementary building blocks of generation, lexico-grammatical"
W03-1601,W02-2121,1,0.626592,"domination of length zero or more where syntactic material (e.g., modi ers) may end up. critical for combining realizations (in step 3 of our algorithm in section 4.1). There are, however, advantages that our approach has. For one, we are not constrained by the isomorphism requirement in a Synchronous TAG derivation. Also, the DSG formalism that we use a ords 4.4 Using resources in our algorithm greater exibility, signi cant in our approach, as discussed later in this paper (and in more detail Step 1 of our algorithm requires matching the semantic side of a resource against the top of the in (Kozlowski, 2002b)). input and testing selectional restrictions. A se4.3 The grammatical formalism mantic side matches if it can be overlaid against Both step 3 of our algorithm (putting re- the input. Details of this process are given alizations together) and the needs of lexico- in (Kozlowski, 2002a). Selectional restrictions grammatical resources (the encapsulation of (type restrictions on arguments) are associated syntactic consequences such as the position with nodes on the semantic side of resources. of argument realizations) place signi cant de- In their evaluation, the appropriate knowledge mands on t"
W03-1601,J01-1004,1,0.808879,"restrictions on arguments) are associated syntactic consequences such as the position with nodes on the semantic side of resources. of argument realizations) place signi cant de- In their evaluation, the appropriate knowledge mands on the grammatical formalism to be used base instance is accessed and its type is tested. in the implementation of the architecture. One More details about using selectional restrictions grammatical formalism that is well-suited for in generation and in our architecture are given our purposes is the D-Tree Substitution Gram- in (Kozlowski et al., 2002). mars (DSG, (Rambow et al., 2001)), a variant Resources for enjoy and please which match of Tree-Adjoining Grammars (TAG). This for- the top of the input in Fig. 1 are shown in malism features an extended domain of locality Fig. 2. In doing the matching, the arguments and exibility in encapsulation of syntactic con- AMY and INTERACTION are uni ed with X and sequences, crucial in our architecture. Y. The dashed outlines around X and Y indicate Consider the elementary DSG structures on that the resource does not realize them. Our althe right-hand-side of the resources for enjoy gorithm calls for the independent recursive realan"
W03-1601,C90-3045,0,0.0397844,"sides contain the verbs enjoy and please in (Nicolov et al., 1995); (Stone and Doran, 1997)). the active voice con guration. The mappings include links between ENJOY and its realization as well as links between the unrealized agent (X) 4.1 Our algorithm Our generation algorithm is a simple, recursive, or theme (Y) and the subject or the complement. Our mapping between semantic and syntactic semantic-head-driven generation process, consistent with the approach described in section 2, constituents bears resemblance to the pairings in but one driven by the semantic input and the Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is lexico-grammatical resources. S S ENJOY EXPERIENCER X NP0 NP0 VP0 THEME VP0 NP0 VP0 VP1 VP1 Y V◆ enjoy S NP1 VP1 NP1 V◆ enjoy V◆ please NP1 S ENJOY EXPERIENCER X NP0 VP0 THEME Amy VP1 Y V◆ please NP1 Figure 2: Two di erent resources for ENJOY the interaction Figure 3: Combining argument realizations with the resources for enjoy and please arguments will be substituted. The positions of both the subject and the complement are encapsulated in these elementary structures. This allows the mapping between semantic and syntactic constituents to be de ne"
W03-1601,P97-1026,0,0.0328712,"dicate ENJOY source, generation can proceed no matter what and the thematic roles EXPERIENCER and THEME. choices are speci ed about these in each indi- The arguments lling those roles (which must be vidual resource. Our approach is fundamen- realized separately, as indicated by dashed outtally di erent from systems that reason directly lines) appear as variables X and Y which will be about syntax and build realizations by syntactic matched against actual arguments. The syntacrank ((Bateman, 1997), (Elhadad et al., 1997); tic sides contain the verbs enjoy and please in (Nicolov et al., 1995); (Stone and Doran, 1997)). the active voice con guration. The mappings include links between ENJOY and its realization as well as links between the unrealized agent (X) 4.1 Our algorithm Our generation algorithm is a simple, recursive, or theme (Y) and the subject or the complement. Our mapping between semantic and syntactic semantic-head-driven generation process, consistent with the approach described in section 2, constituents bears resemblance to the pairings in but one driven by the semantic input and the Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is lexico-grammatical"
W03-1601,W90-0102,0,\N,Missing
W03-2101,J80-3003,0,0.601989,"wedges and lines by their proximity to the chart component in question. The output of the visual extraction component is an XML file that describes the chart and all of its components. 4.4 Applying Discourse Understanding Strategies Many researchers have cast the understanding of discourse and dialogue as a plan recognition problem — that is, the writer or speaker (or characters in the case of a story) has an underlying goal and a plan for accomplishing that goal, and understanding requires that the reader or listener infer the plan and in turn the goal that the plan is intended to achieve. (Perrault and Allen, 1980; Wilensky, 1983; Litman and Allen, 1987; Carberry, 1990; Charniak and Goldman, 1993; Ardisonno and Sestero, 1996) are just a few examples of such systems. Since understanding information graphics is a discourse-level problem, we are extending plan inference techniques to recognizing the intended message of an information graphic(Elzer et al., 2003) and to identifying its contribution to an extended discourse that includes both text and graphics. Planning and plan inference systems require knowledge about goals and how they can be achieved. Typically, this is provided by a library of operators"
W03-2101,W00-1438,1,0.864082,"ibutes of these highlighted items (for example, the attributes of a highlighted bar in a bar chart), which are captured in the XML represen3 Verb phrases in captions also provide evidence, but they suggest particular operators of interest rather than instantiated perceptual tasks, and thus we associate verbs with operators in the plan library. tation of the graphic, are also regarded as salient entities. Salient entities also include those that world knowledge suggests are mutually believed to be of interest to the viewing audience. We envision in the future using the notion of lexical chains(Silber and McCoy, 2000) to identify entities that the accompanying text makes particularly salient. Perceptual tasks that are instantiated with a salient entity and that can be performed on the graphic are designated salient tasks. 4.4.2 The Search Process Candidate tasks consist of the set of perceptual tasks that require the least effort and the set of salient tasks. Once the set of candidate tasks has been identified, plan inference begins. Initial candidate plans are constructed from each operator in which a candidate task appears as a subgoal; the root of the candidate plan is the goal of the operator, and its"
W03-2101,J99-1001,1,\N,Missing
W03-2101,J86-3001,0,\N,Missing
W04-1002,P99-1071,0,0.0394275,"Focused entities that appear as instantiations of parameters in perceptual or cognitive tasks serve as evidence that those tasks might be particularly salient. Similarly, verbs that appear in captions serve as evidence for the salience of particular tasks. For example, the verb beats in a caption such as “Canada Beats Europe” serves as evidence for the salience of a Recognize relative difference task. In the future, we plan to capture the influence of surrounding text by identifying the important concepts from the text using lexical chains. Lexical chains have been used in text summarization (Barzilay et al., 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. Whether a task is salient and the method by which it was made salient are used as evidence in our plan inference system. The graphic design makes some tasks easier than others. We use a set of rules, based on research by cognitive psychologists, to estimate the relative effort of performing different perceptual and cognitive tasks. These rules, described in (Elzer et al., 2004), have been validated by eye-tracking experiments. Since the viewer is intended to recognize the message tha"
W04-1002,J00-3005,0,0.0215232,"istical techniques and identification and extraction of key sentences from documents. However, it is widely acknowledged that to truly understand a text and produce the best summary, one must understand the document and recognize the intentions of the author. Recent work in text summarization has Delaware bankruptcy personal filings 3000 2500 2000 1500 1000 1998 1999 2000 2001 Figure 1: Graphic from a City Newspaper Median Income In thousands of 2001 dollars $15 White women 10 Black women 5 1948 60 70 80 90 01 Figure 2: Graphic from Newsweek Magazine begun to address this issue. For example, (Marcu, 2000) presents algorithms for automatically identifying the rhetorical structure of a text and argues that the hypothesized rhetorical structure can be successfully used in text summarization. Information graphics are an important component of many documents. In some cases, information graphics are stand-alone and constitute the entire document. This is the case for many graphics appearing in newspapers, such as the graphic shown in Figure 1. On the other hand, when an article is comprised of text and graphics, the graphic generally expands on the text and contributes to the discourse purpose (Gros"
W04-1002,J02-4004,1,0.835636,"eters in perceptual or cognitive tasks serve as evidence that those tasks might be particularly salient. Similarly, verbs that appear in captions serve as evidence for the salience of particular tasks. For example, the verb beats in a caption such as “Canada Beats Europe” serves as evidence for the salience of a Recognize relative difference task. In the future, we plan to capture the influence of surrounding text by identifying the important concepts from the text using lexical chains. Lexical chains have been used in text summarization (Barzilay et al., 1999), and our linear time algorithm (Silber and McCoy, 2002) makes their computation feasible even for large texts. Whether a task is salient and the method by which it was made salient are used as evidence in our plan inference system. The graphic design makes some tasks easier than others. We use a set of rules, based on research by cognitive psychologists, to estimate the relative effort of performing different perceptual and cognitive tasks. These rules, described in (Elzer et al., 2004), have been validated by eye-tracking experiments. Since the viewer is intended to recognize the message that the graphic designer wants to convey, we contend that"
W04-1002,J86-3001,0,\N,Missing
W08-1103,W96-0501,0,0.473984,"y that our system generates for the graphic in Figure 2 before the movements between classes, and Table 2 presents the summary after the movements. 5 10 percent 8 6 4 2 0 ’95 ’97 ’98 ’99 ’00 ’01 ’02 Figure 5: Graphic conveying a contrast change. likely to contribute to the descriptor than the texts at the higher levels. We developed a set of heuristics and augmentation rules for constructing the descriptor for the dependent axis and validated them on a previously unseen corpus of graphics. Realizing Summaries To realize the summaries in natural language, we use the FUF/SURGE surface realizer (Elhadad and Robin, 1996) with some changes made to address a few problems encountered with respect to the use of conjunctions and subject-ellipsises. Different strategies are defined in the system for aggregating the realizations of trees that are linked with operators. The strategy selected by the system is based on the relation (such as concession) that holds between the propositions at the root of the trees and the syntactic forms of their realization opportunities. For example, the system uses different strategies for aggregating the trees rooted by the And predicates in Figure 4, where the tree rooted by And(per"
W08-1103,J86-3001,0,0.155274,"propositions conveying the rate of increase in the graphic in Figure 2. Once the propositions to be included in the summary are identified, we assign them to one of these three classes. We hypothesize that the message-related class of propositions should be presented first since this places emphasis on the core message of the graphic. We anticipate that the user will ask follow-up questions after receiving the initial summary. Therefore, it is appropriate to close the initial summary with propositions from the computational class so that the whole graphic is in the user’s focus of attention (Grosz and Sidner, 1986). Thus we hypothesize that a good ordering of propositions in the initial summary is the message-related class, the specific class, and finally the computational class. This produces a partial ordering of the propositions to be included in the summary. Each proposition can be realized as a single sentence. For example, shows(graphic,trend) can be realized as “The graphic shows a trend” or “There is a trend in the graphic”. Consequently, a set of propositions can be viewed as a set of single sentences. Figure 3 shows the propositions in the message related class for the graphic in Figure 2, alo"
W08-1103,W07-1001,0,\N,Missing
W09-2819,W07-2302,0,0.0955946,"references. This application demonstrates one way to help bridge the gap between computational and empirical means of reference generation. 1 Introduction This paper provides a system report on our submission for the GREC-MSR (Main Subject References) Task, one of the two shared task competitions for Generation Challenges 2009. The objective is to select the most appropriate reference to the main subject entity from a given list of alternatives. The corpus consists of introductory sections from approximately 2,000 Wikipedia articles in which references to the main subject have been annotated (Belz and Varges, 2007). The training set contains articles from the categories of cities, countries, mountains, people, and rivers. The overall purpose is to develop guidelines for natural language generation systems to determine what forms of referential expressions are most appropriate in a particular context. 2 Method The first step of our approach was to perform a literature survey of psycholinguistic research related to the production of referring expressions by human beings. Our intuition was that findings in this field could be used to develop a useful set of features with which to train a classifier system"
W09-2819,W99-0108,1,0.870207,"most appropriate in a particular context. 2 Method The first step of our approach was to perform a literature survey of psycholinguistic research related to the production of referring expressions by human beings. Our intuition was that findings in this field could be used to develop a useful set of features with which to train a classifier system to perform the GREC-MSR task. Several common factors governing the interpretation of pronouns were identified by multiple authors (Arnold, 1998; Gordon and Hendrick, 1998). These included Subjecthood, Parallelism, Recency, and Ambiguity. Following (McCoy and Strube, 1999), we selected Recency as our starting point and tracked the intervals between references measured in sentences. Referring expressions which were separated from the most recent reference by more than two sentences were marked as longdistance references. To cover the Subjecthood and Parallelism factors, we extracted the syntactic category of the current and three most recent references directly from the GREC data. This information also helped us determine if the entity was the subject of the sentence at hand, as well as the two previous sentences. Additionally, we tracked whether the entity was"
W09-2819,P04-1052,0,0.0311898,"the most recent reference by more than two sentences were marked as longdistance references. To cover the Subjecthood and Parallelism factors, we extracted the syntactic category of the current and three most recent references directly from the GREC data. This information also helped us determine if the entity was the subject of the sentence at hand, as well as the two previous sentences. Additionally, we tracked whether the entity was in subject position of the sentence where the previous reference appeared. Finally, we made a simple attempt at recognizing potential interfering antecedents (Siddharthan and Copestake, 2004) occurring in the current sentence and the text since that last reference. Observing the performance of prototyping systems led us to include boolean features indicating whether the reference immediately followed the words “and,” “but,” or “then,” or if it appeared between a comma and the word “and.” We also found that non-annotated instances of the entity’s name, which actually serve as references to the name itself rather than to the entity, factor into Recency. Figure 1 provides an example of such a “non-referential instance.” We added a feature to measure distance to these items, similar t"
W09-2821,W07-2302,0,0.0127509,"tification of interfering antecedents. Each subsequent system improved upon the results of the previous iteration. 1 Introduction This paper provides a system report on our submission for the GREC-NEG (Named Entity Generation) Task, one of the two shared task competitions for Generation Challenges 2009. The objective is to select the most appropriate reference to named entities from a given list of alternatives. The corpus consists of introductory sections from approximately 1,000 Wikipedia articles in which single and plural references to all people mentioned in the text have been annotated (Belz and Varges, 2007). The training set contains articles from the categories of Chefs, Composers, and Inventors. GREC-NEG differs from the other challenge task, GREC-MSR (Main Subject References), in that systems must now account for multiple entities rather than a single main subject, and the corpus includes only articles about persons rather than a variety of topics. 2 to consult findings in psycholinguistic research for guidance regarding appropriate feature selection for the production of referring expressions. We relied upon several common factors recognized by multiple authors (Arnold, 1998; Gordon and Hend"
W09-2821,W99-0108,1,0.729319,"ntors. GREC-NEG differs from the other challenge task, GREC-MSR (Main Subject References), in that systems must now account for multiple entities rather than a single main subject, and the corpus includes only articles about persons rather than a variety of topics. 2 to consult findings in psycholinguistic research for guidance regarding appropriate feature selection for the production of referring expressions. We relied upon several common factors recognized by multiple authors (Arnold, 1998; Gordon and Hendrick, 1998), including Subjecthood, Parallelism, Recency, and Ambiguity. We followed (McCoy and Strube, 1999) who stressed the importance of Recency in reference generation. Finally, we made a preliminary attempt at identifying potential interfering antecedents that could affect the Ambiguity of pronouns (Siddharthan and Copestake, 2004). As an initial attempt (UDel-NEG-1), we simply extended our GREC-MSR submission. By adapting our system to account for multiple entities and the slightly different data format, we were able to use the existing classifier to generate references for GREC-NEG. We suspected that accuracy could be improved by retraining the classifier, so our next system (UDel-NEG-2) adde"
W09-2821,P04-1052,0,0.0214838,"out persons rather than a variety of topics. 2 to consult findings in psycholinguistic research for guidance regarding appropriate feature selection for the production of referring expressions. We relied upon several common factors recognized by multiple authors (Arnold, 1998; Gordon and Hendrick, 1998), including Subjecthood, Parallelism, Recency, and Ambiguity. We followed (McCoy and Strube, 1999) who stressed the importance of Recency in reference generation. Finally, we made a preliminary attempt at identifying potential interfering antecedents that could affect the Ambiguity of pronouns (Siddharthan and Copestake, 2004). As an initial attempt (UDel-NEG-1), we simply extended our GREC-MSR submission. By adapting our system to account for multiple entities and the slightly different data format, we were able to use the existing classifier to generate references for GREC-NEG. We suspected that accuracy could be improved by retraining the classifier, so our next system (UDel-NEG-2) added entity and mention numbers as features to train on. Presumably, this could help distinguish between the main subject and secondary entities, as well as plural references. As all named entities are tagged in the GREC-NEG corpus,"
W10-1313,A00-1041,0,0.0206313,"n and potential answers. These systems usually do not rely on external knowledge sources and are limited in the amount of ontological information that can be included in the system. The questions are usually fact-based in form (e.g., “How tall is Mt. Everest?”). These systems take a question and query a potentially large set of documents (e.g., the World Wide Web) to find the answer. A common technique is to determine a question type (e.g., “How many …?” would be classified as ‘numerical’, whereas “Who was …?” would be classified as ‘person’, etc.) and then locate answers of the correct type (Abney et al., 2000; Kwok et al., 2001; Srihari and Li, 2000; Galea, 2003). Questions are also frequently reformulated for pattern matching (e.g., “Who was the first American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the que"
W10-1313,P06-1127,0,0.0147123,"ng a 100-word window surrounding the snippet. We are using only nonstop 104 words in the cluster, and weighting the words based on their total number of occurrences in the windows. These word clusters, along with the expanded baseline words, are used to locate and rank paragraphs in our question document. Our approach is similar in spirit to other researchers using the Web to identify semantic relations. Matsuo et al. (2006) looked at the number of hits of each of two words as a single keyword versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and extracting the relationship of the two words based on the pattern. Sahami and Heilman (2006) used the snippets from a word search to form a set of words weighted using TF/IDF, and then deter"
W10-1313,2020.acl-tutorials.8,0,0.0751272,"Missing"
W10-1313,W97-0704,0,0.0388899,"Missing"
W10-1313,N03-1020,0,0.0129491,"s now the entire content of the paragraph is considered relevant and thus we are trying to generate semantic relationships between the question and potentially unrelated text. While the system only allows us to define AOIs as rectangular areas (and thus we can’t do a sentence-bysentence analysis), we may wish to define AOIs as small as 2 lines of text to narrow in on exactly where subjects chose to focus. 4.2 Ranking System Refinement It is worth mentioning that, while a good deal of research has been done on evaluating the goodness of automatically generated text summaries (Mani et al.,2002; Lin and Hovy, 2003; Santos et al., 2004) our system is intended to mimic the actions of skimmers when answering questions, and thus our measure of goodness will be our system’s ability to recreate the retrieval of text focused on by our visual skimmers. This gives us a distinct advantage over other systems in measuring goodness, as defining a measure of goodness can prove difficult. In future work, we will be exploring different methods of ranking text such that the system returns results most similar to the results obtained from the visual skimming studies. The system will then be used on other questions and d"
W10-1313,W06-1664,0,0.0125281,"ords related semantically to the question, we are taking the top 50 URLs, going to their correlating Web page, locating the snippet of text within the page, and creating a cluster of words using a 100-word window surrounding the snippet. We are using only nonstop 104 words in the cluster, and weighting the words based on their total number of occurrences in the windows. These word clusters, along with the expanded baseline words, are used to locate and rank paragraphs in our question document. Our approach is similar in spirit to other researchers using the Web to identify semantic relations. Matsuo et al. (2006) looked at the number of hits of each of two words as a single keyword versus the number of hits using both words as keywords to rate the semantic similarity of two words. Chen et al. (2006) used a similar approach to determine the semantic similarity between two words: with a Web search using word P as the query term, they counted the number of times word Q occurred in the snippet of text returned, and vice versa. Bollegala et al. (2007) determined semantic relationships by extracting lexico-syntactic patterns from the snippets returned from a search on two keywords (e.g.,“’x’ is a ‘y’”) and"
W10-1313,W04-1012,0,0.0886258,"tent of the paragraph is considered relevant and thus we are trying to generate semantic relationships between the question and potentially unrelated text. While the system only allows us to define AOIs as rectangular areas (and thus we can’t do a sentence-bysentence analysis), we may wish to define AOIs as small as 2 lines of text to narrow in on exactly where subjects chose to focus. 4.2 Ranking System Refinement It is worth mentioning that, while a good deal of research has been done on evaluating the goodness of automatically generated text summaries (Mani et al.,2002; Lin and Hovy, 2003; Santos et al., 2004) our system is intended to mimic the actions of skimmers when answering questions, and thus our measure of goodness will be our system’s ability to recreate the retrieval of text focused on by our visual skimmers. This gives us a distinct advantage over other systems in measuring goodness, as defining a measure of goodness can prove difficult. In future work, we will be exploring different methods of ranking text such that the system returns results most similar to the results obtained from the visual skimming studies. The system will then be used on other questions and documents and compared"
W10-1313,A00-1023,0,0.0390604,"usually do not rely on external knowledge sources and are limited in the amount of ontological information that can be included in the system. The questions are usually fact-based in form (e.g., “How tall is Mt. Everest?”). These systems take a question and query a potentially large set of documents (e.g., the World Wide Web) to find the answer. A common technique is to determine a question type (e.g., “How many …?” would be classified as ‘numerical’, whereas “Who was …?” would be classified as ‘person’, etc.) and then locate answers of the correct type (Abney et al., 2000; Kwok et al., 2001; Srihari and Li, 2000; Galea, 2003). Questions are also frequently reformulated for pattern matching (e.g., “Who was the first American Astronaut in space?” becomes, “The first American Astronaut in space was” (Kwok et al., 2001; Brill et al., 2002)). Many systems submit multiple queries to a document corpus, relying on redundancy of the answer to handle incorrect answers, poorly constructed answers or documents that don’t contain the answer (e.g., Brill et al., 2002; Kwok et al., 2001). For these queries, systems often include synonyms, hypernyms, hyponyms, etc. in the query terms used for document and text retri"
W10-4202,W08-1103,1,0.691352,"boost factor ensures that all propositions will eventually become important enough for inclusion. 3 Application in a Particular Domain This section illustrates the application of our framework to a particular domain and how our framework facilitates flexible content selection. Our example is content selection in the SIGHT system (Elzer et al., 2007), whose goal is to provide visually impaired users with the knowledge that one would gain from viewing information graphics (such as bar charts) that appear in popular media. In the current implementation, SIGHT constructs a brief initial summary (Demir et al., 2008) that conveys the primary message of a bar chart along with its salient features. We enhanced the current SIGHT system to respond to user’s follow-up requests for more information about the graphic, where the request does not specify the kind of information that is desired. The first step in using our framework is determining the set of propositions that might be conveyed in this domain. In our earlier work (Demir et al., 2008), we identified a set of propositions that capture information that could be determined by looking at a bar chart, and for each message type defined in SIGHT, specified"
W10-4202,J97-1004,0,0.0477969,"Sripada et al., 2001) who have been observed to be tolerant of realization problems as long as the appropriate content is expressed. The NLG community has proposed various content selection approaches since early systems (Moore and Paris, 1993; McKeown, 1985) which placed emphasis on text structure and adapted planning techniques or schemas to meet discourse goals. This paper proposes a domain-independent framework which can be incorporated as a content selection component in a system whose goal is to deliver descriptive or explanatory texts, such as the ILEX (O’Donnell et al., 2001), KNIGHT (Lester and Porter, 1997), and POLIBOX (Chiarcos and Stede, 2004) systems. At the core of our framework lies a novel use of a graph-based ranking algorithm, which exploits discourse related considerations in determining what content to convey in response to a request for information. This framework provides the ability to generate successive history-aware texts and the flexibility to generate different texts with different parameter settings. One discourse consideration is the tenet that the propositions selected for inclusion in a text should be in some way related to one another. Thus, the selection process should b"
W10-4202,J93-4004,0,0.433228,"scourse history. We illustrate and evaluate this framework in an accessibility system for sight-impaired individuals. 1 Introduction Content selection is the task responsible for determining what to convey in the output of a generation system at the current exchange (Reiter and Dale, 1997). This very domain dependent task is extremely important from the perspective of users (Sripada et al., 2001) who have been observed to be tolerant of realization problems as long as the appropriate content is expressed. The NLG community has proposed various content selection approaches since early systems (Moore and Paris, 1993; McKeown, 1985) which placed emphasis on text structure and adapted planning techniques or schemas to meet discourse goals. This paper proposes a domain-independent framework which can be incorporated as a content selection component in a system whose goal is to deliver descriptive or explanatory texts, such as the ILEX (O’Donnell et al., 2001), KNIGHT (Lester and Porter, 1997), and POLIBOX (Chiarcos and Stede, 2004) systems. At the core of our framework lies a novel use of a graph-based ranking algorithm, which exploits discourse related considerations in determining what content to convey i"
W10-4202,H93-1032,0,0.1239,"tion in the text, this would introduce redundancy and should be avoided. Many systems (such as MATCH (Walker et al., 2004) and GEA (Carenini and Moore, 2006)) contain a user model which is employed to adapt content selection to the user’s preferences (Reiter and Dale, 1997). Our framework provides a facility to model a stereotypical user by incorporating the a priori importance of propositions. This facility can also be used to capture the preferences of a particular user. In a dialogue system, utterances that are generated without exploiting the previous discourse seem awkward and unnatural (Moore, 1993). Our framework takes the previous discourse into account so as to omit recently communicated propositions and to determine when repetition of a previously communicated proposition is appropriate. To our knowledge, our work is the first effort utilizing a graph-based ranking algorithm for content selection, while taking into account what information preferably should and shouldn’t be conveyed together, the a priori importance of information, and the discourse history. Our framework is a domain-independent methodology containing domain-dependent features that must be instantiated when applying"
W10-4202,W01-0802,0,\N,Missing
W10-4231,W09-2817,0,0.0260242,"Computer and Information Sciences University of Delaware Newark, Delaware, USA [charlieg|sparks|mccoy|kuo]@cis.udel.edu Abstract The GREC Named Entity Challenge 2010 (NEG) is an NLG shared task whereby submitted systems must select a referring expression from a list of options for each mention of each person in a text. The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated. An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al. (2009). Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b). Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task. As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NE"
W10-4231,W09-2818,0,0.045911,"Missing"
W10-4231,W09-2821,1,0.838396,"d Information Sciences University of Delaware Newark, Delaware, USA [charlieg|sparks|mccoy|kuo]@cis.udel.edu Abstract The GREC Named Entity Challenge 2010 (NEG) is an NLG shared task whereby submitted systems must select a referring expression from a list of options for each mention of each person in a text. The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated. An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al. (2009). Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b). Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task. As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NE"
W10-4231,W09-2819,1,0.828286,"d Information Sciences University of Delaware Newark, Delaware, USA [charlieg|sparks|mccoy|kuo]@cis.udel.edu Abstract The GREC Named Entity Challenge 2010 (NEG) is an NLG shared task whereby submitted systems must select a referring expression from a list of options for each mention of each person in a text. The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated. An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al. (2009). Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b). Although our system performed reasonably-well in predicting REG08Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task. As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NE"
W10-4232,W09-2821,1,0.893505,"Missing"
W10-4232,J94-4002,0,0.105023,"pe and case for each RE. Entities found by the NER tool are marked as names, and the additional REs we identified are marked as either pronouns or common nouns. Case values are determined by analyzing the assigned type and any type dependency representation (provided by the parser) involving the entity. At this stage we also note the gender of each pronoun and common noun, the plurality of each reference, and begin to deal with embedded entities. The next step identifies which tagged mentions corefer. We implemented a coreference resolution tool using a shallow rule-based approach inspired by Lappin and Leass (1994) and Bontcheva et al. (2002). Each mention is compared to all previously-seen entities on the basis of case, gender, SYNFUNC, plurality, and type. Each entity is then evaluated in order of appearance and compared to all previous entities starting with the most recent and working back to the first in the text. We apply rules to each of these pairs based on the REG08-Type attribute of the current entity. Names and common nouns are analyzed using string and word token matching. We collected extensive, cross-cultural lists of male and female first names to help identify the gender of named entitie"
W11-0506,W08-1103,1,0.805862,"1) they can be integrated with the summary of a multimodal document’s text, thereby producing a richer summary of the overall document’s content; 2) they can be stored in a digital library along with the graphic itself and used to retrieve appropriate graphics in response to user queries; and 3) for individuals with sight impairments, they can be used along with a screen reader to convey not only the text of a document, but also the content of the document’s graphics. In this paper we present our work on summarizing line graphs. This builds on our previous efforts into summarizing bar charts (Demir et al., 2008; Elzer et al., 2011); however, line graphs have different messages and communicative signals than bar charts and their continuous nature requires different processing. In addition, a very different set of visual features must be taken into account in deciding the importance of including a proposition in a summary. 2 Methodology Most summarization research has focused on extractive techniques by which segments of text are extracted and put together to form the summary. 41 Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 41–48, c Portland,"
W11-0506,W10-4202,1,0.83982,"ght for the associated proposition. Type 3 rules (Message Category + Visual Feature) differ only from type 2 rules in that they are restricted to a particular intended message category, rather than any line graph having the visual feature in question. For example, a proposition comparing the slope of two trends may be appropriate for a graph in the Change-trend message category, but does not make sense for a line graph with only a single trend (e.g., Rising-trend). Once all propositions have been extracted and ranked, these weights are passed along to a graphbased content selection framework (Demir et al., 2010) that iteratively selects for inclusion in the initial summary those propositions which provide the best coverage of the highest-ranked information. 4.3 Sample Rule Application Figures 1 and 4 consist of two different line graphs with the same intended message category: Changetrend. Figure 1 shows a stable trend in annual sea level difference from 1900 to 1930, followed by a rising trend through 2003, while Figure 4 shows a rising trend in Durango sales from 1997 to 1999, followed by a falling trend through 2006. Propositions associated with type 1 rules will have the same weights for both gra"
W11-0506,W96-0501,0,0.116659,"000 1999: 189,840 2006: 70,606 150,000 100,000 50,000 0 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 Figure 4: From “Chrysler: Plant had $800 million impact” in The (Wilmington) News Journal, Feb 15, 2007. “The values vary a lot...”, “The trend is unstable...”), possibly displacing a type 1 proposition that would still appear in the summary for the graph in Figure 4. 5 Future Work Once the propositions that should be included in the summary have been selected, they must be coherently organized and realized as natural language sentences. We anticipate using the FUF/SURGE surface realizer (Elhadad and Robin, 1996); our collected corpus of line graph summaries provides a large set of real-world expressions to draw from when crafting the surface realization forms our system will produce for the final-output summaries. Our summarization methodology must also be evaluated. In particular, we must evaluate the rules for identifying the additional informational propositions that are used to elaborate the overall intended message, and the quality of the summaries both in terms of content and coherence. 6 Related Work Image summarization has focused on constructing a smaller image that contains the important co"
W11-0506,P05-1028,1,0.825611,"r newspapers, generally have a communicative goal or intended message. For example, the graphic in Figure 1 is intended to convey a changing trend in sea levels — relatively flat from 1900 to 1930 and then rising from 1930 to 2003. Thus, using Clark’s view of language, information graphics are a means of communication. Research has shown that the content of information graphics in popular media is usually not repeated in the text of the accompanying article (Carberry et al., 2006). The captions of such graphics are also often uninformative or convey little of the graphic’s high-level message (Elzer et al., 2005). This contrasts with scientific documents in which graphics are often used to visualize data, with explicit references to the graphic being used to explain their content (e.g., “As shown in Fig. A...”). Information graphics in popular media contribute to the overall communicative goal of a multimodal document and should not be ignored. Our work is concerned with the summarization of information graphics from popular media. Such summaries have several major applications: 1) they can be integrated with the summary of a multimodal document’s text, thereby producing a richer summary of the overal"
W11-0506,J86-3001,0,\N,Missing
W11-2306,W08-1103,1,0.896015,"In contrast, graphs in popular media are constructed to make a point which should be obvious without complicated scientific reasoning. We are thus interested in generating a textual presentation of the content of graphs in popular media. Other research has focused on textual descriptions (e.g., Ferres et al. (2007)); however in that work the same information is included in the textual summary for each instance of a graph type (i.e., all summaries of line graphs contain the same sorts of information), and the summary does not attempt to present the overall intended message of the graph. SIGHT (Demir et al., 2008; Elzer et al., 2011) is a natural language system whose overall goal is providing blind users with interactive access to multimodal documents from electronically-available popular media sources. To date, the SIGHT project has concentrated on simple bar charts. Its user interface is implemented as a browser helper object within Internet Explorer that works with the JAWS screen reader. When the system detects a bar chart in a document being read by the user, it prompts the user to use keystrokes to request a brief summary of the graphic capturing its primary contribution to the overall communic"
W11-2306,W10-4202,1,0.838615,"o affected by certain rhetorical devices (WR ) which serve to highlight particular concepts. Being used in an idiom, or compared to another concept by means of juxtaposition suggests that a given concept may hold special significance. Finally, the weights assigned by our graph understanding system for the additional propositions identified in the graphics are incorporated into the ID of the concepts involved as WG . 59 5.3 Selecting Content for a Summary To select concepts for inclusion in the summary, the model will then be passed to a discourse-aware graph-based content selection framework (Demir et al., 2010), which selects concepts one at a time and iteratively re-weights the remaining items so as to include related concepts and avoid redundancy. This algorithm incorporates PageRank (Page et al., 1999), but with several modifications. In addition to centrality assessment based on relationships between concepts, it includes apriori importance nodes enabling us to incorporate concept completeness, number of expressions, document structure, and rhetorical devices. More importantly from a summary generation perspective, the algorithm iteratively picks concepts one at a time, and re-ranks the remainin"
W11-2306,W96-0501,0,0.0964934,"racted communicative signals serve as evidence for or against each candidate message. For Figure 2, our system produces changetrend(1997, rise, 1999, fall, 2006) as the logical representation of the most probable intended message. Since the dependent axis is often not explicitly labeled, a series of heuristics are used to identify an appropriate referent, which we term the measurement axis descriptor. In Figure 2, the measurement axis descriptor is identified as durango sales. The intended message and measurement axis descriptor are then passed to a realization component which uses FUF/SURGE (Elhadad and Robin, 1996) to generate the following initial description: This graphic conveys a changing trend in durango sales, rising from 1997 to 1999 and then falling to 2006. 3 Identifying a Relevant Paragraph In presenting a multimodal document to a user via a screen reader, if the author does not specify a reading order in the accessibility preferences, it is not entirely clear where the description of the graphical content should be given. The text of scientific articles normally makes explicit references to any graphs contained in the document; in this case, it makes sense to insert the graphical description"
W11-2306,W11-2703,1,0.803742,"Missing"
W11-2306,W10-4220,1,0.862276,"Missing"
W11-2306,A92-1027,1,0.58071,"bitrary length. This will permit the user to request a quick overview in order to decide whether to read the original document, or a more comprehensive synopsis to obtain the most important content without having to read the entire article. 5.1 Semantic Modeling of Multimodal Documents Content gathered from the article text by a semantic parser and from the information graphics by our graph understanding system is combined into a single semantic model based on typed, structured objects organized under a foundational ontology (McDonald, 2000a). For the semantic parsing of text, we use Sparser (McDonald, 1992), a bottom-up, phrase-structure-based chart parser, optimized for semantic grammars and partial parsing.3 Using a built-in model of core English grammar plus domain-specific grammars, Sparser extracts information from the text and produces categorized objects as a semantic representation (McDonald, 2000b). The intended message and salient additional propositions identified by our system for the information graphics are decomposed and added to the model constructed by Sparser.4 Model entries contain slots for attributes in the concept category’s ontology definition (fillable by other concepts o"
W11-2306,W00-0109,1,0.570812,"cepts as we wish, at any level of detail, to produce summaries of arbitrary length. This will permit the user to request a quick overview in order to decide whether to read the original document, or a more comprehensive synopsis to obtain the most important content without having to read the entire article. 5.1 Semantic Modeling of Multimodal Documents Content gathered from the article text by a semantic parser and from the information graphics by our graph understanding system is combined into a single semantic model based on typed, structured objects organized under a foundational ontology (McDonald, 2000a). For the semantic parsing of text, we use Sparser (McDonald, 1992), a bottom-up, phrase-structure-based chart parser, optimized for semantic grammars and partial parsing.3 Using a built-in model of core English grammar plus domain-specific grammars, Sparser extracts information from the text and produces categorized objects as a semantic representation (McDonald, 2000b). The intended message and salient additional propositions identified by our system for the information graphics are decomposed and added to the model constructed by Sparser.4 Model entries contain slots for attributes in the"
W11-2306,P03-1018,0,0.0339013,"ion graphics and another producing words relevant to the topics of the individual documents. Let Wg represent the word frequency vector yielding words relevant to the graphics, Wa represent the word frequency vector yielding words relevant to the document topics, and Wp represent the word frequency vector of the pseudo-relevant paragraphs. We compute Wp from the pseudo-relevant paragraphs themselves, and we estimate Wa using the word frequencies from the article text in the documents. Finally, we compute Wg by filtering-out the components of Wa from Wp . This process is related to the work by Widdows (2003) on orthogonal negation of vector spaces. The task can be formulated as follows: 1. Wp = αWa + βWg where α &gt; 0 and β &gt; 0, which means the word frequency vector for the pseudo-relevant paragraphs is a linear combination of the background (topic) word frequency vector and the graphic word vector. 2. &lt; Wa , Wg &gt;= 0 which means the background word vector is orthogonal to the graph description word vector, under the assumption that the graph description word vector is independent of the background word vector and that these two share minimal information. 3. Wg is assumed to be a unit vector, since"
W11-2703,aker-gaizauskas-2010-model,0,0.025664,"re asked to write brief summaries for a series of line graphs. While they did not release a corpus for distribution, their analysis did suggest that a graph’s visual features could be used to help select salient propositions to include in a summary. Although several corpora exist for general image descriptions, we are unaware of any other corpora of human-written summaries for information graphics. J¨orgensen (1998) collected unconstrained descriptions of pictorial images, while Hollink et al. (2004) analyzed descriptions of mental images formed by subjects to illustrate a given text passage. Aker and Gaizauskas (2010) built a corpus of human-generated captions for location-related images. Large collections of general image captions have been assembled for information retrieval tasks (Smeaton and Quigley, 1996; Tribble, 2010). Roy (2002) evaluated automatically-generated descriptions of visual scenes against human-generated descriptions. The developers of the iGraph-Lite system (Ferres et al., 2007) released a corpus of descriptions for over 500 graphs collected from Statistics Canada, but these descriptions were generated automatically by their system and not written by human authors. Additionally, the des"
W11-2703,W02-0109,0,0.0567061,"Missing"
W11-2703,W02-2113,0,0.0334835,"ons of visual scenes against human-generated descriptions. The developers of the iGraph-Lite system (Ferres et al., 2007) released a corpus of descriptions for over 500 graphs collected from Statistics Canada, but these descriptions were generated automatically by their system and not written by human authors. Additionally, the descriptions contained in their corpus focus on the quantitative data presented in the graphics rather than the high-level message, and tend to vary only slightly between graphs.4 Since using corpus texts as a “gold standard” in generation and evaluation can be tricky (Reiter and Sripada, 2002), we tried to mitigate some of the common problems, including giving participants as much time as they wanted for each summary to avoid “hurried writing.” However, as we intend to use this corpus to understand which propositions humans find salient for line graphs, as well as generat4 The iGraph-Lite system provides the same information for each instance of a graph type (i.e., all summaries of line graphs contain the same sorts of information). ing and evaluating new summaries, a larger collection of examples written by many authors for several different graphics was more desirable than a smal"
W11-2703,P06-4018,0,\N,Missing
W14-4410,W04-1013,0,0.0373747,"Missing"
W14-4410,radev-etal-2004-mead,0,0.100247,"Missing"
W14-4410,D11-1024,0,0.0448393,"and analysis (number of topics, initial α and β, number of iterations, and whether to optimize α and β in process). Figure 2 shows summarization with corpus and topic inputs, control settings, and the text summarization product. There are controls for extraction of sentences (Extract α and JSD Divisor) and for composing the summary (Order policy). Topic analysis has become a popular choice for text summarization as seen in Text Analysis Conferences (TAC, 2010; TAC, 2011) with individual team reports (Delort and Alfonseca, 2011; Lui et al., 2011; Mason and Charniak, 2011). Nenkova and McKeown (2012; 2011) included topic analysis among standard methods in their surveys of text summarization methodologies. Haghighi and Vanderwende (2009) explored extensions of LDA topic analysis for use in multiple document summarization tasks. Yet there are many control settings that can affect summarization that have not been explicitly studied or documented, and that are important for reproducing research results. In this text summarization pilot study, we experiment with several control settings. As in Mason and Charniak (2011) we do a general rather than guided summarization. Our primary contribution is ill"
W14-4410,N09-1041,0,\N,Missing
W14-4410,W01-0100,0,\N,Missing
W16-6621,W02-0109,0,0.042139,"Missing"
W16-6621,W14-4409,1,0.845899,"e level of the reader and the creation of the lexicon is guided by defining a domainaware synset for description of line graphs. Other NLG systems decide on text complexity based on available scales such as the D-level sentence complexity (Covington, He, Brown, Naci, & Brown, 2006). One example is presented in (Demir, Carberry, & McCoy, 2012), where tree structures are built representing all the possible ways sentences can be aggregated and the choice of the tree attempts to balance the number of sentences, their D-level complexity, and the types of relative clauses. The work presented in (P. Moraes, McCoy, & Carberry, 2014) describe a template-based approach for creating summaries at different reading levels. It does not, however, present an adaptive approach that can be applied to the micro planning phase of any NLG system. Another area, text simplification, aims to target low-skilled readers and users with language disabilities. SkillSum (Williams & Reiter, 2004, 2005a; Williams, Reiter, & Osman, 2003) is a system which adapts its output for readers with poor literacy after assessing their reading and numeracy skills. Their results show that, for these target readers, the micro planning choices made by SkillS"
W16-6621,W03-2314,0,0.0641898,"sabilities. SkillSum (Williams & Reiter, 2004, 2005a; Williams, Reiter, & Osman, 2003) is a system which adapts its output for readers with poor literacy after assessing their reading and numeracy skills. Their results show that, for these target readers, the micro planning choices made by SkillSum enhanced readability. (Carroll et al., 1999) presents a text simplification methodology to help languageimpaired users; (Rello, Baeza-Yates, Bott, & Saggion, 2013) propose a system that uses lexical simplification to enhance readability and understandability of text for people with dyslexia; while (Siddharthan, 2003) aims to make the text easier to read for some target group (like aphasics and people with low reading ages) or easier to process by some program (like a parser or machine translation system). One of our evaluation experiments (citation suppressed for anonymity) performed with college students showed that the simplest text was rather unpleasant for them to read. We therefore propose a technique that focuses on adjusting the generated text to the reading level of the surrounding text. The closest work to the one proposed in this paper is presented in (Bateman & Paris, 1989). It p resents an app"
W16-6621,W12-2019,0,0.0274372,"lem The search space for the aggregation of propositions problem is defined as: States: A state consists of two parts: a list of unrealized propositions and the realizations performed so far (which can consist of full sentences or sentence fragments). Initial state: The initial state contains the set of all unrealized propositions. Goal 4 Learning Text Complexity Features The features to be used in the heuristic needed to be chosen based on both their effect on text complexity and their usability. The choice of features for constructing the model was made based on the work 123 3 presented by (Vajjala & Meurers, 2012) which uses features that are based on Second Language Acquisition (SLA) research combined with traditional readability features, such as word length and sentence length, in order to classify text into different grades. Their work results in classifiers that outperform previous approaches on readability classification, reaching higher classification accuracy. However, since we still need to map features back to the NLG aggregation phase, the set of features used here represents a subset of the features presented in their work. The final set of features, motivated by (Vajjala & Meurers, 2012),"
W16-6621,N01-1003,0,0.106259,"o tailoring phrasing during the generation of natural text to different types of users. It employs a technique that leverages a knowledge base in order to make decisions during text planning in a rule based fashion. This work, in contrast, generates natural text aimed at a specific reading level Related Work The approach proposed by (Wilkinson, 1995) presents the aggregation process divided into two major steps: semantic grouping and sentence structuring. Although they are interdependent, both are needed in order to achieve aggregation in a text. (Barzilay & Lapata, 2006), (Bayyarapu, 2011), (Walker, Rambow, & Rogati, 2001) are some examples of learning aggregation rules and grouping constraints in order to aggregate text. It differs from our approach in that we are considering readability constraints when making such decisions. (Elhadad, Robin, & McKeown, 1997) present work on lexical choice considering constraints regarding syntax, semantics, pragmatics, the lexicon, and the underlying domain that float from one phase to the next in the generation of text. Our work differs in that lexical items are restrained by their ap122 2 by applying a graph search that allows the automation of the aggregation of proposit"
W16-6621,R11-1012,0,0.0156581,"sents an approach to tailoring phrasing during the generation of natural text to different types of users. It employs a technique that leverages a knowledge base in order to make decisions during text planning in a rule based fashion. This work, in contrast, generates natural text aimed at a specific reading level Related Work The approach proposed by (Wilkinson, 1995) presents the aggregation process divided into two major steps: semantic grouping and sentence structuring. Although they are interdependent, both are needed in order to achieve aggregation in a text. (Barzilay & Lapata, 2006), (Bayyarapu, 2011), (Walker, Rambow, & Rogati, 2001) are some examples of learning aggregation rules and grouping constraints in order to aggregate text. It differs from our approach in that we are considering readability constraints when making such decisions. (Elhadad, Robin, & McKeown, 1997) present work on lexical choice considering constraints regarding syntax, semantics, pragmatics, the lexicon, and the underlying domain that float from one phase to the next in the generation of text. Our work differs in that lexical items are restrained by their ap122 2 by applying a graph search that allows the automati"
W16-6621,E99-1042,0,0.179408,"d approach for creating summaries at different reading levels. It does not, however, present an adaptive approach that can be applied to the micro planning phase of any NLG system. Another area, text simplification, aims to target low-skilled readers and users with language disabilities. SkillSum (Williams & Reiter, 2004, 2005a; Williams, Reiter, & Osman, 2003) is a system which adapts its output for readers with poor literacy after assessing their reading and numeracy skills. Their results show that, for these target readers, the micro planning choices made by SkillSum enhanced readability. (Carroll et al., 1999) presents a text simplification methodology to help languageimpaired users; (Rello, Baeza-Yates, Bott, & Saggion, 2013) propose a system that uses lexical simplification to enhance readability and understandability of text for people with dyslexia; while (Siddharthan, 2003) aims to make the text easier to read for some target group (like aphasics and people with low reading ages) or easier to process by some program (like a parser or machine translation system). One of our evaluation experiments (citation suppressed for anonymity) performed with college students showed that the simplest text w"
W16-6621,J12-3004,1,0.834463,"tification of grade level appropriate and domain aware lexicons. Section 6 shows some examples of summaries generated by the system. Section 7 presents the evaluations of the system and Sections 8 and 9 provide conclusions and thoughts on future work, respectively. 2 propriateness to the level of the reader and the creation of the lexicon is guided by defining a domainaware synset for description of line graphs. Other NLG systems decide on text complexity based on available scales such as the D-level sentence complexity (Covington, He, Brown, Naci, & Brown, 2006). One example is presented in (Demir, Carberry, & McCoy, 2012), where tree structures are built representing all the possible ways sentences can be aggregated and the choice of the tree attempts to balance the number of sentences, their D-level complexity, and the types of relative clauses. The work presented in (P. Moraes, McCoy, & Carberry, 2014) describe a template-based approach for creating summaries at different reading levels. It does not, however, present an adaptive approach that can be applied to the micro planning phase of any NLG system. Another area, text simplification, aims to target low-skilled readers and users with language disabilitie"
W16-6621,W05-1616,0,0.0930841,"Missing"
W16-6621,W03-2317,0,0.118829,"ng all the possible ways sentences can be aggregated and the choice of the tree attempts to balance the number of sentences, their D-level complexity, and the types of relative clauses. The work presented in (P. Moraes, McCoy, & Carberry, 2014) describe a template-based approach for creating summaries at different reading levels. It does not, however, present an adaptive approach that can be applied to the micro planning phase of any NLG system. Another area, text simplification, aims to target low-skilled readers and users with language disabilities. SkillSum (Williams & Reiter, 2004, 2005a; Williams, Reiter, & Osman, 2003) is a system which adapts its output for readers with poor literacy after assessing their reading and numeracy skills. Their results show that, for these target readers, the micro planning choices made by SkillSum enhanced readability. (Carroll et al., 1999) presents a text simplification methodology to help languageimpaired users; (Rello, Baeza-Yates, Bott, & Saggion, 2013) propose a system that uses lexical simplification to enhance readability and understandability of text for people with dyslexia; while (Siddharthan, 2003) aims to make the text easier to read for some target group (like a"
W17-4509,N15-1018,0,0.0190506,"the non-branching document paths to a nested hierarchical structure 2 Software engineering already knows this – that hierarchical structure is less time complex than monolithic. 65 model alignment. Stability is defined as the correlation between aligned topics over documents. Greene et al. (2014) calculates the average of Jaccard scores on sets of popular word ranks between topic combinations of a topic model pair, and determines the model agreement (i.e., stability) as the average over topics of Jaccard scores resulting from the optimal topic alignment by the Hungarian assignment algorithm. Chuang et al. (2015) notes that model alignment is “ill-defined and computationally intractable” with multiple-to-multiple mappings between topics, and adopts the solution of mapping topics upto-one topic.3 Yang et al. (2016) aligns topics for flat topic structures also using the Hungarian assignment algorithm and up-to-one topic correspondence. Stability is measured as agreement between token topic assignments over aligned topic models. We use the Hungarian algorithm and the upto-one topic correspondence. We choose to emphasize topic correspondence based on topic word compositions, as in the generative model, an"
W17-4509,N09-1041,0,0.0501266,"as noting topics below a minimum frequency or comparing divergence of topics from any of uniform, corpus, or power distributions of word frequencies. More powerful methods assess individual and aggregate topic coherence. The current standard is to measure coherence by normalized pairwise mutual information (NPMI) (Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015) versus pairwise probabilities calculated from some very large pertinent corpus. We view test likelihood and topic coherence as largely complementary to topic model stability. 2.3 2.4 Topic Model Based Summarization Haghighi and Vanderwende (2009) examined several hybrid topic models using LDA as a building block and demonstrated the superior efficacy of their hybrid model (general topic, general content topic, detail content topics, and document specific topics) in constructing short summaries for Document Understanding Conferences (U.S. Department of Commerce: National Institute of Standards and Technology, 2015). Delort and Alfonseca (2011); Mason and Charniak (2011) used similar models in short summaries for the Text Analysis Conferences (of Commerce: National Institute of Standards and Technology, 2010, 2011). Celikyilmaz and Hakk"
W17-4509,E14-1056,0,0.0141605,"objective indication of predictability. Teh et al. (2007) provides formulas for calculating test LL(x) for the flat topic structure in both Gibbs sampler and variational inference analysis methods. Assessing quality of individual topics can be as simple as noting topics below a minimum frequency or comparing divergence of topics from any of uniform, corpus, or power distributions of word frequencies. More powerful methods assess individual and aggregate topic coherence. The current standard is to measure coherence by normalized pairwise mutual information (NPMI) (Aletras and Stevenson, 2013; Lau et al., 2014; R¨oder et al., 2015) versus pairwise probabilities calculated from some very large pertinent corpus. We view test likelihood and topic coherence as largely complementary to topic model stability. 2.3 2.4 Topic Model Based Summarization Haghighi and Vanderwende (2009) examined several hybrid topic models using LDA as a building block and demonstrated the superior efficacy of their hybrid model (general topic, general content topic, detail content topics, and document specific topics) in constructing short summaries for Document Understanding Conferences (U.S. Department of Commerce: National"
W17-4509,W11-0507,0,0.0170733,"ery large pertinent corpus. We view test likelihood and topic coherence as largely complementary to topic model stability. 2.3 2.4 Topic Model Based Summarization Haghighi and Vanderwende (2009) examined several hybrid topic models using LDA as a building block and demonstrated the superior efficacy of their hybrid model (general topic, general content topic, detail content topics, and document specific topics) in constructing short summaries for Document Understanding Conferences (U.S. Department of Commerce: National Institute of Standards and Technology, 2015). Delort and Alfonseca (2011); Mason and Charniak (2011) used similar models in short summaries for the Text Analysis Conferences (of Commerce: National Institute of Standards and Technology, 2010, 2011). Celikyilmaz and Hakkani-Tur (2010, 2011) used a more general hierarchical LDA topic model structure, doing hierarchical summarization for longer summaries. Christensen et al. (2014) developed “hierarchical summarization” using temporal hierarchical clustering and budgeting summary component size by cluster. We use a more general hierarchical structured Bayesian topic model similar to Paisley et al. Topic Alignment and Stability Topic models must b"
W17-4509,P14-1085,0,\N,Missing
W17-4509,W13-0102,0,\N,Missing
W90-0101,P83-1012,0,0.076382,"Missing"
W90-0101,C88-2121,0,0.184134,"Missing"
W90-0115,P88-1020,0,0.0582139,"Missing"
W99-0107,J96-2004,0,0.0552804,"Missing"
W99-0107,A97-1051,0,0.0801584,"Missing"
W99-0107,W98-1119,0,0.0872904,"Missing"
W99-0107,H94-1020,0,0.0191554,"a Parsed Corpus All of the applications discussed in section I depend on having a corpus of reliably marked expressions, features, and relations. In order to determine that these dimensions have been &apos;*reliably marked&quot;, we need to measure agreement between two codeas marking the same text. One way to increase the relinb&apos;dity of the coding (regardless of the method used to measure reliability) is to automate part of the coding process. Our system can extract a number of markings, features and relations from the parsed, part-of-speech-tagged corpora of the type found in in the Penn Treebank 2 (Marcus et al., 1994). Use of the Treebank data means we can find most of the markables and many of the necessary features before giving the task to a human coder. We do not try to extract any of the co-reference information from the parsed corpora. Dialog Acts. To determine the domain ofanaphoric antecedent~ the dialog must be divided into short piece. We have chosen to use units based on dialog acts for this task. Therefore, turns have to be segmcnted into dialog ~ t units. Our study of anaphoric extnssious reveals that in a dialog between two participants A and B, the DE&apos;s introduced by A are not added to the s"
W99-0107,W99-0108,1,0.899707,"Missing"
W99-0107,J97-1005,0,0.0606602,"Missing"
W99-0107,J98-2001,0,0.015582,"hod depends on counting how many co-refereace links must be added to one coder&apos;s equivalence classes to wansform the set into that found by the other c~ler. We adopt this method and enable the tool to perform t l ~ computation between any two codings which fully agree on the underlying set o f markahies. Finally. we can measure feature-value agreement by viewing the featme assignment task as a kind o f classification task and then computing Kappa (a), which measures how well the coders a . g ~ l compared to their random eJcpected agreemeat4(CaHetta, 1996). We conform to the method proposed in Poesio & Vieira (1998) for computing actual and expected agreement. (Again we assume the coders have already agreed on the set of +markables.) Suppose we are considering a given feature 2The intent is thatthe coders will achieve a high degree of consistencyif the manual is clear, and then if the manual accurately represents the desired coding style, consistency among coden impliesaccuracyof all the coding~ SAgreemem among a set of n &gt; 2 coders is usmdly calculated as a function of the . ~ pail&apos;wise agreements, so we will discuss only the pa/rwise case here, realizing that the full comlmmion is straightforward. • At"
W99-0107,P98-2204,1,0.827841,"exactly which features are most predictive of co-reference. So. we will try to mark a set of features which is a superset of the necessary features. Drawing on the feature sets used in Connolly et al. (1997) and Ge et al. (1998), we believe the following factors might indicate co-referenco: • Syntactic role (e.g. Subject, Object, Prepositional Object,...), • Pronominalization (yea or no), • Distancebetween EA and Ee (an integer), • Definiteness (yes or no), • Semantic role (e.g. indicating location, manner, time,...), • Nesting depth of an N&apos;P (an integer), • Information status (as defined by Strube (1998)) of the DE, • Gender, Number, Animacy. The tool must allow the coder to assign values for these features to each marked expression, but should not demand that every expression has a value assigned for every feature. Since we cannot claim that this set Of features is exhaustive, the tool must allow further features to be added by the user. Since reliability of feature assignment is important, the tool should have the abifity to extract as many features aspossible automatically (for example, from a parsed corpus). In addition since some features must be hand-marked, the tool must have the abili"
W99-0107,M95-1005,0,0.0632856,"he expressions which are not common to the two codings. This will make it easier to visualize the differences between the codings and reach perfect agreement of markables. The second kind of agreement measures agreement between two coders&apos; co-reference codings. We require that the two coders have the same set of markables before comparing their co-reference annotations, so achieving markable agreement of I is a prerequisite for this calculation. As discussed in section 2.3, the co-reference relation divides the set of markables into equivalence classes. A model-theoretic algorithm proposed by Vilain et al. (1995) uses these Co-reference classes to define a precision and recall metric which yields intuitively plausible results and is easy to calculate. The method depends on counting how many co-refereace links must be added to one coder&apos;s equivalence classes to wansform the set into that found by the other c~ler. We adopt this method and enable the tool to perform t l ~ computation between any two codings which fully agree on the underlying set o f markahies. Finally. we can measure feature-value agreement by viewing the featme assignment task as a kind o f classification task and then computing Kappa"
W99-0107,E99-1006,1,\N,Missing
W99-0107,J86-3001,0,\N,Missing
W99-0107,C98-2199,1,\N,Missing
W99-0108,J95-2003,0,0.584355,"Missing"
W99-0108,J86-3001,0,0.258267,"Missing"
W99-0108,J88-2003,0,0.0392824,"Missing"
W99-0108,J88-2004,0,0.0254088,"Missing"
W99-0108,P87-1022,0,0.560726,"Missing"
W99-0108,P95-1005,0,0.0456911,"Missing"
W99-0108,P98-2204,1,0.912273,"Missing"
W99-0108,J94-2006,1,0.893995,"Missing"
W99-0108,J94-2004,0,0.0364681,"Missing"
W99-0108,C98-2199,1,\N,Missing
W99-0408,P98-2196,1,0.824107,"s native or near-native users of American Sign Language (ASL). This population poses unique challenges for a writing instruction program: their writICICLE Overview ICICLE interacts with its user through a cycle of user input and system response. The user begins this cycle by supplying the system with a multi-sentential piece of writing for analysis. An error identification component uses an English grammar augmented with rules to 47 cover grammatical errors commonly produced by our learner population to process the user's writing, tagging the grammatical errors it finds (Suri and McCoy, 1993; Schneider and McCoy, 1998). In the current implementation of ICICLE, the identified errors are highlighted in a window-based interface using colors which indicate the class of error; for example, all subjectverb agreement errors are highlighted in blue. The user may then explore particular sentences containing errors by clicking on them, causing an editing window to appear with a simple onesentence ""canned"" response explaining the error. The user may then edit the sentence, have it reanalyzed by the system, and paste the results back into the original text. In the completed system, errors identified by the system will"
W99-0408,W97-0508,1,0.711599,"Missing"
W99-0408,J93-4004,0,\N,Missing
W99-0408,C98-2191,1,\N,Missing
