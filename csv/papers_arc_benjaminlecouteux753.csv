2021.iwslt-1.20,{ON}-{TRAC}{'} systems for the {IWSLT} 2021 low-resource speech translation and multilingual speech translation shared tasks,2021,-1,-1,7,1,5778,hang le,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2021, low-resource speech translation and multilingual speech translation. The ON-TRAC Consortium is composed of researchers from three French academic laboratories and an industrial partner: LIA (Avignon Universit{\'e}), LIG (Universit{\'e} Grenoble Alpes), LIUM (Le Mans Universit{\'e}), and researchers from Airbus. A pipeline approach was explored for the low-resource speech translation task, using a hybrid HMM/TDNN automatic speech recognition system fed by wav2vec features, coupled to an NMT system. For the multilingual speech translation task, we investigated the us of a dual-decoder Transformer that jointly transcribes and translates an input speech. This model was trained in order to translate from multiple source languages to multiple target ones."
2020.lrec-1.21,Providing Semantic Knowledge to a Set of Pictograms for People with Disabilities: a Set of Links between {W}ord{N}et and Arasaac: Arasaac-{WN},2020,-1,-1,6,0,5694,didier schwab,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This article presents a resource that links WordNet, the widely known lexical and semantic database, and Arasaac, the largest freely available database of pictograms. Pictograms are a tool that is more and more used by people with cognitive or communication disabilities. However, they are mainly used manually via workbooks, whereas caregivers and families would like to use more automated tools (use speech to generate pictograms, for example). In order to make it possible to use pictograms automatically in NLP applications, we propose a database that links them to semantic knowledge. This resource is particularly interesting for the creation of applications that help people with cognitive disabilities, such as text-to-picto, speech-to-picto, picto-to-speech... In this article, we explain the needs for this database and the problems that have been identified. Currently, this resource combines approximately 800 pictograms with their corresponding WordNet synsets and it is accessible both through a digital collection and via an SQL database. Finally, we propose a method with associated tools to make our resource language-independent: this method was applied to create a first text-to-picto prototype for the French language. Our resource is distributed freely under a Creative Commons license at the following URL: https://github.com/getalp/Arasaac-WN."
2020.lrec-1.302,{F}lau{BERT}: Unsupervised Language Model Pre-training for {F}rench,2020,-1,-1,6,1,5778,hang le,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Language models have become a key step to achieve state-of-the art results in many different Natural Language Processing (NLP) tasks. Leveraging the huge amount of unlabeled texts nowadays available, they provide an efficient way to pre-train continuous word representations that can be fine-tuned for a downstream task, along with their contextualization at the sentence level. This has been widely demonstrated for English using contextualized representations (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019b). In this paper, we introduce and share FlauBERT, a model learned on a very large and heterogeneous French corpus. Models of different sizes are trained using the new CNRS (French National Centre for Scientific Research) Jean Zay supercomputer. We apply our French language models to diverse NLP tasks (text classification, paraphrasing, natural language inference, parsing, word sense disambiguation) and show that most of the time they outperform other pre-training approaches. Different versions of FlauBERT as well as a unified evaluation protocol for the downstream tasks, called FLUE (French Language Understanding Evaluation), are shared to the research community for further reproducible experiments in French NLP."
2020.jeptalnrecital-taln.26,{F}lau{BERT} : des mod{\\`e}les de langue contextualis{\\'e}s pr{\\'e}-entra{\\^\\i}n{\\'e}s pour le fran{\\c{c}}ais ({F}lau{BERT} : Unsupervised Language Model Pre-training for {F}rench),2020,-1,-1,6,1,5778,hang le,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue pr{\'e}-entra{\^\i}n{\'e}s sont d{\'e}sormais indispensables pour obtenir des r{\'e}sultats {\`a} l{'}{\'e}tat-de-l{'}art dans de nombreuses t{\^a}ches du TALN. Tirant avantage de l{'}{\'e}norme quantit{\'e} de textes bruts disponibles, ils permettent d{'}extraire des repr{\'e}sentations continues des mots, contextualis{\'e}es au niveau de la phrase. L{'}efficacit{\'e} de ces repr{\'e}sentations pour r{\'e}soudre plusieurs t{\^a}ches de TALN a {\'e}t{\'e} d{\'e}montr{\'e}e r{\'e}cemment pour l{'}anglais. Dans cet article, nous pr{\'e}sentons et partageons FlauBERT, un ensemble de mod{\`e}les appris sur un corpus fran{\c{c}}ais h{\'e}t{\'e}rog{\`e}ne et de taille importante. Des mod{\`e}les de complexit{\'e} diff{\'e}rente sont entra{\^\i}n{\'e}s {\`a} l{'}aide du nouveau supercalculateur Jean Zay du CNRS. Nous {\'e}valuons nos mod{\`e}les de langue sur diverses t{\^a}ches en fran{\c{c}}ais (classification de textes, paraphrase, inf{\'e}rence en langage naturel, analyse syntaxique, d{\'e}sambigu{\""\i}sation automatique) et montrons qu{'}ils surpassent souvent les autres approches sur le r{\'e}f{\'e}rentiel d{'}{\'e}valuation FLUE {\'e}galement pr{\'e}sent{\'e} ici."
2020.jeptalnrecital-jep.24,Reconnaissance de parole beatbox{\\'e}e {\\`a} l{'}aide d{'}un syst{\\`e}me {HMM}-{GMM} inspir{\\'e} de la reconnaissance automatique de la parole ({BEATBOX} {SOUNDS} {RECOGNITION} {USING} A {SPEECH}-{DEDICATED} {HMM}-{GMM} {BASED} {SYSTEM} 1 Human beatboxing is a vocal art making use of speech organs to produce percussive sounds and imitate musical instruments),2020,-1,-1,5,0,18660,solene evain,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 1 : Journ{\\'e}es d'{\\'E}tudes sur la Parole",0,"Le human-beatbox est un art vocal utilisant les organes de la parole pour produire des sons percussifs et imiter les instruments de musique. La classification des sons du beatbox repr{\'e}sente actuellement un d{\'e}fi. Nous proposons un syst{\`e}me de reconnaissance des sons de beatbox s{'}inspirant de la reconnaissance automatique de la parole. Nous nous appuyons sur la bo{\^\i}te {\`a} outils Kaldi, qui est tr{\`e}s utilis{\'e}e dans le cadre de la reconnaissance automatique de la parole (RAP). Notre corpus est compos{\'e} de sons isol{\'e}s produits par deux beatboxers et se compose de 80 sons diff{\'e}rents. Nous nous sommes concentr{\'e}s sur le d{\'e}codage avec des mod{\`e}les acoustiques monophones, {\`a} base de HMM-GMM. La transcription utilis{\'e}e s{'}appuie sur un syst{\`e}me d{'}{\'e}criture sp{\'e}cifique aux beatboxers, appel{\'e} Vocal Grammatics (VG). Ce syst{\`e}me d{'}{\'e}criture s{'}appuie sur les concepts de la phon{\'e}tique articulatoire."
2020.iwslt-1.2,{ON}-{TRAC} Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at {IWSLT} 2020,2020,28,0,6,0,5712,maha elbayad,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline speech translation and simultaneous speech translation. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Universit{\'e}), LIG (Universit{\'e} Grenoble Alpes), and LIUM (Le Mans Universit{\'e}). Attention-based encoder-decoder models, trained end-to-end, were used for our submissions to the offline speech translation track. Our contributions focused on data augmentation and ensembling of multiple models. In the simultaneous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks."
2019.jeptalnrecital-long.4,"Compression de vocabulaire de sens gr{\\^a}ce aux relations s{\\'e}mantiques pour la d{\\'e}sambigu{\\\\\i}sation lexicale (Sense Vocabulary Compression through Semantic Knowledge for Word Sense Disambiguation)""",2019,-1,-1,2,1,16630,loic vial,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"En D{\'e}sambigu{\""\i}sation Lexicale (DL), les syst{\`e}mes supervis{\'e}s dominent largement les campagnes d{'}{\'e}valuation. La performance et la couverture de ces syst{\`e}mes sont cependant rapidement limit{\'e}s par la faible quantit{\'e} de corpus annot{\'e}s en sens disponibles. Dans cet article, nous pr{\'e}sentons deux nouvelles m{\'e}thodes qui visent {\`a} r{\'e}soudre ce probl{\`e}me en exploitant les relations s{\'e}mantiques entre les sens tels que la synonymie, l{'}hyperonymie et l{'}hyponymie, afin de compresser le vocabulaire de sens de WordNet, et ainsi r{\'e}duire le nombre d{'}{\'e}tiquettes diff{\'e}rentes n{\'e}cessaires pour pouvoir d{\'e}sambigu{\""\i}ser tous les mots de la base lexicale. Nos m{\'e}thodes permettent de r{\'e}duire consid{\'e}rablement la taille des mod{\`e}les de DL neuronaux, avec l{'}avantage d{'}am{\'e}liorer leur couverture sans donn{\'e}es suppl{\'e}mentaires, et sans impacter leur pr{\'e}cision. En plus de nos m{\'e}thodes, nous pr{\'e}sentons un syst{\`e}me de DL qui tire parti des r{\'e}cents travaux sur les repr{\'e}sentations vectorielles de mots contextualis{\'e}es, afin d{'}obtenir des r{\'e}sultats qui surpassent largement l{'}{\'e}tat de l{'}art sur toutes les t{\^a}ches d{'}{\'e}valuation de la DL."
2019.jeptalnrecital-demo.1,"Apporter des connaissances s{\\'e}mantiques {\\`a} un jeu de pictogrammes destin{\\'e} {\\`a} des personnes en situation de handicap : Un ensemble de liens entre {P}rinceton {W}ord{N}et et Arasaac, Arasaac-{WN} (Giving semantic knowledge to a set of pictograms for people with disabilities : a set of links between {W}ord{N}et and Arasaac, Arasaac-{WN} )",2019,-1,-1,5,0,5694,didier schwab,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume IV : D{\\'e}monstrations,0,"Cet article pr{\'e}sente une ressource qui fait le lien entre WordNet et Arasaac, la plus grande base de pictogrammes librement disponible. Cette ressource est particuli{\`e}rement int{\'e}ressante pour la cr{\'e}ation d{'}applications visant l{'}aide aux personnes en situation de handicap cognitif."
2019.gwc-1.14,Sense Vocabulary Compression through the Semantic Knowledge of {W}ord{N}et for Neural Word Sense Disambiguation,2019,27,7,2,1,16630,loic vial,Proceedings of the 10th Global Wordnet Conference,0,"In this article, we tackle the issue of the limited quantity of manually sense annotated corpora for the task of word sense disambiguation, by exploiting the semantic relationships between senses such as synonymy, hypernymy and hyponymy, in order to compress the sense vocabulary of Princeton WordNet, and thus reduce the number of different sense tags that must be observed to disambiguate all words of the lexical database. We propose two different methods that greatly reduce the size of neural WSD models, with the benefit of improving their coverage without additional training data, and without impacting their precision. In addition to our methods, we present a WSD system which relies on pre-trained BERT word vectors in order to achieve results that significantly outperforms the state of the art on all WSD evaluation tasks."
W18-5402,Analyzing Learned Representations of a Deep {ASR} Performance Prediction Model,2018,0,2,4,1,27951,zied elloumi,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"This paper addresses a relatively new task: prediction of ASR performance on unseen broadcast programs. In a previous paper, we presented an ASR performance prediction system using CNNs that encode both text (ASR transcript) and speech, in order to predict word error rate. This work is dedicated to the analysis of speech signal embeddings and text embeddings learnt by the CNN while training our prediction model. We try to better understand which information is captured by the deep model and its relation with different conditioning factors. It is shown that hidden layers convey a clear signal about speech style, accent and broadcast type. We then try to leverage these 3 types of information at training time through multi-task learning. Our experiments show that this allows to train slightly more efficient ASR performance prediction systems that - in addition - simultaneously tag the analyzed utterances according to their speech style, accent and broadcast program origin."
L18-1166,{UFSAC}: Unification of Sense Annotated Corpora and Tools,2018,0,5,2,1,16630,loic vial,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In Word Sense Disambiguation, sense annotated corpora are often essential for evaluating a system and also valuable in order to reach a good efficiency. Always created for a specific purpose, there are today a dozen of sense annotated English corpora, in various formats and using different versions of WordNet. The main hypothesis of this work is that it should be possible to build a disambiguation system by using any of these corpora during the training phase or during the testing phase regardless of their original purpose. In this article, we present UFSAC: a format of corpus that can be used for either training or testing a disambiguation system, and the process we followed for constructing this format. We give to the community the whole set of sense annotated English corpora that we know, in this unified format, when the copyright allows it, with sense keys converted to the last version of WordNet. We also provide the source code for building these corpora from their original data, and a complete Java API for manipulating corpora in this format. The whole resource is available at the following URL: https://github.com/getalp/UFSAC."
2018.jeptalnrecital-long.12,"Approche supervis{\\'e}e {\\`a} base de cellules {LSTM} bidirectionnelles pour la d{\\'e}sambigu{\\\\\i}sation lexicale ({LSTM} Based Supervised Approach for Word Sense Disambiguation)""",2018,-1,-1,2,1,16630,loic vial,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"En d{\'e}sambigu{\""\i}sation lexicale, l{'}utilisation des r{\'e}seaux de neurones est encore peu pr{\'e}sente et tr{\`e}s r{\'e}cente. Cette direction est pourtant tr{\`e}s prometteuse, tant les r{\'e}sultats obtenus par ces premiers syst{\`e}mes arrivent syst{\'e}matiquement en t{\^e}te des campagnes d{'}{\'e}valuation, malgr{\'e} une marge d{'}am{\'e}lioration qui semble encore importante. Nous pr{\'e}sentons dans cet article une nouvelle architecture {\`a} base de r{\'e}seaux de neurones pour la d{\'e}sambigu{\""\i}sation lexicale. Notre syst{\`e}me est {\`a} la fois moins complexe {\`a} entra{\^\i}ner que les syst{\`e}mes neuronaux existants et il obtient des r{\'e}sultats {\'e}tat de l{'}art sur la plupart des t{\^a}ches d{'}{\'e}valuation de la d{\'e}sambigu{\""\i}sation lexicale en anglais. L{'}accent est port{\'e} sur la reproductibilit{\'e} de notre syst{\`e}me et de nos r{\'e}sultats, par l{'}utilisation d{'}un mod{\`e}le de vecteurs de mots, de corpus d{'}apprentissage et d{'}{\'e}valuation librement accessibles."
W17-6940,Sense Embeddings in Knowledge-Based Word Sense Disambiguation,2017,9,0,2,1,16630,loic vial,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,"In this paper, we develop a new way of creating sense vectors for any dictionary, by using an existing word embeddings model, and summing the vectors of the terms inside a sense's definition, weighted in function of their part of speech and their frequency. These vectors are then used for finding the closest senses to any other sense, thus creating a semantic network of related concepts, automatically generated. This network is hence evaluated against the existing semantic network found in WordNet, by comparing its contribution to a knowledge-based method for Word Sense Dis-ambiguation. This method can be applied to any other language which lacks such semantic network, as the creation of word vectors is totally unsupervised, and the creation of sense vectors only needs a traditional dictionary. The results show that our generated semantic network improves greatly the WSD system, almost as much as the manually created one."
2017.jeptalnrecital-long.5,Traitement des Mots Hors Vocabulaire pour la Traduction Automatique de Document {OCR}is{\\'e}s en Arabe (This article presents a new system that automatically translates images of arabic documents),2017,-1,-1,4,0,33219,kamel bouzidi,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Cet article pr{\'e}sente un syst{\`e}me original de traduction de documents num{\'e}ris{\'e}s en arabe. Deux modules sont cascad{\'e}s : un syst{\`e}me de reconnaissance optique de caract{\`e}res (OCR) en arabe et un syst{\`e}me de traduction automatique (TA) arabe-fran{\c{c}}ais. Le couplage OCR-TA a {\'e}t{\'e} peu abord{\'e} dans la litt{\'e}rature et l{'}originalit{\'e} de cette {\'e}tude consiste {\`a} proposer un couplage {\'e}troit entre OCR et TA ainsi qu{'}un traitement sp{\'e}cifique des mots hors vocabulaire (MHV) engendr{\'e}s par les erreurs d{'}OCRisation. Le couplage OCR-TA par treillis et notre traitement des MHV par remplacement selon une mesure composite qui prend en compte forme de surface et contexte du mot, permettent une am{\'e}lioration significative des performances de traduction. Les exp{\'e}rimentations sont r{\'e}alis{\'e}s sur un corpus de journaux num{\'e}ris{\'e}s en arabe et permettent d{'}obtenir des am{\'e}liorations en score BLEU de 3,73 et 5,5 sur les corpus de d{\'e}veloppement et de test respectivement."
2017.jeptalnrecital-demo.9,Uniformisation de corpus anglais annot{\\'e}s en sens (Unification of sense annotated {E}nglish corpora for word sense disambiguation),2017,-1,-1,2,1,16630,loic vial,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Pour la d{\'e}sambigu{\""\i}sation lexicale en anglais, on compte aujourd{'}hui une quinzaine de corpus annot{\'e}s en sens dans des formats souvent diff{\'e}rents et provenant de diff{\'e}rentes versions du Princeton WordNet. Nous pr{\'e}sentons un format pour uniformiser ces corpus, et nous fournissons {\`a} la communaut{\'e} l{'}ensemble des corpus annot{\'e}s en anglais port{\'e}s {\`a} notre connaissance avec des sens uniformis{\'e}s du Princeton WordNet 3.0, lorsque les droits le permettent et le code source pour construire l{'}ensemble des corpus {\`a} partir des donn{\'e}es originales."
2017.jeptalnrecital-court.18,"Repr{\\'e}sentation vectorielle de sens pour la d{\\'e}sambigu{\\\\\i}sation lexicale {\\`a} base de connaissances (Sense Embeddings in Knowledge-Based Word Sense Disambiguation)""",2017,-1,-1,2,1,16630,loic vial,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Dans cet article, nous proposons une nouvelle m{\'e}thode pour repr{\'e}senter sous forme vectorielle les sens d{'}un dictionnaire. Nous utilisons les termes employ{\'e}s dans leur d{\'e}finition en les projetant dans un espace vectoriel, puis en additionnant les vecteurs r{\'e}sultants, avec des pond{\'e}rations d{\'e}pendantes de leur partie du discours et de leur fr{\'e}quence. Le vecteur de sens r{\'e}sultant est alors utilis{\'e} pour trouver des sens reli{\'e}s, permettant de cr{\'e}er un r{\'e}seau lexical de mani{\`e}re automatique. Le r{\'e}seau obtenu est ensuite {\'e}valu{\'e} par rapport au r{\'e}seau lexical de WordNet, construit manuellement. Pour cela nous comparons l{'}impact des diff{\'e}rents r{\'e}seaux sur un syst{\`e}me de d{\'e}sambigu{\""\i}sation lexicale bas{\'e} sur la mesure de Lesk. L{'}avantage de notre m{\'e}thode est qu{'}elle peut {\^e}tre appliqu{\'e}e {\`a} n{'}importe quelle langue ne poss{\'e}dant pas un r{\'e}seau lexical comme celui de WordNet. Les r{\'e}sultats montrent que notre r{\'e}seau automatiquement g{\'e}n{\'e}r{\'e} permet d{'}am{\'e}liorer le score du syst{\`e}me de base, atteignant quasiment la qualit{\'e} du r{\'e}seau de WordNet."
L16-1221,The {CIRDO} Corpus: Comprehensive Audio/Video Database of Domestic Falls of Elderly People,2016,30,6,10,1,18169,michel vacher,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes. In particular, regarding elderly living alone at home, the detection of distress situation after a fall is very important to reassure this kind of population. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The C IRDO corpus is a dataset recorded in realistic conditions in D OMUS , a fully equipped Smart Home with microphones and home automation sensors, in which participants performed scenarios including real falls on a carpet and calls for help. These scenarios were elaborated thanks to a field study involving elderly persons. Experiments related in a first part to distress detection in real-time using audio and speech analysis and in a second part to fall detection using video analysis are presented. Results show the difficulty of the task. The database can be used as standardized database by researchers to evaluate and compare their systems for elderly person{'}s assistance."
L16-1313,{C}irdo{X}: an on/off-line multisource speech and sound analysis software,2016,22,1,5,0,34949,frederic aman,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Vocal User Interfaces in domestic environments recently gained interest in the speech processing community. This interest is due to the opportunity of using it in the framework of Ambient Assisted Living both for home automation (vocal command) and for call for help in case of distress situations, i.e. after a fall. C IRDO X, which is a modular software, is able to analyse online the audio environment in a home, to extract the uttered sentences and then to process them thanks to an ASR module. Moreover, this system perfoms non-speech audio event classification; in this case, specific models must be trained. The software is designed to be modular and to process on-line the audio multichannel stream. Some exemples of studies in which C IRDO X was involved are described. They were operated in real environment, namely a Living lab environment."
2016.jeptalnrecital-jep.4,Acquisition et reconnaissance automatique d{'}expressions et d{'}appels vocaux dans un habitat. (Acquisition and recognition of expressions and vocal calls in a smart home),2016,-1,-1,2,1,18169,michel vacher,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"Cet article pr{\'e}sente un syst{\`e}me capable de reconna{\^\i}tre les appels {\`a} l{'}aide de personnes {\^a}g{\'e}es vivant {\`a} domicile afin de leur fournir une assistance. Le syst{\`e}me utilise une technologie de Reconnaissance Automatique de la Parole (RAP) qui doit fonctionner en conditions de parole distante et avec de la parole expressive. Pour garantir l{'}intimit{\'e}, le syst{\`e}me s{'}ex{\'e}cute localement et ne reconna{\^\i}t que des phrases pr{\'e}d{\'e}finies. Le syst{\`e}me a {\'e}t{\'e} {\'e}valu{\'e} par 17 participants jouant des sc{\'e}narios incluant des chutes dans un Living lab reproduisant un salon. Le taux d{'}erreur de d{\'e}tection obtenu, 29{\%}, est encourageant et souligne les d{\'e}fis {\`a} surmonter pour cette t{\^a}che."
W15-5121,Recognition of Distress Calls in Distant Speech Setting: a Preliminary Experiment in a Smart Home,2015,24,5,2,1,18169,michel vacher,Proceedings of {SLPAT} 2015: 6th Workshop on Speech and Language Processing for Assistive Technologies,0,"This paper presents a system to recognize distress speech in the home of seniors to provide reassurance and assistance. The system is aiming at being integrated into a larger system for Ambient Assisted Living (AAL) using only one microphone with a fix position in a non-intimate room. The paper presents the details of the automatic speech recognition system which must work under distant speech condition and with expressive speech. Moreover, privacy is ensured by running the decoding on-site and not on a remote server. Furthermore the system was biased to recognize only set of sentences defined after a user study. The system has been evaluated in a smart space reproducing a typical living room where 17 participants played scenarios including falls during which they uttered distress calls. The results showed a promising error rate of 29% while emphasizing the challenges of the task. Index Terms: Smart home, Vocal distress call, Applications of speech technology for Ambient Assisted Living"
2015.jeptalnrecital-long.21,Utilisation de mesures de confiance pour am{\\'e}liorer le d{\\'e}codage en traduction de parole,2015,-1,-1,2,0.259544,5610,laurent besacier,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Les mesures de confiance au niveau mot (Word Confidence Estimation - WCE) pour la traduction auto- matique (TA) ou pour la reconnaissance automatique de la parole (RAP) attribuent un score de confiance {\`a} chaque mot dans une hypoth{\`e}se de transcription ou de traduction. Dans le pass{\'e}, l{'}estimation de ces mesures a le plus souvent {\'e}t{\'e} trait{\'e}e s{\'e}par{\'e}ment dans des contextes RAP ou TA. Nous proposons ici une estimation conjointe de la confiance associ{\'e}e {\`a} un mot dans une hypoth{\`e}se de traduction automatique de la parole (TAP). Cette estimation fait appel {\`a} des param{\`e}tres issus aussi bien des syst{\`e}mes de transcription de la parole (RAP) que des syst{\`e}mes de traduction automatique (TA). En plus de la construction de ces estimateurs de confiance robustes pour la TAP, nous utilisons les informations de confiance pour re-d{\'e}coder nos graphes d{'}hypoth{\`e}ses de traduction. Les exp{\'e}rimentations r{\'e}alis{\'e}es montrent que l{'}utilisation de ces mesures de confiance au cours d{'}une seconde passe de d{\'e}codage permettent d{'}obtenir une am{\'e}lioration significative des performances de traduction ({\'e}valu{\'e}es avec la m{\'e}trique BLEU - gains de deux points par rapport {\`a} notre syst{\`e}me de traduc- tion de parole de r{\'e}f{\'e}rence). Ces exp{\'e}riences sont faites pour une t{\^a}che de TAP (fran{\c{c}}ais-anglais) pour laquelle un corpus a {\'e}t{\'e} sp{\'e}cialement con{\c{c}}u (ce corpus, mis {\`a} la disposition de la communaut{\'e} TALN, est aussi d{\'e}crit en d{\'e}tail dans l{'}article)."
2015.iwslt-papers.11,An open-source toolkit for word-level confidence estimation in machine translation,2015,33,5,4,0,5281,christophe servan,Proceedings of the 12th International Workshop on Spoken Language Translation: Papers,0,"Recently, a growing need of Confidence Estimation (CE) for Statistical Machine Translation (SMT) systems in Computer Aided Translation (CAT), was observed. However, most of the CE toolkits are optimized for a single target language (mainly English) and, as far as we know, none of them are dedicated to this specific task and freely available. This paper presents an open-source toolkit for predicting the quality of words of a SMT output, whose novel contributions are (i) support for various target languages, (ii) handle a number of features of different types (system-based, lexical , syntactic and semantic). In addition, the toolkit also integrates a wide variety of Natural Language Processing or Machine Learning tools to pre-process data, extract features and estimate confidence at word-level. Features for Word-level Confidence Estimation (WCE) can be easily added / removed using a configuration file. We validate the toolkit by experimenting in the WCE evaluation framework of WMT shared task with two language pairs: French-English and English-Spanish. The toolkit is made available to the research community with ready-made scripts to launch full experiments on these language pairs, while achieving state-of-the-art and reproducible performances."
W14-3342,{LIG} System for Word Level {QE} task at {WMT}14,2014,9,20,3,1,37407,ngocquang luong,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes our Word-level QE system for WMT 2014 shared task on Spanish-English pair. Compared to WMT 2013, this year's task is different due to the lack of SMT setting information and additional resources. We report how we overcome this challenge to retain most of the important features which performed well last year in our system. Novel features related to the availability of multiple systems output (new point of this year) are also proposed and experimented along with baseline set. The system is optimized by several ways: tuning the classification threshold, combining with WMT 2013 data, and refining using Feature Selection strategy on our development set, before dealing with the test set for submission."
W14-0301,Word Confidence Estimation for {SMT} N-best List Re-ranking,2014,21,5,3,1,37407,ngocquang luong,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"This paper proposes to use Word Confidence Estimation (WCE) information to improve MT outputs via N-best list reranking. From the confidence label assigned for each word in the MT hypothesis, we add six scores to the baseline loglinear model in order to re-rank the N-best list. Firstly, the correlation between the WCE-based sentence-level scores and the conventional evaluation scores (BLEU, TER, TERp-A) is investigated. Then, the N-best list re-ranking is evaluated over different WCE system performance levels: from our real and efficient WCE system (ranked 1st during last WMT 2013 Quality Estimation Task) to an oracle WCE (which simulates an interactive scenario where a user simply validates words of a MT hypothesis and the new output will be automatically re-generated). The results suggest that our real WCE system slightly (but significantly) improves the baseline while the oracle one extremely boosts it; and better WCE leads to better MT quality."
vacher-etal-2014-sweet,The Sweet-Home speech and multimodal corpus for home automation interaction,2014,18,34,2,1,18169,michel vacher,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Ambient Assisted Living aims at enhancing the quality of life of older and disabled people at home thanks to Smart Homes and Home Automation. However, many studies do not include tests in real settings, because data collection in this domain is very expensive and challenging and because of the few available data sets. The S WEET-H OME multimodal corpus is a dataset recorded in realistic conditions in D OMUS, a fully equipped Smart Home with microphones and home automation sensors, in which participants performed Activities of Daily living (ADL). This corpus is made of a multimodal subset, a French home automation speech subset recorded in Distant Speech conditions, and two interaction subsets, the first one being recorded by 16 persons without disabilities and the second one by 6 seniors and 5 visually impaired people. This corpus was used in studies related to ADL recognition, context aware interaction and distant speech recognition applied to home automation controled through voice."
2014.eamt-1.23,An efficient two-pass decoder for {SMT} using word confidence estimation,2014,0,3,3,1,37407,ngocquang luong,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,"During decoding, the Statistical Machine Translation (SMT) decoder travels over all complete paths on the Search Graph (SG), seeks those with cheapest costs and back-tracks to read off the best translations. Although these winners beat the rest in model scores, there is no certain guarantee that they have the highest quality with respect to the human references. This paper exploits Word Confidence Estimation (WCE) scores in the second pass of decoding to enhance the Machine Translation (MT) quality. By using the confidence score of each word in the N-best list to update the cost of SG hypotheses containing it, we hope to  reinforce  or  weaken  them relied on word quality. After the update, new best translations are re-determined using updated costs. In the experiments on our real WCE scores and ideal (oracle) ones, the latter significantly boosts one-pass de-coder by 7.87 BLEU points, meanwhile the former yields an improvement of 1.49 points for the same metric."
W13-3916,Experimental Evaluation of Speech Recognition Technologies for Voice-based Home Automation Control in a Smart Home,2013,-1,-1,2,1,18169,michel vacher,Proceedings of the Fourth Workshop on Speech and Language Processing for Assistive Technologies,0,None
W13-2248,{LIG} System for {WMT}13 {QE} Task: Investigating the Usefulness of Features in Word Confidence Estimation for {MT},2013,10,15,2,1,37407,ngocquang luong,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper presents the LIGxe2x80x99s systems submitted for Task 2 of WMT13 Quality Estimation campaign. This is a word confidence estimation (WCE) task where each participant was asked to label each word in a translated text as a binary ( Keep/Change) or multi-class (Keep/Substitute/Delete) category. We integrate a number of features of various types (system-based, lexical, syntactic and semantic) into the conventional feature set, for our baseline classifier training. After the experiments with all features, we deploy a xe2x80x9cFeature Selectionxe2x80x9d strategy to keep only the best performing ones. Then, a method that combines multiple xe2x80x9cweakxe2x80x9d classifiers to build a strong xe2x80x9ccompositexe2x80x9d classifier by taking advantage of their complementarity is presented and experimented. We then select the best systems for submission and present the official results obtained."
F13-2004,Driven Decoding for machine translation (Vers un d{\\'e}codage guid{\\'e} pour la traduction automatique) [in {F}rench],2013,0,0,1,1,5784,benjamin lecouteux,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
W12-1404,Reconnaissance d{'}ordres domotiques en conditions bruit{\\'e}es pour l{'}assistance {\\`a} domicile (Recognition of Voice Commands by Multisource {ASR} and Noise Cancellation in a Smart Home Environment) [in {F}rench],2012,-1,-1,1,1,5784,benjamin lecouteux,"{JEP}-{TALN}-{RECITAL} 2012, Workshop {ILADI} 2012: Interactions Langagi{\\`e}res pour personnes Ag{\\'e}es Dans les habitats Intelligents ({ILADI} 2012: Language Interaction for Elderly in Smart Homes)",0,None
F12-1083,Reconnaissance automatique de la parole distante dans un habitat intelligent : m{\\'e}thodes multi-sources en conditions r{\\'e}alistes (Distant Speech Recognition in a Smart Home : Comparison of Several Multisource {ASR}s in Realistic Conditions) [in {F}rench],2012,-1,-1,1,1,5784,benjamin lecouteux,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
F12-1088,Pr{\\'e}diction de l{'}indexabilit{\\'e} d{'}une transcription (Prediction of transcription indexability) [in {F}rench],2012,-1,-1,2,0,43534,gregory senay,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
2012.iwslt-evaluation.13,The {LIG} {E}nglish to {F}rench machine translation system for {IWSLT} 2012,2012,15,5,2,0.231177,5610,laurent besacier,Proceedings of the 9th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper presents the LIG participation to the E-F MT task of IWSLT 2012. The primary system proposed made a large improvement (more than 3 point of BLEU on tst2010 set) compared to our last year participation. Part of this improvment was due to the use of an extraction from the Gigaword corpus. We also propose a preliminary adaptation of the driven decoding concept for machine translation. This method allows an efficient combination of machine translation systems, by rescoring the log-linear model at the N-best list level according to auxiliary systems: the basis technique is essentially guiding the search using one or previous system outputs. The results show that the approach allows a significant improvement in BLEU score using Google translate to guide our own SMT system. We also try to use a confidence measure as an additional log-linear feature but we could not get any improvment with this technique."
W11-2154,The {LIGA} ({LIG}/{LIA}) Machine Translation System for {WMT} 2011,2011,7,2,3,0,43044,marion potet,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes the system submitted by the Laboratory of Informatics of Grenoble (LIG) for the fifth Workshop on Statistical Machine Translation. We participated to the news shared translation task for the French-English language pair. We investigated differents techniques to simply deal with Out-Of-Vocabulary words in a statistical phrase-based machine translation system and analyze their impact on translation quality. The final submission is a combination between a standard phrase-based system using the Moses decoder, with appropriate setups and pre-processing, and a lemmatized system to deal with Out-Of-Vocabulary conjugated verbs."
2011.iwslt-evaluation.8,{LIG} {E}nglish-{F}rench spoken language translation system for {IWSLT} 2011,2011,8,1,1,1,5784,benjamin lecouteux,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the system developed by the LIG laboratory for the 2011 IWSLT evaluation. We participated to the English-French MT and SLT tasks. The development of a reference translation system (MT task), as well as an ASR output translation system (SLT task) are presented. We focus this year on the SLT task and on the use of multiple 1-best ASR outputs to improve overall translation quality. The main experiment presented here compares the performance of a SLT system where multiple ASR 1-best are combined before translation (source combination), with a SLT system where multiple ASR 1-best are translated, the system combination being conducted afterwards on the target side (target combination). The experimental results show that the second approach (target combination) overpasses the first one, when the performance is measured with BLEU."
senay-etal-2010-transcriber,Transcriber Driving Strategies for Transcription Aid System,2010,9,4,3,0,43534,gregory senay,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Speech recognition technology suffers from a lack of robustness which limits its usability for fully automated speech-to-text transcription, and manual correction is generally required to obtain perfect transcripts. In this paper, we propose a general scheme for semi-automatic transcription, in which the system and the transcriptionist contribute jointly to the speech transcription. The proposed system relies on the editing of confusion networks and on reactive decoding, the latter one being supposed to take benefits from the manual correction and improve the error rates. In order to reduce the correction time, we evaluate various strategies aiming to guide the transcriptionist towards the critical areas of transcripts. These strategies are based on graph density-based criterion and two semantic consistency criterion; using a corpus-based method and a web-search engine. They allow to indicate to the user the areas which present severe lacks of understandability. We evaluate these driving strategies by simulating the correction process of French broadcast news transcriptions. Results show that interactive decoding improves the correction act efficiency with all driving strategies and semantic information must be integrated into the interactive decoding process."
