2009.jeptalnrecital-recital.7,E09-3003,1,0.789036,"Missing"
2009.jeptalnrecital-recital.7,N07-1058,0,0.0275448,"Missing"
2009.jeptalnrecital-recital.7,W08-0909,0,0.0255943,"Missing"
2009.jeptalnrecital-recital.7,P05-1065,0,0.0686591,"Missing"
2011.jeptalnrecital-court.3,E09-3003,1,0.855621,"Missing"
2011.jeptalnrecital-court.3,P05-1065,0,0.0774079,"Missing"
2011.jeptalnrecital-court.3,W11-0813,1,0.772742,"Missing"
2016.jeptalnrecital-long.17,W14-1206,1,0.889865,"Missing"
2016.jeptalnrecital-long.17,francois-etal-2014-flelex,1,0.889021,"Missing"
2016.jeptalnrecital-long.17,N07-1058,0,0.0924425,"Missing"
2016.jeptalnrecital-long.17,P10-1023,0,0.0319453,"Missing"
2016.jeptalnrecital-long.17,W14-1821,0,0.0297761,"Missing"
2016.jeptalnrecital-long.17,P13-3015,0,0.0471564,"Missing"
2016.jeptalnrecital-long.17,L16-1035,1,0.86958,"Missing"
2020.lrec-1.169,C18-1218,1,0.896808,"Missing"
2020.lrec-1.169,W18-0503,0,0.0227085,"ject to caution. To the best of our knowledge, there is no equivalent corpus for other languages offering the possibility to align sentences at different levels of reading complexity. 2.2. Corpora of Reading Errors Apart from the use of parallel simplified corpora as a gold standard for text simplification, a number of studies have resorted to empirical measures of cognitive difficulty when reading in a native or foreign language including, but not exhaustively, eye-tracking data on readers suffering from ˇ autism spectrum disorder (Yaneva et al., 2016; Stajner et al., 2017) or from dyslexia (Bingel et al., 2018), as well as subjective annotations of difficult words (Paetzold and Specia, 2016; Tack et al., 2016; Yimam et al., 2018). As far as reading errors are concerned, the situation can be considered unsatisfactory given that we find a limited availability of resources of this type. For dyslexia, some examples of corpora with writing errors have been compiled, for instance that of (Pedler, 2007), a dataset of productions of dyslexic English readers (3,134 words and 363 errors), or Dyslist (Rello et al., 2014), a corpus of 83 texts written in Spanish by dyslexic children (with 887 misspelled words)."
2020.lrec-1.169,P11-2117,0,0.0286213,"ways available. In this section, we first report on resources for text simplification that are similar to ours in the sense that they include parallel original and simplified texts. Second, we discuss previous resources having reading errors annotated. 2.1. Parallel Corpora for Text Simplification Researchers in supervised text simplification have used English Wikipedia – Simple English Wikipedia (EW–SEW) ˇ sentence-aligned corpora for training the systems (Stajner and Nisioi, 2018). Zhu et al. (2010) were the first to investigate this way of collecting simpler versions of texts. Soon after, Coster and Kauchak (2011) built a corpus of 137K aligned sentence pairs and computed transformations to compare original to simplified sentences (rewordings, deletions, reorders, mergers and splits). This approach received much attention from researchers working on text simplification in English, until it got criticized by Xu et al. (2015). More recently, (Xu et al., 2015) have advocated for the use of the Newsela corpus (Newsela, 2016) in automatic text simplification and demonstrated its value. This corpus was initially developed by editors and intended for learners of English. The data-set (version 2016-01-29.1) is"
2020.lrec-1.169,W16-4107,1,0.761642,"Junior (BTJ) or Images DOC, which are standard scientific magazines in French for young readers. 1354 Figure 1: Interface for searching a corpus. 3.1. Manual Simplifications a. L EXICAL REPLACEMENTS were performed using Manulex4 (L´et´e et al., 2004), ReSyf5 (Billami et al., 2018) and Lexique36 (New et al., 2001), three available lexical resources in French that contain indications of the presence of a word in a school level (Manulex), reading difficulty grades (ReSyf) and word frequencies in standard oral and written French (Lexique 3). We took into account specifications already defined in Gala and Ziegler (2016) and Franc¸ois et al. (2016). For instance, long and less frequent words, with irregular graphemes and complex syllable structures were modified by simpler synonyms: e.g. volumineux (unwieldy) by gros (big) and ressemblaient (seemed) by e´ taient (were). Text simplification is defined as a reduction of the complexity of a text while preserving its original content (Saggion, 2017). By doing this, the text may be more easily read and understood by a target audience (in our work, we focus on the needs of poor-readers and dyslexic children). From an initial set of 79 original texts, 104 different"
2020.lrec-1.169,rello-etal-2014-dyslist,0,0.029292,"ˇ autism spectrum disorder (Yaneva et al., 2016; Stajner et al., 2017) or from dyslexia (Bingel et al., 2018), as well as subjective annotations of difficult words (Paetzold and Specia, 2016; Tack et al., 2016; Yimam et al., 2018). As far as reading errors are concerned, the situation can be considered unsatisfactory given that we find a limited availability of resources of this type. For dyslexia, some examples of corpora with writing errors have been compiled, for instance that of (Pedler, 2007), a dataset of productions of dyslexic English readers (3,134 words and 363 errors), or Dyslist (Rello et al., 2014), a corpus of 83 texts written in Spanish by dyslexic children (with 887 misspelled words). Such corpora can be used as source of knowledge to study different aspects of dyslexia. They can also be used to develop tools such as spellcheckers and games, and for screening with applications for readers (Rauschenberger et al., 2019), e.g. Dytective (Rello et al., 2016) a web-based game with different stages to detect dyslexia with machine learning prediction models. Nb SIMP 15 45 24 20 104 Level IReST CE1 CE2 CM1 Type of corpus 1 LIT 9 SCI 15 LIT 10 SCI 14 LIT 10 SCI 10 LIT 10 SCI 40 39 Table 1: Di"
2020.lrec-1.169,L18-1479,0,0.217993,"Missing"
2020.lrec-1.169,W17-5030,0,0.100406,"Missing"
2020.lrec-1.169,L16-1035,1,0.909191,"Missing"
2020.lrec-1.169,Q15-1021,0,0.0796819,"text simplification have used English Wikipedia – Simple English Wikipedia (EW–SEW) ˇ sentence-aligned corpora for training the systems (Stajner and Nisioi, 2018). Zhu et al. (2010) were the first to investigate this way of collecting simpler versions of texts. Soon after, Coster and Kauchak (2011) built a corpus of 137K aligned sentence pairs and computed transformations to compare original to simplified sentences (rewordings, deletions, reorders, mergers and splits). This approach received much attention from researchers working on text simplification in English, until it got criticized by Xu et al. (2015). More recently, (Xu et al., 2015) have advocated for the use of the Newsela corpus (Newsela, 2016) in automatic text simplification and demonstrated its value. This corpus was initially developed by editors and intended for learners of English. The data-set (version 2016-01-29.1) is composed of 10,787 news articles in English: 1,911 articles in their original form and in 4 equivalent versions rewritten by humans to suit different reading levels. Having different versions of an original article offers a great potential to study linguistic transformations, which explains why the automatic text"
2020.lrec-1.169,L16-1077,0,0.212121,"ed - using the Lexile formula (Stenner, 1996) - should be subject to caution. To the best of our knowledge, there is no equivalent corpus for other languages offering the possibility to align sentences at different levels of reading complexity. 2.2. Corpora of Reading Errors Apart from the use of parallel simplified corpora as a gold standard for text simplification, a number of studies have resorted to empirical measures of cognitive difficulty when reading in a native or foreign language including, but not exhaustively, eye-tracking data on readers suffering from ˇ autism spectrum disorder (Yaneva et al., 2016; Stajner et al., 2017) or from dyslexia (Bingel et al., 2018), as well as subjective annotations of difficult words (Paetzold and Specia, 2016; Tack et al., 2016; Yimam et al., 2018). As far as reading errors are concerned, the situation can be considered unsatisfactory given that we find a limited availability of resources of this type. For dyslexia, some examples of corpora with writing errors have been compiled, for instance that of (Pedler, 2007), a dataset of productions of dyslexic English readers (3,134 words and 363 errors), or Dyslist (Rello et al., 2014), a corpus of 83 texts writte"
2020.lrec-1.169,W18-0507,1,0.868881,"Missing"
2020.lrec-1.169,C10-1152,0,0.223625,"cifically annotated corpora (i.e., with errors) are very costly to build and not al1353 Nb ORIG 10 25 24 20 79 ways available. In this section, we first report on resources for text simplification that are similar to ours in the sense that they include parallel original and simplified texts. Second, we discuss previous resources having reading errors annotated. 2.1. Parallel Corpora for Text Simplification Researchers in supervised text simplification have used English Wikipedia – Simple English Wikipedia (EW–SEW) ˇ sentence-aligned corpora for training the systems (Stajner and Nisioi, 2018). Zhu et al. (2010) were the first to investigate this way of collecting simpler versions of texts. Soon after, Coster and Kauchak (2011) built a corpus of 137K aligned sentence pairs and computed transformations to compare original to simplified sentences (rewordings, deletions, reorders, mergers and splits). This approach received much attention from researchers working on text simplification in English, until it got criticized by Xu et al. (2015). More recently, (Xu et al., 2015) have advocated for the use of the Newsela corpus (Newsela, 2016) in automatic text simplification and demonstrated its value. This"
2020.readi-1.13,L18-1140,1,0.847959,"Missing"
2020.readi-1.13,francois-etal-2014-flelex,1,0.862687,"Missing"
2020.readi-1.13,L16-1032,1,0.866698,"Missing"
2020.readi-1.13,L16-1035,1,0.895362,"Missing"
2020.readi-1.13,W18-0514,1,0.75536,"Missing"
2020.readi-1.13,W16-6501,0,0.031176,"Missing"
2020.readi-1.5,W03-1602,0,0.308589,"Missing"
2020.readi-1.5,W13-1502,0,0.0506523,"Missing"
2020.readi-1.5,C18-1218,1,0.834201,"ection 3) and discuss these results while proposing some future work directions (Section 4). 2.1 Synonym pair Sentence pair You should go for a walk along the […] to relax 44 characters / target word = n-2 My parents have worked by the […] for many years 45 characters / target word = n-3 Cond. 1 Incomplete letters coast characters = 5 / frequency = 48 / neighbors = 3 shore characters = 5 / frequency = 24 / neighbors = 13 You should go for a walk along the coast to relax Cond. 2 halte 2. Reading material Reading material was created in French using ReSyf, a French lexicon with graded synonyms (Billami et al., 2018) and Lexique3, a lexical database providing word frequencies (in occurrences / million) and word neighborhood size (Coltheart’s N) of standard written and oral French (New et al., 2001). The whole material was created in three steps, in order to generate pairs of synonyms with constrained linguistic properties (i.e. target words) embedded within pairs of interchangeable sentences. An example (in English) is given in Table 1. Non-functional fovea Scotoma Eccentric letter Apparatus & stimuli You should go for a walk along the shore to relax My parents have worked by the shore for many years My p"
C16-1094,J08-1001,0,0.0953665,"the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their characteristics. Barzilay and Lapata (2008) considered a text as a matrix of discourse entities present in each sentence. The cohesive level of a text is then computed based on the transitions between those entities. Pitler and Nenkova (2008) implemented this model through 17 readability variables, but none was significantly correlated with difficulty. Feng et al. (2009) also replicated this technique, without getting more efficient features. Dascalu et al. (2013) computed other characteristics of lexical chains and co-reference pairs (such as the number of chains, the distance between entities, the average word length of entities, etc"
C16-1094,E09-1027,0,0.154132,"itive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohesion aspects for the"
C16-1094,W13-1504,0,0.0306626,"o measured its association with text difficult and obtained a non significant correlation (r = −0.1). Later, McNamara et al. (2010) reached a similar conclusion, showing that an LSA-based variable has not much of a predictive power. On the opposite, Franc¸ois and Fairon (2012; 2013) obtained a higher correlation (r = 0.63) for an L2 corpus, while Dascalu et al. (2013) got good discriminating features using both LSA and LDA (Latent Dirichlet Allocation), when classifying TASA (Touchstone Applied Science Associates) texts. An alternative approach to LSA, Lexical Tightness (LT), was suggested by Flor et al. (2013). They define the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their cha"
C16-1094,E09-3003,1,0.866157,"Missing"
C16-1094,D12-1043,1,0.931401,"Missing"
C16-1094,J95-2003,0,0.785124,", perishing when the ship sank on the 15th April 1912. (Wikipedia) Figure 1: Example of anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker,"
C16-1094,muzerelle-etal-2014-ancor,0,0.0161241,"umptive anaphora or groups (the pronoun ils in Fig. 2 refers to the group composed of Antoine and Catherine). Based on these guidelines, a common batch, composed of 10 randomly selected files, was annotated by all the annotators. It was used to identify annotation divergences between annotators1 and to correct the annotation guide. We computed the overall inter-annotator agreement on this common batch using the mean Krippendorff’s alpha on each text and we obtained 0.47, which corresponds to a moderate agreement between annotators. Such value is however not unusual in co-reference annotation (Muzerelle et al., 2014). Then, following the annotation guide, each expert annotated a batch of 12 texts from the corpus. At the end of the process, the principal annotator checked all batches against the guidelines, thus creating the reference for our experimentation. [Antoine] 1/S/NPr/partie(3) fait la connaissance de [Catherine] 2/CN/NPr/partie(3). [Antoine] 1/S/NPr est [un beau parleur ] 1/X/GNI et [la jeune fille] 2/S/GND [s’] 2/X/Pronref int´eresse a` [lui] 1/OI/Pron. [Ils] 3/S/Pron vont au cin´ema ensemble. Figure 2: Example of annotated data : the number of the entity, the syntactic function and the category"
C16-1094,D08-1020,0,0.15088,"tic features in their cognitive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohe"
C16-1094,J04-3003,0,0.0378274,"anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker, 2005). The composition, the length or the choice of the first mention of the co-referenc"
C18-1218,S12-1046,0,0.0759824,"Missing"
C18-1218,J10-2002,0,0.0361838,"ven two words wi and wj , each associated to a difficulty level (li and lj ) and to a feature vector (vi and vj ), we create a pair &lt; wi , wj &gt; for which a new vector vij is obtained from the combination of the two vectors vi and vj . Several arithmetic operations can be used to carry out 8 The orthographic neighborhood of a word have been defined by Coltheart (1978) as all the words of same length and differing only by one letter (eg. FIST and GIST). 9 For a detailed description of these parameters, see (Gala et al., 2014). 2575 this combination, but we used substraction (vij = vi − vj ), as Tanaka-Ishii et al. (2010) showed that substraction was best for ranking texts by readability. Each pair &lt; wi , wj &gt; was also assigned a new difficulty level (lij ) obtained with the following rule: (1) if li &gt; lj , then lij = 1 and (2) if li &lt; lj , then lij = −1. As we had two original datasets, Manulex-3N and Manulex-Cont, we got two pair datasets. Second, to select the best predictors of word difficulty, we computed the Spearman correlations between each of our 69 variables and the new binary difficulty variable (Lij ). We used only the Manulex-3N dataset for this aim. Table 2 displays the correlation for some of th"
D12-1043,N04-1025,0,0.386002,"nition in the French speaking world as Henry’s. After those two major efforts, few works followed. It is worth mentioning two more authors: Mesnager (1989), who designed a classic formula 467 for children that draw inspiration from the Dale and Chall (1948) formula and Daoust et al. (1996), who developed SATO - CALIBRAGE, a program assessing text difficulty from the first to the eleventh grade. It can be considered as the first “AI formula” for French L1, since it made use of NLP-enabled features. It is also the last formula published for French L1, if we except the adaptation of the model by Collins-Thompson and Callan (2004) to French. As regards to French L2, the literature is even sparser. Tharp (1939) published a first formula taking into account one particularity of the L2 context: the cognates. Those are words sharing a similar form and meaning across two languages and having a facilitating effect in reading. This idea was recently replicated by Uitdenbogerd (2005), who combined a syntactic feature, the mean number of words per sentence, with the number of cognates per 100 words in her formula. Although taking into account this effect of the L1 on L2 reading is very interesting, these two studies are confine"
D12-1043,C10-2032,0,0.34066,"training data ; the use of NPL-enable features able to capture a wider range of readability factors, and the combination of those features through a machine learning 466 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 466–477, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics algorithm. Since the work of Si and Callan (2001), this paradigm have spawn several studies for English (Collins-Thompson and Callan, 2005; Heilman et al., 2008; Schwarm and Ostendorf, 2005; Feng et al., 2010). However, for French, the field is far from being so thriving. To our knowledge, only two “AI readability” have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2). This paper reports some experiments aimed at designing a more efficient readability model for FFL. In Section 2, it is further argue why a new formula was necessary for FFL. Section 3 covers the various methodological steps required to devise the model, whose results are reported in Section 4. Finally, Section 5 discusses some interesting insights gained by this work. 2 Readabilit"
D12-1043,R11-1061,1,0.819508,"Missing"
D12-1043,E09-3003,1,0.922334,"Missing"
D12-1043,W08-0909,0,0.13452,"tbooks, simplified newspapers or web resources) as training data ; the use of NPL-enable features able to capture a wider range of readability factors, and the combination of those features through a machine learning 466 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 466–477, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics algorithm. Since the work of Si and Callan (2001), this paradigm have spawn several studies for English (Collins-Thompson and Callan, 2005; Heilman et al., 2008; Schwarm and Ostendorf, 2005; Feng et al., 2010). However, for French, the field is far from being so thriving. To our knowledge, only two “AI readability” have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2). This paper reports some experiments aimed at designing a more efficient readability model for FFL. In Section 2, it is further argue why a new formula was necessary for FFL. Section 3 covers the various methodological steps required to devise the model, whose results are reported in Section 4. Finally, Section 5 discusses some inter"
D12-1043,2010.jeptalnrecital-recital.2,0,0.0661982,"Missing"
D12-1043,D08-1020,0,0.0366331,"s described by these verbs. We therefore replicated and enhanced the feature set proposed by Franc¸ois (2009), considering either binary indicators or proportions of the use of tenses or moods in a text. 3.2.3 Semantic features The importance of semantic and cognitive factors have been particularly stressed by the structuro-cognitivist paradigm, although Miller and Kintsch (1980), as well as Kemper (1983), eventually admitted not being able to demonstrate the superiority of those new predictors over traditional ones. More recent work also reported limited evidence of this alleged superiority (Pitler and Nenkova, 2008; Feng et al., 2010). In order to clarify as much as possible the situation for FFL, we implemented the following features: Personnalization level: Dale and Tyler (1934) suggested that informal texts should be easier to read and that informality might be assessed through the type of personal pronouns found in a text. On this assumption, 13 variables were defined to take into account various personal pronouns proportions in the text. Conceptual density: Kintsch et al. (1975) showed that the number of propositions as well as the number of different arguments in a sentence influence its reading t"
D12-1043,P05-1065,0,0.573529,"spapers or web resources) as training data ; the use of NPL-enable features able to capture a wider range of readability factors, and the combination of those features through a machine learning 466 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 466–477, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics algorithm. Since the work of Si and Callan (2001), this paradigm have spawn several studies for English (Collins-Thompson and Callan, 2005; Heilman et al., 2008; Schwarm and Ostendorf, 2005; Feng et al., 2010). However, for French, the field is far from being so thriving. To our knowledge, only two “AI readability” have been designed so far for French L1 and only one for French as a foreign language (FFL) (see Section 2). This paper reports some experiments aimed at designing a more efficient readability model for FFL. In Section 2, it is further argue why a new formula was necessary for FFL. Section 3 covers the various methodological steps required to devise the model, whose results are reported in Section 4. Finally, Section 5 discusses some interesting insights gained by thi"
E09-3003,P05-1065,0,0.53726,"new formula for a related language. Therefore, we had to draw our inspiration from the English-speaking world, which has recently experienced a revival of interest in research on readability. Taking advantage of the increasing power of computers and the development of NLP techniques, researchers have been able to experiment with more complex variables. CollinsThompson et al. (2005) presented a variation of a multinomial naive Bayesian classifier they called the “Smoothed Unigram” model. We retained from their work the use of language models instead of word lists to measure lexical complexity. Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Heilman et al. (2007) extended the “Smoothed Unigram” model by the recognition of syntactic structures, in order to assess L2 English • The CEFR was published in 2001, so only 20 textbooks published since then were considered. This restriction also ensures that the language resembles present-day spoken French. Because this research only spans the last year, attempts to discove"
E09-3003,N07-1058,0,0.0669319,"the development of NLP techniques, researchers have been able to experiment with more complex variables. CollinsThompson et al. (2005) presented a variation of a multinomial naive Bayesian classifier they called the “Smoothed Unigram” model. We retained from their work the use of language models instead of word lists to measure lexical complexity. Schwarm and Ostendorf (2005) developed a SVM categoriser combining a classifier based on trigram language models (one for each level of difficulty), some parsing features such as average tree height, and variables traditionally used in readability. Heilman et al. (2007) extended the “Smoothed Unigram” model by the recognition of syntactic structures, in order to assess L2 English • The CEFR was published in 2001, so only 20 textbooks published since then were considered. This restriction also ensures that the language resembles present-day spoken French. Because this research only spans the last year, attempts to discover interesting variables are still at an early stage. We explored the efficiency of some traditional features such as the type-token ratio, the number of letters per word, and the average sentence length, and found that, on our corpus, only th"
E09-3003,W08-0909,0,0.738986,"ens, Greece, 2 April 2009. 2009 Association for Computational Linguistics 19 was adapted for three different educational levels. His formulae are by far the best and most frequently used in the French-speaking world. Later, Richaudeau (1979) suggested a criteria of “linguistic efficiency” based on experiments on shortterm memory, while Mesnager (1989) coined what is still, to the best of our knowledge, the most recent specific formula for French, with children as its target. texts. Later, they improved the combination of their various lexical and grammatical features using regression methods (Heilman et al., 2008). We also found regression methods to be the most efficient of the statistical models with which we experimented. In this article, we consider some ways to adapt these various ideas to the specific case of FFL readability. 3 Corpus description Compared to the mass of studies in English, readability in French has never enthused the research community. The cultural reasons for this are analysed by Boss´e-Andrieu (1993) (who basically argues that the idea of measuring text difficulty objectively seems far too pragmatic for the French spirit). It follows that there is little current research in th"
F12-2016,C10-2013,0,0.0266869,"Missing"
F12-2016,E99-1042,0,0.125646,"Missing"
F12-2016,C96-2183,0,0.314639,"Missing"
F12-2016,Y09-1013,0,0.0206499,"Missing"
F12-2016,W09-1802,0,0.0832225,"Missing"
F12-2016,W03-1602,0,0.150315,"Missing"
F12-2016,N09-2045,0,0.0586264,"Missing"
F12-2016,levy-andrew-2006-tregex,0,0.0718403,"Missing"
F12-2016,E06-1021,0,0.082197,"Missing"
F12-2016,D11-1038,0,0.145607,"Missing"
F12-2016,C10-1152,1,0.927434,"Missing"
F14-1009,P11-2087,0,0.0634643,"Missing"
F14-1009,C96-2183,0,0.0712039,"Missing"
F14-1009,D12-1043,1,0.889099,"Missing"
F14-1009,francois-etal-2014-flelex,1,0.541604,"Missing"
F14-1009,2008.jeptalnrecital-court.10,1,0.828336,"Missing"
F14-1009,P10-1023,0,0.0290368,"Missing"
F14-1009,quasthoff-etal-2006-corpus,0,0.015257,"Missing"
F14-1009,W04-2104,0,0.0312172,"Missing"
F14-1009,S12-1046,0,0.101046,"Missing"
F14-2014,C96-2183,0,0.146042,"Missing"
F14-2014,W08-0909,0,0.0817576,"Missing"
F14-2014,S12-1046,0,0.03083,"Missing"
F14-2014,W12-2019,0,0.0515376,"Missing"
francois-etal-2014-flelex,J90-1003,0,\N,Missing
francois-etal-2014-flelex,J08-4004,0,\N,Missing
francois-etal-2014-flelex,W11-0809,0,\N,Missing
L16-1032,W15-1804,0,0.016218,"automatic processing was problematic in these cases include: compounding, proper names, use of other languages (e.g. English), inconsistent spelling and incorrect optical-character recognition (OCR) of the texts from the corpus. Each of these items have been looked up in the lexical-semantic resource SALDO (Borin et al., 2013) and, if a corresponding entry was found, then the lemma and its POS were manually corrected. Besides that, all participles have been manually converted to either verbs or adjectives to adjust to an updated version of the annotation pipeline currently under development (Adesam et al., 2015). As an additional check, the candidate list was matched with 3 other resources: Base Vocabulary Pool, Kelly and Lexin to identify SVALex items not present in any of the resources. This way, a number of problematic cases, such as MWE written without a space, have been identified and 215 Resource SVALex Swedish Kelly Base Vocabulary Lexin # items 15,681 8,425 8,220 30,684 # overlap N/A 5,757 4,964 9,039 # missing N/A 9,924 10,717 6,642 Table 1: Size of the resources in number of entries. For each resource other than SVALex, the number of overlapping items with SVALEX and missing SVALex items is"
L16-1032,W14-0129,0,0.0568934,"sebooks of one level to favour those encountered in various documents. Another possibility would be to tune the training corpus size in order to limit the amount of peripheral terms at lower levels. We hope that such more educationallyoriented resource could help to answer some questions related to L2 vocabulary learning such as ”How many words per level should learners know?” or ”Which words at which levels?” Other perspectives include adding lexical information to the resource. To this aim, available information from other free lexical resources for Swedish, such as Lexin, Saldo, Swesaurus (Borin and Forsberg, 2014), etc. could be linked to SVALex items, enriching them with definitions, synonyms, English translations, domain mark-up, valency information, selected corpus examples demonstrating different senses of the words, compound analysis, etc. Furthermore, we are considering making SVALex available in linked open data format using the lemon model (McCrae et al., 2011). Some of the aforementioned resources are already available in such a format which would facilitate adopting a similar structure for SVALex. Finally, as regards the SVALex web platform, we plan to offer more diverse and task-related acce"
L16-1032,borin-etal-2012-korp,0,0.147371,"nto texts, exercises, lists and language examples. Apart from that, rich pedagogical and textual annotation has been added to the corpus, which now allows to filter material for texts by their topics and/or genres, and the rest of the material according to target skills and competences (e.g. vocabulary, speaking, reading, listening etc.), formats (brainstorming, gaps, matching, etc.) and units (single words, phrases, dictionary entries etc.). Linguistic annotation in the form of POS-tags, syntactic relations and lemmatization has been automatically added to the corpus using the Korp pipeline (Borin et al., 2012). To ensure comparability to FLELex (Franc¸ois et al., 2014), only a subset of COCTAILL containing texts aimed at reading comprehension has been used for the generation of SVALex. 3.2. Estimation of lexical frequencies The entries in SVALex consist of lemmas2 , their parts of speech (POS) and frequencies across 5 of the 6 CEFR levels. Apart from single words, our list is populated with multi-word expressions (MWE), which were extracted by the Korp-pipeline using a pilot feature based on a knowledge-based approach. If the analyzed token is a potential constituent in an MWE that is listed in the"
L16-1032,francois-etal-2014-flelex,1,0.548364,"Missing"
L16-1032,W14-3502,1,0.917812,"Missing"
L16-1032,N07-1058,0,0.113125,"ion of words, and (2) in which every text has been located on the CEFR scale. To our knowledge, such corpora are only reported for Swedish by Volodina et al. (2014) and for French by Franc¸ois (2014). Similar resources exist for a few other languages, but they deviate in one way or another from the requirements above. For example, a corpus of reading exam materials for Portuguese (Branco et al., 2014) are linked to the CEFR levels, but the size of the resource is rather modest. A corpus with English coursebooks is relatively large, but the texts are linked to a different scale of proficiency (Heilman et al., 2007). 3.1. Source corpus The corpus COCTAILL (Volodina et al., 2014) contains digitised coursebooks used for teaching CEFR-based L2 Swedish, where each level is represented on average by four coursebooks, except for C2, for which no coursebook was available, most likely because it is a near-native proficiency level. Coursebooks have been included into the corpus based on teachers’ judgements, which has been the most important criteria in corpus compilation. This means that only coursebooks that have been confirmed as appropriate for teaching CEFR-based courses at an announced level are included in"
L16-1032,L16-1035,1,0.639613,"Missing"
L16-1032,volodina-kokkinakis-2012-introducing,1,0.839781,"Missing"
L16-1032,W14-3510,1,0.829749,"Missing"
L16-1035,C12-1023,0,0.0953608,"Missing"
L16-1035,W14-1206,1,0.926562,"Missing"
L16-1035,francois-etal-2014-flelex,1,0.558779,"Missing"
L16-1035,S12-1066,0,0.109604,"Missing"
L16-1035,S12-1068,1,0.885579,"Missing"
L16-1035,P13-3015,0,0.407975,"Missing"
L16-1035,shardlow-2014-open,0,0.0241181,"Missing"
L16-1035,S12-1046,0,0.0434548,"Missing"
L16-1035,C04-1146,0,0.0599767,"notated entirely by nonnative speakers. We therefore reduced the initial corpus to a smaller sample of texts, while ensuring that the texts’ vocabulary remained as much varied as possible. To this end, we used a greedy selection algorithm which retrieved from the combined corpus a subset of texts that had the best possible lexical diversity. Given a limitation to the number of lexical units allowed in the final subset, the algorithm searches in the space of all possible subsets to identify a subset of texts that portrays the least lexical overlap. Following the similarity measures proposed by Weeds et al. (2004), we compute the overlap between the vocabulary V of each pair of texts i and j using the Jaccard coefficient (Equation 1). Each vocabulary V is defined as a set of unique (lemma, POS-tag) combinations. overlap(Vi , Vj ) = |Vi ∩ Vj | |Vi ∪ Vj | (1) The algorithm uses each document d in the corpus as a starting point to build a new subset T and then iteratively integrates into T a new document t that shares the least lexical units with the documents already included. After having constructed a complete subset T , the algorithm compares this subset to the previously constructed subsets in order"
L16-1035,D11-1038,0,0.0621475,"Missing"
L16-1035,C10-1152,0,0.118631,"Missing"
L16-1613,W08-0909,0,0.0468459,"Missing"
L18-1140,A00-1031,0,0.453864,"at least questionable lemma (e.g. she instead of her, 1 instead of first); • tags that can be correct, but are erroneous in the specific context of the word (e.g. to tag the word quiet in the peace and quiet as an adjective)1 . We also wanted to use a system able to detect phrasal verbs and multi-word expressions, as they are of prime importance in the L2 learning process (Paquot and Granger, 2012). Based on the previous constraints, the following five POStaggers were compared to find the most suitable one for the task: • the TreeTagger (Schmid, 1994), • a HMM-tagger based on the TnT-tagger (Brants, 2000) provided by the FreeLing library for C++ (Carreras et al., 2004), • the left-to-right model of the Stanford Log-linear POS Tagger (Toutanova et al., 2003), • the tagger provided by the SVMTool (Gim´enez and M`arquez, 2004) • and a tagger based on automatic feature extraction provided by the NLP4J module (Choi, 2016). To compare the taggers, we assessed their performance on one hundred sentences randomly sampled from the corpus. Two human experts manually checked the annotation output of each tagger and assigned each tagged word one of the five following categories: (0): correct lemma and POSt"
L18-1140,carreras-etal-2004-freeling,0,0.186068,"Missing"
L18-1140,N16-1031,0,0.0143668,"they are of prime importance in the L2 learning process (Paquot and Granger, 2012). Based on the previous constraints, the following five POStaggers were compared to find the most suitable one for the task: • the TreeTagger (Schmid, 1994), • a HMM-tagger based on the TnT-tagger (Brants, 2000) provided by the FreeLing library for C++ (Carreras et al., 2004), • the left-to-right model of the Stanford Log-linear POS Tagger (Toutanova et al., 2003), • the tagger provided by the SVMTool (Gim´enez and M`arquez, 2004) • and a tagger based on automatic feature extraction provided by the NLP4J module (Choi, 2016). To compare the taggers, we assessed their performance on one hundred sentences randomly sampled from the corpus. Two human experts manually checked the annotation output of each tagger and assigned each tagged word one of the five following categories: (0): correct lemma and POStag, (1): correct lemma, but wrong POS-tag, (2): wrong lemma, but correct POS-tag, (3): wrong lemma and wrong POS-tag, (4): segmentation error (e.g. it- or ’pages), which usually leads to tagging or lemmatization errors. The agreement between the two human experts was calculated using Cohen’s Kappa. Agreement scores v"
L18-1140,francois-etal-2014-flelex,1,0.536032,"Missing"
L18-1140,L16-1032,1,0.603751,"Missing"
L18-1140,gimenez-marquez-2004-svmtool,0,0.216597,"Missing"
L18-1140,L16-1035,1,0.866869,"Missing"
L18-1140,N03-1033,0,0.160292,"of the word (e.g. to tag the word quiet in the peace and quiet as an adjective)1 . We also wanted to use a system able to detect phrasal verbs and multi-word expressions, as they are of prime importance in the L2 learning process (Paquot and Granger, 2012). Based on the previous constraints, the following five POStaggers were compared to find the most suitable one for the task: • the TreeTagger (Schmid, 1994), • a HMM-tagger based on the TnT-tagger (Brants, 2000) provided by the FreeLing library for C++ (Carreras et al., 2004), • the left-to-right model of the Stanford Log-linear POS Tagger (Toutanova et al., 2003), • the tagger provided by the SVMTool (Gim´enez and M`arquez, 2004) • and a tagger based on automatic feature extraction provided by the NLP4J module (Choi, 2016). To compare the taggers, we assessed their performance on one hundred sentences randomly sampled from the corpus. Two human experts manually checked the annotation output of each tagger and assigned each tagged word one of the five following categories: (0): correct lemma and POStag, (1): correct lemma, but wrong POS-tag, (2): wrong lemma, but correct POS-tag, (3): wrong lemma and wrong POS-tag, (4): segmentation error (e.g. it- or"
pho-etal-2014-multiple,W12-2704,0,\N,Missing
pho-etal-2014-multiple,P03-1054,0,\N,Missing
pho-etal-2014-multiple,P13-2076,1,\N,Missing
pho-etal-2014-multiple,E12-2021,0,\N,Missing
pho-etal-2014-multiple,W06-1416,0,\N,Missing
pho-etal-2014-multiple,P05-1045,0,\N,Missing
R11-1061,D08-1020,0,0.0513646,".173 0.091 0.283 0.03 0.09 −0.0005 0.02 0.02 Table 1: Pearson correlation between independent variables and text difficulty. Significance levels are noted as follows: 1 p < 0.05 ; 2 p < 0.01 ; 3 p < 0.0001 4.2 N-gram Models n-grams in our references. We used the arithmetic mean (meanNGProb), the median (medianNGProb), and the geometrical mean (gmeanNGProb) of those probabilities for a given text. In contrast to MWEs, the use of n-gram models in readability is not new. They were first applied to the field by Si and Callan (2001) as a set of unigram models specific to every level of difficulty. Pitler and Nenkova (2008) later showed that even a single unigram model is an efficient predictor for readability. Meanwhile, higher order models have been developed by Schwarm and Ostendorf (2005) or Kate et al. (2010). The former authors selected the perplexity of a trigram model as one of their predictors, while the latter preferred to directly use the normalized probability outputted by the ngram model (see Equation 1). In this study, we defined the 7 following variables to assess the efficiency of n-gram models in the context of readability: • Once more, for the sake of comparison, we developed a unigram model, b"
R11-1061,E09-3003,1,0.918282,"Missing"
R11-1061,P05-1065,0,0.731475,"p < 0.05 ; 2 p < 0.01 ; 3 p < 0.0001 4.2 N-gram Models n-grams in our references. We used the arithmetic mean (meanNGProb), the median (medianNGProb), and the geometrical mean (gmeanNGProb) of those probabilities for a given text. In contrast to MWEs, the use of n-gram models in readability is not new. They were first applied to the field by Si and Callan (2001) as a set of unigram models specific to every level of difficulty. Pitler and Nenkova (2008) later showed that even a single unigram model is an efficient predictor for readability. Meanwhile, higher order models have been developed by Schwarm and Ostendorf (2005) or Kate et al. (2010). The former authors selected the perplexity of a trigram model as one of their predictors, while the latter preferred to directly use the normalized probability outputted by the ngram model (see Equation 1). In this study, we defined the 7 following variables to assess the efficiency of n-gram models in the context of readability: • Once more, for the sake of comparison, we developed a unigram model, based on Lexique3 probabilites (New et al., 2007) UnigM. We computed all these variables for each order of model from 2 to 5 using the frequencies stored in our two referenc"
R11-1061,C10-1062,0,0.212204,"ram Models n-grams in our references. We used the arithmetic mean (meanNGProb), the median (medianNGProb), and the geometrical mean (gmeanNGProb) of those probabilities for a given text. In contrast to MWEs, the use of n-gram models in readability is not new. They were first applied to the field by Si and Callan (2001) as a set of unigram models specific to every level of difficulty. Pitler and Nenkova (2008) later showed that even a single unigram model is an efficient predictor for readability. Meanwhile, higher order models have been developed by Schwarm and Ostendorf (2005) or Kate et al. (2010). The former authors selected the perplexity of a trigram model as one of their predictors, while the latter preferred to directly use the normalized probability outputted by the ngram model (see Equation 1). In this study, we defined the 7 following variables to assess the efficiency of n-gram models in the context of readability: • Once more, for the sake of comparison, we developed a unigram model, based on Lexique3 probabilites (New et al., 2007) UnigM. We computed all these variables for each order of model from 2 to 5 using the frequencies stored in our two references: Le Soir and Google"
R11-1061,J93-1007,0,0.330808,"significant (Kilgarriff, 2005). The common solution to this issue is to empirically set a higher threshold. It has an obvious flaw: the threshold is only valid for a given corpus or one of comparable size. Once more, the use of a reference circumvents this difficulty: since the size is constant, an optimal threshold can be fixed once and for all. In our study, the selected threshold values were function of the precision of the extractor (see 4.1). The Extractor Regarding the extraction process of MWEs, we use a three-step state-of-the-art procedure which draws on the work of Daile (1995) and Smadja (1993) in that it combines a linguistic filter with association measures (AM). Concretely, the texts are first POS tagged to clear most lexical ambiguities1 . We then identify all nominal MWE candidates in the tagged text with the help of a library of transducers 2 (or syntactic patterns). Finally, the list of candidates is submitted to the statistical validation module which assigns an AM to each of them. After some experiments, we retained the fair log-likelihood ratio (Silva and Lopes, 1999) as our AM, since it allows to process units that are longer than bigrams. As with all measures of associat"
R11-1061,W11-0813,1,0.885649,"Missing"
R13-1013,H05-1043,0,0.241923,"Missing"
R13-1013,W10-3110,0,0.0228226,"Contextual Valence Shifters Noémi Boubel Thomas François Hubert Naets Cental, ILC (Université catholique de Louvain) {noemi.boubel,thomas.francois,hubert.naets}@uclouvain.be Abstract Polanyi and Zaenen (2004) first postulated the existence of contextual valence shifters, which are contextual phenomena altering the prior polarity of a term. Afterwards, some of these phenomena (such as negative or conditional syntactic structures) were dealt with on a case by case basis (Das and Chen, 2001; Na et al., 2004; Popescu and Etzioni, 2005; Pang et al., 2002; Wilson et al., 2005; Wilson et al., 2006; Councill et al., 2010). Studies addressing the phenomenon as a whole flourished later. They aimed at best modelling the expression of opinions (Polanyi and Zaenen, 2004; Taboada et al., 2011; Hatzivassiloglou and Wiebe, 2000; Morsy and Rafea, 2012; Musat and Trausan-Matu, 2010), before embedding those in a classification system. The main purposes of these studies are to determine a list of contextual valence shifters that impact the polarity of a term as well as to define the nature of this impact. However, these lists are often manually built from linguistic intuitions and not learned from language data. Works rel"
R13-1013,J11-2001,0,0.0294771,"n.be Abstract Polanyi and Zaenen (2004) first postulated the existence of contextual valence shifters, which are contextual phenomena altering the prior polarity of a term. Afterwards, some of these phenomena (such as negative or conditional syntactic structures) were dealt with on a case by case basis (Das and Chen, 2001; Na et al., 2004; Popescu and Etzioni, 2005; Pang et al., 2002; Wilson et al., 2005; Wilson et al., 2006; Councill et al., 2010). Studies addressing the phenomenon as a whole flourished later. They aimed at best modelling the expression of opinions (Polanyi and Zaenen, 2004; Taboada et al., 2011; Hatzivassiloglou and Wiebe, 2000; Morsy and Rafea, 2012; Musat and Trausan-Matu, 2010), before embedding those in a classification system. The main purposes of these studies are to determine a list of contextual valence shifters that impact the polarity of a term as well as to define the nature of this impact. However, these lists are often manually built from linguistic intuitions and not learned from language data. Works relying on a corpus of texts to develop resources that best reflect the actual role played by the linguistic context for opinion mining are few. Li et al. (2010) suggested"
R13-1013,H05-1044,0,0.075232,"Missing"
R13-1013,C00-1044,0,0.128309,"and Zaenen (2004) first postulated the existence of contextual valence shifters, which are contextual phenomena altering the prior polarity of a term. Afterwards, some of these phenomena (such as negative or conditional syntactic structures) were dealt with on a case by case basis (Das and Chen, 2001; Na et al., 2004; Popescu and Etzioni, 2005; Pang et al., 2002; Wilson et al., 2005; Wilson et al., 2006; Councill et al., 2010). Studies addressing the phenomenon as a whole flourished later. They aimed at best modelling the expression of opinions (Polanyi and Zaenen, 2004; Taboada et al., 2011; Hatzivassiloglou and Wiebe, 2000; Morsy and Rafea, 2012; Musat and Trausan-Matu, 2010), before embedding those in a classification system. The main purposes of these studies are to determine a list of contextual valence shifters that impact the polarity of a term as well as to define the nature of this impact. However, these lists are often manually built from linguistic intuitions and not learned from language data. Works relying on a corpus of texts to develop resources that best reflect the actual role played by the linguistic context for opinion mining are few. Li et al. (2010) suggested a technique to automatically sele"
R13-1013,C10-1072,0,0.137378,"2004; Taboada et al., 2011; Hatzivassiloglou and Wiebe, 2000; Morsy and Rafea, 2012; Musat and Trausan-Matu, 2010), before embedding those in a classification system. The main purposes of these studies are to determine a list of contextual valence shifters that impact the polarity of a term as well as to define the nature of this impact. However, these lists are often manually built from linguistic intuitions and not learned from language data. Works relying on a corpus of texts to develop resources that best reflect the actual role played by the linguistic context for opinion mining are few. Li et al. (2010) suggested a technique to automatically select polarityshifting features in order to improve a sentiment classification system based on a machine-learning approach. All these studies agree that contextual valence shifters can have diverse impacts on polarized words. They classify them according to the nature of this impact (Polanyi and Zaenen, 2004; Quirk et al., 1985; Kennedy and Inkpen, 2006): inversers invert the polarity of a polarized item, intensifiers intensify it and attenuators diminish it. This study, based on a French corpus, focuses on the issue of contextual valence shifters and p"
R13-1013,H05-2017,0,\N,Missing
R13-1013,W02-1011,0,\N,Missing
W11-0813,J90-1003,0,0.329687,"ly our transducers to the tagged text, we use Unitex (Paumier, 2003). The output of the process is a file containing only the recognized sequences. 3. As we work in the field of indexation, we limit our extraction to nominal terms. 84 2.2 Statistical validation Association measures are conventionally used to automatically determine whether an extracted phrase is an MWE or not. They are mathematical functions that aim to capture the degree of cohesion or association between the constituents. The most frequently used measures are the log-likelihood ratio (Dunning, 1993), the mutual information (Church and Hanks, 1990) or the φ2 (Church and Gale, 1991), although up to 82 measures have been considered by Pecina and Schlesinger (2006). In this paper, we did not aim to compare AMs, but simply to select some effective ones in order to evaluate the relevance of a reference for MWE extraction. However, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer MWEs are quite frequent in any corpus 4 , and they require the definition of a threshold above which an extracted phrase is considered as an MWE. The first aspect is very limiting when de"
W11-0813,J93-1003,0,0.375256,"the TreeTagger (Schmid, 1994). 2. To apply our transducers to the tagged text, we use Unitex (Paumier, 2003). The output of the process is a file containing only the recognized sequences. 3. As we work in the field of indexation, we limit our extraction to nominal terms. 84 2.2 Statistical validation Association measures are conventionally used to automatically determine whether an extracted phrase is an MWE or not. They are mathematical functions that aim to capture the degree of cohesion or association between the constituents. The most frequently used measures are the log-likelihood ratio (Dunning, 1993), the mutual information (Church and Hanks, 1990) or the φ2 (Church and Gale, 1991), although up to 82 measures have been considered by Pecina and Schlesinger (2006). In this paper, we did not aim to compare AMs, but simply to select some effective ones in order to evaluate the relevance of a reference for MWE extraction. However, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer MWEs are quite frequent in any corpus 4 , and they require the definition of a threshold above which an extracted phrase is considered as"
W11-0813,P01-1025,0,0.184328,"1 dispersion points that define n − 1 ""pseudo-bigrams"", they compute the arithmetic mean of the probabilities of the various combinations rather than attempting to pick up the right point. This technique enables the 4. In our test corpus (see Section 4), 2044 MWEs out of 3714 are longer than the bigrams. authors to generalize various conventional measures beyond the bigram level. Among these, we selected the fair log-likelihood ratio as the second AM for our experiments (see Equation 1), given that the classic log-likelihood ratio has been found to be one of the best measures (Dunning, 1993; Evert and Krenn, 2001). LogLik f (w1 · · · wn ) = 2 ∗ log L(p f 1, k f 1, n f 1) + log L(p f 2, k f 2, n f 2) − log L(p f , k f 1, n f 1) − log L(p f , k f 2, n f 2) (1) where k f 1 = f (w1 · · · wn ) n f 1 = Avy k f 2 = Avx − k f 1 nf2 = N −nf1 pf = 1 i=n ∑ f (wi · · · wn) n − 1 i=2 k f 1+k f 2 N pf1 = kf1 nf1 pf2 = kf2 nf2 and N is the number of n-grams in the corpus. Silva and Lopes (1999) also suggested an AM of their own : the Symmetrical Conditional Probability, which corresponds to P(w1 |w2 )P(w2 |w1 ) for a bigram. They defined the fair dispersion point normalization to extend it to larger n-grams, as shown"
W11-0813,pearce-2002-comparative,0,0.0665268,"Missing"
W11-0813,P06-2084,0,0.0161253,"aining only the recognized sequences. 3. As we work in the field of indexation, we limit our extraction to nominal terms. 84 2.2 Statistical validation Association measures are conventionally used to automatically determine whether an extracted phrase is an MWE or not. They are mathematical functions that aim to capture the degree of cohesion or association between the constituents. The most frequently used measures are the log-likelihood ratio (Dunning, 1993), the mutual information (Church and Hanks, 1990) or the φ2 (Church and Gale, 1991), although up to 82 measures have been considered by Pecina and Schlesinger (2006). In this paper, we did not aim to compare AMs, but simply to select some effective ones in order to evaluate the relevance of a reference for MWE extraction. However, association measures present two main shortcomings that were troublesome for us : they are designed for bigrams, although longer MWEs are quite frequent in any corpus 4 , and they require the definition of a threshold above which an extracted phrase is considered as an MWE. The first aspect is very limiting when dealing with real data where longer units are common. The second may be dealt with some experimental process to obtain"
W11-0813,W09-2907,0,0.0257663,"tion Multiword Expressions (MWEs) are commonly defined as “recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages” (Smadja, 1993, 143). Their importance in the field of natural language processing (NLP) is undeniable. Although composed of several words, these sequences are nonetheless considered as simple units with regard to part-of-speech at the lexical as well as syntactic levels. Their identification is therefore essential to the efficiency of applications such as parsing (Nivre and Nilsson, 2004), machine translation (Ren et al., 2009), information extraction, or information retrieval (Vechtomova, 2005). In these systems, the principle of syntactic or semantic/informational unit is particularly important. To address these two limitations, this article describes how an n-gram frequency database can be used to compute association measures (AMs) efficiently, even for small texts. The specificity of this new technique is that AMs are computed on-the-fly, freeing it from the coverage limitation that afflicts more simple techniques based on a dictionary. We start off focussing on our extraction method, and more particularly on th"
W11-0813,C08-3010,0,0.0188599,"ferred from the frequency of the structures of order N, provided the storage and questioning system is efficient enough for real-time applications. The need for efficiency also applies to queries related to the ME measure or the LocalMax algorithm that partly involve the use of wildcards. This type of search tool can be efficiently implemented with a PATRICIA tree (Morrison, 1968). This data structure enables the compression of ngrams that share a common prefix and of the nodes that have only one child. The latter compression is even more effective as most of the n-grams have a unique suffix (Sekine, 2008). Beyond the compression that this structure allows, it also guarantees a very fast access to data insofar as a query is a simple tree traversal that can be done in constant time. In order to further optimize the final data structure, we store the vocabulary in a table and associate an integer as a unique identifier for every word. In this way, we avoid the word repetition (whose size in memory far exceeds that of an integer) in the tree. Moreover, this technique also enables to speed up the query mechanism, since the keys are smaller. We derived two different implementations of this structure"
W11-0813,J93-1007,0,0.945377,"measures based on the frequency of lexical structures. They, therefore, require a critical amount of data and cannot function properly from a simple phrase or even from a short text. 2. Since the syntactic and lexical variability of MWEs may be high, lexical resources learned from a corpus cannot take it into account. The coverage of these resources is indeed too limited when applied to a new text. 1 Introduction Multiword Expressions (MWEs) are commonly defined as “recurrent combinations of words that co-occur more often than expected by chance and that correspond to arbitrary word usages” (Smadja, 1993, 143). Their importance in the field of natural language processing (NLP) is undeniable. Although composed of several words, these sequences are nonetheless considered as simple units with regard to part-of-speech at the lexical as well as syntactic levels. Their identification is therefore essential to the efficiency of applications such as parsing (Nivre and Nilsson, 2004), machine translation (Ren et al., 2009), information extraction, or information retrieval (Vechtomova, 2005). In these systems, the principle of syntactic or semantic/informational unit is particularly important. To addre"
W12-2207,W10-1001,0,0.233849,"on and Callan (2005) found that the classic type-token ratio or number of words not in the 3000words Dale list appeared to perform better than their language model on a corpus from readers, but were poorer predictors on web-extracted texts. In languages other than English, Franc¸ois (2011b) surveyed a wide range of features for French and reports that the feature that uses a limited vocabulary list (just like in some classic formulas) has a stronger correlation with reading difficulty that a unigram model and the best performing syntactic feature was the average number of words per sentences. Aluisio et al. (2010), also, found that the best correlate with difficulty was the average number of words per sentence. All in all, while there is sufficient evidence that the AI paradigm outperforms the classis formulas, classic features have often been shown to make the single strongest predictors. An alternative explanation could be that, by comparison to the simpler statistical analyses that determined the coefficients of the classic formulas, machine learning algorithms, such as support machine vector (SVM) or logistic regression are more sophisticated and better able to learn the regularities in training da"
W12-2207,C10-2032,0,0.534618,"cently, however, the development of efficient natural language processing (NLP) systems and the success of machine learning methods led to 49 NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 49–57, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics a resurgence of interest in readability as it became clear that these developments could impact the design and performance of readability measures. Several studies (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Schwarm and Ostendorf, 2005; Feng et al., 2010) have used NLP-enabled feature extraction and state-of-the-art machine learning algorithms and have reported significant gains in performance, suggesting that the AI approach might be superior to previous attempts. Going beyond reports of performance which are often hard to compare due to a lack of a common gold standard, we are interested in investigating AI approaches more closely with the aim of understanding the reasons behind the reported superiority over classic formulas. AI readability systems use NLP for richer feature extraction and a machine learning algorithm. Given that the classic"
W12-2207,W08-0909,0,0.771136,"s with features used in classic formulas, different machine learning algorithms and the interactions of features with algorithms. There results are reported in Sections 4, 5, and 6, respectively. We conclude in Section 7 with a summary of the main findings and future work. 2 Previous findings Several readability studies in the past decade have reported a performance gain when using NLPenabled features, language models, and machine learning algorithms to evaluate the reading difficulty of a variety of texts (Si and Callan, 2001; Collins50 Thompson and Callan, 2005; Schwarm and Ostendorf, 2005; Heilman et al., 2008; Feng et al., 2010). A first explanation for this superiority would be related to the new predictors used in recent models. Classic formulas relied mostly on surface lexical and syntactic variables such as the average number of words per sentence, the average number of letters per word, the proportion of given POS tags in the text or the proportion of out-of-simple-vocabulary words. In the AI paradigm, several new features have been added, including language models, parse tree-based predictors, probability of discourse relations, estimates of text coherence, etc. It is reasonable to assume th"
W12-2207,C10-1062,0,0.116354,"anation could be that, by comparison to the simpler statistical analyses that determined the coefficients of the classic formulas, machine learning algorithms, such as support machine vector (SVM) or logistic regression are more sophisticated and better able to learn the regularities in training data, thus building more accurate models. Work in this direction has been of smaller scale but already reporting inconsistent results. Heilman et al. (2008) considered the performance of linear regression, ordinal and multinomial logistic regression, and found the latter to be more efficient. However, Kate et al. (2010) obtained contradictory findings, showing that regression-based algorithms perform better, especially when regression trees are used for bagging. For French, Franc¸ois (2011b) found that SVMs were more efficient than linear regression, ordinal and multinomial logistic regression, boosting, and bagging. Finally, it is quite possible that there are interactions between types of features and types of statistical algorithms and these interactions are primarily responsible for the better performance. In what follows, we present the results of three studies (experiments 2-4), comparing the contribut"
W12-2207,2010.jeptalnrecital-recital.2,0,0.0405402,"Missing"
W12-2207,E09-2013,1,0.909793,"Missing"
W12-2207,D08-1020,0,0.0290042,"veral new features have been added, including language models, parse tree-based predictors, probability of discourse relations, estimates of text coherence, etc. It is reasonable to assume that these new features capture a wider range of readability factors thus bringing into the models more and, possibly, better information. However, the evidence from comparative studies is not consistent on this question. In several cases, AI models include features central to classic formulas which, when isolated, appear to be the stronger predictors in the models. An exception to this trend is the work of Pitler and Nenkova (2008) who reported non-significant correlation for the mean number of words per sentence (r = 0.1637, p = 0.3874) and the mean number of characters per word (r = −0.0859, p = 0.6519). In their study, though, they used text quality rather than text difficulty as the dependent variable. The data consisted solely of text from the Wall Street Journal which is “intended for an educated adult audience” text labelled for degrees of reading fluency. Feng et al. (2010) compared a set of similar variables and observed that language models performed better than classic formula features but classic formula fea"
W12-2207,P05-1065,0,0.386089,"declined in the 90s. More recently, however, the development of efficient natural language processing (NLP) systems and the success of machine learning methods led to 49 NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 49–57, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics a resurgence of interest in readability as it became clear that these developments could impact the design and performance of readability measures. Several studies (Si and Callan, 2001; Collins-Thompson and Callan, 2005; Schwarm and Ostendorf, 2005; Feng et al., 2010) have used NLP-enabled feature extraction and state-of-the-art machine learning algorithms and have reported significant gains in performance, suggesting that the AI approach might be superior to previous attempts. Going beyond reports of performance which are often hard to compare due to a lack of a common gold standard, we are interested in investigating AI approaches more closely with the aim of understanding the reasons behind the reported superiority over classic formulas. AI readability systems use NLP for richer feature extraction and a machine learning algorithm. Gi"
W14-1206,W03-1602,0,0.0795097,"ilar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronominal anaphora. Third,"
W14-1206,N09-2045,0,0.0696264,"escribe a general typology of simplification derived from our corpora (Section 2.2). Then, we present the system based on the syntactic part of the typology, which operates in two steps: overgeneration of all possible simplified sentences (Section 2.3.1) and selection of the best subset of candidates using readability criteria (Section 2.3.2) and ILP. Finally, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikiped"
W14-1206,levy-andrew-2006-tregex,0,0.0172946,"ies global changes to the text. For instance, when a simple past is replaced by a present form, we must also adapt the verbs in the surrounding context in accordance with tense agreement. This requires to consider the whole text, or at least the paragraph that contains the modified verbal form, and be able to automatically model tense agreement. Otherwise, we may alter the coherence of the text and decrease its readability. This leaves us with 19 simplification rules.6 To apply them, the candidate structures for simplification first need to be detected using regular expressions, via Tregex 7 (Levy and Andrew, 2006) that allows the retrieval of elements and relationships in a parse tree. In a second step, syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon. The operations to perform depend on the type of rules: 1. For the deletion cases, simply deleting all the elements involved is sufficient (via the delete operation in Tsurgeon). The elements affected by the deletion rules are adverbial clauses, clauses between brackets, some of the subordinate clauses, clauses between commas or introduced by words such as “comme” (as), “v"
W14-1206,E06-1021,0,0.0743117,"terms of language and content. It is available at the address http://fr.vikidia.org 48 WikiExtractor 3 was then applied to the articles to discard the wiki syntax and only keep the raw texts. This corpus comprises 13,638 texts (7,460 from Vikidia and only 6,178 from Wikipedia, since some Vikidia articles had no counterpart in Wikipedia). These articles were subsequently processed to identify parallel sentences (Wikipedia sentence with a simplified equivalent in Vikidia). The alignment has been made partly manually and partly automatically with the monolingual alignment algorithm described in Nelken and Shieber (2006), which relies on a cosine similarity between sentence vectors weighted with the tf-idf. This program outputs alignments between sentences, along with a confidence score. Among these files, twenty articles or excerpts from Wikipedia were selected along with their equivalent in Vikidia. This amounts to 72 sentences for the former and 80 sentences for the latter. The second corpus is composed of 16 narrative texts, and more specifically tales, by Perrault, Maupassant, and Daudet. We used tales since their simplified version was closer to the original than those of longer novels, which made the s"
W14-1206,seretan-2012-acquisition,0,0.0654496,"ia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. This is why, so far, there was no attempt to adapt this machine learning methodology to French. The only previous work on French, to our knowledge, is that of Seretan (2012), which analysed a corpus of newspapers to semi-automatically detect complex structures that has to be simplified. However, her system of rules has not been implemented and evaluated. 2 Methodology 2.1 Corpus Description We based our typology of simplification rules on the analysis of two corpora. More specifically, since our aim is to identify and classify the various strategies used to transform a complex sentence into a more simple one, the corpora had to include parallel sentences. The reason why we analysed two corpora is to determine whether different genres of texts lead to different si"
W14-1206,W13-1502,0,0.169277,"Missing"
W14-1206,C12-1023,0,0.108235,"Missing"
W14-1206,candito-etal-2010-statistical,0,0.0266653,"Missing"
W14-1206,E99-1042,0,0.167523,"us a size roughly similar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronomina"
W14-1206,C96-2183,0,0.874017,"Missing"
W14-1206,Y09-1013,0,0.0159614,"f syntactic simplification for French sentences. The simplification is performed as a two-step process. First, for each sentence of the text, we generate the set of all possible simplifications (overgeneration step), and then, we select the best subset of simplified sentences using several criteria. 2.3.1 Generation of the Simplified Sentences The sentence overgeneration module is based on a set of rules (19 rules), which rely both on morphosyntactic features of words and on syntactic relationships within sentences. To obtain this information, the texts from our corpus are analyzed by MELT 4 (Denis and Sagot, 2009) and Bonsai 5 (Candito et al., 2010) during a preprocessing phase. As a result, texts are represented as syntax trees that include the information necessary to apply our simplification rules. After preprocessing, the set of simplification rules is applied recursively, one sentence at a time, until there is no further structure to simplify. All simplified sentences produced by a given rule are saved and gathered in a set of variants. The rules for syntactic simplification included in our program are of three kinds: deletion rules (12 rules), modification rules (3 rules) and splitting rules (4 r"
W14-1206,D12-1043,1,0.897268,"Missing"
W14-1206,W09-1802,0,0.0352869,"e more than one simplified variant for a given sentence. In this case, the next step consists in selecting the most suitable variant to substitute the original one. The selection process is described in the next section. 2.3.2 Selection of the Best Simplifications Given a set of candidate simplified sentences for a text, our goal is to select the best subset of simplified sentences, that is to say the subset that maximizes some measure of readability. More precisely, text readability is measured through different criteria, which are optimized with an Integer Linear Programming (ILP) approach (Gillick and Favre, 2009). These criteria are rather simple in 6 These 19 rules are available at http://cental.fltr.ucl.ac.be/team/ lbrouwers/rules.pdf 7 http://nlp.stanford.edu/software/tregex.shtml 8 This software is available at the address http://sarrazip.com/dev/ verbiste.html under GNU general public license and was developed by Pierre Sarrazin. 51 this approach. They are used to ensure that not only the syntactic difficulty, but also the lexical complexity decrease, since syntactic transformations may cause lexical or discursive alterations in the text. We considered four criteria to select the most suitable se"
W14-1206,D11-1038,0,0.639109,"the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. Thi"
W14-1206,C10-1152,1,0.964006,"lly, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which"
W14-3502,H05-1103,0,0.0362966,"ibility as regards the input and the feedback offered to the user (Klenner and Visser, 2003). For instance, some adaptive programs are able to select, in an exercise database, an item tailored to the learner’s level (Desmet, 2006). However, it requires the pre-annotation of all the items in terms of difficulty, which restricts the versatility of the user module. Being able to generate suitable exercises on the fly from a corpus appears as a better way to adapt to specific learner difficulties. The automatic generation of exercises has already been researched, mostly for English (Coniam, 1997; Brown et al., 2005; Smith et al., 2009; Chen et al., 2006; Heilman, 2011; Meurers et al., 2010), but also for French (Antoniadis et al., 2005; Selva, 2002). However, the majority of these systems either use excerpts whose difficulty has been manually annotated or excerpts extracted from a large corpus and thus lacking any difficulty annotations. In the first case, the system is able to adapt to the user’s needs only within the limits of the available materials. In the second case, any type of exercise can be generated on the fly, but because there is no control of the difficulty of excerpts, the contextual comp"
W14-3502,P06-4001,0,0.0168819,"dback offered to the user (Klenner and Visser, 2003). For instance, some adaptive programs are able to select, in an exercise database, an item tailored to the learner’s level (Desmet, 2006). However, it requires the pre-annotation of all the items in terms of difficulty, which restricts the versatility of the user module. Being able to generate suitable exercises on the fly from a corpus appears as a better way to adapt to specific learner difficulties. The automatic generation of exercises has already been researched, mostly for English (Coniam, 1997; Brown et al., 2005; Smith et al., 2009; Chen et al., 2006; Heilman, 2011; Meurers et al., 2010), but also for French (Antoniadis et al., 2005; Selva, 2002). However, the majority of these systems either use excerpts whose difficulty has been manually annotated or excerpts extracted from a large corpus and thus lacking any difficulty annotations. In the first case, the system is able to adapt to the user’s needs only within the limits of the available materials. In the second case, any type of exercise can be generated on the fly, but because there is no control of the difficulty of excerpts, the contextual complexity is likely to hinder the user’s c"
W14-3502,E09-1027,0,0.0197567,"olchildren that has been acknowledged as one of the most reliable for this specific population. However, it is with the advent of what François and Fairon (2012) call the “IA readability” that this criterion has somehow becomed the standard approach. This is also due to the fact that Si and Callan (2001) suggested to view text readability assessment as a classification task. It implies to assign training texts to a few number of classes, which may quite logically corresponds to educational levels. Most of the recent readability formulas have adopted this approach (Schwarm and Ostendorf, 2005; Feng et al., 2009; François, 2009; Tanaka-Ishii et al., 2010; Vajjala and Meurers, 2012; Pilán et al., 2014), but, to our knowledge, none of them have systematically addressed the issue of their corpus homogeneity. Although textbooks are indeed written by experts and may even benefit from updates based on teachers’ feedback, the criteria used to select texts are likely to differ from one author to another as well as from one textbook series to another. This is why we decided to investigate this problematic using a corpus of FFL textbooks, which is described in the next section. 17 3 A textbook corpus for Frenc"
W14-3502,C10-2032,0,0.143878,"field date back to the 1920’s (Lively and Pressey, 1923) and have traditionally been carried out by psychologists. However, readability has undergone recent developments. They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scar"
W14-3502,E09-3003,1,0.842804,"been acknowledged as one of the most reliable for this specific population. However, it is with the advent of what François and Fairon (2012) call the “IA readability” that this criterion has somehow becomed the standard approach. This is also due to the fact that Si and Callan (2001) suggested to view text readability assessment as a classification task. It implies to assign training texts to a few number of classes, which may quite logically corresponds to educational levels. Most of the recent readability formulas have adopted this approach (Schwarm and Ostendorf, 2005; Feng et al., 2009; François, 2009; Tanaka-Ishii et al., 2010; Vajjala and Meurers, 2012; Pilán et al., 2014), but, to our knowledge, none of them have systematically addressed the issue of their corpus homogeneity. Although textbooks are indeed written by experts and may even benefit from updates based on teachers’ feedback, the criteria used to select texts are likely to differ from one author to another as well as from one textbook series to another. This is why we decided to investigate this problematic using a corpus of FFL textbooks, which is described in the next section. 17 3 A textbook corpus for French as a foreign l"
W14-3502,D12-1043,1,0.929797,": natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be explained by the high cost needed to create a readability model, especially in terms of the corpus collection process. Moreover, a convenient readability model should be able to output predictions tha"
W14-3502,N07-1058,0,0.0757919,"t by psychologists. However, readability has undergone recent developments. They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be explained by the high cost needed to create a readability model, espe"
W14-3502,W10-1002,0,0.0425427,"Missing"
W14-3502,W14-1821,0,0.189177,"contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be explained by the high cost needed to create a readability model, especially in terms of the corpus collection process. Moreover, a convenient readability model shoul"
W14-3502,P05-1065,0,0.447565,"wever, readability has undergone recent developments. They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be explained by the high cost needed to create a readability model, especially in terms of the corpus"
W14-3502,2002.jeptalnrecital-long.16,0,0.0575678,"select, in an exercise database, an item tailored to the learner’s level (Desmet, 2006). However, it requires the pre-annotation of all the items in terms of difficulty, which restricts the versatility of the user module. Being able to generate suitable exercises on the fly from a corpus appears as a better way to adapt to specific learner difficulties. The automatic generation of exercises has already been researched, mostly for English (Coniam, 1997; Brown et al., 2005; Smith et al., 2009; Chen et al., 2006; Heilman, 2011; Meurers et al., 2010), but also for French (Antoniadis et al., 2005; Selva, 2002). However, the majority of these systems either use excerpts whose difficulty has been manually annotated or excerpts extracted from a large corpus and thus lacking any difficulty annotations. In the first case, the system is able to adapt to the user’s needs only within the limits of the available materials. In the second case, any type of exercise can be generated on the fly, but because there is no control of the difficulty of excerpts, the contextual complexity is likely to hinder the user’s comprehension and his/her ability to perform the exercise. Faced with this challenge, one solution"
W14-3502,J10-2002,0,0.0206191,"ed as one of the most reliable for this specific population. However, it is with the advent of what François and Fairon (2012) call the “IA readability” that this criterion has somehow becomed the standard approach. This is also due to the fact that Si and Callan (2001) suggested to view text readability assessment as a classification task. It implies to assign training texts to a few number of classes, which may quite logically corresponds to educational levels. Most of the recent readability formulas have adopted this approach (Schwarm and Ostendorf, 2005; Feng et al., 2009; François, 2009; Tanaka-Ishii et al., 2010; Vajjala and Meurers, 2012; Pilán et al., 2014), but, to our knowledge, none of them have systematically addressed the issue of their corpus homogeneity. Although textbooks are indeed written by experts and may even benefit from updates based on teachers’ feedback, the criteria used to select texts are likely to differ from one author to another as well as from one textbook series to another. This is why we decided to investigate this problematic using a corpus of FFL textbooks, which is described in the next section. 17 3 A textbook corpus for French as a foreign language 3.1 The collect Wit"
W14-3502,W12-2019,0,0.154683,"the 1920’s (Lively and Pressey, 1923) and have traditionally been carried out by psychologists. However, readability has undergone recent developments. They result from the contact with two other fields: natural language processing (NLP) is used to extract more complex linguistic predictors, whereas artificial intelligence (AI) provides complex statistical algorithms to better exploit the regularities existing between text difficulty and the linguistic predictors. Recent work has been carried out mostly on English as a first language (L1) (Collins-Thompson and Callan, 2005; Feng et al., 2010; Vajjala and Meurers, 2012) or English as a second or foreign language (L2) (Heilman et al., 2007; Schwarm and Ostendorf, 2005), but also on other languages such as Swedish (Pilán et al., 2014), French (François and Fairon, 2012; Todirascu et al., 2013; Dascalu, 2014; François et al., 2014), or Arabic (Al-Khalifa and Al-Ajlan, 2010), among others. Although the field is quite lively, there is only limited work specifically dedicated to the readability of L2 languages. Furthermore, attempts to integrate such L2 readability models within an automatic exercise generation system are even more scarce. In our view, this can be"
W14-3502,W11-1415,0,0.0350182,"Missing"
W16-6510,P06-4018,0,0.0792149,"ed items, especially in cases of misspellings and hyphenation, we experimented with two normalization approaches at the word level: pure Levenshtein distance, and LanguageTool’s output combined with candidate ranking strategies. Our hypothesis has been that normalization should take care of the word-level anomalies of learner language replacing them with a standard variant, so that the automatic annotation in the next step would be more accurate. Approach 1: Levenshtein distance As the first strategy for normalization we experimented with pure Levenshtein distance (LD) as implemented in NLTK (Bird, 2006)1 . LD is a measure for the distance between two strings. In our case, this was the difference between the (possibly) misspelled word and the (probable) target word. Output suggestions were based on SALDO-morphology lexicon (Borin et al., 2013), a full-form lexicon where all inflected forms are listed alongside their base forms and parts of speech. As such, in the cases where the word form was not present in SALDO, we chose the word form in SALDO morphology to which the original word form in our source had the shortest LD, selecting the first suggestion with the shortest edit distance. Suggest"
W16-6510,borin-etal-2012-korp,0,0.134091,"Missing"
W16-6510,L16-1032,1,0.565445,"Missing"
W16-6510,L16-1509,0,0.0430379,"Missing"
W16-6510,L16-1031,1,0.583186,"Missing"
W17-5018,W13-1704,0,0.0227214,"(ICLE) (Granger et al., 2009) and the Cambridge Learner Corpus (CLC) have been the go-to standard. Moreover, the recent years have also seen an increasing availability of learner corpora aligned with the CEFR (Boyd et al., 2014; Vajjala and 2.2 Automated Learner Writing Assessment The advances made towards developing errorannotated and human-graded learner corpora (such as the CLC), as well as understanding the features underlying L2 proficiency, have subsequently furthered the development of systems for automated learner writing assessment, which include intelligent writing assistants (e.g. Andersen et al., 2013) and automated scoring systems (e.g. Yannakoudakis et al., 2011). In the case of automated scoring, two kinds of systems are generally distinguished, viz. automated essay grading (AEG) and automated short answer grading (ASAG)1 , depending on the length and type of texts as well as the kind of scoring method used. However, Burrows et al. (2015, p. 66) observe that ‘[t]he difference between these types can be fuzzy’. Essay grading, on the one hand, is concerned with the evaluation of the quality or proficiency – often by means of a standard scale – of writings spanning several paragraphs or pag"
W17-5018,J08-4004,0,0.138397,"level, in particular for those texts that displayed “no errors” and were written in “mainly accurate English”. This is illustrated in the few texts where the initial A2 level seemed to have been underestimated in favour of a B2 or C1 level. We were therefore interested in examining what characteristics define the texts that were difficult to grade. We measured the difficulty of grading a text on the basis of the per-item observed disagreement Doαi on the label x given by coder c on item i (5.1.1). We derived this measure by decomposing Krippendorff’s formula for the observed disagreement Doα (Artstein and Poesio, 2008, pp. 564-7) , which amounts to two times the per-item empirical variance s2i . 5.2 Automated Scoring Performance The voting classifier described in Section 4 achieves a good human-system agreement5 (HSA) (α = .76, .67 &lt; α &lt; .80) with respect to the answers’ assessed CEFR level obtained by majority voting (Table 5). Although our system did not surpass the strong HHA ceiling we observed earlier (which amounts to α = .82 when using a 5-point scale), the HSA of our ensemble method still outperformed the HSA of its individual classifiers. What is more, in cases where there is a human-system disagr"
W17-5018,boyd-etal-2014-merlin,0,0.0626636,"Missing"
W17-5018,C90-2036,0,0.147376,"Missing"
W17-5018,P14-5010,0,0.00342602,"participants were French-speaking learners of English studying at the bachelor’s and master’s level (all disciplines included). 3 In cases without agreement, the assessed level was derived by taking the nearest integer of the mean of the votes. These cases were then manually verified taking the hesitations observed in the examiners’ comments into account. 172 C2 0 0 0 1 4 C1 0 1 4 2 15 8 B2 0 1 7 28 27 11 B1 3 34 39 26 10 1 A2 19 34 6 0 0 0 A1 16 2 0 0 0 0 A1 A2 B1 B2 C1 C2 assessed level 0 initial level Features As preprocessing step to feature extraction, we used the Stanford CoreNLP suite (Manning et al., 2014) for performing tokenisation, lemmatisation, part-of-speech tagging, constituency and dependency parsing as well as coreferential resolution. We defined a feature set of 18 different families, counting 695 individual feature configurations. We included a number of traditional readability features (Franc¸ois and Fairon, 2012; Vajjala and L˜oo, 2014), including lexical features (word length, number of syllables, lexical frequency from SUBTLEX (Brysbaert and New, 2009), lexical likelihood based on Simple-Good Turing Smoothing (Gale and Sampson, 1995), lexical variation, lexical sophistication and"
W17-5018,W11-2401,0,0.0252175,"al., 2015, p. 61). Its goal is to evaluate the learner responses as regards their correctness with respect to the initial question. The adequacy of the answer is thus compared to a model answer and graded either on a pass/fail basis or along a scale of correctness, using a range of concept and pattern matching techniques, alignment-based evaluation metrics (e.g. BLEU) or machine learning algorithms. In the context of L2 short answer grading, we mainly find systems developed for evaluating responses to reading comprehension questions, such as the CoMiC systems developed for English and German (Meurers et al., 2011). The writing task underlying the current study can be situated between the extreme ends of essay and short answer grading presented above, aiming at assessing the CEFR level associated with short texts. On the one hand, the task is based on a series of questions (e.g.“What is the best book you ever read?”) which are more open-ended than the objective questions generally used in ASAG. On the other hand, contrary to essay writing, the task aims to assess writing proficiency based on a shorter display of writing, by adding more restrictions on the length of the answers (approximately one paragra"
W17-5018,D12-1043,1,0.918346,"Missing"
W17-5018,C16-1198,0,0.585989,"Missing"
W17-5018,W14-3509,0,0.217374,"Missing"
W17-5018,L16-1031,0,0.146157,"Missing"
W17-5018,P11-1019,0,0.0376376,"rpus (CLC) have been the go-to standard. Moreover, the recent years have also seen an increasing availability of learner corpora aligned with the CEFR (Boyd et al., 2014; Vajjala and 2.2 Automated Learner Writing Assessment The advances made towards developing errorannotated and human-graded learner corpora (such as the CLC), as well as understanding the features underlying L2 proficiency, have subsequently furthered the development of systems for automated learner writing assessment, which include intelligent writing assistants (e.g. Andersen et al., 2013) and automated scoring systems (e.g. Yannakoudakis et al., 2011). In the case of automated scoring, two kinds of systems are generally distinguished, viz. automated essay grading (AEG) and automated short answer grading (ASAG)1 , depending on the length and type of texts as well as the kind of scoring method used. However, Burrows et al. (2015, p. 66) observe that ‘[t]he difference between these types can be fuzzy’. Essay grading, on the one hand, is concerned with the evaluation of the quality or proficiency – often by means of a standard scale – of writings spanning several paragraphs or pages. In the context of L2 essay grading, a number of recently dev"
W18-0514,L18-1140,1,0.80822,"Missing"
W18-0514,francois-etal-2014-flelex,1,0.630201,"Missing"
W18-0514,L16-1032,1,0.790104,"Missing"
W18-0514,S12-1066,0,0.0582847,"Missing"
W18-0514,C16-1198,0,0.0367797,"Missing"
W18-0514,2016.gwc-1.43,0,0.371711,"gn language (NT2) which includes frequency distributions of 17,743 words and expressions attested in expert-written textbook texts and readers graded along the scale of the Common European Framework of Reference (CEFR). In essence, the lexicon informs us about what kind of vocabulary should be understood when reading Dutch as a non-native reader at a particular proficiency level. The main novelty of the resource with respect to the previously developed CEFR-graded lexicons concerns the introduction of corpusbased evidence for L2 word sense complexity through the linkage to Open Dutch WordNet (Postma et al., 2016). The resource thus contains, on top of the lemmatised and part-ofspeech tagged lexical entries, a total of 11,999 unique word senses and 8,934 distinct synsets. 1 Introduction In the recent years, a number of graded lexical resources have been developed to further research on first (L1) or second (L2) language complexity. Such a graded lexicon can be defined as a lexical database describing the graded frequency distributions of lexemes as they are attested in authentic pedagogical material along the successive grade levels of a particular language curriculum. The graded lexicons that have bee"
W18-0514,L16-1035,1,0.918501,"Missing"
W18-0514,vossen-etal-2012-dutchsemcor,0,0.0767368,"Missing"
W18-7004,W10-1607,0,0.0834528,"Missing"
W18-7004,W10-1001,0,0.0909843,"Missing"
W18-7004,D12-1043,1,0.813427,"k. 2 et al., 2010) to name a few. Text simplification systems exist for various languages, for example: English (Carroll et al., 1998; Horn et al., 2014; Glavaš and Štajner, 2015; Paetzold and Specia, 2017), Spanish (Bott et al., 2012; Rello et al., 2013a), Swedish (Keskisärkkä, 2012), and Portuguese (Aluísio and Gasperin, 2010). However, to our knowledge, there is no full-fledged ATS system for French available, although some authors have investigated related aspects (i.e. simplified writing for language-impaired readers (Max, 2006), French readability for French as a Foreign Language (FFL) (François and Fairon, 2012), syntactic simplification (Seretan, 2012; Brouwers et al., 2014), and lexical simplification for improving the understanding of medical terms (Grabar et al., 2018)). While first LS systems (Carroll et al., 1998; Devlin, 1999) used to combine WordNet (Miller, 1998) and frequency information from words, more recent ones are more sophisticated and rely on supervised machine learning methods. Their architecture can be represented with four steps as follows (Shardlow, 2014): 1. Complex Word Identification (CWI): aims to identify target words that need simplification. In CWI, the methods based on l"
W18-7004,C18-1218,1,0.929516,"n (ATA), pages 21–28, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics 2005). Gala and Ziegler (2016) also identified that, for French children with dyslexia, inconsistent words as far as the grapheme-phoneme relation is concerned (different length of the number of letters and phonemes in a word) contribute to the difficulty in reading. In this paper, we address the challenging task of LS, which has not yet been systematically investigated for French. We compare two approaches: the first one is based on the exploitation of a lexical resource, ReSyf3 (Billami et al., 2018), which contains disambiguated ranked synonyms in French; the second one is based on word embedding and draws from Glavaš and Štajner (2015). Although previous studies have prioritised statistical methods over the use of resources to acquire synonyms, we are not aware of a previous study having compared statistical models with a disambiguated synonym resource. We believe that this property could significantly enhance the selection of relevant candidates for substitution in a given context. Another property of ReSyf is that synonyms have already been ranked by reading difficulty using Billami e"
W18-7004,W16-4107,1,0.776637,"ading to dyslexia, with a large variability). Ziegler et al. (2003) show that the problems of comprehension among children with reading difficulties are mostly due to the difficulties in decoding words in order to recognise them. In other words, these children do not suffer from 1 https://timssandpirls.bc.edu/ pirls2001i/pdf/p1_IR_book.pdf 2 Cycle des Évaluations Disciplinaires Réalisées sur Échantillons (CEDRE). 21 Proceedings of the 1st Workshop on Automatic Text Adaptation (ATA), pages 21–28, c Tilburg, The Netherlands, November 8 2018. 2018 Association for Computational Linguistics 2005). Gala and Ziegler (2016) also identified that, for French children with dyslexia, inconsistent words as far as the grapheme-phoneme relation is concerned (different length of the number of letters and phonemes in a word) contribute to the difficulty in reading. In this paper, we address the challenging task of LS, which has not yet been systematically investigated for French. We compare two approaches: the first one is based on the exploitation of a lexical resource, ReSyf3 (Billami et al., 2018), which contains disambiguated ranked synonyms in French; the second one is based on word embedding and draws from Glavaš a"
W18-7004,P15-2011,0,0.202267,"r (2016) also identified that, for French children with dyslexia, inconsistent words as far as the grapheme-phoneme relation is concerned (different length of the number of letters and phonemes in a word) contribute to the difficulty in reading. In this paper, we address the challenging task of LS, which has not yet been systematically investigated for French. We compare two approaches: the first one is based on the exploitation of a lexical resource, ReSyf3 (Billami et al., 2018), which contains disambiguated ranked synonyms in French; the second one is based on word embedding and draws from Glavaš and Štajner (2015). Although previous studies have prioritised statistical methods over the use of resources to acquire synonyms, we are not aware of a previous study having compared statistical models with a disambiguated synonym resource. We believe that this property could significantly enhance the selection of relevant candidates for substitution in a given context. Another property of ReSyf is that synonyms have already been ranked by reading difficulty using Billami et al. (2018) method. The paper is organised as follows: Section 2 presents existing methods for LS. Section 3 describes our method and Secti"
W18-7004,P11-2087,0,0.110553,"tecture of the system that we detail in the next subsections. In brief, we start from a sentence in which complex words have been identified. We then use ReSyf to get candidates for substitution (section 3.2). If the complex word has several meanings, we use automatic word sense disambiguation to select the best set of candidates (section 3.3). The last step consists in selecting the simplest candidate to be used in the simplified sentence (section 3.4). 3.1 Candidate word senses Word Sense Disambiguation In practice, this process is not literally respected in LS methods. For some approaches (Biran et al., 2011; Bott et al., 2012; Glavaš and Štajner, 2015), all words are potentially complex and need simplification. Each word is replaced only if it has a simpler synonym. Some other methods merge the SS into the SR step. Here, we consider the Glavaš and Štajner (2015) method as a baseline of the state of the art. This method is based on the exploitation of general resources in a general context. The baseline relies on word embeddings to generate substitute candidates. Glavaš and Štajner (2015) only replace a target word if it has a lower frequency than the selected candidate substitution. However, we"
W18-7004,C12-1023,0,0.358,"substitution in a given context. Another property of ReSyf is that synonyms have already been ranked by reading difficulty using Billami et al. (2018) method. The paper is organised as follows: Section 2 presents existing methods for LS. Section 3 describes our method and Section 4 discusses on the results. Some concluding remarks are to be found at the end, along with future work. 2 et al., 2010) to name a few. Text simplification systems exist for various languages, for example: English (Carroll et al., 1998; Horn et al., 2014; Glavaš and Štajner, 2015; Paetzold and Specia, 2017), Spanish (Bott et al., 2012; Rello et al., 2013a), Swedish (Keskisärkkä, 2012), and Portuguese (Aluísio and Gasperin, 2010). However, to our knowledge, there is no full-fledged ATS system for French available, although some authors have investigated related aspects (i.e. simplified writing for language-impaired readers (Max, 2006), French readability for French as a Foreign Language (FFL) (François and Fairon, 2012), syntactic simplification (Seretan, 2012; Brouwers et al., 2014), and lexical simplification for improving the understanding of medical terms (Grabar et al., 2018)). While first LS systems (Carroll et al., 1"
W18-7004,P14-2075,0,0.133807,"this property could significantly enhance the selection of relevant candidates for substitution in a given context. Another property of ReSyf is that synonyms have already been ranked by reading difficulty using Billami et al. (2018) method. The paper is organised as follows: Section 2 presents existing methods for LS. Section 3 describes our method and Section 4 discusses on the results. Some concluding remarks are to be found at the end, along with future work. 2 et al., 2010) to name a few. Text simplification systems exist for various languages, for example: English (Carroll et al., 1998; Horn et al., 2014; Glavaš and Štajner, 2015; Paetzold and Specia, 2017), Spanish (Bott et al., 2012; Rello et al., 2013a), Swedish (Keskisärkkä, 2012), and Portuguese (Aluísio and Gasperin, 2010). However, to our knowledge, there is no full-fledged ATS system for French available, although some authors have investigated related aspects (i.e. simplified writing for language-impaired readers (Max, 2006), French readability for French as a Foreign Language (FFL) (François and Fairon, 2012), syntactic simplification (Seretan, 2012; Brouwers et al., 2014), and lexical simplification for improving the understanding"
W18-7004,W14-1206,1,0.829361,"or various languages, for example: English (Carroll et al., 1998; Horn et al., 2014; Glavaš and Štajner, 2015; Paetzold and Specia, 2017), Spanish (Bott et al., 2012; Rello et al., 2013a), Swedish (Keskisärkkä, 2012), and Portuguese (Aluísio and Gasperin, 2010). However, to our knowledge, there is no full-fledged ATS system for French available, although some authors have investigated related aspects (i.e. simplified writing for language-impaired readers (Max, 2006), French readability for French as a Foreign Language (FFL) (François and Fairon, 2012), syntactic simplification (Seretan, 2012; Brouwers et al., 2014), and lexical simplification for improving the understanding of medical terms (Grabar et al., 2018)). While first LS systems (Carroll et al., 1998; Devlin, 1999) used to combine WordNet (Miller, 1998) and frequency information from words, more recent ones are more sophisticated and rely on supervised machine learning methods. Their architecture can be represented with four steps as follows (Shardlow, 2014): 1. Complex Word Identification (CWI): aims to identify target words that need simplification. In CWI, the methods based on large corpora and thesaurus dominate the top 10 in SemEval 2016 (P"
W18-7004,seretan-2012-acquisition,0,0.0220269,"systems exist for various languages, for example: English (Carroll et al., 1998; Horn et al., 2014; Glavaš and Štajner, 2015; Paetzold and Specia, 2017), Spanish (Bott et al., 2012; Rello et al., 2013a), Swedish (Keskisärkkä, 2012), and Portuguese (Aluísio and Gasperin, 2010). However, to our knowledge, there is no full-fledged ATS system for French available, although some authors have investigated related aspects (i.e. simplified writing for language-impaired readers (Max, 2006), French readability for French as a Foreign Language (FFL) (François and Fairon, 2012), syntactic simplification (Seretan, 2012; Brouwers et al., 2014), and lexical simplification for improving the understanding of medical terms (Grabar et al., 2018)). While first LS systems (Carroll et al., 1998; Devlin, 1999) used to combine WordNet (Miller, 1998) and frequency information from words, more recent ones are more sophisticated and rely on supervised machine learning methods. Their architecture can be represented with four steps as follows (Shardlow, 2014): 1. Complex Word Identification (CWI): aims to identify target words that need simplification. In CWI, the methods based on large corpora and thesaurus dominate the t"
