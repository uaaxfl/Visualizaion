2005.jeptalnrecital-recital.4,W02-0603,0,0.0438051,"Missing"
2005.jeptalnrecital-recital.4,W98-1239,0,0.214287,"uate the morphological families discovered by the algorithm using a corpus of French medical texts containing words whose morphological structure is complex. 1 Introduction L'analyse des mots en morphèmes, qui sont les plus petites unités porteuses de sens, facilite l'exécution de diverses tâches telles que la recherche d'informations (Hahn et al., 2003), la construction de dictionnaires (Lovis et al., 1995) ou de terminologies (Zweigenbaum, Grabar, 2000). Les méthodes existantes permettent de découvrir des suffixes flexionnels ou dérivationnels (Gaussier, 1999), voire également des préfixes (Déjean, 1998; Schone, Jurafsky, 2001; Goldsmith, 2001; Creutz, Lagus, 2002). Cependant, l'analyse finale d'un mot se limite généralement à 3 unités morphologiques au plus ((préfixe?) + base + (suffixe?)). 555 Delphine Bernhard Certaines langues, comme l'allemand, et langues de spécialité, comme le vocabulaire médical, présentent des caractéristiques nécessitant une segmentation plus fine, notamment en raison du procédé de composition à la base de la formation des mots. La procédure de segmentation que nous proposons permet d'obtenir un découpage de chaque mot sous la forme (préfixe*) + base + (suffixe*) s"
2005.jeptalnrecital-recital.4,W99-0904,0,0.116017,"pattern: (prefix*) + base + (suffix*). We evaluate the morphological families discovered by the algorithm using a corpus of French medical texts containing words whose morphological structure is complex. 1 Introduction L'analyse des mots en morphèmes, qui sont les plus petites unités porteuses de sens, facilite l'exécution de diverses tâches telles que la recherche d'informations (Hahn et al., 2003), la construction de dictionnaires (Lovis et al., 1995) ou de terminologies (Zweigenbaum, Grabar, 2000). Les méthodes existantes permettent de découvrir des suffixes flexionnels ou dérivationnels (Gaussier, 1999), voire également des préfixes (Déjean, 1998; Schone, Jurafsky, 2001; Goldsmith, 2001; Creutz, Lagus, 2002). Cependant, l'analyse finale d'un mot se limite généralement à 3 unités morphologiques au plus ((préfixe?) + base + (suffixe?)). 555 Delphine Bernhard Certaines langues, comme l'allemand, et langues de spécialité, comme le vocabulaire médical, présentent des caractéristiques nécessitant une segmentation plus fine, notamment en raison du procédé de composition à la base de la formation des mots. La procédure de segmentation que nous proposons permet d'obtenir un découpage de chaque mot so"
2005.jeptalnrecital-recital.4,N01-1024,0,0.0331903,"Missing"
2005.jeptalnrecital-recital.4,2003.jeptalnrecital-long.27,0,0.0644246,"Missing"
2007.jeptalnrecital-long.34,baroni-bernardini-2004-bootcat,0,0.0281812,"ion de nouvelles signatures, et par conséquent d’apprentissage de familles, s’achève au bout de 10 à 15 itérations. 372 Apprentissage non supervisé de familles morphologiques 3 Évaluation Afin d’évaluer les résultats de la méthode, nous avons utilisé 4 corpus différents, en anglais et en français, couvrant deux domaines spécialisés distincts, la volcanologie et le cancer du sein. Dans la suite de cet article, ils seront désignés respectivement par volcano-en, volcano-fr, cancer-en et cancer-fr. Ces corpus ont été construits automatiquement à partir du Web en utilisant la méthode décrite dans (Baroni & Bernardini, 2004). Les listes de mots extraites de ces corpus comprennent entre 47 000 et 86 000 formes différentes. 3.1 Méthode d’évaluation L’évaluation des résultats nécessite de disposer de familles morphologiques de référence auxquelles sont comparées les familles obtenues automatiquement par classification. Nous avons utilisé deux sources pour les familles de référence : nous avons d’une part élaboré manuellement des listes de référence et, pour l’anglais, nous avons extrait des familles de référence à partir des segmentations contenues dans la base CELEX (Baayen et al., 1995). Les listes de référence co"
2007.jeptalnrecital-long.34,W02-0603,0,0.0176081,"e l’hyponymie (Buitelaar & Sacaleanu, 2002) ou l’antonymie (Schwab et al., 2005). Les ressources décrivant les liens morphologiques n’étant pas disponibles à l’heure actuelle pour toutes les langues et tous les domaines, ces applications sont fréquemment associées à l’acquisition automatique de connaissances morphologiques à partir de textes. Les méthodes d’analyse morphologique non supervisée sont variées : comparaison de graphies (Zweigenbaum & Grabar, 2000), recherche d’analogies (Lepage, 1998), modèles probabilistes (Creutz & Lagus, 2005) ou segmentation par optimisation (Goldsmith, 2001; Creutz & Lagus, 2002). Elles se distinguent également par le type de résultats obtenus : mots découpés en segments morphémiques ou liens morphologiques. Le travail présenté dans cet article relève du second type de méthode car il consiste en l’acquisition de familles morphologiques, c’est-à-dire des groupes de mots liés deux à deux par un lien morphologique d’affixation (préfixation ou suffixation) ou de composition. Nous formulons la question de l’acquisition de familles morphologiques comme un problème de classification. En effet, l’objectif de la classification est d’organiser un ensemble de données en groupes"
2007.jeptalnrecital-long.34,W99-0904,0,0.096818,"base climat pour former les mots climats et climatique. La même signature se retrouve dans les paires de mots volcans – volcanique et océans – océanique. La notion de signa1 Il faut noter que les préfixes et les suffixes sont acquis automatiquement, de manière non supervisée. Par conséquent, aucune distinction n’est faite entre les affixes flexionnels et dérivationnels. 368 Apprentissage non supervisé de familles morphologiques ture est présente dans de nombreux travaux en acquisition automatique de connaissances morphologiques, parfois sous des dénominations différentes : paires de suffixes (Gaussier, 1999), règles morphologiques (Grabar & Zweigenbaum, 1999) ou schémas de suffixation (Hathout, 2005). Nous allons maintenant détailler l’ensemble des étapes menant à l’acquisition des familles morphologiques. 2.1 Familles initiales Avant apprentissage, il y a autant de familles que de mots dans la liste donnée en entrée : chaque mot constitue sa propre famille. Les familles formées au cours du processus d’apprentissage sont représentées par un radical R. De plus, chaque famille comprend deux sous-familles, sauf si elle correspond à une feuille dans la hiérarchie : dans ce cas, elle contient un mot u"
2007.jeptalnrecital-long.34,J01-2001,0,0.0332416,"ntiques telles que l’hyponymie (Buitelaar & Sacaleanu, 2002) ou l’antonymie (Schwab et al., 2005). Les ressources décrivant les liens morphologiques n’étant pas disponibles à l’heure actuelle pour toutes les langues et tous les domaines, ces applications sont fréquemment associées à l’acquisition automatique de connaissances morphologiques à partir de textes. Les méthodes d’analyse morphologique non supervisée sont variées : comparaison de graphies (Zweigenbaum & Grabar, 2000), recherche d’analogies (Lepage, 1998), modèles probabilistes (Creutz & Lagus, 2005) ou segmentation par optimisation (Goldsmith, 2001; Creutz & Lagus, 2002). Elles se distinguent également par le type de résultats obtenus : mots découpés en segments morphémiques ou liens morphologiques. Le travail présenté dans cet article relève du second type de méthode car il consiste en l’acquisition de familles morphologiques, c’est-à-dire des groupes de mots liés deux à deux par un lien morphologique d’affixation (préfixation ou suffixation) ou de composition. Nous formulons la question de l’acquisition de familles morphologiques comme un problème de classification. En effet, l’objectif de la classification est d’organiser un ensemble"
2007.jeptalnrecital-long.34,P98-1120,0,0.0282206,"367 Delphine Bernhard la structure morphologique des mots pour l’acquisition de relations sémantiques telles que l’hyponymie (Buitelaar & Sacaleanu, 2002) ou l’antonymie (Schwab et al., 2005). Les ressources décrivant les liens morphologiques n’étant pas disponibles à l’heure actuelle pour toutes les langues et tous les domaines, ces applications sont fréquemment associées à l’acquisition automatique de connaissances morphologiques à partir de textes. Les méthodes d’analyse morphologique non supervisée sont variées : comparaison de graphies (Zweigenbaum & Grabar, 2000), recherche d’analogies (Lepage, 1998), modèles probabilistes (Creutz & Lagus, 2005) ou segmentation par optimisation (Goldsmith, 2001; Creutz & Lagus, 2002). Elles se distinguent également par le type de résultats obtenus : mots découpés en segments morphémiques ou liens morphologiques. Le travail présenté dans cet article relève du second type de méthode car il consiste en l’acquisition de familles morphologiques, c’est-à-dire des groupes de mots liés deux à deux par un lien morphologique d’affixation (préfixation ou suffixation) ou de composition. Nous formulons la question de l’acquisition de familles morphologiques comme un p"
2007.jeptalnrecital-long.34,W00-0712,0,0.019226,"tations contenues dans la base CELEX (Baayen et al., 1995). Les listes de référence construites manuellement contiennent des familles de mots pour le domaine du cancer du sein en français et en anglais. Elles contiennent 3 250 familles en anglais et 1 964 familles en français. Les familles morphologiques maximales de CELEX sont déterminées à partir des relations morphologiques de dérivation, de composition et de conversion, ce qui permet d’obtenir 14 880 familles de référence. Nous avons évalué les familles induites par rapport aux familles de référence en utilisant les mesures proposées par (Schone & Jurafsky, 2000; Schone & Jurafsky, 2001). La méthode d’évaluation consiste à faire la somme des proportions de mots corrects (C), insérés (I) et supprimés (D) dans les familles morphologiques de tous les mots w de la liste d’évaluation. Si Xw est l’ensemble des mots appartenant à la famille morphologique d’un mot w selon le système à évaluer et Yw est l’ensemble des mots appartenant à la famille morphologique de w selon CELEX ou toute autre base de référence, alors : C= ! |Xw ∩ Yw | ∀w |Yw | ; D= ! |Yw − (Xw ∩ Yw )| ∀w |Yw | et I = ! |Xw − (Xw ∩ Yw )| ∀w |Yw | À partir de ces valeurs, il est également possi"
2007.jeptalnrecital-long.34,N01-1024,0,0.0280469,"base CELEX (Baayen et al., 1995). Les listes de référence construites manuellement contiennent des familles de mots pour le domaine du cancer du sein en français et en anglais. Elles contiennent 3 250 familles en anglais et 1 964 familles en français. Les familles morphologiques maximales de CELEX sont déterminées à partir des relations morphologiques de dérivation, de composition et de conversion, ce qui permet d’obtenir 14 880 familles de référence. Nous avons évalué les familles induites par rapport aux familles de référence en utilisant les mesures proposées par (Schone & Jurafsky, 2000; Schone & Jurafsky, 2001). La méthode d’évaluation consiste à faire la somme des proportions de mots corrects (C), insérés (I) et supprimés (D) dans les familles morphologiques de tous les mots w de la liste d’évaluation. Si Xw est l’ensemble des mots appartenant à la famille morphologique d’un mot w selon le système à évaluer et Yw est l’ensemble des mots appartenant à la famille morphologique de w selon CELEX ou toute autre base de référence, alors : C= ! |Xw ∩ Yw | ∀w |Yw | ; D= ! |Yw − (Xw ∩ Yw )| ∀w |Yw | et I = ! |Xw − (Xw ∩ Yw )| ∀w |Yw | À partir de ces valeurs, il est également possible de calculer la précisi"
2007.jeptalnrecital-long.34,2005.jeptalnrecital-long.8,0,0.0283908,"morphologique est une tâche importante dans divers domaines du traitement automatique des langues comme la reconnaissance de la parole, la communication alternative et augmentée, la traduction automatique ou la recherche d’informations. Dans ce dernier cas, l’utilité des connaissances morphologiques se justifie par la proximité sémantique des variantes flexionnelles ou dérivationnelles. Il est également possible d’exploiter 367 Delphine Bernhard la structure morphologique des mots pour l’acquisition de relations sémantiques telles que l’hyponymie (Buitelaar & Sacaleanu, 2002) ou l’antonymie (Schwab et al., 2005). Les ressources décrivant les liens morphologiques n’étant pas disponibles à l’heure actuelle pour toutes les langues et tous les domaines, ces applications sont fréquemment associées à l’acquisition automatique de connaissances morphologiques à partir de textes. Les méthodes d’analyse morphologique non supervisée sont variées : comparaison de graphies (Zweigenbaum & Grabar, 2000), recherche d’analogies (Lepage, 1998), modèles probabilistes (Creutz & Lagus, 2005) ou segmentation par optimisation (Goldsmith, 2001; Creutz & Lagus, 2002). Elles se distinguent également par le type de résultats o"
2007.jeptalnrecital-long.34,W05-1729,0,0.0550085,"Missing"
2011.jeptalnrecital-court.6,N10-1086,0,0.0502098,"Missing"
2011.jeptalnrecital-court.6,levy-andrew-2006-tregex,0,0.0623288,"Missing"
2011.jeptalnrecital-court.6,N10-1048,0,0.0309641,"Missing"
2011.jeptalnrecital-court.6,W03-1605,0,0.0322934,"Missing"
2011.jeptalnrecital-long.24,ayache-etal-2006-equer,0,0.0476249,"Missing"
2011.jeptalnrecital-long.24,P08-4006,0,0.0207326,"Missing"
2011.jeptalnrecital-long.24,grappy-etal-2010-corpus,0,0.061322,"Missing"
2011.jeptalnrecital-long.24,hathout-tanguy-2002-webaffix,0,0.140899,"Missing"
2011.jeptalnrecital-long.24,jacquemin-2010-derivational,0,0.0615449,"Missing"
2011.jeptalnrecital-long.24,quintard-etal-2010-question,0,0.022212,"Missing"
2011.jeptalnrecital-long.24,P02-1006,0,0.14817,"Missing"
2011.jeptalnrecital-long.31,W08-0607,0,0.0304446,"Missing"
2011.jeptalnrecital-long.31,W09-1418,0,0.0463343,"Missing"
2011.jeptalnrecital-long.31,W04-3103,0,0.0873363,"Missing"
2011.jeptalnrecital-long.31,P07-1125,0,0.0554435,"Missing"
2011.jeptalnrecital-long.31,W09-1304,0,0.0434246,"Missing"
2011.jeptalnrecital-long.31,D09-1019,0,0.0224095,"Missing"
2011.jeptalnrecital-long.31,W02-1011,0,0.0116152,"Missing"
2011.jeptalnrecital-long.31,N07-2036,0,0.0274297,"Missing"
2011.jeptalnrecital-long.31,P08-1033,0,0.0248729,"Missing"
2011.jeptalnrecital-long.31,W08-0606,0,0.0456914,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-recital.5,1,0.860909,"Missing"
2011.jeptalnrecital-long.33,A94-1019,0,0.0542167,"Missing"
2011.jeptalnrecital-long.33,W09-3102,0,0.0650934,"Missing"
2011.jeptalnrecital-long.33,I05-5002,0,0.087889,"Missing"
2011.jeptalnrecital-long.33,P08-4006,0,0.0470072,"Missing"
2011.jeptalnrecital-long.33,J10-3003,0,0.0590089,"Missing"
2011.jeptalnrecital-long.33,2008.jeptalnrecital-long.23,1,0.825827,"Missing"
2011.jeptalnrecital-long.33,D10-1064,1,0.89007,"Missing"
2011.jeptalnrecital-long.33,max-wisniewski-2010-mining,1,0.850881,"Missing"
2011.jeptalnrecital-long.33,2010.jeptalnrecital-long.13,1,0.853233,"Missing"
2011.jeptalnrecital-long.33,N10-1056,0,0.0334747,"Missing"
2011.jeptalnrecital-long.33,zesch-etal-2008-extracting,0,0.0211202,"Missing"
2016.jeptalnrecital-long.2,W12-2205,0,0.0426167,"Missing"
2016.jeptalnrecital-long.2,W11-2103,0,0.0615295,"Missing"
2016.jeptalnrecital-long.2,N15-1059,0,0.0298148,"Missing"
2016.jeptalnrecital-long.2,francois-etal-2014-flelex,1,0.891696,"Missing"
2016.jeptalnrecital-long.2,D09-1094,0,0.0286724,"Missing"
2016.jeptalnrecital-long.2,P13-1132,0,0.0692508,"Missing"
2016.jeptalnrecital-long.2,P13-3015,0,0.0449907,"Missing"
2016.jeptalnrecital-long.2,S12-1046,0,0.0549283,"Missing"
2016.jeptalnrecital-long.2,J10-2002,0,0.0764448,"Missing"
2016.jeptalnrecital-poster.30,W04-2104,0,0.0708874,"Missing"
2016.jeptalnrecital-poster.30,sagot-2010-lefff,0,0.0842709,"Missing"
2016.jeptalnrecital-poster.30,sagot-2014-delex,0,0.0610175,"Missing"
2016.jeptalnrecital-poster.30,sennrich-kunz-2014-zmorge,0,0.0611327,"Missing"
2019.jeptalnrecital-court.29,W16-3905,0,0.0442456,"Missing"
2019.jeptalnrecital-court.29,L18-1619,1,0.88887,"Missing"
2019.jeptalnrecital-court.29,F12-2024,0,0.0605087,"Missing"
2019.jeptalnrecital-court.29,L18-1347,0,0.0464786,"Missing"
2019.jeptalnrecital-court.29,W16-1715,0,0.0630444,"Missing"
2019.jeptalnrecital-court.29,C94-1097,0,0.736067,"Missing"
2019.jeptalnrecital-court.29,L16-1262,0,0.0602156,"Missing"
2019.jeptalnrecital-court.29,L18-1279,0,0.022254,"Missing"
2019.jeptalnrecital-court.29,K17-3009,0,0.0550734,"Missing"
2019.jeptalnrecital-court.29,taule-etal-2008-ancora,0,0.140256,"Missing"
2019.jeptalnrecital-long.7,L18-1249,1,0.851646,"Missing"
2019.jeptalnrecital-long.7,W18-6006,0,0.0237444,"Missing"
2019.jeptalnrecital-long.7,W17-0406,0,0.0504133,"Missing"
2019.jeptalnrecital-long.7,L18-1290,0,0.0336375,"Missing"
2019.jeptalnrecital-long.7,P92-1002,0,0.548945,"Missing"
2019.jeptalnrecital-long.7,W16-0705,0,0.0382283,"Missing"
2019.jeptalnrecital-long.7,P14-5010,0,0.0056989,"Missing"
2019.jeptalnrecital-long.7,2016.lilt-13.1,0,0.0474309,"Missing"
2019.jeptalnrecital-long.7,W18-5409,0,0.0289731,"Missing"
2019.jeptalnrecital-long.7,N18-2038,0,0.0357671,"Missing"
2019.jeptalnrecital-long.7,N18-1105,0,0.0342437,"Missing"
2019.jeptalnrecital-long.7,tiedemann-2012-parallel,0,0.0611243,"Missing"
C08-4002,H05-1103,0,0.064591,"Missing"
C08-4002,W06-1416,0,0.04396,"Missing"
C08-4002,P06-4001,0,0.0219699,"Missing"
C08-4002,W05-0203,0,0.0602349,"Missing"
C08-4002,W05-0201,0,0.0603001,"Missing"
C08-4002,P98-1032,0,0.0234241,"Missing"
C08-4002,W08-0913,0,0.0224401,"Missing"
C08-4002,N06-1028,0,0.0608849,"Missing"
C08-4002,W08-0912,0,0.0310858,"Missing"
C08-4002,E99-1042,0,0.0589513,"Missing"
C08-4002,W03-1602,0,0.0301108,"Missing"
C08-4002,zesch-etal-2008-extracting,1,0.790896,"Missing"
C08-4002,P08-1021,0,0.0470054,"Missing"
C08-4002,P07-1011,0,0.0375383,"Missing"
C08-4002,D07-1012,0,0.026906,"Missing"
C08-4002,P06-1036,0,0.0441154,"Missing"
C08-4002,W08-0907,0,0.0332319,"Missing"
C08-4002,2005.sigdial-1.10,0,0.0614936,"Missing"
C08-4002,N04-3002,0,0.0608916,"Missing"
C08-4002,W06-1650,0,0.0730046,"Missing"
C08-4002,P07-2032,1,0.807216,"Missing"
C08-4002,P07-1130,1,0.883425,"Missing"
C10-1152,W03-1004,0,0.54068,"Missing"
C10-1152,E99-1042,0,0.966587,"tting, dropping, reordering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. 1 Introduction Sentence simplification transforms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of"
C10-1152,C96-2183,0,0.821322,"ms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role lab"
C10-1152,W08-1105,0,0.612298,"r translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution"
C10-1152,D08-1019,0,0.264012,"r translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution"
C10-1152,J08-3004,0,0.0123846,"ce compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate the simplified sentences using a greedy strategy"
C10-1152,W03-1602,0,0.724115,"ering and substitution integrally. We also describe an efficient method to train our model with a large-scale parallel dataset obtained from the Wikipedia and Simple Wikipedia. The evaluation shows that our model achieves better readability scores than a set of baseline systems. 1 Introduction Sentence simplification transforms long and difficult sentences into shorter and more readable ones. This helps humans read texts more easily and faster. Reading assistance is thus an important application of sentence simplification, especially for people with reading disabilities (Carroll et al., 1999; Inui et al., 2003), low-literacy readers (Watanabe et al., 2009), or non-native speakers (Siddharthan, 2002). Not only human readers but also NLP applications can benefit from sentence simplification. The original motivation for sentence simplification is using it as a preprocessor to facilitate parsing or translation tasks (Chandrasekar et al., 1996). Complex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research"
C10-1152,E06-1021,0,0.0526913,"Missing"
C10-1152,P05-1065,0,0.0122847,"e. This implies that good monolingual translation is not necessarily good simplification. OOV is the percentage of words that are not in the Basic English BE850 list.10 TSM is ranked as the second best system for this criterion. The perplexity (PPL) is a score of text probability measured by a language model and normalized by the number of words in the text (Equ. 6). 10 http://simple.wikipedia.org/wiki/ Wikipedia:Basic_English_alphabetical_ wordlist PPL can be used to measure how tight the language model fits the text. Language models constitute an important feature for assessing readability (Schwarm and Ostendorf, 2005). We train a trigram LM using the simple sentences in PWKP and calculate the PPL with SRILM. TSM gets the best PPL score. From this table, we can conclude that TSM achieves better overall readability than the baseline systems. 1 P P L(text) = P (w1 w2 ...wN )− N (6) There are still some important issues to be considered in future. Based on our observations, the current model performs well for word substitution and segmentation. But the completion of the new sentences is still problematic. For example, we copy the dependent NP to the new sentences. This may break the coherence between sentences"
C10-1152,P08-1040,0,0.0347455,"ex sentences are considered as stumbling blocks for such systems. More recently, sentence simplification has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution are widely accepted as important simplification opera"
C10-1152,P01-1067,0,0.025569,"f (2007) focus on sentence splitting, while sentence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate t"
C10-1152,P02-1039,0,0.00549496,"e splitting, while sentence compression systems (Filippova and Strube, 2008a) mainly use the dropping operation. As far as lexical simplification is concerned, word substitution is usually done by selecting simpler synonyms from Wordnet based on word frequency (Carroll et al., 1999). In this paper, we propose a sentence simplification model by tree transformation which is based 1353 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1353–1361, Beijing, August 2010 on techniques from statistical machine translation (SMT) (Yamada and Knight, 2001; Yamada and Knight, 2002; Graehl et al., 2008). Our model integrally covers splitting, dropping, reordering and phrase/word substitution. The parameters of our model can be efficiently learned from complexsimple parallel datasets. The transformation from a complex sentence to a simple sentence is conducted by applying a sequence of simplification operations. An expectation maximization (EM) algorithm is used to iteratively train our model. We also propose a method based on monolingual word mapping which speeds up the training process significantly. Finally, a decoder is designed to generate the simplified sentences u"
C10-1152,zesch-etal-2008-extracting,1,0.7627,"Missing"
C10-1152,P09-1094,0,0.008128,"ication has also been shown helpful for summarization (Knight and Marcu, 2000), ∗ This work has been supported by the Emmy Noether Program of the German Research Foundation (DFG) under the grant No. GU 798/3-1, and by the Volkswagen Foundation as part of the Lichtenberg-Professorship Program under the grant No. I/82806. Iryna Gurevych1 Department of Computer Science Technische Universit¨at Darmstadt 2 delphine.bernhard@limsi.fr sentence fusion (Filippova and Strube, 2008b), semantic role labeling (Vickrey and Koller, 2008), question generation (Heilman and Smith, 2009), paraphrase generation (Zhao et al., 2009) and biomedical information extraction (Jonnalagadda and Gonzalez, 2009). At sentence level, reading difficulty stems either from lexical or syntactic complexity. Sentence simplification can therefore be classified into two types: lexical simplification and syntactic simplification (Carroll et al., 1999). These two types of simplification can be further implemented by a set of simplification operations. Splitting, dropping, reordering, and substitution are widely accepted as important simplification operations. The splitting operation splits a long sentence into several shorter sentences to de"
C10-2007,P06-1014,0,0.0341143,"nonymy or hypernymy, definitions are readily available, even for less-resourced languages. Moreover, they can be used for a wide variety of tasks, ranging from word sense disambiguation (Lesk, 1986), to producing multiple-choice questions for educational applications (Kulkarni et al., 2007) or synonym discovery (Wang and Hirst, 2009). However, all resources differ in coverage and word sense granularity, which may lead to several shortcomings when using a single resource. For instance, the sense inventory in WordNet has been shown to be too fine-grained for efficient word sense disambiguation (Navigli, 2006; Snow et al., 2007). Moreover, gloss and definition-based measures of semantic relatedness which rely on the overlap between the definition of a target word and its distributional context (Lesk, 1986) or the definition of another concept (Banerjee and Pedersen, 2003) yield low results when the definitions provided are short and do not overlap sufficiently. As a consequence, we propose combining lexical resources to alleviate the coverage and granularity problems. To this aim, we automatically build cross-resource sense clusters. The goal of our approach is to capture redundancy in several res"
C10-2007,P08-1017,0,0.023424,"ages 54–62, Beijing, August 2010 local techniques, which expand the query based on an analysis of the documents returned. Local methods are also known as relevance feedback. A first type of global QE methods relies on external hand-crafted lexical-semantic resources such as WordNet. While expansion based on external resources is deemed more efficient than expansion relying on relevance feedback, it also has to tackle problems of semantic ambiguity, which explains why local analysis has been shown to be generally more effective than global analysis (Xu and Croft, 1996). However, recent work by Fang (2008) has demonstrated that global expansion based on WordNet and co-occurrence based resources can lead to performance improvement in an axiomatic model of information retrieval. Corpus-derived co-occurrence relationships are also exploited for query expansion. Qiu and Frei (1993) build a corpus-based similarity thesaurus using the method described in Sch¨utze (1998) and expand a query with terms which are similar to the query concept based on the similarity thesaurus. Song and Bruza (2003) construct vector representations for terms from the target document collection using the Hyperspace Analogue"
C10-2007,P07-1059,0,0.0360931,"us. Song and Bruza (2003) construct vector representations for terms from the target document collection using the Hyperspace Analogue to Language (HAL) model (Lund and Burgess, 1996). The representations for all the terms in the query are then combined by a restricted form of vector addition. Finally, expansion terms are derived from this combined vector by information flow. Quasi-parallel monolingual corpora have been recently employed for query expansion, using statistical machine translation techniques. Expansion terms are acquired by training a translation model on question-answer pairs (Riezler et al., 2007) or query-snippets pairs (Riezler et al., 2008) and by extracting paraphrases from bilingual phrase tables (Riezler et al., 2007). The main difficulty of QE methods lies in selecting the most relevant expansion terms, especially when the query contains ambiguous words. Moreover, even if the original query is not ambiguous, it might become so after expansion. Recent attempts at integrating word sense disambiguation (WSD) in IR within the CLEF Robust WSD track1 have led to mixed results which show 1 that in most cases WSD does not improve performance of monolingual and cross-lingual IR systems ("
C10-2007,D07-1073,0,0.0477475,"Missing"
C10-2007,C08-1093,0,0.0205693,"esentations for terms from the target document collection using the Hyperspace Analogue to Language (HAL) model (Lund and Burgess, 1996). The representations for all the terms in the query are then combined by a restricted form of vector addition. Finally, expansion terms are derived from this combined vector by information flow. Quasi-parallel monolingual corpora have been recently employed for query expansion, using statistical machine translation techniques. Expansion terms are acquired by training a translation model on question-answer pairs (Riezler et al., 2007) or query-snippets pairs (Riezler et al., 2008) and by extracting paraphrases from bilingual phrase tables (Riezler et al., 2007). The main difficulty of QE methods lies in selecting the most relevant expansion terms, especially when the query contains ambiguous words. Moreover, even if the original query is not ambiguous, it might become so after expansion. Recent attempts at integrating word sense disambiguation (WSD) in IR within the CLEF Robust WSD track1 have led to mixed results which show 1 that in most cases WSD does not improve performance of monolingual and cross-lingual IR systems (Agirre et al., 2009). For query expansion based"
C10-2007,J98-1004,0,0.348833,"Missing"
C10-2007,D07-1107,0,0.0262807,"nymy, definitions are readily available, even for less-resourced languages. Moreover, they can be used for a wide variety of tasks, ranging from word sense disambiguation (Lesk, 1986), to producing multiple-choice questions for educational applications (Kulkarni et al., 2007) or synonym discovery (Wang and Hirst, 2009). However, all resources differ in coverage and word sense granularity, which may lead to several shortcomings when using a single resource. For instance, the sense inventory in WordNet has been shown to be too fine-grained for efficient word sense disambiguation (Navigli, 2006; Snow et al., 2007). Moreover, gloss and definition-based measures of semantic relatedness which rely on the overlap between the definition of a target word and its distributional context (Lesk, 1986) or the definition of another concept (Banerjee and Pedersen, 2003) yield low results when the definitions provided are short and do not overlap sufficiently. As a consequence, we propose combining lexical resources to alleviate the coverage and granularity problems. To this aim, we automatically build cross-resource sense clusters. The goal of our approach is to capture redundancy in several resources, while improv"
C10-2007,R09-1084,0,0.024657,"ition clusters acquired by combining several English lexical resources. 3 Acquisition of Definition Clusters Dictionary definitions constitute a formidable resource for Natural Language Processing. In contrast to explicit structural and semantic relations between word senses such as synonymy or hypernymy, definitions are readily available, even for less-resourced languages. Moreover, they can be used for a wide variety of tasks, ranging from word sense disambiguation (Lesk, 1986), to producing multiple-choice questions for educational applications (Kulkarni et al., 2007) or synonym discovery (Wang and Hirst, 2009). However, all resources differ in coverage and word sense granularity, which may lead to several shortcomings when using a single resource. For instance, the sense inventory in WordNet has been shown to be too fine-grained for efficient word sense disambiguation (Navigli, 2006; Snow et al., 2007). Moreover, gloss and definition-based measures of semantic relatedness which rely on the overlap between the definition of a target word and its distributional context (Lesk, 1986) or the definition of another concept (Banerjee and Pedersen, 2003) yield low results when the definitions provided are s"
C16-1094,J08-1001,0,0.0953665,"the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their characteristics. Barzilay and Lapata (2008) considered a text as a matrix of discourse entities present in each sentence. The cohesive level of a text is then computed based on the transitions between those entities. Pitler and Nenkova (2008) implemented this model through 17 readability variables, but none was significantly correlated with difficulty. Feng et al. (2009) also replicated this technique, without getting more efficient features. Dascalu et al. (2013) computed other characteristics of lexical chains and co-reference pairs (such as the number of chains, the distance between entities, the average word length of entities, etc"
C16-1094,E09-1027,0,0.154132,"itive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohesion aspects for the"
C16-1094,W13-1504,0,0.0306626,"o measured its association with text difficult and obtained a non significant correlation (r = −0.1). Later, McNamara et al. (2010) reached a similar conclusion, showing that an LSA-based variable has not much of a predictive power. On the opposite, Franc¸ois and Fairon (2012; 2013) obtained a higher correlation (r = 0.63) for an L2 corpus, while Dascalu et al. (2013) got good discriminating features using both LSA and LDA (Latent Dirichlet Allocation), when classifying TASA (Touchstone Applied Science Associates) texts. An alternative approach to LSA, Lexical Tightness (LT), was suggested by Flor et al. (2013). They define the LT of a text as the mean value of the Positive Normalized Pointwise Mutual Information for all pairs of content-word tokens in a text. It represents “the degree to which a text tends to use words that are highly inter-associated in the language”. They obtained a good correlation between this new cohesive metric and the grade levels on two corpora (respectively r = −546 and r = −0, 441). Interestingly, they also show that LT works better to discriminate between literary texts than informative ones. Another approach is to detect co-reference chains and compute some of their cha"
C16-1094,E09-3003,1,0.866157,"Missing"
C16-1094,D12-1043,1,0.931401,"Missing"
C16-1094,J95-2003,0,0.785124,", perishing when the ship sank on the 15th April 1912. (Wikipedia) Figure 1: Example of anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker,"
C16-1094,muzerelle-etal-2014-ancor,0,0.0161241,"umptive anaphora or groups (the pronoun ils in Fig. 2 refers to the group composed of Antoine and Catherine). Based on these guidelines, a common batch, composed of 10 randomly selected files, was annotated by all the annotators. It was used to identify annotation divergences between annotators1 and to correct the annotation guide. We computed the overall inter-annotator agreement on this common batch using the mean Krippendorff’s alpha on each text and we obtained 0.47, which corresponds to a moderate agreement between annotators. Such value is however not unusual in co-reference annotation (Muzerelle et al., 2014). Then, following the annotation guide, each expert annotated a batch of 12 texts from the corpus. At the end of the process, the principal annotator checked all batches against the guidelines, thus creating the reference for our experimentation. [Antoine] 1/S/NPr/partie(3) fait la connaissance de [Catherine] 2/CN/NPr/partie(3). [Antoine] 1/S/NPr est [un beau parleur ] 1/X/GNI et [la jeune fille] 2/S/GND [s’] 2/X/Pronref int´eresse a` [lui] 1/OI/Pron. [Ils] 3/S/Pron vont au cin´ema ensemble. Figure 2: Example of annotated data : the number of the entity, the syntactic function and the category"
C16-1094,D08-1020,0,0.15088,"tic features in their cognitive readability formulas improved performance. Chall and Dale (1995, 111) had a more mixed opinion, arguing that variables based on higher textual dimensions “discriminate better among materials requiring greater maturity in reading ability”, while classic lexico-syntactic variables work better to discriminate at lower levels of difficulty. Recently, taking advantage of the opportunities offered by Natural Language Processing (NLP) techniques, readability studies have tried to leverage the semantic and discursive properties of texts to better model text difficulty (Pitler and Nenkova, 2008; Feng et al., 2009). Among those high-level dimensions that have attracted substantial attention are the level of cohesion and coherence of texts. Although psycholinguistic experiments have shown that a higher level of cohesion and coherence between a pair of related sentences decreases their reading time (Kintsch et al., 1975; Mason and Just, 2004), the added value of these textual dimensions for readability models (compared to traditional features) remains unclear, as it will be covered in more details in Section 2. This is why this paper aims at further investigating the importance of cohe"
C16-1094,J04-3003,0,0.0378274,"anaphoric and of co-reference chain. These three devices strengthen the links between several utterances and contribute to the overall understanding of the text (Charolles, 1995). Lexical chains are effective mechanisms to find the main domain or theme of the document. Cohesive devices such as anaphora or co-reference chains correspond to one entity expressed by various linguistic expressions (so called mentions). These expressions are related by complex morpho-syntactic, syntactic or semantic constraints (Grosz et al., 1995). Mentioning the same entity several times reinforces text cohesion (Poesio et al., 2004), (Hobbs, 1979). Cohesive devices reinforce local coherence relations in some specific genres (persuasive genres) (Berzlnovich and Redeker, 2012). An interesting characteristic of cohesive devices is that their use is dependent on the type or genre of texts (Carter-Thomas, 1994). For instance, informative texts use specific referential expressions such as definite or demonstrative noun phrases as mentions, while narrative texts contain more chains composed of proper nouns or personal pronouns (Schnedecker, 2005). The composition, the length or the choice of the first mention of the co-referenc"
E06-2022,W04-0106,0,0.0663465,"Missing"
E06-2022,P98-1092,0,0.0340745,"elying on the detection of classical prefixes and word-initial combining forms. Word-forming units are identified using a regular expression. The system then extracts terms by selecting words which either begin or coalesce with these elements. Next, terms are grouped in families which are displayed as a weighted list in HTML format. 1 Introduction Many methods for the automatic extraction of terms make use of patterns describing the structure of terms. This approach is especially helpful for multi-word terms. Depending on the method, patterns rely on morpho-syntactic properties (Daille, 1996; Ibekwe-SanJuan, 1998), the co-occurrence of terms and connectors (Enguehard, 1992; Baroni and Bernardini, 2004) or the alternation of informative and non-informative words (Vergne, 2005). These patterns use words as basic units and thus apply to multi-word terms. Methods for the acquisition of single-word terms generally depend on frequency-related information. For instance, the frequency of occurrence of a word in a domain-specific corpus can be compared with its frequency of occurrence in a reference corpus (Rayson and Garside, 2000; Baroni and Bernardini, 2004). Technical words usually have a high relative freq"
E06-2022,2005.jeptalnrecital-long.7,0,0.0280889,"i, 2004). Technical words usually have a high relative frequency difference between the domainspecific corpus and the reference corpus. In this paper, we present a pattern-based technique to extract single-word terms. In technical and scientific domains like medicine many terms are derivatives or neoclassical compounds (Cottez, 1984). There are several types of classical word-forming units: prefixes (extra-, anti-), initial combining forms (hydro-, pharmaco-), suffixes (-ism) and final combining forms (-graphy, -logy). Interestingly, these units are rather constant in many European languages (Namer, 2005). Consequently, instead of relying on a subword dictionary to analyse compounds like (Schulz et al., 2002), our method makes use of these regularities to automatically extract prefixes and initial combining forms from corpora. The system then identifies terms by selecting words which either begin or coalesce with these units. Moreover, forming elements are used to group terms in morphological and hence semantic families. The different stages of the process are detailed in section 2. Section 3 describes the results of experiments performed on four corpora, in English and in French. 2 2.1 Descri"
E06-2022,W00-0901,0,0.152294,"epending on the method, patterns rely on morpho-syntactic properties (Daille, 1996; Ibekwe-SanJuan, 1998), the co-occurrence of terms and connectors (Enguehard, 1992; Baroni and Bernardini, 2004) or the alternation of informative and non-informative words (Vergne, 2005). These patterns use words as basic units and thus apply to multi-word terms. Methods for the acquisition of single-word terms generally depend on frequency-related information. For instance, the frequency of occurrence of a word in a domain-specific corpus can be compared with its frequency of occurrence in a reference corpus (Rayson and Garside, 2000; Baroni and Bernardini, 2004). Technical words usually have a high relative frequency difference between the domainspecific corpus and the reference corpus. In this paper, we present a pattern-based technique to extract single-word terms. In technical and scientific domains like medicine many terms are derivatives or neoclassical compounds (Cottez, 1984). There are several types of classical word-forming units: prefixes (extra-, anti-), initial combining forms (hydro-, pharmaco-), suffixes (-ism) and final combining forms (-graphy, -logy). Interestingly, these units are rather constant in man"
E06-2022,W02-0309,0,0.0268895,"ific corpus and the reference corpus. In this paper, we present a pattern-based technique to extract single-word terms. In technical and scientific domains like medicine many terms are derivatives or neoclassical compounds (Cottez, 1984). There are several types of classical word-forming units: prefixes (extra-, anti-), initial combining forms (hydro-, pharmaco-), suffixes (-ism) and final combining forms (-graphy, -logy). Interestingly, these units are rather constant in many European languages (Namer, 2005). Consequently, instead of relying on a subword dictionary to analyse compounds like (Schulz et al., 2002), our method makes use of these regularities to automatically extract prefixes and initial combining forms from corpora. The system then identifies terms by selecting words which either begin or coalesce with these units. Moreover, forming elements are used to group terms in morphological and hence semantic families. The different stages of the process are detailed in section 2. Section 3 describes the results of experiments performed on four corpora, in English and in French. 2 2.1 Description of the method Extraction of words The system takes as input a corpus of texts. Paragraphs written in"
E06-2022,C98-1089,0,\N,Missing
E06-2022,baroni-bernardini-2004-bootcat,0,\N,Missing
F12-2016,C10-2013,0,0.0266869,"Missing"
F12-2016,E99-1042,0,0.125646,"Missing"
F12-2016,C96-2183,0,0.314639,"Missing"
F12-2016,Y09-1013,0,0.0206499,"Missing"
F12-2016,W09-1802,0,0.0832225,"Missing"
F12-2016,W03-1602,0,0.150315,"Missing"
F12-2016,N09-2045,0,0.0586264,"Missing"
F12-2016,levy-andrew-2006-tregex,0,0.0718403,"Missing"
F12-2016,E06-1021,0,0.082197,"Missing"
F12-2016,D11-1038,0,0.145607,"Missing"
F12-2016,C10-1152,1,0.927434,"Missing"
F13-1036,P11-2087,0,0.0575878,"Missing"
F13-1036,E99-1042,0,0.183517,"Missing"
F13-1036,W12-2202,0,0.0436198,"Missing"
F13-1036,S12-1066,0,0.0236179,"Missing"
F13-1036,S12-1068,1,0.888679,"Missing"
F13-1036,quasthoff-etal-2006-corpus,0,0.0705231,"Missing"
F13-1036,D11-1038,0,0.0383291,"Missing"
F13-1036,N10-1056,0,0.0755902,"Missing"
F14-1009,P11-2087,0,0.0634643,"Missing"
F14-1009,C96-2183,0,0.0712039,"Missing"
F14-1009,D12-1043,1,0.889099,"Missing"
F14-1009,francois-etal-2014-flelex,1,0.541604,"Missing"
F14-1009,2008.jeptalnrecital-court.10,1,0.828336,"Missing"
F14-1009,P10-1023,0,0.0290368,"Missing"
F14-1009,quasthoff-etal-2006-corpus,0,0.015257,"Missing"
F14-1009,W04-2104,0,0.0312172,"Missing"
F14-1009,S12-1046,0,0.101046,"Missing"
F14-2008,W11-0809,0,0.0248421,"Missing"
F14-2008,2010.jeptalnrecital-long.3,0,0.0942271,"Missing"
F14-2008,falk-etal-2014-non,1,0.842648,"Missing"
F14-2008,C04-1082,0,0.0380128,"Missing"
F14-2008,quasthoff-etal-2006-corpus,0,0.0573485,"Missing"
F14-2008,sagot-2010-lefff,0,0.0507102,"Missing"
F14-2008,N03-1033,0,0.125577,"Missing"
F14-2023,2003.mtsummit-papers.26,0,0.0271883,"Missing"
F14-2023,P14-1025,0,0.0510341,"Missing"
F14-2023,E12-1060,0,0.0424995,"Missing"
F14-2023,W04-2104,0,0.0549632,"Missing"
falk-etal-2014-non,W11-0134,0,\N,Missing
falk-etal-2014-non,E12-1060,0,\N,Missing
falk-etal-2014-non,W04-2104,0,\N,Missing
falk-etal-2014-non,janssen-2012-neotag,0,\N,Missing
falk-etal-2014-non,Y11-1028,0,\N,Missing
L18-1619,N13-1014,0,0.0243344,"nguages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations for three regional languages of France, namely Alsatian, Occitan and Picard. The tagsets are based on an extended version of the Universal POS tags, with some language-specific additions to account for particular linguistic phenomena. The annotation guidelines as well as the manually annotated corpora are freely available. We plan to use these corpora to devel"
L18-1619,P14-2062,0,0.0289724,"difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimum tag dictionary for the most frequent word types, such as used by (Garrette and Baldridge, 2013). This kind of approach still requires a test corpus to evaluate the tagger; and the performance remains low compared to more resourced languages. 6. Conclusion We have presented our methodology for producing corpora with POS annotations fo"
L18-1619,C94-1097,0,0.819681,"Missing"
L18-1619,L16-1262,0,0.116016,"Missing"
L18-1619,W14-5303,1,0.882391,"Missing"
L18-1619,H01-1035,0,0.160961,"potlights that they lit the pit . Table 10: Annotation example for Picard The annotation guidelines and the corpora are available for all three languages on the Zenodo platform, in the RESTAURE project community (see Section 9. for the corpus list).17 5. Related Work Creating annotated corpora for under-resourced languages presents several difficulties. First, it requires assembling a large textual corpus, which can be a challenge for these languages which have few electronic resources. Work on part-of-speech tagging for under-resourced languages is often based on parallel corpora, following (Yarowsky et al., 2001), but there are no such existing electronic corpora for the three languages considered. Then, a tagset has to be created or adapted to the language, which requires linguistic expertise. Finally, the annotation also requires annotators with linguistic expertise. Crowdsourcing can be used for part-of-speech annotation (Hovy et al., 2014), and was even used for Alsatian (Millour et al., 2017). Yet, crowdsourcing necessitates an adapted platform, and communication to possible speakers, who for example in the case of Picard, are rare. A possible direction for POS tagging could be to create a minimu"
L18-1654,W09-2010,0,0.0378529,"s in words or names. The goal is to encode the input string using simplified phonetic rules. More sophisticated methods like grapheme-to-phoneme can also be used for searching words in corpora and dictionaries.1 The goal is either to be able to retrieve words without knowing their exact spelling, or to abstract from the spelling variations in nonstandardized languages, by using a phonetic index (Divay and Vitale, 1997). In addition to improving the recall of queries, phonetic indexing and grapheme-to-phoneme techniques have been used to normalize texts in non-standard spellings. For instance, Cook and Stevenson (2009) take the phonemes of the standard word into account when normalizing graphemes in SMS text messages. Porta et al. (2013) integrate grapheme-to-phoneme transcription rules as well as rules expressing phonological change in a rule-based transducer from Old Spanish to Modern Spanish. These studies demonstrate that using knowledge about phonemes is relevant when dealing with non-standard spellings. 1 See e.g. the Trésor de la Langue Française French dictionary (http://atilf.atilf.fr/tlf.htm) which uses pseudo-phonetic input or the Picartext textual database for the Picard language (https://www.u-"
L18-1654,P16-1038,0,0.0303055,"ondition is to have such a system at disposal. In particular for low-resource languages, which also present high levels of spelling variation, the development of grapheme-to-phoneme systems is hindered by the lack of complete studies about the phonological system as well as the absence or the small volume of existing pronunciation dictionaries (word-pronunciation pairs). Moreover, when spelling is not normalized, designing grapheme-to-phoneme systems is known to be a complex task (Adda-Decker et al., 2011). The lack of resources can possibly be addressed by re-using data from other languages: Deri and Knight (2016) present an approach to train G2P models for low-resourced languages based on word-pronunciation pairs acquired from Wiktionary and on an adaptation of high-resource languages G2P models to closely related low-resource languages. This method nevertheless requires phoneme inventories to compute a phonetic distance metric between languages. 3. Datasets and Preprocessing There are five main dialectal areas in Alsace, characterized by differences in their sound inventories: Rhine Franconian, South Franconian, High Alemannic, Low Alemannic from the north of the region, and Low Alemannic from the so"
L18-1654,J97-4001,0,0.182538,"when dealing with spelling variation. Phonetic indexing algorithms, like Double Metaphone (Philips, 2000), have been developed in the context of information retrieval to account for spelling differences in words or names. The goal is to encode the input string using simplified phonetic rules. More sophisticated methods like grapheme-to-phoneme can also be used for searching words in corpora and dictionaries.1 The goal is either to be able to retrieve words without knowing their exact spelling, or to abstract from the spelling variations in nonstandardized languages, by using a phonetic index (Divay and Vitale, 1997). In addition to improving the recall of queries, phonetic indexing and grapheme-to-phoneme techniques have been used to normalize texts in non-standard spellings. For instance, Cook and Stevenson (2009) take the phonemes of the standard word into account when normalizing graphemes in SMS text messages. Porta et al. (2013) integrate grapheme-to-phoneme transcription rules as well as rules expressing phonological change in a rule-based transducer from Old Spanish to Modern Spanish. These studies demonstrate that using knowledge about phonemes is relevant when dealing with non-standard spellings"
P09-1082,J03-1002,0,0.00327401,"We use the Wiktionary dump from January 11, 2009. • English and Simple English Wikipedia. We use the Wikipedia dump from February 6, 2007 and the Simple Wikipedia dump from July 24, 2008. The Simple English Wikipedia is an English Wikipedia targeted at non-native speakers of English which uses simpler words than the English Wikipedia. Wikipedia and Simple Wikipedia articles do not directly correspond to glosses such as those found in dictionaries, we therefore considered the first paragraph in articles as a surrogate for glosses. 3.3 Translation Model Training We used the GIZA++ SMT Toolkit4 (Och and Ney, 2003) in order to obtain word-to-word translation probabilities from the parallel datasets described above. As is common practice in translation-based retrieval, we utilised the IBM translation model 1. The only pre-processing steps performed for all parallel datasets were tokenisation and stop word removal.5 3.4 Comparison of Word-to-Word Translations Table 1 gives some examples of word-to-word translations obtained for the different parallel corpora used (the column ALLPool will be described in the next section). As evidenced by this table, Given a list of 86,584 seed lexemes extracted from WordN"
P09-1082,P08-1017,0,0.0249326,"ector-space model for answer finding. 1 Introduction The lexical gap (or lexical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact tra"
P09-1082,P07-1059,0,0.298922,"on retrieval (M¨uller et al., 2007). Berger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translati"
P09-1082,C08-1093,0,0.015246,"erms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically clustered semantically similar questions based on their answers. Murdock and Croft (2005) created a first parallel corpus of synonym pairs extracted from WordNet, and an additional parallel corpus of English words translating to the same Arabic term in a parallel English-Arabic corpus. Similar work has also been performed in the area of query expansion using training data consisting of FAQ pages (Riezler et al., 2007) or queries and clicked snippets from query logs (Riezler et al., 2008). All in all, translation models have been shown to significantly improve the retrieval results over traditional baselines for document retrieval (Berger and Lafferty, 1999), question retrieval in Question & Answer archives (Jeon et al., 2005; Lee et al., 2008; Xue et al., 2008) and for sentence retrieval (Murdock and Croft, 2005). Many of the approaches previously described have used parallel data extracted from the retrieval corpus itself. The translation models obtained are therefore domain and collection-specific, which introduces a bias in the evaluation and makes it difficult to assess t"
P09-1082,W03-1605,0,0.02758,"exical chasm) often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon"
P09-1082,D08-1043,0,0.745039,"ger and Lafferty (1999) have formulated a further solution to the lexical gap problem consisting in integrating monolingual statistical translation models in the retrieval process. Monolingual translation models encode statistical word associations which are trained on parallel monolingual corpora. The major drawback of this approach lies in the limited availability of truly parallel monolingual corpora. In practice, training data for translation-based retrieval often consist in question-answer pairs, usually extracted from the evaluation corpus itself (Riezler et al., 2007; Xue et al., 2008; Lee et al., 2008). While collectionspecific translation models effectively encode statistical word associations for the target document collection, it also introduces a bias in the evaluation and makes it difficult to assess the quality of the translation model per se, independently from a specific task and document collection. In this paper, we propose new kinds of datasets for training domain-independent monolingual translation models. We use the definitions and glosses provided for the same term by different lexical semantic resources to automatically train the translation models. This approach has been ver"
P09-1082,C02-1161,0,0.0305723,"often observed between queries and documents or questions and answers is a pervasive problem both in Information Retrieval (IR) and Question Answering (QA). This problem arises from alternative ways of conveying the same information, due to synonymy or paraphrasing, and is especially severe for retrieval over shorter documents, such as sentence retrieval or question retrieval in Question & Answer archives. Several solutions to this problem have been proposed including query expansion (Riezler et al., 2007; Fang, 2008), query reformulation or paraphrasing (Hermjakob et al., 2002; Tomuro, 2003; Zukerman and Raskutti, 2002) 728 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 728–736, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Subsequent work in this area often used similar kinds of training data such as question-answer pairs from Yahoo! Answers (Lee et al., 2008) or from the Wondir site (Xue et al., 2008). Lee et al. (2008) tried to further improve translation models based on question-answer pairs by selecting the most important terms to build compact translation models. Other kinds of training data have also been proposed. Jeon et al. (2005) automatically c"
P09-1082,H05-1086,0,\N,Missing
S12-1068,C10-1152,1,0.84074,"utions performed by non-native English speakers, we tried to use linguistic resources that best fit this kind of data. In this way, we made the hypothesis that training our system on documents written by or written for nonnative English speakers would be useful. The use of the Simple English version from Wikipedia seems to be a good solution as it is targeted at people who do not have English as their mother tongue. Our hypothesis seems to be correct due to the results we obtained. Morevover, the Simple English Wikipedia has been used previously in work on automatic text simplification, e.g. (Zhu et al., 2010). 1 http://infolingu.univ-mlv.fr/ DonneesLinguistiques/Dictionnaires/ telechargement.html First, we produced a plain text version of the Simple English Wikipedia. We downloaded the dump dated February 27, 2012 and extracted the textual contents using the wikipedia2text tool.2 The final plaintext file contains approximately 10 million words. We extracted word n-grams (n ranging from 1 to 3) and their frequencies from this corpus thanks to the Text-NSP Perl module 3 and its count.pl program, which produces the list of n-grams of a document, with their frequencies. Table 1 gives the number of n-g"
S12-1068,S12-1046,0,\N,Missing
W08-0906,N03-1003,0,0.0446956,"estion paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of syn45 onymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first com"
W08-0906,P01-1008,0,0.0461068,"es. Our methods to identify question paraphrases are detailed in section 4. Finally, we present and analyse the experimental results obtained in section 5 and conclude in section 6. 2 Related Work The identification of question paraphrases in question and answer repositories is related to research focusing on sentence paraphrase identification (section 2.1) and query paraphrasing (section 2.2). The specific features of question paraphrasing have also already been investigated (section 2.3). 2.1 Sentence Paraphrase Identification Paraphrases are alternative ways to convey the same information (Barzilay and McKeown, 2001). Paraphrases can be found at different levels of linguistic structure: words, phrases and whole sentences. While word and phrasal paraphrases can be assimilated to the well-studied notion of syn45 onymy, sentence level paraphrasing is more difficult to grasp and cannot be equated with word-for-word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being com"
W08-0906,C04-1051,0,0.122559,"word or phrase-by-phrase substitution since it might entail changes in the structure of the sentence (Barzilay and Lee, 2003). In practice, sentence paraphrases are identified using various string and semantic similarity measures which aim at capturing the semantic equivalence of the sentences being compared. String similarity metrics, when applied to sentences, consist in comparing the words contained in the sentences. There exist many different string similarity measures: word overlap (Tomuro and Lytinen, 2004), longest common subsequence (Islam and Inkpen, 2007), Levenshtein edit distance (Dolan et al., 2004), word n-gram overlap (Barzilay and Lee, 2003) etc. Semantic similarity measures are obtained by first computing the semantic similarity of the words contained in the sentences being compared. Mihalcea et al. (2006) use both corpusbased and knowledge-based measures of the semantic similarity between words. Both string similarity and semantic similarity might be combined: for instance, Islam and Inkpen (2007) combine semantic similarity with longest common subsequence string similarity, while Li et al. (2006) make additional use of word order similarity. 2.2 Query Paraphrasing In Information Re"
W08-0906,P07-2032,1,0.593697,"Missing"
W11-2840,C08-1022,0,0.0257963,"Missing"
W11-2840,grouin-2008-certification,1,0.881677,"Missing"
W11-2840,P11-1093,0,0.0245815,"Missing"
W11-2840,W11-2838,0,\N,Missing
W11-2840,W10-4236,0,\N,Missing
W14-1206,W03-1602,0,0.0795097,"ilar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronominal anaphora. Third,"
W14-1206,N09-2045,0,0.0696264,"escribe a general typology of simplification derived from our corpora (Section 2.2). Then, we present the system based on the syntactic part of the typology, which operates in two steps: overgeneration of all possible simplified sentences (Section 2.3.1) and selection of the best subset of candidates using readability criteria (Section 2.3.2) and ILP. Finally, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikiped"
W14-1206,levy-andrew-2006-tregex,0,0.0172946,"ies global changes to the text. For instance, when a simple past is replaced by a present form, we must also adapt the verbs in the surrounding context in accordance with tense agreement. This requires to consider the whole text, or at least the paragraph that contains the modified verbal form, and be able to automatically model tense agreement. Otherwise, we may alter the coherence of the text and decrease its readability. This leaves us with 19 simplification rules.6 To apply them, the candidate structures for simplification first need to be detected using regular expressions, via Tregex 7 (Levy and Andrew, 2006) that allows the retrieval of elements and relationships in a parse tree. In a second step, syntactic trees in which a structure requires simplification are modified according a set of operations implemented through Tsurgeon. The operations to perform depend on the type of rules: 1. For the deletion cases, simply deleting all the elements involved is sufficient (via the delete operation in Tsurgeon). The elements affected by the deletion rules are adverbial clauses, clauses between brackets, some of the subordinate clauses, clauses between commas or introduced by words such as “comme” (as), “v"
W14-1206,E06-1021,0,0.0743117,"terms of language and content. It is available at the address http://fr.vikidia.org 48 WikiExtractor 3 was then applied to the articles to discard the wiki syntax and only keep the raw texts. This corpus comprises 13,638 texts (7,460 from Vikidia and only 6,178 from Wikipedia, since some Vikidia articles had no counterpart in Wikipedia). These articles were subsequently processed to identify parallel sentences (Wikipedia sentence with a simplified equivalent in Vikidia). The alignment has been made partly manually and partly automatically with the monolingual alignment algorithm described in Nelken and Shieber (2006), which relies on a cosine similarity between sentence vectors weighted with the tf-idf. This program outputs alignments between sentences, along with a confidence score. Among these files, twenty articles or excerpts from Wikipedia were selected along with their equivalent in Vikidia. This amounts to 72 sentences for the former and 80 sentences for the latter. The second corpus is composed of 16 narrative texts, and more specifically tales, by Perrault, Maupassant, and Daudet. We used tales since their simplified version was closer to the original than those of longer novels, which made the s"
W14-1206,seretan-2012-acquisition,0,0.0654496,"ia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. This is why, so far, there was no attempt to adapt this machine learning methodology to French. The only previous work on French, to our knowledge, is that of Seretan (2012), which analysed a corpus of newspapers to semi-automatically detect complex structures that has to be simplified. However, her system of rules has not been implemented and evaluated. 2 Methodology 2.1 Corpus Description We based our typology of simplification rules on the analysis of two corpora. More specifically, since our aim is to identify and classify the various strategies used to transform a complex sentence into a more simple one, the corpora had to include parallel sentences. The reason why we analysed two corpora is to determine whether different genres of texts lead to different si"
W14-1206,W13-1502,0,0.169277,"Missing"
W14-1206,C12-1023,0,0.108235,"Missing"
W14-1206,candito-etal-2010-statistical,0,0.0266653,"Missing"
W14-1206,E99-1042,0,0.167523,"us a size roughly similar to the Wikipedia-Vikidia corpus. The two corpora created are relevant for a manual analysis, as done in the next section, but they are too small for automatic processing. We plan to implement a method to align automatically the narrative texts in the near future and thus be able to collect a larger corpus. 2.2 levels of transformation: lexical, discursive and syntactic, which can be further divided into subcategories. It is worth mentioning that in previous work, simplification is commonly regarded as pertaining to two categories of phenomena: lexical and syntactic (Carroll et al., 1999; Inui et al., 2003; De Belder and Moens, 2010). Little attention has been paid to discourse in the area of automatic simplification (Siddharthan, 2006). The typology is summarized in Table 1. As regards the lexicon, the phenomena we observed involve four types of substitution. First, difficult terms can be replaced by a synonym or an hypernym perceived as simpler. Second, some anaphoric expressions, considered simpler or more explicit, are preferred to their counterparts in the original texts. For example, in our three tales, simplified nominal anaphora are regularly used instead of pronomina"
W14-1206,C96-2183,0,0.874017,"Missing"
W14-1206,Y09-1013,0,0.0159614,"f syntactic simplification for French sentences. The simplification is performed as a two-step process. First, for each sentence of the text, we generate the set of all possible simplifications (overgeneration step), and then, we select the best subset of simplified sentences using several criteria. 2.3.1 Generation of the Simplified Sentences The sentence overgeneration module is based on a set of rules (19 rules), which rely both on morphosyntactic features of words and on syntactic relationships within sentences. To obtain this information, the texts from our corpus are analyzed by MELT 4 (Denis and Sagot, 2009) and Bonsai 5 (Candito et al., 2010) during a preprocessing phase. As a result, texts are represented as syntax trees that include the information necessary to apply our simplification rules. After preprocessing, the set of simplification rules is applied recursively, one sentence at a time, until there is no further structure to simplify. All simplified sentences produced by a given rule are saved and gathered in a set of variants. The rules for syntactic simplification included in our program are of three kinds: deletion rules (12 rules), modification rules (3 rules) and splitting rules (4 r"
W14-1206,D12-1043,1,0.897268,"Missing"
W14-1206,W09-1802,0,0.0352869,"e more than one simplified variant for a given sentence. In this case, the next step consists in selecting the most suitable variant to substitute the original one. The selection process is described in the next section. 2.3.2 Selection of the Best Simplifications Given a set of candidate simplified sentences for a text, our goal is to select the best subset of simplified sentences, that is to say the subset that maximizes some measure of readability. More precisely, text readability is measured through different criteria, which are optimized with an Integer Linear Programming (ILP) approach (Gillick and Favre, 2009). These criteria are rather simple in 6 These 19 rules are available at http://cental.fltr.ucl.ac.be/team/ lbrouwers/rules.pdf 7 http://nlp.stanford.edu/software/tregex.shtml 8 This software is available at the address http://sarrazip.com/dev/ verbiste.html under GNU general public license and was developed by Pierre Sarrazin. 51 this approach. They are used to ensure that not only the syntactic difficulty, but also the lexical complexity decrease, since syntactic transformations may cause lexical or discursive alterations in the text. We considered four criteria to select the most suitable se"
W14-1206,D11-1038,0,0.639109,"the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which further complicates machine learning. Thi"
W14-1206,C10-1152,1,0.964006,"lly, we evaluate the quality of the syntactically simplified sentences as regards grammaticality, before performing some error analysis (Section 3). of automatic question generation yields better results. Similarly, Lin and Wilbur (2007) and Jonnalagadda et al. (2009) optimized information extraction from biomedical texts using ATS as a preprocessing step. In these studies, the simplifications carried out are generally based on a set of manually defined transformation rules. However, ATS may also be solved with methods from machine translation and machine learning. This lead some researchers (Zhu et al., 2010; Specia, 2010; Woodsend and Lapata, 2011) to train statistical models from comparable corpora of original and simplified texts. The data used in these studies are often based on the English Wikipedia (for original texts) and the Simple English Wikipedia, a simplified version for children and non-native speakers that currently comprises more than 100,000 articles. Similar resources exist for French, such as Vikidia and Wikimini, but texts are far less numerous in these as in their English counterpart. Moreover, the original and simplified versions of an article are not strictly parallel, which"
