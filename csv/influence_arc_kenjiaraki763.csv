1999.mtsummit-1.75,P97-1063,0,0.103789,"Missing"
1999.mtsummit-1.75,Y99-1030,1,0.57846,"source sentence. 2. If found, from that position, start a forward and backward search of matches. The search starts with exact matches and continues with POS tag matches when a POS tag match or a non-contiguous match is encountered. MT Summit VII Sept. To select the best matching sentences, the following similarity score is used. SC is the similarity score. NE the number of exact matches. NP the number of POS tag matches and Di the distance separating a token from the common segment, measured in number of tokens. This equation of similarity score has been proposed and discussed in details in [13], with a description of the preliminary experiments for fixing the value of a. According to it, the suitable value of a is 10. This value makes heavier the presence of exact matches, which are located within or around the common segment, compared to the POS tag matches. In [13], since non-contiguous matches were not considered, Di was ignored and the number of POS tag matches was only taken into account. However, in the present method, search for matches continues until it reaches the head or the end of the sentence. Therefore, Di is introduced to make the difference between matches located ne"
1999.mtsummit-1.75,J93-2003,0,\N,Missing
1999.mtsummit-1.75,J96-4008,0,\N,Missing
2020.gebnlp-1.5,2020.acl-main.485,0,0.0262534,"Missing"
2020.gebnlp-1.5,P19-1166,0,0.205622,"ams. 4.3 Gender bias mitigation We target bias mitigation for gender bias in Japanese embeddings by using Hard Debias (Bolukbasi et al., 2016) and Double-Hard Debias (Wang et al., 2020). 4.3.1 Mitigating methods Hard Debias Hard Debias is a method for bias mitigation by removing gender direction from gender neutral words. Gender direction is defined in advance as the first principal component of gender definition word pairs. Original Hard Debias (Bolukbasi et al., 2016) normalizes a word vector to size 1, but we do not so, because its length can contain important information as pointed out by Ethayarajh et al. (2019). Double-Hard Debias Double-Hard Debias follows Mu and Viswanath (2018), before doing Hard Debias, first centralizing the entire embedding and then removing the dominant principal component of the gender bias. Hard Debias was improved by performing these steps. 4.3.2 Gender Definition and Specific Words Bolukbasi et al. (2016) defines gender specific words in advance, then uses them in the training data and extends the gender specific words with SVM. However, it has been pointed out that searching for gender specific words using embeddings of the bias mitigation targets poses the problem of no"
2020.gebnlp-1.5,W19-3621,0,0.242598,"s well as religion and race. 2.1.2 Approaches to bias mitigation Bolukbasi et al. (2016) confirmed the existence of the gender bias in English word embeddings, and proposed a method called Hard Debias to mitigate the gender bias. Hard Debias uses words that should be neutral to gender, such as “doctor” and “programmer”, and reduces the bias by subtracting the vector components of gender directions from gender neutral words. Gender directions are defined by the first principal component of a word vector of each word consisting of a gender definition word pairs, such as “she” and “he”. However, Gonen and Goldberg (2019) proved experimentally that Hard Debias could not sufficiently remove gender bias and that it can be recovered from embeddings after mitigation. In the work of Mu and Viswanath (2018), the most statistically dominant principal components are encoding the frequency of words. Their method improves performance of embedding by subtracting the common mean vector from each word vector and removing the dominant principal components. Wang et al. (2020) proposed Double-Hard Debias which was inspired by work of Mu and Viswanath (2018). They improved Hard Debias by deciding the dominant principal compone"
2020.gebnlp-1.5,D15-1276,0,0.0622282,"Missing"
2020.gebnlp-1.5,W19-6104,0,0.232531,"” (Blodgett et al., 2020, p.5456). Stereotyping happens in natural language processing tasks when an unfair association of words represents a particular social group with other concepts (not included in its definition), like an analogy of “man is to programmer as woman is to homemaker”. If an AI agent has such stereotypes, they can appear in its output as reported in works on dialogue systems (Liu et al., 2019), possibly harming users. There are several works on stereotypes in word embeddings for English (Bolukbasi et al., 2016; Zhao et al., 2018b; Wang et al., 2020) and some other languages (Sahlgren and Olsson, 2019; Pujari et al., 2019), but to the authors’ best knowledge, research regarding Japanese word embeddings does not exist. In this paper, we analyze the representational bias in Japanese word embeddings, and attempt to mitigate gender bias by using existing methods designed for English. We also show that those methods are difficult to generalize to Japanese. 2 2.1 Related work Bias in word embeddings and its mitigation for English This section describes bias analysis and gender bias mitigation for English word embeddings. 2.1.1 Bias analysis Caliskan et al. (2017) proposed the Word Embedding Asso"
2020.gebnlp-1.5,P19-1159,0,0.0140752,"and anti-stereotypical conditions. For example in the coreference resolution task, it is difficult to correctly link “physician:she” and “secretary:he” for systems which use gender-biased word embeddings, because “physician:he” and “secretary:she” are strongly related more than “physician:she” and “secretary:he” in the word embeddings (Zhao et al., 2018a). Therefore, in recent years, research has been conducted to mitigate the bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Wang et al., 2020). However, to the authors’ best knowledge, most of them have focused on English (Sun et al., 2019; Blodgett et al., 2020), and no study has addressed word embeddings of languages other than Indo-European languages about bias analysis and mitigation. We hypothesize that it is not obvious that the method developed for English can be easily adapted to other languages for two following reasons. First is due to various grammatical features which do not exist in English. Embeddings can have different characteristics depending on language, for example Spanish words have gender which leads to the grammatical gender bias (Zhou et al., 2019). There is a substantial risk that we cannot adapt the bia"
2020.gebnlp-1.5,2020.acl-main.484,0,0.295136,"as observed by Bolukbasi et al. (2016). Such biases cause differences in F1 scores between the pro- and anti-stereotypical conditions. For example in the coreference resolution task, it is difficult to correctly link “physician:she” and “secretary:he” for systems which use gender-biased word embeddings, because “physician:he” and “secretary:she” are strongly related more than “physician:she” and “secretary:he” in the word embeddings (Zhao et al., 2018a). Therefore, in recent years, research has been conducted to mitigate the bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Wang et al., 2020). However, to the authors’ best knowledge, most of them have focused on English (Sun et al., 2019; Blodgett et al., 2020), and no study has addressed word embeddings of languages other than Indo-European languages about bias analysis and mitigation. We hypothesize that it is not obvious that the method developed for English can be easily adapted to other languages for two following reasons. First is due to various grammatical features which do not exist in English. Embeddings can have different characteristics depending on language, for example Spanish words have gender which leads to the gram"
2020.gebnlp-1.5,N18-2003,0,0.158327,"2017). For example, “programmer” and “homemaker” should be gender neutral by definition, but the analogy of “man is to programmer as woman is to homemaker” holds as observed by Bolukbasi et al. (2016). Such biases cause differences in F1 scores between the pro- and anti-stereotypical conditions. For example in the coreference resolution task, it is difficult to correctly link “physician:she” and “secretary:he” for systems which use gender-biased word embeddings, because “physician:he” and “secretary:she” are strongly related more than “physician:she” and “secretary:he” in the word embeddings (Zhao et al., 2018a). Therefore, in recent years, research has been conducted to mitigate the bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Wang et al., 2020). However, to the authors’ best knowledge, most of them have focused on English (Sun et al., 2019; Blodgett et al., 2020), and no study has addressed word embeddings of languages other than Indo-European languages about bias analysis and mitigation. We hypothesize that it is not obvious that the method developed for English can be easily adapted to other languages for two following reasons. First is due to various grammatical feature"
2020.gebnlp-1.5,D18-1521,0,0.224734,"2017). For example, “programmer” and “homemaker” should be gender neutral by definition, but the analogy of “man is to programmer as woman is to homemaker” holds as observed by Bolukbasi et al. (2016). Such biases cause differences in F1 scores between the pro- and anti-stereotypical conditions. For example in the coreference resolution task, it is difficult to correctly link “physician:she” and “secretary:he” for systems which use gender-biased word embeddings, because “physician:he” and “secretary:she” are strongly related more than “physician:she” and “secretary:he” in the word embeddings (Zhao et al., 2018a). Therefore, in recent years, research has been conducted to mitigate the bias in word embeddings (Bolukbasi et al., 2016; Zhao et al., 2018b; Wang et al., 2020). However, to the authors’ best knowledge, most of them have focused on English (Sun et al., 2019; Blodgett et al., 2020), and no study has addressed word embeddings of languages other than Indo-European languages about bias analysis and mitigation. We hypothesize that it is not obvious that the method developed for English can be easily adapted to other languages for two following reasons. First is due to various grammatical feature"
2020.gebnlp-1.5,2020.acl-main.260,0,0.0910242,"luding the neighborhood metric (Gonen and Goldberg, 2019), showed improved results. All of the above-mentioned research examples work on English language. Next, we present studies on the bias inherent in non-English embeddings. 2.2 Word embedding biases in languages other than English There are two major directions of research on non-English word embedding bias. The first is a bias study of multilingual embeddings, which compares what biases exist in embeddings available in both English and other languages, e.g. Spanish and French, and how they differ depending on language (Zhou et al., 2019; Zhao et al., 2020). The second direction is to address biases in monolingual embeddings of languages other than English (Zhou et al., 2019; Sahlgren and Olsson, 2019; Pujari et al., 2019; Raijmakers, 2020). For example, the gender bias has been found and mitigated in Swedish (Sahlgren and Olsson, 2019) and Hindi (Pujari et al., 2019). Both used Hard Debias for gender bias mitigation – Sahlgren and Olsson (2019) could not mitigate the gender bias but Pujari et al. (2019) were able to achieve that goal. However, (Sahlgren and Olsson, 2019) analyzed their results only partially. The problem in the Pujari et al. (2"
2020.gebnlp-1.5,D19-1531,0,0.0387103,"Missing"
A97-2006,C96-2178,1,0.572182,"e translation and a number of problems has been recognized. Rule-based machine translation (Hutchins and Somers, 1992) could not deal adequately with various linguistic phenomena due to the use of limited rules. To resolve this problem, Example-based machine translation (Sato and Nagao, 1990) has recently been proposed. However, this method requires many translation examples to achieve a practical and high-quality translation. Echizen-ya and others previously proposed a method of Machine Translation using Inductive Learning with Genetic Algorithms (GA-ILMT), and this method has been evaluated(Echizen-ya et al., 1996). By applying genetic algorithms, we consider that our proposed method can effectively solve problems that Example-based machine translation would require many translation examples. However, the 3 Improvement in Selection Process In the previous method of selection process described in Section 2, translation rules are evaluated only when they are used in the translation process. These translation rules are part of all the translation rules in the dictionary. Therefore, many erroneous 11 Source Sentence [T r a n s l a t i 7 Pr°cess ~.,,.. ~ Translation Result - - - ] ~ ( ~ (P roofreadin~) [ I ."
A97-2006,C90-3044,0,\N,Missing
C02-1158,C90-3044,0,0.00849175,"-Gakuen University 4-Chome, Asahi-machi, Toyohira-ku Sapporo, 060-8790 Japan tochinai@econ.hokkai-s-u.ac.jp MT have been proposed to overcome the difﬁculties of Rule-Based MT. These approaches correspond to Corpus-Based approach. CorpusBased approach uses translation examples that keep including linguistic knowledge. This means that the system can improve the quality of its translation only by adding new translation examples. However, in Statistical MT(Brown et al., 1990), large amounts of translation examples are required in order to obtain high-quality translation. Moreover, ExampleBased MT(Sato and Nagao, 1990; Watanabe and Takeda, 1998; Brown, 2001; Carl, 2001) which relies on various knowledge resources results in the same diﬃculties as Rule-Based MT. Therefore, Example-Based MT, which automatically acquires the translation rules from only bilingual text corpora, is very eﬀective. However, existing Example-Based MT systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules. In Example-Based MT based on analogical reasoning(Malavazos, 2000; Guvenir, 1998), the diﬀerent parts are replaced by variables to generalize translation examples"
C02-1158,J90-2002,0,0.11196,"efore, Rule-Based MT is time-consuming and expensive. Statistical MT and Example-Based Koji Tochinai Division of Business Administration Hokkai-Gakuen University 4-Chome, Asahi-machi, Toyohira-ku Sapporo, 060-8790 Japan tochinai@econ.hokkai-s-u.ac.jp MT have been proposed to overcome the difﬁculties of Rule-Based MT. These approaches correspond to Corpus-Based approach. CorpusBased approach uses translation examples that keep including linguistic knowledge. This means that the system can improve the quality of its translation only by adding new translation examples. However, in Statistical MT(Brown et al., 1990), large amounts of translation examples are required in order to obtain high-quality translation. Moreover, ExampleBased MT(Sato and Nagao, 1990; Watanabe and Takeda, 1998; Brown, 2001; Carl, 2001) which relies on various knowledge resources results in the same diﬃculties as Rule-Based MT. Therefore, Example-Based MT, which automatically acquires the translation rules from only bilingual text corpora, is very eﬀective. However, existing Example-Based MT systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules. In Example-Based M"
C02-1158,2001.mtsummit-ebmt.1,0,0.0283998,"-ku Sapporo, 060-8790 Japan tochinai@econ.hokkai-s-u.ac.jp MT have been proposed to overcome the difﬁculties of Rule-Based MT. These approaches correspond to Corpus-Based approach. CorpusBased approach uses translation examples that keep including linguistic knowledge. This means that the system can improve the quality of its translation only by adding new translation examples. However, in Statistical MT(Brown et al., 1990), large amounts of translation examples are required in order to obtain high-quality translation. Moreover, ExampleBased MT(Sato and Nagao, 1990; Watanabe and Takeda, 1998; Brown, 2001; Carl, 2001) which relies on various knowledge resources results in the same diﬃculties as Rule-Based MT. Therefore, Example-Based MT, which automatically acquires the translation rules from only bilingual text corpora, is very eﬀective. However, existing Example-Based MT systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules. In Example-Based MT based on analogical reasoning(Malavazos, 2000; Guvenir, 1998), the diﬀerent parts are replaced by variables to generalize translation examples as shown in (1) of Figure 1. However, t"
C02-1158,2001.mtsummit-ebmt.2,0,0.0169764,"060-8790 Japan tochinai@econ.hokkai-s-u.ac.jp MT have been proposed to overcome the difﬁculties of Rule-Based MT. These approaches correspond to Corpus-Based approach. CorpusBased approach uses translation examples that keep including linguistic knowledge. This means that the system can improve the quality of its translation only by adding new translation examples. However, in Statistical MT(Brown et al., 1990), large amounts of translation examples are required in order to obtain high-quality translation. Moreover, ExampleBased MT(Sato and Nagao, 1990; Watanabe and Takeda, 1998; Brown, 2001; Carl, 2001) which relies on various knowledge resources results in the same diﬃculties as Rule-Based MT. Therefore, Example-Based MT, which automatically acquires the translation rules from only bilingual text corpora, is very eﬀective. However, existing Example-Based MT systems using the learning algorithms require large amounts of translation pairs to acquire high-quality translation rules. In Example-Based MT based on analogical reasoning(Malavazos, 2000; Guvenir, 1998), the diﬀerent parts are replaced by variables to generalize translation examples as shown in (1) of Figure 1. However, the number of"
C02-1158,C00-1075,0,0.0655683,"Missing"
C02-1158,C96-2178,1,0.801176,"em generates translation results using acquired translation rules. First, the system selects the sentence translation rules that can be applied to the SL sentence. Second, the system generates the translation results by replacing the variables in the sentence translation rules with the part translation rules. 4.2 Feedback process In the feedback process, the system evaluates the translation rules used. First, the system evaluates the translation rules without variables by using the results of combinations between the translation rules with variables and the translation rules without variables(Echizen-ya et al., 1996). Next, the system evaluates translation rules with variables by using the processes of combinations between the translations rules with variables and the translation rules without variables(Echizen-ya et al., 2000). As a result, the system increases the correct translation frequencies, or the erroneous translation frequencies, of the translation rules by using these evaluation methods for the translation rules. 4.3 Learning process 4.3.1 GA-ILMT In this paper, by using the process of acquisition of translation rules in GA-ILMT, the system acquires both sentence and part translation rules. The"
C02-1158,2000.bcs-1.5,1,0.636202,"s by replacing the variables in the sentence translation rules with the part translation rules. 4.2 Feedback process In the feedback process, the system evaluates the translation rules used. First, the system evaluates the translation rules without variables by using the results of combinations between the translation rules with variables and the translation rules without variables(Echizen-ya et al., 1996). Next, the system evaluates translation rules with variables by using the processes of combinations between the translations rules with variables and the translation rules without variables(Echizen-ya et al., 2000). As a result, the system increases the correct translation frequencies, or the erroneous translation frequencies, of the translation rules by using these evaluation methods for the translation rules. 4.3 Learning process 4.3.1 GA-ILMT In this paper, by using the process of acquisition of translation rules in GA-ILMT, the system acquires both sentence and part translation rules. These rules are then used as starting points when the system performs RCL. 4.3.2 Recursive Chain-link-type Learning(RCL) In this section, we describe the process of acquisition of translation rules using RCL. The detai"
C02-1158,2001.mtsummit-ebmt.3,0,\N,Missing
C02-1158,P98-2223,0,\N,Missing
C02-1158,C98-2218,0,\N,Missing
C96-2178,C90-3044,0,\N,Missing
clark-araki-2012-two,N10-1020,0,\N,Missing
I05-2018,P03-1059,0,0.421362,"Missing"
I05-2018,C02-1052,0,0.246664,"Missing"
I05-2018,J03-3005,0,0.0912711,"Missing"
I05-2018,N04-1016,0,0.0431534,"Missing"
I05-2018,W02-1030,0,\N,Missing
I13-1066,P02-1053,0,0.00529026,"es on the Web, our method greatly reduces the cost of manual construction of training data. Furthermore, in calculating the harmfulness score we apply dependency relations between phrases. Therefore there is no need to check all words proceeding and succeeding the queried words, which greatly reduces processing time. 3 Table 2: Types of phrases applied in the proposed method with examples. Phrase noun-noun noun-verb noun-adjective Proposed method In this section we present an overview of the method for maximization of category relevance. In the proposed method we extend the method proposed by Turney (2002) to calculate the relevance of seed words with entries from the bulletin board pages. Moreover, we apply multiple categories of harmful words and calculate the degree of association separately for each category. Finally, as the harmfulness score (or polarity of “harmfulness”) we choose the maximum value achieved by all categories. The method consists of three steps. (1) Phrase extraction, (2) Categorization and harmful word detection together with harmfulness polarity determination, (3) Relevance maximization. Each of the steps is explained in detail in the following paragraphs. Example サル顔 (s"
N07-2048,P02-1053,0,0.00391549,"Missing"
N07-2048,H05-1043,0,0.0392097,"Missing"
N07-2048,H05-2017,0,\N,Missing
N07-4010,P02-1053,0,0.00479308,"ly, i.e. “Opinion Mining”, has seen increasing attention in recent years. This paper presents a simple opinion mining system (OMS-J) for analyzing Japanese Weblog reviews automatically. The novelty of OMS-J is twofold: First, it provides a GUI using intuitive visual mining graphs aimed at inexperienced users who want to check opinions on the Weblog before purchasing something. These graphs can help the user to make a quick decision on which product is suitable. Secondly, this system combines a supervised and an unsupervised approach to perform opinion mining. In related work (Chaovalit, 2005; Turney, 2002), both supervised and unsupervised approaches have been shown to have their pros and 4. Mining Graphs GUI User Figure 1: System Flow 2 Proposed System 2.1 Information Search The first step is information search. We used the Google search engine1 to get all the information on one product category or one specific product in the Japanese weblog on the Internet. The search keyword is the product category name or the product name. The URL range of the search is restricted by the URL type (e.g. blog, bbs, review). 2.2 Weblog Content Extraction The Content Extraction step first analyzes the Weblog co"
N09-2017,P95-1055,0,\N,Missing
N19-1186,W05-0909,0,0.493628,"between two word embeddings. The distance is obtained using cosine similarity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et a"
N19-1186,W17-4755,0,0.0531052,"rd positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 series do not specifical"
N19-1186,W16-2302,0,0.0500804,"Missing"
N19-1186,P15-2025,0,0.260814,"As described in that paper, Maximum Alignment Similarity (MAS) was found to have higher correlation with human evaluation than BLEU for European-to-English, which has similar grammar structures. For Japanese-toEnglish, which has different grammar structures, Average Alignment Similarity (AAS) showed better correlation with human evaluation than other metrics. However, neither MAS nor AAS uses word position information. Therefore, neither can sufficiently accommodate word order differences. Actually, WE WPI uses not only the word alignment but also word position information. One system, DREEM (Chen and Guo, 2015), learns distributed word representations from a neural network model and from distributed sentence representations computed with a recursive autoencoder. Moreover, it uses a penalty based on translation and reference lengths. By contrast, the WE WPI system specifically examines the difference between the word positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requ"
N19-1186,P05-1033,0,0.0661303,"position information are used to address wordorder differences. We designate this metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). A meta-evaluation using WMT16 metrics shared task set indicates that our WE WPI achieves the highest correlation with human judgment among several representative metrics. 1 Introduction Recent advances in neural machine translation (NMT) (Sutskever et al., 2014; Luong et al., 2015) are remarkable. Results based on human evaluation have demonstrated that NMT outperforms statistical machine translations significantly (Chiang, 2005; Tufis¸ and Ceaus¸u, 2009). The NMT achieved especially high performance in terms of fluency. However, it tends to generate more omission errors than statistical machine translations generate. Unfortunately, it is difficult for automatic evaluation metrics to evaluate outputs with omission errors because those errors are not included as non-match words between the translation and reference. For such cases, the word embedding-based automatic MT evaluation metric, which is based on word position information, is effective. Various automatic evaluation metrics have been proposed for machine trans"
N19-1186,2007.mtsummit-papers.21,1,0.429922,"nce is obtained using cosine similarity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et al. (2017) also proposed a word-alignment-"
N19-1186,L18-1550,0,0.032814,"different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE. Here, IMPACT and RIBES, which are surface-based metrics, are effective for language pairs with greatly different word order, such as English and Japanese. In addition, WE is an automatic MT evaluation metric that does not perform word alignment. It uses only dij = 1.0 − cos sim(ti , rj ) as the dij of Eq. (12) in the WE WPI. In both WE and WE WPI, the word vectors for seven languages (i.e., English, Czech, German, Finnish, Romanian, Russian, and Turkish) were obtained using fastText (Grave et al., 2018). 4.2 Experiment Results and Discussion Tables 4 and 5 respectively present the correlation coefficient of to-English and out-of-English at the system level. Tables 6 and 7 respectively present the correlation coefficients of to-English and outof-English at the segment level. In Tables 4–7, RR represents the correlation based on the relative ranking by human judgment to 5 translations at a time. The bold typeface shows the highest correlation coefficient among all correlation coefficients of metrics. Moreover, the coefficients of MEANT 2.0 described in (Lo, 2017) are added to Tables 4– 6. Here"
N19-1186,D10-1092,0,0.0338529,"ity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et al. (2017) also proposed a word-alignment-based automatic evaluation met"
N19-1186,D15-1166,0,0.0232496,"Missing"
N19-1186,P02-1040,0,0.105581,"high performance in terms of fluency. However, it tends to generate more omission errors than statistical machine translations generate. Unfortunately, it is difficult for automatic evaluation metrics to evaluate outputs with omission errors because those errors are not included as non-match words between the translation and reference. For such cases, the word embedding-based automatic MT evaluation metric, which is based on word position information, is effective. Various automatic evaluation metrics have been proposed for machine translation, but none is sufficient for NMT. Actually, BLEU (Papineni et al., 2002) is the representative metric based on ngram matching. Unfortunately, because it is a surface-level metric, it is difficult to address word meaning during evaluation for MT outputs. The word-embedding-based distance measure for document (Kusner et al., 2016) and the word-alignment-based automatic evaluation metric using word embedding (Matsuo et al., 2017) are effective to address word meanings. Nevertheless, they can only ineffectively accommodate word order differences between the translation and reference. Given those circumstances, a new metric with word embedding-based automatic MT evalua"
N19-1186,P16-1159,0,0.0309751,"Missing"
N19-1186,D09-1117,0,0.0388943,"Missing"
N19-1186,W17-4767,0,0.286166,"een the word positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 ser"
N19-1186,P11-1023,0,0.0223127,"ng word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 series do not specifically examine the position of each word in the translation and reference. Results show that it is difficult to deal sufficiently with language pairs for which the grammar differs. In WE WPI, the evaluation score is calculated using the rela"
P06-4002,J03-3002,0,\N,Missing
P06-4002,J95-4004,0,\N,Missing
P06-4002,P06-4018,0,\N,Missing
P06-4002,I05-2013,0,\N,Missing
P06-4002,2004.jeptalnrecital-recital.7,0,\N,Missing
P10-1012,W05-0909,0,0.211947,"Missing"
P10-1012,2003.mtsummit-papers.9,0,0.0662635,"Missing"
P10-1012,P02-1040,0,0.103516,"valuation has become increasingly important as various machine translation systems have developed. The scores of some automatic evaluation methods can obtain high correlation with human judgment in document-level automatic evaluation(Coughlin, 2007). However, sentence-level automatic evaluation is insuﬃcient. A great gap exists between language processing of automatic evaluation and the processing by humans. Therefore, in recent years, various automatic evaluation methods particularly addressing sentence-level automatic evaluations have been proposed. Methods based on word strings (e.g., BLEU(Papineni et al., 2002), NIST(NIST, 2002), METEOR(Banerjee and Lavie., 2005), ROUGE-L(Lin and Och, 2004), 2 Automatic Evaluation Method using Noun-Phrase Chunking The system based on our method has four processes. First, the system determines the corre108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 108–117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics part”, “the amount” and “crowning drop” are obtained in the reference by chunking. Next, the system determines the corresponding noun phrases from these noun phrases between the MT"
P10-1012,2007.mtsummit-papers.21,1,0.755741,"Missing"
P10-1012,N03-1028,0,0.0161247,"818 METEOR 0.3881 0.4947 0.3127 0.2987 0.4162 0.3416 WER 0.6656 0.6570 0.5740 0.7491 0.5301 0.6031 Our method II 0.7676 0.7217 0.6343 0.7917 0.5474 0.6632 BLEU with our method 0.6395 0.6696 0.5139 0.6611 0.5079 0.5698 12 machine translation systems in respective automatic evaluation methods, and “All” are the correlation coeﬃcients using the scores of 1,200 output sentences obtained using the 12 machine translation systems. ues of the parameter are determined using English sentences from Reuters articles(Utiyama and Isahara, 2003). Moreover, we obtained the noun phrases using a shallow parser(Sha and Pereira, 2003) as the chunking tool. We revised some erroneous results that were obtained using the chunking tool. 3.2 No. 7 0.7682 0.7249 0.7079 0.5340 0.3628 0.5906 0.3937 0.6902 0.7698 0.6459 All 0.6846 0.6574 0.6529 0.4722 0.3326 0.5669 0.2958 0.5205 0.6774 0.5790 3.3 Discussion In Tables 2–5, the “Avg.” score of our method is shown to be higher than those of other methods. Especially in terms of the sentence-level adequacy shown in Tables 2 and 4, “Avg.” of our method is about 0.03 higher than that of IMPACT. Moreover, in system No. 8 and “All” of Tables 2 and 4, the diﬀerences between correlation coeﬃ"
P10-1012,C92-2067,0,0.549814,"using chunking. Secondly, the system calculates word-level scores based on the correct matched words using the determined correspondences of noun phrases. Next, the system calculates phrase-level scores based on the noun-phrase order of appearance. The system calculates the ﬁnal scores combining word-level scores and phrase-level scores. 2.1 Correspondence of Noun Phrases by Chunking The system obtains the noun phrases from each sentence by chunking. It then determines corresponding noun phrases between MT outputs and references calculating the similarity for two noun phrases by the PER score(Su et al., 1992). In that case, PER scores of two kinds are calculated. One is the ratio of the number of match words between an MT output and reference for the number of all words of the MT output. The other is the ratio of the number of match words between the MT output and reference for the number of all words of the reference. The similarity is obtained as an F -measure between two PER scores. The high score represents that the similarity between two noun phrases is high. Figure 1 presents an example of the determination of the corresponding noun phrases. (1) Use of noun phrase chunking MT output : in gen"
P10-1012,2007.mtsummit-wpt.4,0,0.0221051,"quacy and ﬂuency on a scale of 1–5. We used the median value in the evaluation results of three human judges as the ﬁnal scores of 1–5. We calculated Pearson’s correlation eﬃcient and Spearman’s rank correlation eﬃcient between the scores obtained using our method and the scores by human judgments in terms of sentence-level adequacy and ﬂuency. Additionally, we calculated the correlations between the scores using seven other methods and the scores by human judgments to compare our method with other automatic evaluation methods. The other seven methods were IMPACT, ROUGE-L, BLEU1 , NIST, NMGWN(Ehara, 2007; Echizen-ya et al., 2009), METEOR2 , and WER(Leusch et al., 2003). Using our method, 0.1 was used as the value of the parameter α in Eqs. (3)-(10) and 1.1 was used as the value of the parameter β in Eqs. (1)–(10). Moreover, 0.3 was used as the value of the parameter δ in Eq. (13). These valIn Eqs. (9) and (10), cnpp denotes the common noun phrase parts; mcnp and ncnp respectively signify the quantities of common noun phrases in the reference and MT output. Moreover, mno cnp and nno cnp are the quantities of noun phrases except the common noun phrases in the reference and MT output. The values"
P10-1012,P03-1010,0,0.0380106,"ST 0.4218 0.4092 0.1721 0.3521 0.4769 0.3493 NMG-WN 0.6658 0.6068 0.6116 0.6770 0.5740 0.5818 METEOR 0.3881 0.4947 0.3127 0.2987 0.4162 0.3416 WER 0.6656 0.6570 0.5740 0.7491 0.5301 0.6031 Our method II 0.7676 0.7217 0.6343 0.7917 0.5474 0.6632 BLEU with our method 0.6395 0.6696 0.5139 0.6611 0.5079 0.5698 12 machine translation systems in respective automatic evaluation methods, and “All” are the correlation coeﬃcients using the scores of 1,200 output sentences obtained using the 12 machine translation systems. ues of the parameter are determined using English sentences from Reuters articles(Utiyama and Isahara, 2003). Moreover, we obtained the noun phrases using a shallow parser(Sha and Pereira, 2003) as the chunking tool. We revised some erroneous results that were obtained using the chunking tool. 3.2 No. 7 0.7682 0.7249 0.7079 0.5340 0.3628 0.5906 0.3937 0.6902 0.7698 0.6459 All 0.6846 0.6574 0.6529 0.4722 0.3326 0.5669 0.2958 0.5205 0.6774 0.5790 3.3 Discussion In Tables 2–5, the “Avg.” score of our method is shown to be higher than those of other methods. Especially in terms of the sentence-level adequacy shown in Tables 2 and 4, “Avg.” of our method is about 0.03 higher than that of IMPACT. Moreover"
P10-1012,2003.mtsummit-papers.32,0,0.173069,"lue in the evaluation results of three human judges as the ﬁnal scores of 1–5. We calculated Pearson’s correlation eﬃcient and Spearman’s rank correlation eﬃcient between the scores obtained using our method and the scores by human judgments in terms of sentence-level adequacy and ﬂuency. Additionally, we calculated the correlations between the scores using seven other methods and the scores by human judgments to compare our method with other automatic evaluation methods. The other seven methods were IMPACT, ROUGE-L, BLEU1 , NIST, NMGWN(Ehara, 2007; Echizen-ya et al., 2009), METEOR2 , and WER(Leusch et al., 2003). Using our method, 0.1 was used as the value of the parameter α in Eqs. (3)-(10) and 1.1 was used as the value of the parameter β in Eqs. (1)–(10). Moreover, 0.3 was used as the value of the parameter δ in Eq. (13). These valIn Eqs. (9) and (10), cnpp denotes the common noun phrase parts; mcnp and ncnp respectively signify the quantities of common noun phrases in the reference and MT output. Moreover, mno cnp and nno cnp are the quantities of noun phrases except the common noun phrases in the reference and MT output. The values of mno cnp and nno cnp are processed as 1 when no non-correspondi"
P10-1012,P04-1077,0,0.0280422,"ve developed. The scores of some automatic evaluation methods can obtain high correlation with human judgment in document-level automatic evaluation(Coughlin, 2007). However, sentence-level automatic evaluation is insuﬃcient. A great gap exists between language processing of automatic evaluation and the processing by humans. Therefore, in recent years, various automatic evaluation methods particularly addressing sentence-level automatic evaluations have been proposed. Methods based on word strings (e.g., BLEU(Papineni et al., 2002), NIST(NIST, 2002), METEOR(Banerjee and Lavie., 2005), ROUGE-L(Lin and Och, 2004), 2 Automatic Evaluation Method using Noun-Phrase Chunking The system based on our method has four processes. First, the system determines the corre108 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 108–117, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics part”, “the amount” and “crowning drop” are obtained in the reference by chunking. Next, the system determines the corresponding noun phrases from these noun phrases between the MT output and reference. The score between “the end” and “the end part” is the high"
P10-1012,2007.tmi-papers.15,0,0.0356755,"Missing"
P10-1012,P07-1044,0,0.0489386,"Missing"
R13-1030,W10-1754,0,0.01556,"rics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is calculated using Eq. (3): Introduction Various automatic evaluation metrics for machine translation have been proposed through the metrics task on the Workshop on statistical Machine Translation (WMT). One can identify three kinds of automatic evaluation metrics (C. Liu et al., 2010): the heavyweight linguistic approach, which corresponds to RTE (S. Pad´o et al., 2009) and ULC (J. Gim´enez and L. M´ arquez, 2007); the lightweight linguistic approach, which corresponds to METEOR precision = f-measure = 2 × precision × recall precision + recall (3) For example, in the reference “doctor cured a patient” and candidate “doctor treated a patient”, the precision and the recall are respectively 0.75 (= 34 ). Therefore, the f-measure is 0.75 (= 2×0.75×0.75 0.75+0.75 ), even though there is only one non-matching word. This is because the denominator is so small, since the sentences"
R13-1030,W09-0404,0,0.0382904,"Missing"
R13-1030,W07-0734,0,0.0998113,"Missing"
R13-1030,P08-1007,0,0.0438942,"Missing"
R13-1030,P02-1040,0,0.111222,"ort sentences even when only one non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall"
R13-1030,2006.amta-papers.25,0,0.0881626,"non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of re"
R13-1030,D10-1092,0,0.0207844,"r metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is ca"
R13-1030,2009.mtsummit-wpt.2,1,0.789174,"Missing"
R13-1030,2007.mtsummit-papers.21,1,0.818296,"Missing"
R13-1030,W12-6102,1,0.868937,"Missing"
R13-1030,J10-4005,0,0.0257559,"g Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is calculated using Eq. (3): Introduction Various automatic evaluation metrics for machine translation have been proposed through the metrics task on the Workshop on statistical Machine Translation (WMT). One can identify three kinds of automatic evaluation metrics (C. Liu et al., 2010): the heavyweight linguistic approach, which corresponds to RTE (S. Pad´o et al., 2009) and ULC (J. Gim´enez and L. M´ arquez, 2007); the lightweight linguistic"
R13-1030,W10-1703,0,\N,Missing
R13-1030,W11-2103,0,\N,Missing
R13-1030,I08-1042,0,\N,Missing
R13-1030,W11-2100,0,\N,Missing
sjobergh-araki-2008-multi,sjobergh-araki-2008-poorly,1,\N,Missing
sjobergh-araki-2008-multi,H05-1067,0,\N,Missing
sjobergh-araki-2008-poorly,H05-1067,0,\N,Missing
W02-0707,2003.mtsummit-papers.47,0,0.121438,"Missing"
W02-1812,P98-2206,0,0.185383,"t on any speciﬁc language. In this method, we consider that a character string of appearing frequently in a text has a high possibility as a word. The method predicts unknown words by recursively extracting common character strings. With the proposed method, the segmentation results can adapt to diﬀerent users and ﬁelds. To evaluate eﬀectivety for Chinese word segmentation and adaptability for diﬀerent ﬁelds, we have done the evaluation experiment with Chinese text of the two ﬁelds. 1 Instruction In NLP applications, word segmentation of nonsegmented language is a very necessary initial stage(Sun et al., 1998). In the other hands, with the development of the Internet and popularization of computers, a large amount of text information in diﬀerent languages on the Internet are increasing explosively, so it is necessary to develop a common method to deal with multi-language(Yamasita and Matsumoto, 2000). Furthermore, the standard of word segmentation is dependent on a user and destination of use(Sproat et al., 1996), so that it is necessary that word segmentation can adapt users, can deal with multi languages. In our method, we extract recursively a common character string that occur frequently in tex"
W02-1812,J96-3004,0,\N,Missing
W02-1812,C98-2201,0,\N,Missing
W05-1010,2001.mtsummit-ebmt.3,0,\N,Missing
W05-1010,J96-1001,0,\N,Missing
W05-1010,W95-0114,0,\N,Missing
W05-1010,C96-1006,0,\N,Missing
W05-1010,J93-2003,0,\N,Missing
W05-1010,C02-1158,1,\N,Missing
W05-1010,C94-2178,0,\N,Missing
W05-1010,C96-2098,0,\N,Missing
W05-1010,P99-1067,0,\N,Missing
W05-1010,J04-2003,0,\N,Missing
W05-1010,J00-2004,0,\N,Missing
W05-1010,J03-1002,0,\N,Missing
W12-3714,W10-3207,0,0.0657069,"Missing"
W12-3714,N06-1023,0,0.0224144,"Missing"
W12-3714,W05-0308,0,0.0791698,"Missing"
W12-3714,P03-1010,0,\N,Missing
W12-3714,halacsy-etal-2004-creating,0,\N,Missing
W12-3714,glowinska-przepiorkowski-2010-design,0,\N,Missing
W12-3714,E06-1030,0,\N,Missing
W14-2610,C00-1044,0,0.282317,"Missing"
W14-2610,P99-1032,0,0.200252,"Missing"
W14-2610,J86-2003,0,0.16421,"emotional states in an everyday communication. The emotive meaning is conveyed verbally and lexically through exclamations (Beijer, 2002; Ono, 2002), hypocoristics (endearments) (Kamei et al., 1996), vulgarities (Crystal, 1989) or, for example in Japanese, through mimetic expressions (gitaigo) (Baba, 2003). The function of language realized by such elements of language conveying emotive meaning is called the emotive function of language. It was first distinguished by B¨uhler (1934-1990) in his Sprachtheorie as one of three basic functions of language1 . B¨uhler’s theory was picked up later by Jakobson (1960), who by distinguishing three other functions laid the grounds for structural linguistics and communication studies. Introduction Recently the field of sentiment analysis has attracted great interest. It has become popular to try different methods to distinguish between sentences loaded with positive and negative sentiments. However, a few research focused on a task more generic, namely, discriminating whether a sentence is even loaded with emotional content or not. The difficulty of the task is indicated by three facts. Firstly, the task has not been widely undertaken. Secondly, in research w"
W14-2610,W05-0308,0,0.0241026,"l or neutral is to answer the question of whether it can be interpreted as produced in an emotional state. This way the task was studied by Minato et al. (2006), Aman and Szpakowicz (2007) or Neviarouskaya et al. (2011). Subjective vs. Objective: Discriminating between subjective and objective sentences is to say whether the speaker presented the sentence contents from a first-person-centric perspective or from no specific perspective. The research formulating the problem this way include e.g, Wiebe et al. (1999), who classified subjectivity of sentences using naive Bayes classifier, or later Wilson and Wiebe (2005). In other research Yu and Hatzivassiloglou (2003) used supervised learning to detect subjectivity and Hatzivassiloglou and Wiebe (2012) studied the effect of gradable adjectives on sentence subjectivity. (n) k n ∑ (n) k=1 Emotive vs. Non-emotive: Saying that a sentence is emotive means to specify the linguistic features of language which where used to produce a sentence uttered with emphasis. Research that formulated and tackled the problem this way was done by, e.g., Ptaszynski et al. (2009). Each of the above nomenclature implies similar, though slightly different assumptions. For example,"
W14-2610,W03-1017,0,\N,Missing
W14-3349,D10-1092,0,0.0243437,"Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we conﬁrmed that our metric shows stable correlation with human judgment. 1 Introduction In the ﬁeld of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are eﬀective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is eﬀective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F -measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a 2 Score calculation in APAC The APAC score is calculated in two phases. In the ﬁrst phase, the chunk sequence is determined between a candidate translation and the reference. The chunk sequence 381 Proceedings of the Ninth"
W14-3349,W07-0734,0,0.103887,"Missing"
W14-3349,P04-1077,0,0.03671,"te translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we conﬁrmed that our metric shows stable correlation with human judgment. 1 Introduction In the ﬁeld of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are eﬀective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is eﬀective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F -measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a 2 Score calculation in APAC The APAC score is calculated in two p"
W14-3349,W12-3102,0,\N,Missing
W14-3349,P02-1040,0,\N,Missing
W14-3349,W13-2201,0,\N,Missing
W16-5413,W10-0724,0,0.0512599,"Missing"
W16-5413,speer-havasi-2012-representing,0,0.0366269,"engines is limited even for a researchers so we decide to introduce methods that can be used also with relatively smaller, self-made (crawled), corpora. Our research can also improve crowdsourcing methods, because it can decrease costs or be less time-consuming if distinctly wrong entries are automatically filtered out. Last but not least, we work on concepts and relations while in previous research only simple word pairs (e.g. “to throw” + “a ball”) were used. Our contributions presented in this paper can be summarized as follows: • We evaluate Japanese commonsense knowledge from ConceptNet (Speer and Havasi, 2012) (explained in the next section) by using phrase occurrences in a blog corpus. • We apply proposed methods to three relation types to investigate their flexibility. • We analyze evaluation errors, discuss problems of our methods and propose their expansion for increasing efficiency of automatic evaluation. 2 Japanese ConceptNet ConceptNet is a semantic network-like ontology allowing to process commonsense knowledge. It is created from other sources as hand-crafted OMCS or GlobalMind (Chung, 2006), JMdict5 , Wiktionary6 and so on. In ConceptNet, there are two ways of representation. First is a"
Y99-1030,J93-2003,0,0.00619459,"are defined. For sub-sentential alignment, a simple consultation of a machine readable dictionary seems to be a very obvious method. However, the presence of unregistered words, and the difference which may be seen in the surface form of words, or after being processed by a lexical analyzer cause problems. In addition, compound words, which often appear in one-to-many or many-to-many word correspondences, are not covered by single word entry-based dictionaries. The use, not of dictionaries, but of the parallel corpus itself, has been therefore suggested in order to search word correspondences [4, 5, 6]. Sub-sentential alignment methods which have been proposed are based on statistics. The problem with statistical methods is that they are not able to produce reliable results when the corpus size is limited. For unexplored languages whose huge parallel corpus are still not available, these approaches cannot be applied. In addition, correspondences involving multiple tokens have not yet been entirely resolved [4]. Melamed avoids these cases and considers only one-to-one correspondence [5]. On the other hand, although Kitamura&apos; s [6] main goal is not the sub-sentential alignment, but the extrac"
Y99-1030,P97-1063,0,0.0591794,"are defined. For sub-sentential alignment, a simple consultation of a machine readable dictionary seems to be a very obvious method. However, the presence of unregistered words, and the difference which may be seen in the surface form of words, or after being processed by a lexical analyzer cause problems. In addition, compound words, which often appear in one-to-many or many-to-many word correspondences, are not covered by single word entry-based dictionaries. The use, not of dictionaries, but of the parallel corpus itself, has been therefore suggested in order to search word correspondences [4, 5, 6]. Sub-sentential alignment methods which have been proposed are based on statistics. The problem with statistical methods is that they are not able to produce reliable results when the corpus size is limited. For unexplored languages whose huge parallel corpus are still not available, these approaches cannot be applied. In addition, correspondences involving multiple tokens have not yet been entirely resolved [4]. Melamed avoids these cases and considers only one-to-one correspondence [5]. On the other hand, although Kitamura&apos; s [6] main goal is not the sub-sentential alignment, but the extrac"
Y99-1031,C96-2178,1,0.82492,"Missing"
