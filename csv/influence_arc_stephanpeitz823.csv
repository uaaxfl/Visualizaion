2010.amta-papers.8,N09-1025,0,0.0562958,"Missing"
2010.amta-papers.8,J07-2003,0,0.189032,"our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Translation The hierarchical phrase-based approach can be considered to be an extension of the standard phrase-based model. In this model, we allow the phrases to have “gaps”, i.e. we allow noncontiguous parts of the source sentence to be translated into possibly non-contiguous parts of the target sentence. The model can be formalized as a synchronous context-free grammar (Chiang, 2007). The bilingual rules are of the form X → hγ, α, ∼i , (1) where X is a non-terminal, γ and α are strings of terminals and non-terminals, and ∼ is a one-toone correspondence between the non-terminals of α and γ. Two examples of this kind of rules for the German-to-English translation direction are X → hich habe X ∼0 gesehen, I have seen X ∼0 i X → hum X ∼0 zu X ∼1 , in order to X ∼1 X ∼0 i where the indices in the non-terminals represent the correspondence between source and target “gaps”. This model has the additional advantage that reordering is integrated as part of the model itself, as can"
2010.amta-papers.8,W08-1301,0,0.0230466,"al integrity during the extraction process. Instead we produce additional information for each phrase. We mark those phrases that do not fit in the model with a binary feature. In this way we allow the corresponding scaling factors to decide whether the phrase can still be used during decoding. This also means that a scaling factor of zero allows the decoder to fall back to the baseline system during the minimum error rate training. We parse the English target sentences with the Stanford parser1 , which is able to produce deep syntactic parses as well as dependency structures (de Marneffe and Manning, 2008). In the following we will present the three syntactic models that we analyze in this work. 1 http://nlp.stanford.edu/software/lex-parser.shtml 3.1 Parse Matching The first model that we employ is also the simplest one. Given a monolingual sentence (be it in the source or the target language) and the associated parse tree, we will say that a lexical phrase extracted from this sentence is syntactically valid if it corresponds to the yield of one of the nodes in the syntax tree. With this model, we hope that we can guide the decoder to prefer phrases that are syntactically sound rather than usin"
2010.amta-papers.8,D07-1079,0,0.0601923,"l phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal"
2010.amta-papers.8,N04-1035,0,0.0693091,"s and produce additional initial phrases, also with a low probability. An example is shown in Figure 1(d). The additional phrases that are generated when applying these heuristics are not considered for the later extraction of hierarchical phrases. This is due to the large number of phrases that could be extracted when considering the whole set of initial phrases, which would pose efficiency problems for the translation process. In our experiments, the heuristic methods already increased the number of initial phrases roughly by a factor of 2. 3 Syntactic Features Unlike other work, like e.g. (Galley et al., 2004), we are not enforcing any syntactical integrity during the extraction process. Instead we produce additional information for each phrase. We mark those phrases that do not fit in the model with a binary feature. In this way we allow the corresponding scaling factors to decide whether the phrase can still be used during decoding. This also means that a scaling factor of zero allows the decoder to fall back to the baseline system during the minimum error rate training. We parse the English target sentences with the Stanford parser1 , which is able to produce deep syntactic parses as well as dep"
2010.amta-papers.8,P08-1114,0,0.081536,"t can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Translation The hierarchical phrase-based approach can be considered to be an extension of the standard phrase-based model. In this model, we allow the phrases to have “gaps”, i.e. we allow noncontiguous parts of the source sentence to be translated into"
2010.amta-papers.8,P02-1038,1,0.747643,"Missing"
2010.amta-papers.8,P03-1021,0,0.0297927,"ords Vocabulary OOVs Sentences # Words Vocabulary OOVs Chinese English 3 030 696 77 456 152 81 002 954 83 128 213 076 21 059 95 544 1 664 42 930 172 324 6 387 17 202 1 871 50 353 1 357 36 114 149 057 6 418 17 877 1 375 43 724 Table 1: Statistics for the Chinese-English corpus 4 Experimental Results We used the Chinese-English NIST 2006 evaluation set as a development corpus and the NIST 2008 evaluation set as the blind test corpus. The systems were trained on a medium-sized training set. Statistics can be found in Table 1. All systems were optimized for the BLEU score using Och’s MERT method (Och, 2003), with all scaling factors initialized with a value of 0.1. For rescoring with trigram dependency language models we generated 100-best lists after the optimization process. Translation results obtained applying the methods discussed in Section 3 are shown in Table 2. All three methods yield improvements over the baseline system. The string-todependency method has very strong improvements in TER, while the soft syntactic labels perform very good in terms of BLEU. The parsematch approach is somewhat in between. The combination of the methods also leads to nice synergies. With the exception of t"
2010.amta-papers.8,P08-1066,0,0.396344,"on achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into our comparison, since the risk of converging to poor local optima during the optimization procedure increases when too many features are available, thus making it difficult to draw clear conclusions. 2 Hierarchical Machine Tran"
2010.amta-papers.8,N09-1027,0,0.297999,"Missing"
2010.amta-papers.8,2008.iwslt-papers.7,1,0.938414,"ate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic structure of a given parse tree. Further, we include a dependency language model in a string-to-dependency model in the spirit of (Shen et al., 2008). We also derive soft syntactic labels as in (Venugopal et al., 2009), where the generic non-terminal of the hierarchical system is replaced by a syntactic label. Other approaches in this field like (Chiang et al., 2009) and (Marton and Resnik, 2008) go into similar directions, but create a rather large quantity of features. We chose not to include their approaches into ou"
2010.amta-papers.8,W10-1738,1,0.846637,"rious groups report improvement over their baseline systems with different approaches, but it is not clear whether the benefits of the different methods are complementary or if they rather address the same issues. In this work, we compare three recent syntactic methods that enhance the translation quality. We measure their performance individually and in combination with each other on a medium sized NIST Chinese-English task, and offer some analysis of typical translation examples. All the presented methods are released as part of the open source hierarchical machine translation toolkit Jane (Vilar et al., 2010). This paper is organized as follows: We briefly recapitulate the hierarchical phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further de"
2010.amta-papers.8,P01-1067,0,0.0755085,"the presented methods are released as part of the open source hierarchical machine translation toolkit Jane (Vilar et al., 2010). This paper is organized as follows: We briefly recapitulate the hierarchical phrase-based model for machine translation in Section 2. We then describe the additional syntactical models used in this paper in Section 3. Results and detailed analysis on the NIST Chinese-English task are presented in Section 4. We conclude the paper in Section 5. 1.1 Related Work One of the first papers to incorporate syntactic knowledge in a statistical machine translation model was (Yamada and Knight, 2001), although the performance was not on par with other state-of-the-art approaches at that time. Further development in this direction achieved competitive results, as can be seen in (DeNeefe et al., 2007) and later publications by the same group. In contrast to these studies, which propose new models centered around the syntactic information, we focus mainly on methods that can be easily incorporated into an existing hierarchical system. In this work, we employ soft syntactic features comparable to (Vilar et al., 2008). These features measure how much a phrase corresponds to a valid syntactic s"
2010.iwslt-evaluation.22,W06-3103,1,0.835479,"Workshop on Spoken Language Translation (IWSLT 2010). We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Sectio"
2010.iwslt-evaluation.22,N04-4015,0,0.0675793,"We used it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present"
2010.iwslt-evaluation.22,N06-2013,0,0.0454777,"it as an opportunity to incorporate novel methods which have been investigated at RWTH over the last year and which have proven to be successful in other evaluations. We participated in the Arabic-English BTEC task, and used standard alignment and training tools as well as our inhouse phrase-based and open-source hierarchical SMT decoders. We explored and implemented different segmentation tools for Arabic. The methods used to implement those tools vary from rule-based methods (typically encoded as finite state transducers) such as [1], to methods which are statistically-based such as [2] and [3]. All these works have shown that segmentation improves MT quality significantly for both small and large scale tasks. Due to the different methodologies that we apply for segmentation, we expect that there will be complimentary variation in the results achieved by each method. The next step would be to exploit those variations and achieve better results by combining the systems. This paper is organized as follows. In Section 2, we present the data and resources that will be used to build our segmenters and the SMT system. In Section 3, we discuss the problems of Arabic SMT and present the sol"
2010.iwslt-evaluation.22,N04-4038,0,0.0482616,"asily captured by the IBM alignment models. In this work, we experimented with the following segmenters: • FST - A Finite State Transducer-based approach introduced and implemented by [1]. The FST is used as a framework to implement a set of rules for segmentation of Arabic. The prefixes that are split include w,f,k,l,b,Al and s. Suffixes which are segmented are pronouns (objective and possessive). The method is characterized by fast processing speed but suffers from the lack of context in the decision procedure leading to erroneous output. • SVM - we reimplemented the classifier suggested by [4]. In their method, each character is classified by its segment rule (prefix, stem and suffix) and position (beginning and inside segment). Arabic words are segmented according to the ATB scheme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 ."
2010.iwslt-evaluation.22,W07-0813,0,0.052985,"eme. Additionally, feminine marker normalization (tX→p+X) using an SVM model is applied on top of the segmenter output, which proved to be significant for the performance of MT in our experiments. • CRF - we implemented a CRF classifier for segmentation using similar setup of classifiers and classes as in the SVM model. The software we use as an implementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA versi"
2010.iwslt-evaluation.22,P05-1071,0,0.0466539,"lementation of conditional random fields is named CRF++4 . • MorphTagger - is a general architecture for Part-OfSpeech (POS) tagging of natural languages. The architecture was first proposed in [5] and applied for the task of POS tagging of Hebrew. [6] adapted the architecture to the Arabic language. MorphTagger is implemented using Buckwalter Arabic Morphological Analyzer v1.0 (BAMA) as a morphological analyzer and a Hidden-Markov-Model (HMM) (using the SRIML5 toolkit) as the disambiguator component. • MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-tar"
2010.iwslt-evaluation.22,2010.amta-papers.8,1,0.836307,"WTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-terminals is then used to compute a new prob"
2010.iwslt-evaluation.22,2010.iwslt-papers.18,1,0.874946,"h respect to the syntactic constructs. 5.3. Poor-man syntax (POMS) In this approach we apply the same model as described in the previous section, but the method for producing the new 6 http://nlp.stanford.edu/software/lex-parser.shtml JANE BLEU TER 55.4 30.6 55.3 30.2 55.2 29.4 53.9 31.2 54.6 30.2 56.6 28.8| 57.1| 29.4 56.6− 28.9− 53.0 32.4 SYN BLEU TER 55.7 30.8 54.4 31.2 55.7 29.4 54.5 30.9 54.8 31.2 56.5 28.5+ 56.6| 29.2 55.4 30.3 52.7 32.5 POMS BLEU TER 56.1 30.2 56.0− 29.4− 55.2 29.9 54.8 30.8 55.5− 29.7− 56.8− 28.7 57.5∗ 28.5∗ 54.9 29.5 53.4 32.3 non-terminals is altered as described in [16]. Instead of relying on parse trees based on linguistic knowledge we rely on automatic clustering methods. This makes this approach applicable also for underresourced languages for which no linguistic tools may be available. 6. Results The results of the different segmentation methods and schemes are summarized in Table 1. In this table, the best result in a column is marked with |, thus comparing different segmentations for the same decoder setup. We mark with − the best (in a row) performing decoder over a specific segmentation method. ∗ marks the best result overall in the table. For compar"
2010.iwslt-evaluation.22,W10-1747,1,0.898303,"Missing"
2010.iwslt-evaluation.22,2008.iwslt-papers.8,1,0.0907932,"MADA - The Morphological Analysis and Disambiguation of Arabic (MADA) system, developed in [7], can be seen as an extension of an SVM-based system with the incorporation of a morphological analyzer. As in [8], we experiment with different segmentation schemes for each chosen analysis. We use the schemes directly implemented in the MADA version we are using, namely: D1,D2,D3 and the ATB (TB) schemes. 4. Phrase-based system 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which i"
2010.iwslt-evaluation.22,W99-0604,1,0.294484,"m 4.1. Standard phrase-based system (PBT) The phrase-based SMT system used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th Internati"
2010.iwslt-evaluation.22,P10-1049,1,0.304281,"ystem used in this work is an inhouse implementation of state-of-the-art phrase-based MT system as described in [9]. We use the standard set of models with phrase translation probabilities for source-to-target and target-to-source direction, smoothing with lexical weights, a word and phrase penalty, distance-based and lexicalized reordering and an n-gram target language model. 4.2. Phrase training (Forced Alignment-FA) To estimate the phrase translation probabilities we experimented with both standard heuristic phrase extraction ([10]) and a forced alignment training procedure as described in [11]. The latter estimates the probabilities as relative frequencies from the phrase-aligned training data, which is computed by a modified version of the translation decoder. To do this, the translation decoder is constrained to produce the reference translation for each bilingual sentence pair. In order to counteract overfitting, leaving-one-out is applied in training. In addition to providing a statistically well-founded 4 http://crfpp.sourceforge.net/ 5 http://www-speech.sri.com/projects/srilm/ 164 Proceedings of the 7th International Workshop on Spoken Language Translation Paris, December 2nd"
2010.iwslt-evaluation.22,W10-1738,1,0.0939642,"010: IWSLT08 results summary (nocase+punc) System CRF FST MADA ATB MADA D1 MADA D2 MADA D3 MorphTagger SVM TOK PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of consider"
2010.iwslt-evaluation.22,J07-2003,0,0.142759,"K PBT BLEU TER 55.5 29.8− 54.5 30.7 55.1 29.5 54.8 30.8 55.4 29.9 55.4 29.6 56.5| 29.2| 56.1 29.7 55.5− 30.1− FA BLEU TER 56.4− 30.7 55.9 30.3 57.1+ 29.2+ 55.2− 30.6− 55.5 30.1 56.5 30.1 55.8 30.1 55.9 30.0 54.8 30.3 phrase model, the forced alignment procedure has the benefit of producing smaller phrase tables. 5. Hierarchical system 5.1. Standard hierarchical system (JANE) We used the open source hierarchical phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syn"
2010.iwslt-evaluation.22,N09-1027,0,0.0119972,"phrase-based system Jane, developed at RWTH and free for non-commercial use [12]. This approach is an extension of the phrase-based approach, where the phrases are allowed to have gaps [13]. In this way long-range dependencies and reorderings can be modelled in a consistent statistical framework. The system labelled as JANE represents a fairly standard setup of the system and constitutes a baseline upon which the two next systems are built. 5.2. Soft syntax labels (SYN) To extend the hierarchical system with syntax information of the English target side, we derive soft syntactic labels as in [14] with the modifications described in [15]. In this model, instead of considering only a single, generic non-terminal in the underlying grammar, we extend the set of labels to include syntactic categories as found in syntactic parse trees. To extract the syntax information, we parse the English target sentences with the Stanford parser6 . It is important to note that the new non-terminals are considered in a probabilistic way. In this way, the parsing process itself continues to use the generic non-terminal as in the baseline model and the parsing space is unaltered. The extended set of non-ter"
2010.iwslt-evaluation.22,popovic-ney-2006-pos,1,\N,Missing
2010.iwslt-evaluation.22,D11-1033,0,\N,Missing
2010.iwslt-evaluation.22,E09-1044,0,\N,Missing
2010.iwslt-evaluation.22,D09-1022,1,\N,Missing
2010.iwslt-evaluation.22,J93-2003,0,\N,Missing
2010.iwslt-evaluation.22,E06-1005,1,\N,Missing
2010.iwslt-evaluation.22,E03-1076,0,\N,Missing
2010.iwslt-evaluation.22,D08-1089,0,\N,Missing
2010.iwslt-evaluation.22,P03-1054,0,\N,Missing
2010.iwslt-evaluation.22,P02-1040,0,\N,Missing
2010.iwslt-evaluation.22,W06-3110,1,\N,Missing
2010.iwslt-evaluation.22,J10-3008,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-keynotes.2,0,\N,Missing
2010.iwslt-evaluation.22,P10-2041,0,\N,Missing
2010.iwslt-evaluation.22,P08-2030,0,\N,Missing
2010.iwslt-evaluation.22,W07-0734,0,\N,Missing
2010.iwslt-evaluation.22,W06-3105,0,\N,Missing
2010.iwslt-evaluation.22,N03-1017,0,\N,Missing
2010.iwslt-evaluation.22,2008.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,J03-1002,1,\N,Missing
2010.iwslt-evaluation.22,W06-3108,1,\N,Missing
2010.iwslt-evaluation.22,P07-1019,0,\N,Missing
2010.iwslt-evaluation.22,D08-1039,1,\N,Missing
2010.iwslt-evaluation.22,P08-1066,0,\N,Missing
2010.iwslt-evaluation.22,2009.mtsummit-posters.17,0,\N,Missing
2010.iwslt-evaluation.22,2010.iwslt-papers.15,1,\N,Missing
2010.iwslt-evaluation.22,2006.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.1,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.7,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.8,1,\N,Missing
2010.iwslt-evaluation.22,N04-1033,1,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-evaluation.1,0,\N,Missing
2010.iwslt-evaluation.22,D08-1076,0,\N,Missing
2010.iwslt-evaluation.22,P03-1021,0,\N,Missing
2010.iwslt-evaluation.22,2011.iwslt-papers.5,1,\N,Missing
2010.iwslt-evaluation.22,P08-1000,0,\N,Missing
2010.iwslt-papers.18,N04-1035,0,\N,Missing
2010.iwslt-papers.18,E99-1010,0,\N,Missing
2010.iwslt-papers.18,P01-1067,0,\N,Missing
2010.iwslt-papers.18,N09-1027,0,\N,Missing
2010.iwslt-papers.18,N09-1025,0,\N,Missing
2010.iwslt-papers.18,P08-1114,0,\N,Missing
2010.iwslt-papers.18,P05-1033,0,\N,Missing
2010.iwslt-papers.18,N03-1017,0,\N,Missing
2010.iwslt-papers.18,P02-1038,1,\N,Missing
2010.iwslt-papers.18,2008.iwslt-papers.7,1,\N,Missing
2010.iwslt-papers.18,D07-1079,0,\N,Missing
2010.iwslt-papers.18,P08-1066,0,\N,Missing
2010.iwslt-papers.18,W06-3119,0,\N,Missing
2010.iwslt-papers.18,J07-2003,0,\N,Missing
2010.iwslt-papers.18,D08-1076,0,\N,Missing
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.iwslt-papers.7,2006.iwslt-papers.1,1,0.409395,"d by the accuracy of the predicted punctuation as well as by the quality of the final translation output. The paper is organized as follows. In Section 2, a short overview of the published research on punctuation prediction is given. In Section 3, we recapitulate different approaches for punctuation prediction. We present our approach using a statistical phrase-based machine translation system in Section 4, followed by Section 5 describing the system combination. Finally, Section 6 describes the experimental results, followed by a conclusion. 2. Related Work This paper is based on the work of [1]. Amongst others they presented three different approaches to restore punctuation in already segmented ASR output. In addition to implicit punctuation generation in the translation process, punc238 tuation was predicted as pre- and postprocessing step. For punctuation prediction they used the HIDDEN - NGRAM tool from the SRI toolkit [2]. The implicit punctuation generation worked best on IWSLT 2006 corpus, but on TC-STAR 2006 corpus they achieved better results with punctuation prediction on source and target. They pointed out that on small corpora like IWSLT 2006 falsely inserted punctuation"
2011.iwslt-papers.7,2007.iwslt-1.10,0,0.130195,"ed best on IWSLT 2006 corpus, but on TC-STAR 2006 corpus they achieved better results with punctuation prediction on source and target. They pointed out that on small corpora like IWSLT 2006 falsely inserted punctuation marks in the source side deteriorated the performance of the translation system. However, the IWSLT corpus became larger in the last years and therefore we verify the results within IWSLT 2011 SLT task. Furthermore, we use in addition for the punctuation prediction a phrase-based statistical machine translation system. Using MT for punctuation prediction was first described in [3]. In this work, a phrase-based statistical machine translation system was trained on a pseudo-’bilingual’ corpus. The case-sensitive target language text with punctuation was considered as the target language and the text without case information and punctuation was used as source language. They applied this approach as postprocessing step in evaluation campaign of IWSLT 2007 and achieved a significant improvement over the baseline. In [4] the same approach was employed as preprocessing step and compared with the HIDDEN - NGRAM tool within the evaluation campaign of IWSLT 2008. The HIDDEN NGRA"
2011.iwslt-papers.7,D10-1018,0,0.317837,"DDEN NGRAM tool outperformed the MT-based punctuation prediction. Moreover, they achieved further improvements by combining these two methods using a majority voting procedure. In our work, we further investigate this approach and compare it with the HIDDEN - NGRAM tool at different stages at which the prediction is done. In our analysis we consider translation quality at the end of the translation pipeline as well as the accuracy of the punctuation prediction. In contrast to the majority vote, we do a system combination of the hypotheses of all different approaches. The approach described in [5] is based on conditional random fields (CRF). They extended the linear-chain CRF model to a factorial CRF model using two layers with different sets of tags for punctuation marks respectively sentence types. They compared their novel approach with linear-chain CRF model and the HIDDEN - NGRAM tool on the IWSLT 2009 corpus. Besides the comparison of the translation quality in terms of B LEU, they also compared the CRF models with the hidden event language model regarding precision, recall and F1-measure. Both in terms of B LEU and in terms of precision, recall and F1-measure the CRF models outp"
2011.iwslt-papers.7,2008.iwslt-papers.8,1,0.342239,"s in the source sentences become non-aligned. In Figure 3 and Figure 4 is one example for deleting the punctuation marks in the source sentence. Now, we are able to train a monolingual MT system for unpunctuated to punctuated text. The tuning set for the parameter tuning is constructed by removing the punctuation marks from the regular development set source text. As reference we use the original source text with the punctuation left intact. The phrase-based MT system used in this work for the punctuation prediction is an in-house implementation of the state-of-the-art MT decoder described in [6]. We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, an 9-gram source language model and three binary count features. Due to the fact, that we use a monotone alignment, the reordering model is dropped. We also allow longer phrases to capture punctuation dependencies. The optimization is done with standard MERT [7] on 200-best lists with FullPunct Implicit NoPunct system combination Figure 5: System combination of the translation result coming from different punctuation prediction methods. B LEU as optimizat"
2011.iwslt-papers.7,P03-1021,0,0.0466025,"rce text with the punctuation left intact. The phrase-based MT system used in this work for the punctuation prediction is an in-house implementation of the state-of-the-art MT decoder described in [6]. We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, an 9-gram source language model and three binary count features. Due to the fact, that we use a monotone alignment, the reordering model is dropped. We also allow longer phrases to capture punctuation dependencies. The optimization is done with standard MERT [7] on 200-best lists with FullPunct Implicit NoPunct system combination Figure 5: System combination of the translation result coming from different punctuation prediction methods. B LEU as optimization criterion. 200-best lists are chosen to get more different hypotheses. 5. System Combination System combination is used to produce consensus translations from multiple translation hypotheses generated with different systems. We follow an approach similar to the one described in [8, 9]. The basic procedure is, that hypotheses from different translations systems are aligned on the word level to fin"
2011.iwslt-papers.7,E06-1005,1,0.319416,"opped. We also allow longer phrases to capture punctuation dependencies. The optimization is done with standard MERT [7] on 200-best lists with FullPunct Implicit NoPunct system combination Figure 5: System combination of the translation result coming from different punctuation prediction methods. B LEU as optimization criterion. 200-best lists are chosen to get more different hypotheses. 5. System Combination System combination is used to produce consensus translations from multiple translation hypotheses generated with different systems. We follow an approach similar to the one described in [8, 9]. The basic procedure is, that hypotheses from different translations systems are aligned on the word level to find corresponding parts. Based on these alignments, a weighted majority voting on aligned words and additional models are used to produce the consensus translation. In the scope of this work, we will combine translation output from multiple punctuation prediction schemes. Figure 5 shows the basic idea how to use system combination in this task. 6. Experimental Evaluation The methods presented in this paper were evaluated on the IWSLT 2011 English-to-French translation track [10]. IWS"
2011.iwslt-papers.7,J07-2003,0,0.0641623,"able 3). We use a modified development set as described in Section 4. We remove the punctuation of the development and test sets which are available in the MT task of IWSLT 2011 (Table 2). MT SLT English French English French 934 20131 20280 17735 20280 17795 3209 3717 3132 3717 1664 31975 33814 27427 33814 27653 3711 4678 3670 4678 6.2. Hierarchical phrase-based decoder for translation The following MT system is given to compare all punctuation prediction strategies. We use the open source hierarchical phrase-based system Jane [11], which implements the hierarchical approach as introduced by [12]. The search is carried out using the cube pruning algorithm [13]. The models integrated into our Jane systems are: phrase translation probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, four binary count features and an 4-gram language model. For a robust baseline we add a sparse discriminative word lexicon (DWL) model for lexical smoothing and triplets similar to [14]. The model weights are optimized with standard MERT [7] on 100-best lists. 6.3. Comparison of the punct"
2011.iwslt-papers.7,P07-1019,0,0.0607338,"n 4. We remove the punctuation of the development and test sets which are available in the MT task of IWSLT 2011 (Table 2). MT SLT English French English French 934 20131 20280 17735 20280 17795 3209 3717 3132 3717 1664 31975 33814 27427 33814 27653 3711 4678 3670 4678 6.2. Hierarchical phrase-based decoder for translation The following MT system is given to compare all punctuation prediction strategies. We use the open source hierarchical phrase-based system Jane [11], which implements the hierarchical approach as introduced by [12]. The search is carried out using the cube pruning algorithm [13]. The models integrated into our Jane systems are: phrase translation probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, four binary count features and an 4-gram language model. For a robust baseline we add a sparse discriminative word lexicon (DWL) model for lexical smoothing and triplets similar to [14]. The model weights are optimized with standard MERT [7] on 100-best lists. 6.3. Comparison of the punctuation prediction accuracy To assess and compare the punctuation"
2011.iwslt-papers.7,2010.amta-papers.32,1,0.838696,"hierarchical phrase-based system Jane [11], which implements the hierarchical approach as introduced by [12]. The search is carried out using the cube pruning algorithm [13]. The models integrated into our Jane systems are: phrase translation probabilities in both translation directions, word and phrase penalty, binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, four binary count features and an 4-gram language model. For a robust baseline we add a sparse discriminative word lexicon (DWL) model for lexical smoothing and triplets similar to [14]. The model weights are optimized with standard MERT [7] on 100-best lists. 6.3. Comparison of the punctuation prediction accuracy To assess and compare the punctuation prediction performance of the approaches presented in Section 3, we remove all punctuation from test set of the correct manual transcription, and restore the punctuation marks with the HIDDEN NGRAM as well as with our phrase-based decoder for punctuation prediction (PPMT). We use the original test set as reference. We verify the methods before the translation, because the translation process causes too many errors for measuring"
2011.iwslt-papers.7,2006.amta-papers.25,0,0.0866157,". shown in Table 6. When we consider all punctuation, the precision of the prediction with the HIDDEN - NGRAM is slightly higher then PPMT . However, the recall of the prediction with the PPMT is better and this results in a higher F-measure. 6.4. Comparison of the translation quality While a comparison of the punctuation prediction performance might be a good indicator of the overall accuracy of the method, we ultimately want to improve the quality of the translation output. In order to compare the different strategies, we measure the translation quality of all systems in B LEU [15] and T ER [16]. B LEU measures the accuracy of the translation, so higher values in B LEU are better. T ER is an error measure, with lower values indicating better quality. We built five different experimental setups with regards to the description in Subsection 6.2. To compare our new method, we use the HIDDEN - NGRAM tool with the same language model as applied in our phrase-based decoder for punctuation prediction. Thus, we get two systems for F ULL P UNCT and two systems for N O P UNCT. The fifth system is I MPLICIT. Table 7 shows the comparison between the different translation system and both predicti"
2011.iwslt-papers.7,2011.iwslt-evaluation.1,0,0.0685004,"in [8, 9]. The basic procedure is, that hypotheses from different translations systems are aligned on the word level to find corresponding parts. Based on these alignments, a weighted majority voting on aligned words and additional models are used to produce the consensus translation. In the scope of this work, we will combine translation output from multiple punctuation prediction schemes. Figure 5 shows the basic idea how to use system combination in this task. 6. Experimental Evaluation The methods presented in this paper were evaluated on the IWSLT 2011 English-to-French translation track [10]. IWSLT is an annual public evaluation campaign focused on speech translation. The domain of the 2011 translation task is lecture-type talks presented at TED conferences which are also available online2 . Two different conditions were evaluated: Automatic and manual transcription of lectures. While the correct manual transcription also contained punctuation marks, the automatic transcription did not. The automatic transcription used in this work was the 1-best hypothesis from the speech recognition system. The in-domain training data (Table 1) also consisted of transcribed TED lectures as well"
2011.iwslt-papers.7,2008.iwslt-evaluation.3,0,\N,Missing
2011.iwslt-papers.7,P02-1040,0,\N,Missing
2012.eamt-1.66,D10-1054,0,\N,Missing
2012.eamt-1.66,D11-1079,0,\N,Missing
2012.eamt-1.66,E09-1044,0,\N,Missing
2012.eamt-1.66,N04-4026,0,\N,Missing
2012.eamt-1.66,C04-1030,1,\N,Missing
2012.eamt-1.66,D08-1089,0,\N,Missing
2012.eamt-1.66,W10-1738,1,\N,Missing
2012.eamt-1.66,J10-3008,0,\N,Missing
2012.eamt-1.66,P07-2045,0,\N,Missing
2012.eamt-1.66,N09-1049,0,\N,Missing
2012.eamt-1.66,P05-1033,0,\N,Missing
2012.eamt-1.66,J03-1002,1,\N,Missing
2012.eamt-1.66,W06-3108,1,\N,Missing
2012.eamt-1.66,P07-1019,0,\N,Missing
2012.eamt-1.66,2011.eamt-1.37,1,\N,Missing
2012.iwslt-evaluation.7,popovic-ney-2006-pos,1,\N,Missing
2012.iwslt-evaluation.7,N04-4026,0,\N,Missing
2012.iwslt-evaluation.7,D09-1022,1,\N,Missing
2012.iwslt-evaluation.7,J93-2003,0,\N,Missing
2012.iwslt-evaluation.7,E03-1076,0,\N,Missing
2012.iwslt-evaluation.7,N04-4038,0,\N,Missing
2012.iwslt-evaluation.7,C02-1050,0,\N,Missing
2012.iwslt-evaluation.7,P02-1040,0,\N,Missing
2012.iwslt-evaluation.7,W10-1738,1,\N,Missing
2012.iwslt-evaluation.7,P12-3029,0,\N,Missing
2012.iwslt-evaluation.7,J10-3008,0,\N,Missing
2012.iwslt-evaluation.7,2010.iwslt-keynotes.2,0,\N,Missing
2012.iwslt-evaluation.7,P10-2041,0,\N,Missing
2012.iwslt-evaluation.7,P10-1049,1,\N,Missing
2012.iwslt-evaluation.7,P07-2045,0,\N,Missing
2012.iwslt-evaluation.7,P08-2030,0,\N,Missing
2012.iwslt-evaluation.7,W07-0734,0,\N,Missing
2012.iwslt-evaluation.7,2008.iwslt-papers.8,1,\N,Missing
2012.iwslt-evaluation.7,J03-1002,1,\N,Missing
2012.iwslt-evaluation.7,W06-3108,1,\N,Missing
2012.iwslt-evaluation.7,D09-1117,0,\N,Missing
2012.iwslt-evaluation.7,P07-1019,0,\N,Missing
2012.iwslt-evaluation.7,C12-3061,1,\N,Missing
2012.iwslt-evaluation.7,W06-3103,1,\N,Missing
2012.iwslt-evaluation.7,2012.iwslt-papers.18,1,\N,Missing
2012.iwslt-evaluation.7,C12-2091,1,\N,Missing
2012.iwslt-evaluation.7,2010.iwslt-papers.15,1,\N,Missing
2012.iwslt-evaluation.7,2006.iwslt-papers.1,1,\N,Missing
2012.iwslt-evaluation.7,J07-2003,0,\N,Missing
2012.iwslt-evaluation.7,2002.tmi-tutorials.2,0,\N,Missing
2012.iwslt-evaluation.7,P03-1021,0,\N,Missing
2012.iwslt-evaluation.7,2012.eamt-1.60,0,\N,Missing
2012.iwslt-papers.18,2012.eamt-1.60,0,0.0324683,"postprocessed ASR output. 3. Automatically Transcribed Text in Training The starting point of this work is a data source which provides audio recordings, the corresponding manual transcriptions and the translation of these transcriptions. The onlineavailable TED talks are such a kind of source 1 . This website provides manually transcribed and translated lecturetype talks presented at TED conferences. Furthermore, WIT3 (Web Inventory of Transcribed and Translated Talks) redistributes the original content published by the TED website 1 http://www.ted.com/ for the machine translation community [6]. The transcriptions and the translations are processed as parallel bilingual corpus to be able to train an SMT system. Further, development and test sets are provided. In an SLT application, the development and test sets are automatically transcribed speech, which have to be translated into a target language. We assume in this work that the recognitions of the development and test sets do not contain punctuation and casing and the segmentation is given and corresponds to sentence-like units. With an SMT system, the automatically recognized speech is translated. Furthermore, the punctuation an"
2012.iwslt-papers.18,2006.iwslt-papers.1,1,0.952728,"s to be able to train an SMT system. Further, development and test sets are provided. In an SLT application, the development and test sets are automatically transcribed speech, which have to be translated into a target language. We assume in this work that the recognitions of the development and test sets do not contain punctuation and casing and the segmentation is given and corresponds to sentence-like units. With an SMT system, the automatically recognized speech is translated. Furthermore, the punctuation and the case information are restored during the translation process as described in [7]. In order to train such an SMT system, the punctuation and the case information of source language data in the bilingual training corpus are deleted to create a pseudo ASR output. In our work, we train an SMT system on a bilingual corpus with real ASR output instead of pseudo ASR output as source language data. Due to the fact that WIT3 also speciﬁes the talks which were used to create the provided bilingual corpora, we are able to recognize the relevant audio recordings with our ASR system. About 1028 relevant talks are available on the web. In sum, roughly 250 hours of speech have to be rec"
2012.iwslt-papers.18,2005.iwslt-1.19,1,0.891421,"levant talks are available on the web. In sum, roughly 250 hours of speech have to be recognized. Using the automatically transcribed recordings as source language data, we build a new bilingual corpus to train an SMT system for an SLT task. 3.1. Sentence Alignment In general, an ASR system does not provide sentence-wise segmentation. However, a bilingual corpus, which is used to train an SMT system, consists of parallel sentences. In order to align automatic transcriptions sentence-wise to a given segmented manual transcription, we employ an automatic resegmentation algorithm as described in [8]. The re-segmentation algorithm calculates the Levenshtein alignment between the recognition and its manual transcription. By backtracing the decisions of the edit distance algorithm, an alignment between a given sequence of words and an already sentence-wise segmented manual transcription as reference can be found. Thus, the sentence segmentation of the reference is transferred to the recognition. The re-segmentation algorithm is solved by dynamic programming. As mentioned, WIT3 provides manually transcribed text as well as the corresponding translation. First, we align our recognized trainin"
2012.iwslt-papers.18,P12-2006,1,0.818664,"me tool. The recognition is structured in three passes, In the ﬁrst pass, a speaker independent model is used. The recognition result of the ﬁrst pass is used for estimating feature transformations for speaker adaptation (CMLLR). The second pass uses the CMLLR transformed features. Finally, a confusion network decoding is performed on the word lattices obtained from the second pass. and a small amount of in-domain data (ted), see Table 3. The recognition lexicon consists of 150k words. 4.2. MT System The decoder of the phrase-based translation system which is used in this work is described in [10] and is part of RWTH’s open-source SMT toolkit Jane 2.1 2 . We use the standard set of models with phrase translation probabilities and lexical smoothing in both directions, word and phrase penalty, distance-based distortion model, a 4-gram target language model and three binary count features. The features hm ( f1J , eJ1 ) are combined in a weighted log-linear model to ˆ ﬁnd the best translation eˆI1 ˆ eˆI1 = arg max Table 2: Acoustic training data of ASR system Corpus Amount of data [hours] quaero-2011 268h hub4+tdt4 393h epps 102h Table 3: Language model training data of ASR system Corpus G"
2012.iwslt-papers.18,P03-1021,0,0.0859037,"transcribed acoustic data in total, see Table 2. The acoustic training data consists of American broadcast news data (hub4+tdt4), European parliament speeches (epps), and British broadcast conversations (quaero). The MLP is trained on the 268 hours of the quaero corpus only. We use 4500 triphone states and perform eight EM splits, resulting in a GMM with roughly 1.1 million mixture components. The language model is trained on a large amount of news data (Gigaword), the transcriptions of the audio training data, eI1 M ∑ λm hm ( f1J , eJ1 ). (2) m=1 The weights are optimized using standard MERT [11] on 200-best lists with B LEU as optimization criterion. 5. Experimental Evaluation The proposed approach was evaluated on the IWSLT 2012 English-to-French spoken language translation task based on the already mentioned TED talks. For the evaluation, WIT3 provides in-domain bilingual training data based on manually transcribed text and its translation. The 1028 talks (around 250 hours of speech), which corresponds to the bilingual training data, were recognized with the described ASR system. For the baseline model, we removed punctuation and case information of the source language to create ps"
2012.iwslt-papers.18,2006.amta-papers.25,0,0.0328718,"bles are marked as AUTOMATIC - TRANSCRIPTION ◦ MANUAL TRANSCRIPTION . 5.1.3. Training Data Concatenation In contrast to the other two methods, the training corpora MANUAL - TRANSCRIPTION and AUTOMATIC TRANSCRIPTION were combined before the phrase extraction. In particular, MANUAL - TRANSCRIPTION and AUTOMATIC - TRANSCRIPTION were concatenated and the translation model was re-trained. This setup is named AUTOMATIC - TRANSCRIPTION + MANUAL TRANSCRIPTION . 5.2. Results Table 9 shows the comparison between different setups. We measured the translation quality of all systems in B LEU [14] and T ER [15] on the development set as well as on the test set. First, we ran two baseline experiments. Both systems were trained on MANUAL - TRANSCRIPTION. The ﬁrst setup was tuned and tested on the provided development and test sets (IWSLT 2012) and the second one on our own recognitions. It seems that a better W ER results in a higher translation quality. Using AUTOMATIC - TRANSCRIPTION (cn-decoding) performs only slightly better then the baseline. The biggest improvement was achieved by AUTOMATIC - TRANSCRIPTION (cn-decoding) ◦ MANUAL - TRANSCRIPTION in comparison to MANUAL - TRANSCRIPTION (baseline,"
2012.iwslt-papers.18,P02-1040,0,\N,Missing
2012.iwslt-papers.18,J03-1002,1,\N,Missing
2012.iwslt-papers.18,2011.iwslt-papers.7,1,\N,Missing
2013.iwslt-evaluation.15,2012.iwslt-evaluation.7,1,\N,Missing
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.mtsummit-papers.19,P06-1009,0,0.0179529,"very large, and an alignment is seldom provided with the corpora. As CRFs include a summation over all possible target sequences (equation 1), the computational complexity of CRFs can be expressed by a polynomial of the target vocabulary size |Y |with a degree equal to the size of the feature describing the largest tuple in the target sequence (n-gram in language modeling (LM)). Authors have published approaches to move computation time to the lower degree parts of the polynomial, e.g. (Lavergne et al., 2010). However, this only changes constants in the complexity, not the overall complexity. Blunsom and Cohn (2006) and Niehues and Vogel (2008) avoid this problem by improving the alignment A used for the phrase extraction of a phrase based translation (PBT) system. In this case the source and target sequences are given, and the effective target vocabulary are either active or non-active alignments points p(A|y1I , xJ1 ), which is faster to compute than a sequence from Y, but reference alignments are needed, which are usually not provided with a machine translation corpus. Another approach is to manually constrain the summation to a reduced set of target sequences. Blunsom et al. (2008) propose to extend"
2013.mtsummit-papers.19,P08-1024,0,0.0237398,"rall complexity. Blunsom and Cohn (2006) and Niehues and Vogel (2008) avoid this problem by improving the alignment A used for the phrase extraction of a phrase based translation (PBT) system. In this case the source and target sequences are given, and the effective target vocabulary are either active or non-active alignments points p(A|y1I , xJ1 ), which is faster to compute than a sequence from Y, but reference alignments are needed, which are usually not provided with a machine translation corpus. Another approach is to manually constrain the summation to a reduced set of target sequences. Blunsom et al. (2008) propose to extend a hierarchical machine translation system and constrain the summation to the derivations given by this system, while (Lavergne et al., 2011) use a PBT system and use the phrase table to Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 151–158. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. constrain the summation. Unfortunately, both approaches could only improve constrained systems. In (He a"
2013.mtsummit-papers.19,P12-1031,0,0.013349,"008) propose to extend a hierarchical machine translation system and constrain the summation to the derivations given by this system, while (Lavergne et al., 2011) use a PBT system and use the phrase table to Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 151–158. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. constrain the summation. Unfortunately, both approaches could only improve constrained systems. In (He and Deng, 2012) the authors propose to constrain the summation to 100-best lists produced by a generative machine translation system for the training corpus. Up to our knowledge He and Deng (2012) reports the first improvements over a strong baseline with a system similar to CRFs. Over the last years at the same time a different approach to speed up ME model estimation became popular for neural network (NN) language modeling (LM). In (Goodman, 2001) the usage of an intermediate variable c is proposed, called classes. They model the translation problem as a product of first translating the source sequence int"
2013.mtsummit-papers.19,H05-1064,0,0.0272766,", yi , xJ1 ) = PL J l=1 λl hl (yi−1 , yi , x1 ): p(y1I |xJ1 ) PI e =P y˜1I H(yi−1 ,yi ,xJ 1) PI i yi−1 ,xJ i=1 H(˜ 1) i=1 e (1) The feature weights λL 1 are estimated by maximization of the conditional log-likelihood L, which is commonly extended by prior distributions n − cnn |λL 1 |n , e.g. L1 c and L2 c regularpn (λL 1 2 1) ∝ e ization: L= K X k=1 log p({y I1 }k |{xJ1 }k ) − 2 X n=1 n cn ||λL 1 ||n (2) with k = 1, . . . , K summing over the training corpus and {y I1 }k the k-th reference translation. To support a latent variable, e.g. an alignment A, various authors (Quattoni et al., 2007; Koo and Collins, 2005; Yu and Lam, 2008) suggested a summation in the numerator and the denominator of equation 1: p(y1I |xJ1 ) P =P P ˜ A Ae PI H(A,yi−1 ,yi ,xJ 1) PI J ˜ e i=1 H(A,˜yi−1 ,˜yi ,x1 ) i=1 y˜1I ,I(A) (3) Three types of features were used to support the conditional probability, first source-n-gram features depending only on one target symbol yi and a combination of source symA(j)+γ bols xA(j)+γ21 relative to the currently aligned source word xA(j) (with γ1 ≤ γ2 ), with γ1 , γ2 = −5, . . . , 5, γ1 + γ2 + 1 ≤ 3, second target-n-gram features describing the relation of a consecutive set of target symbols"
2013.mtsummit-papers.19,P10-1052,0,0.0687668,"Missing"
2013.mtsummit-papers.19,W11-2168,0,0.0147825,"se based translation (PBT) system. In this case the source and target sequences are given, and the effective target vocabulary are either active or non-active alignments points p(A|y1I , xJ1 ), which is faster to compute than a sequence from Y, but reference alignments are needed, which are usually not provided with a machine translation corpus. Another approach is to manually constrain the summation to a reduced set of target sequences. Blunsom et al. (2008) propose to extend a hierarchical machine translation system and constrain the summation to the derivations given by this system, while (Lavergne et al., 2011) use a PBT system and use the phrase table to Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 151–158. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. constrain the summation. Unfortunately, both approaches could only improve constrained systems. In (He and Deng, 2012) the authors propose to constrain the summation to 100-best lists produced by a generative machine translation system for the training corpus. Up"
2013.mtsummit-papers.19,P10-2041,0,0.0111708,"e package. However, it was trained solely on the TED portion of the training data with the generative training scheme presented in (Wuebker et al., 2010) applying forced alignment. Models include translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased reordering model, an n-gram target language model and three binary count features, and a 4gram language model trained on the TED, Europarl, News-Commentary and Shuffled News corpora from the workshop. From the Shuffled News data 1/8 of the sentences were selected with the technique presented in (Moore and Lewis, 2010). 5.1 SSSD search within CRF framework Table 3 contrasts the results of the two PBT systems (line 1 and line 2) with an independent CRF translation tandem p(cI1 |xJ1 ) / p(y1I |cI1 , xJ1 ). Line 3 to 6 replaced the bigram features in model p(cI1 |xJ1 ) with a LM estimated on all available monolingual training data in IWSLT projected to classes, while line 7 and line 8 used bigram features instead of a LM, and line 9 and line 10 used both. Results were very less affected by the choice of features in p(y1I |cI1 , xJ1 ). A LM or bigram features in p(y1I |cI1 , xJ1 ) did not result in a change in"
2013.mtsummit-papers.19,2012.iwslt-evaluation.16,0,0.0207637,"Missing"
2013.mtsummit-papers.19,W95-0107,0,0.0580499,"features were used to support the conditional probability, first source-n-gram features depending only on one target symbol yi and a combination of source symA(j)+γ bols xA(j)+γ21 relative to the currently aligned source word xA(j) (with γ1 ≤ γ2 ), with γ1 , γ2 = −5, . . . , 5, γ1 + γ2 + 1 ≤ 3, second target-n-gram features describing the relation of a consecutive set of target symbols yi , yi−1 , and third word stem features, including prefixes and suffixes up to length 4 and capitalization. Lehnen et al. (2012) described the use of begin (b), continue (c), and skip (s) labels (inspired by (Ramshaw and Marcus, 1995)) for each target vocabulary word to support monotonous HCRFs (equation 3). At each source symbol the last target symbol is continued, a new one begins, or two target symbols begin. Each target symbol is labeled to make this mapping bijective. At each source symbol all aligned target words are known and at each target symbol all aligned source symbols are known. Features are applied to all combinations of source and target symbols. This modeling restricts the summation over I to I &lt; 2J. The applied CRF software was realized with weighted finite state transducers (Mohri, 2009). A chain represen"
2013.mtsummit-papers.19,P04-1007,0,0.0363719,"g will be presented. To design the tandem of CRF translation model and a phrase based baseline we will evaluate two different ways of n-best list integrations. 1 Introduction Over the last decade a variant of Maximum Entropy (ME) models for sequences, Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Hidden CRFs (HCRFs) (Quattoni et al., 2007) have shown high accuracies in various fields including part-of-speech tagging (Lafferty et al., 2001), semantic tagging (Hahn et al., 2010), chunking (Sha and Pereira, 2003), speech recognition (Zweig and Nguyen, 2009), and language modeling (Roark et al., 2004). But designing (H)CRFs for Statistical Machine Translation (SMT) seems to be a serious challenge. In SMT a sequence xJ1 = x1 , . . . , xJ composed of symbols from a large vocabulary X (of size 10k-100k) is mapped to a sequence y1I = y1 , . . . , yI composed of symbols which are from a large vocabulary Y (10k-100k). The critical points are that the vocabularies are both very large, and an alignment is seldom provided with the corpora. As CRFs include a summation over all possible target sequences (equation 1), the computational complexity of CRFs can be expressed by a polynomial of the target"
2013.mtsummit-papers.19,N03-1028,0,0.0166282,"strong baseline. Results with an independent CRF translation system and n-best list rescoring will be presented. To design the tandem of CRF translation model and a phrase based baseline we will evaluate two different ways of n-best list integrations. 1 Introduction Over the last decade a variant of Maximum Entropy (ME) models for sequences, Conditional Random Fields (CRFs) (Lafferty et al., 2001) and Hidden CRFs (HCRFs) (Quattoni et al., 2007) have shown high accuracies in various fields including part-of-speech tagging (Lafferty et al., 2001), semantic tagging (Hahn et al., 2010), chunking (Sha and Pereira, 2003), speech recognition (Zweig and Nguyen, 2009), and language modeling (Roark et al., 2004). But designing (H)CRFs for Statistical Machine Translation (SMT) seems to be a serious challenge. In SMT a sequence xJ1 = x1 , . . . , xJ composed of symbols from a large vocabulary X (of size 10k-100k) is mapped to a sequence y1I = y1 , . . . , yI composed of symbols which are from a large vocabulary Y (10k-100k). The critical points are that the vocabularies are both very large, and an alignment is seldom provided with the corpora. As CRFs include a summation over all possible target sequences (equation"
2013.mtsummit-papers.19,P10-1049,1,0.840941,"em with forced alignments only trained on the TED training data. The first phrase-based system used the SCSS software variant of the Jane software package (Wuebker et al., 2012) and made use of all available in-domain and out-domain data, part-of-speech-based adjective reordering as preprocessing step, a LM with all available monolingual training data, and a 7-gram word class language model. The second system also uses the SCSS software variant of the Jane software package. However, it was trained solely on the TED portion of the training data with the generative training scheme presented in (Wuebker et al., 2010) applying forced alignment. Models include translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased reordering model, an n-gram target language model and three binary count features, and a 4gram language model trained on the TED, Europarl, News-Commentary and Shuffled News corpora from the workshop. From the Shuffled News data 1/8 of the sentences were selected with the technique presented in (Moore and Lewis, 2010). 5.1 SSSD search within CRF framework Table 3 contrasts the results of the two PBT systems (line 1 and line 2) with an independent"
2013.mtsummit-papers.19,C12-3061,1,0.833458,"no 21.6 250 yes 21.5 60.5 62.3 24.6 25.4 55.1 55.4 9 10 bigram, with LM 250 no 21.8 250 yes 21.7 61.3 61.5 25.2 25.1 55.6 55.3 Table 3: Results with SSSD search within the CRF framework, and without using a PBT system. Language model scales and word penalty were selected to optimize BLEU on the dev set. uation. As baseline system we were provided with the best single PBT system from (Peitz et al., 2012) for English to French and a PBT system with forced alignments only trained on the TED training data. The first phrase-based system used the SCSS software variant of the Jane software package (Wuebker et al., 2012) and made use of all available in-domain and out-domain data, part-of-speech-based adjective reordering as preprocessing step, a LM with all available monolingual training data, and a 7-gram word class language model. The second system also uses the SCSS software variant of the Jane software package. However, it was trained solely on the TED portion of the training data with the generative training scheme presented in (Wuebker et al., 2010) applying forced alignment. Models include translation probabilities and lexical smoothing in both directions, word and phrase penalty, distancebased reorde"
2013.mtsummit-papers.19,W08-0303,0,0.0117087,"is seldom provided with the corpora. As CRFs include a summation over all possible target sequences (equation 1), the computational complexity of CRFs can be expressed by a polynomial of the target vocabulary size |Y |with a degree equal to the size of the feature describing the largest tuple in the target sequence (n-gram in language modeling (LM)). Authors have published approaches to move computation time to the lower degree parts of the polynomial, e.g. (Lavergne et al., 2010). However, this only changes constants in the complexity, not the overall complexity. Blunsom and Cohn (2006) and Niehues and Vogel (2008) avoid this problem by improving the alignment A used for the phrase extraction of a phrase based translation (PBT) system. In this case the source and target sequences are given, and the effective target vocabulary are either active or non-active alignments points p(A|y1I , xJ1 ), which is faster to compute than a sequence from Y, but reference alignments are needed, which are usually not provided with a machine translation corpus. Another approach is to manually constrain the summation to a reduced set of target sequences. Blunsom et al. (2008) propose to extend a hierarchical machine transl"
2013.mtsummit-papers.19,J04-4002,1,0.36621,"1 reord. PBT 2 + Nsum (including target bigram) 250 no 26.4 57.7 29.4 51.9 250 yes 27.1 56.9 30.1 51.2 Table 4: Results of N-best rescoring adding the (H)CRF scores on top of the scores in the n-best lists of the second PBT system trained on TED data. Line 1 indicates the result of the baseline system. Bold face numbers mark the best result with respect to the dev set. scores Nsum , and Nmax . Using fully normalized probabilities did not change the translation quality. On the final augmented n-best lists the weights for n-best list scores were retrained via Minimum Error Rate Training (MERT) (Och and Ney, 2004), initialized with the best weights of the n-best list generating SCSS system. Experiments have shown that the second model p(y1I |cI1 , xJ1 ) did not change the translation quality, and got a zero weight by the MERT training. The results with only the first model p(cI1 |xJ1 ) are shown in table 4 and table 5. To have a fair comparison the parameters of the baseline system (line 1) were reoptimized, too. We have marked the systems giving the best results with respect to the development set. The best systems on the development set produce the best results on the test set, but in some cases the"
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
C12-2091,N10-1033,0,\N,Missing
C12-2091,D12-1041,0,\N,Missing
C12-2091,E03-1076,0,\N,Missing
C12-2091,C08-1064,0,\N,Missing
C12-2091,P02-1040,0,\N,Missing
C12-2091,W10-1738,1,\N,Missing
C12-2091,P10-1049,1,\N,Missing
C12-2091,W06-3123,0,\N,Missing
C12-2091,2006.amta-papers.2,0,\N,Missing
C12-2091,D12-1089,0,\N,Missing
C12-2091,C10-2021,0,\N,Missing
C12-2091,W06-3105,0,\N,Missing
C12-2091,P05-1033,0,\N,Missing
C12-2091,J03-1002,1,\N,Missing
C12-2091,2009.iwslt-papers.2,0,\N,Missing
C12-2091,P07-1019,0,\N,Missing
C12-2091,2010.iwslt-papers.11,1,\N,Missing
C12-2091,D08-1076,0,\N,Missing
C12-2091,P08-1024,0,\N,Missing
C12-3061,J99-4005,0,\N,Missing
C12-3061,N10-1140,0,\N,Missing
C12-3061,D09-1022,1,\N,Missing
C12-3061,C04-1030,1,\N,Missing
C12-3061,W10-1738,1,\N,Missing
C12-3061,D08-1024,0,\N,Missing
C12-3061,P12-3004,0,\N,Missing
C12-3061,N09-1027,0,\N,Missing
C12-3061,P10-1049,1,\N,Missing
C12-3061,P07-2045,0,\N,Missing
C12-3061,N09-1025,0,\N,Missing
C12-3061,J06-4004,0,\N,Missing
C12-3061,N03-1017,0,\N,Missing
C12-3061,P02-1038,1,\N,Missing
C12-3061,2008.iwslt-papers.8,1,\N,Missing
C12-3061,P10-4002,0,\N,Missing
C12-3061,2008.iwslt-papers.7,1,\N,Missing
C12-3061,P07-1019,0,\N,Missing
C12-3061,P12-2006,1,\N,Missing
C12-3061,W06-3119,0,\N,Missing
C12-3061,2010.iwslt-papers.18,1,\N,Missing
C12-3061,2011.iwslt-papers.8,1,\N,Missing
C12-3061,J07-2003,0,\N,Missing
C12-3061,N10-2003,0,\N,Missing
C12-3061,D07-1080,0,\N,Missing
C12-3061,2009.eamt-1.33,1,\N,Missing
C12-3061,P03-1021,0,\N,Missing
C12-3061,2012.eamt-1.66,1,\N,Missing
D13-1138,E99-1010,0,\N,Missing
D13-1138,D08-1089,0,\N,Missing
D13-1138,P02-1040,0,\N,Missing
D13-1138,W10-1738,1,\N,Missing
D13-1138,D07-1091,0,\N,Missing
D13-1138,W12-3125,0,\N,Missing
D13-1138,J04-4002,1,\N,Missing
D13-1138,N03-1017,0,\N,Missing
D13-1138,J03-1002,1,\N,Missing
D13-1138,C12-3061,1,\N,Missing
D13-1138,2010.iwslt-evaluation.11,0,\N,Missing
D13-1138,W04-3250,0,\N,Missing
D13-1138,P03-1021,0,\N,Missing
D13-1138,N13-1003,0,\N,Missing
D19-1453,W18-6318,0,0.112621,"e Inc. {sarthak garg, speitz, udhay, mpaulik}@apple.com Abstract might be helpful for translation. The presence of multi-layer, multi-head attention mechanisms in the Transformer model further complicate interpreting the attention probabilities and extracting high quality discrete alignments from them. Finding source to target word alignments has many applications in the context of MT. A straightforward application of word alignments is to generate bilingual lexica from parallel corpora. Word alignments have also been used for external dictionary assisted translation (Chatterjee et al., 2017; Alkhouli et al., 2018; Arthur et al., 2016) to improve translation of low frequency words or to comply with certain terminology guidelines. Documents and webpages often contain word annotations such as formatting styles and hyperlinks, which need to be preserved in the translation. In such cases, word alignments can be used to transfer these annotations from the source sentence to its translation. In user facing translation services, providing word alignments as additional information to the users might improve their trust and confidence, and also help them to diagnose problems such as under-translation (Tu et al."
D19-1453,W17-4711,0,0.471852,"Missing"
D19-1453,D16-1162,0,0.0490282,"peitz, udhay, mpaulik}@apple.com Abstract might be helpful for translation. The presence of multi-layer, multi-head attention mechanisms in the Transformer model further complicate interpreting the attention probabilities and extracting high quality discrete alignments from them. Finding source to target word alignments has many applications in the context of MT. A straightforward application of word alignments is to generate bilingual lexica from parallel corpora. Word alignments have also been used for external dictionary assisted translation (Chatterjee et al., 2017; Alkhouli et al., 2018; Arthur et al., 2016) to improve translation of low frequency words or to comply with certain terminology guidelines. Documents and webpages often contain word annotations such as formatting styles and hyperlinks, which need to be preserved in the translation. In such cases, word alignments can be used to transfer these annotations from the source sentence to its translation. In user facing translation services, providing word alignments as additional information to the users might improve their trust and confidence, and also help them to diagnose problems such as under-translation (Tu et al., 2016). In this work,"
D19-1453,J93-2003,0,0.113566,"need to half the batch size, increase the number of updates accordingly and adapt the learning rate to 7e-4. We average over the last 10 checkpoints and run inference with a beam size of 5. To fairly compare against state-of-the-art translation setups, we compute B LEU (Papineni et al., 2002) with sacreBLEU (Post, 2018). 5.2 Statistical Baseline For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit5 implemented by Gao and Vogel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vogel et al., 1996), respectively. In particular, we perform 5 iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and symmetrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4. 5.3 Averaging Attention Results For our experiments, we use the data and Transformer model setup described in Section 5.1.1. We perform the evaluation of alignments obtained by layer wise averaging of attention probabilities"
D19-1453,W17-4716,0,0.0237729,"samy Matthias Paulik Apple Inc. {sarthak garg, speitz, udhay, mpaulik}@apple.com Abstract might be helpful for translation. The presence of multi-layer, multi-head attention mechanisms in the Transformer model further complicate interpreting the attention probabilities and extracting high quality discrete alignments from them. Finding source to target word alignments has many applications in the context of MT. A straightforward application of word alignments is to generate bilingual lexica from parallel corpora. Word alignments have also been used for external dictionary assisted translation (Chatterjee et al., 2017; Alkhouli et al., 2018; Arthur et al., 2016) to improve translation of low frequency words or to comply with certain terminology guidelines. Documents and webpages often contain word annotations such as formatting styles and hyperlinks, which need to be preserved in the translation. In such cases, word alignments can be used to transfer these annotations from the source sentence to its translation. In user facing translation services, providing word alignments as additional information to the users might improve their trust and confidence, and also help them to diagnose problems such as under"
D19-1453,2016.amta-researchers.10,0,0.178555,"Missing"
D19-1453,D18-1045,0,0.0177306,"armup over the first 4000 steps and inverse square root as learning rate scheduler. The dropout probability is set to 0.1. Additionally, we apply label smoothing with a factor of 0.1. To conveniently extract word alignments for both translation directions, we train bidirectional models, i.e. our models are able to translate and align from Romanian to English and vice versa. 5.1.2 Align and Translate Task The second setup is based on the WMT‘18 English-German news translation task (Bojar et al., 2018). We apply the same corpus selection for bilingual data and model architecture as suggested by Edunov et al. (2018). However, we slightly modify the preprocessing pipeline to be able to evaluate the alignment quality against the gold alignments provided by Vilar et al. (2006). We use all available bilingual data (Europarl v7, Common Crawl corpus, News Commentary v13 and Rapid corpus of EU press releases) excluding the ParalCrawl corpus. We remove sentences longer than 100 words and sentence pairs with a source/target length ratio exceeding 1.5. This results in 5.2M parallel sentences. We apply the Moses tokenizer (Koehn et al., 2007) without aggressive hyphen splitting and without performing HTML escaping"
D19-1453,W08-0509,0,0.116761,"erparameters are as described in the previous section. Since training the multi-task models consumes more memory, we need to half the batch size, increase the number of updates accordingly and adapt the learning rate to 7e-4. We average over the last 10 checkpoints and run inference with a beam size of 5. To fairly compare against state-of-the-art translation setups, we compute B LEU (Papineni et al., 2002) with sacreBLEU (Post, 2018). 5.2 Statistical Baseline For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit5 implemented by Gao and Vogel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vogel et al., 1996), respectively. In particular, we perform 5 iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and symmetrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4. 5.3 Averaging Attention Results For our experiments, we use the data and Transformer model setup described in"
D19-1453,I17-1004,0,0.0530109,"owed that adding linguistic information from parse trees into one of the attention heads of the transformer model can help in the semantic role labeling. Inspired by Strubell et al. (2018), we inject the alignment information through one of the attention heads for the translation task instead. As a by-product of developing our model, we present a simple way to quantitatively evaluate and analyze the quality of attention probabilities learnt by different parts of the Transformer model with respect to modeling alignments, which contributes to previous work on understanding attention mechanisms (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018). 8 Conclusions This paper addresses the task of jointly learning to produce translations and alignments with a single Transformer model. By using a multi-task objective along with providing full target sentence context to our alignment module, we are able to produce better alignments than previous approaches not relying on external alignment toolkits. We demonstrate that our framework can be extended to use external alignments from G IZA ++ to achieve significantly better alignment results compared to G IZA ++, while maintaining the same 4460"
D19-1453,2005.iwslt-1.8,0,0.490834,"guided alignment training by feeding the current target word to the attention module, providing it more context about the target sentence. 4454 Zenkel et al. (2019) proposed an method that does not rely on alignments from external toolkits for training. They instead add an extra attention layer on top of the Transformer architecture and directly optimize its activations towards predicting the given target word. All the above methods involve training models for both the directions to get bidirectional alignments. These bidirectional alignments are then merged using the grow diagonal heuristic (Koehn et al., 2005). 4 4.1 Proposed Method Averaging Layer-wise Attention Scores The attention heads in a single layer are symmetrical, but the different layers themselves can learn drastically different alignments. To better understand the behavior of the encoder-decoder attention learnt at different layers, we average the attention matrices computed across all heads within each layer and evaluate the obtained alignments. We show that the attention probabilities from the penultimate layer naturally tend to learn alignments and provide significantly better results compared to naively averaging across all layers"
D19-1453,P07-2045,0,0.0120643,"rpus selection for bilingual data and model architecture as suggested by Edunov et al. (2018). However, we slightly modify the preprocessing pipeline to be able to evaluate the alignment quality against the gold alignments provided by Vilar et al. (2006). We use all available bilingual data (Europarl v7, Common Crawl corpus, News Commentary v13 and Rapid corpus of EU press releases) excluding the ParalCrawl corpus. We remove sentences longer than 100 words and sentence pairs with a source/target length ratio exceeding 1.5. This results in 5.2M parallel sentences. We apply the Moses tokenizer (Koehn et al., 2007) without aggressive hyphen splitting and without performing HTML escaping of apostrophes and quotes. Furthermore, we do not normalize punctuation marks. We use newstest2012 as validation and newstest2014 as test set. To achieve state-of-the-art translation results, all models in this setup are trained unidirectional and we change to the big transformer configuration with an embedding size of 1024 and 16 attention heads. The total number of parameters is 213M. We train the layer average baseline with a batch size of 7168 tokens on 64 Volta GPUs for 30k updates and apply a learning rate of 1e3,"
D19-1453,W17-3204,0,0.0299411,"we report significantly better alignment accuracies compared to GIZA++ on three publicly available data sets. Our implementation has been open-sourced1 . 1 Introduction Neural machine translation (NMT) constitutes the state of the art in MT, with the Transformer model architecture (Vaswani et al., 2017) beating other neural architectures in competitive MT evaluations. The attention mechanism used in NMT models was motivated by the need to model word alignments, however it is now well known that the attention probabilities can differ significantly from word alignments in the traditional sense (Koehn and Knowles, 2017), since attending to the context words rather than the aligned source words 1 Code can be found at https://github.com/ pytorch/fairseq/pull/1095 • We use a multi-task loss function combining negative log likelihood (NLL) loss used in regular NMT model training and an alignment loss supervising one attention head to learn alignments (Section 4.2). • Conditioning on past target context is essential for maintaining the auto-regressive property for translation but can be limiting for alignment. We alleviate this problem by conditioning the different components of our multi-task objective on differ"
D19-1453,N18-1125,0,0.134283,"2, the alignment head is trained to model the alignment distribution for the ith target token given only the past target tokens and all source tokens. Since the alignment head does not know the identity of the next target token, it becomes difficult for it to learn this token’s alignment to the source tokens. Previous work has also identified this problem and alleviate 4455 it by feeding the target token to be aligned as an input to the module computing the alignment (Peter et al., 2017), or forcing the module to predict the target token (Zenkel et al., 2019) or its properties, e.g. POS tags (Li et al., 2018). Feeding the next target token assumes that we know it in advance and thus calls for separate translation and alignment models. Forcing the alignment module to predict target token’s properties helps but still passes the information of the target token in an indirect manner. We overcome these limitations by conditioning the two components of our loss function on different amounts of context. The NLL loss Lt is conditioned on the past target tokens to preserve the auto-regressive property: I 1X Lt = − log(p(ei |f1J , ei−1 1 )). I (9) i=1 0 However, the alignment loss La is now conditioned on t"
D19-1453,C16-1291,0,0.139847,"Missing"
D19-1453,D16-1249,0,0.0252646,"Missing"
D19-1453,W03-0301,0,0.434332,"layer average baseline for supervising the alignment head. 5.1.1 Alignment Task The purpose of the this task is to fairly compare with state-of-the-art results in terms of alignment quality and perform a hyperparameter search. We use the same experimental setup as described in (Zenkel et al., 2019). The authors provide pre-processing and scoring scripts2 for three different datasets: Romanian→English, English→French and German→English. Training data and test data for Romanian→English and English→French are provided by the NAACL’03 Building and Using Parallel Texts word alignment shared task3 (Mihalcea and Pedersen, 2003). The Romanian→English training data are augmented by the Europarl v8 corpus increasing the amount of parallel sentences from 49k to 0.4M. For German→English we use the Europarl v7 corpus as training data and the gold alignments4 provided by Vilar et al. (2006). The reference alignments were created by randomly selecting a subset of the Europarl v7 corpus and manually annotating them following the guidelines suggested in (Och 2 https://github.com/lilt/ alignment-scripts 3 http://web.eecs.umich.edu/˜mihalcea/ wpt/index.html#resources 4 https://www-i6.informatik. rwth-aachen.de/goldAlignment/ 44"
D19-1453,P00-1056,0,0.824934,"Missing"
D19-1453,J03-1002,0,0.347931,"cal Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4453–4462, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics • We demonstrate that the system can be supervised using seed alignments obtained by carefully averaging the attention probabilities of a regular NMT model (Section 4.1) or alignments obtained from statistical alignment tools (Section 4.4) We show that our model outperforms previous neural approaches (Peter et al., 2017; Zenkel et al., 2019) and statistical alignment models (Och and Ney, 2003) in terms of alignment accuracy without suffering any degradation of translation accuracy. 2 Preliminaries 2.1 Word Alignment Task Given a sentence f1J = f1 , . . . , fj , . . . fJ in the source language and its translation eI1 = e1 , . . . , ei , . . . eI in the target language, an alignment A is defined as a subset of the Cartesian product of the word positions (Och and Ney, 2003). A ⊆ {(j, i) : j = 1, . . . , J; i = 1, . . . , I} (1) The word alignment task aims to find a discrete alignment representing a many-to-many mapping from the source words to their corresponding translations in the"
D19-1453,P02-1040,0,0.105243,"M. We train the layer average baseline with a batch size of 7168 tokens on 64 Volta GPUs for 30k updates and apply a learning rate of 1e3, β1 = 0.9, β2 = 0.98. The dropout probability is set to 0.3. All other hyperparameters are as described in the previous section. Since training the multi-task models consumes more memory, we need to half the batch size, increase the number of updates accordingly and adapt the learning rate to 7e-4. We average over the last 10 checkpoints and run inference with a beam size of 5. To fairly compare against state-of-the-art translation setups, we compute B LEU (Papineni et al., 2002) with sacreBLEU (Post, 2018). 5.2 Statistical Baseline For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit5 implemented by Gao and Vogel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vogel et al., 1996), respectively. In particular, we perform 5 iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and symmetrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use th"
D19-1453,W18-6304,0,0.0828091,"erfahrensvoraussetzungen (procedural criteria) in one of the preceding sentences and refers later to them by using the term Voraussetzungen (criteria). In the second example, the pronoun you is correctly aligned to the noun Haus 4459 (house) which is just another way to address the audience in the European parliament. Both alignment links are not generated by G IZA ++. This could be related to fact that a statistical model is based on counting co-occurrences. We speculate that to generate such alignment links, a model needs to be able to encode contextual information. Experimental results in (Tang et al., 2018) suggest that NMT models learn to encode contextual information, which seems to be necessary for word sense disambiguation. Since pronouns can be ambiguous references, we assume that both problems are closely related and therefore believe that the ability to encode contextual information may be beneficial for generating word alignments. From our experiments on the WMT’18 dataset, we observe that the alignment quality of the layer average baseline is quite low (cf. Table 4). To further investigate this, we plot the test A ER and the validation NLL loss per epoch (Figure 2). The graph shows that"
D19-1453,P16-1008,0,0.0330672,"al., 2018; Arthur et al., 2016) to improve translation of low frequency words or to comply with certain terminology guidelines. Documents and webpages often contain word annotations such as formatting styles and hyperlinks, which need to be preserved in the translation. In such cases, word alignments can be used to transfer these annotations from the source sentence to its translation. In user facing translation services, providing word alignments as additional information to the users might improve their trust and confidence, and also help them to diagnose problems such as under-translation (Tu et al., 2016). In this work, we introduce an approach that teaches Transformer models to produce translations and interpretable alignments simultaneously: The state of the art in machine translation (MT) is governed by neural approaches, which typically provide superior translation accuracy over statistical approaches. However, on the closely related task of word alignment, traditional statistical word alignment models often remain the go-to solution. In this paper, we present an approach to train a Transformer model to produce both accurate translations and alignments. We extract discrete alignments from"
D19-1453,2006.iwslt-papers.7,0,0.316787,"Missing"
D19-1453,C96-2141,0,0.883967,"ze, increase the number of updates accordingly and adapt the learning rate to 7e-4. We average over the last 10 checkpoints and run inference with a beam size of 5. To fairly compare against state-of-the-art translation setups, we compute B LEU (Papineni et al., 2002) with sacreBLEU (Post, 2018). 5.2 Statistical Baseline For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit5 implemented by Gao and Vogel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vogel et al., 1996), respectively. In particular, we perform 5 iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and symmetrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments to supervise the alignment loss for the method described in Section 4.4. 5.3 Averaging Attention Results For our experiments, we use the data and Transformer model setup described in Section 5.1.1. We perform the evaluation of alignments obtained by layer wise averaging of attention probabilities as described in Section 4"
D19-1453,W18-6319,0,0.027238,"with a batch size of 7168 tokens on 64 Volta GPUs for 30k updates and apply a learning rate of 1e3, β1 = 0.9, β2 = 0.98. The dropout probability is set to 0.3. All other hyperparameters are as described in the previous section. Since training the multi-task models consumes more memory, we need to half the batch size, increase the number of updates accordingly and adapt the learning rate to 7e-4. We average over the last 10 checkpoints and run inference with a beam size of 5. To fairly compare against state-of-the-art translation setups, we compute B LEU (Papineni et al., 2002) with sacreBLEU (Post, 2018). 5.2 Statistical Baseline For both setups, the statistical alignment models are computed with the multi-threaded version of the G IZA ++ toolkit5 implemented by Gao and Vogel (2008). G IZA ++ estimates IBM1-5 models and a first-order hidden Markov model (HMM) as introduced in (Brown et al., 1993) and (Vogel et al., 1996), respectively. In particular, we perform 5 iterations of IBM1, HMM, IBM3 and IBM4. Furthermore, the alignment models are trained in both translation directions and symmetrized by employing the grow-diagonal heuristic (Koehn et al., 2005). We use the resulting word alignments"
D19-1453,E17-2025,0,0.0298444,"/˜mihalcea/ wpt/index.html#resources 4 https://www-i6.informatik. rwth-aachen.de/goldAlignment/ 4456 and Ney, 2003). Data statistics are shown in Table 1. Table 1: Number of sentences for three datasets: German→English (DeEn), Romanian→English (RoEn) and English→French (EnFr). The datasets include training data and test data with gold alignments. training test DeEn 1.9M 508 RoEn 0.5k 248 EnFr 1.1M 447 In all experiments for this task, we employ the base transformer configuration with an embedding size of 512, 6 encoder and decoder layers, 8 attention heads, shared input and output embeddings (Press and Wolf, 2017), the standard relu activation function and sinusoidal positional embedding. The total number of parameters is 60M. We train with a batch size of 2000 tokens on 8 Volta GPUs and use the validation translation loss for early stopping. Furthermore, we use Adam optimizer (Loshchilov and Hutter, 2019) with a learning rate of 3e-4, β1 = 0.9, β2 = 0.98, learning rate warmup over the first 4000 steps and inverse square root as learning rate scheduler. The dropout probability is set to 0.1. Additionally, we apply label smoothing with a factor of 0.1. To conveniently extract word alignments for both tr"
D19-1453,W18-5431,0,0.0275507,"stic information from parse trees into one of the attention heads of the transformer model can help in the semantic role labeling. Inspired by Strubell et al. (2018), we inject the alignment information through one of the attention heads for the translation task instead. As a by-product of developing our model, we present a simple way to quantitatively evaluate and analyze the quality of attention probabilities learnt by different parts of the Transformer model with respect to modeling alignments, which contributes to previous work on understanding attention mechanisms (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018). 8 Conclusions This paper addresses the task of jointly learning to produce translations and alignments with a single Transformer model. By using a multi-task objective along with providing full target sentence context to our alignment module, we are able to produce better alignments than previous approaches not relying on external alignment toolkits. We demonstrate that our framework can be extended to use external alignments from G IZA ++ to achieve significantly better alignment results compared to G IZA ++, while maintaining the same 4460 translation performance. Curre"
D19-1453,P16-1162,0,0.353521,"Missing"
D19-1453,D18-1548,0,0.0340149,"t the to-be-aligned target word. Expanding on this idea, we propose to leverage the full target sentence context leading to A ER improvements. Zenkel et al. (2019) presents an approach that eliminates the reliance on statistical word aligners by instead by directly optimizing the attention activations for predicting the target word. We empirically compare our approach of obtaining high quality alignments without the need of statistical word aligners to Zenkel et al. (2019). Augmenting the task objective with linguistic information, such as word alignments, also has had applications beyond MT. Strubell et al. (2018) showed that adding linguistic information from parse trees into one of the attention heads of the transformer model can help in the semantic role labeling. Inspired by Strubell et al. (2018), we inject the alignment information through one of the attention heads for the translation task instead. As a by-product of developing our model, we present a simple way to quantitatively evaluate and analyze the quality of attention probabilities learnt by different parts of the Transformer model with respect to modeling alignments, which contributes to previous work on understanding attention mechanism"
E14-4034,2013.iwslt-evaluation.1,0,0.0314032,"Missing"
E14-4034,N03-1017,0,0.062816,"lities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for 2 Hierarchi"
E14-4034,P10-2041,0,0.0570292,"Missing"
E14-4034,W05-1506,0,0.0458876,"cess as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are The translation probabilities are computed in source-to-target as well as in target-to-source direction. In the translation processes, these probabilities are integrated in the log-linear combination among other models such as a language model, word lexicon models, word and phrase penalty and binary features marking hierarchical phrases, glue rule and rules with non-terminals at the boundaries. The translation process of hierarchical"
E14-4034,J03-1002,1,0.0201937,"he source and target sentences. This is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source se"
E14-4034,P05-1033,0,0.105325,"approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and constraining the translations to the corresponding target sentences, k-best segmentations are produced. Then, the phrases used for 2 Hierarchical Phrase-based Translation In hierarchical phrase-based translation (Chiang, 2005), discontinuous phrases with “gaps” are allowed. The translation model is formalized as a synchronous context-free grammar (SCFG) 174 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 174–179, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics and consists of bilingual rules, which are based on bilingual standard phrases and discontinuous phrases. Each bilingual rule rewrites a generic non-terminal X into a pair of strings f˜ and e˜ with both terminals and non-terminals in both languages X → hf"
E14-4034,P03-1021,0,0.108177,"al X. With these hierarchical phrases we can define the hierarchical rules in the SCFG. The rule probabilities which are in general defined as relative frequencies are computed based on the joint counts C(X → hf˜, e˜i) of a bilingual rule X → hf˜, e˜i C(X → hf˜, e˜i) pH (f˜|˜ e) = P . ˜0 ˜i) f˜0 C(X → hf , e 3 Translation Model Training We propose following pipeline for consistent hierarchical phrase-based training: First we train a word alignment, from which the baseline translation model is extracted as described in the previous section. The log-linear parameter weights are tuned with MERT (Och, 2003) on a development set to produce the baseline system. Next, we perform decoding on the training data. As the translations are constrained to the given target sentences, we name this step forced decoding in the following. Details are given in the next subsection. Given the counts CF D (X → hf˜, e˜i) of the rules, which have been applied in the forced decoding step, the translation probabilities pF D (f˜|˜ e) for the translation model are recomputed: CF D (X → hf˜, e˜i) pF D (f˜|˜ e) = P . ˜0 ˜i) f˜0 CF D (X → hf , e (3) Finally, using the translation model with the reestimated probabilities, we"
E14-4034,J07-2003,0,0.0709671,"ge part of the SCFG. In this work, we perform this step with a modified version of the CYK+ algorithm (Chappelier and Rajman, 1998). The output of this algorithm is a hypergraph, which represents all possible derivations of the input sentence. A derivation represents an application of rules from the grammar to generate the given input sentence. Using the the associated target part of the applied rule, for each derivation a translation can be constructed. In a second step, the language model score is incorporated. Given the hypergraph, this is done with the cube pruning algorithm presented in (Chiang, 2007). 175 included (except for the language model). Further, leave-one-out is applied to counteract overfitting. Note, that the model weights of the baseline system are used to perform forced decoding. Finally, we extract and count the rules which have been applied in the derivations. These counts are used to recompute the translation probabilities. 3.2 Sentences Run. Words Vocabulary It is focusing the translation of TED talks. Bilingual data statistics are given in Table 1. The baseline system was trained on all available bilingual data and used a 4-gram LM with modified KneserNey smoothing (Kne"
E14-4034,2001.mtsummit-papers.68,0,0.0327359,"Missing"
E14-4034,P11-2031,0,0.0541754,"Missing"
E14-4034,2006.amta-papers.25,0,0.0429498,"Missing"
E14-4034,N13-1073,0,0.0282672,"is is done by synchronous parsing the given sentence pairs. After extracting k-best derivations, we reestimate the translation model probabilities based on collected rule counts. We show the effectiveness of our procedure on the IWSLT German→English and English→French translation tasks. Our results show improvements of up to 1.6 points B LEU. 1 Introduction In state of the art statistical machine translation systems, the translation model is estimated by following heuristic: Given bilingual training data, a word alignment is trained with tools such as GIZA++ (Och and Ney, 2003) or fast align (Dyer et al., 2013). Then, all valid translation pairs are extracted and the translation probabilities are computed as relative frequencies (Koehn et al., 2003). However, this extraction method causes several problems. First, this approach does not consider, whether a translation pair is extracted from a likely alignment or not. Further, during the extraction process, models employed in decoding are not considered. For phrase-based translation, a successful approach addressing these issues is presented in (Wuebker et al., 2010). By applying a phrasebased decoder on the source sentences of the training data and c"
E14-4034,N10-1033,0,0.0224717,"detail. Given a sentence pair of the training data, we constrain the translation of the source sentence to produce the corresponding target sentence. For this constrained decoding process, the language model score is constant as the translation is fixed. Hence, the incorporation of the a language model is not needed. This results in a simplification of the decoding process as we do not have to employ the cube pruning algorithm as described in the previous section. Consequently, forced decoding for hierarchical phrase-based translation is equivalent to synchronous parsing of the training data. Dyer (2010) has described an approach to reduce the average-case run-time of synchronous parsing by splitting one bilingual parse into two successive monolingual parses. We adopt this method and first parse the source sentence and then the target sentence with CYK+. If the given sentence pair has been parsed successfully, we employ a top-down k-best parsing algorithm (Chiang and Huang, 2005) on the resulting hypergraph to find the k-best derivations between the given source and target sentence. In this step, all models of the translation process are The translation probabilities are computed in source-to"
E14-4034,E14-2008,1,0.882213,"Missing"
E14-4034,W10-1738,1,0.904678,"Missing"
E14-4034,P02-1040,0,\N,Missing
E14-4034,P10-1049,1,\N,Missing
E14-4034,W13-0804,1,\N,Missing
E14-4034,C12-3061,1,\N,Missing
N15-1175,D14-1132,0,0.555118,"being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North A"
N15-1175,P08-1024,0,0.0212667,"all development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weig"
N15-1175,J93-2003,0,0.0601724,"performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature. 1 Introduction The main advantage of learning parameters in a discriminative fashion is the possibility to directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its"
N15-1175,J04-2004,0,0.0219315,"errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weights. Model updates are carried out by the RPROP algorithm (Riedmiller and Braun, 1993). However, both approaches only improve over constrained baselines. Our work is inspired by (He and Deng, 2012; Setiawan and Zhou, 2013), where the authors propose to train the standard phrasal and lexical channel models with the growth transformation (GT) algorithm. They use n-best lists on the training data and optimize a maximum expected B LEU objective, that provides a cle"
N15-1175,D08-1024,0,0.0301702,"m expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying trans"
N15-1175,P05-1033,0,0.0663444,"training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear feature weights λm , the translation decoder ˆ searches for the b"
N15-1175,P11-2031,0,0.0801903,"s for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). The 5-gram backoff LM is in total trained on 2.9 billion running words. We use the same data for tuning and testing as Setiawan and Zhou (2013), na"
N15-1175,D08-1089,0,0.444835,"apply leave-one-out with all update strategies. 5.3 Features Maximum expected B LEU training facilitates training of arbitrary features. In this work we apply four types of features. (a) A discriminative phrase table, i.e. one feature for each phrase pair. (b) Lexical features, i.e. one feature for each source-target word pair that appear within the same phrase. (c) Source and target triplet features (Hasan et al., 2008), i.e. triples of one source and two target words or one target and two source words appearing within a single phrase pair. (d) The hierarchical lexicalized reordering model (Galley and Manning, 2008), i.e. one feature for each combination of phrase pair, orientation (monotone (M), swap (S) or discontinuous (D)) and orientation direction (forward or backward). GT is only applied with feature set (a), where we reestimate the two phrasal channel models as was done in (He and Deng, 2012). With the other update algorithms we follow the approach taken in (Auli et al., 2014) and condense each feature type into a small number of models for the log-linear combination, which is afterwards tuned with MERT. (a) and (b) result in a single additional model, (c) in two models (source and target triplets"
N15-1175,N13-1048,0,0.0164076,"ization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the standard phrasebased model. Gao and He (2013) use gradient ascent to train Markov random field models for phrase translation. These models are interpreted as undirected phrase compatibility scores rather than translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training with joint feature selection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just"
N15-1175,P13-1031,0,0.163139,"contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages"
N15-1175,W14-3360,0,0.0118293,"translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training with joint feature selection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just using a small development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used"
N15-1175,D08-1039,1,0.869947,"Missing"
N15-1175,P12-1031,0,0.0870934,"o directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine translation (SMT), extending the generative noisy-channel formulation (Brown et al., 1993) as a discriminative, log-linear 1. We propose to apply the RPROP algorithm for maximum expected B LEU training and perform an experimental comparison with growth transformation (GT) (He and Deng, 2012; Setiawan and Zhou, 2013), stochastic gradient descent (Auli et al., 2014) and AdaGrad (Green et al., 2013). RPROP yields superior performance, reaching a total improvement of 1.2 B LEU points over our IWSLT German→English baseline using 5.22M features. 2. In terms of time and memory efficiency, RPROP clearly outperforms GT. The latter needs to update a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence"
N15-1175,P13-2121,0,0.0909589,"Missing"
N15-1175,D11-1125,0,0.0240956,"rithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the"
N15-1175,W13-2258,1,0.854976,"Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). The 5-gram backoff LM is in total trained on 2.9 billion running words. We use the same data for tuning and testing as Setiawan and Zhou (2013), namely 1275 (tune) and 12393 sentences of web data taken from LDC2010E30, the NIST MT06 evaluation set and an additional single-reference test set from the discussion forum (df) domain containing 1124 sentence pairs. Maximum expected B LEU training is performed on the discussion forum portion of the training data, consisting of 67.8K sentence pairs. On the German→English task of the 9th Workshop on Statistical Machine Translation4 , both translation"
N15-1175,N07-1008,0,0.0157828,"ll development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model and the underlying translation paradigm differs from the standard phrasebased model. Gao and He (2013) use gradient ascent to train Markov random field models for phrase translation. These models are interpreted as undirected phrase compatibility scores rather than translation probabilities. Thus, as in our work, they are not subject to a sum-to-one constraint. Simianer et al. (2012) propose a distributed setup for large-scale discriminative training"
N15-1175,N03-1017,0,0.0329409,"tal comparison. (ii) For the first time, we apply maximum expected B LEU training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear"
N15-1175,W11-2168,0,0.073415,"le training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of LBFGS. In a similar way Lavergne et al. (2011) use the n-gram based approach (Casacuberta and Vidal, 2004; Mari˜no et al., 2006) to model the reordering, phrase alignment, and the language model. A CRF is applied to estimate the phrase weights. Model updates are carried out by the RPROP algorithm (Riedmiller and Braun, 1993). However, both approaches only improve over constrained baselines. Our work is inspired by (He and Deng, 2012; Setiawan and Zhou, 2013), where the authors propose to train the standard phrasal and lexical channel models with the growth transformation (GT) algorithm. They use n-best lists on the training data and optim"
N15-1175,P06-1096,0,0.0900743,"Missing"
N15-1175,J06-4004,0,0.067937,"Missing"
N15-1175,P10-2041,0,0.0520213,"rpora as well as selected parts of the Shuffled News 1 Note that we keep the λ weights fixed throughout all iterations of maximum expected B LEU training. 2 http://www.iwslt2013.org Sentences Run. Words Vocabulary IWSLT German English BOLT Chinese English WMT German English 138K 2.63M 2.70M 75.4K 50.2K 4.08M 78.3M 85.9M 384K 817K 4.09M 105M 104M 659K 649K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system a"
N15-1175,J04-4002,1,0.470015,"paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where E = e1 , . . . , eI denotes the translation hypothesis, F = f1 , . . . , fJ the source sentence, m a model index, and Θ the model parameters. These models include the phrase translation and lexical smoothing scores in both directions, language model (LM) score, distortion penalty, word penalty and phrase penalty (Och and Ney, 2004). Given a source sentence F , the models hm,Θ (E, F ) and the corresponding loglinear feature weights λm , the translation decoder ˆ searches for the best scoring translation E: ˆ = arg max {fΘ (E, F )} E E X fΘ (E, F ) = λm hm,Θ (E, F ) (1) (2) m∈M plied and for simplicity, in the following we will assume the particular derivation for a translation hypothesis to be included in the variable E. The loglinear feature weights are optimized with minimum error rate training (MERT) (Och, 2003). 4 4.1 Update Strategies Previously Proposed Algorithms The Growth Transformation (GT) or Extended Baum-Wel"
N15-1175,P03-1021,0,0.417489,"ur experiments also prove that leave-one-out impacts translation quality. This paper is organized as follows. We review related work in Section 2 and present the translation system in Section 3. In Section 4 we describe the different discriminative update strategies applied in this work and Section 5 derives the complete maximum expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one m"
N15-1175,P02-1040,0,0.0967528,"Missing"
N15-1175,N13-1034,0,0.194761,"exible and efficient discriminative training approach for statistical machine translation. We propose to use the RPROP algorithm for optimizing a maximum expected B LEU objective and experimentally compare it to several other updating schemes. It proves to be more efficient and effective than the previously proposed growth transformation technique and also yields better results than stochastic gradient descent and AdaGrad. We also report strong empirical results on two large scale tasks, namely BOLT Chinese→English and WMT German→English, where our final systems outperform results reported by Setiawan and Zhou (2013) and on matrix.statmt.org. On the WMT task, discriminative training is performed on the full training data of 4M sentence pairs, which is unsurpassed in the literature. 1 Introduction The main advantage of learning parameters in a discriminative fashion is the possibility to directly optimize towards a quality or error measure on the task that is being performed. This stands in contrast to the generative approach, where parameters are chosen to maximize likelihood under a generative story, which often bears little correspondence with the actual application of the model. In statistical machine"
N15-1175,P12-1002,0,0.078287,"ate a much larger number of features due to its renormalization component. On the IWSLT data, RPROP is 6.4 times faster than GT and requires a third of the memory. 3. On the WMT German→English task, we perform discriminative training on 4M sentence 1516 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1516–1526, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics pairs, which, to the best of our knowledge, is 2.4 times the size of the largest training set reported in previous work (1.66M sentences in (Simianer et al., 2012)). This proves the scalability of our approach. 4. On two large scale tasks our experiments show good improvements over strong baselines which include recurrent language modeling components. On the Chinese→English DARPA BOLT task, we achieve nearly twice the improvement reported in (Setiawan and Zhou, 2013) on the same test sets which results in a superior final system. Finally, the best single system reported on matrix.statmt.org is outperformed by 0.8 B LEU points on the WMT German→English newstest2013 set. Our experiments also prove that leave-one-out impacts translation quality. This paper"
N15-1175,W10-1738,1,0.892163,"Missing"
N15-1175,D07-1080,0,0.0269347,"ves the complete maximum expected B LEU training algorithm. Finally, experimental results are given in Section 6 and we conclude with Section 7. 2 Related Work Discriminative training is one of the most active research areas in SMT and it can be integrated into the pipeline at various stages. Och (2003) proposed to apply minimum error rate training (MERT) to optimize the different feature weights in the log-linear model combination on a small development data set. This is still considered to be the state of the art, but is only capable of optimizing a handful of features. More recently, MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have been presented as optimization procedures that can replace MERT and scale to thousands of parameters. In a different line of work, Liang et al. (2006) describe a fully discriminative training pipeline, where more than one million features are tuned on the training data using a perceptron-style update algorithm. The Direct Translation Model 2 introduced 1517 by Ittycheriah and Roukos (2007) is similar in that it also trains millions of features on the training data. However, the weights are estimated based on a maximum entropy model an"
N15-1175,P10-1049,1,0.953347,"ted B LEU objective, that provides a clear training criterion, which is missing e.g. in MIRA estimation. Auli et al. (2014) report good results by applying the same objective function to reordering features, which are trained with stochastic gradient descent (SGD). Our work differs in several key aspects: (i) We propose to apply the RPROP algorithm, which yields superior results to GT, SGD and AdaGrad in our experimental comparison. (ii) For the first time, we apply maximum expected B LEU training on a data set as large as four million sentence pairs. (iii) We apply a leave-one-out heuristic (Wuebker et al., 2010) to make better use of the training data. (iv) We apply phrasal, lexical, reordering and triplet features. (v) Finally, we do not run MERT after each training iteration, which is expensive for large translation systems. 3 Statistical Translation System Our work can be applied to any statistical machine translation paradigm and we will present results on a standard phrase-based translation system (Koehn et al., 2003) and a hierarchical phrase-based translation system (Chiang, 2005). The translation process is implemented as a weighted log-linear combination of several models hm,Θ (E, F ), where"
N15-1175,C12-3061,1,0.935254,"Missing"
N15-1175,D13-1138,1,0.867616,"nglish BOLT Chinese English WMT German English 138K 2.63M 2.70M 75.4K 50.2K 4.08M 78.3M 85.9M 384K 817K 4.09M 105M 104M 659K 649K Table 1: Statistics for the bilingual training data of the IWSLT 2013 German→English, the DARPA BOLT Chinese→English and the WMT 2014 German→English tasks. and LDC English Gigaword corpora. The selection is based on cross-entropy difference (Moore and Lewis, 2010). This makes for a total of 1.7 billion running words for LM training. The baseline further contains a hierarchical reordering model (HRM) (Galley and Manning, 2008) and a 7-gram word class language model (Wuebker et al., 2013). On IWSLT, all results are averages over three independent MERT runs, and we evaluate statistical significance with MultEval (Clark et al., 2011). To confirm our findings, additional experiments are run on two large-scale tasks over strong baselines including recurrent neural language models. On the DARPA BOLT Chinese→English task we use our internal evaluation system as a baseline. It is a powerful hierarchical phrase-based SMT engine with 19 dense features, including an LSTM recurrent neural language model (Sundermeyer et al., 2012) and a hierarchical reordering model (Huck et al., 2013). T"
N15-1175,D13-1112,0,0.0146615,"lection. The training corpus is divided into several shards, on which features are updated via perceptron-style gradient descent. The authors present results showing that training on large data sets improves results over just using a small development corpus. Another approach based on the AdaGrad method that scales to large numbers of sparse features is proposed in (Green et al., 2013; Green et al., 2014). Different from our work, the authors use either the tuning sets or a small subsample of the training data (15k sentences) for discriminative training. A notably different idea is pursued by Yu et al. (2013), who present a large-scale training procedure that explicitly minimizes search errors. This is achieved by force-decoding the training data and updating at the point where the correct derivation drops off the beam. In (Blunsom et al., 2008), conditional random fields (CRFs) are trained within a hierarchical phrase-based translation framework. The hierarchical phrase-based paradigm is used to model the search space in model estimation and search, leaving the hypothesis weighting to CRF features. They constrain search by a beam width for gradient estimation and update the model with the help of"
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2149,P07-1019,0,0.0299407,"k. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The standard models integrated into our Jane systems are: phrase translation probabilities and lexical translation probabilities on phrase level, each for both translation directions, length 405 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405–412, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics penalties on word and phrase level, three binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count feat"
W11-2149,E09-1044,0,0.039482,"Missing"
W11-2149,P03-1054,0,0.00307413,"The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases that meet certain restrictions. The first possibility is what the authors call a fixed dependency structure. With the exception of one word within this phrase, called the head, no outside word may have a dependency within this phrase. Also, all inner words may only depend on each other or on t"
W11-2149,E03-1076,0,0.0408333,"pora were used additionally. For the 109 French-English and LDC Gigaword corpora RWTH applied the data selection technique described in Section 3.1. We examined two different language models, one with LDC data and one without. Systems were optimized on the newstest2009 data set, newstest2008 was used as test set. The scores for newstest2010 are included for completeness. 5.1 Morpho-Syntactic Analysis In order to reduce the source vocabulary size for the German→English translation, the source side was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003). To further reduce translation complexity, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). For additional experiments we used the TreeTagger (Schmid, 1995) to produce a lemmatized version of the German source. 5.2 Optimization Criterion We studied the impact of different optimization criteria on tranlsation performance. The usual practice is to optimize the scaling factors to maximize BLEU. We also experimented with two different combinations of BLEU and Translation Edit Rate (TER): TER−BLEU and TER−4BLEU. The first denotes the equally we"
W11-2149,E06-1005,1,0.834047,"rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count features and an n-gram language model. The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.3 System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (Matusov et al., 2006; Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 3 Translation Modeling We incorporated several novel methods into our systems for the WMT 2011 evaluation. This section provides a short survey of three of the methods which we suppose to be of particular interest. 3.1 Language Model Data Selection For the English and German language models, we applied the data selectio"
W11-2149,D09-1022,1,0.84817,"on automatically selected English data (cf. Section 3.1) from the provided resources including the 109 corpus and LDC Gigaword. The scaling factors of the log-linear model combination are optimized towards BLEU on newstest2009, newstest2010 is used as an unseen test set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment"
W11-2149,P10-2041,0,0.0393086,"). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 3 Translation Modeling We incorporated several novel methods into our systems for the WMT 2011 evaluation. This section provides a short survey of three of the methods which we suppose to be of particular interest. 3.1 Language Model Data Selection For the English and German language models, we applied the data selection method proposed in (Moore and Lewis, 2010). Each sentence is scored by the difference in cross-entropy between a language model trained from in-domain data and a language model trained from a similar-sized sample of the out-of-domain data. As in-domain data we used the news-commentary corpus. The out-of-domain data from which the data was selected are the news crawl corpus for both languages and for English the 109 corpus and the LDC Gigaword data. We used a 3-gram trained with the SRI toolkit to compute the cross-entropy. For the news crawl corpus, only 1/8 of the sentences were discarded. Of the 109 corpus we retained 1/2 and of the"
W11-2149,J03-1002,1,0.00864889,"overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the paper in Section 6. 2 Phrase-Based System Translation Systems For the WMT 2011 evaluation we utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The"
W11-2149,P03-1021,0,0.00696987,"ities and lexical translation probabilities on phrase level, each for both translation directions, length 405 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405–412, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics penalties on word and phrase level, three binary features marking hierarchical phrases, glue rule, and rules with non-terminals at the boundaries, sourceto-target and target-to-source phrase length ratios, four binary count features and an n-gram language model. The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.3 System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (Matusov et al., 2006; Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accord"
W11-2149,P08-1066,0,0.0240437,"are estimated from their relative frequencies in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases that meet certain restrictions. The first possibility is what the authors call a fixed dependency structure. With the exception of one word within this phrase, called the head, no outside word may have a d"
W11-2149,2010.amta-papers.8,1,0.791938,"t set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment (Wuebker et al., 408 2010) on WMT 2010 data only. 4.2 Experimental Results English→French The results for the English→French task are given in Table 4. We likewise submitted two contrastive systems for this translation direction. The first contrastive submission"
W11-2149,2008.iwslt-papers.7,1,0.86264,"t2009, newstest2010 is used as an unseen test set. 4.1 Experimental Results French→English The results for the French→English task are given in Table 3. RWTH’s three submissions – one primary and two contrastive – are labeled accordingly in the table. The first contrastive submission is a phrasebased system with a standard feature set plus an additional triplet lexicon model (Mauser et al., 2009). The triplet lexicon model was trained on in-domain news commentary data only. The second contrastive submission is a hierarchical Jane system with three syntax-based extensions: A parse match model (Vilar et al., 2008), soft syntactic labels (Stein et al., 2010), and the soft string-to-dependency extension as described in Section 3.3. The primary submission combines the phrase-based contrastive system, a hierarchical system that is very similar to the Jane contrastive submission but with a slightly worse language model, and an additional PBT system that has been trained with forced alignment (Wuebker et al., 408 2010) on WMT 2010 data only. 4.2 Experimental Results English→French The results for the English→French task are given in Table 4. We likewise submitted two contrastive systems for this translation"
W11-2149,W10-1738,1,0.820443,"h will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the paper in Section 6. 2 Phrase-Based System Translation Systems For the WMT 2011 evaluation we utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA++ (Och and Ney, 2003) 2.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) was employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The standard models integrated into our Jane systems are: phrase translation probabilities and lexical translation probabilities on"
W11-2149,P10-1049,1,0.92613,"tasks we applied a forced alignment procedure to train the phrase translation model with the EM algorithm, similar to the one described in (DeNero et al., 2006). Here, the phrase translation probabilities are estimated from their relative frequencies in the phrase-aligned training data. The phrase alignment is produced by a modified version of the translation decoder. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid experiments. A detailed description of the training procedure is given in (Wuebker et al., 2010). 3.3 Soft String-to-Dependency Given a dependency tree of the target language, we are able to introduce language models that span over longer distances than the usual n-grams, as in (Shen et al., 2008). To obtain dependency structures, we apply the Stanford parser (Klein and Manning, 2003) on the target side of the training material. RWTH’s open source hierarchical translation toolkit Jane has been extended to include dependency information in the phrase table and to build dependency trees on the output hypotheses at decoding time from this information. Shen et al. (2008) use only phrases tha"
W11-2149,W06-3108,1,0.91274,"rdering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation task by providing an overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the pap"
W11-2149,W06-3110,1,0.909489,"rdering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation task by providing an overview of our translation systems in Section 2. In addition to the baseline features, we adopted several novel methods, which will be presented in Section 3. Details on the respective setups and translation results for the French-English and German-English language pairs (in both translation directions) are given in Sections 4 and 5. We finally conclude the pap"
W11-2149,2008.iwslt-papers.8,1,0.857725,"f the EMNLP 2011 Sixth Workshop on Statistical Machine Translation. Both phrasebased and hierarchical SMT systems were trained for the constrained German-English and French-English tasks in all directions. Experiments were conducted to compare different training data sets, training methods and optimization criteria, as well as additional models on dependency structure and phrase reordering. Further, we applied a system combination technique to create a consensus hypothesis from several different systems. 1 2.1 We applied a phrase-based translation (PBT) system similar to the one described in (Zens and Ney, 2008). Phrase pairs are extracted from a word-aligned bilingual corpus and their translation probability in both directions is estimated by relative frequencies. The standard feature set moreover includes an n-gram language model, phrase-level single-word lexicons and word-, phrase- and distortion-penalties. To lexicalize reordering, a discriminative reordering model (Zens and Ney, 2006a) is used. Parameters are optimized with the Downhill-Simplex algorithm (Nelder and Mead, 1965) on the word graph. Overview We sketch the baseline architecture of RWTH’s setups for the WMT 2011 shared translation ta"
W11-2149,D07-1103,0,\N,Missing
W11-2149,W10-1723,0,\N,Missing
W11-2149,W08-0509,0,\N,Missing
W11-2149,P07-2045,0,\N,Missing
W11-2149,W10-1713,0,\N,Missing
W11-2149,2005.eamt-1.19,0,\N,Missing
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W13-2223,W13-0805,1,0.848446,"OS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its"
W13-2223,E03-1076,0,0.0855399,"n the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. 2.2 System Overview Karlsruhe Institute of Technology Single System 2.2.1 Preprocessing The training data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extra"
W13-2223,P07-2045,0,0.00527113,"ne Translation. The technique of Statistical Post-Editing (Dugast et al., 2007) is used to automatically edit the output of the rule-based system. A Statistical Post-Editing (SPE) module is generated from a bilingual corpus. It is basically a translation module by itself, however it is trained on rule-based • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained on 2M phrases from the news/europarl and CommonCrawl corpora, provided as training data for WMT 2013. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news development set. 5 http://geek.kyloo.net/software 6 The fifth edition of (LDC2011T07) was not used. the English Gigaword 188 0 5:that/1 7:this/3 1 3:is/3 8:was/1 2 0:*EPS*/3 4:it/1 0:*EPS*/3 2:in/1 3 4 0:*EPS*/3 6:the/1 5 0:*EPS*/1 1:future/3 6 Figure 1: Confusion network of four different hypotheses. 3 RWTH Aachen System Combination Table 1: Comparison of single systems tuned on newstest2009 and newstest2010. The results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different transla"
W13-2223,J04-2004,0,0.0419685,"d a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus mode"
W13-2223,W07-0734,0,0.0383886,"results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines. First, a word to word alignment for the given single system hypotheses is produced. In a second step a confusion network is constructed. Then, the hypothesis with the highest probability is extracted from this confusion network. For the alignment procedure, each of the given single systems generates one confusion network with its own as primary system. To this primary system all other hypotheses are aligned using the METEOR (Lavie and Agarwal, 2007) alignment and thus the primary system defines the word order. Once the alignment is given, the corresponding confusion network is constructed. An example is given in Figure 1. The final network for one source sentence is the union of all confusion networks generated from the different primary systems. That allows the system combination to select the word order from different system outputs. Before performing system combination, each translation output was normalized by tokenization and lowercasing. The output of the combination was then truecased based on the original truecased output. The mo"
W13-2223,W07-0732,0,0.0965924,"m = 10, and used k = 300. 2.3.4 translations and reference data. It applies corrections and adaptations learned from a phrase-based 5-gram language model. Using this two-step process will implicitly keep long distance relations and other constraints determined by the rule-based system while significantly improving phrasal fluency. It has the advantage that quality improvements can be achieved with very little but targeted bilingual data, thus significantly reducing training time and increasing translation performance. The basic setup of the SPE component is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the s"
W13-2223,N12-1005,0,0.0335346,"with standard n-gram translation models is that the elementary units are bilingual pairs, which means that the underlying vocabulary can be quite large, even for small translation tasks. Unfortunately, the parallel data available to train these models are typically order of magnitudes smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that LIMSI-CNRS Single System 2.3.1 System overview LIMSI’s system is built with n-code (Crego et al., 2011), an open source statistical machine translation system based on bilingual n-gram3 . In this approach, the translation model relies on a specific decomposition of the joint probability of a sentence pair using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual u"
W13-2223,2010.iwslt-papers.6,0,0.0342874,"Missing"
W13-2223,2012.iwslt-papers.7,1,0.838536,"an compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse tree"
W13-2223,W08-0310,0,0.0216057,"nal in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the same setup as last year6 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we also took advantage of our inhouse text processing tools for the tokenization and detokenization steps (Dchelotte et al., 2008) and our system is built in “true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar ElKahlout and Yvon, 2010)), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 • Named entities are re"
W13-2223,2011.iwslt-papers.5,1,0.849659,"processed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering"
W13-2223,J06-4004,0,0.0399278,"Missing"
W13-2223,D09-1022,1,0.905747,"Missing"
W13-2223,D08-1089,0,0.0206813,"Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185–192, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics additional feature are added to the log-linear combination. The model weights are optimized with standard Mert (Och, 2003a) on 200-best lists. The optimization criterion is B LEU. cleaner corpora, EPPS and NC. Assuming that this corpus is very noisy, we biased our classifier more towards precision than recall. This was realized by giving higher number of f"
W13-2223,2011.iwslt-evaluation.9,1,0.888245,"ing data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilin"
W13-2223,W09-0435,1,0.861514,"ith regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering"
W13-2223,W08-0303,1,0.903038,"Missing"
W13-2223,N04-4026,0,0.016377,"y so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overal"
W13-2223,W09-0413,1,0.842391,"The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the refe"
W13-2223,W05-0836,1,0.864882,"filtering task). 2.1.1 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short"
W13-2223,W11-2124,1,0.875547,"this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model r"
W13-2223,C12-3061,1,0.817205,"e of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2"
W13-2223,J03-1002,1,0.00903936,"translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings"
W13-2223,P03-1021,0,0.694457,"models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overall search is based on a beam-search strategy on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and Mario, 2006). 2.2.6 Language Models We build separate language models and combined them prior to decoding. As word-token based language models, one language model is built on EPPS, NC, and giga corpus, while another one is built u"
W13-2223,W08-1006,0,0.0614361,"and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its probability added as an additional score. 2.1.3 Language Model During decoding a 4-"
W13-2223,2007.tmi-papers.21,0,0.168794,") is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser"
W13-2223,W12-3140,1,\N,Missing
W13-2223,W11-2135,0,\N,Missing
W13-2223,W10-1704,0,\N,Missing
W14-0808,C02-1096,0,0.0982722,"Missing"
W14-0808,P11-2031,0,0.0667223,"Missing"
W14-0808,N13-1073,0,0.0307287,"Missing"
W14-0808,W10-1734,0,0.0398686,"Missing"
W14-0808,E03-1076,0,0.0530794,"raining environments Taking the normalised version of our corpus as a baseline, different training environments have been tested. We designed ﬁve possible training environments in which German compounds were preprocessed. In our ﬁrst experiment (hereinafter “compList”), the list of manually extracted compounds was appended to the end of the training set and no further preprocessing was carried out. In our second experiment (hereinafter “RWTH”), the state-of-the-art compound splitting approach implemented by Popović et al. (2006) was used to split all possible compounds. As also implemented by Koehn and Knight (2003), this approach uses the corpus itself to create a vocabulary that is then subsequently used to calculate the possible splittings in the corpus. It has the advantage of being a stand-alone approach which does not depend on any external resources. A possible drawback of this approach would be that it relies on a large corpus to be able to compute the splittings. Thus, it may not be as efﬁcient with smaller corpora (i.e. if we were to use only the TRIS corpus, for instance). The third experiment (hereinafter “RWTH+compList”) used the split corpus prepared in our second experiment (“RWTH”) but me"
W14-0808,2005.mtsummit-papers.11,0,0.0333363,"Missing"
W14-0808,P03-1021,0,0.14746,"Missing"
W14-0808,2006.amta-papers.25,0,0.0842276,"Missing"
W14-0808,2001.mtsummit-papers.68,0,0.128211,"Missing"
W14-0808,W10-1738,1,0.847398,"le is rather oral and less technical. As compounds tend to be more frequent in domain speciﬁc texts, the TRIS corpus has been used for testing, while the Europarl Corpus has been used in the training set to avoid data scarcity problems and increase the vocabulary coverage of the SMT system. In the case of Machine Translation (MT), both rule-based MT systems (RBMT systems) and Statistical MT systems (SMT systems) encounter problems when dealing with compounds. For the purposes of this paper, the treatment of compounds in German has been tested within the SMT toolkit Jane (Wuebker et al., 2012; Vilar et al., 2010). We have carried out several experiments translating German specialized texts into Spanish to test to which extent incorporating a linguistic analysis of the corpora and compiling compound lists improves the overall SMT results. At this stage, including further linguistic information such as Partof-Speech tagging (POS tagging) or phrase chunking has been disregarded. Forcing the translation of compounds in the phrase tables produced by Jane has also been disregarded. The overall aim was to test how the SMT system performs using different pre-processing strategies of the training data but with"
W14-0808,escartin-2012-design,1,0.268806,"Missing"
W14-0808,weller-heid-2012-analyzing,0,0.0160098,"tenation of the complete Europarl Corpus German→Spanish and a greater part of the TRIS corpus, while in dev and test only texts from the TRIS corpus were used. the list of splittings to be carried out in the corpus. Thus, after all possible splittings were calculated, those splittings that were present in the manually compiled compound list were deleted to ensure that they were not split in the corpus and remained the same. In the fourth experiment (hereinafter “IMS”) we used another compound splitter developed at the Institut für Maschinelle Sprachverarbeitung of the University of Stuttgart (Weller and Heid, 2012). This splitter was also developed using a frequencybased approach. However, in this case the training data consists of a list of lemmatized wordforms together with their POS tags. A set of rules to model transitional elements is also used. While this splitter might be used by processing our corpus with available tools such as TreeTagger (Schmid, 1994)6 and then computing frequencies, in our experiments we used the CELEX7 database for German (Baayen et al., 1993). This was done so because CELEX is an extensive high quality lexical database which already included all the information we needed t"
W14-0808,2008.iwslt-papers.8,1,0.886999,"Missing"
W14-0808,J09-2003,0,\N,Missing
W14-0808,J13-4009,0,\N,Missing
W14-0808,P02-1040,0,\N,Missing
W14-0808,C12-3061,1,\N,Missing
W14-3310,E14-2008,1,0.50059,"nburgh, Scotland † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ {freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ‡ {mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ‡ v1rsennr@staffmail.ed.ac.uk ‡ maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk † {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu ‡ Matthias Abstract joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B"
W14-3310,D08-1089,0,0.0336771,"sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been emp"
W14-3310,N04-1035,0,0.0285873,"gmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an"
W14-3310,P05-1066,1,0.0515024,"employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tune"
W14-3310,P06-1121,0,0.0128251,". The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model"
W14-3310,P13-2071,1,0.0664637,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,2012.iwslt-papers.17,1,0.805256,".1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel"
W14-3310,W13-2212,1,0.898684,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,P12-1031,0,0.00714997,"um Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes"
W14-3310,P13-2121,1,0.0604984,"Missing"
W14-3310,W14-3309,1,0.840972,"btained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translatio"
W14-3310,W11-2123,0,0.00995075,"a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus"
W14-3310,W06-1607,0,0.0222136,"., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering"
W14-3310,W13-0805,1,0.0274022,"GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with s"
W14-3310,2011.iwslt-papers.5,1,0.681344,"rying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase t"
W14-3310,2009.iwslt-papers.4,1,0.346713,"bility, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize B LEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser"
W14-3310,D09-1022,1,0.0473567,"ingle generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on newstest2011 or newstest2012. We kept newstest2013 as an unseen test set wh"
W14-3310,P07-1019,0,0.0254758,"he settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language mod"
W14-3310,2011.iwslt-evaluation.9,1,0.882056,"n, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is d"
W14-3310,W13-2258,1,0.0602517,"r IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHK"
W14-3310,P10-2041,0,0.0226125,"for tuning the system combination or any of the individual systems. In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 6.1 The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1. KIT, UEDIN and RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model. The pairwise difference of the single system performances is up to 1.3 points in B LEU and 2.5 points in T ER. For German→English, our system combination p"
W14-3310,W14-3362,1,0.700694,"ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT trans"
W14-3310,W09-0435,0,0.00463497,"erated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained"
W14-3310,P03-1054,0,0.00425242,"ion obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the sy"
W14-3310,W08-0303,0,0.0192279,"sing an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word ord"
W14-3310,D07-1091,1,0.0290057,"2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→"
W14-3310,E03-1076,1,0.31096,"the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och"
W14-3310,W11-2124,1,0.0228284,"trip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on news"
W14-3310,2005.iwslt-1.8,1,0.035532,"2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single"
W14-3310,J03-1002,1,0.0147027,"of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid,"
W14-3310,E99-1010,0,0.0419329,"l., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 div"
W14-3310,P07-2045,1,0.0154876,"th a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004),"
W14-3310,P03-1021,0,0.0102254,"03) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has"
W14-3310,W14-3317,1,0.820032,"ty, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in B LEU and 1.0 points in T ER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering"
W14-3310,N04-1022,0,0.063305,"it (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual train"
W14-3310,W10-1738,1,0.159493,"Missing"
W14-3310,W08-1005,0,0.0331415,"nce reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2"
W14-3310,P06-1055,0,0.0182004,"dditional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs ar"
W14-3310,W12-3150,1,0.544959,"uster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a"
W14-3310,W08-1006,0,0.0232292,"e system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koeh"
W14-3310,W14-3324,1,0.0949085,"Missing"
W14-3310,2007.tmi-papers.21,0,0.125992,", 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system tra"
W14-3310,C12-3061,1,0.125144,"013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has"
W14-3310,C08-1098,0,0.00933067,"target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newst"
W14-3310,D13-1138,1,0.0721313,", morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules ("
W14-3310,C04-1024,0,0.0194264,"f the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottman"
W14-3310,R13-1079,1,0.28133,"stem and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster e"
W14-3310,N07-1051,0,\N,Missing
W14-3310,W05-0836,1,\N,Missing
W14-3310,W05-0909,0,\N,Missing
W14-3310,N13-1001,1,\N,Missing
W14-3310,2010.iwslt-evaluation.11,1,\N,Missing
W14-3310,2013.iwslt-evaluation.16,1,\N,Missing
W14-3310,W13-2213,1,\N,Missing
W14-3310,W14-3313,1,\N,Missing
W14-3310,W11-2145,1,\N,Missing
W14-3310,2013.iwslt-evaluation.3,1,\N,Missing
W14-3317,popovic-ney-2006-pos,1,\N,Missing
W14-3317,N04-4026,0,\N,Missing
W14-3317,E03-1076,0,\N,Missing
W14-3317,D08-1089,0,\N,Missing
W14-3317,P12-1031,0,\N,Missing
W14-3317,P02-1040,0,\N,Missing
W14-3317,W10-1738,1,\N,Missing
W14-3317,D13-1138,1,\N,Missing
W14-3317,P10-2041,0,\N,Missing
W14-3317,P10-1049,1,\N,Missing
W14-3317,W13-2258,1,\N,Missing
W14-3317,J03-1002,1,\N,Missing
W14-3317,W13-0804,1,\N,Missing
W14-3317,C12-3061,1,\N,Missing
W14-3317,W14-3310,1,\N,Missing
W14-3317,2012.iwslt-papers.7,1,\N,Missing
W14-3317,J07-2003,0,\N,Missing
W14-3317,W11-2123,0,\N,Missing
W14-3317,P13-2121,0,\N,Missing
W14-3317,P03-1021,0,\N,Missing
W14-3317,2011.iwslt-papers.5,1,\N,Missing
W15-3060,E06-1005,1,\N,Missing
W15-3060,E99-1010,0,\N,Missing
W15-3060,D08-1088,1,\N,Missing
W15-3060,P02-1040,0,\N,Missing
W15-3060,P04-1077,0,\N,Missing
W15-3060,P08-2021,0,\N,Missing
W15-3060,W12-3140,1,\N,Missing
W15-3060,N07-2017,1,\N,Missing
W15-3060,E14-2008,1,\N,Missing
W15-3060,N07-1029,0,\N,Missing
W15-3060,J97-3002,0,\N,Missing
W15-3060,D08-1011,0,\N,Missing
W15-3060,W04-3250,0,\N,Missing
W15-3060,W14-3310,1,\N,Missing
W15-3060,N12-1005,0,\N,Missing
W15-3060,W13-2223,1,\N,Missing
